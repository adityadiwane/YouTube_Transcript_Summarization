WEBVTT
Kind: captions
Language: en

00:00:25.476 --> 00:00:26.627
I do two things:

00:00:26.651 --> 00:00:28.769
I design mobile computers
and I study brains.

00:00:28.793 --> 00:00:31.723
Today's talk is about brains
and -- (Audience member cheers)

00:00:31.747 --> 00:00:33.564
Yay! I have a brain fan out there.

00:00:33.588 --> 00:00:36.735
(Laughter)

00:00:36.759 --> 00:00:38.314
If I could have my first slide,

00:00:38.338 --> 00:00:41.187
you'll see the title of my talk
and my two affiliations.

00:00:41.211 --> 00:00:44.679
So what I'm going to talk about is why
we don't have a good brain theory,

00:00:44.703 --> 00:00:46.980
why it is important
that we should develop one

00:00:47.004 --> 00:00:48.487
and what we can do about it.

00:00:48.511 --> 00:00:50.335
I'll try to do all that in 20 minutes.

00:00:50.359 --> 00:00:51.510
I have two affiliations.

00:00:51.534 --> 00:00:54.066
Most of you know me
from my Palm and Handspring days,

00:00:54.090 --> 00:00:56.773
but I also run a nonprofit
scientific research institute

00:00:56.797 --> 00:00:59.429
called the Redwood Neuroscience
Institute in Menlo Park.

00:00:59.453 --> 00:01:02.841
We study theoretical neuroscience
and how the neocortex works.

00:01:02.865 --> 00:01:04.463
I'm going to talk all about that.

00:01:04.487 --> 00:01:07.232
I have one slide on my other life,
the computer life,

00:01:07.256 --> 00:01:08.557
and that's this slide here.

00:01:08.581 --> 00:01:11.849
These are some of the products
I've worked on over the last 20 years,

00:01:11.873 --> 00:01:13.715
starting from the very original laptop

00:01:13.739 --> 00:01:15.526
to some of the first tablet computers

00:01:15.550 --> 00:01:17.848
and so on, ending up
most recently with the Treo,

00:01:17.872 --> 00:01:19.404
and we're continuing to do this.

00:01:19.428 --> 00:01:21.729
I've done this because
I believe mobile computing

00:01:21.753 --> 00:01:23.477
is the future of personal computing,

00:01:23.501 --> 00:01:25.955
and I'm trying to make
the world a little bit better

00:01:25.979 --> 00:01:27.275
by working on these things.

00:01:27.299 --> 00:01:29.173
But this was, I admit, all an accident.

00:01:29.197 --> 00:01:31.505
I really didn't want to do
any of these products.

00:01:31.529 --> 00:01:32.911
Very early in my career

00:01:32.935 --> 00:01:35.625
I decided I was not going to be
in the computer industry.

00:01:35.649 --> 00:01:37.370
Before that, I just have to tell you

00:01:37.394 --> 00:01:40.502
about this picture of Graffiti
I picked off the web the other day.

00:01:40.526 --> 00:01:43.779
I was looking for a picture for Graffiti
that'll text input language.

00:01:43.803 --> 00:01:47.492
I found a website dedicated to teachers
who want to make script-writing things

00:01:47.516 --> 00:01:49.190
across the top of their blackboard,

00:01:49.214 --> 00:01:52.047
and they had added Graffiti to it,
and I'm sorry about that.

00:01:52.071 --> 00:01:54.318
(Laughter)

00:01:54.342 --> 00:01:55.642
So what happened was,

00:01:55.666 --> 00:02:00.565
when I was young and got out
of engineering school at Cornell in '79,

00:02:00.589 --> 00:02:03.776
I went to work for Intel
and was in the computer industry,

00:02:03.800 --> 00:02:07.202
and three months into that,
I fell in love with something else.

00:02:07.226 --> 00:02:10.270
I said, "I made
the wrong career choice here,"

00:02:10.294 --> 00:02:12.533
and I fell in love with brains.

00:02:12.557 --> 00:02:14.090
This is not a real brain.

00:02:14.114 --> 00:02:16.833
This is a picture of one, a line drawing.

00:02:16.857 --> 00:02:18.976
And I don't remember
exactly how it happened,

00:02:19.000 --> 00:02:22.515
but I have one recollection,
which was pretty strong in my mind.

00:02:22.539 --> 00:02:24.149
In September of 1979,

00:02:24.173 --> 00:02:27.537
Scientific American came out
with a single-topic issue about the brain.

00:02:27.561 --> 00:02:29.499
It was one of their best issues ever.

00:02:29.523 --> 00:02:32.470
They talked about the neuron,
development, disease, vision

00:02:32.494 --> 00:02:35.090
and all the things you might want
to know about brains.

00:02:35.114 --> 00:02:36.616
It was really quite impressive.

00:02:36.640 --> 00:02:39.412
One might've had the impression
we knew a lot about brains.

00:02:39.436 --> 00:02:43.631
But the last article in that issue
was written by Francis Crick of DNA fame.

00:02:43.655 --> 00:02:46.679
Today is, I think, the 50th anniversary
of the discovery of DNA.

00:02:46.703 --> 00:02:49.778
And he wrote a story basically saying,
this is all well and good,

00:02:49.802 --> 00:02:52.545
but you know, we don't know
diddly squat about brains,

00:02:52.569 --> 00:02:54.308
and no one has a clue how they work,

00:02:54.332 --> 00:02:56.198
so don't believe what anyone tells you.

00:02:56.222 --> 00:02:58.387
This is a quote
from that article, he says:

00:02:58.411 --> 00:03:02.704
"What is conspicuously lacking" --
he's a very proper British gentleman --

00:03:02.728 --> 00:03:05.558
"What is conspicuously lacking
is a broad framework of ideas

00:03:05.582 --> 00:03:07.934
in which to interpret
these different approaches."

00:03:07.958 --> 00:03:09.926
I thought the word "framework" was great.

00:03:09.950 --> 00:03:11.767
He didn't say we didn't have a theory.

00:03:11.791 --> 00:03:14.516
He says we don't even know
how to begin to think about it.

00:03:14.540 --> 00:03:16.032
We don't even have a framework.

00:03:16.056 --> 00:03:19.106
We are in the pre-paradigm days,
if you want to use Thomas Kuhn.

00:03:19.130 --> 00:03:20.469
So I fell in love with this.

00:03:20.493 --> 00:03:24.068
I said, look: We have all this knowledge
about brains -- how hard can it be?

00:03:24.092 --> 00:03:27.530
It's something we can work on
in my lifetime; I could make a difference.

00:03:27.554 --> 00:03:31.173
So I tried to get out of the computer
business, into the brain business.

00:03:31.197 --> 00:03:33.201
First, I went to MIT,
the AI lab was there.

00:03:33.225 --> 00:03:35.620
I said, I want to build
intelligent machines too,

00:03:35.644 --> 00:03:38.161
but I want to study how brains work first.

00:03:38.185 --> 00:03:40.491
And they said, "Oh, you
don't need to do that.

00:03:40.515 --> 00:03:42.905
You're just going to program
computers, that's all.

00:03:42.929 --> 00:03:44.892
I said, you really ought to study brains.

00:03:44.916 --> 00:03:46.348
They said, "No, you're wrong."

00:03:46.372 --> 00:03:48.618
I said, "No, you're wrong,"
and I didn't get in.

00:03:48.642 --> 00:03:49.720
(Laughter)

00:03:49.744 --> 00:03:51.899
I was a little disappointed --
pretty young --

00:03:51.923 --> 00:03:53.859
but I went back again a few years later,

00:03:53.883 --> 00:03:56.242
this time in California,
and I went to Berkeley.

00:03:56.266 --> 00:03:58.696
And I said, I'll go
in from the biological side.

00:03:58.720 --> 00:04:01.809
So I got in the PhD program in biophysics.

00:04:01.833 --> 00:04:05.243
I was like, I'm studying brains now.
Well, I want to study theory.

00:04:05.267 --> 00:04:07.536
They said, "You can't
study theory about brains.

00:04:07.560 --> 00:04:09.555
You can't get funded for that.

00:04:09.579 --> 00:04:11.734
And as a graduate student,
you can't do that."

00:04:11.758 --> 00:04:12.976
So I said, oh my gosh.

00:04:13.000 --> 00:04:16.155
I was depressed; I said, but I can
make a difference in this field.

00:04:16.179 --> 00:04:18.187
I went back in the computer industry

00:04:18.211 --> 00:04:20.316
and said, I'll have to work
here for a while.

00:04:20.340 --> 00:04:22.733
That's when I designed
all those computer products.

00:04:22.757 --> 00:04:24.058
(Laughter)

00:04:24.082 --> 00:04:26.976
I said, I want to do this
for four years, make some money,

00:04:27.000 --> 00:04:30.976
I was having a family,
and I would mature a bit,

00:04:31.000 --> 00:04:33.816
and maybe the business
of neuroscience would mature a bit.

00:04:33.840 --> 00:04:36.841
Well, it took longer than four years.
It's been about 16 years.

00:04:36.865 --> 00:04:39.581
But I'm doing it now,
and I'm going to tell you about it.

00:04:39.605 --> 00:04:41.891
So why should we have a good brain theory?

00:04:41.915 --> 00:04:45.017
Well, there's lots of reasons
people do science.

00:04:45.041 --> 00:04:47.958
The most basic one is,
people like to know things.

00:04:47.982 --> 00:04:50.177
We're curious, and we go out
and get knowledge.

00:04:50.201 --> 00:04:52.067
Why do we study ants? It's interesting.

00:04:52.091 --> 00:04:55.557
Maybe we'll learn something useful,
but it's interesting and fascinating.

00:04:55.581 --> 00:04:57.638
But sometimes a science
has other attributes

00:04:57.662 --> 00:04:59.491
which makes it really interesting.

00:04:59.515 --> 00:05:02.142
Sometimes a science will tell
something about ourselves;

00:05:02.166 --> 00:05:03.390
it'll tell us who we are.

00:05:03.414 --> 00:05:06.166
Evolution did this
and Copernicus did this,

00:05:06.190 --> 00:05:08.524
where we have a new
understanding of who we are.

00:05:08.548 --> 00:05:11.976
And after all, we are our brains.
My brain is talking to your brain.

00:05:12.000 --> 00:05:14.030
Our bodies are hanging along for the ride,

00:05:14.054 --> 00:05:15.879
but my brain is talking to your brain.

00:05:15.903 --> 00:05:19.151
And if we want to understand
who we are and how we feel and perceive,

00:05:19.175 --> 00:05:20.566
we need to understand brains.

00:05:20.590 --> 00:05:24.374
Another thing is sometimes science leads
to big societal benefits, technologies,

00:05:24.398 --> 00:05:25.689
or businesses or whatever.

00:05:25.713 --> 00:05:28.591
This is one, too, because
when we understand how brains work,

00:05:28.615 --> 00:05:30.679
we'll be able to build
intelligent machines.

00:05:30.703 --> 00:05:32.401
That's a good thing on the whole,

00:05:32.425 --> 00:05:34.283
with tremendous benefits to society,

00:05:34.307 --> 00:05:35.976
just like a fundamental technology.

00:05:36.000 --> 00:05:38.850
So why don't we have
a good theory of brains?

00:05:38.874 --> 00:05:41.042
People have been working
on it for 100 years.

00:05:41.066 --> 00:05:43.785
Let's first take a look
at what normal science looks like.

00:05:43.809 --> 00:05:44.996
This is normal science.

00:05:45.020 --> 00:05:49.094
Normal science is a nice balance
between theory and experimentalists.

00:05:49.118 --> 00:05:51.809
The theorist guy says,
"I think this is what's going on,"

00:05:51.833 --> 00:05:53.794
the experimentalist says, "You're wrong."

00:05:53.818 --> 00:05:56.822
It goes back and forth,
this works in physics, this in geology.

00:05:56.846 --> 00:05:59.855
But if this is normal science,
what does neuroscience look like?

00:05:59.879 --> 00:06:01.674
This is what neuroscience looks like.

00:06:01.698 --> 00:06:03.140
We have this mountain of data,

00:06:03.164 --> 00:06:05.234
which is anatomy, physiology and behavior.

00:06:05.258 --> 00:06:08.452
You can't imagine how much detail
we know about brains.

00:06:08.476 --> 00:06:12.068
There were 28,000 people who went
to the neuroscience conference this year,

00:06:12.092 --> 00:06:14.455
and every one of them
is doing research in brains.

00:06:14.479 --> 00:06:16.173
A lot of data, but no theory.

00:06:16.197 --> 00:06:18.197
There's a little wimpy box on top there.

00:06:18.221 --> 00:06:21.603
And theory has not played a role
in any sort of grand way

00:06:21.627 --> 00:06:23.056
in the neurosciences.

00:06:23.080 --> 00:06:24.320
And it's a real shame.

00:06:24.344 --> 00:06:25.735
Now, why has this come about?

00:06:25.759 --> 00:06:28.747
If you ask neuroscientists
why is this the state of affairs,

00:06:28.771 --> 00:06:30.017
first, they'll admit it.

00:06:30.041 --> 00:06:31.526
But if you ask them, they say,

00:06:31.550 --> 00:06:34.282
there's various reasons
we don't have a good brain theory.

00:06:34.306 --> 00:06:36.275
Some say we still don't have enough data,

00:06:36.299 --> 00:06:39.358
we need more information,
there's all these things we don't know.

00:06:39.382 --> 00:06:42.223
Well, I just told you there's data
coming out of your ears.

00:06:42.247 --> 00:06:45.411
We have so much information,
we don't even know how to organize it.

00:06:45.435 --> 00:06:46.873
What good is more going to do?

00:06:46.897 --> 00:06:50.345
Maybe we'll be lucky and discover
some magic thing, but I don't think so.

00:06:50.369 --> 00:06:53.342
This is a symptom of the fact
that we just don't have a theory.

00:06:53.366 --> 00:06:55.976
We don't need more data,
we need a good theory.

00:06:56.000 --> 00:06:57.798
Another one is sometimes people say,

00:06:57.822 --> 00:07:00.976
"Brains are so complex,
it'll take another 50 years."

00:07:01.000 --> 00:07:04.354
I even think Chris said something
like this yesterday, something like,

00:07:04.378 --> 00:07:07.005
it's one of the most complicated
things in the universe.

00:07:07.029 --> 00:07:09.819
That's not true -- you're more
complicated than your brain.

00:07:09.843 --> 00:07:10.994
You've got a brain.

00:07:11.018 --> 00:07:13.168
And although the brain
looks very complicated,

00:07:13.192 --> 00:07:15.528
things look complicated
until you understand them.

00:07:15.552 --> 00:07:16.887
That's always been the case.

00:07:16.911 --> 00:07:20.154
So we can say, my neocortex,
the part of the brain I'm interested in,

00:07:20.178 --> 00:07:21.330
has 30 billion cells.

00:07:21.354 --> 00:07:23.786
But, you know what?
It's very, very regular.

00:07:23.810 --> 00:07:27.204
In fact, it looks like it's the same thing
repeated over and over again.

00:07:27.228 --> 00:07:29.764
It's not as complex as it looks.
That's not the issue.

00:07:29.788 --> 00:07:32.075
Some people say,
brains can't understand brains.

00:07:32.099 --> 00:07:34.087
Very Zen-like. Woo.

00:07:34.111 --> 00:07:36.299
(Laughter)

00:07:36.323 --> 00:07:39.182
You know, it sounds good, but why?
I mean, what's the point?

00:07:39.206 --> 00:07:41.775
It's just a bunch of cells.
You understand your liver.

00:07:41.799 --> 00:07:43.776
It's got a lot of cells in it too, right?

00:07:43.800 --> 00:07:46.294
So, you know, I don't think
there's anything to that.

00:07:46.318 --> 00:07:48.430
And finally, some people say,

00:07:48.454 --> 00:07:51.437
"I don't feel like a bunch
of cells -- I'm conscious.

00:07:51.461 --> 00:07:53.530
I've got this experience,
I'm in the world.

00:07:53.554 --> 00:07:55.464
I can't be just a bunch of cells."

00:07:55.488 --> 00:07:58.711
Well, people used to believe
there was a life force to be living,

00:07:58.735 --> 00:08:01.144
and we now know
that's really not true at all.

00:08:01.168 --> 00:08:03.066
And there's really no evidence,

00:08:03.090 --> 00:08:06.464
other than that people just disbelieve
that cells can do what they do.

00:08:06.488 --> 00:08:09.529
So some people have fallen
into the pit of metaphysical dualism,

00:08:09.553 --> 00:08:12.283
some really smart people, too,
but we can reject all that.

00:08:12.307 --> 00:08:15.202
(Laughter)

00:08:15.226 --> 00:08:16.967
No, there's something else,

00:08:16.991 --> 00:08:18.976
something really fundamental, and it is:

00:08:19.000 --> 00:08:21.451
another reason why we don't have
a good brain theory

00:08:21.475 --> 00:08:27.010
is because we have an intuitive,
strongly held but incorrect assumption

00:08:27.034 --> 00:08:29.146
that has prevented us
from seeing the answer.

00:08:29.170 --> 00:08:32.958
There's something we believe that just,
it's obvious, but it's wrong.

00:08:32.982 --> 00:08:36.548
Now, there's a history of this in science
and before I tell you what it is,

00:08:36.572 --> 00:08:38.871
I'll tell you about the history
of it in science.

00:08:38.895 --> 00:08:40.805
Look at other scientific revolutions --

00:08:40.829 --> 00:08:42.708
the solar system, that's Copernicus,

00:08:42.732 --> 00:08:45.551
Darwin's evolution,
and tectonic plates, that's Wegener.

00:08:46.059 --> 00:08:48.354
They all have a lot in common
with brain science.

00:08:48.378 --> 00:08:51.044
First, they had a lot
of unexplained data. A lot of it.

00:08:51.068 --> 00:08:53.862
But it got more manageable
once they had a theory.

00:08:53.886 --> 00:08:56.693
The best minds were stumped --
really smart people.

00:08:56.717 --> 00:08:58.721
We're not smarter now than they were then;

00:08:58.745 --> 00:09:01.272
it just turns out it's really
hard to think of things,

00:09:01.296 --> 00:09:03.972
but once you've thought of them,
it's easy to understand.

00:09:03.996 --> 00:09:06.102
My daughters understood
these three theories,

00:09:06.126 --> 00:09:08.644
in their basic framework, in kindergarten.

00:09:08.668 --> 00:09:11.934
It's not that hard --
here's the apple, here's the orange,

00:09:11.958 --> 00:09:13.976
the Earth goes around, that kind of stuff.

00:09:14.000 --> 00:09:16.586
Another thing is the answer
was there all along,

00:09:16.610 --> 00:09:19.389
but we kind of ignored it
because of this obvious thing.

00:09:19.413 --> 00:09:22.263
It was an intuitive,
strongly held belief that was wrong.

00:09:22.287 --> 00:09:23.977
In the case of the solar system,

00:09:24.001 --> 00:09:25.761
the idea that the Earth is spinning,

00:09:25.785 --> 00:09:27.976
the surface is going
a thousand miles an hour,

00:09:28.000 --> 00:09:31.249
and it's going through the solar system
at a million miles an hour --

00:09:31.273 --> 00:09:33.749
this is lunacy; we all know
the Earth isn't moving.

00:09:33.773 --> 00:09:36.650
Do you feel like you're moving
a thousand miles an hour?

00:09:36.674 --> 00:09:39.593
If you said Earth was spinning
around in space and was huge --

00:09:39.617 --> 00:09:42.208
they would lock you up,
that's what they did back then.

00:09:42.232 --> 00:09:45.507
So it was intuitive and obvious.
Now, what about evolution?

00:09:45.531 --> 00:09:46.685
Evolution, same thing.

00:09:46.709 --> 00:09:49.789
We taught our kids the Bible says
God created all these species,

00:09:49.813 --> 00:09:52.956
cats are cats; dogs are dogs;
people are people; plants are plants;

00:09:52.980 --> 00:09:54.221
they don't change.

00:09:54.245 --> 00:09:56.894
Noah put them on the ark
in that order, blah, blah.

00:09:56.918 --> 00:10:00.313
The fact is, if you believe in evolution,
we all have a common ancestor.

00:10:00.337 --> 00:10:03.619
We all have a common ancestor
with the plant in the lobby!

00:10:03.643 --> 00:10:07.329
This is what evolution tells us.
And it's true. It's kind of unbelievable.

00:10:07.353 --> 00:10:09.910
And the same thing about tectonic plates.

00:10:09.934 --> 00:10:11.656
All the mountains and the continents

00:10:11.680 --> 00:10:14.024
are kind of floating around
on top of the Earth.

00:10:14.048 --> 00:10:15.294
It doesn't make any sense.

00:10:15.318 --> 00:10:19.919
So what is the intuitive,
but incorrect assumption,

00:10:19.943 --> 00:10:21.910
that's kept us from understanding brains?

00:10:21.934 --> 00:10:25.227
I'll tell you. It'll seem obvious
that it's correct. That's the point.

00:10:25.251 --> 00:10:28.685
Then I'll make an argument why
you're incorrect on the other assumption.

00:10:28.709 --> 00:10:30.391
The intuitive but obvious thing is:

00:10:30.415 --> 00:10:32.729
somehow, intelligence
is defined by behavior;

00:10:32.753 --> 00:10:35.103
we're intelligent
because of how we do things

00:10:35.127 --> 00:10:36.699
and how we behave intelligently.

00:10:36.723 --> 00:10:38.602
And I'm going to tell you that's wrong.

00:10:38.626 --> 00:10:40.757
Intelligence is defined by prediction.

00:10:40.781 --> 00:10:43.196
I'm going to work you
through this in a few slides,

00:10:43.220 --> 00:10:45.314
and give you an example
of what this means.

00:10:45.338 --> 00:10:46.639
Here's a system.

00:10:46.663 --> 00:10:49.571
Engineers and scientists
like to look at systems like this.

00:10:49.595 --> 00:10:52.758
They say, we have a thing in a box.
We have its inputs and outputs.

00:10:52.782 --> 00:10:56.022
The AI people said, the thing in the box
is a programmable computer,

00:10:56.046 --> 00:10:57.725
because it's equivalent to a brain.

00:10:57.749 --> 00:11:01.255
We'll feed it some inputs and get it
to do something, have some behavior.

00:11:01.279 --> 00:11:04.101
Alan Turing defined the Turing test,
which essentially says,

00:11:04.125 --> 00:11:07.678
we'll know if something's intelligent
if it behaves identical to a human --

00:11:07.702 --> 00:11:09.808
a behavioral metric
of what intelligence is

00:11:09.832 --> 00:11:11.976
that has stuck in our minds
for a long time.

00:11:12.000 --> 00:11:14.392
Reality, though --
I call it real intelligence.

00:11:14.416 --> 00:11:16.591
Real intelligence
is built on something else.

00:11:16.615 --> 00:11:19.829
We experience the world
through a sequence of patterns,

00:11:19.853 --> 00:11:22.002
and we store them, and we recall them.

00:11:22.026 --> 00:11:24.571
When we recall them,
we match them up against reality,

00:11:24.595 --> 00:11:26.846
and we're making predictions all the time.

00:11:26.870 --> 00:11:29.828
It's an internal metric;
there's an internal metric about us,

00:11:29.852 --> 00:11:33.194
saying, do we understand the world,
am I making predictions, and so on.

00:11:33.218 --> 00:11:36.220
You're all being intelligent now,
but you're not doing anything.

00:11:36.244 --> 00:11:39.246
Maybe you're scratching yourself,
but you're not doing anything.

00:11:39.270 --> 00:11:42.426
But you're being intelligent;
you're understanding what I'm saying.

00:11:42.450 --> 00:11:44.745
Because you're intelligent
and you speak English,

00:11:44.769 --> 00:11:46.520
you know the word at the end of this

00:11:46.544 --> 00:11:47.703
sentence.

00:11:47.727 --> 00:11:50.879
The word came to you;
you make these predictions all the time.

00:11:50.903 --> 00:11:52.602
What I'm saying is,

00:11:52.626 --> 00:11:55.257
the internal prediction
is the output in the neocortex,

00:11:55.281 --> 00:11:57.822
and somehow, prediction
leads to intelligent behavior.

00:11:57.846 --> 00:11:58.997
Here's how that happens:

00:11:59.021 --> 00:12:00.976
Let's start with a non-intelligent brain.

00:12:01.000 --> 00:12:04.009
I'll argue a non-intelligent brain,
we'll call it an old brain.

00:12:04.033 --> 00:12:06.904
And we'll say it's
a non-mammal, like a reptile,

00:12:06.928 --> 00:12:08.913
say, an alligator; we have an alligator.

00:12:08.937 --> 00:12:12.308
And the alligator has
some very sophisticated senses.

00:12:12.332 --> 00:12:15.538
It's got good eyes and ears
and touch senses and so on,

00:12:15.562 --> 00:12:17.031
a mouth and a nose.

00:12:17.055 --> 00:12:19.046
It has very complex behavior.

00:12:19.070 --> 00:12:22.976
It can run and hide. It has fears
and emotions. It can eat you.

00:12:23.000 --> 00:12:26.590
It can attack.
It can do all kinds of stuff.

00:12:27.193 --> 00:12:30.049
But we don't consider
the alligator very intelligent,

00:12:30.073 --> 00:12:31.749
not in a human sort of way.

00:12:31.773 --> 00:12:34.129
But it has all this complex
behavior already.

00:12:34.510 --> 00:12:36.311
Now in evolution, what happened?

00:12:36.335 --> 00:12:38.720
First thing that happened
in evolution with mammals

00:12:38.744 --> 00:12:41.275
is we started to develop a thing
called the neocortex.

00:12:41.299 --> 00:12:45.092
I'm going to represent the neocortex
by this box on top of the old brain.

00:12:45.116 --> 00:12:48.469
Neocortex means "new layer."
It's a new layer on top of your brain.

00:12:48.493 --> 00:12:50.836
It's the wrinkly thing
on the top of your head

00:12:50.860 --> 00:12:53.944
that got wrinkly because it got shoved
in there and doesn't fit.

00:12:53.968 --> 00:12:54.976
(Laughter)

00:12:55.000 --> 00:12:57.242
Literally, it's about the size
of a table napkin

00:12:57.266 --> 00:12:58.840
and doesn't fit, so it's wrinkly.

00:12:58.864 --> 00:13:00.609
Now, look at how I've drawn this.

00:13:00.633 --> 00:13:02.019
The old brain is still there.

00:13:02.043 --> 00:13:05.698
You still have that alligator brain.
You do. It's your emotional brain.

00:13:05.722 --> 00:13:08.452
It's all those gut reactions you have.

00:13:08.476 --> 00:13:11.746
On top of it, we have this memory system
called the neocortex.

00:13:11.770 --> 00:13:16.064
And the memory system is sitting
over the sensory part of the brain.

00:13:16.088 --> 00:13:19.143
So as the sensory input
comes in and feeds from the old brain,

00:13:19.167 --> 00:13:21.321
it also goes up into the neocortex.

00:13:21.345 --> 00:13:23.258
And the neocortex is just memorizing.

00:13:23.282 --> 00:13:26.843
It's sitting there saying, I'm going
to memorize all the things going on:

00:13:26.867 --> 00:13:29.886
where I've been, people I've seen,
things I've heard, and so on.

00:13:29.910 --> 00:13:33.272
And in the future, when it sees
something similar to that again,

00:13:33.296 --> 00:13:35.931
in a similar environment,
or the exact same environment,

00:13:35.955 --> 00:13:39.510
it'll start playing it back:
"Oh, I've been here before,"

00:13:39.534 --> 00:13:41.898
and when you were here before,
this happened next.

00:13:41.922 --> 00:13:43.648
It allows you to predict the future.

00:13:43.672 --> 00:13:47.068
It literally feeds back
the signals into your brain;

00:13:47.092 --> 00:13:49.357
they'll let you see
what's going to happen next,

00:13:49.381 --> 00:13:51.976
will let you hear the word
"sentence" before I said it.

00:13:52.000 --> 00:13:55.185
And it's this feeding
back into the old brain

00:13:55.209 --> 00:13:57.786
that will allow you to make
more intelligent decisions.

00:13:57.810 --> 00:14:01.299
This is the most important slide
of my talk, so I'll dwell on it a little.

00:14:01.323 --> 00:14:04.898
And all the time you say,
"Oh, I can predict things,"

00:14:04.922 --> 00:14:08.282
so if you're a rat and you go
through a maze, and you learn the maze,

00:14:08.306 --> 00:14:10.745
next time you're in one,
you have the same behavior.

00:14:10.769 --> 00:14:13.760
But suddenly, you're smarter;
you say, "I recognize this maze,

00:14:13.784 --> 00:14:17.326
I know which way to go; I've been here
before; I can envision the future."

00:14:17.350 --> 00:14:18.518
That's what it's doing.

00:14:18.542 --> 00:14:21.382
This is true for all mammals --

00:14:21.406 --> 00:14:23.437
in humans, it got a lot worse.

00:14:23.461 --> 00:14:26.048
Humans actually developed
the front of the neocortex,

00:14:26.072 --> 00:14:28.293
called the anterior part of the neocortex.

00:14:28.317 --> 00:14:29.755
And nature did a little trick.

00:14:29.779 --> 00:14:32.466
It copied the posterior,
the back part, which is sensory,

00:14:32.490 --> 00:14:33.641
and put it in the front.

00:14:33.665 --> 00:14:36.145
Humans uniquely have
the same mechanism on the front,

00:14:36.169 --> 00:14:37.723
but we use it for motor control.

00:14:37.747 --> 00:14:41.328
So we're now able to do very sophisticated
motor planning, things like that.

00:14:41.352 --> 00:14:44.478
I don't have time to explain,
but to understand how a brain works,

00:14:44.502 --> 00:14:48.039
you have to understand how the first part
of the mammalian neocortex works,

00:14:48.063 --> 00:14:50.356
how it is we store patterns
and make predictions.

00:14:50.380 --> 00:14:52.568
Let me give you
a few examples of predictions.

00:14:52.592 --> 00:14:54.268
I already said the word "sentence."

00:14:54.292 --> 00:14:57.498
In music, if you've heard a song before,

00:14:57.522 --> 00:15:00.431
when you hear it, the next note
pops into your head already --

00:15:00.455 --> 00:15:01.606
you anticipate it.

00:15:01.630 --> 00:15:04.984
With an album, at the end of a song,
the next song pops into your head.

00:15:05.008 --> 00:15:07.313
It happens all the time,
you make predictions.

00:15:07.337 --> 00:15:10.376
I have this thing called
the "altered door" thought experiment.

00:15:10.400 --> 00:15:13.229
It says, you have a door at home;

00:15:13.253 --> 00:15:15.008
when you're here, I'm changing it --

00:15:15.032 --> 00:15:18.228
I've got a guy back at your house
right now, moving the door around,

00:15:18.252 --> 00:15:20.021
moving your doorknob over two inches.

00:15:20.045 --> 00:15:23.629
When you go home tonight, you'll put
your hand out, reach for the doorknob,

00:15:23.653 --> 00:15:25.167
notice it's in the wrong spot

00:15:25.191 --> 00:15:26.878
and go, "Whoa, something happened."

00:15:26.902 --> 00:15:29.003
It may take a second,
but something happened.

00:15:29.027 --> 00:15:31.030
I can change your doorknob
in other ways --

00:15:31.054 --> 00:15:34.295
make it larger, smaller, change
its brass to silver, make it a lever,

00:15:34.319 --> 00:15:36.895
I can change the door;
put colors on, put windows in.

00:15:36.919 --> 00:15:39.070
I can change a thousand things
about your door

00:15:39.094 --> 00:15:41.102
and in the two seconds
you take to open it,

00:15:41.126 --> 00:15:42.848
you'll notice something has changed.

00:15:42.872 --> 00:15:45.456
Now, the engineering approach,
the AI approach to this,

00:15:45.480 --> 00:15:48.155
is to build a door database
with all the door attributes.

00:15:48.179 --> 00:15:50.998
And as you go up to the door,
we check them off one at time:

00:15:51.022 --> 00:15:52.368
door, door, color ...

00:15:52.392 --> 00:15:54.492
We don't do that.
Your brain doesn't do that.

00:15:54.516 --> 00:15:57.056
Your brain is making
constant predictions all the time

00:15:57.080 --> 00:15:59.114
about what will happen
in your environment.

00:15:59.138 --> 00:16:01.884
As I put my hand on this table,
I expect to feel it stop.

00:16:01.908 --> 00:16:04.927
When I walk, every step,
if I missed it by an eighth of an inch,

00:16:04.951 --> 00:16:06.484
I'll know something has changed.

00:16:06.508 --> 00:16:09.328
You're constantly making predictions
about your environment.

00:16:09.352 --> 00:16:10.945
I'll talk about vision, briefly.

00:16:10.969 --> 00:16:12.352
This is a picture of a woman.

00:16:12.376 --> 00:16:15.866
When we look at people, our eyes saccade
over two to three times a second.

00:16:15.890 --> 00:16:18.419
We're not aware of it,
but our eyes are always moving.

00:16:18.443 --> 00:16:21.878
When we look at a face, we typically
go from eye to eye to nose to mouth.

00:16:21.902 --> 00:16:23.771
When your eye moves from eye to eye,

00:16:23.795 --> 00:16:25.953
if there was something
else there like a nose,

00:16:25.977 --> 00:16:29.523
you'd see a nose where an eye
is supposed to be and go, "Oh, shit!"

00:16:29.547 --> 00:16:30.943
(Laughter)

00:16:30.967 --> 00:16:33.076
"There's something wrong
about this person."

00:16:33.100 --> 00:16:35.105
That's because you're making a prediction.

00:16:35.129 --> 00:16:38.568
It's not like you just look over and say,
"What am I seeing? A nose? OK."

00:16:38.592 --> 00:16:41.226
No, you have an expectation
of what you're going to see.

00:16:41.250 --> 00:16:42.401
Every single moment.

00:16:42.425 --> 00:16:45.054
And finally, let's think
about how we test intelligence.

00:16:45.078 --> 00:16:48.159
We test it by prediction:
What is the next word in this ...?

00:16:48.183 --> 00:16:51.810
This is to this as this is to this.
What is the next number in this sentence?

00:16:51.834 --> 00:16:54.524
Here's three visions of an object.
What's the fourth one?

00:16:54.548 --> 00:16:57.052
That's how we test it.
It's all about prediction.

00:16:57.573 --> 00:16:59.767
So what is the recipe for brain theory?

00:17:00.219 --> 00:17:02.585
First of all, we have to have
the right framework.

00:17:02.609 --> 00:17:04.522
And the framework is a memory framework,

00:17:04.546 --> 00:17:06.570
not a computational or behavior framework,

00:17:06.594 --> 00:17:07.757
it's a memory framework.

00:17:07.781 --> 00:17:10.404
How do you store and recall
these sequences of patterns?

00:17:10.428 --> 00:17:11.870
It's spatiotemporal patterns.

00:17:11.894 --> 00:17:14.903
Then, if in that framework,
you take a bunch of theoreticians --

00:17:14.927 --> 00:17:17.173
biologists generally
are not good theoreticians.

00:17:17.197 --> 00:17:20.726
Not always, but generally, there's not
a good history of theory in biology.

00:17:20.750 --> 00:17:23.324
I've found the best people
to work with are physicists,

00:17:23.348 --> 00:17:24.731
engineers and mathematicians,

00:17:24.755 --> 00:17:26.451
who tend to think algorithmically.

00:17:26.475 --> 00:17:29.739
Then they have to learn
the anatomy and the physiology.

00:17:29.763 --> 00:17:34.259
You have to make these theories
very realistic in anatomical terms.

00:17:34.283 --> 00:17:37.048
Anyone who tells you their theory
about how the brain works

00:17:37.072 --> 00:17:39.169
and doesn't tell you exactly
how it's working

00:17:39.193 --> 00:17:40.496
and how the wiring works --

00:17:40.520 --> 00:17:41.787
it's not a theory.

00:17:41.811 --> 00:17:44.644
And that's what we do
at the Redwood Neuroscience Institute.

00:17:44.668 --> 00:17:47.976
I'd love to tell you we're making
fantastic progress in this thing,

00:17:48.000 --> 00:17:51.662
and I expect to be back on this stage
sometime in the not too distant future,

00:17:51.686 --> 00:17:52.850
to tell you about it.

00:17:52.874 --> 00:17:55.468
I'm really excited;
this is not going to take 50 years.

00:17:55.492 --> 00:17:57.070
What will brain theory look like?

00:17:57.094 --> 00:17:59.149
First of all, it's going
to be about memory.

00:17:59.173 --> 00:18:01.995
Not like computer memory --
not at all like computer memory.

00:18:02.019 --> 00:18:03.170
It's very different.

00:18:03.194 --> 00:18:05.451
It's a memory of very
high-dimensional patterns,

00:18:05.475 --> 00:18:07.437
like the things that come from your eyes.

00:18:07.461 --> 00:18:08.898
It's also memory of sequences:

00:18:08.922 --> 00:18:11.652
you cannot learn or recall anything
outside of a sequence.

00:18:11.676 --> 00:18:14.513
A song must be heard
in sequence over time,

00:18:14.537 --> 00:18:16.888
and you must play it back
in sequence over time.

00:18:16.912 --> 00:18:19.361
And these sequences
are auto-associatively recalled,

00:18:19.385 --> 00:18:22.258
so if I see something, I hear something,
it reminds me of it,

00:18:22.282 --> 00:18:23.815
and it plays back automatically.

00:18:23.839 --> 00:18:25.133
It's an automatic playback.

00:18:25.157 --> 00:18:27.705
And prediction of future inputs
is the desired output.

00:18:27.729 --> 00:18:30.349
And as I said, the theory
must be biologically accurate,

00:18:30.373 --> 00:18:32.857
it must be testable
and you must be able to build it.

00:18:32.881 --> 00:18:35.092
If you don't build it,
you don't understand it.

00:18:35.116 --> 00:18:36.648
One more slide.

00:18:36.672 --> 00:18:38.981
What is this going to result in?

00:18:39.005 --> 00:18:41.353
Are we going to really build
intelligent machines?

00:18:41.377 --> 00:18:45.175
Absolutely. And it's going to be
different than people think.

00:18:45.508 --> 00:18:47.900
No doubt that it's going
to happen, in my mind.

00:18:47.924 --> 00:18:51.040
First of all, we're going to build
this stuff out of silicon.

00:18:51.064 --> 00:18:53.976
The same techniques we use to build
silicon computer memories,

00:18:54.000 --> 00:18:55.151
we can use here.

00:18:55.175 --> 00:18:57.284
But they're very different
types of memories.

00:18:57.308 --> 00:18:59.331
And we'll attach
these memories to sensors,

00:18:59.355 --> 00:19:02.132
and the sensors will experience
real-live, real-world data,

00:19:02.156 --> 00:19:03.908
and learn about their environment.

00:19:03.932 --> 00:19:07.377
Now, it's very unlikely the first things
you'll see are like robots.

00:19:07.401 --> 00:19:09.976
Not that robots aren't useful;
people can build robots.

00:19:10.000 --> 00:19:13.767
But the robotics part is the hardest part.
That's old brain. That's really hard.

00:19:13.791 --> 00:19:15.798
The new brain is easier
than the old brain.

00:19:15.822 --> 00:19:18.904
So first we'll do things
that don't require a lot of robotics.

00:19:18.928 --> 00:19:21.107
So you're not going to see C-3PO.

00:19:21.131 --> 00:19:23.616
You're going to see things
more like intelligent cars

00:19:23.640 --> 00:19:26.448
that really understand
what traffic is, what driving is

00:19:26.472 --> 00:19:29.750
and have learned that cars
with the blinkers on for half a minute

00:19:29.774 --> 00:19:31.348
probably aren't going to turn.

00:19:31.372 --> 00:19:32.663
(Laughter)

00:19:32.687 --> 00:19:34.751
We can also do intelligent
security systems.

00:19:34.775 --> 00:19:38.348
Anytime we're basically using our brain
but not doing a lot of mechanics --

00:19:38.372 --> 00:19:40.431
those are the things
that will happen first.

00:19:40.455 --> 00:19:42.275
But ultimately, the world's the limit.

00:19:42.299 --> 00:19:44.031
I don't know how this will turn out.

00:19:44.055 --> 00:19:46.646
I know a lot of people who invented
the microprocessor.

00:19:46.670 --> 00:19:48.834
And if you talk to them,

00:19:48.858 --> 00:19:51.433
they knew what they were doing
was really significant,

00:19:51.457 --> 00:19:53.957
but they didn't really know
what was going to happen.

00:19:53.981 --> 00:19:56.749
They couldn't anticipate
cell phones and the Internet

00:19:56.773 --> 00:19:58.508
and all this kind of stuff.

00:19:58.532 --> 00:20:01.153
They just knew like,
"We're going to build calculators

00:20:01.177 --> 00:20:02.617
and traffic-light controllers.

00:20:02.641 --> 00:20:03.940
But it's going to be big!"

00:20:03.964 --> 00:20:06.305
In the same way, brain science
and these memories

00:20:06.329 --> 00:20:08.554
are going to be a very
fundamental technology,

00:20:08.578 --> 00:20:12.020
and it will lead to unbelievable changes
in the next 100 years.

00:20:12.044 --> 00:20:15.449
And I'm most excited about
how we're going to use them in science.

00:20:15.473 --> 00:20:18.310
So I think that's all my time -- I'm over,

00:20:18.334 --> 00:20:20.611
and I'm going to end my talk right there.

