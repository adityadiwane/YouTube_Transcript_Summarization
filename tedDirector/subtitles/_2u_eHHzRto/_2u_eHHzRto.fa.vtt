WEBVTT
Kind: captions
Language: fa

00:00:00.000 --> 00:00:07.000
Translator: S. Morteza Hashemi
Reviewer: sadegh zabihi

00:00:12.795 --> 00:00:14.391
الگوریتم‌ها همه جا هستند.

00:00:15.931 --> 00:00:19.016
آن‌ها برنده‌ها و بازنده‌ها را
رده‌بندی کرده و از هم جدا می‌کنند

00:00:19.856 --> 00:00:22.103
برنده‌ها استخدام می‌شوند

00:00:22.127 --> 00:00:23.870
یا یک پیشنهاد خوب مالی می‌گیرند.

00:00:23.894 --> 00:00:26.545
اما بازنده‌ها حتی مصاحبه هم نمی‌شوند

00:00:27.410 --> 00:00:29.187
یا هزینه بیشتری برای بیمه می‌پردازند.

00:00:30.017 --> 00:00:33.566
ما با فرمول‌های مرموزی سنجیده می‌شویم
که درکشان نمی‌کنیم.

00:00:34.495 --> 00:00:37.712
که معمولاً سیستمی برای تجدید نظر ندارند.

00:00:39.060 --> 00:00:40.356
این سوالی را بر می‌انگیزد:

00:00:40.380 --> 00:00:43.293
اگر الگوریتم‌ها اشتباه کنند چطور؟

00:00:44.920 --> 00:00:46.960
برای ایجاد یک الگوریتم دو چیز نیاز دارید:

00:00:46.984 --> 00:00:48.965
داده، چیزی که در گذشته اتفاق افتاده،

00:00:48.989 --> 00:00:50.550
و تعریفی برای موفقیت،

00:00:50.574 --> 00:00:53.101
چیزی که به دنبال آن هستید
و معمولاً انتظارش را دارید.

00:00:53.101 --> 00:00:58.092
شما به الگوریتم با دیدن
و درک کردن آموزش می‌دهید.

00:00:58.116 --> 00:01:01.535
الگوریتم درک می‌کند که چه چیزهایی
با موفقیت ارتباط دارند.

00:01:01.559 --> 00:01:04.022
چه موقعیتی به موفقیت منتهی می‌شود؟

00:01:04.701 --> 00:01:06.843
در واقع، همه
از الگوریتم‌ها استفاده می‌کنند.

00:01:06.843 --> 00:01:09.205
فقط آن‌ها را به صورت کد نمی‌نویسند

00:01:09.229 --> 00:01:10.577
بگذارید مثالی بزنم.

00:01:10.601 --> 00:01:13.917
من هر روز از یک الگوریتم
برای پخت غذای خانواده‌ام استفاده می‌کنم.

00:01:13.941 --> 00:01:15.417
داده‌هایی که استفاده می‌کنم

00:01:16.214 --> 00:01:17.873
شامل مواد موجود در آشپزخانه‌ام،

00:01:17.897 --> 00:01:19.424
زمانی که دارم،

00:01:19.448 --> 00:01:20.681
و هدفی که دارم می‌شود.

00:01:20.705 --> 00:01:22.414
و من این داده‌ها را مدیریت می‌کنم.

00:01:22.438 --> 00:01:26.689
من آن بسته‌های کوچک
نودل رامن را غذا به حساب نمی‌آورم.

00:01:26.713 --> 00:01:28.582
(خنده حضار)

00:01:28.606 --> 00:01:30.451
تعریف من از موفقیت این است:

00:01:30.475 --> 00:01:33.134
یک غذا موفق است
اگر فرزندانم سبزیجات بخورند.

00:01:34.001 --> 00:01:36.855
این خیلی متفاوت می‌شد
اگر پسر بزرگ‌ترم مسئول بود.

00:01:36.879 --> 00:01:39.667
از نظر او موفقیت این است
که بتواند زیاد نوتلا بخورد.

00:01:40.999 --> 00:01:43.225
ولی منم که موفقیت را انتخاب می‌کنم.

00:01:43.249 --> 00:01:45.956
من مسئولم.
نظر من است که اهمیت دارد.

00:01:45.980 --> 00:01:48.655
این اولین قانون الگوریتم‌هاست.

00:01:48.679 --> 00:01:51.859
الگوریتم‌ها سلایقی هستند
که در قالب کد ارائه می‌شوند.

00:01:53.382 --> 00:01:57.045
این با چیزی که بیشتر مردم در مورد
الگوریتم‌ها فکر می‌کنند تفاوت دارد.

00:01:57.069 --> 00:02:01.573
آن‌ها فکر می‌کنند که الگوریتم‌ها
عینی، درست و علمی هستند.

00:02:02.207 --> 00:02:03.906
این یک حقهٔ بازاریابی است.

00:02:05.089 --> 00:02:07.214
این هم یک حقهٔ بازاریابی است

00:02:07.238 --> 00:02:10.392
که شما را با الگوریتم‌ها بترسانند،

00:02:10.416 --> 00:02:14.077
که شما را وادار به اعتماد به الگوریتم‌ها
و ترس از آن‌ها کنند

00:02:14.101 --> 00:02:16.119
به دلیل ترس و اعتماد شما به ریاضیات.

00:02:17.387 --> 00:02:22.217
باور کورکورانه به کلان‌داده‌ها
می‌تواند عواقب نادرستی داشته باشد.

00:02:23.504 --> 00:02:26.877
این کیری سورس است.
او مدیر یک دبیرستان در بروکلین است.

00:02:26.901 --> 00:02:29.487
در سال ۲۰۱۱، او به من گفت
معلم‌های مدرسه‌اش

00:02:29.511 --> 00:02:32.238
با یک الگوریتم مرموز
و پیچیده امتیازبندی می‌شوند

00:02:32.262 --> 00:02:33.751
که نام آن
«مدل ارزش افزوده» است.

00:02:34.325 --> 00:02:37.417
من به او گفتم، «خب، فرمول را پیدا کن
و به من نشان بده

00:02:37.441 --> 00:02:38.982
من آن را برایت توضیح می‌دهم».

00:02:39.006 --> 00:02:41.147
او گفت: «من سعی کردم
که فرمول را بدست بیاورم،

00:02:41.171 --> 00:02:43.943
اما رابط اداره‌ی آموزش
به من گفت که ریاضیات است

00:02:43.967 --> 00:02:45.893
و من آن را درک نخواهم کرد.»

00:02:47.086 --> 00:02:48.424
بدتر هم می‌شود.

00:02:48.448 --> 00:02:51.978
روزنامه «نیویورک پست» درخواستی
مربوط به «سند آزادی اطلاعات» منتشر کرد،

00:02:52.002 --> 00:02:54.961
نام تمام معلم‌ها
و تمام نمراتشان را به دست آورد

00:02:54.985 --> 00:02:57.857
و برای خجالت‌زده کردن معلمان
آنها را منتشر کرد.

00:02:58.904 --> 00:03:02.764
وقتی من خواستم فرمول‌ها 
و کد را از همین طریق ببینم،

00:03:02.788 --> 00:03:04.937
به من گفتند نمی‌توانم.

00:03:04.961 --> 00:03:06.197
از این کار منع شدم.

00:03:06.221 --> 00:03:07.395
بعداً فهمیدم

00:03:07.419 --> 00:03:10.285
که هیچ‌کس در شهر نیویورک
به آن فرمول دسترسی نداشت.

00:03:10.309 --> 00:03:11.614
هیچ‌کس آن را نمی‌فهمید.

00:03:13.749 --> 00:03:16.973
بعداً یک فرد واقعاً باهوش
به نام «گری روبنشتاین» درگیر این موضوع شد.

00:03:16.997 --> 00:03:20.618
او ۶۶۵ معلم که در داده‌های
نیویورک پست بودند را یافت

00:03:20.642 --> 00:03:22.508
که هر یک، دو نمره داشتند.

00:03:22.532 --> 00:03:24.413
اگر یک معلم در دو کلاس هفتم و هشتم

00:03:24.437 --> 00:03:26.876
تدریس کند ممکن است این اتفاق رخ دهد.

00:03:26.900 --> 00:03:28.808
او تصمیم گرفت این داده‌ها را ترسیم کند.

00:03:28.808 --> 00:03:30.455
هر نقطه نشان‌دهنده‌ی یک معلم است.

00:03:30.924 --> 00:03:33.303
(خنده‌ی حضار)

00:03:33.327 --> 00:03:34.848
این چیست؟

00:03:34.872 --> 00:03:36.149
(خنده‌ی حضار)

00:03:36.173 --> 00:03:39.619
هرگز نباید برای ارزیابی افراد
مورد استفاده قرار می‌گرفت.

00:03:39.643 --> 00:03:41.739
تقریباً یک تولیدکننده‌ی اعداد تصادفی است.

00:03:41.739 --> 00:03:44.539
(تشوق حضار)

00:03:44.563 --> 00:03:45.725
اما استفاده شد.

00:03:45.749 --> 00:03:46.925
این سارا ویساکی است.

00:03:46.949 --> 00:03:49.124
او به همراه ۲۰۵ معلم دیگر

00:03:49.148 --> 00:03:51.810
از ناحیه‌ی مدارس واشنگتون دی‌سی اخراج شد.

00:03:51.834 --> 00:03:54.743
علی‌رغم اینکه توصیه‌نامه‌های 
خیلی خوبی از طرف مدیر

00:03:54.767 --> 00:03:56.195
و خانواده‌های شاگردانش داشت.

00:03:57.210 --> 00:03:59.242
می‌دانم بسیاری از شما
چه فکر می‌کنید،

00:03:59.266 --> 00:04:01.753
خصوصاً دانشمندان داده،
و خبرگان هوش مصنوعی.

00:04:01.777 --> 00:04:06.003
شما فکر می‌کنید، «من هرگز الگوریتمی
به این ناسازگاری نخواهم ساخت».

00:04:06.673 --> 00:04:08.866
اما الگوریتم‌ها می‌توانند اشتباه کنند،

00:04:08.866 --> 00:04:12.978
حتی آثار مخرب عمیقی داشته باشند،
در صورتی که نیت‌شان خیر بوده است.

00:04:14.351 --> 00:04:16.730
برخلاف هواپیمایی
که بد طراحی شده است

00:04:16.754 --> 00:04:18.755
و سقوط می‌کند و همه آن را می‌بینند،

00:04:18.779 --> 00:04:20.629
الگوریتمی که بد طراحی شده باشد

00:04:22.065 --> 00:04:25.930
ممکن است مدت زیادی
به صورت خاموش تخریب کند.

00:04:27.568 --> 00:04:29.138
این راجر ایلز است.

00:04:29.162 --> 00:04:31.162
(خنده‌ی حضار)

00:04:32.344 --> 00:04:34.732
او در سال ۱۹۹۶ فاکس نیوز را تأسیس کرد.

00:04:35.256 --> 00:04:37.837
بیش از ۲۰ زن از آزار جنسی
شکایت کردند.

00:04:37.861 --> 00:04:41.096
آن‌ها می‌گفتند که اجازه موفقیت
در فاکس نیوز را ندارند.

00:04:41.120 --> 00:04:43.640
او سال گذشته برکنار شد،
اما اخیراً دیده شده

00:04:43.664 --> 00:04:46.334
که مشکلات باقی مانده‌اند.

00:04:47.474 --> 00:04:49.194
این مسئله این سؤال را برمی‌انگیزد:

00:04:49.194 --> 00:04:51.782
فاکس نیوز باید چه کند
تا بهبود یابد؟

00:04:53.065 --> 00:04:56.106
خب، چطور است فرایند استخدم را

00:04:56.130 --> 00:04:58.134
با یک الگوریتم یادگیری ماشین جایگزین کنیم؟

00:04:58.134 --> 00:04:59.633
به نظر خوب می‌آید، نه؟

00:04:59.633 --> 00:05:00.727
به آن فکر کنید.

00:05:00.751 --> 00:05:02.856
داده، داده‌ها چه خواهند بود؟

00:05:02.880 --> 00:05:07.827
یک انتخاب منطقی سابقه‌ درخواست‌های استخدام
در ۲۱ سال گذشته‌ی فاکس نیوز است.

00:05:07.851 --> 00:05:09.353
منطقی است.

00:05:09.377 --> 00:05:11.315
تعریف موفقیت چطور؟

00:05:11.741 --> 00:05:13.065
یک انتخاب منطقی این است که،

00:05:13.089 --> 00:05:14.867
چه کسی در فاکس نیوز موفق است؟

00:05:14.891 --> 00:05:18.471
به نظرم کسی که مثلاً،
چهار سال در آنجا مانده باشد

00:05:18.495 --> 00:05:20.149
و حداقل یک بار ارتقاء گرفته باشد.

00:05:20.636 --> 00:05:22.247
به نظرم منطقی است.

00:05:22.247 --> 00:05:24.575
سپس الگوریتم را آموزش می‌دهیم.

00:05:24.599 --> 00:05:28.476
الگوریتم آموزش داده می‌شود که بگردد
و بفهمد چه چیزی منجر به موفقیت شده است.

00:05:29.039 --> 00:05:31.251
بر اساس این تعریف

00:05:31.251 --> 00:05:34.675
که چه جور درخواست‌هایی در گذشته
منجر به موفقیت شده‌اند؟

00:05:36.020 --> 00:05:38.145
حالا به این فکر کنید که اگر الگوریتم را

00:05:38.145 --> 00:05:40.374
روی درخواست‌های فعلی اجرا کنیم
چه خواهد شد؟

00:05:40.939 --> 00:05:42.568
زن‌ها حذف می‌شوند

00:05:43.483 --> 00:05:47.413
چون شبیه افرادی که در گذشته 
موفق بوده‌اند به نظر نمی‌رسند.

00:05:51.572 --> 00:05:54.109
الگوریتم‌ها چیزی را عادلانه نمی‌کنند

00:05:54.133 --> 00:05:56.827
اگر آن‌ها را خوش‌بینانه
و کورکورانه به کار ببرید.

00:05:56.851 --> 00:05:58.333
چیزی را عادلانه نمی‌کنند.

00:05:58.357 --> 00:06:00.485
آن‌ها تجربیات و الگوهای گذشته‌ی ما را

00:06:00.509 --> 00:06:01.692
تکرار می‌کنند.

00:06:01.716 --> 00:06:03.655
وضعیت موجود را خودکارسازی می‌کنند.

00:06:04.538 --> 00:06:06.927
اگر دنیای ما بی‌نقص بود،
این عالی بود،

00:06:07.725 --> 00:06:09.037
اما این‌طور نیست.

00:06:09.061 --> 00:06:13.163
و اضافه می‌کنم که اکثر شرکت‌ها
دادخواست‌های شرم‌آوری ندارند،

00:06:14.266 --> 00:06:16.854
اما به دانشمندان داده در این شرکت‌ها

00:06:16.878 --> 00:06:19.067
گفته می‌شود که داده‌ها را دنبال کنند،

00:06:19.091 --> 00:06:21.234
و روی دقت تمرکز کنند.

00:06:22.093 --> 00:06:24.094
به این فکر کنید که این به چه معنی است.

00:06:24.094 --> 00:06:27.525
چون ما همه تعصباتی داریم،
یعنی ممکن است تبعیض جنسی

00:06:27.549 --> 00:06:29.775
یا هر نوع تعصب دیگر را به کد تبدیل کنیم.

00:06:31.308 --> 00:06:32.729
یک آزمایش فکری،

00:06:32.753 --> 00:06:34.262
چون آن را دوست دارم:

00:06:35.394 --> 00:06:38.369
یک جامعه کاملاً تفکیک‌شده --

00:06:40.067 --> 00:06:43.395
تفکیک‌شده‌ی نژادی، در تمام شهرها،
تمام محله‌ها

00:06:43.419 --> 00:06:45.800
و پلیس‌ها را برای تشخیص جرم

00:06:45.800 --> 00:06:47.673
فقط به محله‌ی اقلیت‌ها می‌فرستیم.

00:06:48.271 --> 00:06:50.490
داده‌های دستگیری‌ها 
خیلی تبعیض‌آمیز خواهد بود.

00:06:51.671 --> 00:06:54.446
چه خوا هد شد اگر علاوه بر این، 
تعدادی دانشمند داده‌ بیابیم

00:06:54.446 --> 00:06:58.431
و به آن‌ها پول بدهیم تا محل وقوع
جرایم بعدی را پیش‌بینی کنند؟

00:06:59.095 --> 00:07:00.582
محله‌ی اقلیت‌ها.

00:07:01.105 --> 00:07:04.230
و یا پیش‌بینی کنند مجرمِ بعدی
که خواهد بود؟

00:07:04.708 --> 00:07:06.103
یک [فردِ] اقلیت.

00:07:07.769 --> 00:07:11.310
دانشمندان داده به عظمت
و دقتِ مدلِشان

00:07:11.334 --> 00:07:12.631
افتخار خواهند کرد،

00:07:12.655 --> 00:07:13.954
و حق دارند.

00:07:15.771 --> 00:07:20.386
آیا این جدی نیست؟
اما ما این تفکیک‌های شدید را

00:07:20.410 --> 00:07:22.177
در بسیاری شهرهای بزرگ و کوچک داریم،

00:07:22.177 --> 00:07:23.614
و شواهدی زیادی از تعصبات پلیسی

00:07:23.638 --> 00:07:26.326
و داده‌های سیستم قضایی،
در دست داریم.

00:07:27.452 --> 00:07:30.267
و در واقع نقاط کانونی را پیش‌بینی می‌کنیم،

00:07:30.291 --> 00:07:32.121
مکان‌هایی که جرم در آن رخ خواهد داد.

00:07:32.221 --> 00:07:36.087
در حقیقت، جنایتکاری فردی
را پیش‌بینی می‌کنیم.

00:07:36.111 --> 00:07:37.881
میزان جنایتکاری افراد را.

00:07:38.792 --> 00:07:42.755
سازمان خبری پروپابلیکا
به یکی از الگوریتم‌های

00:07:42.779 --> 00:07:44.803
به ظاهر [تشخیص‌دهنده‌ی] «ریسک تکرار جرم»

00:07:44.827 --> 00:07:45.990
نگاهی انداخته است.

00:07:46.014 --> 00:07:49.208
که در فلوریدا حین صدور رأی
قضات استفاده می‌شود.

00:07:50.231 --> 00:07:53.816
برنابرد، در سمت چپ، مرد سیاه‌پوست،
امتیاز ۱۰ از ۱۰ گرفته بود.

00:07:54.999 --> 00:07:57.006
دیلان، در سمت راست، ۳ از ۱۰.

00:07:57.030 --> 00:07:59.531
۱۰ از ۱۰، ریسک زیاد.
۳ از ۱۰، ریسک کم.

00:08:00.418 --> 00:08:02.993
هر دوی آن‌ها به خاطر حمل 
مواد مخدر دستگیر شده بودند.

00:08:02.993 --> 00:08:04.531
هر دوی آن‌ها سابقه‌دار بودند،

00:08:04.531 --> 00:08:06.811
اما دیلان سابقه‌ی تبه‌کاری داشت

00:08:06.835 --> 00:08:08.341
ولی برنارد نداشت.

00:08:09.638 --> 00:08:12.704
این مسئله به این خاطر اهمیت دارد
که هر چه امتیاز شما بالاتر باشد،

00:08:12.728 --> 00:08:16.201
احتمال این‌که محکومیت طولانی‌تری
بگیرید افزایش می‌یابد.

00:08:18.114 --> 00:08:19.408
قضیه چیست؟

00:08:20.346 --> 00:08:21.678
داده‌شویی.

00:08:22.750 --> 00:08:27.177
فرایندی که طی آن فناوری‌گرایان
حقایق زشت را در جعبه‌های سیاه

00:08:27.201 --> 00:08:29.022
پنهان می‌کنند.

00:08:29.046 --> 00:08:30.336
و آن را «عینی» می‌خوانند؛

00:08:31.140 --> 00:08:33.248
آن‌را «شایسته‌سالاری» خطاب می‌کنند.

00:08:34.938 --> 00:08:37.623
در حالی که این الگوریتم‌ها مخفی،
مهم و ویران‌گر هستند،

00:08:37.623 --> 00:08:39.834
من برای آن‌ها نامی در نظر گرفته‌ام:

00:08:39.858 --> 00:08:41.857
«سلاح کشتار ریاضی.»

00:08:41.881 --> 00:08:43.445
(خنده حضار)

00:08:43.469 --> 00:08:46.523
(تشویق حضار)

00:08:46.547 --> 00:08:48.901
این‌ها همه جا هستند،
و این یک اشتباه نیست.

00:08:49.515 --> 00:08:53.238
این شرکت‌های خصوصی
الگوریتم‌هایی خصوصی

00:08:53.262 --> 00:08:54.654
برای اهداف شخصی می‌سازند.

00:08:55.034 --> 00:08:58.248
حتی همان‌هایی که درباره‌شان صحبت کردم
برای معلمان و پلیس عمومی،

00:08:58.272 --> 00:09:00.361
آن‌ها هم توسط شرکت‌های خصوصی
ساخته شده بودند

00:09:00.361 --> 00:09:02.396
و به مؤسسات دولتی فروخته شده بودند.

00:09:02.420 --> 00:09:04.293
به آن «سس مخصوص» خودشان می‌گویند

00:09:04.317 --> 00:09:06.735
برای همین نمی‌توانند درباره‌ی آن
به ما توضیح دهند.

00:09:06.735 --> 00:09:08.689
قدرت خصوصی هم هست.

00:09:09.744 --> 00:09:14.439
آن‌ها به خاطر داشتن 
حق محرمانگی سود می‌برند.

00:09:16.934 --> 00:09:19.868
ممکن است فکر کنید،
چون این چیزها خصوصی هستند

00:09:19.892 --> 00:09:21.050
و رقابت وجود دارد،

00:09:21.074 --> 00:09:23.380
شاید بازار آزاد
این مسئله را حل کند.

00:09:23.404 --> 00:09:24.653
این‌طور نخواهد شد.

00:09:24.677 --> 00:09:27.797
پول زیادی از بی‌عدالتی
به دست می‌آید.

00:09:28.947 --> 00:09:32.316
علاوه بر این، ما 
عامل‌های اقتصادیِ منطقی‌ای نیستیم.

00:09:32.851 --> 00:09:34.143
همه‌ی ما تعصباتی داریم.

00:09:34.780 --> 00:09:38.157
ما همه نژادپرست و متعصبیم
به طرقی که دوست داشتیم نباشیم،

00:09:38.181 --> 00:09:40.200
به طرقی که حتی نمی‌دانیم.

00:09:41.172 --> 00:09:44.253
هر چند در مجموع این را می‌دانیم

00:09:44.277 --> 00:09:47.497
چون جامعه‌شناسان
مدام این را با آزمایش‌هایی که می‌کنند،

00:09:47.521 --> 00:09:49.186
ثابت کرده‌اند.

00:09:49.210 --> 00:09:51.778
آن‌ها تعدادی درخواست
استخدام را ارسال می‌کنند

00:09:51.802 --> 00:09:54.493
که به یک اندازه واجد شرایطند
اما برخی نام‌های سفیدپوستی

00:09:54.493 --> 00:09:56.033
و برخی نام‌های سیاه‌پوستی دارند،

00:09:56.057 --> 00:09:58.751
و نتیجه همواره ناامیدکننده است، همیشه.

00:09:59.330 --> 00:10:01.101
بنابراین این ما هستیم که تعصب داریم،

00:10:01.125 --> 00:10:04.554
و این تعصبات را
با داده‌هایی که جمع‌آوری می‌کنیم

00:10:04.578 --> 00:10:06.390
به الگوریتم‌ها تزریق می‌کنیم.

00:10:06.414 --> 00:10:09.157
مثلاً من تصمیمی گرفتم 
به ریمن نودل فکر نکنم

00:10:09.181 --> 00:10:10.806
به نظرم نامربوط بود.

00:10:10.830 --> 00:10:16.514
اما با اعتماد به داده‌هایی
که از تجربیات گذشته یاد می‌گیرند

00:10:16.538 --> 00:10:18.552
و با انتخاب تعریف موفقیت،

00:10:18.576 --> 00:10:22.559
چطور می‌توانیم از الگوریتم‌ها
انتظار داشته باشیم جان سالم به در ببرند؟

00:10:22.583 --> 00:10:24.939
نمی‌توانیم. باید آن‌ها را بررسی کنیم.

00:10:25.985 --> 00:10:27.694
باید عدالت را در آن‌ها بررسی کنیم.

00:10:27.718 --> 00:10:30.429
خبر خوب این‌که،
می‌توانیم عدالت را در آنان بررسی کنیم.

00:10:30.453 --> 00:10:33.805
می‌توان الگوریتم‌ها را بازجویی کرد.

00:10:33.829 --> 00:10:35.863
و آن‌ها هر بار به ما
حقیقت را خواهند گفت.

00:10:35.887 --> 00:10:38.830
و می‌توانیم آن‌ها را درست کنیم.
می‌توانیم آن‌ها را بهتر کنیم.

00:10:38.830 --> 00:10:40.779
من به این حساب‌رسی الگوریتمی می‌گویم،

00:10:40.803 --> 00:10:42.482
و آن را به شما توضیح می‌دهم.

00:10:42.506 --> 00:10:44.702
نخست، بررسی درستی داده‌ها.

00:10:45.952 --> 00:10:48.609
برای الگوریتم ریسک تکرار جنایت
که درباره‌اش صحبت کردم،

00:10:49.402 --> 00:10:52.975
بررسی درستی داده به این معنی است
که باید با این حقیقت کنار بیاییم

00:10:52.999 --> 00:10:56.525
که در ایالات متحده، سیاه‌پوستان
و سفیدپوستان به میزان یکسانی گُل می‌کشند

00:10:56.549 --> 00:10:59.034
اما سیاه‌پوستان به مراتب
بیشتر دستگیر می‌شوند

00:10:59.058 --> 00:11:02.242
چهار تا پنچ برابر بیشتر
وابسته به منطقه.

00:11:03.137 --> 00:11:05.963
این تعصب در سایر رده‌های جرم چطور است،

00:11:05.987 --> 00:11:07.438
و چطور آن را در نظر بگیریم؟

00:11:07.982 --> 00:11:11.021
دوم، باید درباره‌ی تعریف موفقیت فکر کنیم،

00:11:11.045 --> 00:11:12.426
آن را حسابرسی کنیم.

00:11:12.450 --> 00:11:15.202
الگوریتم استخدام را به خاطر دارید؟
درباره‌ی آن صحبت کردیم.

00:11:15.226 --> 00:11:18.391
فردی که چهارسال بماند
و یک بار ارتقاء گرفته باشد؟

00:11:18.415 --> 00:11:20.184
خب، این یک کارمند موفق است.

00:11:20.208 --> 00:11:23.567
اما علاوه بر این کارمندی است 
که در فرهنگش مورد حمایت قرار گرفته است.

00:11:23.909 --> 00:11:25.835
بنابراین، آن هم می‌تواند متعصبانه باشد.

00:11:25.859 --> 00:11:27.924
باید این دو را از هم جدا کنیم.

00:11:27.948 --> 00:11:29.338
برای مثال باید

00:11:29.338 --> 00:11:31.594
به مصاحبه‌ی ارکستر ناپیدا بنگریم.

00:11:31.618 --> 00:11:34.374
در این مصاحبه افراد مصاحبه‌گر
در پسِ یک پرده هستند.

00:11:34.766 --> 00:11:36.697
آن‌چه می‌خواهم به آن فکر کنم

00:11:36.721 --> 00:11:40.138
این است که افرادی که گوش می‌دهند
متوجه شده‌اند چه چیزی مهم است

00:11:40.162 --> 00:11:42.191
و چه چیزی مهم نیست،

00:11:42.215 --> 00:11:44.274
و به خاطر آن
حواس‌شان پرت نمی‌شود.

00:11:44.781 --> 00:11:47.530
زمانی که مصاحبه‌ی ارکستر ناپیدا شروع شد،

00:11:47.554 --> 00:11:50.998
تعداد زنان در ارکسترها
پنچ برابر شد.

00:11:52.073 --> 00:11:54.088
سپس، باید دقت را در نظر بگیریم.

00:11:55.053 --> 00:11:58.787
اینجاست که مدل ارزش افزوده
برای معلمان بلافاصله در هم می‌شکند.

00:11:59.398 --> 00:12:01.560
البته هیچ الگوریتمی بی‌نقص نیست،

00:12:02.440 --> 00:12:06.045
بنابراین باید خطای
تمام الگوریتم‌ها را در نظر بگیریم.

00:12:06.656 --> 00:12:11.015
این خطاها تا چه حد پر تکرارند،
و این مدل برای چه کسانی بد عمل می‌کند؟

00:12:11.670 --> 00:12:13.388
هزینه‌ی این خطا چقدر است؟

00:12:14.254 --> 00:12:16.361
و در نهایت، باید 

00:12:17.793 --> 00:12:20.399
آثار بلند مدت الگوریتم‌ها را 
در نظر بگیریم.

00:12:20.686 --> 00:12:22.893
حلقه‌های بازخوردی که تشدید کننده‌اند.

00:12:23.406 --> 00:12:24.642
به نظر انتزاعی می‌رسد،

00:12:24.666 --> 00:12:27.530
اما تصور کنید اگر مهندسان فیسبوک
پیش از آن‌که تصمیم بگیرند

00:12:28.090 --> 00:12:31.741
فقط چیزهایی را به ما نشان بدهند 
که دوستانمان فرستاده‌اند،

00:12:31.741 --> 00:12:33.581
این مسئله را در نظر نگرفته بودند.

00:12:33.581 --> 00:12:36.815
دو پیام دیگر هم دارم،
یکی برای دانشمندان داده.

00:12:37.270 --> 00:12:40.679
دانشمندان داده: ما نباید
داوران حقیقت باشیم.

00:12:41.340 --> 00:12:44.607
ما باید مترجمان گفتگوهای اخلاقی باشیم

00:12:44.607 --> 00:12:46.861
که در جامعه‌ی بزرگتر رخ می‌دهد.

00:12:47.399 --> 00:12:49.532
(تشویق حضار)

00:12:49.556 --> 00:12:51.112
و بقیه‌ی شما،

00:12:51.831 --> 00:12:53.227
کسانی که دانشمند داده نیستند:

00:12:53.251 --> 00:12:54.749
این یک امتحان ریاضی نیست.

00:12:55.452 --> 00:12:56.800
این یک جنگ سیاسی است.

00:12:58.407 --> 00:13:02.314
ما باید مسئولیت‌پذیری را
از اربابانِ الگوریتمی‌مان مطالبه کنیم.

00:13:03.938 --> 00:13:05.437
(تشویق حضار)

00:13:05.461 --> 00:13:09.686
عصر ایمان کورکورانه
به داده‌های عظیم باید پایان بیابد.

00:13:09.710 --> 00:13:10.877
خیلی متشکرم.

00:13:10.901 --> 00:13:16.204
(تشویق حضار)

