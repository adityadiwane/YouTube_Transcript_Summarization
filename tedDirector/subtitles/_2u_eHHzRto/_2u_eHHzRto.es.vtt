WEBVTT
Kind: captions
Language: es

00:00:00.000 --> 00:00:07.000
Traductor: Silvina Katz
Revisor: Lidia Cámara de la Fuente

00:00:12.795 --> 00:00:14.391
Hay algoritmos por todos lados.

00:00:15.931 --> 00:00:19.056
Ordenan y separan a los ganadores 
de los perdedores.

00:00:19.839 --> 00:00:22.103
Los ganadores consiguen el trabajo

00:00:22.127 --> 00:00:23.870
o buenas condiciones de crédito.

00:00:23.894 --> 00:00:26.865
A los perdedores ni siquiera 
se les invita a una entrevista

00:00:27.410 --> 00:00:29.187
o tienen que pagar más por el seguro.

00:00:30.017 --> 00:00:33.566
Se nos califica mediante fórmulas
secretas que no entendemos

00:00:34.495 --> 00:00:37.712
y a las que no se puede apelar.

00:00:39.060 --> 00:00:40.356
Eso plantea una pregunta:

00:00:40.380 --> 00:00:43.293
¿Qué pasa si los algoritmos se equivocan?

00:00:44.920 --> 00:00:46.960
Un algoritmo necesita dos cosas:

00:00:46.984 --> 00:00:48.965
datos ocurridos en el pasado

00:00:48.989 --> 00:00:50.550
y una definición del éxito;

00:00:50.574 --> 00:00:53.031
esto es, lo que uno quiere y lo que desea.

00:00:53.055 --> 00:00:58.092
Los algoritmos se entrenan
mirando, descubriendo.

00:00:58.116 --> 00:01:01.535
El algoritmo calcula a qué
se asocia el éxito,

00:01:01.559 --> 00:01:04.022
qué situaciones llevan al éxito.

00:01:04.701 --> 00:01:06.463
En general todos usamos algoritmos

00:01:06.487 --> 00:01:09.205
pero no los formalizamos 
mediante un código escrito.

00:01:09.229 --> 00:01:10.577
Les doy un ejemplo.

00:01:10.601 --> 00:01:13.917
Yo uso un algoritmo todos los días
para preparar la comida en casa.

00:01:13.941 --> 00:01:15.417
Los datos que uso

00:01:16.214 --> 00:01:17.873
son los ingredientes de la cocina,

00:01:17.897 --> 00:01:19.424
el tiempo que tengo

00:01:19.448 --> 00:01:20.681
y lo ambiciosa que estoy.

00:01:20.705 --> 00:01:22.414
Y así organizo los datos.

00:01:22.438 --> 00:01:26.689
No incluyo esos paquetitos
de fideos como comida.

00:01:26.713 --> 00:01:28.582
(Risas)

00:01:28.606 --> 00:01:30.451
Mi definición del éxito es:

00:01:30.475 --> 00:01:33.134
la comida tiene éxito, 
si mis hijos comen verdura.

00:01:34.001 --> 00:01:36.855
Lo que sería muy distinto, 
si mi hijito tuviera el control.

00:01:36.879 --> 00:01:39.667
Para él el éxito es comer
mucha Nutella.

00:01:40.999 --> 00:01:43.225
Pero yo soy quien elige el éxito.

00:01:43.249 --> 00:01:45.956
Estoy al mando. Mi opinión cuenta.

00:01:45.980 --> 00:01:48.655
Esa es la primera regla de los algoritmos.

00:01:48.679 --> 00:01:51.859
Los algoritmos se embeben en código.

00:01:53.382 --> 00:01:57.045
Es muy diferente a cómo la gente 
se imagina los algoritmos.

00:01:57.069 --> 00:02:01.573
Se creen que los algoritmos son
objetivos, verdaderos y científicos.

00:02:02.207 --> 00:02:03.906
Ese en un truco del marketing.

00:02:05.089 --> 00:02:07.214
Tambien es un truco del marketing

00:02:07.238 --> 00:02:10.392
la intimidación con algoritmos,

00:02:10.416 --> 00:02:14.077
que nos hacer confiar 
y temer los algoritmos

00:02:14.101 --> 00:02:16.559
porque confiamos y tememos 
las matemáticas.

00:02:17.387 --> 00:02:22.217
Muchas cosas pueden salir mal si
confiamos a ciegas en datos masivos.

00:02:23.504 --> 00:02:26.877
Esta es Kiri Soares. Es la directora 
de una escuela de Brooklyn.

00:02:26.901 --> 00:02:29.371
En 2011 me contó que 
sus maestros se clasificaban

00:02:29.371 --> 00:02:32.212
mediante un algoritmo complejo y secreto

00:02:32.212 --> 00:02:33.981
llamado "modelo del valor añadido".

00:02:34.325 --> 00:02:37.417
Le dije, "Intente saber 
cuál es la fórmula, muéstremela.

00:02:37.441 --> 00:02:38.982
Se la voy a explicar".

00:02:39.006 --> 00:02:41.147
Me respondió, 
"Trate de conseguir la fórmula,

00:02:41.171 --> 00:02:43.943
pero un conocido del Departamento 
de Educación me dijo

00:02:43.967 --> 00:02:46.073
que era matemática y 
que no la entendería".

00:02:47.086 --> 00:02:48.424
Esto se pone peor.

00:02:48.448 --> 00:02:51.978
El New York Post la solicitó bajo la 
Ley de Libertad a la Información.

00:02:52.002 --> 00:02:54.961
Obtuvo los nombres de los maestros
y su puntuación

00:02:54.985 --> 00:02:57.767
y los publicó como un acto para
avergonzar a los maestros.

00:02:58.904 --> 00:03:02.764
Cuando intenté conseguir las fórmulas en 
código base, usando el mismo mecanismo,

00:03:02.788 --> 00:03:04.937
me dijeron que no se podía.

00:03:04.961 --> 00:03:06.197
Me lo negaron.

00:03:06.221 --> 00:03:07.395
Más tarde descubrí

00:03:07.419 --> 00:03:10.285
que nadie tenía derecho 
a la fórmula en Nueva York.

00:03:10.309 --> 00:03:11.614
Nadie lo podía entender.

00:03:13.749 --> 00:03:16.973
Entonces apareció un tipo muy 
inteligente, Gary Rubenstein.

00:03:16.997 --> 00:03:20.618
Localizó a 665 maestros por
los datos del New York Post

00:03:20.642 --> 00:03:22.508
que tenían dos puntuaciones.

00:03:22.532 --> 00:03:24.413
Eso podía ocurrir si enseñaban

00:03:24.437 --> 00:03:26.876
matemática en 7º y 8º grado.

00:03:26.900 --> 00:03:28.438
Decidió hacer un gráfico.

00:03:28.462 --> 00:03:30.635
Donde cada punto representa 
a un maestro.

00:03:30.924 --> 00:03:33.303
(Risas)

00:03:33.327 --> 00:03:34.848
Y eso ¿qué es?

00:03:34.872 --> 00:03:36.149
(Risas)

00:03:36.173 --> 00:03:39.619
Eso no debiera haberse usado nunca
para evaluar a una persona.

00:03:39.643 --> 00:03:41.569
Es casi un generador de números al azar.

00:03:41.593 --> 00:03:44.539
(Aplausos)

00:03:44.563 --> 00:03:45.725
Pero lo fue.

00:03:45.749 --> 00:03:46.925
Esta es Sarah Wysocki.

00:03:46.949 --> 00:03:49.124
La echaron junto a otros 205 maestros

00:03:49.148 --> 00:03:51.810
de una escuela en Washington DC,

00:03:51.834 --> 00:03:54.743
a pesar de tener muy buena recomendación 
de la directora

00:03:54.767 --> 00:03:56.605
y de los padres de sus alumnos.

00:03:56.840 --> 00:03:58.942
Me imagino lo que estarán pensando,

00:03:58.942 --> 00:04:01.753
especialmente los cientificos de 
datos, los expertos en IA

00:04:01.777 --> 00:04:06.003
Pensarán "Nosotros nunca produciríamos
un algoritmo tan inconsistente."

00:04:06.673 --> 00:04:08.356
Pero los algoritmos a veces fallan,

00:04:08.380 --> 00:04:12.978
y tambien provocar mucha destrucción
sin querer.

00:04:14.351 --> 00:04:16.730
Y mientras un avión mal diseñado

00:04:16.754 --> 00:04:18.755
se estrella y todos lo ven,

00:04:18.779 --> 00:04:20.629
un algoritmo mal diseñado

00:04:22.065 --> 00:04:25.930
puede funcionar mucho tiempo
provocando un desastre silenciosamente.

00:04:27.568 --> 00:04:29.138
Este es Roger Ailes.

00:04:29.162 --> 00:04:31.162
(Risas)

00:04:32.344 --> 00:04:34.732
Fundador de Fox News en el 1996.

00:04:35.256 --> 00:04:37.837
Mas de 20 mujeres se quejaron de
acoso sexual.

00:04:37.861 --> 00:04:41.096
Dijeron que no pudieron 
tener éxito en Fox News.

00:04:41.120 --> 00:04:43.850
Lo echaron el año pasado,
pero hemos visto que hace poco

00:04:43.850 --> 00:04:46.334
los problemas han continuado.

00:04:47.474 --> 00:04:48.874
Esto plantea una pregunta:

00:04:48.898 --> 00:04:51.782
¿Qué debe hacer Fox News para cambiar?

00:04:53.065 --> 00:04:55.340
Y si substituyeran su mecanismo
de contratación

00:04:55.340 --> 00:04:57.784
con un algoritmo de auto-
aprendizaje automatizado?

00:04:57.808 --> 00:04:59.403
¿Suena bien?

00:04:59.427 --> 00:05:00.727
Piénsenlo,

00:05:00.751 --> 00:05:02.856
Los datos, ¿qué datos serían?

00:05:02.880 --> 00:05:07.827
Una eleccion razonable serian las últimas
21 solicitudes recibidas por Fox News

00:05:07.851 --> 00:05:09.353
Razonable.

00:05:09.377 --> 00:05:11.315
Y ¿cuál sería la definición del éxito?

00:05:11.741 --> 00:05:12.929
Algo razonable sería

00:05:12.929 --> 00:05:14.867
preguntar, quién es exitoso en Fox News.

00:05:14.891 --> 00:05:18.471
Me imagino que alguien que
hubiera estado alli unos 4 años

00:05:18.495 --> 00:05:20.499
y subido de puesto por lo menosuna vez.

00:05:20.636 --> 00:05:22.197
¿Suena razonable?

00:05:22.221 --> 00:05:24.575
Y así se adiestraría el algoritmo.

00:05:24.599 --> 00:05:28.476
Se adiestraría para buscar a gente 
que logra el éxito.

00:05:29.039 --> 00:05:33.357
Y qué solicitudes antiguas 
llegaron al éxito

00:05:33.381 --> 00:05:34.675
según esa definición.

00:05:36.020 --> 00:05:37.795
Ahora piensen que ocurriría

00:05:37.819 --> 00:05:40.374
si lo usáramos con los candidatos de hoy.

00:05:40.939 --> 00:05:42.568
Filtraría a las mujeres

00:05:43.483 --> 00:05:47.413
ya que no parecen ser personas que
hayan tenido éxito en el pasado.

00:05:51.572 --> 00:05:54.109
Los algoritmos no son justos

00:05:54.133 --> 00:05:56.827
si uno usa algoritmos a ciegas.

00:05:56.851 --> 00:05:58.333
No son justos.

00:05:58.357 --> 00:06:00.485
Repiten prácticas anteriores,

00:06:00.509 --> 00:06:01.692
nuestros patrones.

00:06:01.716 --> 00:06:03.655
Automatizan al status quo.

00:06:04.538 --> 00:06:06.927
Sería genial en un mundo perfecto,

00:06:07.725 --> 00:06:09.037
pero no lo tenemos.

00:06:09.061 --> 00:06:13.163
Y aclaro que la mayoria de las empresas
no estan involucradas en litigios,

00:06:14.266 --> 00:06:16.854
pero los cientificos de datos 
de esas empresas

00:06:16.878 --> 00:06:19.067
emplean esos datos

00:06:19.091 --> 00:06:21.234
para lograr la precisión.

00:06:22.093 --> 00:06:23.474
Piensen qué significa esto.

00:06:23.498 --> 00:06:27.525
Porque todos tenemos prejuicios,
y así podríamos codificar sexismo

00:06:27.549 --> 00:06:29.385
u otro tipo de fanatismo.

00:06:31.308 --> 00:06:32.949
Un experimento de pensamiento,

00:06:32.949 --> 00:06:34.262
porque me gusta,

00:06:35.394 --> 00:06:38.369
una sociedad totalmente segregada.

00:06:40.067 --> 00:06:43.395
segregada racialmente, 
todas las ciudades y los barrios

00:06:43.419 --> 00:06:46.456
y donde enviamos a la policia
solo a barrios minoritarios

00:06:46.480 --> 00:06:47.673
para detectar delitos.

00:06:48.271 --> 00:06:50.490
Los arrestos serían sesgados.

00:06:51.671 --> 00:06:54.246
Y, además, elegimos a los
cientificos de datos

00:06:54.270 --> 00:06:58.431
y pagamos por los datos para predecir
dónde ocurrirán los próximos delitos.

00:06:59.095 --> 00:07:00.582
El barrio de una minoría.

00:07:01.105 --> 00:07:04.230
O a predecir quien será 
el próximo criminal.

00:07:04.708 --> 00:07:06.103
Una minoría.

00:07:07.769 --> 00:07:11.310
Los cientificos de datos se jactarían
de su grandeza y de la precisión

00:07:11.334 --> 00:07:12.631
de su modelo,

00:07:12.655 --> 00:07:13.954
y tendrían razón.

00:07:15.771 --> 00:07:20.386
La realidad no es tan drástica,
pero tenemos grandes segregaciones

00:07:20.410 --> 00:07:21.697
en muchas ciudades

00:07:21.721 --> 00:07:23.614
y tenemos muchas pruebas

00:07:23.638 --> 00:07:26.326
de datos políticos y 
legislativos sesgados.

00:07:27.452 --> 00:07:30.267
Y podemos predecir puntos calientes,

00:07:30.291 --> 00:07:32.151
lugares donde podrá ocurrir un delito

00:07:32.221 --> 00:07:36.087
Y así predecir un crimen individual

00:07:36.111 --> 00:07:37.881
y la criminalidad de los individuos.

00:07:38.792 --> 00:07:42.755
El organismo de noticias ProPublica 
lo estudió hace poco.

00:07:42.779 --> 00:07:44.803
un algoritmo de "riesgo recidivista"

00:07:44.827 --> 00:07:45.990
según los llaman

00:07:46.014 --> 00:07:49.208
usado en Florida
al hacer sentencias judiciales.

00:07:50.231 --> 00:07:53.816
Bernardo, a la izquierda, un hombre negro
sacó una puntuación de 10 de 10.

00:07:54.999 --> 00:07:57.006
Dylan, a la derecha, 3 de 10.

00:07:57.030 --> 00:07:59.531
10 de 10, alto riesgo
3 de 10, bajo riesgo.

00:08:00.418 --> 00:08:02.803
Los sentenciaron por tener drogas.

00:08:02.827 --> 00:08:04.371
Ambos con antecedentes penales

00:08:04.371 --> 00:08:06.811
pero Dylan habia cometido un delito

00:08:06.835 --> 00:08:08.011
Bernard, no.

00:08:09.638 --> 00:08:12.704
Esto importa porque
a mayor puntuación

00:08:12.728 --> 00:08:16.201
mayor probabilidad de 
una sentencia más larga.

00:08:18.114 --> 00:08:19.408
¿Que sucede?

00:08:20.346 --> 00:08:21.678
Lavado de datos.

00:08:22.750 --> 00:08:27.177
El proceso que se usa para
ocultar verdades feas

00:08:27.201 --> 00:08:29.022
dentro de una caja negra
de algoritmos

00:08:29.046 --> 00:08:30.336
y llamarlos objetivos;

00:08:31.140 --> 00:08:32.708
llamándolos meritocráticos

00:08:34.938 --> 00:08:37.323
cuando son secretos,
importantes y destructivos

00:08:37.347 --> 00:08:39.834
Les puse un nombre a estos algoritmos:

00:08:39.858 --> 00:08:41.857
"armas matemáticas de destrucción"

00:08:41.881 --> 00:08:43.445
(Risas)

00:08:43.469 --> 00:08:46.523
(Aplausos)

00:08:46.547 --> 00:08:48.901
Estan en todos sitios

00:08:49.515 --> 00:08:53.238
Son empresas privadas
que construyen algoritmos privados

00:08:53.262 --> 00:08:54.654
para fines privados.

00:08:55.034 --> 00:08:58.248
Incluso los mencionados
de los maestros y la policía pública

00:08:58.272 --> 00:09:00.141
fueron diseñados por empresas privadas

00:09:00.165 --> 00:09:02.396
y vendidos a 
instituciones gubernamentales.

00:09:02.420 --> 00:09:04.293
Lo llaman su "salsa secreta"

00:09:04.317 --> 00:09:06.445
por eso no nos pueden hablar de ello.

00:09:06.469 --> 00:09:08.689
Es un poder privado

00:09:09.744 --> 00:09:14.439
que saca provecho por su
autoridad inescrutable.

00:09:16.934 --> 00:09:19.868
Entonces uno ha de pensar,
ya que todo esto es privado

00:09:19.892 --> 00:09:21.050
y hay competición,

00:09:21.074 --> 00:09:23.380
tal vez un mercado libre
podrá solucionarlo

00:09:23.404 --> 00:09:24.653
Pero no.

00:09:24.677 --> 00:09:27.797
Se puede ganar mucho dinero
con la injusticia.

00:09:28.947 --> 00:09:32.316
Tampoco somos agentes 
económicos racionales.

00:09:32.851 --> 00:09:34.143
Todos tenemos prejuicios

00:09:34.780 --> 00:09:38.157
Somos racistas y fanáticos
de una forma que no quisiéramos,

00:09:38.181 --> 00:09:40.200
de maneras que desconocemos.

00:09:41.172 --> 00:09:44.253
Lo sabemos al sumarlo

00:09:44.277 --> 00:09:47.497
porque los sociólogos
lo han demostrado consistentemente

00:09:47.521 --> 00:09:49.090
con experimentos que construyeron

00:09:49.090 --> 00:09:51.502
donde mandan una cantidad de solicitudes
de empleo

00:09:51.502 --> 00:09:54.943
de personas de calificaciones iguales
pero algunas con apellidos blancos

00:09:54.943 --> 00:09:56.363
y otras con apellidos negros,

00:09:56.363 --> 00:09:58.751
y los resultados siempre los 
decepcionan, siempre.

00:09:59.330 --> 00:10:01.101
Nosotros somos los prejuiciosos

00:10:01.125 --> 00:10:04.554
que inyectamos prejuicios
a nuestros algoritmos

00:10:04.578 --> 00:10:06.390
al elegir qué datos recoger,

00:10:06.414 --> 00:10:09.157
así como yo elegí no pensar 
en los fideos--

00:10:09.181 --> 00:10:10.806
Y decidi que no era importante.

00:10:10.830 --> 00:10:16.514
Pero tenerle confianza a los datos
basados en prácticas pasadas

00:10:16.538 --> 00:10:18.552
y eligiendo la definición del éxito,

00:10:18.576 --> 00:10:22.559
¿cómo pretendemos que los
algoritmos emerjan intactos?

00:10:22.583 --> 00:10:24.939
No podemos. Tenemos que verificarlos.

00:10:25.985 --> 00:10:27.694
Hay que revisarlos por equidad.

00:10:27.718 --> 00:10:30.429
Y las buenas noticias son

00:10:30.453 --> 00:10:33.805
que los algoritmos pueden ser 
interrogados,

00:10:33.829 --> 00:10:35.863
y nos dirán la verdad todas las veces.

00:10:35.887 --> 00:10:38.380
Y los podemos arreglar.
Y mejorarlos.

00:10:38.404 --> 00:10:40.779
Lo explico. Esto se llama revisión 
del algoritmo,

00:10:40.803 --> 00:10:42.482
lo explico.

00:10:42.506 --> 00:10:44.702
Primero, verificación de 
integridad de datos.

00:10:45.952 --> 00:10:48.609
por el riesgo recidivista.

00:10:49.402 --> 00:10:52.975
La verificación de la integridad de datos 
implicaría una conciliación

00:10:52.999 --> 00:10:56.525
que en EE. UU. los blancos y los 
negros fuman marihuana

00:10:56.549 --> 00:10:59.034
pero a los negros es mas fácil que 
los arresten

00:10:59.058 --> 00:11:02.242
más probablemente cuatro o cinco 
veces más dependiendo de la zona.

00:11:03.137 --> 00:11:05.963
Y ¿cómo son los prejuicios en 
otras categorías criminales,

00:11:05.987 --> 00:11:07.438
y cómo lo justificamos?

00:11:07.982 --> 00:11:11.021
Segundo, debemos pensar 
en la definición del éxito,

00:11:11.045 --> 00:11:12.426
revisarla.

00:11:12.450 --> 00:11:15.202
¿Recuerdan el algoritmo
de la contratación?

00:11:15.226 --> 00:11:18.391
alguien que se queda cuatro años 
y asciende de cargo una vez?

00:11:18.415 --> 00:11:20.184
Ese es el empleado exitoso,

00:11:20.208 --> 00:11:23.287
pero tambien es el empleado
apoyado por la cultura.

00:11:23.909 --> 00:11:25.835
Esto puede ser bastante injusto.

00:11:25.859 --> 00:11:27.924
Tenemos que separar dos cosas.

00:11:27.948 --> 00:11:30.374
Mirar a la audicion de una 
orquesta de ciegos

00:11:30.398 --> 00:11:31.594
por ejemplo.

00:11:31.618 --> 00:11:34.374
Los que dan la audición están 
detrás de la partitura.

00:11:34.766 --> 00:11:36.697
Lo que quiero que piensen

00:11:36.721 --> 00:11:40.138
es que la gente que escucha
decide lo que es importante

00:11:40.162 --> 00:11:42.191
y lo que no lo es,

00:11:42.215 --> 00:11:44.274
sin que eso nos distraiga.

00:11:44.781 --> 00:11:47.530
Cuando empezaron las audiciones 
de orquesta de ciegos

00:11:47.554 --> 00:11:50.998
la cantidad de mujeres aumentó
un factor de cinco veces.

00:11:52.073 --> 00:11:54.088
Tambien hay que pensar en la precisión

00:11:55.053 --> 00:11:58.787
y así el modelo 
del valor añadido fallaría.

00:11:59.398 --> 00:12:01.560
Por supuesto ningún algoritmo es perfecto,

00:12:02.440 --> 00:12:06.045
asi que hay que considerar los 
errores de cada algoritmo.

00:12:06.656 --> 00:12:11.015
¿Qué frecuencia tienen los errores
y con quiénes falla?

00:12:11.670 --> 00:12:13.388
Y ¿cuál es el costo de dicha falla?

00:12:14.254 --> 00:12:16.461
Y por último, tenemos que considerar

00:12:17.793 --> 00:12:19.979
los efectos a largo plazo 
de los algoritmos,

00:12:20.686 --> 00:12:22.893
los bucles de retroalimentación 
que engendran.

00:12:23.406 --> 00:12:24.642
Eso suena a abstracto.

00:12:24.666 --> 00:12:28.090
Pero imagínese si los ingenieros 
de Facebook lo hubieran considerado

00:12:28.090 --> 00:12:32.945
antes de mostrarnos cosas
publicadas por nuestros amigos.

00:12:33.581 --> 00:12:36.815
Tengo dos mensajes,
uno para los científicos de datos.

00:12:37.270 --> 00:12:40.679
Cientificos de datos: no debemos
ser los árbitros de la verdad.

00:12:41.340 --> 00:12:45.123
Debemos ser tradutores de las
discusiones éticas que ocurren

00:12:45.147 --> 00:12:46.441
en toda la sociedad.

00:12:47.399 --> 00:12:49.532
(Aplausos)

00:12:49.556 --> 00:12:51.112
Y para el resto de Uds.

00:12:51.831 --> 00:12:53.567
los que no son científicos de datos:

00:12:53.567 --> 00:12:55.429
esta no es un examen de matemáticas.

00:12:55.452 --> 00:12:56.800
Es una lucha politica.

00:12:58.407 --> 00:13:02.314
Tenemos que exigir responsabilidad
a los lores de los algoritmos.

00:13:03.938 --> 00:13:05.437
(Aplausos)

00:13:05.461 --> 00:13:09.686
La era de la fe ciega en los
datos masivos debe terminar.

00:13:09.710 --> 00:13:10.877
Muchas gracias.

00:13:10.901 --> 00:13:17.421
(Aplauso)

