WEBVTT
Kind: captions
Language: pt-PT

00:00:00.000 --> 00:00:07.000
Tradutor: Marta Sousa
Revisora: Margarida Ferreira

00:00:12.795 --> 00:00:14.891
Os algoritmos estão em todo o lado.

00:00:15.931 --> 00:00:19.056
Eles classificam e separam
os vencedores dos perdedores.

00:00:19.839 --> 00:00:22.103
Os vencedores ficam com o emprego

00:00:22.103 --> 00:00:24.126
ou uma boa oferta
para o cartão de crédito.

00:00:24.126 --> 00:00:26.521
Os que perdem nem sequer vão à entrevista

00:00:27.410 --> 00:00:29.187
mas pagam mais pelo seguro.

00:00:30.017 --> 00:00:33.566
Estamos a ser marcados
com fórmulas secretas que não entendemos

00:00:34.495 --> 00:00:37.712
e que, muitas vezes,
não têm sistemas de recurso.

00:00:39.060 --> 00:00:40.356
Isto traz-nos a pergunta:

00:00:40.380 --> 00:00:43.293
E se os algoritmos estão errados?

00:00:44.920 --> 00:00:46.960
Para criar um algoritmo é preciso:

00:00:46.984 --> 00:00:48.965
dados — o que aconteceu no passado;

00:00:48.989 --> 00:00:50.550
e uma definição de sucesso,

00:00:50.574 --> 00:00:53.191
aquilo de que estamos à procura
e que queremos atingir.

00:00:53.231 --> 00:00:58.092
Um algoritmo treina-se
com a procura, com a descoberta.

00:00:58.116 --> 00:01:01.535
O algoritmo descobre
o que está associado ao sucesso.

00:01:01.559 --> 00:01:04.022
Qual a situação que leva ao sucesso?

00:01:04.701 --> 00:01:06.463
Na verdade, todos usam algoritmos.

00:01:06.487 --> 00:01:09.205
Só que as pessoas não os formalizam
em código escrito.

00:01:09.229 --> 00:01:10.577
Deixem-me dar um exemplo.

00:01:10.601 --> 00:01:13.917
Eu uso um algoritmo todos os dias
para cozinhar para a minha família.

00:01:13.941 --> 00:01:15.417
Os dados que eu uso

00:01:16.214 --> 00:01:17.873
são os ingredientes que tenho,

00:01:17.897 --> 00:01:19.424
o tempo disponível,

00:01:19.448 --> 00:01:20.681
a minha ambição,

00:01:20.705 --> 00:01:22.414
e junto esses dados todos.

00:01:22.438 --> 00:01:26.689
Não contabilizo aqueles pacotes pequenos
de "noodles" como comida.

00:01:26.713 --> 00:01:28.582
(Risos)

00:01:28.606 --> 00:01:30.451
A minha definição de sucesso é:

00:01:30.475 --> 00:01:33.134
a refeição é conseguida
se os filhos comerem vegetais.

00:01:34.001 --> 00:01:36.855
Seria diferente se o meu filho
mais novo fosse o responsável.

00:01:36.879 --> 00:01:39.667
Ele diria que o sucesso
seria ele comer muita Nutella.

00:01:40.999 --> 00:01:43.225
Mas eu posso escolher o sucesso.

00:01:43.249 --> 00:01:45.956
Sou eu a responsável.
A minha opinião conta.

00:01:45.980 --> 00:01:48.655
Essa é a primeira regra
dos algoritmos.

00:01:48.679 --> 00:01:51.859
Os algoritmos são opiniões
embutidas em código.

00:01:53.382 --> 00:01:57.045
É muito diferente daquilo que muitos
pensam sobre este assunto.

00:01:57.069 --> 00:02:01.573
As pessoas pensam que os algoritmos
são objetivos, verdadeiros e científicos.

00:02:02.207 --> 00:02:03.906
Isso é um truque de "marketing".

00:02:05.089 --> 00:02:07.214
O que também é um truque

00:02:07.238 --> 00:02:10.392
é o facto de os algoritmos
nos intimidarem,

00:02:10.416 --> 00:02:14.077
para nos fazer ter confiança
e medo deles,

00:02:14.101 --> 00:02:16.119
porque confiamos
e receamos a matemática.

00:02:17.387 --> 00:02:22.217
Muita coisa pode correr mal
se confiarmos cegamente nos "big data".

00:02:23.504 --> 00:02:26.877
Esta é Kiri Soares e é diretora
de uma escola secundária em Brooklyn.

00:02:26.901 --> 00:02:29.487
Em 2011, ela disse-me que
os professores eram avaliados

00:02:29.511 --> 00:02:32.238
com um algoritmo secreto e complexo

00:02:32.262 --> 00:02:34.151
chamado "modelo de valor acrescentado".

00:02:34.325 --> 00:02:37.417
Eu disse-lhe:
"Bem, se descobrir a fórmula, mostre-ma".

00:02:37.441 --> 00:02:38.982
"Eu explico-lha".

00:02:39.006 --> 00:02:41.147
Ela respondeu: "Bom, eu tentei obtê-la"

00:02:41.171 --> 00:02:43.943
"mas o Departamento de Educação
disse que era matemática"

00:02:43.967 --> 00:02:45.513
"e que eu não iria entendê-la".

00:02:47.086 --> 00:02:48.424
E ainda é pior.

00:02:48.448 --> 00:02:51.978
O jornal "The New York Post" invocou
a lei da Liberdade de Informação,

00:02:52.002 --> 00:02:54.961
reuniu os nomes de todos os professores
e os seus resultados

00:02:54.985 --> 00:02:58.677
e publicou todas essas informações
como forma de envergonhar os professores.

00:02:58.904 --> 00:03:02.764
Quando tentei aceder às fórmulas,
ao código-fonte, através dos mesmos meios,

00:03:02.788 --> 00:03:04.937
disseram-me que não podia fazê-lo.

00:03:04.961 --> 00:03:06.197
Foi-me negado.

00:03:06.221 --> 00:03:07.395
Mais tarde, descobri

00:03:07.419 --> 00:03:10.285
que ninguém em Nova Iorque
teve acesso a essa fórmula.

00:03:10.309 --> 00:03:11.614
Ninguém a percebia.

00:03:13.749 --> 00:03:16.973
Então, envolveu-se um tipo
muito inteligente: Gary Rubenstein.

00:03:16.997 --> 00:03:20.618
Ele descobriu 665 professores,
através dos dados publicados no jornal,

00:03:20.642 --> 00:03:22.508
que tiveram dois resultados diferentes.

00:03:22.532 --> 00:03:24.413
Podia acontecer
se estivessem a lecionar

00:03:24.437 --> 00:03:26.876
matemática de sétimo
e matemática de oitavo ano.

00:03:26.900 --> 00:03:28.818
Decidiu representá-los graficamente

00:03:28.832 --> 00:03:30.815
em que cada ponto representa
um professor.

00:03:30.924 --> 00:03:32.983
(Risos)

00:03:33.327 --> 00:03:34.808
O que é isto?

00:03:34.872 --> 00:03:36.149
(Risos)

00:03:36.173 --> 00:03:39.619
Isto nunca poderia ter sido usado
para uma avaliação individual.

00:03:39.643 --> 00:03:41.569
É como um gerador
de números aleatórios.

00:03:41.593 --> 00:03:44.539
(Aplausos)

00:03:44.563 --> 00:03:45.725
Mas foi utilizado.

00:03:45.749 --> 00:03:46.925
Esta é Sarah Wysocki.

00:03:46.949 --> 00:03:49.124
Foi despedida,
juntamente com 205 professores

00:03:49.148 --> 00:03:51.810
de escolas do distrito de Washington, DC,

00:03:51.834 --> 00:03:54.743
embora tivesse excelentes
recomendações do seu diretor

00:03:54.767 --> 00:03:56.195
e dos pais dos seus alunos.

00:03:56.750 --> 00:03:58.302
Sei o que muitos estão a pensar,

00:03:58.302 --> 00:04:01.829
sobretudo os cientistas de dados
especialistas em Inteligência Artificial.

00:04:01.829 --> 00:04:06.003
Estão a pensar: "Eu nunca faria
um algoritmo tão inconsistente".

00:04:06.673 --> 00:04:08.356
Mas os algoritmos podem correr mal,

00:04:08.380 --> 00:04:12.978
chegando a ter efeitos profundamente
destrutivos, cheios de boas intenções.

00:04:14.351 --> 00:04:16.730
Enquanto que um avião
que é mal arquitetado

00:04:16.754 --> 00:04:18.755
se despenha e todos veem,

00:04:18.779 --> 00:04:20.629
um algoritmo mal projetado

00:04:22.065 --> 00:04:25.930
pode ser utilizado durante muito tempo,
causando estragos silenciosamente.

00:04:27.568 --> 00:04:29.138
Este é Roger Ailes.

00:04:29.162 --> 00:04:31.162
(Risos)

00:04:32.344 --> 00:04:34.732
Fundou a Fox News em 1996.

00:04:35.256 --> 00:04:37.837
Mais de 20 mulheres
queixaram-se de assédio sexual.

00:04:37.861 --> 00:04:41.096
Diziam que, na Fox News,
não lhes era permitido terem sucesso.

00:04:41.120 --> 00:04:43.640
Ailes foi despedido no ano passado,
mas, recentemente,

00:04:43.664 --> 00:04:46.334
temos visto que os problemas persistem.

00:04:47.474 --> 00:04:48.874
Isto leva-nos a perguntar:

00:04:48.898 --> 00:04:51.782
O que é que a Fox News deveria fazer
para virar a página?

00:04:53.065 --> 00:04:56.106
E se tivessem substituído
o seu processo de recrutamento

00:04:56.106 --> 00:04:58.170
por um algoritmo
de aprendizagem automática?

00:04:58.200 --> 00:04:59.403
Parece-vos bem, não é?

00:04:59.427 --> 00:05:00.727
Pensem nisso.

00:05:00.751 --> 00:05:02.856
Que dados poderiam ser?

00:05:02.880 --> 00:05:07.827
Uma escolha razoável seriam os currículos
recebidos nos últimos 21 anos.

00:05:07.851 --> 00:05:09.353
Razoável.

00:05:09.377 --> 00:05:11.315
E qual seria a definição de sucesso?

00:05:11.741 --> 00:05:13.065
A escolha razoável seria...

00:05:13.089 --> 00:05:14.867
Bem, quem tem sucesso na Fox News?

00:05:14.891 --> 00:05:18.471
Eu diria alguém que lá trabalhou
durante quatro anos

00:05:18.495 --> 00:05:20.629
e que foi promovido, pelo menos, uma vez.

00:05:20.636 --> 00:05:22.197
Parece razoável.

00:05:22.221 --> 00:05:24.575
E aí, o algoritmo seria treinado.

00:05:24.599 --> 00:05:28.476
Seria treinado para procurar pessoas
e perceber o que levava ao sucesso,

00:05:29.039 --> 00:05:33.357
que tipo de currículos
eram propícios a isso,

00:05:33.381 --> 00:05:34.675
seguindo essa definição.

00:05:36.020 --> 00:05:37.795
Pensem no que poderia acontecer

00:05:37.819 --> 00:05:40.374
se o aplicássemos
ao conjunto atual de candidaturas.

00:05:40.939 --> 00:05:42.568
Filtraria as mulheres,

00:05:43.483 --> 00:05:47.413
porque não foram as pessoas
que tiveram sucesso no passado.

00:05:51.572 --> 00:05:54.109
Os algoritmos não são justos,

00:05:54.133 --> 00:05:56.827
se os aplicarmos
de ânimo leve e às cegas.

00:05:56.851 --> 00:05:58.333
Eles não agem com justiça.

00:05:58.357 --> 00:06:00.485
Eles repetem o que fizemos no passado,

00:06:00.509 --> 00:06:01.692
os nossos padrões.

00:06:01.716 --> 00:06:03.655
Eles automatizam o "status quo".

00:06:04.538 --> 00:06:06.927
Isso seria incrível
se o mundo fosse perfeito.

00:06:07.725 --> 00:06:09.037
Mas não é.

00:06:09.061 --> 00:06:13.163
E digo-vos mais: a maioria das empresas
não têm processos legais em curso,

00:06:14.266 --> 00:06:16.854
mas essas empresas dizem
aos seus cientistas de dados

00:06:16.878 --> 00:06:19.067
para seguirem os dados,

00:06:19.091 --> 00:06:21.234
para se focarem na precisão.

00:06:22.093 --> 00:06:23.474
Pensem no que isso significa.

00:06:23.498 --> 00:06:27.525
Como todos temos preconceitos,
eles podiam codificar o sexismo

00:06:27.549 --> 00:06:29.385
ou qualquer outro tipo de sectarismo.

00:06:31.308 --> 00:06:32.729
Um exercício intelectual,

00:06:32.753 --> 00:06:34.262
porque gosto de fazer isso:

00:06:35.394 --> 00:06:38.369
uma sociedade inteiramente segregada

00:06:40.067 --> 00:06:43.395
— todas as cidades, os bairros,
tudo segregado racialmente —

00:06:43.419 --> 00:06:46.456
e onde só enviamos a polícia
a bairros minoritários

00:06:46.480 --> 00:06:47.673
para combater o crime.

00:06:48.271 --> 00:06:50.490
Os dados sobre os detidos
seriam tendenciosos.

00:06:51.671 --> 00:06:54.246
E se tivéssemos cientistas de dados
para esta situação

00:06:54.270 --> 00:06:58.431
e lhes pagássemos para preverem
onde iria ocorrer o crime seguinte?

00:06:59.095 --> 00:07:00.582
Num bairro minoritário.

00:07:01.105 --> 00:07:04.230
Ou para preverem quem seria
o criminoso seguinte?

00:07:04.708 --> 00:07:06.103
Alguém da minoria.

00:07:07.769 --> 00:07:12.310
Os cientistas de dados iriam vangloriar-se
da eficiência e precisão do seu modelo

00:07:12.655 --> 00:07:13.954
e teriam razão.

00:07:15.771 --> 00:07:20.386
A realidade não é tão drástica,
mas temos segregações graves

00:07:20.410 --> 00:07:21.697
em várias cidades e vilas,

00:07:21.721 --> 00:07:23.614
e existem inúmeras provas

00:07:23.638 --> 00:07:26.326
de que os dados do sistema de justiça
são tendenciosos.

00:07:27.452 --> 00:07:30.267
E nós prevemos lugares críticos,

00:07:30.291 --> 00:07:31.821
locais onde irão ocorrer crimes.

00:07:32.221 --> 00:07:36.087
E prevemos a criminalidade individual,

00:07:36.111 --> 00:07:37.881
a criminalidade de indivíduos.

00:07:38.792 --> 00:07:42.755
A agência de notícias ProPublica
analisou recentemente

00:07:42.779 --> 00:07:44.803
um algoritmo de "risco de reincidência",

00:07:44.827 --> 00:07:45.990
como lhe chamam,

00:07:46.014 --> 00:07:49.208
que os júris usam na Flórida,
durante os julgamentos.

00:07:50.231 --> 00:07:53.816
À esquerda, temos Bernard, de cor negra,
que teve uma pontuação de 10 em 10.

00:07:54.939 --> 00:07:57.306
Dylan, à direita,
teve uma pontuação de 3 em 10.

00:07:57.346 --> 00:07:59.841
10 em 10 é risco elevado. 
3 em 10 é risco reduzido.

00:08:00.418 --> 00:08:02.803
Foram ambos a julgamento
por posse de droga.

00:08:02.827 --> 00:08:03.981
Ambos tinham cadastro,

00:08:04.005 --> 00:08:06.811
mas Dylan já tinha cometido
um assalto à mão armada

00:08:06.835 --> 00:08:08.011
e o Bernard não.

00:08:09.638 --> 00:08:12.704
Isto é importante, porque,
quanto mais alta é a pontuação,

00:08:12.728 --> 00:08:16.201
maior a probabilidade
de a sentença ser mais longa.

00:08:18.114 --> 00:08:19.408
O que está a acontecer?

00:08:20.346 --> 00:08:21.678
Lavagem de dados.

00:08:22.750 --> 00:08:27.177
É um processo em que os tecnólogos
escondem verdades muito graves

00:08:27.201 --> 00:08:29.022
dentro de algoritmos de caixa negra

00:08:29.046 --> 00:08:30.336
e chamam-lhes objetivos;

00:08:31.140 --> 00:08:32.708
chamam-lhes meritocráticos.

00:08:34.938 --> 00:08:37.323
Quando são secretos,
importantes e destrutivos

00:08:37.347 --> 00:08:39.834
eu chamo-lhes da seguinte maneira:

00:08:39.858 --> 00:08:41.857
"armas de destruição maciça".

00:08:41.881 --> 00:08:43.445
(Risos)

00:08:43.469 --> 00:08:46.523
(Aplausos)

00:08:46.547 --> 00:08:48.901
Estão por todo o lado
e não são um erro.

00:08:49.515 --> 00:08:53.238
São empresas privadas que estão
a criar algoritmos privados

00:08:53.262 --> 00:08:54.654
para objetivos privados.

00:08:55.034 --> 00:08:58.248
Mesmo os que mencionei aqui
para os professores e a polícia,

00:08:58.272 --> 00:09:00.141
foram criados por empresas privadas

00:09:00.165 --> 00:09:02.396
e vendidos a instituições governamentais.

00:09:02.420 --> 00:09:04.293
Chamam-lhes o seu "molho secreto"

00:09:04.317 --> 00:09:06.445
— é por isso que não nos podem contar.

00:09:06.469 --> 00:09:08.689
Trata-se, também, de poder privado.

00:09:09.744 --> 00:09:14.439
Estão a lucrar para dominarem
a autoridade do inescrutável.

00:09:16.934 --> 00:09:18.468
Agora, vocês podem pensar:

00:09:18.502 --> 00:09:21.050
se tudo isto é privado
e existe concorrência,

00:09:21.074 --> 00:09:23.380
talvez o mercado livre
corrija este problema.

00:09:23.404 --> 00:09:24.653
Não, não o fará.

00:09:24.677 --> 00:09:27.797
Pode fazer-se muito dinheiro
com a injustiça.

00:09:28.947 --> 00:09:32.316
Além disso, nós não somos
agentes económicos racionais.

00:09:32.851 --> 00:09:34.143
Somos todos tendenciosos.

00:09:34.780 --> 00:09:38.157
Somos racistas e intolerantes
em proporções horríveis,

00:09:38.181 --> 00:09:40.200
em proporções que nem nós sabemos.

00:09:41.172 --> 00:09:44.253
Mas sabemos que isto acontece
em níveis agregados,

00:09:44.277 --> 00:09:47.321
porque os sociólogos
têm vindo a demonstrá-lo,

00:09:47.321 --> 00:09:48.980
através de experiências,

00:09:48.980 --> 00:09:51.712
em que se enviam vários currículos
em resposta a anúncios,

00:09:51.712 --> 00:09:54.403
igualmente qualificados,
mas alguns com nomes caucasianos

00:09:54.403 --> 00:09:56.033
e outros com nomes de raça negra,

00:09:56.057 --> 00:09:58.751
e os resultados são sempre
desconcertantes... Sempre!

00:09:59.330 --> 00:10:01.101
Somos nós que somos tendenciosos

00:10:01.125 --> 00:10:04.554
e estamos a colocar
esses preconceitos nos algoritmos,

00:10:04.578 --> 00:10:06.390
ao escolhermos os dados,

00:10:06.414 --> 00:10:09.157
tal como eu decidi
em relação aos "noodles"

00:10:09.181 --> 00:10:10.806
— decidi que eram irrelevantes.

00:10:10.830 --> 00:10:16.514
Mas, ao confiarmos em dados
que têm, por base, acontecimentos passados

00:10:16.538 --> 00:10:18.552
e ao escolhermos a definição de sucesso,

00:10:18.576 --> 00:10:22.559
como é que podemos esperar
que os algoritmos saiam ilesos?

00:10:22.583 --> 00:10:24.939
Não podemos. Temos de os verificar.

00:10:25.985 --> 00:10:27.694
Temos de ver o nível de justiça.

00:10:27.718 --> 00:10:30.429
A boa notícia é que podemos fazer isso.

00:10:30.453 --> 00:10:33.805
Os algoritmos podem ser questionados

00:10:33.829 --> 00:10:35.863
e as respostas são sempre verdadeiras.

00:10:35.887 --> 00:10:38.380
Podemos corrigi-los.
Podemos torná-los melhores.

00:10:38.404 --> 00:10:40.779
Posso chamar-lhe "auditoria algorítmica"

00:10:40.803 --> 00:10:42.482
e explico-vos em que consiste.

00:10:42.506 --> 00:10:44.702
Primeiro, verificar
a integridade dos dados.

00:10:45.952 --> 00:10:48.609
Em relação ao risco de reincidência
de que já vos falei,

00:10:49.402 --> 00:10:52.975
verificar a integridade dos dados
significa que concordamos com o facto

00:10:52.999 --> 00:10:56.525
de que, nos EUA, tanto os brancos
como os negros fumam erva,

00:10:56.529 --> 00:10:59.034
mas os negros têm
maior probabilidade de ser detidos

00:10:59.058 --> 00:11:02.242
— quatro ou cinco vezes mais
probabilidades, dependendo da zona.

00:11:03.137 --> 00:11:05.963
Como é que se comporta
esta tendência, noutros crimes

00:11:05.987 --> 00:11:07.438
e como é que lidamos com isso?

00:11:07.982 --> 00:11:11.021
Segundo, devemos pensar
na definição de sucesso,

00:11:11.045 --> 00:11:12.426
rever esse conceito.

00:11:12.450 --> 00:11:15.202
Lembrem-se do algoritmo
de contratação de que já falámos.

00:11:15.206 --> 00:11:18.471
Alguém que fica na empresa
durante quatro anos e é promovido uma vez?

00:11:18.471 --> 00:11:20.184
É um trabalhador bem-sucedido,

00:11:20.208 --> 00:11:23.287
mas também é alguém
que apoia a cultura da empresa.

00:11:23.749 --> 00:11:25.799
Assim, vemos que também
é muito tendencioso.

00:11:25.799 --> 00:11:27.924
É necessário separar estas duas coisas.

00:11:27.948 --> 00:11:29.764
Tomemos como exemplo

00:11:29.788 --> 00:11:31.594
uma audição às cegas
de uma orquestra

00:11:31.618 --> 00:11:34.614
As pessoas que fazem a audição
escondem-se atrás duma cortina.

00:11:34.766 --> 00:11:36.697
O que é importante reter

00:11:36.721 --> 00:11:40.138
é que as pessoas que estão a ouvir
decidiram o que é importante

00:11:40.162 --> 00:11:42.191
e o que não é importante,

00:11:42.215 --> 00:11:44.274
e não se deixam distrair.

00:11:44.781 --> 00:11:47.530
Quando as audições às cegas começaram,

00:11:47.554 --> 00:11:50.998
o número de mulheres em orquestras
aumentou cinco vezes.

00:11:52.073 --> 00:11:54.088
Em seguida, temos de considerar
a precisão.

00:11:55.053 --> 00:11:58.787
É aqui que falharia o "modelo de valor
acrescentado" dos professores.

00:11:59.398 --> 00:12:01.560
Claro que nenhum algoritmo é perfeito,

00:12:02.440 --> 00:12:06.045
é por isso que temos de considerar
os erros de cada um.

00:12:06.656 --> 00:12:11.015
Com que frequência existem erros
e com quem é que este modelo falha?

00:12:11.670 --> 00:12:13.388
Qual é o custo desta falha?

00:12:14.254 --> 00:12:16.461
Por último, temos de considerar

00:12:17.793 --> 00:12:19.979
os efeitos a longo prazo dos algoritmos,

00:12:20.686 --> 00:12:22.893
o "feedback" que está programado.

00:12:23.406 --> 00:12:24.642
Isto parece abstrato,

00:12:24.666 --> 00:12:27.330
mas imaginem se os engenheiros
do Facebook consideravam

00:12:28.090 --> 00:12:32.945
mostrar-nos apenas
o que os nossos amigos publicam.

00:12:33.581 --> 00:12:36.815
Tenho mais duas mensagens,
uma delas para os cientistas de dados.

00:12:37.270 --> 00:12:40.679
Cientistas de dados: nós não
devemos ser os árbitros da verdade.

00:12:41.340 --> 00:12:44.273
Devemos ser tradutores
de discussões éticas

00:12:44.297 --> 00:12:46.701
que acontecem em sociedades mais amplas.

00:12:47.399 --> 00:12:49.532
(Aplausos)

00:12:49.556 --> 00:12:51.112
E aos restantes,

00:12:51.661 --> 00:12:53.437
aos que não são cientistas de dados:

00:12:53.471 --> 00:12:55.189
isto não é um teste matemático.

00:12:55.452 --> 00:12:56.990
É uma luta política.

00:12:58.407 --> 00:13:02.314
Precisamos de exigir a responsabilização
dos soberanos dos nossos algoritmos.

00:13:03.938 --> 00:13:05.437
(Aplausos)

00:13:05.461 --> 00:13:09.686
A era da fé cega nos "big data"
tem de acabar.

00:13:09.710 --> 00:13:10.877
Muito obrigada.

00:13:10.901 --> 00:13:16.204
(Aplausos)

