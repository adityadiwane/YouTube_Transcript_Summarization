WEBVTT
Kind: captions
Language: ja

00:00:00.000 --> 00:00:07.000
翻訳: Kazunori Akashi
校正: Yasushi Aoki

00:00:12.795 --> 00:00:14.681
アルゴリズムは どこにでもあります

00:00:15.931 --> 00:00:19.286
アルゴリズムが勝者と敗者を分けます

00:00:19.839 --> 00:00:21.987
勝者は仕事を手に入れ

00:00:21.987 --> 00:00:23.900
有利なクレジットカードを
申し込めます

00:00:23.900 --> 00:00:26.925
一方 敗者は就職面接すら受けられず

00:00:27.410 --> 00:00:29.557
保険料は より高くなります

00:00:30.017 --> 00:00:33.766
私たちは 理解できない上に
不服申し立ての機会もない—

00:00:34.495 --> 00:00:37.822
秘密の数式によって
格付けされているのです

00:00:39.000 --> 00:00:40.356
そこで疑問が湧いてきます

00:00:40.380 --> 00:00:43.293
もしアルゴリズムが間違っていたら？

00:00:44.810 --> 00:00:46.544
アルゴリズムを作る時
必要なものが２つあります

00:00:46.544 --> 00:00:48.889
データ つまり過去の出来事の記録と

00:00:48.889 --> 00:00:50.580
人が追い求める「成功」を
定義する基準です

00:00:50.580 --> 00:00:52.985
人が追い求める「成功」を
定義する基準です

00:00:52.985 --> 00:00:57.876
そして観察と理解を通して
アルゴリズムを訓練します

00:00:57.876 --> 00:01:01.409
アルゴリズムに 成功と関係する要素を
理解させるためです

00:01:01.409 --> 00:01:04.372
どんな状況が 成功に繋がるのでしょう？

00:01:04.621 --> 00:01:06.463
実は アルゴリズムは
誰でも使っています

00:01:06.487 --> 00:01:09.049
プログラムに書かないだけです

00:01:09.049 --> 00:01:10.391
１つ例を挙げましょう

00:01:10.391 --> 00:01:13.841
私は毎日アルゴリズムを使って
家族の食事を用意します

00:01:13.841 --> 00:01:16.094
私が利用するデータは

00:01:16.094 --> 00:01:17.873
台所にどんな材料があるか

00:01:17.873 --> 00:01:19.418
どれだけ時間をかけられるか

00:01:19.418 --> 00:01:20.731
どれだけ料理に凝るかで

00:01:20.731 --> 00:01:22.434
私はそのデータをまとめます

00:01:22.438 --> 00:01:26.689
ちなみにインスタントラーメンは
食べ物とは認めません

00:01:26.703 --> 00:01:28.582
（笑）

00:01:28.606 --> 00:01:30.451
私にとって成功の基準は

00:01:30.475 --> 00:01:33.374
子供たちが野菜を食べることです

00:01:33.921 --> 00:01:36.509
もし下の息子が決めるなら
基準はガラッと変わり

00:01:36.509 --> 00:01:39.867
「いっぱいチョコナッツクリームを
食べられれば成功」と言うでしょう

00:01:40.999 --> 00:01:43.225
でも基準を決めるのは私です

00:01:43.249 --> 00:01:45.956
責任者は私で
私の意見が重要なんですから

00:01:45.980 --> 00:01:48.655
これがアルゴリズムの第１のルールです

00:01:48.679 --> 00:01:52.119
アルゴリズムとはプログラムに
埋め込まれた意見なのです

00:01:53.302 --> 00:01:57.045
これは ほとんどの人が持つ
アルゴリズムのイメージとはかけ離れています

00:01:57.069 --> 00:02:01.987
人々はアルゴリズムが客観的で正しく
科学的なものと思っていますが

00:02:01.987 --> 00:02:04.426
それはマーケティング上のトリックです

00:02:05.089 --> 00:02:07.214
アルゴリズムで人を怯ませるのも

00:02:07.238 --> 00:02:10.306
マーケティングのトリックですし

00:02:10.306 --> 00:02:14.041
アルゴリズムを信用させたり
恐れさせたりするのもそう

00:02:14.041 --> 00:02:16.849
皆 数学を恐れつつ信用していますから

00:02:17.387 --> 00:02:22.547
ビッグデータを盲信すると
いろいろな問題が生じかねません

00:02:23.424 --> 00:02:26.877
彼女はキリ・ソアーズ
ブルックリンの高校で校長をしています

00:02:26.901 --> 00:02:29.487
2011年に彼女が教えてくれたのですが
彼女の学校では

00:02:29.511 --> 00:02:32.238
「付加価値モデル」という
複雑な秘密のアルゴリズムで

00:02:32.242 --> 00:02:34.321
教員が評価されている
ということでした

00:02:34.325 --> 00:02:36.931
私は こう伝えました
「数式を調べてみましょう

00:02:36.931 --> 00:02:38.976
見せてくれれば説明しますよ」

00:02:38.976 --> 00:02:41.081
すると彼女は
「数式を入手しようとしたら

00:02:41.081 --> 00:02:43.993
市教育局の担当者に『これは数学ですよ
理解できないでしょう』と

00:02:43.993 --> 00:02:45.583
言われたんです」

00:02:47.086 --> 00:02:48.424
事態はさらに深刻化します

00:02:48.448 --> 00:02:51.978
ニューヨーク・ポスト紙が
情報自由法に基づく開示請求をして

00:02:52.002 --> 00:02:54.961
ニューヨーク市の全教員の
名前とスコアを手に入れ

00:02:54.985 --> 00:02:58.327
教員を辱めるような
データを公表しました

00:02:58.904 --> 00:03:02.728
一方 私がソース・コードを
同じ方法で手に入れようとしたところ

00:03:02.728 --> 00:03:04.937
無理だと言われました

00:03:04.961 --> 00:03:06.197
却下されたのです

00:03:06.221 --> 00:03:07.395
後にわかったことですが

00:03:07.419 --> 00:03:10.259
ニューヨーク市で その数式を
見られる人は誰もおらず

00:03:10.259 --> 00:03:12.304
誰も理解していなかったのです

00:03:13.689 --> 00:03:16.917
その後 ゲイリー・ルービンスタインという
頭のキレる人物が登場します

00:03:16.917 --> 00:03:18.952
彼はニューヨーク・ポスト紙のデータから

00:03:18.952 --> 00:03:22.452
２種類のスコアを持っている
教員665名を見つけ出しました

00:03:22.452 --> 00:03:24.413
それに該当するのは 例えば

00:03:24.437 --> 00:03:26.840
数学を７年生と８年生で
教えている場合です

00:03:26.840 --> 00:03:28.548
彼は２種類のスコアを散布図にしました

00:03:28.548 --> 00:03:30.795
点はそれぞれ 先生を表します

00:03:30.884 --> 00:03:33.303
（笑）

00:03:33.327 --> 00:03:35.138
これは どういうことでしょう？

00:03:35.138 --> 00:03:36.149
（笑）

00:03:36.173 --> 00:03:39.563
こんなものを教員の個人評価に
使ってはいけません

00:03:39.563 --> 00:03:41.739
まるで乱数発生器じゃないですか

00:03:41.739 --> 00:03:44.313
（拍手）

00:03:44.313 --> 00:03:45.689
でも実際に使われたんです

00:03:45.689 --> 00:03:46.829
彼女はサラ・ワイサキ

00:03:46.829 --> 00:03:50.044
他の205人のワシントンD.C.学区の
先生たちと共に

00:03:50.044 --> 00:03:51.704
解雇されました

00:03:51.704 --> 00:03:54.727
校長や保護者からの評価は
非常に高かったのにです

00:03:54.727 --> 00:03:56.385
校長や保護者からの評価は
非常に高かったのにです

00:03:57.210 --> 00:03:59.186
皆さんが今 考えていることは
わかります

00:03:59.186 --> 00:04:01.833
特にデータサイエンティストや
AIの専門家なら思うでしょう

00:04:01.833 --> 00:04:06.153
「自分なら そんなデタラメな
アルゴリズムは作らない」って

00:04:06.603 --> 00:04:08.416
でもアルゴリズムは誤ることもあれば

00:04:08.416 --> 00:04:13.378
善意に基づいていても
破壊的な影響を及ぼすことだってあります

00:04:14.351 --> 00:04:16.694
飛行機なら 設計がまずければ

00:04:16.694 --> 00:04:18.755
墜落しますし
その様子が見えますが

00:04:18.779 --> 00:04:21.149
アルゴリズムだと設計がまずくても

00:04:21.995 --> 00:04:26.640
長期間に渡って 音もなく
大惨事をもたらし続けかねないんです

00:04:27.568 --> 00:04:29.398
彼はロジャー・エイルズ

00:04:29.398 --> 00:04:31.162
（笑）

00:04:32.344 --> 00:04:34.962
1996年にFOXニュースを創設しました

00:04:35.256 --> 00:04:37.837
20人以上の女性が
セクハラ被害を訴えました

00:04:37.861 --> 00:04:41.060
またキャリアアップを
妨害されたそうです

00:04:41.060 --> 00:04:43.640
彼自身は2016年に地位を追われましたが

00:04:43.664 --> 00:04:46.774
最近のニュースにある通り
問題は依然残っています

00:04:47.474 --> 00:04:48.874
ここで疑問が湧いてきます

00:04:48.898 --> 00:04:52.042
再起をはかるために
FOXニュースは何をすべきか？

00:04:53.065 --> 00:04:56.106
人材採用プロセスを
機械学習アルゴリズムに

00:04:56.130 --> 00:04:57.784
替えるのはどうでしょう？

00:04:57.808 --> 00:04:59.403
いいアイデアでしょう？

00:04:59.427 --> 00:05:00.727
検討してみましょう

00:05:00.751 --> 00:05:02.856
まずデータには
何が使えるでしょう？

00:05:02.880 --> 00:05:07.827
過去21年間に FOXニュースに送られた
履歴書がいいでしょう

00:05:07.851 --> 00:05:09.353
妥当なデータです

00:05:09.377 --> 00:05:11.315
では成功の基準は？

00:05:11.741 --> 00:05:13.065
妥当な基準は…

00:05:13.089 --> 00:05:14.867
どんな人がFOXニュースで
成功するんでしょう？

00:05:14.891 --> 00:05:18.471
例えば ４年在職して
最低１回は昇進していれば

00:05:18.495 --> 00:05:20.469
成功と言えそうです

00:05:20.636 --> 00:05:22.197
妥当な基準です

00:05:22.221 --> 00:05:24.575
それをアルゴリズムに学習させます

00:05:24.599 --> 00:05:28.796
人々を探って
何が成功につながるか—

00:05:29.039 --> 00:05:32.541
これまで どんな履歴書が
成功に繋がってきたのかを

00:05:32.541 --> 00:05:35.425
この基準に従って学習させるのです

00:05:36.020 --> 00:05:37.649
さて このアルゴリズムを

00:05:37.649 --> 00:05:40.534
現在の就職希望者に
当てはめると どうなるでしょう？

00:05:40.939 --> 00:05:43.138
まず女性は除外されるでしょう

00:05:43.483 --> 00:05:47.413
過去に成功してきたようには
見えないからです

00:05:51.462 --> 00:05:54.109
配慮もなく やみくもに
アルゴリズムを適用しても

00:05:54.133 --> 00:05:56.801
物事は公平にはならないんです

00:05:56.801 --> 00:05:58.333
アルゴリズムは公平を生みません

00:05:58.357 --> 00:06:00.485
過去の行為や行動パターンを

00:06:00.509 --> 00:06:01.692
繰り返し

00:06:01.716 --> 00:06:04.035
自動的に現状を維持するだけです

00:06:04.538 --> 00:06:07.137
この世界が完璧なら
それでいいんでしょうが

00:06:07.725 --> 00:06:09.037
そうではありません

00:06:09.061 --> 00:06:13.733
さらに付け加えると ほとんどの企業は
みっともない裁判を抱えている訳ではありませんが

00:06:14.266 --> 00:06:16.748
こういった企業にいる
データサイエンティストは

00:06:16.748 --> 00:06:18.671
正確性に焦点を当て

00:06:18.671 --> 00:06:21.504
データに従うよう指示されています

00:06:22.093 --> 00:06:23.474
その意味を考えてみましょう

00:06:23.498 --> 00:06:25.349
誰でもバイアスを持っているので

00:06:25.349 --> 00:06:30.175
アルゴリズムに性差別や その他の偏見が
コード化されている可能性があります

00:06:31.308 --> 00:06:32.729
思考実験をしてみましょう

00:06:32.753 --> 00:06:34.502
私は思考実験が好きなので

00:06:35.394 --> 00:06:38.869
人種を完全に隔離した
社会があるとします

00:06:40.067 --> 00:06:43.395
どの街でも どの地域でも
人種は隔離され

00:06:43.419 --> 00:06:45.800
犯罪を見つけるために
警察を送り込むのは

00:06:45.800 --> 00:06:48.193
マイノリティーが住む地域だけです

00:06:48.211 --> 00:06:51.270
すると逮捕者のデータは
かなり偏ったものになるでしょう

00:06:51.671 --> 00:06:54.246
さらに データサイエンティストを
探してきて

00:06:54.270 --> 00:06:58.731
報酬を払い 次の犯罪が起こる場所を
予測させたらどうなるでしょう？

00:06:59.015 --> 00:07:00.812
マイノリティーの地域になります

00:07:01.105 --> 00:07:04.640
あるいは 次に犯罪を犯しそうな人を
予測させたら？

00:07:04.708 --> 00:07:06.443
マイノリティーでしょうね

00:07:07.769 --> 00:07:11.214
データサイエンティストは
モデルの素晴らしさと正確さを

00:07:11.214 --> 00:07:12.631
自慢するでしょうし

00:07:12.655 --> 00:07:14.384
確かにその通りでしょう

00:07:15.771 --> 00:07:19.430
さて 現実はそこまで極端ではありませんが

00:07:19.430 --> 00:07:21.697
実際に多くの市や町で
深刻な人種差別があり

00:07:21.721 --> 00:07:24.634
警察の活動や司法制度のデータが
偏っているという

00:07:24.634 --> 00:07:26.626
証拠が揃っています

00:07:27.452 --> 00:07:30.267
実際にホットスポットと呼ばれる
犯罪多発地域を

00:07:30.291 --> 00:07:32.151
予測しています

00:07:32.221 --> 00:07:35.611
さらには個々人の犯罪傾向を

00:07:35.611 --> 00:07:38.041
実際に予測しています

00:07:38.792 --> 00:07:42.755
報道組織プロパブリカが最近
いわゆる「再犯リスク」アルゴリズムの

00:07:42.779 --> 00:07:44.777
１つを取り上げ調査しました

00:07:44.777 --> 00:07:45.890
１つを取り上げ調査しました

00:07:45.890 --> 00:07:49.788
フロリダ州で 判事による
量刑手続に使われているものです

00:07:50.231 --> 00:07:54.106
左側の黒人男性バーナードのスコアは
10点満点の10点で

00:07:54.999 --> 00:07:57.006
右の白人ディランは３点でした

00:07:57.030 --> 00:08:00.041
10点中10点はハイリスクで
3点はローリスクです

00:08:00.418 --> 00:08:02.707
２人とも麻薬所持で逮捕され

00:08:02.707 --> 00:08:04.131
どちらも前科はありましたが

00:08:04.131 --> 00:08:06.515
３点のディランには重罪の前科があり

00:08:06.515 --> 00:08:09.021
10点のバーナードにはありませんでした

00:08:09.638 --> 00:08:12.704
これが重要な理由は
スコアが高ければ高いほど

00:08:12.728 --> 00:08:16.201
刑期が長くなる
傾向があるからです

00:08:18.054 --> 00:08:19.708
どうなっているのでしょう？

00:08:20.346 --> 00:08:22.298
これは「データ・ロンダリング」です

00:08:22.750 --> 00:08:27.177
このプロセスを通して 技術者が
ブラックボックスのようなアルゴリズムの内部に

00:08:27.201 --> 00:08:29.022
醜い現実を隠し

00:08:29.046 --> 00:08:31.030
「客観的」とか

00:08:31.030 --> 00:08:33.358
「能力主義」と称しているんです

00:08:34.938 --> 00:08:37.623
秘密にされている
重要で破壊的なアルゴリズムを

00:08:37.623 --> 00:08:39.834
私はこんな名前で呼んでいます

00:08:39.858 --> 00:08:41.857
「大量破壊数学」です

00:08:41.881 --> 00:08:43.445
（笑）

00:08:43.469 --> 00:08:46.523
（拍手）

00:08:46.547 --> 00:08:49.475
それは間違いなく
どこにでも存在します

00:08:49.475 --> 00:08:53.238
民間企業が 私的なアルゴリズムを
私的な目的で

00:08:53.262 --> 00:08:54.834
作っているんです

00:08:55.034 --> 00:08:58.172
先程お話しした
教員や警察向けのアルゴリズムでさえ

00:08:58.172 --> 00:09:00.025
民間企業が制作し

00:09:00.025 --> 00:09:02.396
政府機関に販売したものです

00:09:02.420 --> 00:09:04.293
アルゴリズムは
「秘伝のタレ」だから

00:09:04.317 --> 00:09:06.445
公開できないと
企業側は主張します

00:09:06.469 --> 00:09:09.259
また アルゴリズムは私的な権力です

00:09:09.744 --> 00:09:14.939
この謎めいた存在が持つ権威を振りかざして
企業は利益を得ています

00:09:16.864 --> 00:09:19.868
ただ こう思うかもしれません
アルゴリズムが民間のものなら

00:09:19.892 --> 00:09:21.050
競争があるので

00:09:21.074 --> 00:09:23.380
自由市場の力が
問題を解決するのではないか…

00:09:23.404 --> 00:09:24.653
でも そうはいきません

00:09:24.677 --> 00:09:27.797
不公平は大きな利益を
生み出しますから

00:09:28.947 --> 00:09:32.846
それに我々人間は
合理的経済人ではなく

00:09:32.851 --> 00:09:34.433
誰もがバイアスを持っています

00:09:34.780 --> 00:09:38.011
私たちは 自分が望みも
気づきもしない形で

00:09:38.011 --> 00:09:40.710
差別や偏見を持っているのです

00:09:41.172 --> 00:09:44.253
全体を俯瞰して見ると
そのことがわかります

00:09:44.277 --> 00:09:47.051
なぜなら社会学者が
考案した実験を通して

00:09:47.051 --> 00:09:49.186
一貫して実証されてきたからです

00:09:49.210 --> 00:09:51.778
その実験では研究者が
履歴書を大量に送付しました

00:09:51.802 --> 00:09:54.147
同じように資格は満たしていますが
一部は白人っぽい名前で

00:09:54.147 --> 00:09:55.777
一部は黒人っぽい名前

00:09:55.777 --> 00:09:58.951
そして結果は
常にがっかりするものでした

00:09:59.330 --> 00:10:01.101
つまりバイアスがあるのは私たちで

00:10:01.125 --> 00:10:03.318
どんなデータを集め選ぶかによって

00:10:03.318 --> 00:10:06.390
そのバイアスをアルゴリズムに
注入しているんです

00:10:06.414 --> 00:10:09.131
これは私がインスタントラーメンを
含めないのと同じで

00:10:09.131 --> 00:10:10.926
不適切だと決めたのは
私なんです

00:10:10.926 --> 00:10:16.348
しかし実際に過去の行動を元にした
データを信頼し

00:10:16.348 --> 00:10:18.552
成功の基準を恣意的に選びながら

00:10:18.576 --> 00:10:22.559
どうして欠陥のないアルゴリズムを
期待できるのでしょう？

00:10:22.583 --> 00:10:25.289
それは無理です
チェックが必要なんです

00:10:25.865 --> 00:10:27.694
公平性を確かめる必要があるんです

00:10:27.718 --> 00:10:30.223
幸い公正性は確認できます

00:10:30.223 --> 00:10:33.599
アルゴリズムに問いただせば

00:10:33.599 --> 00:10:35.863
常に本当のことしか
答えないので

00:10:35.887 --> 00:10:38.380
修正を加え より良いものに
作り替えられます

00:10:38.404 --> 00:10:40.779
私は これを
アルゴリズム監査と呼んでいます

00:10:40.803 --> 00:10:42.482
その手順を説明しましょう

00:10:42.506 --> 00:10:44.892
まずはデータ完全性チェックです

00:10:45.952 --> 00:10:48.609
先ほど登場した
再犯リスク・アルゴリズムの場合—

00:10:49.402 --> 00:10:52.975
データ完全性チェックとは
事実を直視するという意味になるでしょう

00:10:52.999 --> 00:10:56.525
例えばアメリカでは 大麻の使用率は
白人と黒人で同じなのに

00:10:56.549 --> 00:10:59.034
逮捕される割合は
黒人の方がはるかに高く

00:10:59.058 --> 00:11:02.242
地域によっては
４〜５倍になるという事実があります

00:11:03.137 --> 00:11:05.927
このようなバイアスは
他の犯罪では どんな形で表れ

00:11:05.927 --> 00:11:07.938
私たちは それを
どう説明したらいいでしょうか？

00:11:07.942 --> 00:11:11.021
次に 私たちは成功の基準について
考えなければなりません

00:11:11.045 --> 00:11:12.426
その基準を監査するのです

00:11:12.450 --> 00:11:15.026
採用アルゴリズムを
思い出してください

00:11:15.026 --> 00:11:18.335
勤続年数が４年で
昇進１回の人はどうだったでしょう

00:11:18.335 --> 00:11:20.184
その人は成功した社員でしょうが

00:11:20.208 --> 00:11:23.467
同時に その会社の文化に
支持されたとも言えます

00:11:23.839 --> 00:11:25.835
ただ その文化に
バイアスがあるかもしれないので

00:11:25.859 --> 00:11:27.924
この２つは分けて考える必要があります

00:11:27.948 --> 00:11:30.374
一つの例として オーケストラの
ブラインド・オーディションを見るべきでしょう

00:11:30.398 --> 00:11:31.594
一つの例として オーケストラの
ブラインド・オーディションを見るべきでしょう

00:11:31.618 --> 00:11:34.684
オーディションを受ける人は
衝立の向こうにいます

00:11:34.766 --> 00:11:36.697
ここで注目したいのは

00:11:36.721 --> 00:11:40.082
審査員は 何が重要で
何が重要でないかを

00:11:40.082 --> 00:11:42.045
あらかじめ決めて

00:11:42.045 --> 00:11:44.754
重要でないものに
惑わされないようにしている点です

00:11:44.761 --> 00:11:47.530
ブラインド・オーディションを
するようになって

00:11:47.554 --> 00:11:51.318
女性がオーケストラに占める割合は
５倍に増えました

00:11:52.073 --> 00:11:54.478
次に正確性を吟味しなければなりません

00:11:55.053 --> 00:11:59.107
教員向けの付加価値モデルなら
すぐ落第になる項目です

00:11:59.398 --> 00:12:02.220
当然 完璧なアルゴリズムなどないので

00:12:02.440 --> 00:12:06.345
あらゆるアルゴリズムの
誤りを検討する必要があります

00:12:06.656 --> 00:12:11.585
誤りを起こす頻度は？
どんな相手だと そのモデルは機能しないのか？

00:12:11.670 --> 00:12:14.138
失敗した時の損失規模は？

00:12:14.254 --> 00:12:16.711
そして最後に考えなければならないのは

00:12:17.793 --> 00:12:20.689
アルゴリズムの長期的影響 つまり

00:12:20.689 --> 00:12:23.166
それによって生じる
フィードバック・ループです

00:12:23.166 --> 00:12:25.312
抽象的な話に
聞こえるかもしれませんが

00:12:25.312 --> 00:12:27.900
もしFacebookのエンジニアが
友人の投稿だけを表示する前に

00:12:27.900 --> 00:12:33.165
フィードバック・ループの影響を
考慮していたらと考えてみてください

00:12:33.581 --> 00:12:37.125
伝えたいことは あと２つ
１つはデータサイエンティストに向けたものです

00:12:37.270 --> 00:12:40.999
私たちデータサイエンティストが
真実を決めるべきではありません

00:12:41.340 --> 00:12:44.877
私たちは もっと広い社会に生じる
倫理的な議論を

00:12:44.877 --> 00:12:46.931
解釈する存在であるべきです

00:12:47.399 --> 00:12:49.532
（拍手）

00:12:49.556 --> 00:12:51.842
そしてデータサイエンティスト以外の
皆さん—

00:12:51.842 --> 00:12:53.227
そしてデータサイエンティスト以外の
皆さん—

00:12:53.251 --> 00:12:55.439
この状況は数学のテストではなく

00:12:55.452 --> 00:12:57.580
政治闘争なのです

00:12:58.407 --> 00:13:02.754
専制君主のようなアルゴリズムに対して
私たちは説明を求める必要があります

00:13:03.938 --> 00:13:05.437
（拍手）

00:13:05.461 --> 00:13:09.686
ビッグデータを盲信する時代は
終わらせるべきです

00:13:09.710 --> 00:13:10.877
ありがとうございました

00:13:10.901 --> 00:13:16.204
（拍手）

