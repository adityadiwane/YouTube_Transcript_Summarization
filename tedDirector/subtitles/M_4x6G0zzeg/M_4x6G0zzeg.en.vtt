WEBVTT
Kind: captions
Language: en

00:00:12.540 --> 00:00:16.820
People have been using media
to talk about sex for a long time.

00:00:17.420 --> 00:00:20.860
Love letters, phone sex, racy Polaroids.

00:00:21.300 --> 00:00:27.356
There's even a story of a girl who eloped
with a man that she met over the telegraph

00:00:27.380 --> 00:00:29.020
in 1886.

00:00:30.380 --> 00:00:35.476
Today we have sexting,
and I am a sexting expert.

00:00:35.500 --> 00:00:37.380
Not an expert sexter.

00:00:38.620 --> 00:00:42.796
Though, I do know what this means --
I think you do too.

00:00:42.820 --> 00:00:44.195
[it's a penis]

00:00:44.220 --> 00:00:46.580
(Laughter)

00:00:48.180 --> 00:00:54.516
I have been studying sexting since
the media attention to it began in 2008.

00:00:54.540 --> 00:00:57.516
I wrote a book on the moral
panic about sexting.

00:00:57.540 --> 00:00:59.156
And here's what I found:

00:00:59.180 --> 00:01:02.396
most people are worrying
about the wrong thing.

00:01:02.420 --> 00:01:06.596
They're trying to just prevent
sexting from happening entirely.

00:01:06.620 --> 00:01:08.156
But let me ask you this:

00:01:08.180 --> 00:01:13.036
As long as it's completely consensual,
what's the problem with sexting?

00:01:13.060 --> 00:01:17.076
People are into all sorts of things
that you may not be into,

00:01:17.100 --> 00:01:19.396
like blue cheese or cilantro.

00:01:19.420 --> 00:01:21.060
(Laughter)

00:01:22.420 --> 00:01:26.556
Sexting is certainly risky,
like anything that's fun,

00:01:26.580 --> 00:01:33.276
but as long as you're not sending an image
to someone who doesn't want to receive it,

00:01:33.300 --> 00:01:35.076
there's no harm.

00:01:35.100 --> 00:01:37.716
What I do think is a serious problem

00:01:37.740 --> 00:01:40.996
is when people share
private images of others

00:01:41.020 --> 00:01:42.540
without their permission.

00:01:43.180 --> 00:01:45.516
And instead of worrying about sexting,

00:01:45.540 --> 00:01:50.100
what I think we need to do
is think a lot more about digital privacy.

00:01:50.700 --> 00:01:52.900
The key is consent.

00:01:53.500 --> 00:01:56.796
Right now most people
are thinking about sexting

00:01:56.820 --> 00:01:59.780
without really thinking
about consent at all.

00:02:00.220 --> 00:02:03.900
Did you know that we currently
criminalize teen sexting?

00:02:05.220 --> 00:02:08.676
It can be a crime because
it counts as child pornography,

00:02:08.700 --> 00:02:11.556
if there's an image of someone under 18,

00:02:11.580 --> 00:02:12.916
and it doesn't even matter

00:02:12.940 --> 00:02:17.140
if they took that image of themselves
and shared it willingly.

00:02:17.620 --> 00:02:20.596
So we end up with this
bizarre legal situation

00:02:20.620 --> 00:02:25.156
where two 17-year-olds
can legally have sex in most US states

00:02:25.180 --> 00:02:27.060
but they can't photograph it.

00:02:28.380 --> 00:02:32.636
Some states have also tried
passing sexting misdemeanor laws

00:02:32.660 --> 00:02:35.676
but these laws repeat the same problem

00:02:35.700 --> 00:02:39.460
because they still
make consensual sexting illegal.

00:02:40.340 --> 00:02:41.596
It doesn't make sense

00:02:41.620 --> 00:02:46.356
to try to ban all sexting
to try to address privacy violations.

00:02:46.380 --> 00:02:47.876
This is kind of like saying,

00:02:47.900 --> 00:02:53.340
let's solve the problem of date rape
by just making dating completely illegal.

00:02:54.940 --> 00:03:00.316
Most teens don't get arrested for sexting,
but can you guess who does?

00:03:00.340 --> 00:03:05.316
It's often teens who are disliked
by their partner's parents.

00:03:05.340 --> 00:03:10.140
And this can be because of class bias,
racism or homophobia.

00:03:10.780 --> 00:03:13.556
Most prosecutors are,
of course, smart enough

00:03:13.580 --> 00:03:19.060
not to use child pornography charges
against teenagers, but some do.

00:03:19.540 --> 00:03:23.196
According to researchers
at the University of New Hampshire

00:03:23.220 --> 00:03:28.876
seven percent of all child pornography
possession arrests are teens,

00:03:28.900 --> 00:03:31.900
sexting consensually with other teens.

00:03:33.300 --> 00:03:35.836
Child pornography is a serious crime,

00:03:35.860 --> 00:03:39.540
but it's just not
the same thing as teen sexting.

00:03:40.860 --> 00:03:44.276
Parents and educators
are also responding to sexting

00:03:44.300 --> 00:03:47.436
without really thinking
too much about consent.

00:03:47.460 --> 00:03:51.580
Their message to teens is often:
just don't do it.

00:03:52.020 --> 00:03:55.516
And I totally get it --
there are serious legal risks

00:03:55.540 --> 00:03:59.196
and of course, that potential
for privacy violations.

00:03:59.220 --> 00:04:00.476
And when you were a teen,

00:04:00.500 --> 00:04:03.900
I'm sure you did exactly
as you were told, right?

00:04:05.260 --> 00:04:08.716
You're probably thinking,
my kid would never sext.

00:04:08.740 --> 00:04:12.196
And that's true, your little angel
may not be sexting

00:04:12.220 --> 00:04:15.356
because only 33 percent

00:04:15.380 --> 00:04:17.740
of 16- and 17-year-olds are sexting.

00:04:19.020 --> 00:04:23.636
But, sorry, by the time they're older,
odds are they will be sexting.

00:04:23.660 --> 00:04:29.860
Every study I've seen puts the rate
above 50 percent for 18- to 24-year-olds.

00:04:30.540 --> 00:04:33.596
And most of the time, nothing goes wrong.

00:04:33.620 --> 00:04:38.996
People ask me all the time things like,
isn't sexting just so dangerous, though?

00:04:39.020 --> 00:04:42.596
It's like you wouldn't
leave your wallet on a park bench

00:04:42.620 --> 00:04:46.060
and you expect it's going to get stolen
if you do that, right?

00:04:46.700 --> 00:04:48.156
Here's how I think about it:

00:04:48.180 --> 00:04:52.116
sexting is like leaving your wallet
at your boyfriend's house.

00:04:52.140 --> 00:04:53.916
If you come back the next day

00:04:53.940 --> 00:04:56.220
and all the money is just gone,

00:04:56.860 --> 00:04:58.980
you really need to dump that guy.

00:04:59.510 --> 00:05:01.680
(Laughter)

00:05:03.180 --> 00:05:05.500
So instead of criminalizing sexting

00:05:05.540 --> 00:05:08.156
to try to prevent
these privacy violations,

00:05:08.180 --> 00:05:11.476
instead we need to make consent central

00:05:11.500 --> 00:05:15.580
to how we think about the circulation
of our private information.

00:05:16.300 --> 00:05:20.556
Every new media technology
raises privacy concerns.

00:05:20.580 --> 00:05:25.196
In fact, in the US the very first
major debates about privacy

00:05:25.220 --> 00:05:29.716
were in response to technologies
that were relatively new at the time.

00:05:29.740 --> 00:05:33.636
In the late 1800s,
people were worried about cameras,

00:05:33.660 --> 00:05:37.116
which were just suddenly
more portable than ever before,

00:05:37.140 --> 00:05:39.636
and newspaper gossip columns.

00:05:39.660 --> 00:05:43.476
They were worried that the camera
would capture information about them,

00:05:43.500 --> 00:05:46.700
take it out of context
and widely disseminate it.

00:05:47.060 --> 00:05:48.676
Does this sound familiar?

00:05:48.700 --> 00:05:53.556
It's exactly what we're worrying about
now with social media and drone cameras,

00:05:53.580 --> 00:05:55.220
and, of course, sexting.

00:05:55.740 --> 00:05:57.956
And these fears about technology,

00:05:57.980 --> 00:05:59.196
they make sense

00:05:59.220 --> 00:06:02.636
because technologies
can amplify and bring out

00:06:02.660 --> 00:06:05.380
our worst qualities and behaviors.

00:06:05.980 --> 00:06:08.356
But there are solutions.

00:06:08.380 --> 00:06:11.940
And we've been here before
with a dangerous new technology.

00:06:12.540 --> 00:06:16.316
In 1908, Ford introduced the Model T car.

00:06:16.340 --> 00:06:18.916
Traffic fatality rates were rising.

00:06:18.940 --> 00:06:21.740
It was a serious problem --
it looks so safe, right?

00:06:23.900 --> 00:06:27.876
Our first response
was to try to change drivers' behavior,

00:06:27.900 --> 00:06:31.620
so we developed speed limits
and enforced them through fines.

00:06:32.060 --> 00:06:33.916
But over the following decades,

00:06:33.940 --> 00:06:39.436
we started to realize the technology
of the car itself is not just neutral.

00:06:39.460 --> 00:06:42.676
We could design the car to make it safer.

00:06:42.700 --> 00:06:46.156
So in the 1920s, we got
shatter-resistant windshields.

00:06:46.180 --> 00:06:48.676
In the 1950s, seat belts.

00:06:48.700 --> 00:06:51.780
And in the 1990s, airbags.

00:06:52.260 --> 00:06:54.636
All three of these areas:

00:06:54.660 --> 00:06:59.436
laws, individuals and industry
came together over time

00:06:59.460 --> 00:07:03.236
to help solve the problem
that a new technology causes.

00:07:03.260 --> 00:07:06.500
And we can do the same thing
with digital privacy.

00:07:06.980 --> 00:07:09.740
Of course, it comes back to consent.

00:07:10.180 --> 00:07:11.396
Here's the idea.

00:07:11.420 --> 00:07:15.236
Before anyone can distribute
your private information,

00:07:15.260 --> 00:07:17.500
they should have to get your permission.

00:07:18.060 --> 00:07:22.876
This idea of affirmative consent
comes from anti-rape activists

00:07:22.900 --> 00:07:26.676
who tell us that we need consent
for every sexual act.

00:07:26.700 --> 00:07:31.276
And we have really high standards
for consent in a lot of other areas.

00:07:31.300 --> 00:07:33.156
Think about having surgery.

00:07:33.180 --> 00:07:34.796
Your doctor has to make sure

00:07:34.820 --> 00:07:38.860
that you are meaningfully and knowingly
consenting to that medical procedure.

00:07:39.340 --> 00:07:43.036
This is not the type of consent
like with an iTunes Terms of Service

00:07:43.060 --> 00:07:46.716
where you just scroll to the bottom
and you're like, agree, agree, whatever.

00:07:46.740 --> 00:07:48.460
(Laughter)

00:07:48.980 --> 00:07:54.236
If we think more about consent,
we can have better privacy laws.

00:07:54.260 --> 00:07:57.676
Right now, we just don't have
that many protections.

00:07:57.700 --> 00:08:01.276
If your ex-husband or your ex-wife
is a terrible person,

00:08:01.300 --> 00:08:05.516
they can take your nude photos
and upload them to a porn site.

00:08:05.540 --> 00:08:08.756
It can be really hard
to get those images taken down.

00:08:08.780 --> 00:08:09.996
And in a lot of states,

00:08:10.020 --> 00:08:13.836
you're actually better off
if you took the images of yourself

00:08:13.860 --> 00:08:16.660
because then you can
file a copyright claim.

00:08:17.140 --> 00:08:19.196
(Laughter)

00:08:19.220 --> 00:08:22.196
Right now, if someone
violates your privacy,

00:08:22.220 --> 00:08:26.420
whether that's an individual
or a company or the NSA,

00:08:27.100 --> 00:08:29.836
you can try filing a lawsuit,

00:08:29.860 --> 00:08:31.996
though you may not be successful

00:08:32.020 --> 00:08:36.796
because many courts assume
that digital privacy is just impossible.

00:08:36.820 --> 00:08:40.260
So they're not willing
to punish anyone for violating it.

00:08:41.020 --> 00:08:43.916
I still hear people
asking me all the time,

00:08:43.940 --> 00:08:49.236
isn't a digital image somehow blurring
the line between public and private

00:08:49.260 --> 00:08:50.900
because it's digital, right?

00:08:51.420 --> 00:08:52.756
No! No!

00:08:52.780 --> 00:08:56.116
Everything digital
is not just automatically public.

00:08:56.140 --> 00:08:58.036
That doesn't make any sense.

00:08:58.060 --> 00:09:01.556
As NYU legal scholar
Helen Nissenbaum tells us,

00:09:01.580 --> 00:09:04.196
we have laws and policies and norms

00:09:04.220 --> 00:09:07.356
that protect all kinds
of information that's private,

00:09:07.380 --> 00:09:10.796
and it doesn't make a difference
if it's digital or not.

00:09:10.820 --> 00:09:13.476
All of your health records are digitized

00:09:13.500 --> 00:09:16.636
but your doctor can't
just share them with anyone.

00:09:16.660 --> 00:09:21.116
All of your financial information
is held in digital databases,

00:09:21.140 --> 00:09:25.380
but your credit card company can't
just post your purchase history online.

00:09:26.900 --> 00:09:32.356
Better laws could help address
privacy violations after they happen,

00:09:32.380 --> 00:09:36.756
but one of the easiest things
we can all do is make personal changes

00:09:36.780 --> 00:09:39.460
to help protect each other's privacy.

00:09:40.180 --> 00:09:42.076
We're always told that privacy

00:09:42.100 --> 00:09:45.156
is our own, sole,
individual responsibility.

00:09:45.180 --> 00:09:49.436
We're told, constantly monitor
and update your privacy settings.

00:09:49.460 --> 00:09:54.260
We're told, never share anything
you wouldn't want the entire world to see.

00:09:55.220 --> 00:09:56.436
This doesn't make sense.

00:09:56.460 --> 00:09:59.436
Digital media are social environments

00:09:59.460 --> 00:10:03.740
and we share things
with people we trust all day, every day.

00:10:04.580 --> 00:10:07.556
As Princeton researcher
Janet Vertesi argues,

00:10:07.580 --> 00:10:11.596
our data and our privacy,
they're not just personal,

00:10:11.620 --> 00:10:14.196
they're actually interpersonal.

00:10:14.220 --> 00:10:17.476
And so one thing you can do
that's really easy

00:10:17.500 --> 00:10:22.596
is just start asking for permission before
you share anyone else's information.

00:10:22.620 --> 00:10:27.156
If you want to post a photo
of someone online, ask for permission.

00:10:27.180 --> 00:10:29.636
If you want to forward an email thread,

00:10:29.660 --> 00:10:31.036
ask for permission.

00:10:31.060 --> 00:10:33.836
And if you want to share
someone's nude selfie,

00:10:33.860 --> 00:10:36.140
obviously, ask for permission.

00:10:37.380 --> 00:10:41.836
These individual changes can really
help us protect each other's privacy,

00:10:41.860 --> 00:10:45.660
but we need technology companies
on board as well.

00:10:46.180 --> 00:10:50.676
These companies have very little
incentive to help protect our privacy

00:10:50.700 --> 00:10:53.996
because their business models
depend on us sharing everything

00:10:54.020 --> 00:10:56.260
with as many people as possible.

00:10:56.900 --> 00:10:58.836
Right now, if I send you an image,

00:10:58.860 --> 00:11:01.956
you can forward that
to anyone that you want.

00:11:01.980 --> 00:11:06.236
But what if I got to decide
if that image was forwardable or not?

00:11:06.260 --> 00:11:10.316
This would tell you, you don't
have my permission to send this image out.

00:11:10.340 --> 00:11:14.476
We do this kind of thing all the time
to protect copyright.

00:11:14.500 --> 00:11:19.276
If you buy an e-book, you can't just
send it out to as many people as you want.

00:11:19.300 --> 00:11:21.860
So why not try this with mobile phones?

00:11:22.780 --> 00:11:27.556
What you can do is we can demand
that tech companies add these protections

00:11:27.580 --> 00:11:31.316
to our devices and our platforms
as the default.

00:11:31.340 --> 00:11:34.756
After all, you can choose
the color of your car,

00:11:34.780 --> 00:11:37.620
but the airbags are always standard.

00:11:39.900 --> 00:11:43.716
If we don't think more
about digital privacy and consent,

00:11:43.740 --> 00:11:46.460
there can be serious consequences.

00:11:47.180 --> 00:11:49.436
There was a teenager from Ohio --

00:11:49.460 --> 00:11:52.300
let's call her Jennifer,
for the sake of her privacy.

00:11:52.940 --> 00:11:56.516
She shared nude photos of herself
with her high school boyfriend,

00:11:56.540 --> 00:11:58.060
thinking she could trust him.

00:11:59.540 --> 00:12:01.476
Unfortunately, he betrayed her

00:12:01.500 --> 00:12:04.476
and sent her photos
around the entire school.

00:12:04.500 --> 00:12:08.020
Jennifer was embarrassed and humiliated,

00:12:08.620 --> 00:12:12.756
but instead of being compassionate,
her classmates harassed her.

00:12:12.780 --> 00:12:14.636
They called her a slut and a whore

00:12:14.660 --> 00:12:16.620
and they made her life miserable.

00:12:17.180 --> 00:12:20.860
Jennifer started missing school
and her grades dropped.

00:12:21.340 --> 00:12:25.140
Ultimately, Jennifer decided
to end her own life.

00:12:26.540 --> 00:12:29.236
Jennifer did nothing wrong.

00:12:29.260 --> 00:12:31.516
All she did was share a nude photo

00:12:31.540 --> 00:12:34.356
with someone she thought
that she could trust.

00:12:34.380 --> 00:12:36.996
And yet our laws tell her

00:12:37.020 --> 00:12:41.180
that she committed a horrible crime
equivalent to child pornography.

00:12:41.740 --> 00:12:43.236
Our gender norms tell her

00:12:43.260 --> 00:12:46.476
that by producing
this nude image of herself,

00:12:46.500 --> 00:12:49.700
she somehow did the most
horrible, shameful thing.

00:12:50.220 --> 00:12:54.436
And when we assume that privacy
is impossible in digital media,

00:12:54.460 --> 00:12:59.980
we completely write off and excuse
her boyfriend's bad, bad behavior.

00:13:01.020 --> 00:13:06.756
People are still saying all the time
to victims of privacy violations,

00:13:06.780 --> 00:13:08.036
"What were you thinking?

00:13:08.060 --> 00:13:10.540
You should have never sent that image."

00:13:11.460 --> 00:13:15.460
If you're trying to figure out
what to say instead, try this.

00:13:15.980 --> 00:13:19.500
Imagine you've run into your friend
who broke their leg skiing.

00:13:20.060 --> 00:13:24.636
They took a risk to do something fun,
and it didn't end well.

00:13:24.660 --> 00:13:27.196
But you're probably
not going to be the jerk who says,

00:13:27.220 --> 00:13:29.660
"Well, I guess you shouldn't
have gone skiing then."

00:13:31.900 --> 00:13:34.036
If we think more about consent,

00:13:34.060 --> 00:13:37.316
we can see that victims
of privacy violations

00:13:37.340 --> 00:13:39.076
deserve our compassion,

00:13:39.100 --> 00:13:43.700
not criminalization, shaming,
harassment or punishment.

00:13:44.260 --> 00:13:48.756
We can support victims,
and we can prevent some privacy violations

00:13:48.780 --> 00:13:53.100
by making these legal,
individual and technological changes.

00:13:53.660 --> 00:13:59.476
Because the problem is not sexting,
the issue is digital privacy.

00:13:59.500 --> 00:14:01.860
And one solution is consent.

00:14:02.500 --> 00:14:07.076
So the next time a victim
of a privacy violation comes up to you,

00:14:07.100 --> 00:14:09.836
instead of blaming them,
let's do this instead:

00:14:09.860 --> 00:14:13.276
let's shift our ideas
about digital privacy,

00:14:13.300 --> 00:14:15.940
and let's respond with compassion.

00:14:16.500 --> 00:14:17.716
Thank you.

00:14:17.740 --> 00:14:23.876
(Applause)

