WEBVTT
Kind: captions
Language: nl

00:00:00.000 --> 00:00:07.000
Vertaald door: Nino van Rikxoort
Nagekeken door: Marleen Laschet

00:00:12.310 --> 00:00:16.820
Men gebruikt de media al een lange tijd 
om over seks te praten.

00:00:17.420 --> 00:00:21.340
Liefdesbrieven, 
telefoonseks, erotische foto's.

00:00:21.340 --> 00:00:24.390
Er is zelf een verhaal
over een meisje dat weggelopen is

00:00:24.390 --> 00:00:27.390
met een man die ze ontmoet heeft
via de telegraaf,

00:00:27.390 --> 00:00:28.820
in het jaar 1886.

00:00:30.430 --> 00:00:32.760
Vandaag gaat het over sexting

00:00:33.183 --> 00:00:35.353
en ik ben een sexting-expert.

00:00:35.353 --> 00:00:37.386
Niet een expert in sexting!

00:00:38.560 --> 00:00:42.810
Maar ik weet wel wat dit betekent
-- en u vast ook.

00:00:42.810 --> 00:00:44.436
[Het is een penis]

00:00:44.436 --> 00:00:47.412
(Gelach)

00:00:48.220 --> 00:00:50.406
Ik bestudeer sexting

00:00:50.406 --> 00:00:54.266
al sinds de media het in 2008
aandacht begonnen te geven.

00:00:54.550 --> 00:00:57.760
Ik heb een boek geschreven
over de morele paniek over sexting.

00:00:57.760 --> 00:00:59.176
En dit is wat ik ontdekt heb:

00:00:59.290 --> 00:01:02.376
de meeste mensen maken zich zorgen 
over het verkeerde ding.

00:01:02.510 --> 00:01:06.326
Ze proberen te voorkomen 
dat sexting sowieso gebeurt.

00:01:06.700 --> 00:01:08.300
Laat me jullie een vraag stellen:

00:01:08.300 --> 00:01:13.046
als het met wederzijdse toestemming is, 
wat is dan het probleem met sexting?

00:01:13.076 --> 00:01:17.166
Mensen houden van allerlei dingen
waar jijzelf misschien niet van houdt,

00:01:17.170 --> 00:01:19.616
zoals blauwschimmelkaas of koriander.

00:01:19.616 --> 00:01:22.662
(Gelach)

00:01:22.662 --> 00:01:26.408
Sexting is echt wel riskant,
zoals alles wat leuk is,

00:01:26.408 --> 00:01:31.034
maar zolang je maar geen foto stuurt

00:01:31.034 --> 00:01:33.424
naar iemand die ze niet wil ontvangen,

00:01:33.424 --> 00:01:35.134
geeft het niet.

00:01:35.144 --> 00:01:37.749
Wat ik daarentegen
wel een serieus probleem vind,

00:01:37.749 --> 00:01:41.166
is als mensen de privé-foto's
van anderen delen

00:01:41.166 --> 00:01:43.140
zonder hun toestemming.

00:01:43.140 --> 00:01:45.510
In plaats van ons zorgen 
te maken over sexting,

00:01:45.510 --> 00:01:50.346
vind ik dat we meer moeten nadenken 
over digitale privacy.

00:01:50.650 --> 00:01:53.010
De sleutel is toestemming.

00:01:53.650 --> 00:01:56.910
Op dit moment denkt men over sexting

00:01:56.910 --> 00:02:00.086
zonder te denken aan toestemming.

00:02:00.086 --> 00:02:04.342
Wist je dat we op dit moment 
tiener-sexting criminaliseren?

00:02:05.340 --> 00:02:08.866
Het kan een misdaad zijn,
omdat het valt onder kinderpornografie

00:02:08.866 --> 00:02:11.596
als er een foto is van iemand
onder de 18 jaar,

00:02:11.596 --> 00:02:13.042
en het maakt niet eens uit

00:02:13.042 --> 00:02:17.548
of ze die foto van zichzelf gemaakt hebben
en haar vrijwillig hebben verspreid.

00:02:17.560 --> 00:02:20.640
Dus zitten we nu met deze
bizarre wettelijke situatie,

00:02:20.640 --> 00:02:25.106
waarbij twee 17-jarigen legaal seks
kunnen hebben in de meeste staten,

00:02:25.160 --> 00:02:27.566
maar er geen foto's van mogen maken.

00:02:28.690 --> 00:02:32.450
Sommige staten hebben ook geprobeerd
om sexting een overtreding te maken,

00:02:32.640 --> 00:02:35.746
maar deze wetten herhalen het probleem,

00:02:35.750 --> 00:02:40.486
want ze maken nog steeds sexting illegaal,
zelfs bij wederzijds goedvinden.

00:02:40.486 --> 00:02:41.772
Het is gewoon absurd

00:02:41.772 --> 00:02:46.122
om alle sexting te verbannen
om schendingen van privacy aan te pakken.

00:02:46.432 --> 00:02:47.988
Dat is hetzelfde als zeggen:

00:02:47.988 --> 00:02:50.474
laten we het probleem
van date-verkrachting oplossen

00:02:50.474 --> 00:02:53.864
door daten in zijn geheel
illegaal te maken.

00:02:54.940 --> 00:03:00.240
De meeste jongeren worden niet opgepakt 
voor sexting, maar raad eens wie wel?

00:03:00.240 --> 00:03:05.296
Meestal jongeren die door hun schoonouders
niet leuk gevonden worden.

00:03:05.296 --> 00:03:10.086
Dit kan zijn door bijvoorbeeld
klassenverschil, racisme of homofobie.

00:03:10.696 --> 00:03:13.562
De meeste officieren van justitie
zijn natuurlijk slim genoeg

00:03:13.562 --> 00:03:17.630
om jongeren niet aan te klagen
voor kinderpornografie,

00:03:17.630 --> 00:03:19.490
maar sommigen doen het wel.

00:03:19.490 --> 00:03:23.200
Volgens onderzoekers
van de Universiteit van New Hampsire

00:03:23.250 --> 00:03:27.920
betreffen 7% van alle arrestaties
wegens kinderporno

00:03:27.920 --> 00:03:32.080
jongeren die met wederzijdse goedkeuring
sexten met andere jongeren.

00:03:33.440 --> 00:03:35.826
Kinderpornografie is een ernstige misdaad,

00:03:35.826 --> 00:03:39.722
maar het is niet hetzelfde 
als jongeren-sexting.

00:03:40.790 --> 00:03:44.450
Ouders en opvoeders
reageren ook op sexting

00:03:44.450 --> 00:03:47.326
zonder na te denken 
over of beiden het goedvinden.

00:03:47.440 --> 00:03:51.996
Hun boodschap aan jongeren
is meestal: gewoon niet doen.

00:03:52.140 --> 00:03:55.636
En ik snap het wel --
er zijn wettelijke risico's aan verbonden

00:03:55.636 --> 00:03:59.036
en natuurlijk, die potentiële
schending van privacy.

00:03:59.330 --> 00:04:00.746
En toen je zelf jong was,

00:04:00.746 --> 00:04:04.202
deed je alles precies
zoals je verteld werd, toch?

00:04:05.270 --> 00:04:08.946
Waarschijnlijk denk je nu
dat jouw kind nooit sext.

00:04:08.946 --> 00:04:12.336
En het is waar, jouw kleine engel
sext misschien niet,

00:04:12.336 --> 00:04:15.482
want slechts 33%

00:04:15.482 --> 00:04:17.982
van de 16- en 17-jarigen sexten.

00:04:18.980 --> 00:04:23.656
Maar, het spijt me, als ze ouder worden,
zullen ze waarschijnlijk wel sexten.

00:04:23.660 --> 00:04:25.386
In elke studie die ik gezien heb,

00:04:25.386 --> 00:04:29.916
ligt het cijfer boven de 50%
voor 18- tot 24-jarigen.

00:04:30.416 --> 00:04:33.636
En meestal gaat er niets fout.

00:04:33.650 --> 00:04:39.070
Mensen vragen me altijd dingen als:
is sexting niet gevaarlijk?

00:04:39.070 --> 00:04:42.966
Als je je portemonnee
op een parkbankje laat liggen,

00:04:42.966 --> 00:04:46.222
verwacht je toch ook dat hij
gestolen gaat worden, toch?

00:04:46.222 --> 00:04:48.308
Dit is hoe ik er over denk:

00:04:48.320 --> 00:04:52.190
sexting is als je portemonnee
laten liggen in je vriendjes huis.

00:04:52.190 --> 00:04:54.060
Als je de volgende dag terugkomt

00:04:54.060 --> 00:04:56.230
en al het geld is gewoon weg,

00:04:56.230 --> 00:04:59.340
moet je het echt uitmaken met die kerel.

00:04:59.340 --> 00:05:03.486
(Gelach)

00:05:03.520 --> 00:05:05.666
Dus in plaats van sexting 
te criminaliseren

00:05:05.666 --> 00:05:08.216
om privacy-schending te voorkomen,

00:05:08.216 --> 00:05:11.632
moet wederzijdse toestemming 
het uitgangspunt worden

00:05:11.632 --> 00:05:15.528
bij het nadenken over de circulatie
van onze privé-informatie.

00:05:16.160 --> 00:05:20.656
Elke nieuwe mediatechnologie
brengt privacy-problemen met zich mee.

00:05:20.656 --> 00:05:25.072
In feite waren de allereerste
grote debatten in de VS over privacy

00:05:25.072 --> 00:05:29.468
een reactie op technologieën
die op dat moment relatief nieuw waren.

00:05:29.690 --> 00:05:33.506
Aan het einde van de 19de eeuw
maakten mensen zich zorgen over camera's,

00:05:33.630 --> 00:05:36.830
die opeens draagbaarder waren
dan ooit tevoren,

00:05:37.090 --> 00:05:39.536
en roddelartikelen in kranten.

00:05:39.680 --> 00:05:43.306
Ze waren bezorgd dat de camera
informatie over hen zou vastleggen,

00:05:43.306 --> 00:05:45.266
die informatie uit de context zou halen

00:05:45.266 --> 00:05:46.996
en hem zou verspreiden.

00:05:46.996 --> 00:05:48.346
Klinkt dit herkenbaar?

00:05:48.650 --> 00:05:51.472
Het is precies hetzelfde
als waar wij ons zorgen over maken,

00:05:51.472 --> 00:05:53.534
met sociale media en drone-camera's

00:05:53.534 --> 00:05:55.266
en natuurlijk sexting.

00:05:55.700 --> 00:05:59.040
En deze angsten over technologie
zijn wel begrijpelijk,

00:05:59.056 --> 00:06:02.736
omdat technologieën 
onze slechtste eigenschappen en gedrag

00:06:02.736 --> 00:06:05.366
naar boven kunnen halen en versterken.

00:06:05.980 --> 00:06:08.436
Maar er bestaan oplossingen.

00:06:08.436 --> 00:06:11.942
En we hebben dit eerder meegemaakt
met een gevaarlijke nieuwe technologie.

00:06:12.522 --> 00:06:16.302
In 1908 introduceerde Ford 
de Model T-auto.

00:06:16.310 --> 00:06:18.966
Sterftecijfers in het verkeer stegen.

00:06:18.966 --> 00:06:22.126
Het was een ernstig probleem --
het ziet er zo veilig uit, toch?

00:06:23.930 --> 00:06:28.016
We probeerden toen eerst het gedrag
van de bestuurders te veranderen,

00:06:28.016 --> 00:06:31.982
dus ontwikkelden we snelheidslimieten
en we dwongen ze af door boetes.

00:06:31.982 --> 00:06:35.464
Maar in de loop van de volgende decennia
begonnen we ons te realiseren

00:06:35.464 --> 00:06:39.330
dat de technologie van de auto 
niet neutraal was.

00:06:39.330 --> 00:06:42.236
We konden de auto ontwerpen
om hem veiliger te maken.

00:06:42.720 --> 00:06:46.410
In de jaren 20 kregen we voorruiten
die niet konden breken.

00:06:46.410 --> 00:06:48.786
In de jaren 50 kregen we gordels.

00:06:48.786 --> 00:06:51.852
En in de jaren 90 airbags.

00:06:52.300 --> 00:06:54.276
Al deze drie gebieden,

00:06:54.686 --> 00:06:59.556
wetten, individuen en industrie, 
kwamen in de loop van de tijd samen

00:06:59.556 --> 00:07:03.136
om het probleem op te lossen
dat de nieuwe technologie meebrengt.

00:07:03.136 --> 00:07:06.492
En we kunnen dit ook doen
met digitale privacy.

00:07:06.950 --> 00:07:10.106
Het komt uiteraard neer
op wederzijdse toestemming.

00:07:10.150 --> 00:07:11.216
Dit is het idee.

00:07:11.346 --> 00:07:15.122
Voordat iemand je privégegevens
kan verspreiden,

00:07:15.122 --> 00:07:17.668
zouden ze je toestemming moeten krijgen.

00:07:18.020 --> 00:07:22.776
Dit idee van actief toestemming geven
komt van anti-verkrachtingsactivisten

00:07:22.790 --> 00:07:26.806
die ons vertellen dat we toestemming
nodig hebben voor elke seksuele handeling.

00:07:26.806 --> 00:07:30.846
En we hebben echt hoge normen 
inzake toestemming op vele andere vlakken.

00:07:31.286 --> 00:07:33.082
Bijvoorbeeld bij een operatie.

00:07:33.082 --> 00:07:37.288
De dokter moet zich ervan vergewissen
dat je met gezond verstand toestemt

00:07:37.288 --> 00:07:39.378
in het ondergaan
van die medische procedure.

00:07:39.378 --> 00:07:43.454
Dit is niet hetzelfde als toestemmen
met de iTunes-servicevoorwaarde,

00:07:43.454 --> 00:07:46.530
waar je naar beneden scrolt
en zonder te lezen op akkoord klikt.

00:07:46.560 --> 00:07:49.276
(Gelach)

00:07:49.276 --> 00:07:54.062
Als we meer over toestemming nadenken,
kunnen we betere privacy-wetten krijgen.

00:07:54.090 --> 00:07:57.306
Op dit moment hebben we gewoon
niet veel bescherming.

00:07:57.620 --> 00:08:00.966
Als je ex-man of ex-vrouw
een verschrikkelijke persoon is,

00:08:00.966 --> 00:08:05.182
kunnen ze je naaktfoto's
uploaden naar een porno-website.

00:08:05.490 --> 00:08:08.630
Het kan erg lastig zijn
om die foto's van het web te krijgen.

00:08:08.630 --> 00:08:10.036
En in veel staten

00:08:10.036 --> 00:08:13.702
kun je beter zeggen
dat je de foto's zelf genomen hebt,

00:08:13.702 --> 00:08:17.902
omdat je dan aanspraak kan maken
op de auteursrechten.

00:08:17.902 --> 00:08:19.248
(Gelach)

00:08:19.248 --> 00:08:22.304
Op dit moment,
als iemand je privacy schendt,

00:08:22.304 --> 00:08:27.080
of dat nou een individu is,
of een bedrijf, of de NSA,

00:08:27.080 --> 00:08:29.670
kun je proberen 
een rechtszaak aan te spannen,

00:08:29.670 --> 00:08:32.146
maar die zal misschien
niet succesvol zijn,

00:08:32.146 --> 00:08:37.082
omdat veel rechtbanken aannemen
dat digitale privacy gewoon onmogelijk is.

00:08:37.082 --> 00:08:40.462
Dus willen ze eigenlijk
niemand straffen die het schendt.

00:08:41.180 --> 00:08:43.846
Nog steeds vragen mensen mij:

00:08:43.846 --> 00:08:49.272
"Vervaagt een digitale foto
niet de lijn tussen openbaar en privé,

00:08:49.272 --> 00:08:51.408
omdat het digitaal is, toch?"

00:08:51.408 --> 00:08:52.804
Nee! Nee!

00:08:52.804 --> 00:08:56.260
Alles wat digitaal is,
is niet automatisch openbaar.

00:08:56.260 --> 00:08:57.926
Dat slaat helemaal nergens op.

00:08:58.020 --> 00:09:01.436
NYU-jurist Helen Nissenbaum vertelt ons

00:09:01.456 --> 00:09:04.252
dat we wetten, beleidsmaatregelen
en normen hebben

00:09:04.252 --> 00:09:07.376
die allerlei persoonlijke
informatie beschermen

00:09:07.380 --> 00:09:10.766
en het maakt geen verschil
of het digitaal is of niet.

00:09:10.766 --> 00:09:13.576
Al je medische gegevens
zijn gedigitaliseerd,

00:09:13.576 --> 00:09:16.402
maar je dokter kan ze niet
zomaar met iedereen delen.

00:09:16.402 --> 00:09:20.778
Al je financiële informatie
wordt opgeslagen in digitale databases,

00:09:21.000 --> 00:09:23.150
maar je creditcardmaatschappij
kan niet zomaar

00:09:23.150 --> 00:09:25.730
je aankoopgeschiedenis online zetten.

00:09:26.950 --> 00:09:32.096
Betere wetgeving kan privacyschending
achteraf helpen aan te pakken,

00:09:32.096 --> 00:09:35.152
maar een van de makkelijkste dingen
die we allemaal kunnen doen,

00:09:35.152 --> 00:09:37.032
is zelf persoonlijk veranderingen maken

00:09:37.032 --> 00:09:39.792
om elkaars privacy te beschermen.

00:09:40.170 --> 00:09:44.980
Men zegt ons steeds dat privacy
onze eigen verantwoordelijkheid is.

00:09:45.230 --> 00:09:46.756
Men zegt ons dat we voortdurend

00:09:46.756 --> 00:09:49.436
onze privacy-instellingen
moeten controleren en herzien.

00:09:49.436 --> 00:09:54.270
Ze zeggen dat we nooit iets moeten delen
wat de hele wereld nooit mag zien.

00:09:55.210 --> 00:09:56.450
Dit slaat nergens op.

00:09:56.460 --> 00:09:59.436
Digitale media zijn sociale omgevingen

00:09:59.580 --> 00:10:03.596
en wij delen dingen met mensen
die we vertrouwen, elke dag weer.

00:10:04.580 --> 00:10:07.156
Zoals Princeton-onderzoeker 
Janet Vertesi betoogt:

00:10:07.580 --> 00:10:11.626
"Onze data en onze privacy
zijn niet alleen persoonlijk,

00:10:11.630 --> 00:10:14.016
ze zijn eigenlijk interpersoonlijk."

00:10:14.340 --> 00:10:17.556
Iets wat je dus erg makkelijk kunt doen,

00:10:17.600 --> 00:10:19.616
is beginnen met toestemming te vragen

00:10:19.616 --> 00:10:22.406
voordat je iemand anders 
zijn informatie deelt.

00:10:22.406 --> 00:10:25.642
Als je een foto van iemand
online wil posten,

00:10:25.642 --> 00:10:27.112
vraag om toestemming.

00:10:27.112 --> 00:10:29.658
Als je een email door wilt sturen,

00:10:29.658 --> 00:10:30.988
vraag om toestemming.

00:10:31.160 --> 00:10:33.836
En als je iemands naaktfoto
wil verspreiden,

00:10:33.836 --> 00:10:36.166
vraag dan, natuurlijk, om toestemming.

00:10:37.360 --> 00:10:40.060
Deze individuele veranderingen
kunnen ons echt helpen

00:10:40.060 --> 00:10:41.850
om elkaars privacy te beschermen,

00:10:41.850 --> 00:10:45.646
maar daar hebben wij ook 
technologiebedrijven voor nodig.

00:10:46.300 --> 00:10:50.626
Deze bedrijven hebben weinig reden
om onze privacy te helpen beschermen,

00:10:50.626 --> 00:10:53.050
omdat hun bedrijfsmodel
ervan afhankelijk is

00:10:53.050 --> 00:10:56.296
dat wij alles delen
met zoveel mogelijk mensen.

00:10:56.940 --> 00:10:59.126
Op dit moment,
als ik je een afbeelding stuur,

00:10:59.126 --> 00:11:01.652
kan je die naar iedereen
doorsturen die je maar wil.

00:11:02.110 --> 00:11:06.036
Maar als ík nu eens kon bepalen
of je die afbeelding door kon sturen?

00:11:06.280 --> 00:11:10.120
Dan zou je weten dat je geen toestemming
had om de afbeelding door te sturen.

00:11:10.440 --> 00:11:14.486
Dit doen we bijna altijd
om auteursrechten te beschermen.

00:11:14.530 --> 00:11:19.210
Als je een e-book koopt,
kun je het boek niet met iedereen delen.

00:11:19.320 --> 00:11:22.756
Waarom kunnen we dit niet proberen
met onze mobiele telefoons?

00:11:22.830 --> 00:11:26.046
We kunnen technologiebedrijven eisen

00:11:26.046 --> 00:11:28.686
deze beschermingen standaard toe te voegen

00:11:28.686 --> 00:11:31.012
op onze apparaten en platformen.

00:11:31.480 --> 00:11:34.680
Tenslotte kun je de kleur
van je auto kiezen,

00:11:34.680 --> 00:11:37.776
maar de airbags zijn altijd hetzelfde.

00:11:39.910 --> 00:11:43.890
Als we niet méér gaan nadenken
over digitale privacy en toestemming,

00:11:43.890 --> 00:11:46.536
kan dat ernstige gevolgen hebben.

00:11:47.350 --> 00:11:49.450
Er was ooit een tiener uit Ohio --

00:11:49.450 --> 00:11:52.686
laten we haar Jennifer noemen,
voor haar eigen privacy.

00:11:52.686 --> 00:11:56.506
Ze had naaktfoto's van zichzelf
gedeeld met haar vriendje,

00:11:56.506 --> 00:11:58.582
omdat ze dacht dat ze hem kon vertrouwen.

00:11:59.570 --> 00:12:01.756
Helaas verraadde hij haar.

00:12:01.756 --> 00:12:04.476
Hij deelde al haar foto's
met de hele school.

00:12:04.476 --> 00:12:08.242
Jennifer was beschaamd en vernederd.

00:12:08.562 --> 00:12:12.818
In plaats van mededogen te tonen, 
pestten haar klasgenoten haar.

00:12:12.818 --> 00:12:14.628
Ze noemden haar een slet en een hoer

00:12:14.628 --> 00:12:17.178
en ze maakten van haar leven
een echte nachtmerrie.

00:12:17.178 --> 00:12:20.978
Jennifer begon te spijbelen
en haar cijfers kelderden omlaag.

00:12:21.470 --> 00:12:25.306
Uiteindelijk heeft Jennifer
een einde gemaakt aan haar leven.

00:12:26.820 --> 00:12:29.076
Jennifer had niets fout gedaan.

00:12:29.210 --> 00:12:31.886
Het enige wat ze gedaan had,
was een naaktfoto delen

00:12:31.886 --> 00:12:34.372
met iemand die ze dacht
te kunnen vertrouwen.

00:12:34.372 --> 00:12:37.732
En toch zeggen onze wetten

00:12:37.732 --> 00:12:41.788
dat ze een verschrikkelijk misdrijf
heeft gepleegd, gelijk aan kinderporno.

00:12:41.860 --> 00:12:43.806
Onze geslachtsnormen zeggen dat zij,

00:12:43.806 --> 00:12:46.436
door het maken
van de naaktfoto van haarzelf,

00:12:46.840 --> 00:12:50.186
de meest afschuwelijke,
schandelijke daad gesteld heeft.

00:12:50.220 --> 00:12:54.380
En als we aannemen dat privacy 
onmogelijk is in de digitale media,

00:12:54.480 --> 00:13:00.216
vergeten we compleet de fout
van het slechte gedrag van haar vriendje.

00:13:01.160 --> 00:13:06.346
Mensen zeggen nog steeds
tegen slachtoffers van privacyschendingen:

00:13:06.740 --> 00:13:08.240
"Wat dacht je toen je dat deed?

00:13:08.240 --> 00:13:10.910
Je had die foto nooit mogen sturen!"

00:13:11.600 --> 00:13:15.850
Als je niet weet wat je anders
moet zeggen, probeer dan dit eens.

00:13:15.850 --> 00:13:19.976
Stel je komt een vriend tegen die
zijn been gebroken heeft bij het skiën.

00:13:20.130 --> 00:13:24.420
Ze hebben een risico genomen om iets
leuks te doen, maar het ging fout.

00:13:24.420 --> 00:13:27.180
Maar je gaat waarschijnlijk niet
de eikel zijn die zegt:

00:13:27.180 --> 00:13:29.926
"Nou, dan had je maar 
niet moeten gaan skiën."

00:13:31.670 --> 00:13:34.236
Als we meer nadenken
over wederzijdse toestemming,

00:13:34.236 --> 00:13:37.482
komen we erachter
dat slachtoffers van privacyschending

00:13:37.482 --> 00:13:39.142
ons medegevoel verdienen,

00:13:39.142 --> 00:13:43.638
niet criminalisering, beschaming,
intimideren of straffen.

00:13:44.270 --> 00:13:46.400
We kunnen slachtoffers helpen

00:13:46.400 --> 00:13:48.760
en we kunnen sommige
privacyschendingen voorkomen

00:13:48.760 --> 00:13:53.376
door juridische, persoonlijke 
en technologische aanpassingen.

00:13:53.750 --> 00:13:59.430
Want het probleem is niet sexting,
het probleem is digitale privacy.

00:13:59.480 --> 00:14:02.046
En één oplossing is toestemming.

00:14:02.490 --> 00:14:03.640
Dus de volgende keer

00:14:03.640 --> 00:14:06.770
dat je een slachtoffer
van privacyschending tegenkomt,

00:14:07.010 --> 00:14:08.770
in plaats van hen de schuld te geven,

00:14:08.770 --> 00:14:10.060
doe liever dit:

00:14:10.060 --> 00:14:12.700
laten we onze ideëen
over digitale privacy veranderen

00:14:12.940 --> 00:14:16.146
en laten we antwoorden met mededogen.

00:14:16.475 --> 00:14:17.925
Bedankt.

00:14:17.925 --> 00:14:23.876
(Applaus)

