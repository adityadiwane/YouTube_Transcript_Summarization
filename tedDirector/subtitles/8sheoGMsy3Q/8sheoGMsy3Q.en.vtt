WEBVTT
Kind: captions
Language: en

00:00:12.811 --> 00:00:16.364
We've evolved with tools,
and tools have evolved with us.

00:00:16.388 --> 00:00:21.407
Our ancestors created these
hand axes 1.5 million years ago,

00:00:21.431 --> 00:00:24.453
shaping them to not only
fit the task at hand

00:00:24.477 --> 00:00:25.945
but also their hand.

00:00:26.747 --> 00:00:28.327
However, over the years,

00:00:28.351 --> 00:00:30.881
tools have become
more and more specialized.

00:00:31.293 --> 00:00:35.130
These sculpting tools
have evolved through their use,

00:00:35.154 --> 00:00:38.716
and each one has a different form
which matches its function.

00:00:38.740 --> 00:00:41.408
And they leverage
the dexterity of our hands

00:00:41.432 --> 00:00:44.562
in order to manipulate things
with much more precision.

00:00:45.338 --> 00:00:48.404
But as tools have become
more and more complex,

00:00:48.428 --> 00:00:52.306
we need more complex controls
to control them.

00:00:52.714 --> 00:00:57.158
And so designers have become
very adept at creating interfaces

00:00:57.182 --> 00:01:00.920
that allow you to manipulate parameters
while you're attending to other things,

00:01:00.944 --> 00:01:03.823
such as taking a photograph
and changing the focus

00:01:03.847 --> 00:01:05.183
or the aperture.

00:01:05.918 --> 00:01:10.137
But the computer has fundamentally
changed the way we think about tools

00:01:10.161 --> 00:01:12.175
because computation is dynamic.

00:01:12.564 --> 00:01:14.715
So it can do a million different things

00:01:14.739 --> 00:01:17.088
and run a million different applications.

00:01:17.112 --> 00:01:20.857
However, computers have
the same static physical form

00:01:20.881 --> 00:01:22.817
for all of these different applications

00:01:22.841 --> 00:01:25.618
and the same static
interface elements as well.

00:01:25.976 --> 00:01:28.396
And I believe that this
is fundamentally a problem,

00:01:28.420 --> 00:01:31.416
because it doesn't really allow us
to interact with our hands

00:01:31.440 --> 00:01:34.867
and capture the rich dexterity
that we have in our bodies.

00:01:36.026 --> 00:01:40.562
And my belief is that, then,
we must need new types of interfaces

00:01:40.586 --> 00:01:44.345
that can capture these
rich abilities that we have

00:01:44.369 --> 00:01:46.739
and that can physically adapt to us

00:01:46.763 --> 00:01:49.013
and allow us to interact in new ways.

00:01:49.037 --> 00:01:51.624
And so that's what I've been doing
at the MIT Media Lab

00:01:51.648 --> 00:01:52.972
and now at Stanford.

00:01:53.901 --> 00:01:57.512
So with my colleagues,
Daniel Leithinger and Hiroshi Ishii,

00:01:57.536 --> 00:01:58.925
we created inFORM,

00:01:58.949 --> 00:02:01.447
where the interface can actually
come off the screen

00:02:01.471 --> 00:02:03.747
and you can physically manipulate it.

00:02:03.771 --> 00:02:06.514
Or you can visualize
3D information physically

00:02:06.538 --> 00:02:10.052
and touch it and feel it
to understand it in new ways.

00:02:15.889 --> 00:02:19.965
Or you can interact through gestures
and direct deformations

00:02:19.989 --> 00:02:22.286
to sculpt digital clay.

00:02:26.474 --> 00:02:29.555
Or interface elements can arise
out of the surface

00:02:29.579 --> 00:02:30.951
and change on demand.

00:02:30.975 --> 00:02:33.483
And the idea is that for each
individual application,

00:02:33.507 --> 00:02:36.807
the physical form can be matched
to the application.

00:02:37.196 --> 00:02:39.301
And I believe this represents a new way

00:02:39.325 --> 00:02:41.275
that we can interact with information,

00:02:41.299 --> 00:02:42.728
by making it physical.

00:02:43.142 --> 00:02:45.357
So the question is, how can we use this?

00:02:45.810 --> 00:02:49.500
Traditionally, urban planners
and architects build physical models

00:02:49.524 --> 00:02:52.334
of cities and buildings
to better understand them.

00:02:52.358 --> 00:02:56.573
So with Tony Tang at the Media Lab,
we created an interface built on inFORM

00:02:56.597 --> 00:03:01.580
to allow urban planners
to design and view entire cities.

00:03:01.604 --> 00:03:05.861
And now you can walk around it,
but it's dynamic, it's physical,

00:03:05.885 --> 00:03:07.585
and you can also interact directly.

00:03:07.609 --> 00:03:09.347
Or you can look at different views,

00:03:09.371 --> 00:03:12.188
such as population or traffic information,

00:03:12.212 --> 00:03:13.774
but it's made physical.

00:03:14.996 --> 00:03:18.806
We also believe that these dynamic
shape displays can really change

00:03:18.830 --> 00:03:21.790
the ways that we remotely
collaborate with people.

00:03:21.814 --> 00:03:24.117
So when we're working together in person,

00:03:24.141 --> 00:03:25.799
I'm not only looking at your face

00:03:25.823 --> 00:03:28.861
but I'm also gesturing
and manipulating objects,

00:03:28.885 --> 00:03:32.675
and that's really hard to do
when you're using tools like Skype.

00:03:33.905 --> 00:03:36.891
And so using inFORM,
you can reach out from the screen

00:03:36.915 --> 00:03:39.027
and manipulate things at a distance.

00:03:39.051 --> 00:03:42.226
So we used the pins of the display
to represent people's hands,

00:03:42.250 --> 00:03:46.756
allowing them to actually touch
and manipulate objects at a distance.

00:03:50.519 --> 00:03:54.793
And you can also manipulate
and collaborate on 3D data sets as well,

00:03:54.817 --> 00:03:58.486
so you can gesture around them
as well as manipulate them.

00:03:58.510 --> 00:04:02.900
And that allows people to collaborate
on these new types of 3D information

00:04:02.924 --> 00:04:06.535
in a richer way than might
be possible with traditional tools.

00:04:07.870 --> 00:04:10.623
And so you can also
bring in existing objects,

00:04:10.647 --> 00:04:13.861
and those will be captured on one side
and transmitted to the other.

00:04:13.885 --> 00:04:16.671
Or you can have an object that's linked
between two places,

00:04:16.695 --> 00:04:18.779
so as I move a ball on one side,

00:04:18.803 --> 00:04:20.730
the ball moves on the other as well.

00:04:22.278 --> 00:04:25.381
And so we do this by capturing
the remote user

00:04:25.405 --> 00:04:28.210
using a depth-sensing camera
like a Microsoft Kinect.

00:04:28.758 --> 00:04:31.775
Now, you might be wondering
how does this all work,

00:04:31.799 --> 00:04:35.450
and essentially, what it is,
is 900 linear actuators

00:04:35.474 --> 00:04:37.760
that are connected to these
mechanical linkages

00:04:37.784 --> 00:04:41.530
that allow motion down here
to be propagated in these pins above.

00:04:41.554 --> 00:04:45.121
So it's not that complex
compared to what's going on at CERN,

00:04:45.145 --> 00:04:47.471
but it did take a long time
for us to build it.

00:04:47.495 --> 00:04:49.750
And so we started with a single motor,

00:04:49.774 --> 00:04:51.189
a single linear actuator,

00:04:51.816 --> 00:04:54.979
and then we had to design
a custom circuit board to control them.

00:04:55.003 --> 00:04:57.055
And then we had to make a lot of them.

00:04:57.079 --> 00:05:00.693
And so the problem with having
900 of something

00:05:00.717 --> 00:05:03.837
is that you have to do
every step 900 times.

00:05:03.861 --> 00:05:06.218
And so that meant that we had
a lot of work to do.

00:05:06.242 --> 00:05:09.974
So we sort of set up
a mini-sweatshop in the Media Lab

00:05:09.998 --> 00:05:13.710
and brought undergrads in and convinced
them to do "research" --

00:05:13.734 --> 00:05:14.748
(Laughter)

00:05:14.772 --> 00:05:17.790
and had late nights
watching movies, eating pizza

00:05:17.814 --> 00:05:19.642
and screwing in thousands of screws.

00:05:19.666 --> 00:05:20.864
You know -- research.

00:05:20.888 --> 00:05:22.435
(Laughter)

00:05:22.459 --> 00:05:25.777
But anyway, I think that we were
really excited by the things

00:05:25.801 --> 00:05:27.497
that inFORM allowed us to do.

00:05:27.521 --> 00:05:31.721
Increasingly, we're using mobile devices,
and we interact on the go.

00:05:31.745 --> 00:05:34.424
But mobile devices, just like computers,

00:05:34.448 --> 00:05:36.759
are used for so many
different applications.

00:05:36.783 --> 00:05:38.776
So you use them to talk on the phone,

00:05:38.800 --> 00:05:41.956
to surf the web, to play games,
to take pictures

00:05:41.980 --> 00:05:43.689
or even a million different things.

00:05:43.713 --> 00:05:46.697
But again, they have the same
static physical form

00:05:46.721 --> 00:05:48.839
for each of these applications.

00:05:48.863 --> 00:05:52.226
And so we wanted to know how can we take
some of the same interactions

00:05:52.250 --> 00:05:53.933
that we developed for inFORM

00:05:53.957 --> 00:05:55.845
and bring them to mobile devices.

00:05:56.427 --> 00:06:00.074
So at Stanford, we created
this haptic edge display,

00:06:00.098 --> 00:06:03.275
which is a mobile device
with an array of linear actuators

00:06:03.299 --> 00:06:04.646
that can change shape,

00:06:04.670 --> 00:06:08.567
so you can feel in your hand
where you are as you're reading a book.

00:06:09.058 --> 00:06:12.795
Or you can feel in your pocket
new types of tactile sensations

00:06:12.819 --> 00:06:14.621
that are richer than the vibration.

00:06:14.645 --> 00:06:17.880
Or buttons can emerge from the side
that allow you to interact

00:06:17.904 --> 00:06:19.612
where you want them to be.

00:06:21.334 --> 00:06:24.731
Or you can play games
and have actual buttons.

00:06:25.786 --> 00:06:27.302
And so we were able to do this

00:06:27.326 --> 00:06:32.080
by embedding 40 small, tiny
linear actuators inside the device,

00:06:32.104 --> 00:06:34.159
and that allow you not only to touch them

00:06:34.183 --> 00:06:36.087
but also back-drive them as well.

00:06:36.911 --> 00:06:41.089
But we've also looked at other ways
to create more complex shape change.

00:06:41.113 --> 00:06:44.505
So we've used pneumatic actuation
to create a morphing device

00:06:44.529 --> 00:06:48.394
where you can go from something
that looks a lot like a phone ...

00:06:48.418 --> 00:06:50.648
to a wristband on the go.

00:06:51.720 --> 00:06:54.559
And so together with Ken Nakagaki
at the Media Lab,

00:06:54.583 --> 00:06:57.137
we created this new
high-resolution version

00:06:57.161 --> 00:07:03.111
that uses an array of servomotors
to change from interactive wristband

00:07:03.135 --> 00:07:06.263
to a touch-input device

00:07:06.287 --> 00:07:07.532
to a phone.

00:07:07.556 --> 00:07:09.214
(Laughter)

00:07:10.104 --> 00:07:12.276
And we're also interested
in looking at ways

00:07:12.300 --> 00:07:14.927
that users can actually
deform the interfaces

00:07:14.951 --> 00:07:17.839
to shape them into the devices
that they want to use.

00:07:17.863 --> 00:07:20.271
So you can make something
like a game controller,

00:07:20.295 --> 00:07:22.925
and then the system will understand
what shape it's in

00:07:22.949 --> 00:07:24.568
and change to that mode.

00:07:26.052 --> 00:07:27.624
So, where does this point?

00:07:27.648 --> 00:07:29.576
How do we move forward from here?

00:07:29.600 --> 00:07:32.195
I think, really, where we are today

00:07:32.219 --> 00:07:34.973
is in this new age
of the Internet of Things,

00:07:34.997 --> 00:07:36.788
where we have computers everywhere --

00:07:36.812 --> 00:07:38.930
they're in our pockets,
they're in our walls,

00:07:38.954 --> 00:07:42.520
they're in almost every device
that you'll buy in the next five years.

00:07:42.544 --> 00:07:45.425
But what if we stopped
thinking about devices

00:07:45.449 --> 00:07:47.843
and think instead about environments?

00:07:47.867 --> 00:07:50.379
And so how can we have smart furniture

00:07:50.403 --> 00:07:53.719
or smart rooms or smart environments

00:07:53.743 --> 00:07:56.835
or cities that can adapt to us physically,

00:07:56.859 --> 00:08:01.090
and allow us to do new ways
of collaborating with people

00:08:01.114 --> 00:08:03.352
and doing new types of tasks?

00:08:03.376 --> 00:08:06.760
So for the Milan Design Week,
we created TRANSFORM,

00:08:06.784 --> 00:08:10.608
which is an interactive table-scale
version of these shape displays,

00:08:10.632 --> 00:08:13.815
which can move physical objects
on the surface; for example,

00:08:13.839 --> 00:08:16.096
reminding you to take your keys.

00:08:16.120 --> 00:08:20.602
But it can also transform
to fit different ways of interacting.

00:08:20.626 --> 00:08:21.943
So if you want to work,

00:08:21.967 --> 00:08:24.959
then it can change to sort of
set up your work system.

00:08:24.983 --> 00:08:26.934
And so as you bring a device over,

00:08:26.958 --> 00:08:29.696
it creates all the affordances you need

00:08:29.720 --> 00:08:34.520
and brings other objects
to help you accomplish those goals.

00:08:37.139 --> 00:08:38.700
So, in conclusion,

00:08:38.724 --> 00:08:42.723
I really think that we need to think
about a new, fundamentally different way

00:08:42.747 --> 00:08:44.905
of interacting with computers.

00:08:45.551 --> 00:08:48.505
We need computers
that can physically adapt to us

00:08:48.529 --> 00:08:51.130
and adapt to the ways
that we want to use them

00:08:51.154 --> 00:08:55.701
and really harness the rich dexterity
that we have of our hands,

00:08:55.725 --> 00:08:59.996
and our ability to think spatially
about information by making it physical.

00:09:00.663 --> 00:09:04.659
But looking forward, I think we need
to go beyond this, beyond devices,

00:09:04.683 --> 00:09:08.076
to really think about new ways
that we can bring people together,

00:09:08.100 --> 00:09:11.118
and bring our information into the world,

00:09:11.142 --> 00:09:15.095
and think about smart environments
that can adapt to us physically.

00:09:15.119 --> 00:09:16.683
So with that, I will leave you.

00:09:16.707 --> 00:09:17.858
Thank you very much.

00:09:17.882 --> 00:09:21.474
(Applause)

