WEBVTT
Kind: captions
Language: pt

00:00:00.000 --> 00:00:07.000
Tradutor: Leonardo Silva
Revisor: leonardo ost bento

00:00:12.450 --> 00:00:16.420
Quando criança, eu era
fundamentalmente "nerd".

00:00:17.250 --> 00:00:19.316
Acho que alguns de vocês também eram.

00:00:19.340 --> 00:00:20.346
(Risos)

00:00:20.370 --> 00:00:23.796
E o senhor, que deu a risada mais alta,
provavelmente ainda é.

00:00:23.820 --> 00:00:25.916
(Risos)

00:00:25.930 --> 00:00:29.496
Cresci numa cidadezinha nas planícies
empoeiradas do norte do Texas,

00:00:29.520 --> 00:00:32.836
filho de um xerife
que era filho de um pastor.

00:00:32.850 --> 00:00:35.200
Me meter em confusão não era uma opção.

00:00:35.740 --> 00:00:39.116
Então, comecei a ler livros
de cálculo pra me distrair.

00:00:39.140 --> 00:00:40.606
(Risos)

00:00:40.620 --> 00:00:42.340
Você também!

00:00:42.360 --> 00:00:46.156
Isso me levou a criar um lêiser,
um computador e protótipos de foguete,

00:00:46.180 --> 00:00:49.640
o que me levou a inventar combustível
de foguete em meu quarto.

00:00:49.660 --> 00:00:53.336
Bem, em termos científicos,

00:00:53.350 --> 00:00:56.716
chamamos isso de "péssima ideia".

00:00:56.740 --> 00:00:57.956
(Risos)

00:00:57.980 --> 00:01:00.156
Mais ou menos na mesma época,

00:01:00.180 --> 00:01:02.570
"2001: Uma odisseia no espaço",
de Stanley Kubrick,

00:01:02.590 --> 00:01:06.030
estreou nos cinemas
e minha vida mudou para sempre.

00:01:06.050 --> 00:01:10.646
Adorei tudo naquele filme,
principalmente o HAL 9000.

00:01:10.660 --> 00:01:12.746
HAL era um computador consciente,

00:01:12.760 --> 00:01:17.776
projetado para conduzir
a nave Discovery da Terra até Júpiter.

00:01:17.790 --> 00:01:19.840
HAL também era um personagem imperfeito,

00:01:19.860 --> 00:01:24.570
pois, no fim, decidiu priorizar
a missão em vez da vida humana.

00:01:24.590 --> 00:01:26.636
Bem, HAL era um personagem de ficção,

00:01:26.650 --> 00:01:29.316
mas, mesmo assim, 
traz à tona nossos medos,

00:01:29.330 --> 00:01:31.556
medo de sermos subjugados

00:01:31.580 --> 00:01:34.596
por uma inteligência artificial
sem sentimentos,

00:01:34.620 --> 00:01:37.000
indiferente à nossa humanidade.

00:01:37.540 --> 00:01:40.176
Acredito que esses medos
não têm fundamento.

00:01:40.190 --> 00:01:44.480
De fato, vivemos numa época incrível
da história da humanidade,

00:01:44.500 --> 00:01:49.556
em que, por nos recursarmos a aceitar
as limitações do nosso corpo e mente,

00:01:49.580 --> 00:01:54.886
criamos máquinas graciosas
e de linda e primorosa complexidade,

00:01:54.910 --> 00:01:59.416
que vão ampliar a experiência humana
de formas além do que imaginamos.

00:01:59.440 --> 00:02:02.116
Depois de trabalhar na Academia
da Força Aérea americana

00:02:02.130 --> 00:02:05.746
e chegar ao Comando Espacial americano,
me tornei engenheiro de sistemas,

00:02:05.770 --> 00:02:08.556
e recentemente estou ajudando
num problema de engenharia

00:02:08.580 --> 00:02:11.066
relacionado à missão da NASA a Marte.

00:02:11.080 --> 00:02:13.590
Bem, nos voos espaciais à Lua,

00:02:13.624 --> 00:02:16.760
podemos contar com o apoio
do controle de missão em Houston,

00:02:16.760 --> 00:02:18.836
que monitora todos os aspectos do voo.

00:02:18.836 --> 00:02:22.292
Porém, Marte fica 200 vezes
mais distante de nós que a Lua

00:02:22.413 --> 00:02:25.933
e, consequentemente,
leva em média 13 minutos

00:02:25.953 --> 00:02:28.796
para que um sinal de comunicação
viaje da Terra a Marte.

00:02:28.820 --> 00:02:32.220
Se houver algum problema,
não dá tempo de resolver.

00:02:32.550 --> 00:02:35.166
Então, uma solução razoável de engenharia

00:02:35.180 --> 00:02:40.726
é colocarmos o controle de missão
dentro da nave Orion.

00:02:40.750 --> 00:02:43.616
Outra ideia fascinante 
no projeto da missão

00:02:43.630 --> 00:02:46.576
é colocar robôs humanoides
na superfície de Marte

00:02:46.590 --> 00:02:48.516
antes de os humanos chegarem,

00:02:48.540 --> 00:02:50.196
para construírem instalações

00:02:50.220 --> 00:02:54.050
e depois atuarem como colaboradores
da equipe científica.

00:02:55.120 --> 00:02:57.866
Bom, de uma perspectiva de engenharia,

00:02:57.880 --> 00:03:01.156
ficou bem claro pra mim
que o que eu precisava projetar

00:03:01.180 --> 00:03:03.356
era uma inteligência artificial

00:03:03.380 --> 00:03:05.756
moderna, colaborativa
e com habilidade social.

00:03:05.780 --> 00:03:10.076
Em outras palavras, eu precisava criar
algo bem parecido com o HAL,

00:03:10.100 --> 00:03:12.516
mas sem suas tendências homicidas.

00:03:12.540 --> 00:03:13.900
(Risos)

00:03:14.700 --> 00:03:16.556
Mas espera um pouco:

00:03:16.580 --> 00:03:20.476
será mesmo possível criar
uma inteligência artificial assim?

00:03:20.500 --> 00:03:21.956
Na verdade, é possível sim.

00:03:21.980 --> 00:03:25.216
De muitas formas, trata-se de um desafio
complexo de engenharia,

00:03:25.236 --> 00:03:26.716
com alguns elementos de IA;

00:03:26.740 --> 00:03:31.436
não da ideia mirabolante
de criar uma IA repulsiva.

00:03:31.460 --> 00:03:36.516
Parafraseando Alan Turing,
não quero criar uma máquina consciente.

00:03:36.546 --> 00:03:38.116
Não vou criar um HAL.

00:03:38.140 --> 00:03:43.826
Só quero criar um cérebro simples,
algo que cause a ilusão de inteligência.

00:03:44.820 --> 00:03:47.956
A arte e a ciência da computação
evoluíram bastante

00:03:47.980 --> 00:03:49.476
desde que HAL surgiu no cinema,

00:03:49.500 --> 00:03:52.716
e imagino que, se o Dr. Chandra,
seu inventor, estivesse aqui hoje,

00:03:52.740 --> 00:03:55.076
ele teria um monte de perguntas pra nós.

00:03:55.100 --> 00:03:57.196
Será que conseguimos

00:03:57.220 --> 00:04:01.236
pegar um sistema com milhões
e milhões de dispositivos,

00:04:01.260 --> 00:04:02.716
entrar com os dados,

00:04:02.740 --> 00:04:04.996
prever suas falhas
e nos anteciparmos a elas?

00:04:05.020 --> 00:04:06.086
Sim.

00:04:06.100 --> 00:04:09.326
Será possível criar sistemas que conversem
conosco em nossa língua?

00:04:09.340 --> 00:04:10.316
Sim.

00:04:10.340 --> 00:04:13.616
Será possível criar sistemas
que reconheçam objetos e emoções,

00:04:13.640 --> 00:04:17.036
que reajam emotivamente,
usem de estratégia e até leitura labial?

00:04:17.060 --> 00:04:18.026
Sim.

00:04:18.050 --> 00:04:20.476
Será possível criar um sistema
que estabeleça metas,

00:04:20.500 --> 00:04:24.116
faça planos e aprenda
ao longo do caminho?

00:04:24.140 --> 00:04:25.276
Sim.

00:04:25.300 --> 00:04:28.646
Será possível criarmos sistemas
com capacidade cognitiva?

00:04:28.670 --> 00:04:30.236
Estamos aprendendo a fazer isso.

00:04:30.260 --> 00:04:34.240
Será possível criarmos sistemas
que tenham princípios éticos e morais?

00:04:34.270 --> 00:04:36.630
Precisamos aprender a fazer isso.

00:04:37.120 --> 00:04:38.556
Então, digamos, por exemplo,

00:04:38.580 --> 00:04:41.476
que seja possível criar
uma inteligência artificial assim,

00:04:41.500 --> 00:04:43.526
para esse tipo de missão e para outras.

00:04:43.550 --> 00:04:47.636
A próxima pergunta que precisamos
fazer é: devemos temer essa inteligência?

00:04:47.646 --> 00:04:52.546
Bom, toda nova tecnologia
traz algum tipo de preocupação.

00:04:52.580 --> 00:04:54.316
Quando surgiram os carros,

00:04:54.340 --> 00:04:58.276
as pessoas acharam que veríamos
a destruição da família.

00:04:58.290 --> 00:05:01.026
Quando surgiram os telefones,

00:05:01.040 --> 00:05:03.996
as pessoas temiam que ele destruiria
a socialização entre as pessoas.

00:05:04.020 --> 00:05:07.846
Quando vimos a palavra 
escrita se espalhar,

00:05:07.870 --> 00:05:10.406
acharam que perderíamos
nossa capacidade de memorizar.

00:05:10.420 --> 00:05:12.556
Tudo isso é de certa forma verdade,

00:05:12.580 --> 00:05:14.906
mas também é verdade que essas tecnologias

00:05:14.930 --> 00:05:18.396
nos possibilitaram ampliar
a experiência humana

00:05:18.420 --> 00:05:20.590
de formas profundas.

00:05:21.660 --> 00:05:23.940
Vou explicar melhor.

00:05:24.940 --> 00:05:29.676
Não temo a criação de uma IA assim,

00:05:29.700 --> 00:05:33.516
porque ela vai acabar assimilando
alguns dos nossos valores.

00:05:33.540 --> 00:05:37.036
Pensem só: criar um sistema cognitivo
é fundamentalmente diferente

00:05:37.060 --> 00:05:40.356
de criar um sistema complexo
e tradicional do passado.

00:05:40.380 --> 00:05:42.836
Não a programamos. Nós lhe ensinamos.

00:05:42.860 --> 00:05:45.516
Para ensinar um sistema
a reconhecer flores,

00:05:45.540 --> 00:05:48.496
mostro a ele milhares de flores
do tipo de que gosto.

00:05:48.510 --> 00:05:50.766
Para ensinar um sistema a jogar...

00:05:50.780 --> 00:05:52.820
eu faria isso, e vocês também...

00:05:54.420 --> 00:05:56.460
Eu gosto de jogos. Qual é!

00:05:57.260 --> 00:06:00.046
Para ensinar um sistema
a jogar, por exemplo, o "Go",

00:06:00.060 --> 00:06:02.750
eu o faria jogá-lo várias vezes,
mas, no processo,

00:06:02.770 --> 00:06:06.316
eu também o ensinaria a diferenciar
um jogo bom de um jogo ruim.

00:06:06.340 --> 00:06:10.036
Se quisesse criar um assistente jurídico
com inteligência artificial,

00:06:10.060 --> 00:06:13.080
eu lhe ensinaria algumas leis,
mas, ao mesmo tempo,

00:06:13.100 --> 00:06:17.620
colocaria nele o senso de compaixão
e de justiça que acompanham essas leis.

00:06:18.300 --> 00:06:21.356
Em termos científicos,
é o que chamamos de situação fática,

00:06:21.380 --> 00:06:23.396
e eis o mais importante:

00:06:23.420 --> 00:06:24.876
ao criar essas máquinas,

00:06:24.900 --> 00:06:28.316
estamos ensinando a elas
algo dos nossos valores.

00:06:28.340 --> 00:06:31.426
Sendo assim, eu confio
na inteligência artificial

00:06:31.440 --> 00:06:35.140
tanto quanto num humano
bem treinado; quem sabe até mais.

00:06:35.740 --> 00:06:36.976
Talvez vocês perguntem:

00:06:36.990 --> 00:06:42.566
"E quanto a agentes maliciosos,
tipo, uma ONG bem financiada?"

00:06:43.050 --> 00:06:46.906
Não temo uma inteligência artificial
nas mãos de um "lobo solitário".

00:06:46.920 --> 00:06:49.050
Obviamente, não podemos nos proteger

00:06:49.070 --> 00:06:51.420
contra todos os possíveis
atos de violência,

00:06:51.450 --> 00:06:56.746
mas a verdade é que um sistema assim
requer treinamento sólido e inteligente

00:06:56.770 --> 00:06:59.066
muito além das capacidades 
de um indivíduo.

00:06:59.090 --> 00:07:00.156
Além disso,

00:07:00.180 --> 00:07:03.636
a coisa vai além de simplesmente
injetar um vírus virtual no mundo,

00:07:03.660 --> 00:07:06.766
pressionando um botão e, de repente,
propagando-o por toda parte,

00:07:06.780 --> 00:07:09.186
fazendo notebooks explodirem por aí.

00:07:09.210 --> 00:07:13.656
Esses tipos de coisas são bem maiores,
e certamente os veremos chegar.

00:07:14.200 --> 00:07:17.396
Se eu temo que uma
inteligência artificial assim

00:07:17.420 --> 00:07:19.980
possa ameaçar toda a espécie humana?

00:07:20.000 --> 00:07:24.430
Ao assistir a filmes
como "Matrix", "Metrópolis",

00:07:24.450 --> 00:07:27.556
"O Exterminador do Futuro",
ou a séries como "Westworld",

00:07:27.570 --> 00:07:29.836
vemos que todos falam desse tipo de medo.

00:07:29.860 --> 00:07:34.046
No livro "Superintelligence",
do filósofo Nick Bostrom,

00:07:34.060 --> 00:07:35.716
ele aborda esse tema

00:07:35.740 --> 00:07:39.636
e explica que uma superinteligência
pode não só ser perigosa,

00:07:39.660 --> 00:07:43.636
mas também representar uma ameaça
à existência da espécie humana.

00:07:43.660 --> 00:07:47.446
O argumento básico do Dr. Bostrom
é o de que esses sistemas

00:07:47.460 --> 00:07:51.916
vão acabar tendo tamanha
sede por informação

00:07:51.940 --> 00:07:54.836
que vão talvez aprender a aprender

00:07:54.860 --> 00:07:57.476
e vão acabar descobrindo
que podem ter objetivos

00:07:57.500 --> 00:07:59.720
que são contrários 
às necessidades humanas.

00:07:59.750 --> 00:08:01.676
O Dr. Bostrom têm alguns seguidores.

00:08:01.700 --> 00:08:06.020
Pessoas como Elon Musk
e Stephen Hawking concordam com ele.

00:08:06.630 --> 00:08:11.960
Com todo o respeito
a essas mentes brilhantes,

00:08:11.980 --> 00:08:14.276
creio que estejam
fundamentalmente equivocados.

00:08:14.300 --> 00:08:17.476
São muitos os aspectos do argumento
do Dr. Bostrom para discutir

00:08:17.500 --> 00:08:19.496
e não tenho tenho para falar sobre todos,

00:08:19.510 --> 00:08:22.356
mas, rapidamente, pensem no seguinte:

00:08:22.380 --> 00:08:26.026
ser "super" no saber é bem diferente
de ser "super" no fazer.

00:08:26.040 --> 00:08:28.086
HAL foi uma ameaça
à tripulação da Discovery

00:08:28.100 --> 00:08:32.366
só porque comandava
todos os aspectos da nave.

00:08:32.380 --> 00:08:34.926
Assim teria que ser
com uma superinteligência.

00:08:34.940 --> 00:08:37.376
Ela teria que ter controle
sobre todo o nosso mundo.

00:08:37.400 --> 00:08:40.216
Isso é coisa da Skynet, do filme
"O Exterminador do Futuro",

00:08:40.240 --> 00:08:43.556
uma superinteligência
que controlava a vontade humana,

00:08:43.570 --> 00:08:47.426
que controlava qualquer dispositivo
em qualquer parte do mundo.

00:08:47.450 --> 00:08:51.056
Em termos práticos, isso não vai rolar.

00:08:51.070 --> 00:08:54.146
Não estamos criando IAs
que controlem o clima,

00:08:54.170 --> 00:08:55.556
que controlem as marés,

00:08:55.580 --> 00:08:58.956
ou que comandem a nós,
humanos inconstantes e caóticos.

00:08:58.980 --> 00:09:02.876
Além do mais, se uma inteligência
artificial dessas existisse,

00:09:02.900 --> 00:09:05.836
ela teria que competir
nas economias mundiais

00:09:05.860 --> 00:09:08.380
e, portanto, competir 
conosco por recursos.

00:09:08.890 --> 00:09:12.156
No fim... não contem isso ao Siri...

00:09:12.176 --> 00:09:13.636
sempre é possível desligá-la.

00:09:13.660 --> 00:09:15.780
(Risos)

00:09:17.090 --> 00:09:22.066
Estamos numa jornada incrível
de coevolução com nossas máquinas.

00:09:22.090 --> 00:09:27.226
Não seremos no futuro
os mesmos humanos que somos hoje.

00:09:27.240 --> 00:09:30.396
Preocupar-se agora com o surgimento
de uma superinteligência

00:09:30.420 --> 00:09:33.476
é, de várias maneiras, 
uma distração perigosa

00:09:33.500 --> 00:09:35.836
porque a ascensão da própria computação

00:09:35.860 --> 00:09:38.876
nos traz diversas questões,
humanas e sociais,

00:09:38.900 --> 00:09:41.100
com as quais agora devemos lidar.

00:09:41.120 --> 00:09:43.996
Como organizar a sociedade

00:09:44.020 --> 00:09:46.356
quando a demanda
por trabalho humano diminuir?

00:09:46.380 --> 00:09:50.196
Como disseminar entendimento
e educação por todo o planeta

00:09:50.220 --> 00:09:51.996
sem desrespeitar nossas diferenças?

00:09:52.020 --> 00:09:56.220
Como prolongar e melhorar a vida com esse
tipo de inteligência na área de saúde?

00:09:56.240 --> 00:10:01.496
Como posso usar a computação
para fazer com que cheguemos às estrelas?

00:10:01.510 --> 00:10:03.620
Isso é o mais empolgante.

00:10:04.120 --> 00:10:08.040
As oportunidades de usar a computação
para ampliar a experiência humana

00:10:08.060 --> 00:10:11.340
estão a nosso alcance, aqui e agora,

00:10:11.360 --> 00:10:13.340
e estamos apenas começando.

00:10:13.900 --> 00:10:15.136
Muito obrigado.

00:10:15.150 --> 00:10:17.208
(Aplausos)

