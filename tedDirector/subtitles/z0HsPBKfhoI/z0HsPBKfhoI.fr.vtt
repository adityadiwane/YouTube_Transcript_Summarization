WEBVTT
Kind: captions
Language: fr

00:00:00.000 --> 00:00:07.000
Traducteur: Morgane Quilfen
Relecteur: Fatima Zahra El Hafa

00:00:12.580 --> 00:00:16.420
Quand j'étais enfant,
j'étais l'exemple parfait de l'intello.

00:00:17.140 --> 00:00:19.316
Certains d'entre vous
devaient l'être aussi.

00:00:19.340 --> 00:00:20.556
(Rires)

00:00:20.580 --> 00:00:23.796
Et vous, monsieur, qui avez ri
le plus fort, devez encore l'être.

00:00:23.820 --> 00:00:26.076
(Rires)

00:00:26.100 --> 00:00:29.596
J'ai grandi dans une petite ville
des plaines poussiéreuses du Texas,

00:00:29.620 --> 00:00:32.956
fils d'un shérif
qui était fils de pasteur.

00:00:32.980 --> 00:00:34.940
Hors de question de m'attirer des ennuis.

00:00:35.860 --> 00:00:39.116
J'ai donc commencé à lire
des livres de calcul pour le plaisir.

00:00:39.140 --> 00:00:40.676
(Rires)

00:00:40.700 --> 00:00:42.396
Vous aussi.

00:00:42.420 --> 00:00:46.156
Cela m'a mené à créer un laser,
un ordinateur et des micro fusées

00:00:46.180 --> 00:00:49.180
et à faire du carburant
pour les fusées dans ma chambre.

00:00:49.780 --> 00:00:53.436
En termes scientifiques,

00:00:53.460 --> 00:00:56.716
cela s'appelle une très mauvaise idée.

00:00:56.740 --> 00:00:57.956
(Rires)

00:00:57.980 --> 00:01:00.156
A peu près au même moment,

00:01:00.180 --> 00:01:03.396
« 2001 : l'odyssée de l'espace »
de Stanley Kubrick est sorti

00:01:03.420 --> 00:01:05.620
et cela a changé ma vie.

00:01:06.100 --> 00:01:08.156
J'aimais tout dans ce film,

00:01:08.180 --> 00:01:10.716
en particulier HAL 9000.

00:01:10.740 --> 00:01:12.796
HAL était un ordinateur sensible

00:01:12.820 --> 00:01:15.276
conçu pour guider Discovery,
le vaisseau spatial,

00:01:15.300 --> 00:01:17.836
de la Terre à Jupiter.

00:01:17.860 --> 00:01:19.916
HAL était aussi un personnage imparfait

00:01:19.940 --> 00:01:21.660
puisqu'à la fin, il choisissait

00:01:21.660 --> 00:01:24.660
la valeur de la mission
plutôt que la vie humaine.

00:01:24.660 --> 00:01:26.756
HAL était un personnage de fiction

00:01:26.780 --> 00:01:29.436
mais il s'adresse malgré tout à nos peurs,

00:01:29.460 --> 00:01:31.556
nos peurs d'être assujettis

00:01:31.580 --> 00:01:34.596
par une intelligence artificielle
sans émotions

00:01:34.620 --> 00:01:36.580
qui est indifférente à notre humanité.

00:01:37.700 --> 00:01:40.276
Je crois que de telles peurs
sont infondées.

00:01:40.300 --> 00:01:42.996
En fait, nous vivons un moment remarquable

00:01:43.020 --> 00:01:44.556
dans l'histoire de l'humanité

00:01:44.580 --> 00:01:49.556
où, guidés par notre refus d'accepter
les limites de nos corps et esprit,

00:01:49.580 --> 00:01:51.276
nous construisons des machines

00:01:51.300 --> 00:01:54.916
d'une complexité et d'une grâce
exquises et magnifiques

00:01:54.940 --> 00:01:56.996
qui étendront l'expérience humaine

00:01:57.020 --> 00:01:58.700
bien au-delà de notre imagination.

00:01:59.540 --> 00:02:02.116
Après une carrière qui m'a mené
de l'Air Force Academy

00:02:02.140 --> 00:02:04.076
au Space Command aujourd'hui,

00:02:04.100 --> 00:02:05.826
je suis devenu ingénieur de systèmes

00:02:05.826 --> 00:02:08.556
et j'ai récemment participé
à un problème d'ingénierie

00:02:08.580 --> 00:02:11.156
associé à la mission sur Mars de la NASA.

00:02:11.180 --> 00:02:13.676
Pour les vols dans l'espace
jusqu'à la Lune,

00:02:13.700 --> 00:02:16.836
nous comptons sur le centre
de contrôle de mission de Houston

00:02:16.860 --> 00:02:18.836
pour surveiller tous les aspects du vol.

00:02:18.860 --> 00:02:22.396
Cependant, Mars est à une distance
200 fois plus importante

00:02:22.420 --> 00:02:25.636
et le résultat est
qu'il faut en moyenne 13 minutes

00:02:25.660 --> 00:02:28.796
pour qu'un signal voyage
de la Terre jusqu'à Mars.

00:02:28.820 --> 00:02:32.220
S'il y a un problème,
c'est beaucoup trop long.

00:02:32.660 --> 00:02:35.156
Une solution d'ingénierie raisonnable

00:02:35.180 --> 00:02:37.816
nous a poussés à placer
le centre de contrôle de mission

00:02:37.816 --> 00:02:40.796
entre les murs de l'engin spatial Orion.

00:02:40.820 --> 00:02:43.716
Une autre idée fascinante
dans le profil de la mission

00:02:43.740 --> 00:02:46.636
place les robots humanoïdes
sur la surface de Mars

00:02:46.660 --> 00:02:48.516
avant l'arrivée des humains,

00:02:48.540 --> 00:02:50.226
pour construire des infrastructures

00:02:50.226 --> 00:02:53.580
puis servir de membres collaboratifs
à l'équipe scientifique.

00:02:55.220 --> 00:02:57.956
En observant cela
d'un point de vue d'ingénieur,

00:02:57.980 --> 00:03:01.156
il m'est clairement apparu
que je devais concevoir

00:03:01.180 --> 00:03:03.976
une intelligence artificielle
intelligente, collaborative

00:03:03.976 --> 00:03:05.756
et socialement intelligente.

00:03:05.780 --> 00:03:10.076
En d'autres mots, je devais créer
quelque chose ressemblant à HAL

00:03:10.100 --> 00:03:12.516
mais sans tendances meurtrières.

00:03:12.540 --> 00:03:13.900
(Rires)

00:03:14.740 --> 00:03:16.556
Marquons un instant de pause.

00:03:16.580 --> 00:03:20.476
Est-il réellement possible de créer
une telle intelligence artificielle ?

00:03:20.500 --> 00:03:21.956
C'est possible.

00:03:21.980 --> 00:03:23.236
De bien des façons,

00:03:23.260 --> 00:03:25.236
c'est un problème d'ingénierie complexe

00:03:25.260 --> 00:03:26.716
avec un peu d'IA,

00:03:26.740 --> 00:03:31.436
non pas un problème inextricable d'AI
qui nécessite de l'ingénierie.

00:03:31.460 --> 00:03:34.116
Pour paraphraser Alan Turing,

00:03:34.140 --> 00:03:36.516
créer une machine sensible
ne m'intéresse pas.

00:03:36.540 --> 00:03:38.116
Je ne crée pas HAL.

00:03:38.140 --> 00:03:40.556
Tout ce que je veux
c'est un cerveau simple,

00:03:40.580 --> 00:03:43.700
quelque chose qui offre
l'illusion de l'intelligence.

00:03:44.820 --> 00:03:47.956
L'art et la science de l'informatique
ont beaucoup progressé

00:03:47.980 --> 00:03:49.476
depuis que HAL était au cinéma.

00:03:49.500 --> 00:03:52.786
J'imagine que si son inventeur,
Dr Chandra était présent aujourd'hui,

00:03:52.786 --> 00:03:55.076
il aurait beaucoup
de questions à nous poser.

00:03:55.100 --> 00:03:57.196
Est-il vraiment possible pour nous

00:03:57.220 --> 00:04:01.236
de prendre un système de millions
et millions d'appareils,

00:04:01.260 --> 00:04:02.716
lire leurs flux de données,

00:04:02.740 --> 00:04:04.996
prévoir leurs défaillances et agir avant ?

00:04:05.020 --> 00:04:06.236
Oui.

00:04:06.260 --> 00:04:09.436
Et créer des systèmes parlant
avec les humains dans leur langue ?

00:04:09.460 --> 00:04:10.676
Oui.

00:04:10.700 --> 00:04:13.676
Et créer des systèmes reconnaissant
les objets et les émotions,

00:04:13.700 --> 00:04:17.076
étant eux-mêmes émotifs,
jouant à des jeux, lisant sur les lèvres ?

00:04:17.100 --> 00:04:18.316
Oui.

00:04:18.340 --> 00:04:20.576
Et créer des systèmes
établissant des objectifs,

00:04:20.576 --> 00:04:24.116
mettant des plans en œuvre
et apprenant au passage ?

00:04:24.140 --> 00:04:25.356
Oui.

00:04:25.380 --> 00:04:28.716
Et créer des systèmes
qui ont une théorie de l'esprit ?

00:04:28.740 --> 00:04:30.236
Nous apprenons à le faire.

00:04:30.260 --> 00:04:33.740
Et créer des systèmes ayant
des principes éthiques et moraux ?

00:04:34.300 --> 00:04:36.340
Nous devons apprendre à le faire.

00:04:37.180 --> 00:04:38.556
Acceptons un instant

00:04:38.580 --> 00:04:41.576
qu'il soit possible de créer
une telle intelligence artificielle

00:04:41.576 --> 00:04:43.636
pour ce genre de missions et d'autres.

00:04:43.660 --> 00:04:46.196
La question suivante
qu'il faut se poser est :

00:04:46.220 --> 00:04:47.676
devrions-nous la craindre ?

00:04:47.700 --> 00:04:49.676
Toute nouvelle technologie

00:04:49.700 --> 00:04:52.596
entraîne de l'inquiétude.

00:04:52.620 --> 00:04:54.316
Au début des voitures,

00:04:54.340 --> 00:04:58.356
les gens se lamentaient que nous voyions
la destruction de la famille.

00:04:58.380 --> 00:05:01.076
A l'arrivée des téléphones,

00:05:01.100 --> 00:05:03.996
les gens craignaient
la fin de toute conversation civile.

00:05:04.020 --> 00:05:07.956
À un moment donné, les mots écrits
sont devenus omniprésents,

00:05:07.980 --> 00:05:10.476
les gens pensaient
que nous perdrions notre mémoire.

00:05:10.500 --> 00:05:12.556
Toutes ces choses sont en partie vraies,

00:05:12.580 --> 00:05:14.996
mais ces technologies

00:05:15.020 --> 00:05:18.396
ont aussi apporté des choses
qui ont étendu l'expérience humaine

00:05:18.420 --> 00:05:20.300
de façon profonde.

00:05:21.660 --> 00:05:23.940
Allons un peu plus loin.

00:05:24.940 --> 00:05:29.676
Je n'ai pas peur de la création
d'une telle IA

00:05:29.700 --> 00:05:33.516
car elle finira par incarner
certaines de nos valeurs.

00:05:33.540 --> 00:05:34.370
Considérez ceci :

00:05:34.370 --> 00:05:37.060
créer un système cognitif
est fondamentalement différent

00:05:37.060 --> 00:05:40.356
de créer un système traditionnel
plein de logiciels comme auparavant.

00:05:40.380 --> 00:05:42.836
Nous ne les programmons pas,
nous leur apprenons.

00:05:42.860 --> 00:05:45.516
Afin d'apprendre à un système
à reconnaître des fleurs,

00:05:45.540 --> 00:05:48.556
je lui montre des milliers
de fleurs que j'aime.

00:05:48.580 --> 00:05:50.926
Afin d'apprendre à un système
à jouer à un jeu --

00:05:50.926 --> 00:05:52.820
Je le ferais, vous aussi.

00:05:54.420 --> 00:05:56.460
J'aime les fleurs, allez.

00:05:57.260 --> 00:06:00.116
Pour apprendre à un système
à jouer au jeu de Go,

00:06:00.140 --> 00:06:02.296
je devrais jouer des milliers
de parties de Go

00:06:02.296 --> 00:06:03.876
mais au passage, je lui apprends

00:06:03.900 --> 00:06:06.316
à discerner un bon mouvement d'un mauvais.

00:06:06.340 --> 00:06:10.036
Si je veux créer une intelligence
artificielle assistante juridique,

00:06:10.060 --> 00:06:11.836
je lui apprendrais des corpus de loi

00:06:11.860 --> 00:06:14.716
mais en même temps, je lie cela

00:06:14.740 --> 00:06:17.620
à la compassion et la justice
qui font partie de la loi.

00:06:18.380 --> 00:06:21.426
En termes scientifiques,
cela s'appelle des vérités fondamentales

00:06:21.426 --> 00:06:23.396
et voici ce qui est important :

00:06:23.420 --> 00:06:24.876
en produisant ces machines,

00:06:24.900 --> 00:06:28.316
nous leur enseignons
une partie de nos valeurs.

00:06:28.340 --> 00:06:31.476
Pour cela, j'ai autant confiance,
si ce n'est pas plus,

00:06:31.500 --> 00:06:35.140
en une intelligence artificielle
qu'en un être humain bien entraîné.

00:06:35.900 --> 00:06:37.116
Vous allez demander :

00:06:37.140 --> 00:06:39.756
qu'en est-il des hors-la-loi,

00:06:39.780 --> 00:06:43.116
des quelques organisations
non gouvernementales bien financées ?

00:06:43.120 --> 00:06:45.360
Je n'ai pas peur
d'une intelligence artificielle

00:06:45.360 --> 00:06:46.980
dans les mains d'un seul individu.

00:06:46.980 --> 00:06:51.516
Nous ne pouvons pas nous protéger
des actes de violence aveugles,

00:06:51.540 --> 00:06:53.676
mais un tel système

00:06:53.700 --> 00:06:56.796
requiert un entraînement
substantiel et raffiné

00:06:56.820 --> 00:06:59.116
qui va bien au-delà
des ressources d'un individu.

00:06:59.140 --> 00:07:00.356
En outre,

00:07:00.380 --> 00:07:03.620
il s'agit de bien plus que d'injecter
un virus internet au monde

00:07:03.620 --> 00:07:06.946
où en appuyant sur une touche,
il se retrouve à des millions d'endroits

00:07:06.946 --> 00:07:09.236
et des ordinateurs explosent
un peu partout.

00:07:09.260 --> 00:07:12.076
Ce genre de substances
sont bien plus grandes

00:07:12.100 --> 00:07:13.815
et nous les verrons sûrement venir.

00:07:14.340 --> 00:07:17.396
Ai-je peur qu'une telle
intelligence artificielle

00:07:17.420 --> 00:07:19.380
menace l'humanité ?

00:07:20.100 --> 00:07:24.476
Si vous regardez des films
tels que « Matrix », « Metropolis »,

00:07:24.500 --> 00:07:27.676
« Terminator » ou des séries
telles que « Westworld »,

00:07:27.700 --> 00:07:29.836
ils évoquent tous ce genre de peur.

00:07:29.860 --> 00:07:34.156
Dans le livre « Superintelligence »
du philosophe Nick Bostrom,

00:07:34.180 --> 00:07:35.716
il évoque ce thème

00:07:35.740 --> 00:07:39.756
et note qu'une super-intelligence
pourrait être non seulement dangereuse,

00:07:39.780 --> 00:07:43.636
mais représenter une menace existentielle
envers l'humanité tout entière.

00:07:43.660 --> 00:07:45.876
L'argument fondamental
du docteur Bostrom

00:07:45.900 --> 00:07:48.636
est que de tels systèmes finiront

00:07:48.660 --> 00:07:51.916
par avoir une telle soif
insatiable d'informations

00:07:51.940 --> 00:07:54.836
qu'ils apprendront peut-être à apprendre

00:07:54.860 --> 00:07:57.476
et finiront par découvrir
qu'ils ont des objectifs

00:07:57.500 --> 00:07:59.796
qui sont contraires aux besoins humains.

00:07:59.820 --> 00:08:01.676
Le docteur Bostrom a des partisans.

00:08:01.700 --> 00:08:06.020
Il est soutenu par des gens
tels qu'Elon Musk et Stephen Hawking.

00:08:06.700 --> 00:08:09.100
Avec tout le respect dû

00:08:09.980 --> 00:08:11.996
à ces brillants esprits,

00:08:12.020 --> 00:08:14.276
je crois qu'ils ont fondamentalement tort.

00:08:14.300 --> 00:08:17.486
L'argument du Dr Bostrom
contient nombre d'éléments à décortiquer

00:08:17.500 --> 00:08:19.636
et je n'ai pas le temps pour tous,

00:08:19.660 --> 00:08:22.356
mais, brièvement, considérez ceci :

00:08:22.380 --> 00:08:26.116
un super-savoir est très différent
d'une super-action.

00:08:26.140 --> 00:08:28.036
HAL était une menace pour l'équipage

00:08:28.060 --> 00:08:32.476
uniquement s'il commandait
tous les aspects de Discovery.

00:08:32.500 --> 00:08:34.996
C'en est de même
pour une super-intelligence.

00:08:35.020 --> 00:08:37.556
Il lui faudrait des réplications
dans le monde entier.

00:08:37.556 --> 00:08:40.356
C'est le truc avec Skynet
dans le fim « Terminator »

00:08:40.380 --> 00:08:42.236
où nous avons une super-intelligence

00:08:42.260 --> 00:08:43.686
commandant la volonté humaine,

00:08:43.686 --> 00:08:47.516
contrôlant tous les appareils
à tous les coins du monde.

00:08:47.540 --> 00:08:48.996
D'un point de vue pratique,

00:08:49.020 --> 00:08:51.116
cela n'arrivera pas.

00:08:51.140 --> 00:08:54.196
Nous ne créons pas d'AI
qui contrôle la météo,

00:08:54.220 --> 00:08:55.556
qui dirige les vagues,

00:08:55.580 --> 00:08:58.956
qui nous commande,
nous humains capricieux et chaotiques.

00:08:58.980 --> 00:09:02.876
En outre, si une telle
intelligence artificielle existait,

00:09:02.900 --> 00:09:05.836
elle devrait rivaliser
avec les économies humaines

00:09:05.860 --> 00:09:08.380
et se battre contre nous
pour des ressources.

00:09:09.020 --> 00:09:10.236
Au final --

00:09:10.260 --> 00:09:11.500
ne le dites pas à Siri --

00:09:12.260 --> 00:09:13.826
on peut toujours les débrancher.

00:09:13.826 --> 00:09:15.780
(Rires)

00:09:17.180 --> 00:09:19.636
Nous participons à un voyage incroyable

00:09:19.660 --> 00:09:22.156
de coévolution avec nos machines.

00:09:22.180 --> 00:09:24.676
Les humains que nous sommes aujourd'hui

00:09:24.700 --> 00:09:27.236
ne sont pas les humains de demain.

00:09:27.260 --> 00:09:30.396
S'inquiéter maintenant
de l'essor d'une super-intelligence

00:09:30.420 --> 00:09:33.476
est, de bien des façons,
une distraction dangereuse

00:09:33.500 --> 00:09:35.836
car l'essor de l'informatique lui-même

00:09:35.860 --> 00:09:38.876
nous amène nombre de problèmes
humains et sociétaux

00:09:38.900 --> 00:09:40.540
dont nous devons nous occuper.

00:09:41.180 --> 00:09:43.996
Comment organiser au mieux la société

00:09:44.020 --> 00:09:46.356
quand le besoin
de travail humain diminue ?

00:09:46.380 --> 00:09:50.196
Comment apporter compréhension
et éducation à travers le monde

00:09:50.220 --> 00:09:51.996
tout en respectant les différences ?

00:09:52.020 --> 00:09:54.510
Comment étendre
et améliorer la vie humaine

00:09:54.510 --> 00:09:56.300
grâce à la médecine cognitive ?

00:09:56.300 --> 00:09:59.156
Comment utiliser l'informatique

00:09:59.180 --> 00:10:00.940
pour nous envoyer dans les étoiles ?

00:10:01.580 --> 00:10:03.620
C'est cela qui est excitant.

00:10:04.220 --> 00:10:08.216
Les opportunités d'utiliser l'informatique
pour faire progresser l'expérience humaine

00:10:08.216 --> 00:10:09.556
sont à notre portée,

00:10:09.580 --> 00:10:11.436
ici et maintenant,

00:10:11.460 --> 00:10:13.140
et nous ne faisons que commencer.

00:10:14.100 --> 00:10:15.316
Merci beaucoup.

00:10:15.340 --> 00:10:19.626
(Applaudissements)

