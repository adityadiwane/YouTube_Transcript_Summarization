WEBVTT
Kind: captions
Language: cs

00:00:00.000 --> 00:00:07.000
Překladatel: Kateřina Jabůrková
Korektor: Richard Cvach

00:00:12.580 --> 00:00:16.420
V dětství jsem byl typickým nerdem,

00:00:17.140 --> 00:00:19.316
myslím, že pár z vás taky.

00:00:19.340 --> 00:00:20.556
(smích)

00:00:20.580 --> 00:00:23.796
A vy, pane, který se smějete nejhlasitěji,
jím ještě možná jste.

00:00:23.820 --> 00:00:26.076
(smích)

00:00:26.100 --> 00:00:29.596
Vyrůstal jsem v malém městě 
na prašných pláních severního Texasu,

00:00:29.620 --> 00:00:32.956
syn šerifa, který byl synem kněze.

00:00:32.980 --> 00:00:34.900
Moc rošťáren jsem provádět nemohl.

00:00:35.860 --> 00:00:39.116
A tak jsem si z nudy 
začal číst o integrálech.

00:00:39.140 --> 00:00:40.676
(smích)

00:00:40.700 --> 00:00:42.396
Vy také, že?

00:00:42.420 --> 00:00:46.156
To mě dovedlo k postavení modelu
laseru a počítače a rakety

00:00:46.180 --> 00:00:49.180
a to zase k vytvoření raketového paliva
v mé ložnici.

00:00:49.780 --> 00:00:53.436
Ve vědeckých termínech tomu říkáme

00:00:53.460 --> 00:00:56.716
velmi špatný nápad.

00:00:56.740 --> 00:00:57.956
(smích)

00:00:57.980 --> 00:01:00.156
Ve stejném čase se do kin dostala

00:01:00.180 --> 00:01:03.396
"2001: Vesmírná odysea" Stanleyho Kubricka

00:01:03.420 --> 00:01:05.620
a můj život se navždy změnil.

00:01:06.100 --> 00:01:08.156
Miloval jsem vše o tom filmu,

00:01:08.180 --> 00:01:10.716
zejména Hala 9000.

00:01:10.740 --> 00:01:12.796
HAL byl cítícím počítačem,

00:01:12.820 --> 00:01:15.276
navrženým k navádění 
vesmírné lodě Discovery

00:01:15.300 --> 00:01:17.836
ze Země na Jupiter.

00:01:17.860 --> 00:01:19.916
HAL měl také charakterovou vadu,

00:01:19.940 --> 00:01:24.220
protože na konci si cenil více mise 
než lidského života.

00:01:24.660 --> 00:01:26.756
HAL byl fiktivní postavou,

00:01:26.780 --> 00:01:29.436
nicméně hovoří k našemu strachu,

00:01:29.460 --> 00:01:31.556
strachu z podřízení se

00:01:31.580 --> 00:01:34.596
nějaké necítící, umělé inteligenci,

00:01:34.620 --> 00:01:36.580
která je lhostejná k našemu lidství.

00:01:37.700 --> 00:01:40.276
Věřím, že tento strach je neopodstatněný.

00:01:40.300 --> 00:01:42.996
Nalézáme se vskutku

00:01:43.020 --> 00:01:44.556
v pozoruhodném období,

00:01:44.580 --> 00:01:49.556
odmítnutí omezení našich těl a mysli

00:01:49.580 --> 00:01:51.276
nás vede ke stavbě výjimečných

00:01:51.300 --> 00:01:54.916
nádherně komplexních a půvabných strojů,

00:01:54.940 --> 00:01:56.996
které rozšíří lidskou zkušenost

00:01:57.020 --> 00:01:59.240
způsoby přesahujícími naši představivost.

00:01:59.540 --> 00:02:02.116
Po kariéře, která mne zavedla 
z letecké akademie

00:02:02.140 --> 00:02:04.076
přes Vesmírné velení až do dnešního dne,

00:02:04.100 --> 00:02:05.796
jsem se stal systémovým inženýrem

00:02:05.820 --> 00:02:08.556
a nedávno se nechal vtáhnout 
do inženýrského problému,

00:02:08.580 --> 00:02:11.156
souvisejícího s misí NASA na Mars.

00:02:11.180 --> 00:02:13.676
Ve vesmírných letech na Měsíc,

00:02:13.700 --> 00:02:16.836
můžeme spoléhat 
na řídicí středisko v Houstonu,

00:02:16.860 --> 00:02:18.836
které dohlíží na všechny aspekty letu.

00:02:18.860 --> 00:02:22.396
Nicméně Mars je 200krát dále

00:02:22.420 --> 00:02:25.636
a signálu průměrně trvá 13 minut

00:02:25.660 --> 00:02:28.796
cestovat ze Země na Mars.

00:02:28.820 --> 00:02:32.220
Pokud se objeví problém,
není zde dost času.

00:02:32.660 --> 00:02:35.156
A rozumné inženýrské řešení

00:02:35.180 --> 00:02:37.756
nám navrhuje umístit kontrolu mise

00:02:37.780 --> 00:02:40.796
mezi stěny vesmírné lodi Orion.

00:02:40.820 --> 00:02:43.716
Další fascinující myšlenka v profilu mise

00:02:43.740 --> 00:02:46.636
umisťuje humanoidní roboty na povrch Marsu

00:02:46.660 --> 00:02:48.516
před příletem samotných lidí,

00:02:48.540 --> 00:02:50.196
aby nejprve postavili zařízení

00:02:50.220 --> 00:02:53.580
a později sloužili jako členové
vědeckého týmu.

00:02:55.220 --> 00:02:57.956
Když se na to dívám pohledem inženýra,

00:02:57.980 --> 00:03:01.156
zdálo se mi jasné, že potřebuji navrhnout

00:03:01.180 --> 00:03:03.356
malou, spolupracující,

00:03:03.380 --> 00:03:05.756
sociálně inteligentní umělou inteligenci.

00:03:05.780 --> 00:03:10.076
Jinými slovy jsem potřeboval postavit
něco velmi podobného HALovi,

00:03:10.100 --> 00:03:12.516
ale bez vražedných tendencí.

00:03:12.540 --> 00:03:13.900
(smích)

00:03:14.740 --> 00:03:16.556
Zastavme se na chvilku.

00:03:16.580 --> 00:03:20.476
Je možné postavit 
takovou umělou inteligenci?

00:03:20.500 --> 00:03:21.956
Vlastně ano.

00:03:21.980 --> 00:03:23.236
Z mnoha pohledů

00:03:23.260 --> 00:03:25.236
je to složitý inženýrský problém

00:03:25.260 --> 00:03:26.716
s prvky AI,

00:03:26.740 --> 00:03:31.436
není to žádný otravný AI problém,
který potřebuje být vyřešen.

00:03:31.460 --> 00:03:34.116
Abych parafrázoval Alana Turinga,

00:03:34.140 --> 00:03:36.516
nezajímá mne postavení 
cítícího stroje.

00:03:36.540 --> 00:03:38.116
Nestavím HALa.

00:03:38.140 --> 00:03:40.556
Chci jen jednoduchý mozek,

00:03:40.580 --> 00:03:43.700
který poskytuje iluzi inteligence.

00:03:44.820 --> 00:03:47.330
Umění a věda o počítačích 
podstoupila dlouhou cestu

00:03:47.330 --> 00:03:49.476
od doby, kdy se HAL objevil 
na obrazovce

00:03:49.500 --> 00:03:52.716
a kdyby zde byl jeho tvůrce, Dr. Chandra,

00:03:52.740 --> 00:03:55.076
měl by pro nás mnoho otázek.

00:03:55.100 --> 00:03:57.196
Je pro nás opravdu možné

00:03:57.220 --> 00:04:01.236
číst datové toky v systému

00:04:01.260 --> 00:04:02.716
s miliony zařízení,

00:04:02.740 --> 00:04:04.996
předvídat jejich selhání 
a jednat v předstihu?

00:04:05.020 --> 00:04:06.150
Ano.

00:04:06.150 --> 00:04:09.436
Můžeme postavit systém, který
komunikuje s lidmi přirozeným jazykem?

00:04:09.460 --> 00:04:10.460
Ano.

00:04:10.460 --> 00:04:13.766
Můžeme postavit systém, který
rozlišuje objekty, identifikuje emoce,

00:04:13.766 --> 00:04:17.076
emoce samotné, hraje hry
a dokonce odečítá ze rtů?

00:04:17.100 --> 00:04:18.244
Ano.

00:04:18.244 --> 00:04:20.500
Můžeme postavit systém, 
který stanovuje cíle,

00:04:20.500 --> 00:04:24.116
provádí plány ve vztahu k těm cílům
a průběžně se učí?

00:04:24.140 --> 00:04:25.356
Ano.

00:04:25.380 --> 00:04:28.716
Můžeme postavit systémy,
které mají teorii mysli?

00:04:28.740 --> 00:04:30.236
Toto se učíme udělat.

00:04:30.260 --> 00:04:33.740
Můžeme postavit systémy, které
mají etické a morální základy?

00:04:34.300 --> 00:04:36.340
To se musíme naučit.

00:04:37.180 --> 00:04:38.556
Přijměme na okamžik to,

00:04:38.580 --> 00:04:41.476
že je možné postavit takovou
umělou inteligenci

00:04:41.500 --> 00:04:43.636
pro tento typ mise a další.

00:04:43.660 --> 00:04:46.196
Další otázka, kterou si musíme položit,

00:04:46.220 --> 00:04:47.676
zní, měli bychom se jí bát?

00:04:47.700 --> 00:04:49.676
Každá technologie sebou přináší

00:04:49.700 --> 00:04:52.596
určitou míru očekávání.

00:04:52.620 --> 00:04:54.316
Když jsme poprvé spatřili auta,

00:04:54.340 --> 00:04:58.356
lidé lamentovali, že uvidíme
rozpad rodiny.

00:04:58.380 --> 00:05:01.076
Když se objevily telefony,

00:05:01.100 --> 00:05:03.996
lidé se obávali, že to zničí
obyčejnou konverzaci.

00:05:04.020 --> 00:05:07.956
V určitém období jsme viděli
všudypřítomné psané slovo,

00:05:07.980 --> 00:05:10.476
lidé se obávali ztráty
schopnosti pamatovat si.

00:05:10.500 --> 00:05:12.556
Vše v sobě má zrnko pravdy,

00:05:12.604 --> 00:05:15.020
ale je také pravdou, že tyto technologie

00:05:15.020 --> 00:05:18.396
nám přinesly věci významným způsobem

00:05:18.420 --> 00:05:20.300
rozšiřující lidskou zkušenost.

00:05:21.660 --> 00:05:23.940
Pokročme trochu dále.

00:05:24.940 --> 00:05:29.676
Nebojím se vytvoření takové AI,

00:05:29.700 --> 00:05:33.516
protože bude ztělesňovat naše hodnoty.

00:05:33.540 --> 00:05:37.036
Zvažte: vytvoření kognitivního systému
je zcela odlišné

00:05:37.060 --> 00:05:40.356
od vytvoření tradičního softwarového
systému minulosti.

00:05:40.380 --> 00:05:42.836
Neprogramujeme je. 
Učíme je.

00:05:42.860 --> 00:05:45.516
Aby se systém naučil rozpoznávat květiny,

00:05:45.516 --> 00:05:48.532
ukážeme mu tisíce květin, které máme rádi.

00:05:48.532 --> 00:05:50.788
Abychom naučili systém hrát hru --

00:05:50.860 --> 00:05:52.820
Já bych to udělal. Vy taky.

00:05:54.420 --> 00:05:56.460
Mám rád květiny. No tak.

00:05:57.260 --> 00:06:00.116
Abychom naučili hrát systém hru jako Go,

00:06:00.140 --> 00:06:02.196
musel bych hrát tisíce her Go,

00:06:02.220 --> 00:06:03.876
ale v průběhu je také učím,

00:06:03.900 --> 00:06:06.316
jak rozpoznat dobrou hru od špatné.

00:06:06.340 --> 00:06:10.036
Pokud chci stvořit uměle inteligentního 
právního asistenta,

00:06:10.060 --> 00:06:11.836
naučím jej část práva,

00:06:11.860 --> 00:06:14.716
ale současně mu dávám

00:06:14.740 --> 00:06:17.620
smysl pro soucit a spravedlnost,
které jsou součástí práva.

00:06:18.380 --> 00:06:21.356
Terminologií vědy to nazýváme 
základní pravda

00:06:21.380 --> 00:06:23.396
a zde je to důležité:

00:06:23.420 --> 00:06:24.876
při vytváření těchto strojů

00:06:24.900 --> 00:06:28.316
jim učením dáváme smysl pro hodnoty.

00:06:28.340 --> 00:06:31.476
Věřím umělé inteligenci stejně,

00:06:31.500 --> 00:06:35.140
ne-li více, jako dobře vyškolené osobě.

00:06:35.900 --> 00:06:37.116
Můžete se ale zeptat,

00:06:37.140 --> 00:06:39.756
a co nezávislí agenti,

00:06:39.780 --> 00:06:43.116
třeba dobře financované
nevládní organizace?

00:06:43.140 --> 00:06:46.956
Nebojím se umělé inteligence
v rukou osamělého vlka.

00:06:46.980 --> 00:06:51.516
Je zřejmé, že se nemůžeme chránit
proti všem náhodným násilným činům,

00:06:51.540 --> 00:06:53.676
ale realita je systém, vyžadující

00:06:53.700 --> 00:06:56.796
hodně a přesný trénink,

00:06:56.820 --> 00:06:59.116
který je mimo možnosti jednotlivců.

00:06:59.140 --> 00:07:00.356
A navíc,

00:07:00.380 --> 00:07:03.636
je to mnohem více než jen vypustit
do světa internetový virus,

00:07:03.660 --> 00:07:06.756
který se stisknutím tlačítka ihned 
objeví na milionech míst

00:07:06.780 --> 00:07:09.236
a laptopy začnou všude vybuchovat.

00:07:09.260 --> 00:07:12.076
Tento typ prostředků je mnohem větší

00:07:12.100 --> 00:07:13.815
a určitě je uvidíme přicházet.

00:07:14.340 --> 00:07:17.396
Bojím se, že by takováto umělá inteligence

00:07:17.420 --> 00:07:19.380
mohla ohrozit celé lidstvo?

00:07:20.100 --> 00:07:24.476
Když shlédnete filmy 
jako "Matrix", "Metropolis"

00:07:24.500 --> 00:07:27.676
"Terminátor", seriály jako "Westworld,"

00:07:27.700 --> 00:07:29.836
všechny hovoří o tomto typu strachu.

00:07:29.860 --> 00:07:34.156
V knize "Superinteligence" 
filozofa Nicka Bostroma

00:07:34.180 --> 00:07:35.716
vyzvedává toto téma

00:07:35.740 --> 00:07:39.756
a poznamenává, že superinteligence
může být nejenom nebezpečná,

00:07:39.780 --> 00:07:43.636
ale může znamenat existenční hrozbu
celému lidstvu.

00:07:43.660 --> 00:07:45.876
Základním argumentem Dr. Bostroma je,

00:07:45.900 --> 00:07:48.636
že takový systém bude nakonec mít

00:07:48.660 --> 00:07:51.916
neukojitelný hlad po informacích,

00:07:51.940 --> 00:07:54.836
že se nakonec naučí jak se učit

00:07:54.860 --> 00:07:57.476
a zjistí, že mohou mít cíle,

00:07:57.500 --> 00:07:59.796
které jsou v protikladu 
s potřebami člověka.

00:07:59.820 --> 00:08:01.676
Dr. Bostrom má mnoho následovníků.

00:08:01.700 --> 00:08:06.020
Podporují jej lidé jako
Elon Musk a Stephen Hawking.

00:08:06.700 --> 00:08:09.100
Se vší úctou

00:08:09.980 --> 00:08:11.996
k těmto brilantním mozkům věřím,

00:08:12.020 --> 00:08:14.276
že se zásadně mýlí.

00:08:14.300 --> 00:08:17.476
Je zde mnoho částí argumentů
dr. Bostroma k diskusi

00:08:17.500 --> 00:08:19.636
a já nemám čas se jim všem věnovat,

00:08:19.660 --> 00:08:22.356
ale jen stručně zvažte toto:

00:08:22.380 --> 00:08:26.116
super vědění je velmi odlišné 
od super konání.

00:08:26.140 --> 00:08:28.036
HAL byl hrozbou posádce Discovery

00:08:28.060 --> 00:08:32.476
jen dokud ovládal všechny části Discovery.

00:08:32.500 --> 00:08:34.996
Tak by to mohlo být i se superinteligencí.

00:08:35.020 --> 00:08:37.516
Musel by to být vládce
celého našeho světa.

00:08:37.540 --> 00:08:40.356
To je Skynet z filmu "Terminátor",

00:08:40.380 --> 00:08:42.236
ve kterém byla superinteligence,

00:08:42.260 --> 00:08:43.636
která velela lidské vůli,

00:08:43.660 --> 00:08:47.516
která řídila každé zařízení,
v každém koutku světa.

00:08:47.540 --> 00:08:48.996
Prakticky,

00:08:49.020 --> 00:08:51.116
to se prostě nestane.

00:08:51.140 --> 00:08:54.196
Nestavíme AI, které ovládají počasí,

00:08:54.220 --> 00:08:55.556
řídí vlny,

00:08:55.580 --> 00:08:58.956
které řídí nás, rozmarné, chaotické lidi.

00:08:58.980 --> 00:09:02.876
Navíc pokud by taková 
umělá inteligence existovala,

00:09:02.900 --> 00:09:05.836
bude muset soupeřit 
s lidskými ekonomikami

00:09:05.860 --> 00:09:08.380
a soupeřit s námi o zdroje.

00:09:09.020 --> 00:09:10.236
Nakonec --

00:09:10.260 --> 00:09:11.500
neříkejte to Siri --

00:09:11.940 --> 00:09:13.706
vždycky je můžeme 
odpojit ze zásuvky.

00:09:13.706 --> 00:09:15.780
(smích)

00:09:17.180 --> 00:09:19.636
Jsme na neuvěřitelné cestě

00:09:19.660 --> 00:09:22.156
koevoluce s našimi stroji.

00:09:22.180 --> 00:09:24.676
Lidé dneška

00:09:24.700 --> 00:09:27.236
nejsou lidé, kteří budou v budoucnu.

00:09:27.260 --> 00:09:30.396
Obávat se nyní vzestupu superinteligence

00:09:30.420 --> 00:09:33.476
je v mnoha ohledech nebezpečné rozptýlení,

00:09:33.500 --> 00:09:35.836
protože sám vzestup počítačů

00:09:35.860 --> 00:09:38.876
sebou přináší mnohé 
lidské a společenské otázky,

00:09:38.900 --> 00:09:40.540
kterým se nyní musíme věnovat.

00:09:41.180 --> 00:09:43.996
Jak nejlépe zorganizovat společnost,

00:09:44.020 --> 00:09:46.356
když se zmenší potřeba lidské práce?

00:09:46.380 --> 00:09:50.196
Jak mohu přinést porozumění
a vzdělávání napříč zeměkoulí

00:09:50.220 --> 00:09:51.996
a stále respektovat naše odlišnosti?

00:09:52.020 --> 00:09:56.276
Jak mohu rozšířit a podpořit lidský život
pomocí kognitivní zdravotní péče?

00:09:56.300 --> 00:09:59.156
Jak mohu použít počítače,

00:09:59.180 --> 00:10:00.940
aby nám pomohly dostat se ke hvězdám?

00:10:01.580 --> 00:10:03.620
A to je vzrušující.

00:10:04.220 --> 00:10:06.556
Příležitosti využít počítače

00:10:06.580 --> 00:10:08.116
k lidskému pokroku

00:10:08.140 --> 00:10:09.556
jsou v našem dosahu,

00:10:09.580 --> 00:10:11.436
zde a nyní,

00:10:11.460 --> 00:10:13.140
a to jsme teprve na začátku.

00:10:14.100 --> 00:10:15.316
Děkuji.

00:10:15.340 --> 00:10:19.626
(potlesk)

