WEBVTT
Kind: captions
Language: ro

00:00:00.000 --> 00:00:07.000
Traducător: Denise RQ
Corector: Bianca-Ioanidia Mirea

00:00:12.580 --> 00:00:16.420
Când eram copil,
eram tocilar prin chintesență.

00:00:17.140 --> 00:00:19.316
Cred că și unii dintre voi
ați fost la fel.

00:00:19.340 --> 00:00:20.556
(Râsete)

00:00:20.580 --> 00:00:23.064
Și dumneavoastră domnule,
care ați râs cel mai tare,

00:00:23.089 --> 00:00:24.596
probabil că încă mai sunteți.

00:00:24.620 --> 00:00:26.076
(Râsete)

00:00:26.100 --> 00:00:29.596
Am crescut într-un mic oraș
în câmpiile prăfuite din nordul Texasului,

00:00:29.620 --> 00:00:32.956
fiu de șerif care a fost fiul unui pastor.

00:00:32.980 --> 00:00:34.900
A intra în belele nu era o opțiune.

00:00:35.860 --> 00:00:39.216
Așa că am început să citesc cărți
cu calcule ca sursă de distracție.

00:00:39.240 --> 00:00:40.676
(Râsete)

00:00:40.700 --> 00:00:42.396
Și tu la fel.

00:00:42.420 --> 00:00:46.256
Și asta m-a făcut să construiesc un laser
și un computer și să modelez rachete,

00:00:46.280 --> 00:00:49.879
iar apoi să fac combustibil
pentru rachete în dormitorul meu.

00:00:49.880 --> 00:00:56.716
Acum, în termeni științifici,
numim asta o idee foarte proastă.

00:00:56.740 --> 00:00:57.760
(Râsete)

00:00:57.800 --> 00:01:01.061
Cam pe atunci, „2001: Odiseea spațială”
a lui Stanley Kubrick

00:01:01.061 --> 00:01:02.996
apăruse în cinematografe,

00:01:03.420 --> 00:01:05.620
iar viața mi s-a schimbat
pentru totdeauna.

00:01:06.100 --> 00:01:10.716
Am iubit absolut totul legat de acel film,
în special pe HAL 9000.

00:01:10.740 --> 00:01:12.796
HAL era un computer dotat cu conștiință,

00:01:12.820 --> 00:01:15.276
conceput pentru a ghida
nava spațială Discovery

00:01:15.300 --> 00:01:17.836
de la Pământ până la Jupiter.

00:01:17.860 --> 00:01:19.916
HAL avea de asemenea
un caracter imperfect,

00:01:19.940 --> 00:01:24.220
întrucât a ales să prețuiască mai mult 
misiunea, în defavoarea vieții umane.

00:01:24.660 --> 00:01:26.756
HAL a fost un personaj fictiv,

00:01:26.780 --> 00:01:27.859
dar cu toate acestea,

00:01:27.884 --> 00:01:31.556
el vorbește despre temerile noastre,
temerile noastre de a fi subjugați

00:01:31.580 --> 00:01:34.596
de o inteligență artificială
fără sentimente,

00:01:34.620 --> 00:01:36.880
care este indiferentă în fața omenirii.

00:01:37.700 --> 00:01:40.276
Cred că astfel de temeri sunt nefondate.

00:01:40.300 --> 00:01:44.556
Într-adevăr, ne aflăm într-un moment
remarcabil în istoria omenirii,

00:01:44.580 --> 00:01:49.556
când, motivați de refuzul de a accepta
limitele corpurilor și minților noastre,

00:01:49.580 --> 00:01:54.916
construim mașinării de un rafinament
și o grație frumoasă și complexă

00:01:54.940 --> 00:01:56.996
care va extinde cunoașterea umană

00:01:57.020 --> 00:01:58.930
dincolo de imaginația noastră.

00:01:58.930 --> 00:02:02.286
După o carieră care mi-a purtat pașii
de la Academia Forțelor Aeriene

00:02:02.286 --> 00:02:04.416
la Space Command în prezent,

00:02:04.416 --> 00:02:05.916
am devenit inginer de sisteme

00:02:05.916 --> 00:02:08.556
și recent am fost implicat
într-o problemă de inginerie

00:02:08.580 --> 00:02:11.156
în legătură cu misiunea NASA către Marte.

00:02:11.180 --> 00:02:13.676
În zborurile spațiale către Lună

00:02:13.700 --> 00:02:16.936
ne putem baza că cei de la controlul
misiunii din Houston

00:02:16.960 --> 00:02:18.836
urmăresc toate aspectele unui zbor.

00:02:18.860 --> 00:02:22.396
Însă Marte e de 200 de ori mai departe

00:02:22.420 --> 00:02:25.636
și în consecință, e nevoie
în medie de 13 minute

00:02:25.660 --> 00:02:28.796
ca un sunet să parcurgă distanța
dintre Pământ și Marte.

00:02:28.820 --> 00:02:32.220
Dacă apare o problemă,
nu există suficient timp.

00:02:32.660 --> 00:02:35.156
Astfel, o soluție rezonabilă de inginerie

00:02:35.180 --> 00:02:40.796
ne determină să punem controlul misiunii
între pereții navetei spațiale Orion.

00:02:40.820 --> 00:02:43.716
O altă idee fascinantă
din profilul misiunii

00:02:43.740 --> 00:02:46.636
e plasarea de roboți humanoizi
pe suprafața planetei Marte

00:02:46.660 --> 00:02:48.516
înaintea sosirii oamenilor,

00:02:48.540 --> 00:02:51.096
mai întâi pentru a construi
echipamente și instalații

00:02:51.120 --> 00:02:54.780
și apoi pentru a servi ca membri
colaboratori ai echipei științifice.

00:02:55.220 --> 00:02:58.056
Pe măsură ce priveam asta
dintr-o perspectivă inginerească,

00:02:58.080 --> 00:03:01.556
devenea foarte clar pentru mine
că ceea ce aveam nevoie să creez

00:03:01.580 --> 00:03:03.356
era o inteligență artificială

00:03:03.380 --> 00:03:06.256
abilă, cooperantă și dotată 
cu inteligență socială.

00:03:06.280 --> 00:03:10.076
Cu alte cuvinte, aveam nevoie
să construiesc ceva asemănător lui HAL,

00:03:10.100 --> 00:03:12.516
dar fără tendințele sale ucigătoare.

00:03:12.540 --> 00:03:13.900
(Râsete)

00:03:14.740 --> 00:03:16.556
Să facem o mică pauză.

00:03:16.580 --> 00:03:20.476
Este oare posibil să construim
o asemenea inteligență artificială?

00:03:20.500 --> 00:03:21.956
De fapt, chiar este.

00:03:21.980 --> 00:03:25.236
În multe feluri, aceasta este
o problemă grea de inginerie

00:03:25.260 --> 00:03:27.200
cu elemente de inteligență artificială,

00:03:27.200 --> 00:03:31.446
și nu o problemă inextricabilă 
de IA care necesită inginerie.

00:03:31.466 --> 00:03:33.890
Ca să-l parafrazez pe Alan Turing:

00:03:33.890 --> 00:03:37.256
„Nu sunt interesat să construiesc 
o mașinărie dotată cu conștiință”.

00:03:37.256 --> 00:03:38.316
Nu construiesc un HAL.

00:03:38.316 --> 00:03:43.700
Tot ce vreau este un creier simplu,
ceva ce oferă iluzia inteligenței.

00:03:44.820 --> 00:03:47.890
Arta și știința informatică
au parcurs un drum lung

00:03:47.890 --> 00:03:49.520
de când HAL a fost adus pe ecran

00:03:49.520 --> 00:03:53.316
și-mi imaginez că dacă inventatorul său,
Dr. Chandra, ar fi fost aici azi,

00:03:53.366 --> 00:03:55.866
ar fi avut o listă întreagă
de întrebări pentru noi.

00:03:55.866 --> 00:03:57.496
E într-adevăr posibil pentru noi

00:03:57.496 --> 00:04:01.136
să folosim un sistem de milioane
și milioane de dispozitive,

00:04:01.260 --> 00:04:03.116
ca să citim în fluxurile lor de date,

00:04:03.140 --> 00:04:05.596
pentru a le prezice erorile
și a acționa înainte?

00:04:05.620 --> 00:04:06.636
Da.

00:04:06.660 --> 00:04:10.236
Putem construi sisteme care să converseze
cu oamenii în limba lor naturală?

00:04:10.260 --> 00:04:11.276
Da.

00:04:11.300 --> 00:04:14.776
Putem construi sisteme care să recunoască
obiecte, să identifice emoții,

00:04:14.800 --> 00:04:17.876
să arate emoții, să joace jocuri
și chiar să citească de pe buze?

00:04:17.900 --> 00:04:18.916
Da.

00:04:18.940 --> 00:04:21.576
Putem construi sisteme
care să stabilească obiective,

00:04:21.600 --> 00:04:25.616
să realizeze planuri pentru îndeplinirea
acestor obiective și să învețe pe parcurs?

00:04:25.640 --> 00:04:26.656
Da.

00:04:26.780 --> 00:04:29.416
Putem construi sisteme
care să aibă o gândire empatică?

00:04:29.440 --> 00:04:30.536
Asta învățăm să facem.

00:04:30.560 --> 00:04:33.940
Putem construi sisteme care să aibă
o baza etică și morală?

00:04:34.300 --> 00:04:36.340
Asta trebuie să învățăm cum se face.

00:04:37.180 --> 00:04:39.156
Așa că haideți să acceptăm pentru moment

00:04:39.180 --> 00:04:42.276
că este posibil să construim
o astfel de inteligență artificială

00:04:42.300 --> 00:04:44.536
pentru acest tip de misiuni
și pentru altele.

00:04:44.560 --> 00:04:47.296
Următoarea întrebare
pe care trebuie să v-o puneți e:

00:04:47.320 --> 00:04:49.076
ar trebui să ne fie frică de ea?

00:04:49.100 --> 00:04:50.660
Ei bine, fiecare tehnologie nouă

00:04:50.660 --> 00:04:52.790
ne dă palpitații
într-o anumită măsură.

00:04:52.790 --> 00:04:54.916
Când au apărut 
pentru prima oară automobilele,

00:04:54.940 --> 00:04:58.466
oamenii s-au lamentat că vom asista
la destrămarea familiei.

00:04:58.466 --> 00:05:01.076
Când au apărut 
pentru prima oară telefoanele,

00:05:01.100 --> 00:05:04.396
oamenii erau îngrijorați
că vor distruge complet conversația.

00:05:04.420 --> 00:05:07.860
La un moment dat am văzut
cum cuvântul scris a devenit omniprezent,

00:05:07.880 --> 00:05:10.746
iar oamenii au crezut că vom pierde
abilitatea de a memora.

00:05:10.746 --> 00:05:13.256
Toate acestea sunt adevărate
până la un anumit punct,

00:05:13.280 --> 00:05:15.996
dar de asemenea este adevărat
și că aceste tehnologii

00:05:16.020 --> 00:05:18.696
ne-au adus lucruri care ne-au lărgit
cunoașterea umană

00:05:18.720 --> 00:05:20.500
în câteva moduri profunde.

00:05:21.660 --> 00:05:23.940
Așa că hai să mergem un pic mai departe.

00:05:24.940 --> 00:05:29.676
Eu nu mă tem de crearea
unei astfel de inteligențe artificiale

00:05:29.700 --> 00:05:33.516
pentru că în final ea va îngloba
câteva dintre valorile noastre.

00:05:33.540 --> 00:05:35.151
Luați în considerare acest lucru:

00:05:35.176 --> 00:05:38.036
construirea unui sistem cognitiv
este fundamental diferită

00:05:38.060 --> 00:05:41.356
de construirea unui sistem
software-intensiv tradițional, din trecut.

00:05:41.380 --> 00:05:42.836
Nu le programăm. Le învățăm.

00:05:42.860 --> 00:05:45.516
Pentru a învăța un sistem
cum să recunoască florile,

00:05:45.540 --> 00:05:48.556
îi arăt mii de flori
din diverse tipuri care-mi plac.

00:05:48.580 --> 00:05:50.936
Pentru a învăța un sistem
cum să joace un joc...

00:05:50.960 --> 00:05:53.220
Ei bine, eu aș face-o. Și tu la fel.

00:05:54.420 --> 00:05:56.460
Îmi plac florile. Pe bune!

00:05:57.260 --> 00:06:00.116
Ca să înveți un sistem
cum se joacă un joc de GO,

00:06:00.140 --> 00:06:02.196
ar trebui să joc mii de partide de GO,

00:06:02.220 --> 00:06:04.176
dar în acest proces aș învăța de asemenea

00:06:04.200 --> 00:06:06.416
cum să discern
între un joc bun și unul prost.

00:06:06.440 --> 00:06:09.936
Dacă vreau să creez IA
care să ofere asistență juridică,

00:06:10.060 --> 00:06:14.716
o s-o învăț corpusul legislativ,
dar în același timp voi integra

00:06:14.740 --> 00:06:18.350
și simțul clemenței și al dreptății
ca parte a legii respective.

00:06:18.380 --> 00:06:21.556
În termeni științifici, asta este
ceea ce numim adevărul de bază,

00:06:21.580 --> 00:06:24.876
și asta e ce contează cu adevărat:
când producem aceste mașinării,

00:06:24.900 --> 00:06:28.316
le învățăm o parte din valorile noastre.

00:06:28.340 --> 00:06:31.476
În acest scop, mă încred
în inteligență artificială

00:06:31.500 --> 00:06:35.140
la fel, dacă nu și mai mult
decât într-un om care este bine instruit.

00:06:35.900 --> 00:06:39.756
Dar ați putea întreba: ce ne facem 
cu agenții necinstiți,

00:06:39.780 --> 00:06:43.116
cu anumite ONG-uri bine finanțate?

00:06:43.140 --> 00:06:46.956
Nu mă tem de o inteligență artificială
aflată în mâinile unui lup singuratic.

00:06:46.980 --> 00:06:49.406
Cu certitudine, nu ne putem proteja

00:06:49.431 --> 00:06:51.816
împotriva tuturor actelor
aleatorii de violență,

00:06:51.840 --> 00:06:53.876
dar realitatea e că un asemenea sistem

00:06:53.900 --> 00:06:56.696
necesită o instruire
substanțială și subtilă,

00:06:56.820 --> 00:06:59.116
cu mult peste resursele
unui singur individ.

00:06:59.140 --> 00:07:01.140
În plus, înseamnă mult mai mult

00:07:01.200 --> 00:07:03.756
decât să introduci
un virus informatic în societate,

00:07:03.796 --> 00:07:07.166
unde apeși pe un buton deodată
și se răspândește în milioane de locuri

00:07:07.216 --> 00:07:09.236
și laptopurile explodează pretutindeni.

00:07:09.260 --> 00:07:12.076
Ei bine, acest fel de conținut
este mult mai amplu

00:07:12.100 --> 00:07:13.815
și cu siguranță ne așteptam la el.

00:07:14.340 --> 00:07:17.396
Să mă tem că o astfel
de inteligență artificială

00:07:17.420 --> 00:07:19.380
ar putea amenința întreaga umanitate?

00:07:20.100 --> 00:07:21.930
Dacă vă uitați la filme

00:07:21.955 --> 00:07:27.676
precum „Matrix”, „Metropolis”
„Terminatorul”, seriale ca „Westworld”

00:07:27.700 --> 00:07:29.836
toate vorbesc despre acest tip de teamă.

00:07:29.860 --> 00:07:34.156
Într-adevăr, în cartea „Superinteligență”,
filozoful Nick Bostrom

00:07:34.180 --> 00:07:35.716
abordează această temă

00:07:35.740 --> 00:07:39.756
și observă că superinteligența
poate fi nu doar periculoasă,

00:07:39.780 --> 00:07:43.636
ci poate reprezenta o amenințare
existențială la adresa întregii omeniri.

00:07:43.660 --> 00:07:45.876
Argumentul de bază al Dr. Bostrom

00:07:45.900 --> 00:07:48.636
este că astfel de sisteme vor sfârși

00:07:48.660 --> 00:07:51.916
prin a avea o nestăvilită
sete de informație

00:07:51.940 --> 00:07:54.836
așa încât, probabil,
vor învăța cum să învețe

00:07:54.860 --> 00:07:57.476
și în final să descopere
că pot avea interese

00:07:57.500 --> 00:07:59.796
care sunt contrarii nevoilor umanității.

00:07:59.820 --> 00:08:02.376
Dr. Bostrom are un număr
de persoane care-l urmăresc.

00:08:02.400 --> 00:08:06.320
El este sprijinit de persoane
ca Elon Musk și Stephen Hawking.

00:08:06.700 --> 00:08:11.996
Cu tot respectul
pentru aceste minți luminate,

00:08:12.020 --> 00:08:14.276
cred că greșesc fundamental.

00:08:14.300 --> 00:08:17.372
Există numeroase aspecte de disecat
la argumentul Dr. Bostrom

00:08:18.000 --> 00:08:19.736
și nu am timp să le expun pe toate,

00:08:19.760 --> 00:08:22.356
dar foarte pe scurt, 
luați în considerare asta:

00:08:22.380 --> 00:08:26.116
super cunoașterea este
foarte diferită de super acțiunea.

00:08:26.140 --> 00:08:28.536
HAL a fost o amenințare
pentru echipajul Discovery

00:08:28.560 --> 00:08:32.476
doar în măsura în care HAL avea comanda
tuturor aspectelor legate de Discovery.

00:08:32.500 --> 00:08:34.996
Așa ar trebui să fie 
și în cazul unei superinteligențe.???

00:08:35.020 --> 00:08:37.516
ar trebui să stăpânească
toată lumea noastră.

00:08:37.540 --> 00:08:40.656
Aceste idei provin de la Skynet 
din filmul „Terminatorul”

00:08:40.680 --> 00:08:43.636
în care avem o superinteligență
care comanda voința umană,

00:08:43.660 --> 00:08:47.516
care controla fiecare dispozitiv
aflat în orice colț al lumii.

00:08:47.540 --> 00:08:51.116
Practic vorbind, asta nu se va întâmpla.

00:08:51.140 --> 00:08:54.296
Nu vom construi inteligențe artificiale
care să controleze vremea,

00:08:54.320 --> 00:08:55.556
să direcționeze valurile,

00:08:55.580 --> 00:08:58.956
care să ne comande pe noi,
oameni capricioși și haotici.

00:08:58.980 --> 00:09:02.876
Și mai mult decât atât, dacă astfel
de inteligență artificială ar exista,

00:09:02.900 --> 00:09:05.836
ar trebui să concureze
cu economiile oamenilor,

00:09:05.860 --> 00:09:08.380
și astfel să concureze
pentru resurse cu noi.

00:09:09.020 --> 00:09:11.500
Și la sfârșit...
Nu-i spuneți asta lui Siri...

00:09:12.260 --> 00:09:14.336
Noi putem oricând să-i scoatem din priză.

00:09:14.360 --> 00:09:15.780
(Râsete)

00:09:17.180 --> 00:09:22.156
Suntem într-o incredibilă călătorie
de evoluție comună cu mașinăriile noastre.

00:09:22.180 --> 00:09:27.236
Oamenii de azi
nu sunt oamenii de mâine.

00:09:27.260 --> 00:09:30.396
Să îți faci griji acum
de ascensiunea superinteligenței

00:09:30.420 --> 00:09:33.576
este într-o mulțime de feluri,
o periculoasă distragere a atenției,

00:09:33.600 --> 00:09:35.836
deoarece însăși ascensiunea computerelor

00:09:35.860 --> 00:09:38.876
ne aduce o serie de probleme
umane și sociale

00:09:38.900 --> 00:09:40.640
la care trebuie să participăm acum.

00:09:41.180 --> 00:09:43.996
Cum să organizez societatea cel mai bine

00:09:44.020 --> 00:09:46.656
când nevoia de munci executate
de oameni se diminuează?

00:09:46.680 --> 00:09:50.196
Cum să aduc înțelegere și educație
de-a lungul globului

00:09:50.220 --> 00:09:52.396
și totuși să respect
diferențele dintre noi?

00:09:52.420 --> 00:09:54.617
Cum aș putea să prelungesc
și să îmbunătățesc

00:09:54.642 --> 00:09:56.676
viața prin asistență medicală cognitivă?

00:09:56.700 --> 00:10:00.940
Cum aș putea utiliza informatica
pentru a ne ajuta să ajungem la stele?

00:10:01.580 --> 00:10:03.620
Și acesta este lucrul interesant.

00:10:04.220 --> 00:10:06.556
Oportunitățile de a utiliza
informatica

00:10:06.580 --> 00:10:09.556
pentru a îmbunătăți cunoașterea umană
stă în puterea noastră,

00:10:09.580 --> 00:10:13.140
aici și acum și suntem doar la început.

00:10:14.100 --> 00:10:15.316
Vă mulțumesc foarte mult.

00:10:15.340 --> 00:10:19.626
(Aplauze)

