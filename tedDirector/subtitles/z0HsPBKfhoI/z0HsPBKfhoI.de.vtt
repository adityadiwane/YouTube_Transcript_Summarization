WEBVTT
Kind: captions
Language: de

00:00:00.000 --> 00:00:07.000
Übersetzung: Christian Kuhna
Lektorat: P Hakenberg

00:00:12.580 --> 00:00:16.420
Als Kind war ich der typische Nerd.

00:00:17.140 --> 00:00:19.316
Ich glaube, das waren 
manche von Ihnen auch.

00:00:19.340 --> 00:00:20.556
(Gelächter)

00:00:20.580 --> 00:00:23.796
Sie, der am lautesten gelacht hat, 
Sie sind es wohl immer noch.

00:00:23.820 --> 00:00:25.980
(Gelächter)

00:00:25.980 --> 00:00:29.596
Ich wuchs in einer Kleinstadt in 
den staubigen Ebenen Nordtexas' auf,

00:00:29.620 --> 00:00:32.956
der Sohn eines Sheriffs, 
der der Sohn eines Pastors war.

00:00:32.980 --> 00:00:34.900
Ich durfte keinen Ärger machen.

00:00:35.860 --> 00:00:39.116
Ich fing aus Vergnügen an, 
Mathebücher zu lesen.

00:00:39.140 --> 00:00:40.676
(Gelächter)

00:00:40.700 --> 00:00:42.396
Das haben Sie auch gemacht.

00:00:42.420 --> 00:00:46.156
Das führte mich dazu, einen Laser und
einen Computer und Modellraketen zu bauen

00:00:46.180 --> 00:00:49.180
und Raketenbrennstoff 
in meinem Schlafzimmer herzustellen.

00:00:49.780 --> 00:00:53.436
In wissenschaftlicher Terminologie

00:00:53.460 --> 00:00:56.716
nennen wir das eine sehr schlechte Idee.

00:00:56.740 --> 00:00:57.956
(Gelächter)

00:00:57.980 --> 00:01:00.156
Um die Zeit herum

00:01:00.180 --> 00:01:03.396
kam Stanley Kubricks "2001: 
Odyssee im Weltraum" in die Kinos

00:01:03.420 --> 00:01:05.620
und mein Leben hatte sich 
für immer verändert.

00:01:06.100 --> 00:01:08.156
Ich liebte alles in diesem Film,

00:01:08.180 --> 00:01:10.650
besonders den HAL 9000.

00:01:10.650 --> 00:01:12.796
HAL war ein empfindungsfähiger Computer,

00:01:12.820 --> 00:01:15.276
dazu entworfen, das Raumschiff Discovery

00:01:15.300 --> 00:01:17.836
von der Erde zum Jupiter zu leiten.

00:01:17.860 --> 00:01:19.916
HAL war auch ein fehlerhafter Charakter,

00:01:19.940 --> 00:01:24.220
da er am Ende beschloss, die Mission 
über menschliches Leben zu setzen.

00:01:24.660 --> 00:01:26.756
HAL war ein fiktiver Charakter,

00:01:26.780 --> 00:01:29.436
aber nichtsdestotrotz 
spricht er unsere Ängste an,

00:01:29.460 --> 00:01:31.556
unsere Ängste vor der Unterwerfung

00:01:31.580 --> 00:01:34.596
durch irgendeine nicht fühlende 
künstliche Intelligenz,

00:01:34.620 --> 00:01:36.730
der unsere Menschlichkeit 
gleichgültig ist.

00:01:37.700 --> 00:01:40.276
Ich glaube, dass solche Ängste 
unbegründet sind.

00:01:40.300 --> 00:01:42.996
Wir befinden uns wirklich
in einer bemerkenswerten Zeit

00:01:43.020 --> 00:01:45.186
in der Geschichte der 
Menschheit, in der wir,

00:01:45.186 --> 00:01:49.556
getrieben durch die Weigerung die Grenzen
von Körper und Geist zu akzeptieren,

00:01:49.580 --> 00:01:51.276
Maschinen bauen,

00:01:51.300 --> 00:01:54.800
von exquisiter, wunderschöner 
Komplexität und Grazie,

00:01:54.800 --> 00:01:56.996
die die menschliche
Erfahrung erweitern werden,

00:01:57.020 --> 00:01:58.870
weit über unsere Vorstellung hinaus.

00:01:59.540 --> 00:02:02.116
Nach einer Karriere
bei der Luftwaffen-Akademie

00:02:02.140 --> 00:02:04.076
bin ich jetzt beim Weltraumkommando,

00:02:04.100 --> 00:02:05.630
als Systemingenieur.

00:02:05.630 --> 00:02:08.556
Und kürzlich wurde ich in ein 
technisches Problem einbezogen,

00:02:08.580 --> 00:02:11.156
das mit der NASA-Mission 
zum Mars zu tun hat.

00:02:11.180 --> 00:02:13.676
In Raumflügen zum Mond

00:02:13.700 --> 00:02:16.836
können wir uns auf die Einsatzleitstelle 
in Houston verlassen,

00:02:16.860 --> 00:02:18.836
die über alle Aspekte des Fluges wacht.

00:02:18.860 --> 00:02:22.396
Allerdings ist der Mars 200-mal weiter weg

00:02:22.420 --> 00:02:25.636
und daher dauert es 
im Durchschnitt 13 Minuten,

00:02:25.660 --> 00:02:28.796
bis ein Signal von der Erde 
zum Mars gelangt.

00:02:28.820 --> 00:02:32.220
Wenn es ein Problem gibt, 
bleibt nicht genügend Zeit.

00:02:32.660 --> 00:02:36.490
Und so mussten wir, für eine
sinnvolle technische Lösung,

00:02:36.490 --> 00:02:40.796
die Einsatzleitstelle im
Orion-Raumschiffes platzieren.

00:02:40.820 --> 00:02:43.716
Eine andere faszinierende Idee, 
im Profil der Mission,

00:02:43.740 --> 00:02:46.636
setzt humanoide Roboter 
auf die Marsoberfläche,

00:02:46.660 --> 00:02:48.516
bevor die Menschen selbst ankommen,

00:02:48.540 --> 00:02:50.190
um zunächst Gebäude zu bauen,

00:02:50.190 --> 00:02:53.830
und später als kollaborative Mitglieder 
im Wissenschaftsteam zu dienen.

00:02:55.030 --> 00:02:57.956
Als ich dies aus der technischen
Perspektive betrachtete,

00:02:57.980 --> 00:03:01.156
wurde mir sehr klar, dass das, 
was ich bauen musste,

00:03:01.180 --> 00:03:03.356
eine schlaue, kollaborative,

00:03:03.380 --> 00:03:05.756
sozial intelligente 
künstliche Intelligenz war.

00:03:05.780 --> 00:03:10.076
Anders gesagt, ich musste etwas 
ganz Ähnliches wie HAL bauen,

00:03:10.100 --> 00:03:12.516
ohne die mörderischen Neigungen.

00:03:12.540 --> 00:03:13.900
(Gelächter)

00:03:14.740 --> 00:03:16.556
Lassen Sie uns kurz anhalten.

00:03:16.580 --> 00:03:20.476
Ist es wirklich möglich, eine solche 
künstliche Intelligenz zu bauen?

00:03:20.500 --> 00:03:21.956
Nun, das ist es.

00:03:21.980 --> 00:03:23.236
In vieler Hinsicht

00:03:23.260 --> 00:03:25.236
ist dies ein rein technisches Problem

00:03:25.260 --> 00:03:26.716
mit KI-Anteilen,

00:03:26.740 --> 00:03:31.436
kein vages KI-Problem, 
das technisch gelöst werden muss.

00:03:31.460 --> 00:03:34.116
Um es mit Alan Turing zu sagen:

00:03:34.140 --> 00:03:36.516
Ich will keine empfindungsfähige 
Maschine bauen.

00:03:36.540 --> 00:03:38.116
Ich baue keinen HAL.

00:03:38.140 --> 00:03:40.556
Alles was ich will, 
ist ein simples Gehirn,

00:03:40.580 --> 00:03:43.700
etwas, das die Illusion 
von Intelligenz bietet.

00:03:44.820 --> 00:03:47.760
Kunst und Lehre der Informatik 
sind weit gekommen,

00:03:47.760 --> 00:03:49.476
seit HAL auf der Leinwand erschien,

00:03:49.500 --> 00:03:52.716
und ich glaube, wenn sein Erfinder, 
Dr. Chandra, heute hier wäre,

00:03:52.740 --> 00:03:55.076
hätte er viele Fragen an uns.

00:03:55.100 --> 00:03:57.196
Ist es uns wirklich möglich

00:03:57.220 --> 00:04:01.150
ein System von abermillionen 
Geräten zu nehmen,

00:04:01.150 --> 00:04:02.630
ihre Datenströme zu lesen,

00:04:02.630 --> 00:04:05.066
ihre Fehler vorher zu sagen 
und vorher zu handeln?

00:04:05.066 --> 00:04:06.090
Ja.

00:04:06.090 --> 00:04:09.390
Können wir Systeme bauen,
die mit Menschen natürlich sprechen?

00:04:09.390 --> 00:04:10.130
Ja.

00:04:10.130 --> 00:04:13.676
Können wir Systeme bauen, die 
Gegenstände und Emotionen erkennen,

00:04:13.700 --> 00:04:17.076
sich ausdrücken, Spiele spielen 
und sogar Lippen lesen?

00:04:17.100 --> 00:04:18.060
Ja.

00:04:18.060 --> 00:04:20.476
Können wir ein System bauen, 
das Ziele setzt,

00:04:20.500 --> 00:04:24.116
das Pläne für diese Ziele ausführt 
und auf dem Weg dahin lernt?

00:04:24.140 --> 00:04:25.220
Ja.

00:04:25.220 --> 00:04:28.716
Können wir Systeme bauen, die eine 
Naive Theorie [Theory of mind] haben?

00:04:28.740 --> 00:04:30.236
Das lernen wir gerade.

00:04:30.260 --> 00:04:33.740
Können wir Systeme mit ethischen 
und moralischen Grundlagen bauen?

00:04:34.300 --> 00:04:36.340
Das müssen wir noch lernen.

00:04:36.610 --> 00:04:38.966
So lassen Sie uns für 
einen Moment akzeptieren,

00:04:38.966 --> 00:04:41.476
dass eine solche KI möglich ist,

00:04:41.500 --> 00:04:43.636
für solche und andere Aufgaben.

00:04:43.660 --> 00:04:46.196
Die nächste Frage, 
die Sie sich stellen müssen, ist:

00:04:46.220 --> 00:04:47.676
Sollten wir sie fürchten?

00:04:47.700 --> 00:04:49.676
Jede neue Technologie

00:04:49.700 --> 00:04:52.596
bringt eine gewisse Angst mit sich.

00:04:52.620 --> 00:04:54.316
Als wir die ersten Autos sahen,

00:04:54.340 --> 00:04:58.356
beklagten manche, dass wir
die Zerstörung der Familie sehen würden.

00:04:58.380 --> 00:05:00.990
Als die ersten Telefone aufkamen,

00:05:00.990 --> 00:05:04.146
waren Leute besorgt, dies würde
alle höflichen Gespräche zerstören.

00:05:04.146 --> 00:05:07.956
Irgendwann setzte sich das 
geschriebene Wort durch,

00:05:07.980 --> 00:05:10.476
und man fürchtete um das Auswendiglernen.

00:05:10.500 --> 00:05:12.556
Das ist alles in gewisser Hinsicht wahr,

00:05:12.580 --> 00:05:14.996
aber es ist auch wahr, 
dass diese Technologien

00:05:15.020 --> 00:05:18.596
uns Dinge gebracht haben, die unsere 
menschliche Erfahrung ganz wesentlich

00:05:18.596 --> 00:05:20.300
erweitert haben.

00:05:21.660 --> 00:05:23.940
Lassen Sie uns noch 
einen Schritt weiter gehen.

00:05:24.940 --> 00:05:29.676
Ich fürchte die Erschaffung 
einer solchen KI nicht,

00:05:29.700 --> 00:05:33.516
da sie schließlich manche 
unserer Werte verkörpern wird.

00:05:33.540 --> 00:05:37.036
Bedenken Sie: Ein kognitives System 
zu bauen ist grundsätzlich anders

00:05:37.060 --> 00:05:40.356
als ein traditionelles software-
intensives System der Vergangenheit.

00:05:40.380 --> 00:05:42.836
Wir programmieren sie nicht. 
Wir lehren sie.

00:05:42.860 --> 00:05:45.516
Um einem System beizubringen,
eine Blume zu erkennen,

00:05:45.540 --> 00:05:48.500
zeige ich ihm tausende 
meiner Lieblingsblumen.

00:05:48.500 --> 00:05:50.836
Um einem System ein Spiel beizubringen --

00:05:50.860 --> 00:05:52.820
Ich würde. Sie würden auch.

00:05:54.420 --> 00:05:56.460
Ich mag Blumen. Wirklich.

00:05:57.260 --> 00:06:00.000
Um einem System
ein Spiel wie Go beizubringen,

00:06:00.000 --> 00:06:02.246
würde ich es tausende 
Go-Spiele spielen lassen,

00:06:02.246 --> 00:06:03.876
aber dann bringe ich ihm auch bei,

00:06:03.900 --> 00:06:06.316
zu erkennen, ob ein Spiel 
gut oder schlecht ist.

00:06:06.340 --> 00:06:09.900
Wenn ich einen KI-Rechtsassistenten baue,

00:06:09.900 --> 00:06:11.836
werde ich ihm Gesetze beibringen,

00:06:11.860 --> 00:06:14.716
gleichzeitig füge ich ein Verständnis

00:06:14.740 --> 00:06:17.620
für Gnade und Gerechtigkeit hinzu, 
was Teil des Rechts ist.

00:06:18.380 --> 00:06:21.356
In der Wissenschaft nennen
wir das Grundwahrheit.

00:06:21.380 --> 00:06:23.280
Das ist wichtig:

00:06:23.280 --> 00:06:24.876
Wenn wir diese Maschinen bauen,

00:06:24.900 --> 00:06:28.316
werden wir ihnen unsere Werte beibringen.

00:06:28.340 --> 00:06:31.736
Und insofern vertraue ich 
künstlicher Intelligenz genauso,

00:06:31.736 --> 00:06:35.140
oder gar mehr, als einem 
gut ausgebildeten Menschen.

00:06:35.900 --> 00:06:37.116
Aber, werden Sie fragen,

00:06:37.140 --> 00:06:39.756
was ist mit Schurken,

00:06:39.780 --> 00:06:43.116
einer gut finanzierten, 
nichtstaatlichen Organisation?

00:06:43.140 --> 00:06:46.956
Ich habe keine Angst vor einer KI 
in der Hand eines Einzelgängers.

00:06:46.980 --> 00:06:51.516
Wir können uns nicht vor jeder 
willkürlichen Gewalttat schützen,

00:06:51.540 --> 00:06:53.676
aber die Realität ist, 
dass solch ein System

00:06:53.700 --> 00:06:56.450
ausgiebiges und raffiniertes 
Training braucht,

00:06:56.450 --> 00:06:59.326
was weit über die Möglichkeiten
eines Einzelnen hinaus geht.

00:06:59.326 --> 00:07:00.900
Außerdem ist es schwieriger

00:07:00.900 --> 00:07:03.636
als einen Internet-Virus 
in die Welt zu injizieren,

00:07:03.660 --> 00:07:06.756
wo man einen Knopf drückt, 
und plötzlich ist es überall

00:07:06.780 --> 00:07:09.236
und Laptops fangen überall an, 
zu explodieren.

00:07:09.260 --> 00:07:12.076
Diese Materie ist viel schwieriger,

00:07:12.100 --> 00:07:14.165
und wir werden es sicher kommen sehen.

00:07:14.340 --> 00:07:17.396
Ob ich fürchte, dass eine solche KI

00:07:17.420 --> 00:07:19.380
die ganze Menschheit bedrohen könnte?

00:07:20.100 --> 00:07:24.476
Wenn man sich Filme ansieht, 
wie "Matrix", "Metropolis",

00:07:24.500 --> 00:07:27.676
"Terminator", oder Serien wie "Westworld",

00:07:27.700 --> 00:07:29.836
sprechen die alle von dieser Angst.

00:07:29.860 --> 00:07:34.156
In dem Buch "Superintelligenz"
des Philosophen Nick Bostrom,

00:07:34.180 --> 00:07:35.716
nimmt er dieses Thema auf,

00:07:35.740 --> 00:07:39.756
und merkt an, dass eine Superintelligenz 
nicht nur gefährlich wäre,

00:07:39.780 --> 00:07:43.636
sie könnte eine existenzielle Bedrohung 
für die ganze Menschheit sein.

00:07:43.660 --> 00:07:45.876
Dr. Bostroms Hauptargument ist,

00:07:45.900 --> 00:07:48.636
dass solche Systeme schlussendlich

00:07:48.660 --> 00:07:51.916
einen unstillbaren 
Informationshunger haben,

00:07:51.940 --> 00:07:54.836
dass sie vielleicht 
lernen werden, zu lernen,

00:07:54.860 --> 00:07:57.476
und schließlich entdecken, 
dass sie Ziele haben,

00:07:57.500 --> 00:07:59.796
die den menschlichen 
Bedürfnissen widersprechen.

00:07:59.820 --> 00:08:01.676
Dr. Bostrom hat viele Anhänger.

00:08:01.700 --> 00:08:06.020
Er wird von Leuten wie Elon Musk 
und Stephen Hawking unterstützt.

00:08:06.700 --> 00:08:09.100
Bei allem Respekt

00:08:09.980 --> 00:08:11.996
vor diesen brillanten Köpfen,

00:08:12.020 --> 00:08:14.276
ich glaube, dass sie sich gewaltig irren.

00:08:14.300 --> 00:08:17.476
Viele Argumente von Dr. Bostrom 
kann man widerlegen.

00:08:17.500 --> 00:08:19.866
Ich habe keine Zeit, 
sie alle zu widerlegen.

00:08:19.866 --> 00:08:22.356
Bedenken sie nur kurz dies:

00:08:22.380 --> 00:08:25.950
Super-Wissen ist etwas 
ganz anderes als Super-Tun.

00:08:25.950 --> 00:08:28.476
HAL war erst dann 
eine Bedrohung für die Mannschaft,

00:08:28.476 --> 00:08:32.476
als HAL alle Aspekte
der Discovery befehligte.

00:08:32.500 --> 00:08:34.950
Das müsste bei einer
Superintelligenz der Fall sein.

00:08:34.950 --> 00:08:37.666
Sie müsste die Gewalt über 
unsere ganze Welt haben.

00:08:37.666 --> 00:08:40.356
Das ist wie das Skynet 
aus dem Film "Terminator",

00:08:40.380 --> 00:08:42.236
in dem eine Superintelligenz

00:08:42.260 --> 00:08:43.976
den menschlichen Willen beherrschte

00:08:43.976 --> 00:08:47.516
und jedes Gerät in
jedem Winkel der Welt steuerte.

00:08:47.540 --> 00:08:48.996
Praktisch gesehen

00:08:49.020 --> 00:08:51.116
wird das nicht passieren.

00:08:51.140 --> 00:08:54.196
Wir bauen keine KIs, 
die das Wetter kontrollieren,

00:08:54.220 --> 00:08:55.556
die die Gezeiten steuern,

00:08:55.580 --> 00:08:58.956
die uns kapriziöse, chaotische 
Menschen befehligt.

00:08:58.980 --> 00:09:02.876
Und außerdem: Wenn eine 
solche KI existieren würde,

00:09:02.900 --> 00:09:05.836
müsste sie mit menschlichen 
Wirtschaftssystemen konkurrieren,

00:09:05.860 --> 00:09:08.380
und somit mit uns um 
Ressourcen konkurrieren.

00:09:09.020 --> 00:09:09.930
Schlussendlich --

00:09:09.930 --> 00:09:11.500
verraten Sie das Siri nicht --

00:09:12.130 --> 00:09:13.726
können wir sie immer ausstecken.

00:09:13.726 --> 00:09:15.780
(Gelächter)

00:09:17.180 --> 00:09:19.636
Wir sind auf einer unglaublichen Reise

00:09:19.660 --> 00:09:22.156
der Co-Evolution mit unseren Maschinen.

00:09:22.180 --> 00:09:24.676
Die Menschen, die wir heute sind,

00:09:24.700 --> 00:09:27.236
sind nicht die Menschen,
die wir dann sein werden.

00:09:27.260 --> 00:09:30.396
Die aktuelle Sorge vor der 
Machtergreifung einer Superintelligenz

00:09:30.420 --> 00:09:33.476
ist in vieler Hinsicht eine 
gefährliche Ablenkung,

00:09:33.500 --> 00:09:35.836
weil die Zunahme 
der Rechenleistung an sich,

00:09:35.860 --> 00:09:38.876
eine Reihe von menschlichen 
und sozialen Fragen aufwirft,

00:09:38.900 --> 00:09:40.720
denen wir uns jetzt zuwenden müssen.

00:09:41.180 --> 00:09:43.996
Wie organisieren wir 
die Gesellschaft am besten,

00:09:44.020 --> 00:09:46.356
wenn der Bedarf an 
menschlicher Arbeit abnimmt?

00:09:46.380 --> 00:09:49.930
Wie können wir weltweit 
Verständigung und Bildung bereitstellen,

00:09:49.930 --> 00:09:52.206
und dabei unsere 
Unterschiedlichkeit achten?

00:09:52.206 --> 00:09:56.276
Wie das menschliches Leben durch 
ein kognitives Gesundheitswesen erweitern?

00:09:56.300 --> 00:09:59.156
Wie können wir Rechenleistung nutzen,

00:09:59.180 --> 00:10:00.940
um uns zu den Sternen zu bringen?

00:10:01.580 --> 00:10:03.620
Und das ist der aufregende Teil.

00:10:04.220 --> 00:10:06.556
Die Chancen, diese 
Rechenleistung zu nutzen,

00:10:06.580 --> 00:10:08.436
um menschliche Erfahrung zu erweitern,

00:10:08.436 --> 00:10:09.556
sind heute gegeben,

00:10:09.580 --> 00:10:11.436
hier und jetzt,

00:10:11.460 --> 00:10:13.140
und wir fangen gerade erst an.

00:10:14.100 --> 00:10:15.316
Herzlichen Dank!

00:10:15.340 --> 00:10:19.626
(Beifall)

