WEBVTT
Kind: captions
Language: ru

00:00:00.000 --> 00:00:07.000
Переводчик: Alena Chernykh
Редактор: Marina Lee

00:00:12.580 --> 00:00:16.420
В детстве я был типичным ботаником.

00:00:17.140 --> 00:00:19.316
Думаю, некоторые из вас тоже.

00:00:19.460 --> 00:00:20.456
(Смех)

00:00:20.580 --> 00:00:23.786
А вы, смеётесь громче всех,
наверняка таким и остались.

00:00:23.960 --> 00:00:25.956
(Смех)

00:00:26.100 --> 00:00:29.426
Я вырос в маленьком городке
среди равнин Северного Техаса.

00:00:29.620 --> 00:00:32.786
Мой отец был шерифом, а дед — пастором.

00:00:32.980 --> 00:00:35.570
Никаких вариантов бедокурить.

00:00:35.860 --> 00:00:39.306
Поэтому ради забавы я начал читать
книги по математическому анализу.

00:00:39.470 --> 00:00:40.566
(Смех)

00:00:40.770 --> 00:00:42.396
И вы — тоже!

00:00:42.610 --> 00:00:46.016
Благодаря этому я сконструировал
в своей комнате лазер,

00:00:46.180 --> 00:00:49.370
компьютер и модель,
а потом и топливо для ракеты.

00:00:50.380 --> 00:00:53.226
Научный термин для этого —

00:00:53.460 --> 00:00:56.556
очень плохая идея.

00:00:56.740 --> 00:00:57.816
(Смех)

00:00:57.980 --> 00:01:00.076
Как раз в то время

00:01:00.180 --> 00:01:03.286
вышел фильм Стенли Кубрика
«Космическая одиссея 2001 года»,

00:01:03.420 --> 00:01:05.620
он кардинально изменил мою жизнь.

00:01:05.990 --> 00:01:08.386
Мне нравилось в этом фильме
абсолютно всё,

00:01:08.490 --> 00:01:10.606
особенно HAL 9000.

00:01:10.740 --> 00:01:12.796
HAL был разумным компьютером,

00:01:12.910 --> 00:01:15.276
управляющим космическим
кораблём Discovery,

00:01:15.470 --> 00:01:17.766
направляющимся от Земли к Юпитеру.

00:01:17.880 --> 00:01:20.286
У него был небезупречный характер,

00:01:20.480 --> 00:01:24.220
и в итоге человеческой жизни
он предпочёл миссию.

00:01:24.550 --> 00:01:26.676
HAL был вымышленным персонажем,

00:01:26.850 --> 00:01:29.286
но тем не менее, он вызывает страх,

00:01:29.460 --> 00:01:31.416
страх быть порабощённым

00:01:31.580 --> 00:01:34.546
бесчувственным искусственным интеллектом,

00:01:34.700 --> 00:01:36.580
равнодушным к человеку.

00:01:37.700 --> 00:01:40.126
Я считаю, такие страхи безосновательны.

00:01:40.330 --> 00:01:42.836
В самом деле, мы живём
в удивительное время

00:01:43.020 --> 00:01:44.536
в человеческой истории,

00:01:44.650 --> 00:01:49.376
когда, не принимая ограничения
наших тел и нашего разума,

00:01:49.520 --> 00:01:51.126
мы создаём механизмы

00:01:51.300 --> 00:01:54.756
изумительной, невероятной
сложности и тонкости,

00:01:54.940 --> 00:01:56.806
которые расширят способности человека

00:01:57.020 --> 00:01:58.700
за грани нашего воображения.

00:01:59.510 --> 00:02:01.986
Начав карьеру в Академии ВВС,

00:02:02.110 --> 00:02:04.006
продолжив в Космическом
командовании ВС США,

00:02:04.100 --> 00:02:05.696
я стал инженером-системотехником.

00:02:05.820 --> 00:02:08.556
А в настоящее время занимаюсь 
инженерными разработками,

00:02:08.710 --> 00:02:11.156
связанными с миссией НАСА на Марсе.

00:02:11.310 --> 00:02:13.676
Сейчас в полётах на Луну

00:02:13.800 --> 00:02:16.666
мы можем полагаться
на управление полётами в Хьюстоне,

00:02:16.860 --> 00:02:18.836
чтобы следить за всеми аспектами полёта.

00:02:18.990 --> 00:02:22.236
Однако Марс в 200 раз дальше Луны,

00:02:22.420 --> 00:02:25.516
и поэтому сигнал от Земли к Марсу

00:02:25.660 --> 00:02:28.696
идёт в среднем 13 минут.

00:02:28.820 --> 00:02:32.220
Случись какая-то неполадка,
времени не хватит.

00:02:32.660 --> 00:02:35.016
Поэтому разумное инженерное решение —

00:02:35.180 --> 00:02:37.656
разместить центр управления полётами

00:02:37.780 --> 00:02:40.636
внутри космического корабля Orion.

00:02:40.820 --> 00:02:43.586
Другая увлекательная идея для миссии —

00:02:43.740 --> 00:02:46.516
отправить роботов-гуманоидов на Марс

00:02:46.660 --> 00:02:48.326
до прилёта людей:

00:02:48.470 --> 00:02:50.126
сначала они построят базы,

00:02:50.320 --> 00:02:53.580
а потом будут помогать научной команде.

00:02:55.220 --> 00:02:57.916
Когда я посмотрел на это как инженер,

00:02:58.080 --> 00:03:00.830
стало ясно, что нужно спроектировать

00:03:00.980 --> 00:03:03.186
умный, способный к сотрудничеству,

00:03:03.380 --> 00:03:05.776
социально-сознательный
искусственный интеллект.

00:03:05.976 --> 00:03:09.956
Другими словами, нужно было создать
что-то очень похожее на HAL,

00:03:10.100 --> 00:03:12.396
но без склонности к убийствам.

00:03:12.540 --> 00:03:13.900
(Смех)

00:03:14.740 --> 00:03:16.466
Давайте на минуту остановимся.

00:03:16.660 --> 00:03:20.376
Реально ли сделать такой
искусственный интеллект?

00:03:20.500 --> 00:03:21.796
Вполне.

00:03:21.950 --> 00:03:23.106
Во многом

00:03:23.260 --> 00:03:25.106
это сложная инженерная задача

00:03:25.260 --> 00:03:26.716
с использованием ИИ,

00:03:26.880 --> 00:03:31.306
а не запутанный клубок проблем ИИ,
который нужно распутать.

00:03:31.460 --> 00:03:34.016
Перефразируя Алана Туринга,

00:03:34.140 --> 00:03:36.406
я не собираюсь создавать разумного робота.

00:03:36.540 --> 00:03:38.126
Я не собираюсь создавать HAL.

00:03:38.240 --> 00:03:40.766
Мне всего лишь нужен простой разум

00:03:40.910 --> 00:03:43.700
с иллюзией интеллекта.

00:03:44.820 --> 00:03:47.846
Искусство и наука о вычислительной
технике прошли долгий путь

00:03:47.980 --> 00:03:49.326
с появления HAL на экранах,

00:03:49.500 --> 00:03:52.596
и я представляю, сколько вопросов
возникло бы у его изобретателя

00:03:52.740 --> 00:03:54.926
д-ра Чандра, будь он сейчас здесь.

00:03:55.100 --> 00:03:57.046
Возможно ли на самом деле

00:03:57.220 --> 00:04:01.006
взять систему из миллионов устройств,

00:04:01.190 --> 00:04:02.566
прочитать их потоки данных,

00:04:02.740 --> 00:04:04.856
предугадать их ошибки и заранее исправить?

00:04:05.020 --> 00:04:05.926
Да.

00:04:06.100 --> 00:04:09.276
Можно ли создать механизмы,
которые говорят на человеческом языке?

00:04:09.460 --> 00:04:10.366
Да.

00:04:10.530 --> 00:04:13.506
Создать механизмы, способные
распознавать объекты, эмоции,

00:04:13.700 --> 00:04:16.896
выражать свои эмоции,
играть и даже читать по губам?

00:04:17.100 --> 00:04:18.096
Да.

00:04:18.240 --> 00:04:20.376
Механизмы, которые смогут
формулировать цели,

00:04:20.500 --> 00:04:24.116
составлять планы для их достижения
и учиться в процессе их выполнения?

00:04:24.280 --> 00:04:25.206
Да.

00:04:25.320 --> 00:04:28.686
Можем ли мы создать механизмы,
способные понимать чужое сознание?

00:04:28.850 --> 00:04:30.226
Мы работаем над этим.

00:04:30.390 --> 00:04:33.860
Можем ли мы создать механизмы
с этическими и нравственными основами?

00:04:34.300 --> 00:04:36.450
Это задача для нас на будущее.

00:04:37.060 --> 00:04:38.416
Давайте на минуту представим

00:04:38.580 --> 00:04:41.306
возможность создания
такого искусственного разума

00:04:41.500 --> 00:04:43.456
именно для таких целей, и не только.

00:04:43.660 --> 00:04:46.066
Следующий вопрос, который вы
должны себе задать:

00:04:46.220 --> 00:04:47.676
а следует ли нам его бояться?

00:04:47.860 --> 00:04:49.646
Любая новая технология

00:04:49.840 --> 00:04:52.456
вызывает некоторое беспокойство.

00:04:52.620 --> 00:04:54.296
Когда впервые появились автомобили,

00:04:54.500 --> 00:04:58.186
люди переживали, что это разрушит семьи.

00:04:58.380 --> 00:05:00.906
Когда появились телефоны,

00:05:01.100 --> 00:05:03.846
люди боялись, что перестанут
общаться вживую.

00:05:04.020 --> 00:05:07.816
В какой-то момент мы увидели,
что распространилась письменность,

00:05:07.980 --> 00:05:10.436
и подумали, что потеряем
способность к запоминанию.

00:05:10.620 --> 00:05:12.426
В этом есть доля истины,

00:05:12.580 --> 00:05:14.866
но также правда и то, что эти технологии

00:05:15.020 --> 00:05:18.246
привели нас к огромнейшему расширению

00:05:18.420 --> 00:05:20.300
сознания и возможностей человека.

00:05:21.660 --> 00:05:23.940
Давайте пойдём дальше.

00:05:24.940 --> 00:05:29.446
Я не боюсь появления ИИ
с такими возможностями,

00:05:29.700 --> 00:05:33.376
потому что со временем он
вберёт в себя наши ценности.

00:05:33.540 --> 00:05:36.896
Подумайте, создание мыслящей
системы принципиально отличается

00:05:37.060 --> 00:05:40.356
от создания традиционных систем,
требующих множества программ.

00:05:40.500 --> 00:05:42.666
Мы не программируем ИИ. Мы его обучаем.

00:05:42.860 --> 00:05:45.356
Чтобы научить систему распознать цветок,

00:05:45.540 --> 00:05:48.436
я показываю ей тысячи цветов,
которые нравятся мне.

00:05:48.580 --> 00:05:50.686
Чтобы научить систему, как играть...

00:05:50.860 --> 00:05:52.820
Ну, я бы научил. Вы бы тоже.

00:05:54.420 --> 00:05:56.460
Ну же! Я люблю цветы.

00:05:57.260 --> 00:05:59.976
Чтобы научить систему играть,
например, в игру го,

00:06:00.140 --> 00:06:02.066
ей нужно сыграть в неё тысячи раз,

00:06:02.220 --> 00:06:03.716
но в процессе я буду обучать её

00:06:03.900 --> 00:06:06.206
отличать хорошую игру от плохой.

00:06:06.420 --> 00:06:09.886
Если я захочу создать
помощника юриста с ИИ,

00:06:10.060 --> 00:06:12.006
я познакомлю его с законодательством,

00:06:12.170 --> 00:06:14.606
при этом обучая его

00:06:14.740 --> 00:06:17.710
милосердию и справедливости,
которые являются частью закона.

00:06:18.380 --> 00:06:21.206
Специалисты называют это
контрольными данными,

00:06:21.380 --> 00:06:23.256
и вот что самое важное:

00:06:23.420 --> 00:06:24.756
создавая эти машины,

00:06:24.900 --> 00:06:28.206
мы прививаем им наши ценности.

00:06:28.340 --> 00:06:31.366
И с этой точки зрения
я доверяю ИИ столь же,

00:06:31.500 --> 00:06:35.140
если не больше, чем человеку
с хорошим воспитанием.

00:06:35.900 --> 00:06:36.976
Но вы можете спросить:

00:06:37.140 --> 00:06:39.636
а как насчёт неконтролируемых лиц,

00:06:39.780 --> 00:06:42.976
например, хорошо финансируемых
неправительственных организаций?

00:06:43.140 --> 00:06:46.696
Я не боюсь ИИ в руках таких одиночек.

00:06:46.980 --> 00:06:51.296
Очевидно, мы не можем защитить себя
от всех случайных актов насилия,

00:06:51.540 --> 00:06:53.536
но на деле такая система требует

00:06:53.700 --> 00:06:56.636
значительной подготовки
и тщательного обучения,

00:06:56.820 --> 00:06:59.116
что далеко за пределами частных ресурсов.

00:06:59.250 --> 00:07:00.226
Более того,

00:07:00.380 --> 00:07:03.476
это сложнее, чем распространить
интернет-вирус по всему миру,

00:07:03.660 --> 00:07:06.616
когда достаточно нажать кнопку —
и он повсюду,

00:07:06.780 --> 00:07:09.066
везде начинают взрываться компьютеры.

00:07:09.260 --> 00:07:11.936
Эти вещи гораздо глобальнее,

00:07:12.100 --> 00:07:13.815
и мы обязательно их увидим.

00:07:14.340 --> 00:07:17.226
Боюсь ли я, что такой
искусственный интеллект

00:07:17.420 --> 00:07:19.380
станет угрозой для человечества?

00:07:20.100 --> 00:07:24.216
Если вспомнить фильмы
«Матрица», «Метрополь»,

00:07:24.350 --> 00:07:27.526
«Терминатор», сериал «Западный мир»,

00:07:27.700 --> 00:07:29.836
во всех говорится о подобном страхе.

00:07:30.030 --> 00:07:34.006
Философ Ник Бостром
в книге «Искусственный интеллект»

00:07:34.180 --> 00:07:35.716
поднимает эту проблему

00:07:35.870 --> 00:07:39.616
и пишет, что ИИ может
быть не только опасен,

00:07:39.780 --> 00:07:43.516
он может быть угрозой
существованию человечества.

00:07:43.660 --> 00:07:45.766
Главный аргумент Бострома:

00:07:45.900 --> 00:07:48.526
со временем у этих машин появится

00:07:48.660 --> 00:07:51.776
ненасытная жажда информации,

00:07:51.940 --> 00:07:54.696
они, возможно, научатся
учиться самостоятельно

00:07:54.860 --> 00:07:57.466
и в конце концов обнаружат,
что у них могут быть цели,

00:07:57.500 --> 00:07:59.656
которые противоречат
потребностям человека.

00:07:59.820 --> 00:08:01.676
У Бострома есть последователи.

00:08:01.850 --> 00:08:06.020
Его поддерживают такие люди,
как Элон Маск и Стивен Хокинг.

00:08:06.700 --> 00:08:09.100
При всём уважении

00:08:09.980 --> 00:08:11.826
к этим выдающимся умам,

00:08:12.020 --> 00:08:14.136
я всё-таки полагаю, что они ошибаются.

00:08:14.300 --> 00:08:17.346
Можно поспорить со многими
аргументами Бострома,

00:08:17.500 --> 00:08:19.486
и у меня нет на это времени,

00:08:19.660 --> 00:08:22.236
но вкратце, знайте,

00:08:22.380 --> 00:08:25.936
что сверхзнание — не то же самое,
что сверхвозможности.

00:08:26.140 --> 00:08:28.006
HAL был угрозой для экипажа Discovery

00:08:28.180 --> 00:08:32.306
только пока он контролировал
управление Discovery.

00:08:32.500 --> 00:08:34.806
То же и с суперинтеллектом.

00:08:35.020 --> 00:08:37.346
Он должен будет господствовать над миром.

00:08:37.540 --> 00:08:40.196
Как Скайнет из фильма «Терминатор»,

00:08:40.380 --> 00:08:42.066
где суперинтеллект

00:08:42.260 --> 00:08:43.496
командовал человеком,

00:08:43.660 --> 00:08:47.326
который управлял каждым устройством
в любой части мира.

00:08:47.540 --> 00:08:48.856
На практике

00:08:49.020 --> 00:08:50.986
такого не произойдёт.

00:08:51.140 --> 00:08:54.026
Мы не создаём ИИ,
который контролирует погоду,

00:08:54.220 --> 00:08:55.406
управляет приливами,

00:08:55.580 --> 00:08:58.796
командует нами, непредсказуемыми людьми.

00:08:58.980 --> 00:09:02.716
И даже если такой ИИ и появился бы,

00:09:02.900 --> 00:09:05.706
ему пришлось бы иметь дело
с нашей экономикой

00:09:05.860 --> 00:09:08.380
и конкурировать с нами
за владение ресурсами.

00:09:09.020 --> 00:09:10.116
И наконец,

00:09:10.260 --> 00:09:11.500
не говорите об этом Siri.

00:09:12.260 --> 00:09:13.636
Всегда можно их отключить.

00:09:13.810 --> 00:09:15.780
(Смех)

00:09:17.180 --> 00:09:19.476
Мы эволюционируем с нашими машинами,

00:09:19.660 --> 00:09:22.016
и это — невероятное путешествие.

00:09:22.180 --> 00:09:24.516
Человек станет

00:09:24.700 --> 00:09:27.086
совершенно другим в будущем.

00:09:27.260 --> 00:09:30.286
Беспокоиться из-за суперинтеллекта —

00:09:30.420 --> 00:09:33.346
во многом опасная трата времени,

00:09:33.500 --> 00:09:35.736
потому что сама компьютеризация

00:09:35.860 --> 00:09:38.826
поднимает общечеловеческие
и социальные проблемы,

00:09:39.010 --> 00:09:40.650
которые мы должны разрешить.

00:09:41.180 --> 00:09:43.866
Как лучше организовать общество,

00:09:43.890 --> 00:09:46.356
когда уменьшается необходимость
в человеческом труде?

00:09:46.380 --> 00:09:50.006
Как добиться взаимопонимания
и дать образование всему миру,

00:09:50.150 --> 00:09:51.926
при этом учитывая различия?

00:09:52.080 --> 00:09:56.146
Как продлить и улучшить жизнь
через разумное здравоохранение?

00:09:56.300 --> 00:09:59.006
Как с помощью компьютеров

00:09:59.180 --> 00:10:00.940
достичь звёзд?

00:10:01.580 --> 00:10:03.620
И вот это вдохновляет.

00:10:04.220 --> 00:10:06.486
Возможность использовать компьютеры

00:10:06.610 --> 00:10:08.026
для расширения опыта человека

00:10:08.140 --> 00:10:09.436
вполне достижима

00:10:09.580 --> 00:10:11.296
здесь и сейчас.

00:10:11.460 --> 00:10:13.140
Мы стоим в самом начале пути.

00:10:14.100 --> 00:10:15.346
Большое спасибо.

00:10:15.910 --> 00:10:19.346
(Аплодисменты)

