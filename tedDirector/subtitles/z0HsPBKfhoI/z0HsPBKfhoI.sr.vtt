WEBVTT
Kind: captions
Language: sr

00:00:00.000 --> 00:00:07.000
Prevodilac: Milenka Okuka
Lektor: Mile Živković

00:00:12.580 --> 00:00:16.420
Kad sam bio dete,
bio sam štreber do koske.

00:00:17.140 --> 00:00:19.316
Mislim da su i neki od vas to bili.

00:00:19.340 --> 00:00:20.550
(Smeh)

00:00:20.550 --> 00:00:23.876
A vi, gospodine, koji se najglasnije
smejete, verovatno ste to i dalje.

00:00:23.876 --> 00:00:26.076
(Smeh)

00:00:26.100 --> 00:00:29.596
Odrastao sam u gradiću
u prašnjavim ravnicama severnog Teksasa

00:00:29.620 --> 00:00:32.956
kao sin šerifa koji je sin pastora.

00:00:32.980 --> 00:00:34.930
Upadanje u nevolju nije dolazilo u obzir.

00:00:35.860 --> 00:00:39.116
Te sam iz zabave počeo da čitam
knjige o proračunu.

00:00:39.140 --> 00:00:40.676
(Smeh)

00:00:40.700 --> 00:00:42.396
I vi ste.

00:00:42.420 --> 00:00:46.156
Zbog toga sam sagradio laser,
kompjuter i modele raketa,

00:00:46.180 --> 00:00:49.180
a to me je navelo da napravim
raketno gorivo u spavaćoj sobi.

00:00:49.780 --> 00:00:53.436
Sad, u naučnom smislu,

00:00:53.460 --> 00:00:56.716
ovo nazivamo veoma lošom idejom.

00:00:56.740 --> 00:00:57.956
(Smeh)

00:00:57.980 --> 00:01:00.156
Otprilike u to vreme,

00:01:00.180 --> 00:01:03.396
u bioskope je stigla "Odiseja u svemiru:
2001" Stenlija Kjubrika,

00:01:03.420 --> 00:01:05.620
i moj život je izmenjen zauvek.

00:01:06.100 --> 00:01:08.156
Sve mi se sviđalo kod tog filma,

00:01:08.180 --> 00:01:10.716
naročito HAL 9000.

00:01:10.740 --> 00:01:12.796
Sad, HAL je bio svesni kompjuter

00:01:12.820 --> 00:01:15.276
osmišljen da vodi kosmički brod Diskaveri

00:01:15.300 --> 00:01:17.836
od Zemlje do Jupitera.

00:01:17.860 --> 00:01:19.916
HAL je takođe bio faličan lik

00:01:19.940 --> 00:01:24.220
jer je na kraju odabrao
vrednost misije umesto ljudskog života.

00:01:24.660 --> 00:01:26.756
Sad, HAL je bio fiktivan lik,

00:01:26.780 --> 00:01:29.436
ali bez obzira na to,
obraća se našim strahovima,

00:01:29.460 --> 00:01:31.556
našim strahovima da ćemo pasti pod uticaj

00:01:31.580 --> 00:01:34.596
nekakve bezosećajne,
veštačke inteligencije

00:01:34.620 --> 00:01:36.660
koja je ravnodušna prema našoj ljudskosti.

00:01:37.700 --> 00:01:40.276
Verujem da su takvi strahovi neosnovani.

00:01:40.300 --> 00:01:42.996
Uistinu se nalazimo u izvanrednom vremenu

00:01:43.020 --> 00:01:44.556
u ljudskoj istoriji,

00:01:44.580 --> 00:01:49.556
u kom, pokrenuti odbijanjem da prihvatimo
granice naših tela i naših umova,

00:01:49.580 --> 00:01:51.276
gradimo mašine

00:01:51.300 --> 00:01:54.916
izuzetne, prelepe složenosti i stila

00:01:54.940 --> 00:01:56.996
koje će da prošire ljudsko iskustvo

00:01:57.020 --> 00:01:58.700
na nezamislive načine.

00:01:59.460 --> 00:02:02.266
Nakon karijere koja seže
od akademije ratnog vazduhoplovstva

00:02:02.266 --> 00:02:04.076
preko svemirskog zapovednika do sada,

00:02:04.100 --> 00:02:05.796
postao sam inženjer za sisteme,

00:02:05.820 --> 00:02:08.556
a nedavno me je privukao
inženjerski problem

00:02:08.580 --> 00:02:11.156
u vezi sa NASA-inom misijom na Marsu.

00:02:11.180 --> 00:02:13.676
Sad, tokom svemirskih letova na Mesec

00:02:13.700 --> 00:02:16.836
možemo da se oslonimo
na kontrolu misije u Hjustonu

00:02:16.860 --> 00:02:18.836
da će da prati sve aspekte leta.

00:02:18.860 --> 00:02:22.396
Međutim, Mars je 200 puta dalje,

00:02:22.420 --> 00:02:25.636
i zbog toga je potrebno
u proseku 13 minuta

00:02:25.660 --> 00:02:28.796
za signal da putuje od Zemlje do Marsa.

00:02:28.820 --> 00:02:32.220
Ako imamo problem, nema dovoljno vremena.

00:02:32.660 --> 00:02:35.156
Pa razumno inženjeresko rešenje

00:02:35.180 --> 00:02:37.756
iziskuje od nas da smestimo
kontrolu misije

00:02:37.780 --> 00:02:40.796
unutar zidova kosmičkog broda Orion.

00:02:40.820 --> 00:02:43.716
Još jedna fantastična ideja
u profilu misije

00:02:43.740 --> 00:02:46.636
smešta humanoidne robote
na površinu Marsa

00:02:46.660 --> 00:02:48.516
pre nego što ljudi lično stignu,

00:02:48.540 --> 00:02:50.196
prvo da bi gradili objekte

00:02:50.220 --> 00:02:53.580
i kasnije da služe kao saradnici
u naučnoj ekipi.

00:02:55.220 --> 00:02:57.956
Sad, dok sam ovo posmatrao
sa inženjerske tačke gledišta,

00:02:57.980 --> 00:03:01.156
bilo mi je veoma jasno
da je potrebno da projektujem

00:03:01.180 --> 00:03:03.356
pametnu, sarađivačku,

00:03:03.380 --> 00:03:05.756
društveno inteligentnu
veštačku inteligenciju.

00:03:05.780 --> 00:03:10.076
Drugim rečima, trebalo je da sagradim
nešto prilično slično HAL-u,

00:03:10.100 --> 00:03:12.516
ali bez ubilačkih tendencija.

00:03:12.540 --> 00:03:13.900
(Smeh)

00:03:14.740 --> 00:03:16.556
Zastanimo na trenutak.

00:03:16.580 --> 00:03:20.476
Da li je zaista moguće sagraditi
takvu veštačku inteligenciju?

00:03:20.500 --> 00:03:21.956
Zapravo jeste.

00:03:21.980 --> 00:03:23.236
Na razne načine,

00:03:23.260 --> 00:03:25.236
ovo je težak inženjerski problem

00:03:25.260 --> 00:03:26.716
sa elementima VI,

00:03:26.740 --> 00:03:31.436
a ne nekakav neprijatan problem VI
koji je potrebno projektovati.

00:03:31.460 --> 00:03:34.116
Da parafraziram Alana Tjuringa:

00:03:34.140 --> 00:03:36.516
ne zanima me izgradnja svesne mašine.

00:03:36.540 --> 00:03:38.116
Ne pravim HAL-a.

00:03:38.140 --> 00:03:40.556
Ja tragam za prostim mozgom,

00:03:40.580 --> 00:03:43.700
nečim što pruža iluziju inteligencije.

00:03:44.820 --> 00:03:47.956
Umetnost nauke i računarstva
su prešle dug put

00:03:47.980 --> 00:03:49.476
od vremena prikazivanja HAL-a,

00:03:49.500 --> 00:03:52.716
i pretpostavljam da je njegov tvorac
dr Čandra ovde danas,

00:03:52.740 --> 00:03:55.076
imao bi gomilu pitanja za nas.

00:03:55.100 --> 00:03:57.196
Da li je zaista moguće da mi

00:03:57.220 --> 00:04:01.236
uzmemo sistem od miliona
i miliona uređaja,

00:04:01.260 --> 00:04:02.716
koji učitava dotok podataka,

00:04:02.720 --> 00:04:05.026
i da predvidimo njihove kvarove
i delamo unapred?

00:04:05.026 --> 00:04:06.220
Da.

00:04:06.220 --> 00:04:09.556
Možemo li sagraditi sisteme koji
pričaju s ljudima na prirodnom jeziku?

00:04:09.556 --> 00:04:10.676
Da.

00:04:10.700 --> 00:04:13.676
Možemo li sagraditi sisteme
koji prepoznaju predmete, osećanja,

00:04:13.700 --> 00:04:17.060
ispoljavaju emocije,
igraju igrice, čak i čitaju sa usana?

00:04:17.060 --> 00:04:18.110
Da.

00:04:18.110 --> 00:04:20.476
Možemo li sagraditi sistem
koji postavlja ciljeve,

00:04:20.500 --> 00:04:24.116
koji pravi planove za te ciljeve
i usput uči?

00:04:24.140 --> 00:04:25.356
Da.

00:04:25.380 --> 00:04:28.716
Možemo li sagraditi sisteme
koji sadrže teoriju uma?

00:04:28.740 --> 00:04:30.236
Ovo učimo da radimo.

00:04:30.260 --> 00:04:33.740
Možemo li sagraditi sisteme
koji imaju etičke i moralne osnove?

00:04:34.300 --> 00:04:36.340
Ovo moramo da naučimo da radimo.

00:04:37.180 --> 00:04:38.556
Zato, prihvatimo za trenutak

00:04:38.580 --> 00:04:41.476
da je moguće sagraditi
takvu veštačku inteligenciju

00:04:41.500 --> 00:04:43.636
za ovakvu i slične misije.

00:04:43.660 --> 00:04:46.196
Sledeće pitanje
koje morate postaviti sebi je:

00:04:46.220 --> 00:04:47.676
da li treba da je se plašimo?

00:04:47.700 --> 00:04:49.676
Sad, svaka nova tehnologija

00:04:49.700 --> 00:04:52.590
sa sobom nosi izvesnu dozu strepnje.

00:04:52.590 --> 00:04:54.356
Kada smo prvi put videli automobile,

00:04:54.356 --> 00:04:58.356
ljudi su se žalili da ćemo biti svedoci
uništenja porodice.

00:04:58.380 --> 00:05:01.076
Kada smo prvi put videli telefon,

00:05:01.100 --> 00:05:03.996
ljudi su bili zabrinuti
da će da uništi učtiv razgovor.

00:05:04.020 --> 00:05:07.950
U jednom trenutku smo videli
kako pisana reč počinje da preovlađuje,

00:05:07.950 --> 00:05:10.526
ljudi su smatrali da ćemo izgubiti
sposobnost pamćenja.

00:05:10.526 --> 00:05:12.556
Sve ovo je donekle tačno,

00:05:12.580 --> 00:05:14.996
ali je takođe tačno
da nam je ova tehnologija

00:05:15.020 --> 00:05:18.396
donela stvari koje su proširile
ljudsko iskustvo

00:05:18.420 --> 00:05:20.300
na izvesne suštinske načine.

00:05:21.660 --> 00:05:23.940
Stoga, pođimo malo dalje s ovim.

00:05:24.940 --> 00:05:29.676
Ne strahujem od stvaranja VI na ovaj način

00:05:29.700 --> 00:05:33.516
jer će vremenom
otelotvoriti neke od naših vrednosti.

00:05:33.540 --> 00:05:37.036
Razmotrite sledeće: izgradnja kognitivnog
sistema je temeljno različita

00:05:37.060 --> 00:05:40.356
od izgradnje tradicionalnog
softverskog sistema iz prošlosti.

00:05:40.380 --> 00:05:42.836
Ne programiramo ih. Podučavamo ih.

00:05:42.860 --> 00:05:45.516
Kako bih naučio sistem
da raspoznaje cveće,

00:05:45.540 --> 00:05:48.556
pokazujem mu na hiljade cvetova
koji se meni sviđaju.

00:05:48.580 --> 00:05:50.836
Kako bih naučio sistem da igra igru -

00:05:50.860 --> 00:05:52.820
Pa, bih. I vi biste.

00:05:54.420 --> 00:05:56.460
Volim cveće. Pa šta.

00:05:57.260 --> 00:06:00.116
Kako bih naučio sistem
da igra igru poput goa,

00:06:00.140 --> 00:06:02.196
morao bih da odigram
na hiljade partija goa,

00:06:02.220 --> 00:06:03.876
ali bih ga usput naučio

00:06:03.900 --> 00:06:06.316
kako da razlikuje dobru od loše partije.

00:06:06.340 --> 00:06:10.030
Ako bih želeo da stvorim pravnog
asistenta sa veštačkom inteligencijom,

00:06:10.030 --> 00:06:11.936
podučio bih ga zakonskim pravilncima,

00:06:11.936 --> 00:06:14.716
ali bih mu istovremeno s tim mešao

00:06:14.740 --> 00:06:17.620
osećanja milosti i pravde
koja su delovi tog zakona.

00:06:18.380 --> 00:06:21.356
U naučnom smislu, ovo je nešto
što nazivamo temeljnom istinom,

00:06:21.380 --> 00:06:23.396
a ovo je važna tačka:

00:06:23.420 --> 00:06:24.876
stvarajući ove mašine,

00:06:24.900 --> 00:06:28.316
sledi da ih učimo svesti
o našim vrednostima.

00:06:28.340 --> 00:06:31.476
U tom smislu, verujem
veštačkoj inteligenciji

00:06:31.500 --> 00:06:35.140
isto, ako ne i više,
kao i dobro obučenim ljudima.

00:06:35.900 --> 00:06:37.116
Međutim, možda se pitate,

00:06:37.140 --> 00:06:39.756
šta je sa odmetnicima,

00:06:39.780 --> 00:06:43.116
nekim dobro finansiranim
nevladinim organizacijama?

00:06:43.140 --> 00:06:46.956
Ne plašim se veštačke inteligencije
u rukama vukova samotnjaka.

00:06:46.980 --> 00:06:51.516
Očito se ne možemo zaštititi
od svih nasumičnih činova nasilja,

00:06:51.540 --> 00:06:53.676
ali u stvarnosti sličan sistem

00:06:53.700 --> 00:06:56.796
zahteva značajanu i prefinjenu obuku

00:06:56.820 --> 00:06:59.116
koja prevazilazi resurse pojedinaca.

00:06:59.140 --> 00:07:00.356
A zatim,

00:07:00.380 --> 00:07:03.636
to je daleko više od pukog ubrizgavanja
internet virusa u svet,

00:07:03.660 --> 00:07:06.756
gde pritisnete dugme
i iznenada se nalazi na milion mesta

00:07:06.780 --> 00:07:09.236
i laptopovi počinju svuda da eksplodiraju.

00:07:09.260 --> 00:07:12.076
Sad, ovakve materije su daleko veće,

00:07:12.100 --> 00:07:13.815
i sigurno ćemo ih doživeti.

00:07:14.340 --> 00:07:17.396
Da li me je strah da će takva
veštačka inteligencija

00:07:17.420 --> 00:07:19.380
možda ugroziti čitavo čovečanstvo?

00:07:20.100 --> 00:07:24.476
Ako pogledate filmove,
poput "Matriksa", "Metropolisa",

00:07:24.500 --> 00:07:27.676
"Terminatora",
serije poput "Zapadnog sveta",

00:07:27.700 --> 00:07:29.836
svi se oni bave tim strahom.

00:07:29.860 --> 00:07:34.156
Zaista, u knjizi "Superinteligencija"
filozofa Nika Bostroma,

00:07:34.180 --> 00:07:35.716
on se bavi ovom tematikom

00:07:35.740 --> 00:07:39.756
i razmatra da bi superinteligencija
mogla ne samo da bude opasna,

00:07:39.780 --> 00:07:43.636
već bi mogla predstavljati egzistencijalnu
pretnju čitavom čovečanstvu.

00:07:43.660 --> 00:07:45.876
Osnovni argument dr Bostroma

00:07:45.900 --> 00:07:48.636
je da bi slični sistemi vremenom

00:07:48.660 --> 00:07:51.916
imali takavu nezasitu žudnju
za informacijama

00:07:51.940 --> 00:07:54.836
da bi možda naučili kako da uče

00:07:54.860 --> 00:07:57.476
i vremenom bi otkrili
da možda imaju ciljeve

00:07:57.500 --> 00:07:59.796
koji su suprotni ljudskim potrebama.

00:07:59.820 --> 00:08:01.676
Dr Bostrom ima brojne pratioce.

00:08:01.700 --> 00:08:06.020
Podržavaju ga ljudi poput
Ilona Maska i Stivena Hokinga.

00:08:06.700 --> 00:08:09.100
Uz dužno poštovanje

00:08:09.980 --> 00:08:11.996
tim briljantnim umovima,

00:08:12.020 --> 00:08:14.276
verujem da suštinski greše.

00:08:14.300 --> 00:08:17.470
Sad, argument dr Bostroma
je suviše složen za razlaganje,

00:08:17.470 --> 00:08:19.676
i ja nemam vremena
da ga u potpunosti razložim,

00:08:19.676 --> 00:08:22.356
ali veoma kratko, razmotrite sledeće:

00:08:22.380 --> 00:08:26.110
superznanje je veoma različito
od superdelanja.

00:08:26.110 --> 00:08:28.066
HAL je bio pretnja za posadu Diskaverija,

00:08:28.066 --> 00:08:32.476
samo zato što je HAL zapovedao
svim aspektima Diskaverija.

00:08:32.500 --> 00:08:34.996
Dakle, moralo bi da se radi
o superinteligenciji.

00:08:35.020 --> 00:08:37.516
Morala bi da ima vlast
u čitavom našem svetu.

00:08:37.540 --> 00:08:40.356
Tako je u slučaju Skajneta
iz filma "Terminator"

00:08:40.380 --> 00:08:42.230
u kom smo imali superinteligenciju

00:08:42.230 --> 00:08:43.896
koja je zapovedala ljudskom voljom,

00:08:43.896 --> 00:08:47.516
koja je upravljala svakim uređajem
u svakom kutku sveta.

00:08:47.540 --> 00:08:48.996
Prosto govoreći,

00:08:49.020 --> 00:08:51.116
to se neće desiti.

00:08:51.140 --> 00:08:54.196
Ne pravimo VI
koja kontroliše vremenske prilike,

00:08:54.220 --> 00:08:55.556
koja upravlja plimama,

00:08:55.580 --> 00:08:58.956
koja zapoveda nama,
hirovitim, haotičnim ljudima.

00:08:58.980 --> 00:09:02.876
Potom, kad bi postojala
slična veštačka inteligencija,

00:09:02.900 --> 00:09:05.836
morala bi da se takmiči
sa ljudskim ekonomijama,

00:09:05.860 --> 00:09:08.380
a time i sa nama zbog resursa.

00:09:09.020 --> 00:09:10.236
I na kraju -

00:09:10.260 --> 00:09:11.500
nemojte ovo reći Siri -

00:09:12.260 --> 00:09:13.686
možemo uvek da ih isključimo.

00:09:13.686 --> 00:09:15.780
(Smeh)

00:09:17.180 --> 00:09:19.636
Na neverovatnom smo putu

00:09:19.660 --> 00:09:22.156
koevolucije sa našim mašinama.

00:09:22.180 --> 00:09:24.676
Ljudi kakvi smo danas,

00:09:24.700 --> 00:09:27.236
nećemo biti ljudi sutrašnjice.

00:09:27.260 --> 00:09:30.396
Briga zbog uspona superinteligencije

00:09:30.420 --> 00:09:33.476
je na razne načine
opasno odvraćanje pažnje

00:09:33.500 --> 00:09:35.836
jer uspon samog računarstva

00:09:35.860 --> 00:09:38.876
nam donosi brojna
ljudska i društvena pitanja

00:09:38.900 --> 00:09:40.540
kojima se odmah moramo baviti.

00:09:41.180 --> 00:09:43.996
Kako da organizujemo društvo
na najbolji način

00:09:44.020 --> 00:09:46.356
kada se smanji potreba
za ljudskom radnom snagom?

00:09:46.380 --> 00:09:50.196
Kako da donesemo razumevanje
i obrazovanje širom sveta,

00:09:50.220 --> 00:09:51.996
a da i dalje poštujemo naše razlike?

00:09:52.020 --> 00:09:56.276
Kako bismo mogli da proširimo i unapredimo
ljudski život kroz kognitivno zdravstvo?

00:09:56.300 --> 00:09:59.156
Kako da koristimo računare

00:09:59.180 --> 00:10:00.940
da nas odvedu do zvezda?

00:10:01.580 --> 00:10:03.620
A to je uzbudljivo.

00:10:04.220 --> 00:10:06.486
Šansa da se koriste računari

00:10:06.486 --> 00:10:08.236
da bismo unapredili ljudsko iskustvo

00:10:08.236 --> 00:10:09.556
su nam na dohvat ruke,

00:10:09.580 --> 00:10:11.436
ovde i sad,

00:10:11.460 --> 00:10:13.140
a tek smo počeli.

00:10:14.100 --> 00:10:15.316
Mnogo vam hvala.

00:10:15.340 --> 00:10:19.626
(Aplauz)

