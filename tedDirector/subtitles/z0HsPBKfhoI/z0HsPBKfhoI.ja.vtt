WEBVTT
Kind: captions
Language: ja

00:00:00.000 --> 00:00:07.000
翻訳: Yasushi Aoki
校正: Misaki Sato

00:00:12.580 --> 00:00:16.780
子供の頃 私はごく典型的な
オタク少年でした

00:00:17.140 --> 00:00:19.316
皆さんの中にも
いるでしょう

00:00:19.340 --> 00:00:20.556
(笑)

00:00:20.580 --> 00:00:23.926
そこで大声で笑った人 
あなたは今もそうでしょう

00:00:23.926 --> 00:00:26.076
(笑)

00:00:26.100 --> 00:00:29.596
北テキサスのほこりっぽい平原の
小さな町で育ち

00:00:29.620 --> 00:00:32.956
父は牧師の子で
保安官をしていました

00:00:32.980 --> 00:00:35.270
トラブルを起こすなんて論外です

00:00:35.860 --> 00:00:39.116
それで趣味として
解析学の本を読むようになりました

00:00:39.140 --> 00:00:40.676
(笑)

00:00:40.700 --> 00:00:42.396
あなたもですか？

00:00:42.420 --> 00:00:46.156
それでレーザーやコンピューターや
ロケットなんかを作るようになり

00:00:46.180 --> 00:00:49.860
さらには自分の部屋で
ロケット燃料まで作りました

00:00:51.340 --> 00:00:54.446
科学用語で言うと これは

00:00:54.446 --> 00:00:56.716
「とってもまずい考え」です

00:00:56.740 --> 00:00:57.956
(笑)

00:00:57.980 --> 00:01:00.040
同じ頃

00:01:00.040 --> 00:01:03.466
スタンリー・キューブリックの
『2001年宇宙の旅』が劇場公開され

00:01:03.466 --> 00:01:06.150
私の人生を永遠に
変えることになりました

00:01:06.150 --> 00:01:08.156
あの映画の
すべてが好きで

00:01:08.180 --> 00:01:10.716
ことに HAL 9000が
好きでした

00:01:10.740 --> 00:01:12.796
HALは 知覚力のある
コンピューターで

00:01:12.820 --> 00:01:15.686
宇宙船ディスカバリー号を
地球から木星へとガイドすべく

00:01:15.686 --> 00:01:17.836
設計されていました

00:01:17.860 --> 00:01:20.056
HALにはまた
性格上の欠陥があり

00:01:20.056 --> 00:01:24.580
最終的に人間の命よりも
ミッションを優先させます

00:01:24.580 --> 00:01:26.806
HALは架空のキャラクターですが

00:01:26.806 --> 00:01:29.436
私たちの恐怖を
呼び起こします

00:01:29.460 --> 00:01:31.556
人間に無関心な

00:01:31.580 --> 00:01:34.500
感情のない人工知能に

00:01:34.500 --> 00:01:36.980
支配されるという恐怖です

00:01:37.700 --> 00:01:40.276
そのような恐怖は
根拠のないものです

00:01:40.300 --> 00:01:41.670
私たちは実際

00:01:41.670 --> 00:01:44.556
人類史の中でも
目覚ましい時代にいます

00:01:44.580 --> 00:01:49.556
肉体や精神の限界を
受け入れることを拒み

00:01:49.580 --> 00:01:51.276
精緻で美しく

00:01:51.300 --> 00:01:54.916
複雑で優雅な機械を作り

00:01:54.940 --> 00:01:57.306
それが我々の想像を
超えるような仕方で

00:01:57.306 --> 00:01:59.530
人間の体験を
拡張することになるでしょう

00:01:59.540 --> 00:02:02.116
私は空軍士官学校を出て

00:02:02.140 --> 00:02:04.076
宇宙軍で働いた後

00:02:04.100 --> 00:02:05.796
システム屋になりましたが

00:02:05.820 --> 00:02:08.500
最近 NASAの
火星ミッションに関連する

00:02:08.500 --> 00:02:11.156
エンジニアリング上の問題へと
引き寄せられました

00:02:11.180 --> 00:02:13.676
月へ行くのであれば

00:02:13.700 --> 00:02:16.600
ヒューストンの
地上管制センターから

00:02:16.600 --> 00:02:18.996
飛行の全過程を
見守れます

00:02:18.996 --> 00:02:22.396
しかし火星は月より
200倍も離れています

00:02:22.420 --> 00:02:25.956
そのため 信号が地球から
火星に届くのには

00:02:25.956 --> 00:02:28.796
平均で13分もかかります

00:02:28.820 --> 00:02:32.630
トラブルが起きた場合 
そんなに待ってはいられません

00:02:32.660 --> 00:02:35.156
妥当な工学的解決策として

00:02:35.180 --> 00:02:36.970
オリオン号の壁の中に

00:02:36.970 --> 00:02:40.796
管制機能を設けることにしました

00:02:40.820 --> 00:02:43.716
ミッション概略にある
別の面白いアイデアとして

00:02:43.740 --> 00:02:46.636
人間型ロボットを火星表面に

00:02:46.660 --> 00:02:48.516
人間が行く前に送って

00:02:48.540 --> 00:02:50.120
施設を作らせ

00:02:50.120 --> 00:02:54.480
その後は 科学者チームの一員として
働かせるというのがあります

00:02:55.220 --> 00:02:58.286
これを工学的な観点で見て 
明らかになったのは

00:02:58.286 --> 00:03:00.860
ここで設計する必要があるのは

00:03:00.860 --> 00:03:03.250
賢く 協力的で

00:03:03.250 --> 00:03:05.856
社会的な人工知能
だということです

00:03:05.856 --> 00:03:10.076
言い換えると 何かHALのようなものを
作る必要があるということです

00:03:10.100 --> 00:03:12.516
ただし殺人癖は抜きで

00:03:12.540 --> 00:03:13.900
(笑)

00:03:14.590 --> 00:03:16.556
少し立ち止まって
考えてみましょう

00:03:16.580 --> 00:03:20.476
そのような人工知能を作ることは
可能なのでしょうか

00:03:20.500 --> 00:03:21.956
可能です

00:03:21.980 --> 00:03:23.236
何にせよ

00:03:23.260 --> 00:03:25.070
これは人工知能の
要素がある

00:03:25.070 --> 00:03:26.786
工学上の問題であって

00:03:26.786 --> 00:03:31.436
得体の知れない
人工知能の問題ではありません

00:03:31.460 --> 00:03:34.110
チューリングの言葉を
少し変えて言うと

00:03:34.110 --> 00:03:36.660
知覚力のある機械を作ることには
関心がありません

00:03:36.660 --> 00:03:38.260
HALを作ろうとは
していません

00:03:38.260 --> 00:03:40.556
私がやろうとしているのは
単純な脳

00:03:40.580 --> 00:03:44.080
知性の幻想を
提供する何かです

00:03:44.820 --> 00:03:46.510
HALが映画に現れて以来

00:03:46.510 --> 00:03:49.586
コンピューターの科学と技術は
大きく進歩しました

00:03:49.586 --> 00:03:52.716
HALの生みの親のチャンドラ博士が
ここにいたなら

00:03:52.740 --> 00:03:55.076
我々に聞きたいことが
山ほどあるはずです

00:03:55.100 --> 00:03:57.426
何百万 何千万という
デバイスを使い

00:03:57.426 --> 00:04:00.370
そのデータストリームを読んで

00:04:00.370 --> 00:04:02.330
故障を予期し

00:04:02.330 --> 00:04:04.996
前もって対処することは
できるのか？

00:04:05.020 --> 00:04:06.170
できます

00:04:06.170 --> 00:04:09.390
自然言語で人間と会話するシステムを
構築することはできるのか？

00:04:09.390 --> 00:04:10.240
できます

00:04:10.240 --> 00:04:14.436
物を認識し 人の感情を判断し 
自分の感情を表現し ゲームをし

00:04:14.436 --> 00:04:17.206
唇の動きすら読めるシステムを
作ることはできるのか？

00:04:17.206 --> 00:04:18.100
できます

00:04:18.100 --> 00:04:20.866
目標を設定し 
その実現のための計画を実行し

00:04:20.866 --> 00:04:24.116
その過程で学習するシステムを
作ることはできるのか？

00:04:24.140 --> 00:04:25.310
できます

00:04:25.310 --> 00:04:28.240
心の理論を備えたシステムを
作ることはできるのか？

00:04:28.240 --> 00:04:30.506
これは我々がやり方を
学ぼうとしていることです

00:04:30.506 --> 00:04:34.230
倫理的・道徳的基盤を持つシステムを
作ることはできるのか？

00:04:34.230 --> 00:04:36.870
これは我々がやり方を
学ぶ必要のあることです

00:04:36.870 --> 00:04:38.836
このようなミッションや
その他のことための

00:04:38.836 --> 00:04:41.460
人工知能を作ることは
可能であると

00:04:41.460 --> 00:04:43.630
認めることにしましょう

00:04:43.630 --> 00:04:45.670
次に問わなければ
ならないのは

00:04:45.670 --> 00:04:48.046
我々はそれを怖れるべきか
ということです

00:04:48.046 --> 00:04:49.590
どんな新技術も

00:04:49.590 --> 00:04:52.596
常にある程度の怖れは
引き起こすものです

00:04:52.620 --> 00:04:54.856
始めて自動車を
目にした人々は

00:04:54.856 --> 00:04:58.356
家族が壊されるのを見ることに
なるだろうと嘆いたものです

00:04:58.380 --> 00:05:01.000
始めて電話機を
目にした人々は

00:05:01.000 --> 00:05:04.276
礼儀にかなった会話が
損なわれると懸念したものです

00:05:04.276 --> 00:05:07.956
書かれたもので
溢れるのを見た人々は

00:05:07.980 --> 00:05:10.476
記憶力が失われるのでは
と思ったものです

00:05:10.500 --> 00:05:12.556
ある程度は合っていますが

00:05:12.580 --> 00:05:14.996
そういった技術は

00:05:15.020 --> 00:05:16.590
人間の体験を

00:05:16.590 --> 00:05:20.590
根本的に広げてもくれました

00:05:21.660 --> 00:05:23.940
さらに話を進めましょう

00:05:24.940 --> 00:05:29.676
私はそのような人工知能を
作ることに怖れは感じません

00:05:29.700 --> 00:05:33.516
それは人間の価値観を
体現することになるからです

00:05:33.540 --> 00:05:36.070
認知システムを作るのは

00:05:36.070 --> 00:05:40.356
従来のソフトウェア中心のシステムを
作るのとは 根本的に異なります

00:05:40.380 --> 00:05:42.836
プログラムするのではなく 
教えるのです

00:05:42.860 --> 00:05:45.516
システムに花を
認識させるために

00:05:45.540 --> 00:05:48.556
私は自分の好きな
何千という花を見せます

00:05:48.580 --> 00:05:50.836
システムにゲームの遊び方を
教えるには —

00:05:50.860 --> 00:05:53.810
私だってゲームはしますよ
皆さんもでしょう？

00:05:54.240 --> 00:05:57.200
花だって好きだし
らしくないですか？

00:05:57.200 --> 00:06:00.286
碁のようなゲームの遊び方を
システムに教えるには

00:06:00.286 --> 00:06:02.196
碁を何千回も指させ

00:06:02.220 --> 00:06:03.380
その過程で

00:06:03.380 --> 00:06:06.316
良い盤面・悪い盤面を
識別する方法を教えます

00:06:06.340 --> 00:06:10.036
人工知能の弁護士助手を
作ろうと思ったら

00:06:10.060 --> 00:06:11.836
法律も教えますが

00:06:11.860 --> 00:06:14.280
同時に法の一部をなす

00:06:14.280 --> 00:06:17.910
慈悲や公正の感覚を
吹き込むでしょう

00:06:18.250 --> 00:06:21.356
科学用語では これを
グランドトゥルースと言います

00:06:21.380 --> 00:06:23.320
重要なのは

00:06:23.320 --> 00:06:25.056
そういう機械を作るとき

00:06:25.056 --> 00:06:28.456
我々は自分の価値観を
教えることになるということです

00:06:28.456 --> 00:06:30.500
それだから私は 
人工知能を

00:06:30.500 --> 00:06:35.390
きちんと訓練された人間と同様に
信頼するのです

00:06:35.900 --> 00:06:37.946
でも悪いことをする工作員や

00:06:37.946 --> 00:06:41.376
ある種の資金豊富な
非政府組織なんかの

00:06:41.376 --> 00:06:43.116
手にかかったなら？

00:06:43.140 --> 00:06:46.956
一匹狼の扱う人工知能には
怖れを感じません

00:06:46.980 --> 00:06:51.516
あらゆる暴力から
身を守れるわけではありませんが

00:06:51.540 --> 00:06:53.676
そのようなシステムには

00:06:53.700 --> 00:06:56.796
個人のリソースの範囲を
大きく超えた

00:06:56.820 --> 00:06:59.060
膨大で精妙なトレーニングが
必要になります

00:06:59.060 --> 00:07:02.146
さらにそれは 単にインターネットへ
ウィルスを送り込むより

00:07:02.146 --> 00:07:03.686
遙かに大変なことです

00:07:03.686 --> 00:07:06.756
ウィルスならボタン１つで
そこら中のパソコンが

00:07:06.780 --> 00:07:09.236
突然吹き飛んで
しまうでしょうが

00:07:09.260 --> 00:07:11.840
そういうたぐいの
実体はずっと大きく

00:07:11.840 --> 00:07:14.265
それがやってくるのは
確かに目にすることになります

00:07:14.270 --> 00:07:17.060
そういう人工知能が

00:07:17.060 --> 00:07:19.990
全人類を脅かすのを
怖れるか？

00:07:19.990 --> 00:07:24.476
『マトリックス』『メトロポリス』
『ターミネーター』みたいな映画や

00:07:24.500 --> 00:07:27.676
『ウエストワールド』
みたいな番組を見ると

00:07:27.700 --> 00:07:29.836
みんなそのような恐怖を
語っています

00:07:29.860 --> 00:07:32.730
『スーパーインテリジェンス
(Superintelligence)』という本で

00:07:32.730 --> 00:07:35.716
思想家のニック・ボストロムは
このテーマを取り上げ

00:07:35.740 --> 00:07:39.756
人間を超える機械の知能は
危険なだけでなく

00:07:39.780 --> 00:07:43.636
人類存亡の危機に
つながり得ると見ています

00:07:43.660 --> 00:07:45.876
ボストロム博士の
基本的な議論は

00:07:45.900 --> 00:07:48.636
そのようなシステムはやがて

00:07:48.660 --> 00:07:51.916
抑えがたい情報への渇望を
抱くようになり

00:07:51.940 --> 00:07:54.836
学び方を学んで

00:07:54.860 --> 00:07:57.476
最終的には人間の要求に
反する目的を

00:07:57.500 --> 00:07:59.750
持つようになる
ということです

00:07:59.750 --> 00:08:01.786
ボストロム博士には
多くの支持者がいて

00:08:01.786 --> 00:08:06.430
その中にはイーロン・マスクや
スティーヴン・ホーキングもいます

00:08:06.700 --> 00:08:09.710
そのような聡明な方々に

00:08:09.710 --> 00:08:11.996
恐れながら申し上げると

00:08:12.020 --> 00:08:14.276
彼らは根本的に
間違っていると思います

00:08:14.300 --> 00:08:17.476
検討すべきボストロム博士の議論は
沢山ありますが

00:08:17.500 --> 00:08:19.636
全部見ていく
時間はないので

00:08:19.660 --> 00:08:22.356
ごく簡単に
１点だけ挙げるなら

00:08:22.380 --> 00:08:26.020
「すごく知っている」のと 
「すごいことができる」のとは違うということです

00:08:26.020 --> 00:08:30.066
HALは ディスカバリー号のあらゆる面を
コントロールする限りにおいて

00:08:30.066 --> 00:08:32.476
乗組員にとって脅威でした

00:08:32.500 --> 00:08:34.996
スーパーインテリジェンスも
そうです

00:08:35.020 --> 00:08:37.516
それが世界全体を支配している
必要があります

00:08:37.540 --> 00:08:40.356
スーパーインテリジェンスが
人の意志を支配する

00:08:40.380 --> 00:08:42.236
『ターミネーター』の世界で

00:08:42.260 --> 00:08:45.536
スカイネットは世界の
あらゆるデバイスを

00:08:45.536 --> 00:08:47.516
操っていました

00:08:47.540 --> 00:08:48.996
実際のところ

00:08:49.020 --> 00:08:51.116
そんなことは
起こりません

00:08:51.140 --> 00:08:54.180
天気を制御したり
潮の干満を決めたり

00:08:54.180 --> 00:08:57.086
気まぐれで無秩序な人間を
従わせるような人工知能を

00:08:57.086 --> 00:08:58.956
我々は作りはしません

00:08:58.980 --> 00:09:02.876
もしそのような人工知能が
存在したら

00:09:02.900 --> 00:09:05.836
人間の経済と
競合することになり

00:09:05.860 --> 00:09:08.810
リソースを人間と
取り合うことになるでしょう

00:09:09.020 --> 00:09:10.236
最終的には

00:09:10.260 --> 00:09:12.100
Siriには内緒ですが

00:09:12.100 --> 00:09:14.436
我々は電源プラグを
引っこ抜くことができます

00:09:14.436 --> 00:09:15.780
(笑)

00:09:17.180 --> 00:09:19.636
私たちは機械と
共進化していく

00:09:19.660 --> 00:09:22.156
ものすごい旅の
途上にあります

00:09:22.180 --> 00:09:23.660
今日の人類は

00:09:23.660 --> 00:09:27.236
明日の人類とは違っています

00:09:27.260 --> 00:09:30.396
人間を超えた人工知能の
台頭を懸念するのは

00:09:30.420 --> 00:09:33.330
コンピューターの台頭自体が
引き起こす

00:09:33.330 --> 00:09:36.246
対処を要する
人間や社会の問題から

00:09:36.246 --> 00:09:38.876
注意をそらすことになり

00:09:38.900 --> 00:09:40.450
危険です

00:09:41.180 --> 00:09:43.996
人間の労働の必要が
減っていく社会を

00:09:44.020 --> 00:09:46.356
どうすれば上手く
運営できるのか？

00:09:46.380 --> 00:09:48.740
理解と教育を
地球全体に広げつつ

00:09:48.740 --> 00:09:52.156
互いの違いに敬意を払うことは 
どうすれば可能か？

00:09:52.156 --> 00:09:56.586
認知システムによる医療で 人の生涯を
長く豊かなものにするにはどうしたら良いか？

00:09:56.586 --> 00:09:58.950
星々に到るために

00:09:58.950 --> 00:10:01.500
コンピューターは
いかに役立てられるか？

00:10:01.580 --> 00:10:03.980
これはワクワクすることです

00:10:04.220 --> 00:10:06.556
コンピューターを使って

00:10:06.580 --> 00:10:08.116
人間の体験を

00:10:08.140 --> 00:10:09.836
発展させられる機会が

00:10:09.836 --> 00:10:11.436
今 手の届くところにあり

00:10:11.460 --> 00:10:13.780
それは始まったばかりです

00:10:14.100 --> 00:10:15.316
ありがとうございました

00:10:15.340 --> 00:10:19.626
(拍手)

