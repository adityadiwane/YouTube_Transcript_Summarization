WEBVTT
Kind: captions
Language: ko

00:00:00.000 --> 00:00:07.000
번역: Yeonsoo Kwon
검토: Jihyeon J. Kim

00:00:12.580 --> 00:00:16.420
제가 어렸을 적,
저는 전형적인 괴짜였습니다.

00:00:17.140 --> 00:00:19.316
여러분 중 몇몇도 그랬을 거예요.

00:00:19.340 --> 00:00:20.556
(웃음)

00:00:20.580 --> 00:00:23.796
거기 가장 크게 웃으신 분은
여전히 그럴 것 같군요.

00:00:23.820 --> 00:00:26.076
(웃음)

00:00:26.100 --> 00:00:29.596
저는 북텍사스 황무지의 작은 마을에서

00:00:29.620 --> 00:00:32.956
목사인 할아버지와 
보안관인 아버지 밑에서 자랐습니다.

00:00:32.980 --> 00:00:34.900
말썽피우는 일은 생각도 
할 수 없었습니다.

00:00:35.860 --> 00:00:39.116
그래서 저는 미적분학 책을 
재미로 읽었습니다.

00:00:39.140 --> 00:00:40.676
(웃음)

00:00:40.700 --> 00:00:42.396
당신도 그랬잖아요.

00:00:42.420 --> 00:00:46.156
그것으로 저는 레이저, 컴퓨터와
모델 로켓을 만들었지요.

00:00:46.180 --> 00:00:49.180
그러다 보니 제 방에 
로켓 연료를 만들게 됐어요.

00:00:49.780 --> 00:00:53.436
현재, 과학적 관점에서

00:00:53.460 --> 00:00:56.716
우리는 이것을 매우 
나쁜 생각이라고 하죠.

00:00:56.740 --> 00:00:57.956
(웃음)

00:00:57.980 --> 00:01:00.156
또한 그 시절에

00:01:00.180 --> 00:01:03.396
스탠리 큐브릭의 
"2001년: 스페이스 오디세이"가

00:01:03.420 --> 00:01:05.620
개봉하였으며 제 삶은 완전히 바뀌었죠.

00:01:06.100 --> 00:01:08.156
저는 그 영화의 모든 것이 좋았습니다.

00:01:08.180 --> 00:01:10.716
특히 HAL 9000은 특별했죠.

00:01:10.740 --> 00:01:12.796
HAL은 지각을 할 수 있는 컴퓨터로

00:01:12.820 --> 00:01:15.276
디스커버리호가 지구에서 목성으로 가도록

00:01:15.300 --> 00:01:17.836
가이드하는 역할을 했습니다.

00:01:17.860 --> 00:01:19.916
HAL은 또한 결함이 있어서

00:01:19.940 --> 00:01:24.220
결국 끝엔 인간의 생명을 뒤로하고 
주어진 임무를 택하게 됩니다.

00:01:24.660 --> 00:01:26.756
HAL은 허구적 캐릭터이지만

00:01:26.780 --> 00:01:27.920
그럼에도 불구하고 

00:01:27.920 --> 00:01:30.090
그는 우리에게 공포감을 선사합니다.

00:01:30.090 --> 00:01:34.596
인류에게 냉담한 감정이 없는 
인공적인 지능에게

00:01:34.620 --> 00:01:36.580
지배당하게 될 것이라는 공포죠.

00:01:37.700 --> 00:01:40.276
저는 그러한 공포가 
근거 없다고 생각합니다.

00:01:40.300 --> 00:01:42.996
확실히 우리는 인간 역사에서

00:01:43.020 --> 00:01:44.556
매우 중요한 시기에 있습니다.

00:01:44.580 --> 00:01:49.556
우리는 우리 몸과 정신의 
한계를 받아들이길 거부하며

00:01:49.580 --> 00:01:51.276
기계를 만들어내고 있습니다.

00:01:51.300 --> 00:01:54.916
우아하고 복잡하면서도
아름답고 정교하게요.

00:01:54.940 --> 00:01:56.996
그리고 그것은 인간의 경험을

00:01:57.020 --> 00:01:58.700
상상을 초월할만큼 확장시킬 것입니다.

00:01:59.540 --> 00:02:02.116
저는 공군사관학교에서

00:02:02.140 --> 00:02:04.076
현재 우주 사령부로 와서

00:02:04.100 --> 00:02:05.566
시스템 엔지니어가 됐고

00:02:05.566 --> 00:02:10.880
최근 나사의 화성과 관련된 
기술문제에 관여하고 있습니다.

00:02:11.180 --> 00:02:13.676
오늘날 우리는
휴스턴 우주 비행 관제센터를 통해

00:02:13.700 --> 00:02:16.836
달을 향한 우주 비행을

00:02:16.860 --> 00:02:18.836
모든 측면에서 지켜볼 수 있습니다.

00:02:18.860 --> 00:02:22.396
하지만 화성은 
200배 멀리 떨어져 있으며

00:02:22.420 --> 00:02:28.856
신호가 지구에서 화성까지
평균적으로 13분 걸립니다.

00:02:28.856 --> 00:02:32.220
문제가 있다면 
시간이 충분하지 않다는 것이지요.

00:02:32.660 --> 00:02:35.156
그리고 합당한 기술적 방안은

00:02:35.180 --> 00:02:37.756
우주 비행 관제 센터를

00:02:37.780 --> 00:02:40.796
오리온 우주선 벽 안에 넣는 것이지요.

00:02:40.820 --> 00:02:43.716
다른 흥미로운 임무 개요 아이디어는

00:02:43.740 --> 00:02:46.636
사람이 직접 도착하기 전에

00:02:46.660 --> 00:02:48.516
인간형 로봇을 먼저 화성에 두어

00:02:48.540 --> 00:02:50.196
시설들을 먼저 짓게 한 후

00:02:50.220 --> 00:02:53.580
과학팀의 협력 멤버로 두는 것입니다.

00:02:55.220 --> 00:02:57.956
이제 제가 이것을 기술적 측면에서 보면

00:02:57.980 --> 00:03:01.156
제가 만들어야 하는 것이

00:03:01.180 --> 00:03:03.356
똑똑하고 협력적인

00:03:03.380 --> 00:03:05.756
사회적 지능의 인공 지능임을 
알 수 있습니다.

00:03:05.780 --> 00:03:10.076
다른 말로, 저는 살인하는 경향이 없는

00:03:10.100 --> 00:03:12.516
HAL을 만들어야 한다는 것입니다.

00:03:12.540 --> 00:03:13.900
(웃음)

00:03:14.740 --> 00:03:16.556
잠시만 멈추고 생각하여봅시다.

00:03:16.580 --> 00:03:20.476
그러한 인공지능을 만드는 것이 
가능할까요?

00:03:20.500 --> 00:03:21.956
사실, 가능합니다.

00:03:21.980 --> 00:03:23.236
다양한 측면에서

00:03:23.260 --> 00:03:26.756
이것은 AI 요소가 있는 
어려운 기술 문제이지

00:03:26.756 --> 00:03:31.436
가벼운 설계적 인공지능 
문제가 아닙니다.

00:03:31.460 --> 00:03:34.116
앨런 튜링의 말을 정리하여 보자면

00:03:34.140 --> 00:03:36.516
저는 감각이 있는 기계를 만드는 것에
흥미가 있는 것이 아닙니다.

00:03:36.540 --> 00:03:38.116
저는 HAL을 만드는 것이 아닙니다.

00:03:38.140 --> 00:03:40.556
저는 지능의 환상을 주는

00:03:40.580 --> 00:03:43.700
간단한 두뇌를 만들려는 것일 뿐입니다.

00:03:44.820 --> 00:03:46.700
HAL이 상영된 후에

00:03:46.700 --> 00:03:49.476
기계적 과학과 예술은 발전해왔으며

00:03:49.500 --> 00:03:52.716
만일 발명가 샨드라가 
오늘 이곳에 있었더라면

00:03:52.740 --> 00:03:55.076
우리에게 많은 질문이 있었을 거라고
저는 상상합니다.

00:03:55.100 --> 00:03:57.196
수많은 기계들중 하나의 시스템을 뽑아

00:03:57.220 --> 00:04:01.236
데이타의 흐름을 읽고

00:04:01.260 --> 00:04:02.716
그들의 실패를 예견하는게

00:04:02.740 --> 00:04:04.996
정말 가능한 것일까요?

00:04:05.020 --> 00:04:06.236
네.

00:04:06.260 --> 00:04:09.436
우리와 자연스럽게 대화하는 시스템을
만들 수 있을까요?

00:04:09.460 --> 00:04:10.676
네.

00:04:10.700 --> 00:04:13.676
물건을 인식하고, 감정을 알아보며,
감정을 나타내고

00:04:13.700 --> 00:04:17.076
게임을 하며, 입술을 읽는,
그런 시스템을 만들 수 있을까요?

00:04:17.100 --> 00:04:18.316
네.

00:04:18.340 --> 00:04:22.116
목표를 세워 그 목표를 향해 나아가며

00:04:22.116 --> 00:04:24.116
그 과정에서 학습하는 
시스템을 만들 수 있을까요?

00:04:24.140 --> 00:04:25.356
네.

00:04:25.380 --> 00:04:28.716
마음 이론을 가지고 있는 
시스템을 만들 수 있을까요?

00:04:28.740 --> 00:04:30.236
이것을 향해 우리는 나아가고 있습니다.

00:04:30.260 --> 00:04:33.740
윤리적, 도덕적 배경을 가지고 있는 
시스템을 만들 수 있을까요?

00:04:34.300 --> 00:04:36.340
우리가 해야만 하는 것입니다.

00:04:37.180 --> 00:04:38.556
그러므로 이러한 사명을 가진

00:04:38.580 --> 00:04:41.476
인공지능을 만드는 것이 가능하다는 것을

00:04:41.500 --> 00:04:43.636
받아들여 봅시다.

00:04:43.660 --> 00:04:46.196
그렇다면 이 질문을 
스스로에게 물어보세요.

00:04:46.220 --> 00:04:47.676
우리는 두려워해야 할까요?

00:04:47.700 --> 00:04:49.676
오늘날 모든 새로운 기술은

00:04:49.700 --> 00:04:52.596
어느 정도의 두려움을 가져오지요.

00:04:52.620 --> 00:04:54.316
우리가 처음 차를 보았을 때

00:04:54.340 --> 00:04:58.356
사람들은 그것이 가족들을 
무너뜨릴까봐 걱정했었습니다.

00:04:58.380 --> 00:05:01.076
우리가 처음 전화를 보았을 때

00:05:01.100 --> 00:05:03.996
사람들은 그것이 대화를 없애버릴까봐
걱정했었습니다.

00:05:04.020 --> 00:05:07.956
글자를 쓰는것이 보편화 되어갈 때

00:05:07.980 --> 00:05:10.476
사람들은 기억력을 잃게 
될 거라고 생각했었죠.

00:05:10.500 --> 00:05:12.556
어느 정도 맞는 말일 수 있지만

00:05:12.580 --> 00:05:18.426
기술이 사람의 삶을 
더 깊게 해주는 경우도

00:05:18.426 --> 00:05:20.300
있었다고 볼 수 있죠.

00:05:21.660 --> 00:05:23.940
더 나아가봅시다.

00:05:24.940 --> 00:05:29.676
저는 인공지능이 우리의 가치를
궁극적으로 구현할 것이라고 믿기에

00:05:29.700 --> 00:05:33.516
인공지능의 발명을 
두려워하지 않습니다.

00:05:33.540 --> 00:05:37.036
이것을 고려하세요:
인식적인 시스템을 만드는 것은

00:05:37.060 --> 00:05:40.356
과거의 소프트웨어 중심 시스템을
만드는 것과 다릅니다.

00:05:40.380 --> 00:05:42.836
우리는 그것을 
프로그램하지 않고 가르칩니다.

00:05:42.860 --> 00:05:45.516
꽃을 알아보는것을 시스템에게 
가르치기 위해서는

00:05:45.540 --> 00:05:48.556
수천 종류의 꽃을 보여주여야 합니다.

00:05:48.580 --> 00:05:50.836
시스템에게 게임하는 법을
가르치기 위해서는

00:05:50.860 --> 00:05:52.820
저도 그러고 싶고, 
여러분도 그러겠지요.

00:05:54.420 --> 00:05:56.460
저는 꽃을 좋아해요, 정말로요.

00:05:57.260 --> 00:06:00.116
시스템에게 바둑같은 게임을 
가르치기 위해서는

00:06:00.140 --> 00:06:02.196
수천 번의 바둑 게임을 
해주어야 할 것이며

00:06:02.220 --> 00:06:03.876
그 과정에서 잘한 게임과 못한 게임을

00:06:03.900 --> 00:06:06.316
구별하는것도 가르쳐야겠지요.

00:06:06.340 --> 00:06:10.036
인공 지능에게 법률 보조일을 
시키기 위해서는

00:06:10.060 --> 00:06:11.836
대전 뿐만이 아니라

00:06:11.860 --> 00:06:17.406
자비와 정의등의 개념 
역시 가르쳐야겠지요.

00:06:18.380 --> 00:06:21.356
과학적으로, 우리가 말하는 
근본적인 진실 및

00:06:21.380 --> 00:06:23.396
중요한 사실이 여기 있습니다:

00:06:23.420 --> 00:06:24.876
이러한 기계를 만들기 위해서는

00:06:24.900 --> 00:06:28.316
우리는 우리의 가치를 그들에게
가르쳐야합니다.

00:06:28.340 --> 00:06:31.476
그 목적을 위해서, 저는 인공지능이

00:06:31.500 --> 00:06:35.140
잘 훈련받은 사람보다 같거나
더 대단하다고 믿습니다.

00:06:35.900 --> 00:06:37.116
하지만 여러분

00:06:37.140 --> 00:06:39.756
잘 설립된 로그 에이전트와 같은

00:06:39.780 --> 00:06:43.116
비정부기관에 대하여 물을수 있겠죠?

00:06:43.140 --> 00:06:46.956
저는 외로운 늑대에게 있는
인공지능을 두려워하지 않습니다.

00:06:46.980 --> 00:06:51.516
분명히, 우리는 마구잡이의 폭력에서
우리를 보호할 수 없지만

00:06:51.540 --> 00:06:53.676
실제적으로 보았을때

00:06:53.700 --> 00:06:56.796
이는 근본적인 트레이닝을 요구하고

00:06:56.820 --> 00:06:59.116
이것은 개인의 능력치 
바깥에 있는 일입니다.

00:06:59.140 --> 00:07:00.356
더 나아가 이것은

00:07:00.380 --> 00:07:03.636
인터넷 바이러스를 세상에 주입하여

00:07:03.660 --> 00:07:06.756
하나의 버튼을 눌러
전세계 인터넷을 폭파시키는것보다

00:07:06.780 --> 00:07:09.236
더 힘든 일입니다.

00:07:09.260 --> 00:07:12.076
이제 이러한 것들은 훨씬 커졌으며

00:07:12.100 --> 00:07:13.815
우리는 이것들이 다가오는것을
확실하게 볼 수 있죠.

00:07:14.340 --> 00:07:17.396
제가 인공지능이 인류를 위협할까봐

00:07:17.420 --> 00:07:19.380
두려워할까요?

00:07:20.100 --> 00:07:24.476
"메트릭스", "메트로폴리스", 
"터미네이터" 같은

00:07:24.500 --> 00:07:27.676
서구의 영화들을 보면

00:07:27.700 --> 00:07:29.836
영화들은 이러한 두려움을
다루고 있습니다.

00:07:29.860 --> 00:07:34.156
철학자 닉보스트롬의 
"초지능"이라는 책에서

00:07:34.180 --> 00:07:35.716
그는 이 주제를 다루며

00:07:35.740 --> 00:07:39.756
초지능이 위험할 뿐만 아니라

00:07:39.780 --> 00:07:43.636
인류에 있어서 모든 위험을
대변한다고 합니다.

00:07:43.660 --> 00:07:45.876
필자의 근본적인 주장은

00:07:45.900 --> 00:07:48.636
이러한 시스템이 결국

00:07:48.660 --> 00:07:51.916
탐욕적으로 정보를 원하게 되어

00:07:51.940 --> 00:07:54.836
인간의 필요와

00:07:54.860 --> 00:07:57.476
반대되는 목적을

00:07:57.500 --> 00:07:59.796
이루게 될거라는 것이죠.

00:07:59.820 --> 00:08:01.676
보스트롬 교수를 
따르는 사람들이 많습니다.

00:08:01.700 --> 00:08:06.020
엘론머스크와 스티븐 호킹같은 
사람들의 지지를 받고있죠.

00:08:06.700 --> 00:08:09.100
천부적인 마음들에

00:08:09.980 --> 00:08:11.996
외람된 말씀입니다만

00:08:12.020 --> 00:08:14.276
저는 그들이 근본적으로 
틀렸다고 생각합니다.

00:08:14.300 --> 00:08:17.476
보스트롬 교수의 주장에는
살펴보아야 할 것들이 많고

00:08:17.500 --> 00:08:19.636
지금 이것을 다 다룰 시간은 없지만

00:08:19.660 --> 00:08:22.356
아주 간략하게, 이것을 생각하여보세요.

00:08:22.380 --> 00:08:26.116
많이 아는 것은 
많이 하는 것과 다릅니다.

00:08:26.140 --> 00:08:28.036
HAL이 탐사팀에게 
위협이 되었던 이유는

00:08:28.060 --> 00:08:32.476
단지 HAL이 탐사의 모든 측면에서
명령을 내렸기 때문이죠.

00:08:32.500 --> 00:08:34.996
초지능과 관련이 있습니다.

00:08:35.020 --> 00:08:37.516
그것은 우리 세상의 지배권을 
가지게 될것입니다.

00:08:37.540 --> 00:08:40.356
초지능이 사람이 원하는것을

00:08:40.380 --> 00:08:42.236
명령받하게 되는것은

00:08:42.260 --> 00:08:43.636
영화 "터미네이터"의 
스카이넷을 통해 다뤄지는데

00:08:43.660 --> 00:08:47.516
이는 모든 기기들이
전세계에 다 있는 경우입니다.

00:08:47.540 --> 00:08:48.996
솔직하게 얘기하여보자면,

00:08:49.020 --> 00:08:51.116
이것은 일어나지 않을 일입니다.

00:08:51.140 --> 00:08:54.196
우리는 날씨를 조정하거나, 
파도를 이끌거나

00:08:54.220 --> 00:08:55.556
예측할수 없고 혼란스러운
사람을 명령하는

00:08:55.580 --> 00:08:58.956
인공지능을 만들고있는것이 아닙니다.

00:08:58.980 --> 00:09:02.876
그리고 이러한 인공지능이 있다면,

00:09:02.900 --> 00:09:05.836
그것은 자원을 위하여

00:09:05.860 --> 00:09:08.380
인류의 경제와 경쟁해야겠지요.

00:09:09.020 --> 00:09:10.236
결국에는

00:09:10.260 --> 00:09:11.500
시리에게는 말하지 마세요.

00:09:12.260 --> 00:09:13.636
우리는 언제나 
전원을 빼버릴 수 있습니다.

00:09:13.660 --> 00:09:15.780
(웃음)

00:09:17.180 --> 00:09:19.636
우리는 기계들과 공진화하는

00:09:19.660 --> 00:09:22.156
놀랄만한 여행을 하고 있습니다.

00:09:22.180 --> 00:09:24.676
미래의 인간들은

00:09:24.700 --> 00:09:27.236
오늘날의 인간들과 다르겠지요.

00:09:27.260 --> 00:09:30.396
초지능의 출현을 지금 걱정하는것은

00:09:30.420 --> 00:09:33.476
기술의 발전이 우리에게

00:09:33.500 --> 00:09:35.836
관심을 가져야만하는 많은

00:09:35.860 --> 00:09:38.876
사회적, 인류적 이슈를 가져오기에

00:09:38.900 --> 00:09:40.540
위험한 분산이라고 볼 수 있습니다.

00:09:41.180 --> 00:09:43.996
그럼 어떻게 해야 인간의 노동이 
덜 중요해진 사회를

00:09:44.020 --> 00:09:46.356
잘 구성할 수 있을까요?

00:09:46.380 --> 00:09:50.196
어떻게 해야 차이점을 존중하며

00:09:50.220 --> 00:09:51.996
교육과 이해를 세계적으로 
가져올 수 있을까요?

00:09:52.020 --> 00:09:56.276
인식적인 건강관리 시스템으로
어떻게 인간의 삶을 발전시킬까요?

00:09:56.300 --> 00:09:59.156
어떻게 기술적인 것들을 이용하여

00:09:59.180 --> 00:10:00.940
우리가 별을 따도록 도울 수 있을까요?

00:10:01.580 --> 00:10:03.620
그것이 이 일의 신나는 부분입니다.

00:10:04.220 --> 00:10:06.556
이제 우리는

00:10:06.580 --> 00:10:09.596
기계를 통한 인류 
발전을 이룰 수 있으며

00:10:09.596 --> 00:10:13.256
지금 이곳에서 우리는 
시작하고 있습니다.

00:10:14.100 --> 00:10:15.316
감사합니다.

00:10:15.340 --> 00:10:19.626
(박수)

