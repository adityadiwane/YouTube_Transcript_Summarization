WEBVTT
Kind: captions
Language: fa

00:00:00.000 --> 00:00:07.000
Translator: Leila Ataei
Reviewer: soheila Jafari

00:00:12.580 --> 00:00:16.420
وقتی بچه بودم،‌
اصل درسخوان بودم.

00:00:17.140 --> 00:00:19.316
فکر کنم بعضی از شما همینطوری بودید.

00:00:19.340 --> 00:00:20.556
(خنده)

00:00:20.580 --> 00:00:23.796
و شما آقا که از همه بلندتر خندیدی، 
احتمالا هنوز هم هستی.

00:00:23.820 --> 00:00:26.076
(خنده)

00:00:26.100 --> 00:00:29.596
در شهری کوچک در دشتهای غبارآلود
شما تگزاس بزرگ شدم،

00:00:29.620 --> 00:00:32.956
پسر یک کلانتر که خودش 
پسر یک کشیش بود.

00:00:32.980 --> 00:00:34.900
پس جایی برای خرابکاری نمیماند.

00:00:35.860 --> 00:00:39.116
و من محض تفریح کتابهای حساب
میخواندم.

00:00:39.140 --> 00:00:40.676
(خنده)

00:00:40.700 --> 00:00:42.396
شما هم اینطور بودید.

00:00:42.420 --> 00:00:46.156
که باعث شد یک لیزر و کامپیوتر
و مدل موشکی بسازم،

00:00:46.180 --> 00:00:49.180
و این که باعث شد در اتاق خوابم 
سوخت موشک بسازم.

00:00:49.780 --> 00:00:53.436
الان در عبارات علمی

00:00:53.460 --> 00:00:56.716
این را یک ایده بد میخوانیم.

00:00:56.740 --> 00:00:57.956
(خنده)

00:00:57.980 --> 00:01:00.156
و هم زمان

00:01:00.180 --> 00:01:03.396
فیلم « ۲۰۰۱: یک ادیسه فضایی»
استنلی کوبریک به روی پرده آمد،

00:01:03.420 --> 00:01:05.620
و زندگی من برای همیشه تغییر کرد.

00:01:06.100 --> 00:01:08.156
از همه چیز فیلم خوشم میامد

00:01:08.180 --> 00:01:10.716
بخصوص اچ‌ای‌ال ۹۰۰۰.

00:01:10.740 --> 00:01:12.796
این اچ‌ای‌ال یک کامپیوتر احساسی بود

00:01:12.820 --> 00:01:15.276
که جهت راهنمایی سفینه فضایی
دیسکاوری در رفتن از زمین

00:01:15.300 --> 00:01:17.836
به مشتری طراحی شده بود.

00:01:17.860 --> 00:01:19.916
ای‌اچ‌ال همینطور یک شخصیت معیوب بود.

00:01:19.940 --> 00:01:24.220
چون در آخر ماموریت را بجای جان بشر
انتخاب کرد.

00:01:24.660 --> 00:01:26.756
ای‌اچ ال یک شخصیت تخیلی بود

00:01:26.780 --> 00:01:29.436
اما با این وجود 
درباره ترسهای ما حرف می‌زد،

00:01:29.460 --> 00:01:31.556
ترسمان از مقهور شدن از سوی یک

00:01:31.580 --> 00:01:34.596
هوش مصنوعی بی‌احساس

00:01:34.620 --> 00:01:36.580
که به بشریت بی‌اعتناست.

00:01:37.700 --> 00:01:40.276
به باور من این ترسها بی‌اساس است.

00:01:40.300 --> 00:01:42.996
ما در واقع در برهه قابل توجهی
از تاریخ بشر

00:01:43.020 --> 00:01:44.556
بسر می‌بریم،

00:01:44.580 --> 00:01:49.556
جاییکه با نگیزش انکار در پذیرش 
محدودیتهای ذهنی و جسمی خود

00:01:49.580 --> 00:01:51.276
ماشینهایی را می سزیم

00:01:51.300 --> 00:01:54.916
که با پیچیدگی زیبا، منحصربفرد و 
زیبندگی

00:01:54.940 --> 00:01:56.996
تجربه بشری را تا ورای تصورات ما

00:01:57.020 --> 00:01:58.700
گسترش خواهند داد.

00:01:59.540 --> 00:02:02.116
بعد از مسیر حرفه‌ای که من
را از آکادمی نیروی هوایی

00:02:02.140 --> 00:02:04.076
به فرماندهی فضایی و الان

00:02:04.100 --> 00:02:05.796
به این مهندس سیستمی که شدم کشاند،

00:02:05.820 --> 00:02:08.556
و تازگی درگیر مشکل مهندسی شده‌ام

00:02:08.580 --> 00:02:11.156
که مرتبط با ماموریت ناسا به مریخ است.

00:02:11.180 --> 00:02:13.676
الان در سفرهای فضایی به ماه

00:02:13.700 --> 00:02:16.836
می‌توانیم به کنترل ماموریت
در هوستون تکیه کنیم

00:02:16.860 --> 00:02:18.836
تا کلیه جوانب یک پرواز را نظارت کند.

00:02:18.860 --> 00:02:22.396
هرچند، مریخ ۲۰۰ بار دورتر است،

00:02:22.420 --> 00:02:25.636
و در نتیجه بطور متوسط ۱۳ دقیقه
طول می‌کشد

00:02:25.660 --> 00:02:28.796
تا یک سیگنال از زمین به ماه برسد.

00:02:28.820 --> 00:02:32.220
اگر مشکلی باشد
زمان کافی نداریم.

00:02:32.660 --> 00:02:35.156
و برای یک راه‌حل مهندسی منطقی

00:02:35.180 --> 00:02:37.756
از ما این را طلب می‌کند
که کنترل گر ماموریت

00:02:37.780 --> 00:02:40.796
داخل دیوارهای فضای پیمای اوریون باشد.

00:02:40.820 --> 00:02:43.716
یک ایده جذاب دیگر در روند ماموریت

00:02:43.740 --> 00:02:46.636
استفاده از روباتهای انسانی در سطح مریخ است

00:02:46.660 --> 00:02:48.516
پیش از رسیدن انسانه،

00:02:48.540 --> 00:02:50.196
اول این که امکاناتی را برپا کنند

00:02:50.220 --> 00:02:53.580
و بعد بعنوان اعضای همکار
تیم علمی خدمت کنند.

00:02:55.220 --> 00:02:57.956
الان که از دید مهندسی به این نگاه می‌کنم

00:02:57.980 --> 00:03:01.156
برایم کاملا روش می‌شود
که آنچه بعنوان معمار لازم داشتم

00:03:01.180 --> 00:03:03.356
یک هوش مصنوعی زیرک،

00:03:03.380 --> 00:03:05.756
همکاری کننده و دارای هوش اجتماعی بود.

00:03:05.780 --> 00:03:10.076
بعبارتی چیزی به اچ‌ای‌ال بسازم

00:03:10.100 --> 00:03:12.516
اما بدون گرایشات انتحاری.

00:03:12.540 --> 00:03:13.900
(خنده)

00:03:14.740 --> 00:03:16.556
برای لحظه‌ای بیایید مکث کنیم.

00:03:16.580 --> 00:03:20.476
آیا امکان سک هوش مصنوعی مثل آن وجود دارد؟

00:03:20.500 --> 00:03:21.956
راستش هست.

00:03:21.980 --> 00:03:23.236
از بسیاری جهات

00:03:23.260 --> 00:03:25.236
این یک مشکل سخت مهندسی با اجزای

00:03:25.260 --> 00:03:26.716
ماشین ابر هوشمند است،

00:03:26.740 --> 00:03:31.436
نه یک جور گوی مودار خیس از
یک مساله ابرهوشمند که نیاز به مهندسی دارد.

00:03:31.460 --> 00:03:34.116
بگذارید از آلن تورینگ جور دیگر 
نقل قول کنم،

00:03:34.140 --> 00:03:36.516
من علاقه‌ای به ساخت یک
ماشین احساسی ندارم.

00:03:36.540 --> 00:03:38.116
من اچ‌ای‌ال نمیسازم.

00:03:38.140 --> 00:03:40.556
تمام انچه دنبالشم یک مغز ساده است،

00:03:40.580 --> 00:03:43.700
چیزی که شبحی از هوش را ارائه کند.

00:03:44.820 --> 00:03:47.956
هنر و علم محاسبات کامپیوتری 
از زمان ای اچ‌ال بر روی پرده

00:03:47.980 --> 00:03:49.476
روزهای زیادی را پشت سر گذاشته

00:03:49.500 --> 00:03:52.716
ومن تصور می کنم اگر مخترع آن
دکتر چاندرا امروز اینجا بود،

00:03:52.740 --> 00:03:55.076
کلی پرسش برای ما داشت.

00:03:55.100 --> 00:03:57.196
آیا واقعا برای ما امکان دارد

00:03:57.220 --> 00:04:01.236
یک سیستم چند میلیونی را بنابر
میلیونها دستگاه در اختیار بگیریم

00:04:01.260 --> 00:04:02.716
تا جریان داده‌های آنها را بخواند،

00:04:02.740 --> 00:04:04.996
تا ایراداتشان را پیش‌بینی کند 
و پیشاپیش عمل کند.

00:04:05.020 --> 00:04:06.236
بله.

00:04:06.260 --> 00:04:09.436
آیا می‌توان سیستمهایی ساخت که 
با انسانها به زبان عادی صحبت کنند؟

00:04:09.460 --> 00:04:10.676
بله.

00:04:10.700 --> 00:04:13.676
آیا می‌توان سیستمهایی ساخت که
اشیا و احساست را بشناسند،

00:04:13.700 --> 00:04:17.076
در احساسات خود اغراق کنند،
بازی کنند و حتی لب خوانی انجام دهند؟

00:04:17.100 --> 00:04:18.316
بله.

00:04:18.340 --> 00:04:20.476
آیا می‌توان سیستمهای ساخت که هدف
تعیین کنند،

00:04:20.500 --> 00:04:24.116
که طرحهایی را علیه آن اهداف انجام دهند
و در طول مسیر به یاد گرفتن بپردازند؟

00:04:24.140 --> 00:04:25.356
بله.

00:04:25.380 --> 00:04:28.716
آیا می‌توان سیستمهای ساخت
که دارای نظریه ذهنی باشند؟

00:04:28.740 --> 00:04:30.236
کاری که الان داریم
میاموزیم.

00:04:30.260 --> 00:04:33.740
آیا می‌توان سیستمهای ساخت
که پایه اخلاقی و وجدانی دارند؟

00:04:34.300 --> 00:04:36.340
چیزی که باید انجامش را بیاموزیم.

00:04:37.180 --> 00:04:38.556
خب بگذارید برای
لحظه‌ای بپذیریم

00:04:38.580 --> 00:04:41.476
که امکان ساخت چنین هوش مصنوعی

00:04:41.500 --> 00:04:43.636
برای چنین ماموریتهایی وجود دارد.

00:04:43.660 --> 00:04:46.196
این سوال بعدی که باید از خودتان بپرسید،

00:04:46.220 --> 00:04:47.676
آیا باید از آن ترسید؟

00:04:47.700 --> 00:04:49.676
خب با آمدن هر فناوری جدیدی

00:04:49.700 --> 00:04:52.596
دلهره و واهمه‌ای هم همراهش میاید.

00:04:52.620 --> 00:04:54.316
وقتی برای بار اول ماشینها را دیدیم،

00:04:54.340 --> 00:04:58.356
مردم تاسف می‌خورند که ما شاهد 
از دست رفتن خانواده خواهیم بود.

00:04:58.380 --> 00:05:01.076
وقتی برای اول تلفن آمد،

00:05:01.100 --> 00:05:03.996
مردم نگران بودند که نکند اثری از 
مکالمه مدنی نماند.

00:05:04.020 --> 00:05:07.956
در مرحله‌ای از زمان که ما شاهد بودیم
کلام نوشتاری فراگیر شد،

00:05:07.980 --> 00:05:10.476
مردم فکر می کردند توانایی خود
برای بخاطر سپردن را از دست می دهیم.

00:05:10.500 --> 00:05:12.556
این چیزها تا حدی درست هستند،

00:05:12.580 --> 00:05:14.996
اما همچنین این مورد هم هست
که این فناوریها

00:05:15.020 --> 00:05:18.396
برای چیزهایی را آورده‌اند که
که تجربیات بشری را عمیقا

00:05:18.420 --> 00:05:20.300
گسترش داده است.

00:05:21.660 --> 00:05:23.940
پس بیاید کمی این را جلوتر ببریم.

00:05:24.940 --> 00:05:29.676
من از آفرینش یک ابرهوش مصنوعی 
مثل این واهمه ندارم،

00:05:29.700 --> 00:05:33.516
چون سرانجام برخی از 
ارزشهای ما را دربر خواهد گرفت.

00:05:33.540 --> 00:05:37.036
این را تصور کنید: ساخت یک سیستم ادراکی
که اساسا متفاوت است

00:05:37.060 --> 00:05:40.356
از ساخت یک نظام فشرده- نرم‌افزاری
سنتی از گذشته.

00:05:40.380 --> 00:05:42.836
آنها را برنامه ریزی نمی‌کنیم. 
به آنه موزش می دهیم.

00:05:42.860 --> 00:05:45.516
برای آموزش به یک سیستم
که چطور گلها را تشخیص دهد،

00:05:45.540 --> 00:05:48.556
هزارن گل از انواعی که دوست دارم
را نشان می دهم.

00:05:48.580 --> 00:05:50.836
برای این که سیستم نحوه 
انجام بازیی را بیاموزد.

00:05:50.860 --> 00:05:52.820
خب این را می کنم. شما هم همینطور.

00:05:54.420 --> 00:05:56.460
گلها را دوست دارم. یالا.

00:05:57.260 --> 00:06:00.116
برای آموزش به سیستمی که بتواند
بازی مثل Go را انجام دهد،

00:06:00.140 --> 00:06:02.196
هزاران بار گذاشتم که 
بازیهای Go را انجام دهد

00:06:02.220 --> 00:06:03.876
اما در این روند 
آن را آموزش هم می‌دهم.

00:06:03.900 --> 00:06:06.316
چطور یک بازی خوب را از بد
تشخیص دهد.

00:06:06.340 --> 00:06:10.036
اکر بخواهم یک دستیار حقوقی
باهوش مصنوعی خلق کنم،

00:06:10.060 --> 00:06:11.836
کمی از مجموعه قانون را خواهم آموخت

00:06:11.860 --> 00:06:14.716
البته هم زمان با آن مفاهیم

00:06:14.740 --> 00:06:17.620
عدالت و بخشش را که بخشی از قانون 
هستند را خواهم آموخت.

00:06:18.380 --> 00:06:21.356
در عبارات علمی،
این را حقیقت مبنا مینامیم

00:06:21.380 --> 00:06:23.396
و نکته مهم این است:

00:06:23.420 --> 00:06:24.876
در تولید چنین ماشینهایی

00:06:24.900 --> 00:06:28.316
درواقع به آنها مفهوم
ارزشهایمان را میاموزیم.

00:06:28.340 --> 00:06:31.476
به همین ترتیب، به هوش مصنوعی
همان اندازه‌ای باور دارم

00:06:31.500 --> 00:06:35.140
که به یک انسانی که بخوبی
آموزش دیده.

00:06:35.900 --> 00:06:37.116
اما ممکن است سوال کنید،

00:06:37.140 --> 00:06:39.756
ماموران زیرک چطور،

00:06:39.780 --> 00:06:43.116
یک سازمانه غیردولتی پولدار؟

00:06:43.140 --> 00:06:46.956
من ازاین که هوش مصنوعی در اختیار
یک گرگ تنها باشد نمی‌ترسم.

00:06:46.980 --> 00:06:51.516
معلوم است که ما نمی‌توانیم از خودمان
در مقابل تمامی اعمال خشونت‌امیز تصادفی حمایت کنیم

00:06:51.540 --> 00:06:53.676
اما حقیقت این است که چنین سیستمی

00:06:53.700 --> 00:06:56.796
نیازمند آموزش مفصل و موشکفانه‌ای دارد

00:06:56.820 --> 00:06:59.116
که فرای منابع فردی است.

00:06:59.140 --> 00:07:00.356
و علاوه براین

00:07:00.380 --> 00:07:03.636
بسیار فراتر از ویروس اینترنتی 
وارد کردن به جهان است

00:07:03.660 --> 00:07:06.756
جاییکه با یک کلیک، به یکباره ویروس
در میلیونها جا پراکنده شده

00:07:06.780 --> 00:07:09.236
و لپ تاپ ها در همه جا شروع 
به ترکیدن می کنند.

00:07:09.260 --> 00:07:12.076
خب مفاهیمی این چنینی بسیار بزرگترند

00:07:12.100 --> 00:07:13.815
و ما بی شک شاهد این اتفاقات
خواهیم بود.

00:07:14.340 --> 00:07:17.396
آیا از یک چنین هوش مصنوعی می‌ترسم

00:07:17.420 --> 00:07:19.380
که ممکن است تهدیدی برای بشریت باشد؟

00:07:20.100 --> 00:07:24.476
اگر به فیلمهایی مثل ماتریکس، متروپلیس،
ترمیناتور یا

00:07:24.500 --> 00:07:27.676
سریالی مثل وست ورلد نگاه کنید،

00:07:27.700 --> 00:07:29.836
تمامی از از چنین ترسی حرف می‌زنند.

00:07:29.860 --> 00:07:34.156
درواقع، در کتاب «ابرهوش صنوعی»
نوشته نیکو بوستروم فیلسوف،

00:07:34.180 --> 00:07:35.716
او با این موضوع پیش می‌رود

00:07:35.740 --> 00:07:39.756
و مشاهده‌اش این است که یک ابرهوش مصنوعی 
نه تنها شاید خطرناک بشد

00:07:39.780 --> 00:07:43.636
بلکه نماینده تهدیدی نابودگر 
برای کل بشریت باشد.

00:07:43.660 --> 00:07:45.876
استدلال اولیه دکتر بوستروم

00:07:45.900 --> 00:07:48.636
این است که چنین سیستم‌هایی عاقبت

00:07:48.660 --> 00:07:51.916
چنان عطش سیری‌ناپذیری 
برای اطلاعات خواهند داشت

00:07:51.940 --> 00:07:54.836
که شاید نحوه یادگرفتن را یاد بگشیرند

00:07:54.860 --> 00:07:57.476
و سرانجام کشف کنند که اهدافی دارند

00:07:57.500 --> 00:07:59.796
که مغایر با نیازهای بشر است.

00:07:59.820 --> 00:08:01.676
دکتر بوستروم کلی دنبال کننده دارد.

00:08:01.700 --> 00:08:06.020
او از سوی کسانی مثل الون موسک 
و استفن هاوکینگ حمایت می‌شود.

00:08:06.700 --> 00:08:09.100
با تمامی احترامی که برای این

00:08:09.980 --> 00:08:11.996
ذهنهای بی‌نظیر قائل هستم،

00:08:12.020 --> 00:08:14.276
اعتقاد دارم که آنها 
از پایه اشتباه می کنند.

00:08:14.300 --> 00:08:17.476
اکنون قسمتهای زیادیاز استدلال دکتر بوستروم است
که باید از چمدان خارج شود

00:08:17.500 --> 00:08:19.636
و من زمان کافی را برای همه آنها ندارم،

00:08:19.660 --> 00:08:22.356
فقط خلاصه این را بگویم:

00:08:22.380 --> 00:08:26.116
مدعی العلوم با همه کاره بودن
خیلی فرق دارد.

00:08:26.140 --> 00:08:28.036
ای‌اچ ال تهدیدی برای خدمه دیسکاوری بود

00:08:28.060 --> 00:08:32.476
تنها جاییکه ای‌اچ ال همه جوانب
را در دیسکاوری فرماندهی می‌کرد.

00:08:32.500 --> 00:08:34.996
بریا این کار باید ابرهوشمند بود.

00:08:35.020 --> 00:08:37.516
باید بر همه دنیای ما سلطه داشته باشد.

00:08:37.540 --> 00:08:40.356
اینها از ویژگیهای اسکای‌نت در فیلم
ترمیناتور(نابودگر) است

00:08:40.380 --> 00:08:42.236
که درآن یک ابرهوش مصنوعی داشتیم

00:08:42.260 --> 00:08:43.636
که بر اراده بشر حک می‌راند،

00:08:43.660 --> 00:08:47.516
همه چیز و همه جا را در زمین هدایت می‌کرد.

00:08:47.540 --> 00:08:48.996
اما در عالم واقعیت

00:08:49.020 --> 00:08:51.116
این اتفاق نخواهد افتاد.

00:08:51.140 --> 00:08:54.196
ما ماشینهای ابرهوشمندی را نمی‌سازیم
که هوا را کنترل کنند،

00:08:54.220 --> 00:08:55.556
که جزر و مد را جهت‌ دهند،

00:08:55.580 --> 00:08:58.956
که به ما انسانهای دمدمی‌مزاج 
و بی‌نظم فرمان دهند.

00:08:58.980 --> 00:09:02.876
و علاوه بر آن، اگر چنین هوش مصنوعی 
وجود داشته باشد،

00:09:02.900 --> 00:09:05.836
باید با اقتصادهای بشری رقابت کند،

00:09:05.860 --> 00:09:08.380
و به این وسیله برای منابع با ما رقابت کند.

00:09:09.020 --> 00:09:10.236
و در آخر--

00:09:10.260 --> 00:09:11.500
به «سیری» این را نگویید--

00:09:12.260 --> 00:09:13.636
همیشه می توانیم از
برق آنها را بکشیم.

00:09:13.660 --> 00:09:15.780
(خنده)

00:09:17.180 --> 00:09:19.636
ما در یک سفر باورنکردنی از

00:09:19.660 --> 00:09:22.156
هم‌فرگشتی با ماشینهای خود 
بسر می‌بریم.

00:09:22.180 --> 00:09:24.676
این آدمهایی که امروز هستیم

00:09:24.700 --> 00:09:27.236
آن آدمهایی که بعدا خواهیم شد نیستم.

00:09:27.260 --> 00:09:30.396
الان نگران بودن درباره خیزش یک ابرهوشمند

00:09:30.420 --> 00:09:33.476
از بسیاری جهات انحرافی خطرناک است

00:09:33.500 --> 00:09:35.836
چون خیزش محاسبات کامپیوتری خودش

00:09:35.860 --> 00:09:38.876
کلی مسائل اجتماعی و انسانی را برایمان
به ارمغان می‌اورد

00:09:38.900 --> 00:09:40.540
که باید به آنها رسیدگی کنیم.

00:09:41.180 --> 00:09:43.996
چطور به بهترین نحو جامعه‌ای
را برنامه ریزی کنم

00:09:44.020 --> 00:09:46.356
وقتی نیاز به نیروی کار بشری نمی‌ماند؟

00:09:46.380 --> 00:09:50.196
چطور می‌توانم تفاهم و تعلیم
را به جهان بیاورم

00:09:50.220 --> 00:09:51.996
و هنوز احترام به تفاوتها
هم باشد.

00:09:52.020 --> 00:09:56.276
چطور

00:09:56.300 --> 00:09:59.156
چطور شاید از محاسبات کامپیوتری
در کمک به

00:09:59.180 --> 00:10:00.940
رسیدن به 
ستاره‌ها استفاده کنم؟

00:10:01.580 --> 00:10:03.620
و همین هیجان‌انگیزه است.

00:10:04.220 --> 00:10:06.556
فرصتهایی برای استقاده از
محاسبات کامپیوتری

00:10:06.580 --> 00:10:08.116
برای پیشرفت تجربه بشر

00:10:08.140 --> 00:10:09.556
اینجا و الان

00:10:09.580 --> 00:10:11.436
در دسترس ما است.

00:10:11.460 --> 00:10:13.140
و تازه اول راه هستیم.

00:10:14.100 --> 00:10:15.316
خیلی متشکرم.

00:10:15.340 --> 00:10:19.626
(تشویق)

