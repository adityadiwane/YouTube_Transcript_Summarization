WEBVTT
Kind: captions
Language: en

00:00:12.580 --> 00:00:16.420
When I was a kid,
I was the quintessential nerd.

00:00:17.140 --> 00:00:19.316
I think some of you were, too.

00:00:19.340 --> 00:00:20.556
(Laughter)

00:00:20.580 --> 00:00:23.796
And you, sir, who laughed the loudest,
you probably still are.

00:00:23.820 --> 00:00:26.076
(Laughter)

00:00:26.100 --> 00:00:29.596
I grew up in a small town
in the dusty plains of north Texas,

00:00:29.620 --> 00:00:32.956
the son of a sheriff
who was the son of a pastor.

00:00:32.980 --> 00:00:34.900
Getting into trouble was not an option.

00:00:35.860 --> 00:00:39.116
And so I started reading
calculus books for fun.

00:00:39.140 --> 00:00:40.676
(Laughter)

00:00:40.700 --> 00:00:42.396
You did, too.

00:00:42.420 --> 00:00:46.156
That led me to building a laser
and a computer and model rockets,

00:00:46.180 --> 00:00:49.180
and that led me to making
rocket fuel in my bedroom.

00:00:49.780 --> 00:00:53.436
Now, in scientific terms,

00:00:53.460 --> 00:00:56.716
we call this a very bad idea.

00:00:56.740 --> 00:00:57.956
(Laughter)

00:00:57.980 --> 00:01:00.156
Around that same time,

00:01:00.180 --> 00:01:03.396
Stanley Kubrick's "2001: A Space Odyssey"
came to the theaters,

00:01:03.420 --> 00:01:05.620
and my life was forever changed.

00:01:06.100 --> 00:01:08.156
I loved everything about that movie,

00:01:08.180 --> 00:01:10.716
especially the HAL 9000.

00:01:10.740 --> 00:01:12.796
Now, HAL was a sentient computer

00:01:12.820 --> 00:01:15.276
designed to guide the Discovery spacecraft

00:01:15.300 --> 00:01:17.836
from the Earth to Jupiter.

00:01:17.860 --> 00:01:19.916
HAL was also a flawed character,

00:01:19.940 --> 00:01:24.220
for in the end he chose
to value the mission over human life.

00:01:24.660 --> 00:01:26.756
Now, HAL was a fictional character,

00:01:26.780 --> 00:01:29.436
but nonetheless he speaks to our fears,

00:01:29.460 --> 00:01:31.556
our fears of being subjugated

00:01:31.580 --> 00:01:34.596
by some unfeeling, artificial intelligence

00:01:34.620 --> 00:01:36.580
who is indifferent to our humanity.

00:01:37.700 --> 00:01:40.276
I believe that such fears are unfounded.

00:01:40.300 --> 00:01:42.996
Indeed, we stand at a remarkable time

00:01:43.020 --> 00:01:44.556
in human history,

00:01:44.580 --> 00:01:49.556
where, driven by refusal to accept
the limits of our bodies and our minds,

00:01:49.580 --> 00:01:51.276
we are building machines

00:01:51.300 --> 00:01:54.916
of exquisite, beautiful
complexity and grace

00:01:54.940 --> 00:01:56.996
that will extend the human experience

00:01:57.020 --> 00:01:58.700
in ways beyond our imagining.

00:01:59.540 --> 00:02:02.116
After a career that led me
from the Air Force Academy

00:02:02.140 --> 00:02:04.076
to Space Command to now,

00:02:04.100 --> 00:02:05.796
I became a systems engineer,

00:02:05.820 --> 00:02:08.556
and recently I was drawn
into an engineering problem

00:02:08.580 --> 00:02:11.156
associated with NASA's mission to Mars.

00:02:11.180 --> 00:02:13.676
Now, in space flights to the Moon,

00:02:13.700 --> 00:02:16.836
we can rely upon
mission control in Houston

00:02:16.860 --> 00:02:18.836
to watch over all aspects of a flight.

00:02:18.860 --> 00:02:22.396
However, Mars is 200 times further away,

00:02:22.420 --> 00:02:25.636
and as a result it takes
on average 13 minutes

00:02:25.660 --> 00:02:28.796
for a signal to travel
from the Earth to Mars.

00:02:28.820 --> 00:02:32.220
If there's trouble,
there's not enough time.

00:02:32.660 --> 00:02:35.156
And so a reasonable engineering solution

00:02:35.180 --> 00:02:37.756
calls for us to put mission control

00:02:37.780 --> 00:02:40.796
inside the walls of the Orion spacecraft.

00:02:40.820 --> 00:02:43.716
Another fascinating idea
in the mission profile

00:02:43.740 --> 00:02:46.636
places humanoid robots
on the surface of Mars

00:02:46.660 --> 00:02:48.516
before the humans themselves arrive,

00:02:48.540 --> 00:02:50.196
first to build facilities

00:02:50.220 --> 00:02:53.580
and later to serve as collaborative
members of the science team.

00:02:55.220 --> 00:02:57.956
Now, as I looked at this
from an engineering perspective,

00:02:57.980 --> 00:03:01.156
it became very clear to me
that what I needed to architect

00:03:01.180 --> 00:03:03.356
was a smart, collaborative,

00:03:03.380 --> 00:03:05.756
socially intelligent
artificial intelligence.

00:03:05.780 --> 00:03:10.076
In other words, I needed to build
something very much like a HAL

00:03:10.100 --> 00:03:12.516
but without the homicidal tendencies.

00:03:12.540 --> 00:03:13.900
(Laughter)

00:03:14.740 --> 00:03:16.556
Let's pause for a moment.

00:03:16.580 --> 00:03:20.476
Is it really possible to build
an artificial intelligence like that?

00:03:20.500 --> 00:03:21.956
Actually, it is.

00:03:21.980 --> 00:03:23.236
In many ways,

00:03:23.260 --> 00:03:25.236
this is a hard engineering problem

00:03:25.260 --> 00:03:26.716
with elements of AI,

00:03:26.740 --> 00:03:31.436
not some wet hair ball of an AI problem
that needs to be engineered.

00:03:31.460 --> 00:03:34.116
To paraphrase Alan Turing,

00:03:34.140 --> 00:03:36.516
I'm not interested
in building a sentient machine.

00:03:36.540 --> 00:03:38.116
I'm not building a HAL.

00:03:38.140 --> 00:03:40.556
All I'm after is a simple brain,

00:03:40.580 --> 00:03:43.700
something that offers
the illusion of intelligence.

00:03:44.820 --> 00:03:47.956
The art and the science of computing
have come a long way

00:03:47.980 --> 00:03:49.476
since HAL was onscreen,

00:03:49.500 --> 00:03:52.716
and I'd imagine if his inventor
Dr. Chandra were here today,

00:03:52.740 --> 00:03:55.076
he'd have a whole lot of questions for us.

00:03:55.100 --> 00:03:57.196
Is it really possible for us

00:03:57.220 --> 00:04:01.236
to take a system of millions
upon millions of devices,

00:04:01.260 --> 00:04:02.716
to read in their data streams,

00:04:02.740 --> 00:04:04.996
to predict their failures
and act in advance?

00:04:05.020 --> 00:04:06.236
Yes.

00:04:06.260 --> 00:04:09.436
Can we build systems that converse
with humans in natural language?

00:04:09.460 --> 00:04:10.676
Yes.

00:04:10.700 --> 00:04:13.676
Can we build systems
that recognize objects, identify emotions,

00:04:13.700 --> 00:04:17.076
emote themselves,
play games and even read lips?

00:04:17.100 --> 00:04:18.316
Yes.

00:04:18.340 --> 00:04:20.476
Can we build a system that sets goals,

00:04:20.500 --> 00:04:24.116
that carries out plans against those goals
and learns along the way?

00:04:24.140 --> 00:04:25.356
Yes.

00:04:25.380 --> 00:04:28.716
Can we build systems
that have a theory of mind?

00:04:28.740 --> 00:04:30.236
This we are learning to do.

00:04:30.260 --> 00:04:33.740
Can we build systems that have
an ethical and moral foundation?

00:04:34.300 --> 00:04:36.340
This we must learn how to do.

00:04:37.180 --> 00:04:38.556
So let's accept for a moment

00:04:38.580 --> 00:04:41.476
that it's possible to build
such an artificial intelligence

00:04:41.500 --> 00:04:43.636
for this kind of mission and others.

00:04:43.660 --> 00:04:46.196
The next question
you must ask yourself is,

00:04:46.220 --> 00:04:47.676
should we fear it?

00:04:47.700 --> 00:04:49.676
Now, every new technology

00:04:49.700 --> 00:04:52.596
brings with it
some measure of trepidation.

00:04:52.620 --> 00:04:54.316
When we first saw cars,

00:04:54.340 --> 00:04:58.356
people lamented that we would see
the destruction of the family.

00:04:58.380 --> 00:05:01.076
When we first saw telephones come in,

00:05:01.100 --> 00:05:03.996
people were worried it would destroy
all civil conversation.

00:05:04.020 --> 00:05:07.956
At a point in time we saw
the written word become pervasive,

00:05:07.980 --> 00:05:10.476
people thought we would lose
our ability to memorize.

00:05:10.500 --> 00:05:12.556
These things are all true to a degree,

00:05:12.580 --> 00:05:14.996
but it's also the case
that these technologies

00:05:15.020 --> 00:05:18.396
brought to us things
that extended the human experience

00:05:18.420 --> 00:05:20.300
in some profound ways.

00:05:21.660 --> 00:05:23.940
So let's take this a little further.

00:05:24.940 --> 00:05:29.676
I do not fear the creation
of an AI like this,

00:05:29.700 --> 00:05:33.516
because it will eventually
embody some of our values.

00:05:33.540 --> 00:05:37.036
Consider this: building a cognitive system
is fundamentally different

00:05:37.060 --> 00:05:40.356
than building a traditional
software-intensive system of the past.

00:05:40.380 --> 00:05:42.836
We don't program them. We teach them.

00:05:42.860 --> 00:05:45.516
In order to teach a system
how to recognize flowers,

00:05:45.540 --> 00:05:48.556
I show it thousands of flowers
of the kinds I like.

00:05:48.580 --> 00:05:50.836
In order to teach a system
how to play a game --

00:05:50.860 --> 00:05:52.820
Well, I would. You would, too.

00:05:54.420 --> 00:05:56.460
I like flowers. Come on.

00:05:57.260 --> 00:06:00.116
To teach a system
how to play a game like Go,

00:06:00.140 --> 00:06:02.196
I'd have it play thousands of games of Go,

00:06:02.220 --> 00:06:03.876
but in the process I also teach it

00:06:03.900 --> 00:06:06.316
how to discern
a good game from a bad game.

00:06:06.340 --> 00:06:10.036
If I want to create an artificially
intelligent legal assistant,

00:06:10.060 --> 00:06:11.836
I will teach it some corpus of law

00:06:11.860 --> 00:06:14.716
but at the same time I am fusing with it

00:06:14.740 --> 00:06:17.620
the sense of mercy and justice
that is part of that law.

00:06:18.380 --> 00:06:21.356
In scientific terms,
this is what we call ground truth,

00:06:21.380 --> 00:06:23.396
and here's the important point:

00:06:23.420 --> 00:06:24.876
in producing these machines,

00:06:24.900 --> 00:06:28.316
we are therefore teaching them
a sense of our values.

00:06:28.340 --> 00:06:31.476
To that end, I trust
an artificial intelligence

00:06:31.500 --> 00:06:35.140
the same, if not more,
as a human who is well-trained.

00:06:35.900 --> 00:06:37.116
But, you may ask,

00:06:37.140 --> 00:06:39.756
what about rogue agents,

00:06:39.780 --> 00:06:43.116
some well-funded
nongovernment organization?

00:06:43.140 --> 00:06:46.956
I do not fear an artificial intelligence
in the hand of a lone wolf.

00:06:46.980 --> 00:06:51.516
Clearly, we cannot protect ourselves
against all random acts of violence,

00:06:51.540 --> 00:06:53.676
but the reality is such a system

00:06:53.700 --> 00:06:56.796
requires substantial training
and subtle training

00:06:56.820 --> 00:06:59.116
far beyond the resources of an individual.

00:06:59.140 --> 00:07:00.356
And furthermore,

00:07:00.380 --> 00:07:03.636
it's far more than just injecting
an internet virus to the world,

00:07:03.660 --> 00:07:06.756
where you push a button,
all of a sudden it's in a million places

00:07:06.780 --> 00:07:09.236
and laptops start blowing up
all over the place.

00:07:09.260 --> 00:07:12.076
Now, these kinds of substances
are much larger,

00:07:12.100 --> 00:07:13.815
and we'll certainly see them coming.

00:07:14.340 --> 00:07:17.396
Do I fear that such
an artificial intelligence

00:07:17.420 --> 00:07:19.380
might threaten all of humanity?

00:07:20.100 --> 00:07:24.476
If you look at movies
such as "The Matrix," "Metropolis,"

00:07:24.500 --> 00:07:27.676
"The Terminator,"
shows such as "Westworld,"

00:07:27.700 --> 00:07:29.836
they all speak of this kind of fear.

00:07:29.860 --> 00:07:34.156
Indeed, in the book "Superintelligence"
by the philosopher Nick Bostrom,

00:07:34.180 --> 00:07:35.716
he picks up on this theme

00:07:35.740 --> 00:07:39.756
and observes that a superintelligence
might not only be dangerous,

00:07:39.780 --> 00:07:43.636
it could represent an existential threat
to all of humanity.

00:07:43.660 --> 00:07:45.876
Dr. Bostrom's basic argument

00:07:45.900 --> 00:07:48.636
is that such systems will eventually

00:07:48.660 --> 00:07:51.916
have such an insatiable
thirst for information

00:07:51.940 --> 00:07:54.836
that they will perhaps learn how to learn

00:07:54.860 --> 00:07:57.476
and eventually discover
that they may have goals

00:07:57.500 --> 00:07:59.796
that are contrary to human needs.

00:07:59.820 --> 00:08:01.676
Dr. Bostrom has a number of followers.

00:08:01.700 --> 00:08:06.020
He is supported by people
such as Elon Musk and Stephen Hawking.

00:08:06.700 --> 00:08:09.100
With all due respect

00:08:09.980 --> 00:08:11.996
to these brilliant minds,

00:08:12.020 --> 00:08:14.276
I believe that they
are fundamentally wrong.

00:08:14.300 --> 00:08:17.476
Now, there are a lot of pieces
of Dr. Bostrom's argument to unpack,

00:08:17.500 --> 00:08:19.636
and I don't have time to unpack them all,

00:08:19.660 --> 00:08:22.356
but very briefly, consider this:

00:08:22.380 --> 00:08:26.116
super knowing is very different
than super doing.

00:08:26.140 --> 00:08:28.036
HAL was a threat to the Discovery crew

00:08:28.060 --> 00:08:32.476
only insofar as HAL commanded
all aspects of the Discovery.

00:08:32.500 --> 00:08:34.996
So it would have to be
with a superintelligence.

00:08:35.020 --> 00:08:37.516
It would have to have dominion
over all of our world.

00:08:37.540 --> 00:08:40.356
This is the stuff of Skynet
from the movie "The Terminator"

00:08:40.380 --> 00:08:42.236
in which we had a superintelligence

00:08:42.260 --> 00:08:43.636
that commanded human will,

00:08:43.660 --> 00:08:47.516
that directed every device
that was in every corner of the world.

00:08:47.540 --> 00:08:48.996
Practically speaking,

00:08:49.020 --> 00:08:51.116
it ain't gonna happen.

00:08:51.140 --> 00:08:54.196
We are not building AIs
that control the weather,

00:08:54.220 --> 00:08:55.556
that direct the tides,

00:08:55.580 --> 00:08:58.956
that command us
capricious, chaotic humans.

00:08:58.980 --> 00:09:02.876
And furthermore, if such
an artificial intelligence existed,

00:09:02.900 --> 00:09:05.836
it would have to compete
with human economies,

00:09:05.860 --> 00:09:08.380
and thereby compete for resources with us.

00:09:09.020 --> 00:09:10.236
And in the end --

00:09:10.260 --> 00:09:11.500
don't tell Siri this --

00:09:12.260 --> 00:09:13.636
we can always unplug them.

00:09:13.660 --> 00:09:15.780
(Laughter)

00:09:17.180 --> 00:09:19.636
We are on an incredible journey

00:09:19.660 --> 00:09:22.156
of coevolution with our machines.

00:09:22.180 --> 00:09:24.676
The humans we are today

00:09:24.700 --> 00:09:27.236
are not the humans we will be then.

00:09:27.260 --> 00:09:30.396
To worry now about the rise
of a superintelligence

00:09:30.420 --> 00:09:33.476
is in many ways a dangerous distraction

00:09:33.500 --> 00:09:35.836
because the rise of computing itself

00:09:35.860 --> 00:09:38.876
brings to us a number
of human and societal issues

00:09:38.900 --> 00:09:40.540
to which we must now attend.

00:09:41.180 --> 00:09:43.996
How shall I best organize society

00:09:44.020 --> 00:09:46.356
when the need for human labor diminishes?

00:09:46.380 --> 00:09:50.196
How can I bring understanding
and education throughout the globe

00:09:50.220 --> 00:09:51.996
and still respect our differences?

00:09:52.020 --> 00:09:56.276
How might I extend and enhance human life
through cognitive healthcare?

00:09:56.300 --> 00:09:59.156
How might I use computing

00:09:59.180 --> 00:10:00.940
to help take us to the stars?

00:10:01.580 --> 00:10:03.620
And that's the exciting thing.

00:10:04.220 --> 00:10:06.556
The opportunities to use computing

00:10:06.580 --> 00:10:08.116
to advance the human experience

00:10:08.140 --> 00:10:09.556
are within our reach,

00:10:09.580 --> 00:10:11.436
here and now,

00:10:11.460 --> 00:10:13.140
and we are just beginning.

00:10:14.100 --> 00:10:15.316
Thank you very much.

00:10:15.340 --> 00:10:19.626
(Applause)

