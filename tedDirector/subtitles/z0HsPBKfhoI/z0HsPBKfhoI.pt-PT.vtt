WEBVTT
Kind: captions
Language: pt-PT

00:00:00.000 --> 00:00:07.000
Tradutor: Yali Gao
Revisora: Margarida Ferreira

00:00:12.580 --> 00:00:16.267
Quando eu era miúdo,
era um grandessíssimo "nerd".

00:00:17.140 --> 00:00:19.430
Acho que alguns de vocês também eram.

00:00:19.463 --> 00:00:20.641
(Risos)

00:00:20.713 --> 00:00:24.062
E o senhor, que riu mais alto, 
provavelmente ainda é.

00:00:24.124 --> 00:00:25.971
(Risos)

00:00:26.100 --> 00:00:27.700
Cresci numa cidade pequena

00:00:27.700 --> 00:00:29.858
nas planícies empoeiradas
do norte do Texas,

00:00:29.905 --> 00:00:33.117
filho de um xerife
que era filho de um pastor.

00:00:33.180 --> 00:00:35.433
Meter-me em sarilhos estava fora de causa.

00:00:35.860 --> 00:00:39.287
Por isso, comecei a ler 
livros de cálculo por diversão.

00:00:39.358 --> 00:00:40.676
(Risos)

00:00:40.785 --> 00:00:42.681
Vocês também fizeram isso.

00:00:42.743 --> 00:00:46.270
Isso levou-me a construir um laser,
um computador e modelos de foguetões

00:00:46.313 --> 00:00:49.818
e levou-me a fazer combustível
para foguetões, no meu quarto.

00:00:51.894 --> 00:00:53.636
Em termos científicos,

00:00:53.688 --> 00:00:56.773
chamamos a isso uma ideia muito má.

00:00:56.825 --> 00:00:58.060
(Risos)

00:00:58.180 --> 00:01:00.089
Por volta dessa época,

00:01:00.389 --> 00:01:03.691
"2001: A Odisseia no Espaço", de 
Stanley Kubrick, chegou ao cinema

00:01:03.734 --> 00:01:06.124
e a minha vida mudou para sempre.

00:01:06.277 --> 00:01:08.365
Adorei tudo naquele filme,

00:01:08.399 --> 00:01:10.906
especialmente o HAL 9000,

00:01:10.997 --> 00:01:13.138
O HAL era um computador sensível

00:01:13.172 --> 00:01:15.466
concebido para guiar a astronave Discovery

00:01:15.509 --> 00:01:17.836
da Terra para Júpiter.

00:01:18.079 --> 00:01:20.192
O HAL também era
um personagem com defeitos,

00:01:20.254 --> 00:01:24.496
porque, no final, preferiu valorizar 
a missão em vez da vida humana.

00:01:24.840 --> 00:01:26.898
O HAL era uma personagem fictícia,

00:01:26.932 --> 00:01:29.455
mas, mesmo assim,
fala dos nossos medos,

00:01:29.517 --> 00:01:31.794
dos nossos medos de sermos subjugados

00:01:31.827 --> 00:01:34.843
por alguma inteligência 
insensível, artificial

00:01:34.896 --> 00:01:37.056
que é indiferente à nossa humanidade.

00:01:37.700 --> 00:01:40.333
Acredito que tais medos são infundados.

00:01:40.395 --> 00:01:43.005
Na verdade, estamos numa época notável

00:01:43.048 --> 00:01:44.794
da história humana,

00:01:44.818 --> 00:01:49.556
em que, impelidos pela recusa de aceitar
os limites do nosso corpo e da nossa mente,

00:01:49.580 --> 00:01:51.352
estamos a construir máquinas

00:01:51.404 --> 00:01:54.916
de complexidade e graça 
requintada e bonita

00:01:54.940 --> 00:01:57.148
que alargará a experiência humana

00:01:57.200 --> 00:01:59.290
de formas para além da nossa imaginação.

00:01:59.492 --> 00:02:02.430
Depois duma carreira que me levou 
da Academia da Força Aérea

00:02:02.463 --> 00:02:04.314
até ao Comando Espacial, agora,

00:02:04.357 --> 00:02:06.110
tornei-me engenheiro de sistemas,

00:02:06.157 --> 00:02:08.775
e recentemente, envolvi-me
num problema de engenharia

00:02:08.818 --> 00:02:11.194
associado à missão da NASA em Marte.

00:02:11.322 --> 00:02:13.790
Em voos espaciais para a Lua,

00:02:13.852 --> 00:02:16.836
podemos confiar no controlo 
da missão em Houston

00:02:16.898 --> 00:02:19.045
para observar todos os aspetos dum voo.

00:02:19.136 --> 00:02:22.472
No entanto, Marte está
200 vezes mais longe

00:02:22.515 --> 00:02:25.912
e, em resultado, são precisos
13 minutos, em média,

00:02:25.955 --> 00:02:28.919
para um sinal viajar 
da Terra até Marte.

00:02:28.991 --> 00:02:32.372
Se houver problemas, 
não há tempo suficiente.

00:02:32.660 --> 00:02:35.241
Assim, uma solução razoável de engenharia

00:02:35.284 --> 00:02:37.860
pede-nos para pôr o controlo da missão

00:02:37.894 --> 00:02:40.548
dentro das paredes
da astronave Orion.

00:02:40.820 --> 00:02:43.830
Uma outra ideia fascinante,
no perfil da missão,

00:02:43.863 --> 00:02:46.797
coloca robôs humanoides
na superfície de Marte

00:02:46.869 --> 00:02:48.782
antes de os seres humanos chegarem,

00:02:48.825 --> 00:02:50.567
primeiro para construir instalações

00:02:50.615 --> 00:02:54.103
e depois, para servir como membros 
colaborativos da equipa de ciência

00:02:55.220 --> 00:02:58.051
Como olhei para isto 
numa perspetiva de engenharia

00:02:58.094 --> 00:03:01.403
percebi rapidamente que aquilo
de que precisava para arquitetar

00:03:01.456 --> 00:03:03.356
era de uma inteligência artificial,

00:03:03.380 --> 00:03:05.898
esperta, colaborativa
e socialmente inteligente.

00:03:05.980 --> 00:03:10.009
Por outras palavras, tinha de construir
algo muito parecido com um HAL

00:03:10.100 --> 00:03:12.516
mas sem tendências homicidas.

00:03:12.682 --> 00:03:14.042
(Risos)

00:03:14.740 --> 00:03:16.679
Vamos fazer uma pausa por um momento

00:03:16.741 --> 00:03:20.476
Será realmente possível construir
uma inteligência artificial assim?

00:03:20.623 --> 00:03:21.956
Na verdade, é.

00:03:22.046 --> 00:03:23.236
De muitas formas,

00:03:23.260 --> 00:03:25.236
isso é um árduo problema de engenharia

00:03:25.260 --> 00:03:27.277
com elementos de inteligência artificial,

00:03:27.292 --> 00:03:31.331
não um problema complicado de IA 
que precisa de ser projetado.

00:03:31.460 --> 00:03:33.935
Parafraseando Alan Turing,

00:03:33.958 --> 00:03:36.668
não estou interessado
em construir uma máquina sensível.

00:03:36.692 --> 00:03:38.325
Não estou a construir um HAL.

00:03:38.378 --> 00:03:40.756
Só estou a procurar um cérebro simples,

00:03:40.789 --> 00:03:43.995
algo que ofereça a ilusão de inteligência.

00:03:44.820 --> 00:03:48.098
A arte e a ciência da informática
têm percorrido um longo caminho

00:03:48.151 --> 00:03:49.809
desde que o HAL esteve no ecrã

00:03:49.833 --> 00:03:52.935
e imagino que, se o seu inventor
Dr. Chandra estivesse aqui hoje,

00:03:52.968 --> 00:03:55.209
teria um monte de questões para nós.

00:03:55.280 --> 00:03:57.196
Será realmente possível

00:03:57.220 --> 00:04:01.236
termos um sistema de milhões
e milhões de dispositivos,

00:04:01.260 --> 00:04:02.830
para ler nos fluxos de dados,

00:04:02.882 --> 00:04:05.138
prever as suas falhas
e agir com antecedência?

00:04:05.172 --> 00:04:06.236
É, sim.

00:04:06.269 --> 00:04:09.645
Podemos construir sistemas que conversam
com humanos em língua natural?

00:04:09.679 --> 00:04:10.676
Podemos, sim.

00:04:10.700 --> 00:04:14.142
Podemos construir sistemas que
reconheçam objetos, identifiquem emoções,

00:04:14.166 --> 00:04:17.037
se emocionem,
joguem jogos e até leiam lábios?

00:04:17.100 --> 00:04:18.316
Podemos, sim.

00:04:18.340 --> 00:04:20.713
Podemos construir um sistema
que estabeleça metas,

00:04:20.728 --> 00:04:24.258
que execute planos para essas metas
e aprenda ao longo do caminho?

00:04:24.311 --> 00:04:25.489
Podemos, sim.

00:04:25.532 --> 00:04:28.792
Podemos construir sistemas
que tenham uma teoria de pensamento?

00:04:28.854 --> 00:04:30.426
Estamos a aprender a fazer isso.

00:04:30.479 --> 00:04:34.044
Podemos construir sistemas que têm
um fundamento ético e moral?

00:04:34.300 --> 00:04:36.673
Temos de aprender a fazer isso.

00:04:37.180 --> 00:04:38.756
Vamos aceitar, por instantes,

00:04:38.789 --> 00:04:41.533
que é possível construir 
essa inteligência artificial

00:04:41.585 --> 00:04:43.778
para este tipo de missão e outros.

00:04:43.812 --> 00:04:46.196
A pergunta seguinte
que devemos fazer é:

00:04:46.286 --> 00:04:47.990
Devemos ter medo dela?

00:04:48.052 --> 00:04:49.733
Cada nova tecnologia

00:04:49.785 --> 00:04:52.415
traz consigo uma certa trepidação.

00:04:52.620 --> 00:04:54.611
Quando viram carros pela primeira vez,

00:04:54.673 --> 00:04:58.356
as pessoas queixaram-se
de que veríamos a destruição da família.

00:04:58.541 --> 00:05:01.218
Quando viram os telefones,
pela primeira vez,

00:05:01.252 --> 00:05:04.357
as pessoas recearam que fossem
destruir toda a conversação civil.

00:05:04.391 --> 00:05:08.041
A certa altura, vimos a palavra escrita
tornar-se universal.

00:05:08.180 --> 00:05:10.799
As pessoas pensaram perder
a capacidade de memorizar.

00:05:10.833 --> 00:05:12.917
Tudo isto é verdade, até certo ponto,

00:05:12.951 --> 00:05:15.224
mas acontece também que essas tecnologias

00:05:15.286 --> 00:05:18.396
trouxeram-nos coisas
que ampliaram a experiência humana

00:05:18.420 --> 00:05:20.366
de formas profundas.

00:05:21.660 --> 00:05:24.206
Então, vamos levar isso 
um pouco mais longe.

00:05:24.940 --> 00:05:29.399
Eu não temo a criação
duma IA como esta,

00:05:29.700 --> 00:05:33.516
porque ela acabará por incorporar
alguns dos nossos valores.

00:05:33.711 --> 00:05:37.264
Considerem isto: construir um sistema
cognitivo é totalmente diferente

00:05:37.288 --> 00:05:40.822
de construir um sistema tradicional
de software intensivo do passado.

00:05:40.875 --> 00:05:43.036
Não os programamos. Ensinamo-los.

00:05:43.107 --> 00:05:45.535
Para ensinar um sistema 
a reconhecer flores,

00:05:45.587 --> 00:05:48.556
mostro-lhe milhares de flores
dos tipos que gosto.

00:05:48.665 --> 00:05:50.836
Para ensinar um sistema
a jogar um jogo...

00:05:51.002 --> 00:05:53.610
— bem, eu ensinaria,
vocês também ensinariam.

00:05:54.420 --> 00:05:56.574
Adoro flores. Vamos lá.

00:05:57.260 --> 00:06:00.116
Para ensinar um sistema
a jogar um jogo como o Go,

00:06:00.159 --> 00:06:02.291
deixá-lo-ia jogar
milhares de jogos do Go,

00:06:02.324 --> 00:06:04.009
mas no processo também lhe ensino

00:06:04.061 --> 00:06:06.439
como discernir um jogo bom
de um jogo mau.

00:06:06.587 --> 00:06:10.150
Se eu quiser criar um assistente legal
artificialmente inteligente,

00:06:10.183 --> 00:06:12.416
vou ensinar-lhe algumas noções de leis

00:06:12.445 --> 00:06:14.839
mas, ao mesmo tempo,
estou a introduzir-lhe

00:06:14.882 --> 00:06:18.020
o sentido de piedade e de justiça
que faz parte dessas leis.

00:06:18.380 --> 00:06:21.584
Em termos científicos, é aquilo
a que chamamos verdade fundamental,

00:06:21.627 --> 00:06:23.548
e este é o ponto importante:

00:06:23.591 --> 00:06:25.095
ao produzir estas máquinas,

00:06:25.128 --> 00:06:28.106
estamos a ensinar-lhes
um sentido dos nossos valores.

00:06:28.340 --> 00:06:32.114
Para esse fim, confio tanto
numa inteligência artificial,

00:06:32.185 --> 00:06:35.549
— ou mesmo mais — do que
num ser humano que é bem formado.

00:06:35.900 --> 00:06:37.316
Mas podem perguntar:

00:06:37.349 --> 00:06:39.908
"E quanto a agentes desonestos,

00:06:39.970 --> 00:06:43.116
"a alguma organização
não governamental bem financiada?"

00:06:43.301 --> 00:06:46.956
Eu não temo uma inteligência artificial
na mão de um lobo solitário.

00:06:47.180 --> 00:06:49.244
Claramente, não conseguimos proteger-nos

00:06:49.282 --> 00:06:51.597
contra todos os atos
fortuitos de violência,

00:06:51.663 --> 00:06:53.828
mas a realidade é que um sistema destes

00:06:53.880 --> 00:06:56.796
requer formação substancial
e formação subtil

00:06:56.848 --> 00:06:59.230
muito para além
dos recursos de um indivíduo.

00:06:59.311 --> 00:07:00.460
E além disso,

00:07:00.503 --> 00:07:03.797
é muito mais do que injetar
um vírus de Internet no mundo

00:07:03.840 --> 00:07:06.756
em que apertamos um botão,
e ele aparece num milhão de lugares

00:07:06.780 --> 00:07:09.483
e os portáteis começam a explodir
por toda a parte.

00:07:09.526 --> 00:07:12.228
Estes tipos de substância 
são muito maiores,

00:07:12.280 --> 00:07:14.195
e certamente vamos vê-los chegar.

00:07:14.473 --> 00:07:17.396
Será que receio
que essa inteligência artificial

00:07:17.420 --> 00:07:19.722
possa ameaçar toda a humanidade?

00:07:20.100 --> 00:07:24.152
Se olharmos para filmes
como "The Matrix", "Metropolis",

00:07:24.214 --> 00:07:27.676
"O Exterminador Implacável", 
séries como "Westworld",

00:07:27.700 --> 00:07:29.836
todos falam desse tipo de medo.

00:07:30.012 --> 00:07:34.156
De facto, no livro "Superintelligence"
do filósofo Nick Bostrom,

00:07:34.180 --> 00:07:35.820
ele pega neste tema

00:07:35.863 --> 00:07:39.822
e nota que uma superinteligência
pode não só ser perigosa,

00:07:39.903 --> 00:07:43.636
pode representar uma ameaça existencial
para toda a humanidade.

00:07:43.755 --> 00:07:45.933
O argumento básico do Dr. Bostrom

00:07:45.985 --> 00:07:48.902
é que tais sistemas vão acabar por ter

00:07:48.964 --> 00:07:51.973
uma tal ânsia insaciável de informação

00:07:52.035 --> 00:07:54.836
que talvez aprendam como aprender

00:07:54.936 --> 00:07:57.523
e acabem por descobrir
que podem ter metas

00:07:57.576 --> 00:07:59.891
que são contrárias 
às necessidades humanas.

00:07:59.943 --> 00:08:01.895
O Dr. Bostrom tem muitos seguidores.

00:08:01.957 --> 00:08:06.210
É apoiado por pessoas como
Elon Musk e Stephen Hawking.

00:08:06.795 --> 00:08:09.338
Com o devido respeito

00:08:09.980 --> 00:08:11.996
a essas mentes brilhantes,

00:08:12.020 --> 00:08:14.495
acho que estão fundamentalmente errados.

00:08:14.547 --> 00:08:17.552
Há muitas peças do argumento
do Dr. Bostrom para desmontar,

00:08:17.614 --> 00:08:19.912
mas não tenho tempo
para desmontá-las a todas,

00:08:19.964 --> 00:08:22.356
mas muito rapidamente,
considerem isto:

00:08:22.580 --> 00:08:26.077
o super conhecimento
é muito diferente da super ação.

00:08:26.140 --> 00:08:28.750
O HAL só foi uma ameaça
para a tripulação do Discovery

00:08:28.793 --> 00:08:32.542
na medida em que o HAL comandava
todos os aspetos do Discovery.

00:08:32.623 --> 00:08:35.110
Então teria de ser com
superinteligência.

00:08:35.181 --> 00:08:37.668
Teria que ter o domínio 
sobre todo o nosso mundo.

00:08:37.721 --> 00:08:40.517
Isto é o material do Skynet
de “O Exterminador Implacável”

00:08:40.580 --> 00:08:42.426
em que tínhamos uma superinteligência

00:08:42.488 --> 00:08:44.055
que comandava a vontade humana,

00:08:44.088 --> 00:08:47.325
que dirigia todos os dispositivos
que estavam pelo mundo inteiro.

00:08:47.540 --> 00:08:49.157
Em termos práticos,

00:08:49.182 --> 00:08:51.049
isso não vai acontecer.

00:08:51.140 --> 00:08:54.243
Não estamos a construir IA
que controlam o tempo,

00:08:54.296 --> 00:08:55.879
que dirigem as marés,

00:08:55.913 --> 00:08:59.108
que comandam os seres humanos
caprichosos e caóticos.

00:08:59.199 --> 00:09:02.790
E além disso, se existisse
tal inteligência artificial,

00:09:02.861 --> 00:09:05.883
teria de competir com economias humanas,

00:09:05.936 --> 00:09:08.722
e, portanto, competir connosco
pelos recursos.

00:09:09.020 --> 00:09:10.236
E, no final,

00:09:10.260 --> 00:09:12.176
— não digam isto ao Siri —

00:09:12.260 --> 00:09:14.102
podemos sempre desligá-los.

00:09:14.169 --> 00:09:15.780
(Risos)

00:09:17.275 --> 00:09:19.636
Estamos numa viagem incrível

00:09:19.660 --> 00:09:22.156
de coevolução com as nossas máquinas.

00:09:22.322 --> 00:09:24.799
Os seres humanos que somos hoje

00:09:24.852 --> 00:09:27.236
não são os seres humanos
que seremos depois.

00:09:27.412 --> 00:09:30.596
Preocuparmo-nos agora com o aparecimento
duma superinteligência

00:09:30.648 --> 00:09:33.628
é, em muitos aspetos,
uma diversão perigosa.

00:09:33.690 --> 00:09:36.007
porque o aparecimento
da própria informática

00:09:36.060 --> 00:09:39.018
traz-nos uma série
de questões humanas e sociais

00:09:39.061 --> 00:09:41.101
às quais devemos agora dar atenção.

00:09:41.180 --> 00:09:43.996
Como é que organizo melhor a sociedade

00:09:44.020 --> 00:09:46.517
quando a necessidade
de trabalho humano diminui?

00:09:46.561 --> 00:09:50.281
Como posso trazer compreensão
e educação a todo o mundo

00:09:50.324 --> 00:09:52.243
sem desrespeitar as nossas diferenças?

00:09:52.277 --> 00:09:56.428
Como posso ampliar e melhorar a vida humana
através da assistência médica cognitiva?

00:09:56.480 --> 00:09:59.279
Como posso usar a informática

00:09:59.332 --> 00:10:01.349
para nos levar às estrelas?

00:10:01.580 --> 00:10:03.696
E esta é a coisa excitante.

00:10:04.220 --> 00:10:06.556
As oportunidades de usar a informática

00:10:06.580 --> 00:10:08.516
para fazer avançar a experiência humana

00:10:08.568 --> 00:10:11.070
estão ao nosso alcance,
aqui e agora,

00:10:11.460 --> 00:10:13.578
e estamos apenas a começar.

00:10:14.100 --> 00:10:15.392
Muito obrigado.

00:10:15.425 --> 00:10:18.521
(Aplausos)

