WEBVTT
Kind: captions
Language: tr

00:00:00.000 --> 00:00:07.000
Çeviri: Enes Anbar
Gözden geçirme: Cihan Ekmekçi

00:00:12.580 --> 00:00:16.420
Çocukken tam bir inektim.

00:00:17.164 --> 00:00:19.340
Galiba bazılarınız da inektiniz.

00:00:19.340 --> 00:00:20.556
(Kahkaha)

00:00:20.580 --> 00:00:23.796
Ve siz beyefendi, en sesli gülen,
siz muhtemelen hala ineksiniz.

00:00:23.820 --> 00:00:26.076
(Kahkaha)

00:00:26.100 --> 00:00:29.596
Kuzey Teksas'ın tozlu düz arazilerinde 
küçük bir kasabada büyüdüm.

00:00:29.620 --> 00:00:32.956
Babam şerif, 
büyükbabam ise bir papazdı.

00:00:32.980 --> 00:00:34.900
Başımı belaya sokmak 
hiç kaçınılmaz değildi.

00:00:35.860 --> 00:00:39.116
Bu yüzden öylesine matematik 
kitapları okumaya başladım.

00:00:39.140 --> 00:00:40.676
(Kahkaha)

00:00:40.700 --> 00:00:42.396
Sen de okudun.

00:00:42.420 --> 00:00:46.156
Bu okumalar lazer, bilgisayar ve
maket roketler yapmamı sağladı.

00:00:46.180 --> 00:00:49.180
Bunlar ise roket yakıtı yapmamı sağladı.

00:00:49.780 --> 00:00:53.436
Şimdi, bilimsel olarak

00:00:53.460 --> 00:00:56.716
biz buna "çok kötü bir fikir" deriz.

00:00:56.740 --> 00:00:57.956
(Kahkaha)

00:00:57.980 --> 00:01:00.156
O zamanlarda,

00:01:00.180 --> 00:01:03.396
Stanley Kubrick'in filmi 
"2001: Uzay Macerası" sinemalara gelmişti,

00:01:03.420 --> 00:01:05.620
ve benim hayatım sonsuza kadar değişmişti.

00:01:06.100 --> 00:01:08.156
Bu filmi çok sevmiştim,

00:01:08.180 --> 00:01:10.716
özellikle HAL 9000'i.

00:01:10.740 --> 00:01:12.796
HAL, uzay gemisi Keşif'i Dünya'dan

00:01:12.820 --> 00:01:15.276
Jüpiter'e götürmek için tasarlanmış

00:01:15.300 --> 00:01:17.836
duyguları olan bir bilgisayar.

00:01:17.860 --> 00:01:19.916
HAL ayrıca kusurlu bir karaktere sahip.

00:01:19.940 --> 00:01:24.220
Çünkü sonunda, görevi 
insan hayatına tercih etmişti.

00:01:24.660 --> 00:01:26.756
HAL uydurma bir karakter,

00:01:26.780 --> 00:01:29.436
ama yine de o korkularımızı simgeliyor,

00:01:29.460 --> 00:01:31.556
İnsanlığımızı umursamayan

00:01:31.580 --> 00:01:34.596
duygusuz bir yapay zeka tarafından

00:01:34.620 --> 00:01:36.580
kontrol altına alınma korkumuz.

00:01:37.700 --> 00:01:40.276
Bu tür korkuların yersiz 
olduğuna inanıyorum.

00:01:40.300 --> 00:01:42.996
Gerçekten de insan tarihinde

00:01:43.020 --> 00:01:44.556
olağanüstü bir zamandayız.

00:01:44.580 --> 00:01:49.556
Vücudumuzun ve aklımızın
limitleri olduğunu kabul etmeyişimizle

00:01:49.580 --> 00:01:51.276
Yaşam deneyimini

00:01:51.300 --> 00:01:54.916
hayallerin ötesinde geliştirecek

00:01:54.940 --> 00:01:56.996
üstün bir karmaşıklık ve zarafete sahip

00:01:57.020 --> 00:01:58.700
makineler üretiyoruz.

00:01:59.540 --> 00:02:02.116
Beni hava harp okulundan 
hava komutasına, oradan da

00:02:02.140 --> 00:02:04.076
şimdiye getiren kariyerden sonra

00:02:04.100 --> 00:02:05.796
sistem mühendisi oldum.

00:02:05.820 --> 00:02:08.556
Geçenlerde NASA'nın Mars göreviyle ilgili

00:02:08.580 --> 00:02:11.156
bir mühendislik probleminde yer aldım.

00:02:11.180 --> 00:02:13.676
Ay'a giden uzay uçuşlarında

00:02:13.700 --> 00:02:16.836
uçuşla ilgili her şeyde

00:02:16.860 --> 00:02:18.836
Houston'daki görev merkezine bağlıyız.

00:02:18.860 --> 00:02:22.396
Fakat, Mars 200 kat uzakta.

00:02:22.420 --> 00:02:25.636
Sonuç olarak, bir sinyalin
Dünya'dan Mars'a ulaşması

00:02:25.660 --> 00:02:28.796
ortalama 13 dakika sürüyor.

00:02:28.820 --> 00:02:32.220
Bir sorun varsa,
yeteri kadar zaman yok.

00:02:32.660 --> 00:02:35.156
Makul bir mühendislik çözümü sunmak için

00:02:35.180 --> 00:02:37.756
görev kontrolü uzay aracı Orion'ın

00:02:37.780 --> 00:02:40.796
içine koymamız gerekir.

00:02:40.820 --> 00:02:43.716
Görev profilindeki başka
bir enteresan fikir

00:02:43.740 --> 00:02:46.636
insanlar Mars'a varmadan önce,

00:02:46.660 --> 00:02:48.516
Mars'ın yüzeyine insansı robotlar

00:02:48.540 --> 00:02:50.196
gönderip tesisler kurdurmak,

00:02:50.220 --> 00:02:53.580
sonra onları araştırma ekibinin 
ortak mensubu yapmak.

00:02:55.220 --> 00:02:57.956
Bu fikre bir mühendisin 
bakış açısından baktığımda,

00:02:57.980 --> 00:03:01.156
tasarlamam gereken şeyin
akıllı, iş birliği yapabilen,

00:03:01.180 --> 00:03:03.356
sosyal zekaya sahip
bir yapay zeka olduğu

00:03:03.380 --> 00:03:05.756
çok açık ve netti.

00:03:05.780 --> 00:03:10.076
Başka deyişle, HAL'e çok benzeyen 
bir şey yapmam gerekti.

00:03:10.100 --> 00:03:12.516
ama adam öldürme meyili olmayan bir HAL.

00:03:12.540 --> 00:03:13.900
(Kahkaha)

00:03:14.740 --> 00:03:16.556
Biraz ara verelim.

00:03:16.580 --> 00:03:20.476
Böyle bir yapay zeka yapmak 
gerçekten mümkün mü?

00:03:20.500 --> 00:03:21.956
Aslında mümkün.

00:03:21.980 --> 00:03:23.236
Bir çok yönüyle,

00:03:23.260 --> 00:03:25.236
yapay zeka ilkeleriyle, bu zor

00:03:25.260 --> 00:03:26.716
bir mühendislik problemidir,

00:03:26.740 --> 00:03:31.436
mühendisliği yapılacak 
basit bir yapay zeka problemi değil.

00:03:31.460 --> 00:03:34.116
Alan Turing'in dediği gibi,

00:03:34.140 --> 00:03:36.516
Duyguları olan bir makine 
yapmakla ilgilenmiyorum.

00:03:36.540 --> 00:03:38.116
Bir HAL yapmıyorum.

00:03:38.140 --> 00:03:40.556
Tek yapmak istediğim,

00:03:40.580 --> 00:03:43.700
zeka yanılsaması gösteren basit bir beyin.

00:03:44.820 --> 00:03:47.956
HAL ilk gösterildiğinden beri,
programlama sanatı ve bilimi

00:03:47.980 --> 00:03:49.476
uzun yol kat etti.

00:03:49.500 --> 00:03:52.716
HAL'in yaratıcısı eğer burada olsaydı,

00:03:52.740 --> 00:03:55.076
eminim bize soracağı çok soru olurdu.

00:03:55.100 --> 00:03:57.196
"Milyonlarca cihazdan oluşan bir sistemin

00:03:57.220 --> 00:04:01.236
veri akışını okuyup
onların hata ve eylemlerini

00:04:01.260 --> 00:04:02.716
önceden tahmin etmek

00:04:02.740 --> 00:04:04.996
gerçekten mümkün mü?"

00:04:05.020 --> 00:04:06.236
Evet.

00:04:06.260 --> 00:04:09.436
"İnsanlarla doğal dilde sohbet edebilen
sistemler yapabilir miyiz?"

00:04:09.460 --> 00:04:10.676
Evet.

00:04:10.700 --> 00:04:13.676
"Objeleri ve duyguları 
tanımlayabilen, duygusal davranan

00:04:13.700 --> 00:04:17.076
oyun oynayabilen ve hatta dudak
okuyabilen sistemler yapabilir miyiz?

00:04:17.100 --> 00:04:18.316
Evet.

00:04:18.340 --> 00:04:20.476
''Hedefler koyup 
o hedefleri gerçekleştirmek

00:04:20.500 --> 00:04:24.116
için planlar yapan ve bu esnada
öğrenen bir sistem yapabilir miyiz?''

00:04:24.140 --> 00:04:25.356
Evet.

00:04:25.380 --> 00:04:28.716
"Akıl teorisi olan sistemler
yapabilir miyiz?"

00:04:28.740 --> 00:04:30.236
İşte bunu yapmayı öğreniyoruz.

00:04:30.260 --> 00:04:33.740
"Etik ve ahlaki temeli olan 
sistemler yapabilir miyiz?"

00:04:34.300 --> 00:04:36.340
İşte bunu öğrenmek zorundayız.

00:04:37.180 --> 00:04:38.556
Farz edelim ki

00:04:38.580 --> 00:04:41.476
bu ve benzeri amaçlar için bu tür

00:04:41.500 --> 00:04:43.636
bir yapay zeka geliştirmek mümkün olsun.

00:04:43.660 --> 00:04:46.196
Kendinize sormanız gereken 
ilk soru şu olmalı:

00:04:46.220 --> 00:04:47.676
Bundan korkmalı mıyız?

00:04:47.700 --> 00:04:49.676
Her yeni teknoloji

00:04:49.700 --> 00:04:52.596
korkuyu da kendiyle birlikte getirir.

00:04:52.620 --> 00:04:54.316
Arabaları ilk gördüğümüzde

00:04:54.340 --> 00:04:58.356
insanlar ailenin yok oluşunu
göreceğimizden yakındılar.

00:04:58.380 --> 00:05:01.076
Telefonlar hayatımıza girdiğinde

00:05:01.100 --> 00:05:03.996
herkes bunun medeni konuşmayı
tahrip edeceğinden endişeliydi.

00:05:04.020 --> 00:05:07.956
Yazılı kelimelerin kalıcı 
olduğunu anladığımız zaman

00:05:07.980 --> 00:05:10.476
insanlar ezber yeteneğimizin
kaybolacağını sandılar.

00:05:10.500 --> 00:05:12.556
Bütün bunlar bir noktaya kadar doğru

00:05:12.580 --> 00:05:14.996
Fakat bu teknolojilerin 
yaşam deneyimimizi

00:05:15.020 --> 00:05:18.396
inanılmaz şekillerde geliştirecek şeyler

00:05:18.420 --> 00:05:20.300
getirdiği de bir gerçek.

00:05:21.660 --> 00:05:23.940
Hadi bunu bir adım daha öteye taşıyalım.

00:05:24.940 --> 00:05:29.676
Bu tür bir yapay zekanın 
yaratılmasından korkmuyorum,

00:05:29.700 --> 00:05:33.516
çünkü zamanla yapay zeka
bazı değerlerimizi cisimleştirecek.

00:05:33.540 --> 00:05:37.036
Şunu düşünün: Bilişsel bir sistem yapmak
geleneksel yazılım yoğunluklu

00:05:37.060 --> 00:05:40.356
bir sistem yapmaktan temel olarak farklı.

00:05:40.380 --> 00:05:42.836
Onları programlamıyoruz. 
Onlara öğretiyoruz.

00:05:42.860 --> 00:05:45.516
Bir sisteme çiçekleri nasıl 
tanıyacağını öğretmek için

00:05:45.540 --> 00:05:48.556
ona sevdiğim türde 
binlerce çiçek gösteririm.

00:05:48.580 --> 00:05:50.836
Bir sisteme oyun oynamayı 
öğretmek için

00:05:50.860 --> 00:05:52.820
Öğretirdim, sen de öğretirdin.

00:05:54.420 --> 00:05:56.460
Çiçekleri severim, hadi ama.

00:05:57.260 --> 00:06:00.116
Bir sisteme Go gibi bir oyunu
oynamayı öğretmek için

00:06:00.140 --> 00:06:02.196
Ona binlerce Go oynattırırdım.

00:06:02.220 --> 00:06:03.876
Fakat bu süreçte ona ayrıca

00:06:03.900 --> 00:06:06.316
iyi oyunu kötü oyundan 
ayırmayı da öğretirim.

00:06:06.340 --> 00:06:10.036
Yapay zekası olan bir 
avukat yardımcısı yaratmak istersem,

00:06:10.060 --> 00:06:11.836
ona biraz hukuk dağarcığı öğretirim,

00:06:11.860 --> 00:06:14.716
ama aynı zamanda 
hukukun bir parçası olan

00:06:14.740 --> 00:06:17.620
merhamet ve adalet 
duygusunu da aşılarım.

00:06:18.380 --> 00:06:21.356
Bilimsel olarak söyleyecek olursam,
buna "kesin referans" deriz

00:06:21.380 --> 00:06:23.396
İşte önemli nokta şu:

00:06:23.420 --> 00:06:24.876
Bu makineleri üretirken

00:06:24.900 --> 00:06:28.316
onlara kendi değerlerimizden
bir parça da öğretiriz.

00:06:28.340 --> 00:06:31.476
Bu noktaya kadar, daha çok olmasa da 
yapay zekaya iyi yetişmiş

00:06:31.500 --> 00:06:35.140
bir insana güvendiğim kadar güvenirim.

00:06:35.900 --> 00:06:37.116
Fakat şunu sorabilirsiniz:

00:06:37.140 --> 00:06:39.756
Ya kötüye kullanan 3. partiler?

00:06:39.780 --> 00:06:43.116
bazı iyi finanse edilmiş
sivil toplum kuruluşları?

00:06:43.140 --> 00:06:46.956
Yalnız bir kurdun elindeki 
yapay zekadan korkmam.

00:06:46.980 --> 00:06:51.516
Açıkçası, kendimizi bütün rastgele
şiddet olaylarına karşı koruyamayız.

00:06:51.540 --> 00:06:53.676
Ancak gerçeklik öyle bir sistem ki

00:06:53.700 --> 00:06:56.796
insan kaynaklarının çok ötesinde
oldukça büyük

00:06:56.820 --> 00:06:59.116
ve kıvrak zekaya sahip
bir eğitime ihtiyaç duyar.

00:06:59.140 --> 00:07:00.356
Dahası...

00:07:00.380 --> 00:07:03.636
Bir düğmeye basıp milyonlarca 
yere yayılan ve

00:07:03.660 --> 00:07:06.756
laptopların her yerde patlamasına 
yol açan bir internet virüsü

00:07:06.780 --> 00:07:09.236
sızdırmaktan çok daha fazlası.

00:07:09.260 --> 00:07:12.076
Bu tür maddeler çok daha büyük,

00:07:12.100 --> 00:07:13.815
ve kesinlikle bunlara tanık olacağız.

00:07:14.340 --> 00:07:17.396
Yapay zekanın bütün insanlığı

00:07:17.420 --> 00:07:19.380
tehdit edebileceğinden korkuyor muyum?

00:07:20.100 --> 00:07:24.476
"The Matrix", "Metropolis", "Terminator"
gibi filmlere ya da

00:07:24.500 --> 00:07:27.676
"Westworld" gibi dizilere bakarsanız

00:07:27.700 --> 00:07:29.836
onlar hep bu tür bir korkudan bahsederler.

00:07:29.860 --> 00:07:34.156
Gerçekten de filozof Nick Bostrom 
"Süper Zeka" kitabında

00:07:34.180 --> 00:07:35.716
bu konudan bahseder.

00:07:35.740 --> 00:07:39.756
Süper zekanın sadece tehlikeli 
olabileceğini değil

00:07:39.780 --> 00:07:43.636
ayrıca varoluşsal bir tehdit 
oluşturabileceğini gözlemliyor.

00:07:43.660 --> 00:07:45.876
Dr. Bostrom'un temel argümanı şöyle:

00:07:45.900 --> 00:07:48.636
Bu sistemler zamanla o kadar çok

00:07:48.660 --> 00:07:51.916
bilgiye doyumsuzca aç olacaklar ki

00:07:51.940 --> 00:07:54.836
belki de öğrenmeyi öğrenecekler

00:07:54.860 --> 00:07:57.476
ve zamanla insanların 
ihtiyaçlarıyla çelişen

00:07:57.500 --> 00:07:59.796
amaçlara sahip olacaklar.

00:07:59.820 --> 00:08:01.676
Dr. Bostrom'un bir çok takipçisi var.

00:08:01.700 --> 00:08:06.020
Elon Musk ve Stephen Hawking gibi
insanlar tarafından destekleniyor.

00:08:06.700 --> 00:08:09.100
Bu parlak beyinlere

00:08:09.980 --> 00:08:11.996
saygısızlık etmek istemem ama

00:08:12.020 --> 00:08:14.276
onların aslında hatalı 
olduğuna inanıyorum.

00:08:14.300 --> 00:08:17.476
Dr. Bostrom'un argümanlarının
ayrıntılarına inmek isterdim ama

00:08:17.500 --> 00:08:19.636
o kadar zamanım yok.

00:08:19.660 --> 00:08:22.356
Şunu çok kısaca bir düşünün:

00:08:22.380 --> 00:08:26.116
Süper bilmek süper yapmaktan çok farklı.

00:08:26.140 --> 00:08:28.036
HAL Keşif'i bütün yönleriyle

00:08:28.060 --> 00:08:32.476
komuta ettiği sürece,
o Keşif tayfasına karşı bir tehditti.

00:08:32.500 --> 00:08:34.996
Bu yüzden onun süper 
zekaya sahip olması gerekir.

00:08:35.020 --> 00:08:37.516
Onun bütün dünya üzerinde 
egemenliği olurdu.

00:08:37.540 --> 00:08:40.356
Bu The Terminator'deki Skynet şeyi işte,

00:08:40.380 --> 00:08:42.236
İnsan iradesi üzerinde kontrolü olan,

00:08:42.260 --> 00:08:43.636
dünyanın her bir köşesindeki

00:08:43.660 --> 00:08:47.516
her bir cihazı yöneten 
bir süper zeka.

00:08:47.540 --> 00:08:48.996
Gerçekçi konuşursak,

00:08:49.020 --> 00:08:51.116
bu olmayacak.

00:08:51.140 --> 00:08:54.196
Bizler havayı kontrol eden, 
zamanı yönlendiren,

00:08:54.220 --> 00:08:55.556
kaprisli, karmakarışık

00:08:55.580 --> 00:08:58.956
insanları komuta eden 
yapay zekalar yapmıyoruz.

00:08:58.980 --> 00:09:02.876
Buna ek olarak, 
böyle bir yapay zeka olsaydı,

00:09:02.900 --> 00:09:05.836
insan ekonomisiyle yarışmak zorunda 
olurdu, dolayısıyla

00:09:05.860 --> 00:09:08.380
kaynaklar için bizimle yarışırlardı.

00:09:09.020 --> 00:09:10.236
Sonunda

00:09:10.260 --> 00:09:11.500
Siri'ye şunu söylemeyin:

00:09:12.260 --> 00:09:13.636
"Hiç olmadı fişini çekeriz."

00:09:13.660 --> 00:09:15.780
(Kahkaha)

00:09:17.180 --> 00:09:19.636
Makinelerimizle mükemmel bir

00:09:19.660 --> 00:09:22.156
evrim birlikteliği yolculuğundayız.

00:09:22.180 --> 00:09:24.676
Bugünkü olduğumuz insan

00:09:24.700 --> 00:09:27.236
o zamankiyle aynı olmayacak.

00:09:27.260 --> 00:09:30.396
Süper zekanın yükselişi hakkında
şimdi bu kadar endişelenmemiz

00:09:30.420 --> 00:09:33.476
bir çok yönüyle tehlikeli bir 
dikkat dağınıklığıdır,

00:09:33.500 --> 00:09:35.836
çünkü programlamanın yükselişi

00:09:35.860 --> 00:09:38.876
bize çözmemiz gereken bir çok
insancıl ve toplumsal

00:09:38.900 --> 00:09:40.540
sorunlar getirecek.

00:09:41.180 --> 00:09:43.996
İnsanların iş gücüne olan 
ihtiyaç azaldığında

00:09:44.020 --> 00:09:46.356
toplumda düzeni en iyi nasıl sağlarım:

00:09:46.380 --> 00:09:50.196
Dünya geneline nasıl anlayış ve 
eğitim getiririm ve yine de

00:09:50.220 --> 00:09:51.996
farklılıklarımıza saygı gösteririm?

00:09:52.020 --> 00:09:56.276
İnsan hayatını bilişsel sağlık hizmeti ile
nasıl genişletebilir ve geliştirebilirim?

00:09:56.300 --> 00:09:59.156
Bizi yıldızlara götürecek programlamayı

00:09:59.180 --> 00:10:00.940
nasıl kullabilirim?

00:10:01.580 --> 00:10:03.620
İşte bu heyecan verici bir şey.

00:10:04.220 --> 00:10:06.556
İnsan deneyimini ilerletmek için

00:10:06.580 --> 00:10:08.116
programlama kullanma fırsatı

00:10:08.140 --> 00:10:09.556
elimizde

00:10:09.580 --> 00:10:11.436
Bu aralar

00:10:11.460 --> 00:10:13.140
daha yeni başlıyoruz.

00:10:14.100 --> 00:10:15.316
Çok teşekkürler.

00:10:15.340 --> 00:10:19.626
(Alkış)

