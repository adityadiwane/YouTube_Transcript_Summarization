WEBVTT
Kind: captions
Language: hu

00:00:00.000 --> 00:00:07.000
Fordító: Péter Pallós
Lektor: Melissa Csikszentmihályi

00:00:12.580 --> 00:00:16.420
Kiskoromban a végletekig kocka voltam.

00:00:17.140 --> 00:00:19.316
Szerintem páran önök közül is.

00:00:19.340 --> 00:00:20.556
(Nevetés)

00:00:20.580 --> 00:00:23.796
Maga, uram, aki a leghangosabban
nevetett, talán még mindig az.

00:00:23.820 --> 00:00:26.030
(Nevetés)

00:00:26.030 --> 00:00:29.596
Észak-Texas poros pusztai 
kisvárosában nőttem föl,

00:00:29.620 --> 00:00:32.956
egy sheriff fiaként, 
akinek lelkész volt az apja.

00:00:32.980 --> 00:00:34.900
Rosszalkodni nem volt lehetőségem.

00:00:35.730 --> 00:00:39.116
Szórakozásból differenciál- és
integrálszámítás-könyvet kezdtem olvasni.

00:00:39.140 --> 00:00:40.816
(Nevetés)

00:00:40.816 --> 00:00:42.396
Maguk is.

00:00:42.440 --> 00:00:46.176
Így kezdtem lézert, számítógépet
és rakétamodellt építeni.

00:00:46.180 --> 00:00:49.870
Aztán rákaptam arra, hogy a hálószobámban
rakéta-hajtóanyagot kontyvasszak..

00:00:51.820 --> 00:00:53.436
Tudományos értelemben

00:00:53.460 --> 00:00:56.716
ez nagyon rossz ötlet.

00:00:56.740 --> 00:00:57.956
(Nevetés)

00:00:57.980 --> 00:01:00.156
Akkortájt kezdték játszani

00:01:00.180 --> 00:01:03.396
Stanley Kubrick 2001: Űrodüsszeia
c. filmjét a mozikban,

00:01:03.420 --> 00:01:05.620
s életem mindörökre megváltozott.

00:01:06.100 --> 00:01:08.156
Minden rettentően tetszett a filmben,

00:01:08.180 --> 00:01:10.716
különösen a HAL 9000.

00:01:10.740 --> 00:01:12.796
A HAL tudományos számítógép volt,

00:01:12.820 --> 00:01:16.056
amelyet a Discovery űrhajó
Föld–Jupiter közötti útjának

00:01:16.070 --> 00:01:17.836
irányítására terveztek.

00:01:17.860 --> 00:01:19.916
A HAL ráadásul romlott jellemű volt,

00:01:19.940 --> 00:01:24.220
mert végül az emberi életnél 
többre értékelte a küldetést.

00:01:24.660 --> 00:01:26.756
A HAL kitalált szereplő volt,

00:01:26.780 --> 00:01:29.436
de azért hangot ad félelmünknek,

00:01:29.460 --> 00:01:31.556
azon félelmünknek, hogy alá vagyunk vetve

00:01:31.580 --> 00:01:34.596
bizonyos érzéketlen
mesterséges intelligenciának (MI),

00:01:34.620 --> 00:01:36.580
aki közömbös emberi mivoltunk iránt.

00:01:37.700 --> 00:01:40.276
Ezek a félelmek alaptalanok.

00:01:40.300 --> 00:01:42.996
Olyan nevezetes időket élünk

00:01:43.020 --> 00:01:44.556
most történelmünkben,

00:01:44.580 --> 00:01:49.556
amelyben testünk és lelkünk korlátait
vagy tagadva vagy elfogadva

00:01:49.580 --> 00:01:51.276
kiváló, gyönyörűen bonyolult

00:01:51.300 --> 00:01:54.916
és kecses gépeket építünk,

00:01:54.940 --> 00:01:56.996
amelyek képzeletünket túlszárnyalva

00:01:57.020 --> 00:01:58.700
kibővítik emberi élményeinket.

00:01:59.540 --> 00:02:02.116
Pályafutásom során,
amely a Légierő Akadémiától

00:02:02.140 --> 00:02:04.076
az Űrparancsnokságig ívelt,

00:02:04.100 --> 00:02:05.796
rendszermérnök lettem.

00:02:05.820 --> 00:02:08.556
Nemrég a NASA 
Mars-expedíciójával kapcsolatos

00:02:08.580 --> 00:02:11.156
egyik mérnöki feladatba vontak be.

00:02:11.180 --> 00:02:13.676
A holdutazáskor támaszkodhatunk

00:02:13.700 --> 00:02:16.740
az expedíció Houstonból 
történő irányítására,

00:02:16.750 --> 00:02:18.836
amellyel leshetjük
az út minden mozzanatát.

00:02:18.860 --> 00:02:22.396
De a Mars 200-szor messzebb van,

00:02:22.420 --> 00:02:25.636
ezért a jelek átlag 13 perc alatt

00:02:25.660 --> 00:02:28.796
jutnak el a Földről a Marsra.

00:02:28.820 --> 00:02:32.220
Ha baj történik, szűk az idő.

00:02:32.660 --> 00:02:35.156
Ezért az észszerű mérnöki megoldás

00:02:35.180 --> 00:02:37.756
azt diktálja, hogy az utazás irányítását

00:02:37.780 --> 00:02:40.796
az Orion űrhajón belül oldjuk meg.

00:02:40.820 --> 00:02:43.716
Az utazáshoz kapcsolódik
egy másik elragadó ötlet:

00:02:43.740 --> 00:02:46.636
vigyünk a Mars felszínére
humanoid robotokat,

00:02:46.660 --> 00:02:48.516
még az ember odaérkezése előtt,

00:02:48.540 --> 00:02:50.196
előbb a berendezések megépítésére,

00:02:50.220 --> 00:02:53.580
majd hogy hasznos tagjai 
legyenek a tudóscsoportnak.

00:02:55.220 --> 00:02:57.956
Mérnöki szemszögből nézve

00:02:57.980 --> 00:02:59.780
világossá vált számomra,

00:02:59.810 --> 00:03:03.050
hogy okos, együttműködő,

00:03:03.070 --> 00:03:05.996
szociálisan intelligens
mesterséges értelmet kell alkotnom.

00:03:06.016 --> 00:03:10.076
Azaz, a HAL-hoz valami 
nagyon hasonlót kell építenem,

00:03:10.100 --> 00:03:12.516
de olyant, ami gyilkos hajlamoktól mentes.

00:03:12.540 --> 00:03:13.900
(Nevetés)

00:03:14.740 --> 00:03:16.556
Egy pillanat.

00:03:16.580 --> 00:03:20.476
Tényleg lehetséges ilyen 
mesterséges értelmet létrehozni?

00:03:20.500 --> 00:03:21.956
A válasz: igen.

00:03:21.980 --> 00:03:23.236
Ez több szempontból is

00:03:23.260 --> 00:03:25.236
nehéz mérnöki feladat,

00:03:25.260 --> 00:03:26.716
de az MI elemeivel

00:03:26.740 --> 00:03:31.436
nem olyan, amit ne lehetne megoldani.

00:03:31.460 --> 00:03:34.116
Alan Turing után szabadon:

00:03:34.140 --> 00:03:36.516
Nem érdekel érző gép megépítése.

00:03:36.540 --> 00:03:38.116
Nem építek HAL-t.

00:03:38.140 --> 00:03:40.556
Csupán egyszerű agyat akarok,

00:03:40.580 --> 00:03:43.700
ami az intelligencia illúzióját kelti.

00:03:44.820 --> 00:03:47.956
A számítástechnika művészete
és tudománya hosszú utat tett meg

00:03:47.980 --> 00:03:49.476
a HAL képernyőre kerüléséig,

00:03:49.500 --> 00:03:52.716
és elképzelem, ha feltalálója,
dr. Chandra most itt lenne,

00:03:52.740 --> 00:03:55.076
lenne hozzánk jó pár kérdése.

00:03:55.100 --> 00:03:57.196
Meg tudjuk-e valósítani,

00:03:57.220 --> 00:04:01.236
hogy milliók rendszerét
milliónyi gépre telepítve

00:04:01.260 --> 00:04:02.716
elolvassuk az adatfolyamukat,

00:04:02.740 --> 00:04:04.996
hogy előre megtippeljük 
hibáikat és tetteiket?

00:04:05.020 --> 00:04:06.236
Igen.

00:04:06.260 --> 00:04:09.436
Képesek vagyunk-e emberi nyelven
beszélő gépeket építeni?

00:04:09.460 --> 00:04:10.180
Igen.

00:04:10.210 --> 00:04:13.676
Képesek vagyunk-e tárgyfölismerő,
érzelmeket azonosító,

00:04:13.700 --> 00:04:17.076
érzelmüket kifejező, játékot játszó 
és szájról olvasó gépeket építeni?

00:04:17.100 --> 00:04:18.316
Igen.

00:04:18.340 --> 00:04:20.476
Képesek vagyunk-e célokat kitűző,

00:04:20.500 --> 00:04:24.116
célokat megvalósító és menet
közben tanuló gépeket építeni?

00:04:24.140 --> 00:04:24.996
Igen.

00:04:25.380 --> 00:04:28.716
Képesek vagyunk-e tudatelmélettel 
bíró rendszereket építeni?

00:04:28.740 --> 00:04:30.236
Ezt most tanuljuk.

00:04:30.260 --> 00:04:33.740
Képesek vagyunk-e etikai és erkölcsi
alapokon álló gépeket építeni?

00:04:34.300 --> 00:04:36.340
Ezt még meg kell tanulnunk.

00:04:37.180 --> 00:04:38.556
Egyelőre fogadjuk el,

00:04:38.580 --> 00:04:41.476
hogy űrutazásokra képesek
vagyunk megépíteni

00:04:41.500 --> 00:04:43.636
ilyen mesterséges értelmet.

00:04:43.660 --> 00:04:46.196
A következő kérdés:

00:04:46.220 --> 00:04:47.676
kell-e félnünk tőle?

00:04:47.700 --> 00:04:49.676
Minden új technológia

00:04:49.700 --> 00:04:52.596
bizonyos mértékű felbolydulást okoz.

00:04:52.620 --> 00:04:54.316
Az első kocsik megpillantásakor

00:04:54.340 --> 00:04:58.356
az emberek azon siránkoztak,
hogy tönkremennek a családok.

00:04:58.380 --> 00:05:01.076
Az első telefonok megjelenésekor

00:05:01.100 --> 00:05:03.996
az emberek attól tartottak, 
hogy kipusztul az emberi beszéd.

00:05:04.020 --> 00:05:07.956
Amikor elterjedt a nyomtatott írás,

00:05:07.980 --> 00:05:10.476
az emberek azt hitték,
elvész az emlékezőtehetségünk.

00:05:10.500 --> 00:05:12.556
Bizonyos fokig ez mind igaz,

00:05:12.580 --> 00:05:14.996
de az is tény, hogy az új technológiák

00:05:15.020 --> 00:05:16.886
alapvetően az emberi élmények

00:05:16.920 --> 00:05:20.300
kibővítését eredményezték.

00:05:21.660 --> 00:05:23.940
Gondolkodjunk el ezen egy kicsit.

00:05:24.940 --> 00:05:29.676
Nem félek az MI létrehozásától,

00:05:29.700 --> 00:05:33.516
mert meg fogja testesíteni
némely értékünket.

00:05:33.540 --> 00:05:37.036
Vegyük tekintetbe, hogy kognitív rendszert
építeni teljesen más,

00:05:37.060 --> 00:05:40.356
mint hagyományos szoftveres rendszert.

00:05:40.380 --> 00:05:42.836
Az MI-ket nem programozzuk,
hanem tanítjuk.

00:05:42.860 --> 00:05:45.516
Hogy megtanítsam a virág fölismerését,

00:05:45.540 --> 00:05:48.556
a rendszernek ezernyi
ilyesféle virágot mutatok.

00:05:48.580 --> 00:05:50.836
Hogy megtanítsam játékot játszani...

00:05:50.860 --> 00:05:52.820
Megtenném. Önök is.

00:05:54.420 --> 00:05:56.460
Szeretem a virágokat. Tényleg.

00:05:57.260 --> 00:06:00.116
Hogy megtanítsam a gépnek 
a <i>go</i> játékot játszani,

00:06:00.140 --> 00:06:02.196
ezernyi játszmát kell vele játszanom,

00:06:02.220 --> 00:06:03.876
de közben tanítom is,

00:06:03.900 --> 00:06:06.316
hogyan különböztesse meg
a jó játszmát a rossztól.

00:06:06.340 --> 00:06:10.036
Ha mesterségesen intelligens
jogi segéderőt akarnék csinálni,

00:06:10.060 --> 00:06:12.006
betanítanék neki néhány törvénykönyvet,

00:06:12.006 --> 00:06:14.216
de egyben beleplántálnám

00:06:14.216 --> 00:06:17.760
az irgalom s az igazságosság érzését,
amelyek hozzátartoznak a joghoz.

00:06:18.350 --> 00:06:21.546
Tudományosan ezt hívjuk alapigazságnak,

00:06:21.576 --> 00:06:23.396
s ez a fontos, ez a lényeg:

00:06:23.420 --> 00:06:24.876
a gépek készítésekor

00:06:24.900 --> 00:06:28.316
ezért tanítjuk nekik 
az értékeink iránti fogékonyságot.

00:06:28.340 --> 00:06:31.476
Ezért ugyanúgy bízom, ha nem jobban,

00:06:31.500 --> 00:06:35.140
a mesterséges értelemben, 
mint a jól képzett emberben.

00:06:35.900 --> 00:06:37.916
De kérdezhetik, mi a helyzet

00:06:38.756 --> 00:06:39.976
a gazemberekkel,

00:06:39.976 --> 00:06:43.116
a gazdag civil szervezetekkel?

00:06:43.140 --> 00:06:46.956
Nem tartok attól, hogy a MI
egy magányos farkas kezébe kerül.

00:06:46.980 --> 00:06:51.516
Tény, hogy nem tudjuk magunkat megvédeni
a spontán erőszakos cselekedetektől,

00:06:51.540 --> 00:06:53.676
de az a helyzet, hogy egy ilyen rendszer

00:06:53.700 --> 00:06:56.796
alapos, kifinomult kiképzést igényel,

00:06:56.820 --> 00:06:59.116
ami jócskán meghaladja
egy egyén erőforrásait.

00:06:59.140 --> 00:07:00.356
Továbbá,

00:07:00.380 --> 00:07:03.636
ez sokkal több, mint egy internetvírust
kibocsátani a világba;

00:07:03.660 --> 00:07:06.756
ott csak leütünk egy billentyűt,
és hirtelen milliónyi helyen

00:07:06.780 --> 00:07:09.236
és laptopon működésbe lépnek.

00:07:09.260 --> 00:07:12.076
Ezek az állományok sokkal nagyobbak,

00:07:12.100 --> 00:07:13.815
és biztos eljön az idejük.

00:07:14.340 --> 00:07:17.396
Féljek-e, hogy ilyen mesterséges értelem

00:07:17.420 --> 00:07:19.380
az egész emberiséget veszélyeztetheti?

00:07:20.100 --> 00:07:24.476
Ha megnézzük a Mátrix, a Metropolis,

00:07:24.500 --> 00:07:27.676
a Terminátor c. filmeket,
a Westworld c. filmsorozatot,

00:07:27.700 --> 00:07:29.836
mind az efféle félelemről szólnak.

00:07:29.860 --> 00:07:34.156
Nick Bostrom filozófus
a Szuperintelligencia c. könyvében

00:07:34.180 --> 00:07:35.716
foglalkozik a témával:

00:07:35.740 --> 00:07:39.756
figyelmeztet, hogy a szuper-intelligencia
nemcsak veszélyes lehet,

00:07:39.780 --> 00:07:43.636
hanem az emberiség létét is fenyegetheti.

00:07:43.660 --> 00:07:45.876
Dr. Bostrom fő érve,

00:07:45.900 --> 00:07:48.636
hogy az ilyen rendszereknek végül

00:07:48.660 --> 00:07:51.916
olyan telhetetlen információéhségük lehet,

00:07:51.940 --> 00:07:54.836
hogy talán azt is megtanulják,
hogyan kell tanulni,

00:07:54.860 --> 00:07:57.476
és végső fokon rájönnek,
hogy olyan céljaik lehetnek,

00:07:57.500 --> 00:07:59.796
amelyek ellentétesek
az emberi szükségletekkel.

00:07:59.820 --> 00:08:01.676
Dr. Bostromnak sok követője van.

00:08:01.700 --> 00:08:06.020
Elon Musk 
és Stephen Hawking is támogatja például.

00:08:06.700 --> 00:08:09.100
E ragyogó elmék iránti

00:08:09.980 --> 00:08:11.996
minden kötelező tiszteletem ellenére,

00:08:12.020 --> 00:08:14.276
úgy vélem, hogy alapvetően tévednek.

00:08:14.300 --> 00:08:17.476
Dr. Bostromnak számos, 
megvizsgálásra érdemes érve van,

00:08:17.500 --> 00:08:19.636
de most itt nincs időnk mindegyikre.

00:08:19.660 --> 00:08:22.356
Dióhéjban, gondoljuk át a következőket:

00:08:22.380 --> 00:08:26.116
a szuper tudás egészen más,
mint a szuper cselekedet.

00:08:26.140 --> 00:08:28.986
A HAL akkor jelentett fenyegetést
a Discovery személyzetére,

00:08:29.036 --> 00:08:32.476
amikor a Discoveryt 
teljesen ő irányította.

00:08:32.500 --> 00:08:34.996
Ehhez szuper intelligensnek kell lennie.

00:08:35.020 --> 00:08:37.516
Ez világuralmat jelentene.

00:08:37.540 --> 00:08:40.260
Aztán itt van a Skynet a Terminátorból,

00:08:40.260 --> 00:08:41.920
amelyben szuper intelligencia van,

00:08:41.930 --> 00:08:43.636
amely az emberi akaratot irányítja;

00:08:43.660 --> 00:08:47.516
az egész világon minden készüléket
a Skynet irányít.

00:08:47.540 --> 00:08:48.996
Ilyen a gyakorlatban

00:08:49.020 --> 00:08:51.116
nem történhet meg.

00:08:51.140 --> 00:08:54.196
Nem építünk olyan MI-t,
amely az időjárást

00:08:54.220 --> 00:08:55.556
vagy a dagályt irányítja,

00:08:55.580 --> 00:08:58.956
vagy nekünk, szeszélyes,
zűrös embereknek parancsol.

00:08:58.980 --> 00:09:02.876
Továbbá, ha létezne is
ilyen mesterséges értelem,

00:09:02.900 --> 00:09:05.836
meg kellene küzdenie 
az emberi takarékoskodással,

00:09:05.860 --> 00:09:08.380
és ezért harcolnia kellene
velünk az erőforrásokért.

00:09:09.020 --> 00:09:09.916
Végül...

00:09:10.180 --> 00:09:11.500
– meg ne mondják Sirinek –,

00:09:12.260 --> 00:09:13.746
bármikor kikapcsolhatjuk őket.

00:09:13.766 --> 00:09:15.780
(Nevetés)

00:09:17.180 --> 00:09:19.636
A gépeinkkel közös

00:09:19.660 --> 00:09:22.156
hihetetlen fejlődési úton járunk.

00:09:22.180 --> 00:09:24.676
Ma emberekként nem olyanok vagyunk,

00:09:24.700 --> 00:09:27.236
mint amilyenek egykor leszünk.

00:09:27.260 --> 00:09:30.396
A szuper intelligencia megjelenése
miatti mai aggodalmak

00:09:30.420 --> 00:09:33.476
több szempontból
veszélyes figyelemelterelés,

00:09:33.500 --> 00:09:35.836
mert a számítástechnika fejlődése
már önmagában is

00:09:35.860 --> 00:09:38.876
egy csomó emberi 
és társadalmi kérdést vet föl,

00:09:38.900 --> 00:09:40.540
amelyekbe be kell kapcsolódnunk.

00:09:41.180 --> 00:09:43.996
Hogyan szervezzem 
a legjobban a társadalmat,

00:09:44.020 --> 00:09:46.356
ha a munkaerő-szükséglet csökken?

00:09:46.380 --> 00:09:50.100
Hogyan érhető el megértés
és végezhető oktatás a Földön,

00:09:50.120 --> 00:09:51.996
a különbségek tiszteletben tartásával?

00:09:52.020 --> 00:09:56.276
Hogyan hosszabbítható meg és javítható
az élet kognitív egészségügy révén?

00:09:56.300 --> 00:09:59.156
Hogyan használhatom a számítástechnikát,

00:09:59.180 --> 00:10:00.940
hogy eljussunk a csillagokba?

00:10:01.580 --> 00:10:03.620
Ez itt az izgató kérdés.

00:10:04.190 --> 00:10:06.686
A számítástechnikában rejlő 
lehetőségek kihasználása

00:10:06.706 --> 00:10:08.126
az emberi élmények javítására

00:10:08.166 --> 00:10:09.556
jelenleg csak

00:10:09.580 --> 00:10:11.436
karnyújtásnyira van,

00:10:11.460 --> 00:10:13.140
és ez csupán a kezdet.

00:10:14.100 --> 00:10:15.316
Köszönöm szépen.

00:10:15.340 --> 00:10:19.626
(Taps)

