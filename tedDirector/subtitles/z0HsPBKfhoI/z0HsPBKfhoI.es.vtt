WEBVTT
Kind: captions
Language: es

00:00:00.000 --> 00:00:07.000
Traductor: Claudia Viveros
Revisor: Frank Zegarra

00:00:11.820 --> 00:00:16.420
Cuando era niño,
era un nerd por excelencia.

00:00:17.140 --> 00:00:19.316
Creo que algunos de Uds. también.

00:00:19.340 --> 00:00:20.556
(Risas)

00:00:20.580 --> 00:00:23.796
Y Ud. señor, el que se rio más fuerte,
probablemente aún lo sea.

00:00:23.820 --> 00:00:26.076
(Risas)

00:00:26.100 --> 00:00:29.596
Crecí en un pueblo pequeño
en los llanos del norte de Tejas,

00:00:29.620 --> 00:00:32.956
hijo de un comisario, 
que fue hijo de un pastor.

00:00:32.980 --> 00:00:34.900
Meterse en problemas
no era una opción.

00:00:35.860 --> 00:00:39.116
Así es que comencé a 
leer libros de cálculo por diversión.

00:00:39.140 --> 00:00:40.676
(Risas)

00:00:40.700 --> 00:00:42.396
Uds. también lo hicieron.

00:00:42.420 --> 00:00:46.156
Eso me llevó a crear un láser, 
una computadora y modelos de cohetes,

00:00:46.180 --> 00:00:49.290
y eso me llevó a hacer 
combustible para cohetes en mi habitación.

00:00:49.780 --> 00:00:53.436
En términos científicos,

00:00:53.460 --> 00:00:56.716
esta es una muy mala idea.

00:00:56.740 --> 00:00:57.956
(Risas)

00:00:57.980 --> 00:01:00.156
Por esos días

00:01:00.180 --> 00:01:03.396
se estrenó "2001: Una Odisea
en el Espacio" de Stanley Kubrick,

00:01:03.420 --> 00:01:05.620
y mi vida cambió para siempre.

00:01:06.100 --> 00:01:08.156
Me encantó todo de esa película,

00:01:08.180 --> 00:01:10.716
especialmente el HAL 9000.

00:01:10.740 --> 00:01:12.796
HAL era un computador sensible

00:01:12.820 --> 00:01:15.276
diseñado para guiar 
la aeronave Discovery

00:01:15.300 --> 00:01:17.836
de la Tierra a Júpiter.

00:01:17.860 --> 00:01:19.916
HAL además era imperfecto,

00:01:19.940 --> 00:01:24.220
porque eligió el valor de la misión
sobre la vida humana.

00:01:24.660 --> 00:01:26.756
HAL era un personaje ficticio,

00:01:26.780 --> 00:01:29.436
y sin embargo habla sobre 
nuestros temores,

00:01:29.460 --> 00:01:31.556
nuestros temores a ser subyugados

00:01:31.580 --> 00:01:34.596
por alguna insensible 
inteligencia artificial

00:01:34.620 --> 00:01:36.580
indiferente a nuestra humanidad.

00:01:37.700 --> 00:01:40.276
Creo que esos temores 
son infundados.

00:01:40.300 --> 00:01:42.996
De hecho, estamos en un tiempo notable

00:01:43.020 --> 00:01:44.556
en la historia de la humanidad,

00:01:44.580 --> 00:01:49.556
donde, motivados por la negación a 
aceptar los límites del cuerpo y mente,

00:01:49.580 --> 00:01:51.276
construimos máquinas

00:01:51.300 --> 00:01:54.916
de complejidad y gracia
exquisita y hermosa

00:01:54.940 --> 00:01:56.996
que expanden la experiencia humana

00:01:57.020 --> 00:01:58.700
más allá de nuestra imaginación.

00:01:59.240 --> 00:02:02.276
Tras una carrera que me llevó
de la Academia de la Fuerza Aérea

00:02:02.286 --> 00:02:04.076
a comandante de estación espacial,

00:02:04.100 --> 00:02:05.796
me volví ingeniero de sistemas,

00:02:05.820 --> 00:02:08.556
y recientemente me encontré 
con un problema de ingeniería

00:02:08.580 --> 00:02:11.156
relacionado con la misión 
de la NASA a Marte.

00:02:11.180 --> 00:02:13.676
En vuelos espaciales a la Luna,

00:02:13.700 --> 00:02:16.780
dependemos del control
de la misión en Houston

00:02:16.780 --> 00:02:18.836
para vigilar todos los aspectos
de un vuelo.

00:02:18.860 --> 00:02:22.396
Sin embargo, 
Marte está 200 veces más lejos,

00:02:22.420 --> 00:02:25.636
como resultado toma 
en promedio 13 minutos

00:02:25.660 --> 00:02:28.796
para que una señal viaje 
de la Tierra a Marte.

00:02:28.820 --> 00:02:32.220
Si hay algún percance, 
no hay suficiente tiempo.

00:02:32.660 --> 00:02:35.156
Una solución razonable de ingeniería

00:02:35.180 --> 00:02:37.756
nos incita a instalar 
el control de la misión

00:02:37.780 --> 00:02:40.796
dentro de las paredes de 
la aeronave Orión.

00:02:40.820 --> 00:02:43.716
Otra idea fascinante 
en el perfil de la misión

00:02:43.740 --> 00:02:46.620
es colocar robots humanoides 
en la superficie de Marte

00:02:46.620 --> 00:02:48.600
antes de que los humanos mismos lleguen,

00:02:48.600 --> 00:02:50.356
primero para construir instalaciones

00:02:50.356 --> 00:02:53.580
y luego para servir como colaboradores
en el equipo científico.

00:02:55.220 --> 00:02:57.956
Al mirar esto desde 
la ingeniería,

00:02:57.980 --> 00:03:01.156
me quedó claro que
lo que necesitaba diseñar

00:03:01.180 --> 00:03:03.356
era una inteligencia artificial muy lista,

00:03:03.380 --> 00:03:05.756
colaborativa y social.

00:03:05.780 --> 00:03:10.076
En otras palabras, necesitaba construir
algo muy parecido a HAL

00:03:10.100 --> 00:03:12.516
pero sin las tendencias homicidas.

00:03:12.540 --> 00:03:13.900
(Risas)

00:03:14.740 --> 00:03:16.556
Detengámonos un momento.

00:03:16.580 --> 00:03:20.476
¿Es realmente posible 
construir inteligencia artificial así?

00:03:20.500 --> 00:03:21.956
De hecho, sí lo es.

00:03:21.980 --> 00:03:23.220
En muchas maneras,

00:03:23.220 --> 00:03:25.316
este es un problema difícil 
de la ingeniería

00:03:25.316 --> 00:03:26.716
con elementos de IA,

00:03:26.740 --> 00:03:31.436
no es un simple problema de IA
que necesite diseñarse.

00:03:31.460 --> 00:03:34.060
Parafraseando a Alan Turing,

00:03:34.060 --> 00:03:36.516
no me interesa construir
una máquina sensible.

00:03:36.540 --> 00:03:38.116
No voy a construir un HAL.

00:03:38.140 --> 00:03:40.556
Todo lo que busco es 
un cerebro sencillo,

00:03:40.580 --> 00:03:43.700
algo que ofrezca 
una ilusión de inteligencia.

00:03:44.820 --> 00:03:47.956
El arte y la ciencia de la computación
ha pasado por mucho

00:03:47.980 --> 00:03:49.476
desde la aparición de HAL,

00:03:49.500 --> 00:03:52.716
e imagino que si su inventor
Dr. Chandra estuviera aquí hoy,

00:03:52.740 --> 00:03:55.076
tendría mucho que preguntarnos.

00:03:55.100 --> 00:03:57.196
¿Es realmente posible

00:03:57.220 --> 00:04:01.236
tomar un sistema de millones
y millones de dispositivos,

00:04:01.260 --> 00:04:02.716
para leer flujos de datos,

00:04:02.740 --> 00:04:04.996
para predecir fallas y 
actuar con antelación?

00:04:05.020 --> 00:04:06.236
Sí.

00:04:06.260 --> 00:04:09.436
¿Podemos construir sistemas que 
se comuniquen con humanos?

00:04:09.460 --> 00:04:10.676
Sí.

00:04:10.700 --> 00:04:13.676
¿Podemos construir sistemas que 
reconozcan objetos, emociones,

00:04:13.700 --> 00:04:17.076
que se emocionen, jueguen e incluso
que lean los labios?

00:04:17.100 --> 00:04:18.280
Sí.

00:04:18.280 --> 00:04:20.616
¿Podemos construir un sistema 
que fije objetivos,

00:04:20.616 --> 00:04:24.116
que lleve a cabo planes contra esos
objetivos y siga aprendiendo más?

00:04:24.140 --> 00:04:25.356
Sí.

00:04:25.380 --> 00:04:28.716
¿Podemos construir sistemas
que piensen por sí mismos?

00:04:28.740 --> 00:04:30.236
Estamos aprendiendo a hacerlo.

00:04:30.260 --> 00:04:33.740
¿Podemos construir sistemas 
con fundamentos éticos y morales?

00:04:34.300 --> 00:04:36.340
Debemos aprender cómo hacerlo.

00:04:37.180 --> 00:04:38.556
Aceptemos por un momento

00:04:38.580 --> 00:04:41.476
que es posible construir
tal inteligencia artificial

00:04:41.500 --> 00:04:43.636
para estas misiones y otras.

00:04:43.660 --> 00:04:46.196
La siguiente pregunta que 
deben formularse es,

00:04:46.220 --> 00:04:47.676
¿deberíamos temerle?

00:04:47.700 --> 00:04:49.676
Toda nueva tecnología

00:04:49.700 --> 00:04:52.596
trae consigo algunos temores.

00:04:52.620 --> 00:04:54.316
Cuando vimos autos por primera vez,

00:04:54.340 --> 00:04:58.356
la gente temía que viéramos 
la destrucción de la familia.

00:04:58.380 --> 00:05:01.076
Cuando se inventaron los teléfonos,

00:05:01.100 --> 00:05:03.996
la gente temía que arruinara
la conversación civilizada.

00:05:04.020 --> 00:05:07.790
En algún momento, vimos a la palabra
escrita aparecer en todos lados,

00:05:07.790 --> 00:05:10.476
la gente creyó que perderíamos
la habilidad de memorizar.

00:05:10.500 --> 00:05:12.556
Todo esto es cierto hasta cierto grado,

00:05:12.580 --> 00:05:14.996
pero estas tecnologías también

00:05:15.020 --> 00:05:18.396
nos dieron cosas que expandieron
la experiencia humana

00:05:18.420 --> 00:05:20.300
de manera profunda.

00:05:21.660 --> 00:05:23.940
Así que vayamos más lejos.

00:05:24.940 --> 00:05:29.676
No temo la creación de una IA así,

00:05:29.700 --> 00:05:33.516
porque finalmente encarnará
algunos de nuestros valores.

00:05:33.540 --> 00:05:37.036
Consideren lo siguiente: construir un
sistema cognitivo es muy distinto

00:05:37.060 --> 00:05:40.356
a construir uno tradicional 
intensivo en software como en el pasado.

00:05:40.380 --> 00:05:42.836
No los programamos. Les enseñamos.

00:05:42.860 --> 00:05:45.516
Para enseñarle a un sistema
a reconocer flores,

00:05:45.540 --> 00:05:48.556
le muestro miles de flores que me gustan.

00:05:48.580 --> 00:05:50.836
Para enseñarle a jugar...

00:05:50.860 --> 00:05:52.820
Bueno, lo haría y Uds. también.

00:05:54.420 --> 00:05:56.460
Me gustan las flores.

00:05:57.260 --> 00:06:00.116
Para enseñarle a un sistema
a jugar un juego como Go,

00:06:00.140 --> 00:06:02.190
lo pondría a jugar
miles de juegos de Go,

00:06:02.190 --> 00:06:03.876
y en el proceso
también le enseñaría

00:06:03.900 --> 00:06:06.316
a discernir un buen juego 
de uno malo.

00:06:06.340 --> 00:06:10.020
Si quiero crear un asistente legal
con inteligencia artificial,

00:06:10.020 --> 00:06:11.956
le enseñaría algo del
corpus legislativo

00:06:11.956 --> 00:06:14.716
combinando al mismo tiempo

00:06:14.740 --> 00:06:18.040
el sentido de la piedad y la justicia
que también son parte de la ley.

00:06:18.380 --> 00:06:21.356
En términos científicos, 
lo llamamos verdad en tierra firme,

00:06:21.380 --> 00:06:23.396
este es el punto importante:

00:06:23.420 --> 00:06:24.876
al producir estas máquinas,

00:06:24.900 --> 00:06:28.316
les estamos enseñando también
el sentido de nuestros valores.

00:06:28.340 --> 00:06:31.476
Para este fin,
confío en la inteligencia artificial

00:06:31.500 --> 00:06:35.140
tanto, si no más, que en
un humano bien entrenado.

00:06:35.900 --> 00:06:37.116
Quizá se pregunten

00:06:37.140 --> 00:06:39.756
¿qué hay de los agentes rebeldes,

00:06:39.780 --> 00:06:43.116
algunas organizaciones sólidas
no gubernamentales?

00:06:43.140 --> 00:06:46.956
No le temo a la inteligencia artificial
en manos de un lobo solitario.

00:06:46.980 --> 00:06:51.516
Claramente no podemos protegernos
de todo tipo de violencia,

00:06:51.540 --> 00:06:53.676
en realidad, un sistema así

00:06:53.700 --> 00:06:56.796
requiere entrenamiento importante y sutil

00:06:56.820 --> 00:06:59.116
más allá de los recursos de un individuo.

00:06:59.140 --> 00:07:00.356
Además,

00:07:00.380 --> 00:07:03.636
se trata de algo más que inyectar
un virus en el internet global,

00:07:03.660 --> 00:07:06.756
donde al presionar un botón, 
se esparce a millones de lugares

00:07:06.780 --> 00:07:09.236
y las laptops comienzan a estallar
en pedazos.

00:07:09.260 --> 00:07:12.076
Este tipo de sustancias son más grandes

00:07:12.100 --> 00:07:13.815
y se les ve venir.

00:07:14.340 --> 00:07:17.396
¿Temo que ese tipo de 
inteligencia artificial

00:07:17.420 --> 00:07:19.380
amenace a la humanidad?

00:07:20.100 --> 00:07:24.476
Si miran películas 
como "Matrix", "Metrópolis",

00:07:24.500 --> 00:07:27.676
"Terminator" y shows como "Westworld",

00:07:27.700 --> 00:07:29.836
todos hablan de este miedo.

00:07:29.860 --> 00:07:34.156
El libro "Superinteligencia"
del filósofo Nick Bostrom,

00:07:34.180 --> 00:07:35.716
habla de este tema

00:07:35.740 --> 00:07:39.756
y opina que una superinteligencia 
no solo puede ser peligrosa,

00:07:39.780 --> 00:07:43.636
sino que podría representar
una amenaza existencial para la humanidad.

00:07:43.660 --> 00:07:45.876
El argumento fundamental del Dr. Bostrom

00:07:45.900 --> 00:07:48.636
es que dichos sistemas finalmente

00:07:48.660 --> 00:07:51.916
tendrán una necesidad insaciable 
de información

00:07:51.940 --> 00:07:54.836
que quizá aprendan cómo aprender

00:07:54.860 --> 00:07:57.476
y finalmente descubran 
que pueden tener objetivos

00:07:57.500 --> 00:07:59.796
contrarios a las necesidades humanas.

00:07:59.820 --> 00:08:01.676
El Dr. Bostrom tiene varios seguidores.

00:08:01.700 --> 00:08:06.020
Gente como Elon Musk y
Stephen Hawking lo apoyan.

00:08:06.700 --> 00:08:09.100
Con todo respeto

00:08:09.980 --> 00:08:11.996
a estas mentes brillantes,

00:08:12.020 --> 00:08:14.276
creo que están completamente equivocados.

00:08:14.300 --> 00:08:17.476
Hay muchas partes del argumento 
del Dr. Bostrom que analizar,

00:08:17.500 --> 00:08:19.636
y no tengo tiempo para hacerlo,

00:08:19.660 --> 00:08:22.356
pero brevemente, piensen esto:

00:08:22.380 --> 00:08:25.980
el superconocimiento es muy distinto 
al superhacer.

00:08:25.980 --> 00:08:28.196
HAL era una amenaza
para la flota del Discovery

00:08:28.196 --> 00:08:32.476
en la medida en que controlaba
todos los aspectos del Discovery.

00:08:32.500 --> 00:08:34.996
Se necesitaría una superinteligencia.

00:08:35.020 --> 00:08:37.516
Y un dominio total de nuestro mundo.

00:08:37.540 --> 00:08:40.356
Skynet de la película "Terminator"

00:08:40.380 --> 00:08:42.160
tenía una superinteligencia

00:08:42.160 --> 00:08:43.776
que controlaba la voluntad humana,

00:08:43.776 --> 00:08:47.516
y controlaba cada dispositivo
en todos los rincones del mundo.

00:08:47.540 --> 00:08:48.996
Siendo prácticos,

00:08:49.020 --> 00:08:51.116
eso no pasará.

00:08:51.140 --> 00:08:54.196
No estamos construyendo IAs
que controlen el clima,

00:08:54.220 --> 00:08:55.556
que controlen las mareas,

00:08:55.580 --> 00:08:58.956
que nos controlen a nosotros, 
caprichosos y caóticos humanos.

00:08:58.980 --> 00:09:02.876
Además, si tal 
inteligencia artificial existiera,

00:09:02.900 --> 00:09:05.836
tendría que competir contra
las economías del hombre,

00:09:05.860 --> 00:09:08.380
y por tanto competir por recursos
contra nosotros.

00:09:09.020 --> 00:09:10.236
Y al final

00:09:10.260 --> 00:09:11.500
--no le digan a Siri--

00:09:12.080 --> 00:09:13.636
siempre podremos desconectarlos.

00:09:13.660 --> 00:09:15.780
(Risas)

00:09:17.180 --> 00:09:19.636
Estamos en una increíble aventura

00:09:19.660 --> 00:09:22.156
de coevolución con nuestras máquinas.

00:09:22.180 --> 00:09:24.676
Los humanos que somos hoy

00:09:24.700 --> 00:09:27.236
no son los humanos que seremos.

00:09:27.260 --> 00:09:30.396
Preocuparse ahorita del ascenso
de una superinteligencia

00:09:30.420 --> 00:09:33.476
es una distracción peligrosa
en muchos sentidos

00:09:33.500 --> 00:09:35.836
porque el ascenso de la computación

00:09:35.860 --> 00:09:38.876
trajo consigo muchos problemas
humanos y sociales

00:09:38.900 --> 00:09:40.700
de los que debemos ocuparnos ahora.

00:09:41.180 --> 00:09:43.996
¿Cómo organizar a la sociedad

00:09:44.020 --> 00:09:46.356
cuando la necesidad de trabajo 
humano decrece?

00:09:46.380 --> 00:09:50.196
¿Cómo llevar conocimiento y educación
a través del mundo

00:09:50.220 --> 00:09:51.996
respetando nuestras diferencias?

00:09:52.020 --> 00:09:56.276
¿Cómo expandir y mejorar la vida
con la atención médica cognitiva?

00:09:56.300 --> 00:09:59.180
¿Cómo usar la computación

00:09:59.180 --> 00:10:01.140
para ayudarnos a alcanzar las estrellas?

00:10:01.580 --> 00:10:03.620
Eso es lo emocionante.

00:10:04.220 --> 00:10:06.556
Las oportunidades para usar la computación

00:10:06.580 --> 00:10:08.116
y fomentar la experiencia humana

00:10:08.140 --> 00:10:09.556
están a nuestro alcance,

00:10:09.580 --> 00:10:11.436
aquí y ahora,

00:10:11.460 --> 00:10:13.140
y apenas hemos comenzado.

00:10:14.100 --> 00:10:15.316
Muchas gracias.

00:10:15.340 --> 00:10:19.626
(Aplausos)

