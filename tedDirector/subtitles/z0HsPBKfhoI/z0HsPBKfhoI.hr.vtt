WEBVTT
Kind: captions
Language: hr

00:00:00.000 --> 00:00:07.000
Prevoditelj: Stjepan Mateljan
Recezent: Tilen Pigac - EFZG

00:00:12.580 --> 00:00:16.420
Kad sam bio klinac,
bio sam onaj pravi štreber.

00:00:17.140 --> 00:00:19.316
MIslim kako su i neki od vas bili, 
također.

00:00:19.340 --> 00:00:20.556
(Smijeh)

00:00:20.580 --> 00:00:23.796
A vi, gospodine, koji ste se najglasnije 
nasmijali,vjerojatno to i dalje jeste.

00:00:23.820 --> 00:00:26.076
(Smijeh)

00:00:26.100 --> 00:00:29.596
Odrastao sam u malenom gradu
na prašnjavim ravnicama sjevernog Teksasa,

00:00:29.620 --> 00:00:32.956
sin šerifa 
koji je bio sin pastora.

00:00:32.980 --> 00:00:34.900
Upadanje u nevolje nije bila mogućnost.

00:00:35.860 --> 00:00:39.116
I tako sam počeo čitati
knjige iz (*infinitezimalnog) računa za zabavu.

00:00:39.140 --> 00:00:40.676
(Smijeh)

00:00:40.700 --> 00:00:42.396
I vi ste, također.

00:00:42.420 --> 00:00:46.156
To me dovelo do izgradnje lasera
i računala i modelarskih raketa,

00:00:46.180 --> 00:00:49.180
a to me dovelo do izrade
raketnog goriva u mojoj spavaćoj sobi.

00:00:49.780 --> 00:00:53.436
Sad, u znanstvenim terminima,

00:00:53.460 --> 00:00:56.716
to nazivamo vrlo lošom idejom.

00:00:56.740 --> 00:00:57.956
(Smijeh)

00:00:57.980 --> 00:01:00.156
Nekako oko tog doba,

00:01:00.180 --> 00:01:03.396
Kubrick-ova "2001: Odiseja u Svemiru"
je došla u kina,

00:01:03.420 --> 00:01:05.620
i moj se život zauvijek promijenio.

00:01:06.100 --> 00:01:08.156
Volio sam sve oko tog filma,

00:01:08.180 --> 00:01:10.716
posebno HAL 9000.

00:01:10.740 --> 00:01:12.796
Sad, HAL je bio svjesno računalo

00:01:12.820 --> 00:01:15.276
osmišljeno da vodi svemirsku 
letjelicu Discovery

00:01:15.300 --> 00:01:17.836
od Zemlje do Jupitera.

00:01:17.860 --> 00:01:19.916
HAL je također bio manjkav lik,

00:01:19.940 --> 00:01:24.220
jer je na kraju birao vrednovati
misiju više od ljudskog života.

00:01:24.660 --> 00:01:26.756
Sad, HAL je bio izmišljeni lik,

00:01:26.780 --> 00:01:29.436
ali ipak se obraćao našim strahovima,

00:01:29.460 --> 00:01:31.556
našim strahovima o bivanju podčinjenim

00:01:31.580 --> 00:01:34.596
od strane neke
bezosjećajne, umjetne inteligencije

00:01:34.620 --> 00:01:36.580
koja je ravnodušna spram naše ljudskosti.

00:01:37.700 --> 00:01:40.276
Vjerujem kako su takvi strahovi 
neutemeljeni.

00:01:40.300 --> 00:01:42.996
Zbilja, stojimo na upečatljivom mjestu

00:01:43.020 --> 00:01:44.556
u ljudskoj povijesti,

00:01:44.580 --> 00:01:49.556
gdje, potaknuti odbijanjem prihvaćanja
granica naših tijela i naših umova,

00:01:49.580 --> 00:01:51.276
gradimo strojeve,

00:01:51.300 --> 00:01:54.916
fine, prekrasne
složenosti i ljepote

00:01:54.940 --> 00:01:56.996
koji će proširiti ljudsko iskustvo

00:01:57.020 --> 00:01:58.700
na načine onkraj naše mašte.

00:01:59.540 --> 00:02:02.116
Nakon karijere koja me vodila
od Akademije zračnih snaga

00:02:02.140 --> 00:02:04.076
preko Svemirskog zapovjedništva do sada,

00:02:04.100 --> 00:02:05.796
postao sam sistemski inženjer,

00:02:05.820 --> 00:02:08.556
i nedavno sam uvučen
u inženjerski problem

00:02:08.580 --> 00:02:11.156
vezan uz NASA-inu misiju na Mars.

00:02:11.180 --> 00:02:13.676
Sad, u svemirskim letovima do Mjeseca,

00:02:13.700 --> 00:02:16.836
možemo se osloniti
na kontrolu misije u Houstonu

00:02:16.860 --> 00:02:18.836
kako će paziti na sve aspekte leta.

00:02:18.860 --> 00:02:22.396
Međutim, Mars je 200 puta udaljeniji,

00:02:22.420 --> 00:02:25.636
te kao rezultat treba
prosječno 13 minuta

00:02:25.660 --> 00:02:28.796
signalu da otputuje
od Zemlje do Marsa.

00:02:28.820 --> 00:02:32.220
Ako ima nevolje,
nema dovoljno vremena.

00:02:32.660 --> 00:02:35.156
I tako razumno inženjersko rješenje

00:02:35.180 --> 00:02:37.756
kaže nam neka stavimo kontrolu misije

00:02:37.780 --> 00:02:40.796
unutar zidova svemirske letjelice Orion.

00:02:40.820 --> 00:02:43.716
Druga očaravajuća ideja
u profilu misije

00:02:43.740 --> 00:02:46.636
stavlja humanoidne robote
na površinu Marsa

00:02:46.660 --> 00:02:48.516
prije nego sami ljudi doputuju,

00:02:48.540 --> 00:02:50.196
prvo kako bi sagradili postrojenja

00:02:50.220 --> 00:02:53.580
a kasnije kako bi služili kao
pomoćni članovi znanstvenog tima.

00:02:55.220 --> 00:02:57.956
Sad, kad sam gledao na ovo
iz inženjerskog kuta,

00:02:57.980 --> 00:03:01.156
postalo mi je vrlo jasno
kako je ono što trebam osmisliti

00:03:01.180 --> 00:03:03.356
bila pametna, uslužna,

00:03:03.380 --> 00:03:05.756
društveno inteligentna
umjetna inteligencija.

00:03:05.780 --> 00:03:10.076
Drugim riječima, trebao sam složiti
nešto vrlo slično HAL-u

00:03:10.100 --> 00:03:12.516
ali bez ubilačkih sklonosti.

00:03:12.540 --> 00:03:13.900
(Smijeh)

00:03:14.740 --> 00:03:16.556
Zaustavimo se na trenutak.

00:03:16.580 --> 00:03:20.476
Je li stvarno moguće sagraditi
umjetnu inteligenciju poput ove?

00:03:20.500 --> 00:03:21.956
Zapravo, jest.

00:03:21.980 --> 00:03:23.236
Na puno načina,

00:03:23.260 --> 00:03:25.236
ovo je tvrdi inženjerski problem

00:03:25.260 --> 00:03:26.716
s elementima UI,

00:03:26.740 --> 00:03:31.436
ne neki UI problem mokre lopte kose
kojeg treba osmisliti.

00:03:31.460 --> 00:03:34.116
Da parafraziram Alana Turinga:

00:03:34.140 --> 00:03:36.516
Ne zanima me
izgradnja svjesnog stroja.

00:03:36.540 --> 00:03:38.116
Ne gradim HAL-a.

00:03:38.140 --> 00:03:40.556
Sve za čime idem je
jednostavan mozak,

00:03:40.580 --> 00:03:43.700
nešto što nudi privid inteligencije.

00:03:44.820 --> 00:03:47.956
Umjetnost i znanost računarstva
je prošla dalek put

00:03:47.980 --> 00:03:49.476
otkad je HAL bio na ekranu,

00:03:49.500 --> 00:03:52.716
i mogu zamisliti kad bi njegov izumitelj
dr. Chandra bio ovdje danas,

00:03:52.740 --> 00:03:55.076
imao bi cijeli niz pitanja za nas.

00:03:55.100 --> 00:03:57.196
Je li nam zbilja moguće

00:03:57.220 --> 00:04:01.236
uzeti sustav od milijuna
i milijuna uređaja,

00:04:01.260 --> 00:04:02.716
čitati njihove tokove podataka,

00:04:02.740 --> 00:04:04.996
predviđati njihove kvarove
te djelovati unaprijed?

00:04:05.020 --> 00:04:06.236
Da.

00:04:06.260 --> 00:04:09.436
Možemo li graditi sustave koji
razgovaraju s ljudima prirodnim jezikom?

00:04:09.460 --> 00:04:10.676
Da.

00:04:10.700 --> 00:04:13.676
Možemo li graditi sustave
koji prepoznaju objekte, identificiraju osjećaje,

00:04:13.700 --> 00:04:17.076
sami izražavaju emocije,
igraju igre i čak čitaju s usana?

00:04:17.100 --> 00:04:18.316
Da.

00:04:18.340 --> 00:04:20.476
Možemo li sagraditi sustav
koji postavlja ciljeve,

00:04:20.500 --> 00:04:24.116
potom izvršava planove
prema tim ciljevima te usput uči?

00:04:24.140 --> 00:04:25.356
Da.

00:04:25.380 --> 00:04:28.716
Možemo li sagraditi sustave
koji imaju teoriju uma?

00:04:28.740 --> 00:04:30.236
To upravo učimo.

00:04:30.260 --> 00:04:33.740
Možemo li sagraditi sustave
koji imaju etičke i moralne temelje?

00:04:34.300 --> 00:04:36.340
Ovo moramo naučiti kako.

00:04:37.180 --> 00:04:38.556
Pa prihvatimo na trenutak

00:04:38.580 --> 00:04:41.476
kako je moguće sagraditi
takvu umjetnu inteligenciju

00:04:41.500 --> 00:04:43.636
za ovu vrstu misije i slično.

00:04:43.660 --> 00:04:46.196
Slijedeće pitanje
koje biste se trebali pitati je,

00:04:46.220 --> 00:04:47.676
bismo li je se trebali bojati?

00:04:47.700 --> 00:04:49.676
Sad, svaka nova tehnologija

00:04:49.700 --> 00:04:52.596
sa sobom donosi
neku mjeru strepnje.

00:04:52.620 --> 00:04:54.316
Kad smo prvo vidjeli aute,

00:04:54.340 --> 00:04:58.356
ljudi su se žalili kako ćemo vidjeti
uništenje obitelji.

00:04:58.380 --> 00:05:01.076
Kad smo prvo vidjeli telefone kako dolaze,

00:05:01.100 --> 00:05:03.996
ljudi su brinuli kako će uništiti
sav uljudan razgovor.

00:05:04.020 --> 00:05:07.956
U točci vremena kad smo vidjeli
kako pisana riječ sve više prožima,

00:05:07.980 --> 00:05:10.476
ljudi su mislili kako ćemo izgubiti
našu sposobnost pamćenja.

00:05:10.500 --> 00:05:12.556
Sve su te stvari točne do nekog stupnja,

00:05:12.580 --> 00:05:14.996
ali je također slučaj
kako su nam te tehnologije

00:05:15.020 --> 00:05:18.396
dovele stvari koje su
proširile ljudsko iskustvo

00:05:18.420 --> 00:05:20.300
na neke duboke načine.

00:05:21.660 --> 00:05:23.940
Pa hajdemo s ovim još malo dalje.

00:05:24.940 --> 00:05:29.676
Ne bojim se
stvaranje UI poput ove,

00:05:29.700 --> 00:05:33.516
jer će s vremenom
utjeloviti neke od naših vrijednosti.

00:05:33.540 --> 00:05:37.036
Uzmite u obzir ovo: izgradnja
spoznajnog sustava je iz temelja različita

00:05:37.060 --> 00:05:40.356
od izgradnje tradicionalnog softverski 
intenzivnog sustava u prošlosti.

00:05:40.380 --> 00:05:42.836
Ne programiramo ih. Učimo ih.

00:05:42.860 --> 00:05:45.516
Kako bismo sustav naučili
prepoznavati cvijeće,

00:05:45.540 --> 00:05:48.556
pokazujem mu tisuće vrsta cvjetova
koje mi se sviđaju.

00:05:48.580 --> 00:05:50.836
Kako bismo sustav naučili
igrati igru --

00:05:50.860 --> 00:05:52.820
Dobro, ja bih. I vi bi.

00:05:54.420 --> 00:05:56.460
Sviđa mi se cvijeće. Hajde.

00:05:57.260 --> 00:06:00.116
Naučiti sustav kako
igrati igru kakva je Go,

00:06:00.140 --> 00:06:02.196
pustit ću ga neka odigra 
tisuće partija Go-a,

00:06:02.220 --> 00:06:03.876
ali ću ga u postupku
također naučiti i

00:06:03.900 --> 00:06:06.316
kako razaznati
dobru partiju od loše partije.

00:06:06.340 --> 00:06:10.036
Ako želim stvoriti umjetno
inteligentnog pravnog savjetnika,

00:06:10.060 --> 00:06:11.836
naučit ću ga neki korpus zakona

00:06:11.860 --> 00:06:14.716
ali ću u isto vrijeme spojiti s njim

00:06:14.740 --> 00:06:17.620
osjećaj milosti i pravde
koji je dio tog zakona.

00:06:18.380 --> 00:06:21.356
Znanstvenim rječnikom, to je
ono što zovemo neposredna informacija,

00:06:21.380 --> 00:06:23.396
a evo važnog momenta:

00:06:23.420 --> 00:06:24.876
u stvaranju svih tih strojeva,

00:06:24.900 --> 00:06:28.316
mi ih dakle učimo
smislu naših vrijednosti.

00:06:28.340 --> 00:06:31.476
Stoga, vjerujem
umjetnoj inteligenciji

00:06:31.500 --> 00:06:35.140
isto, ako ne i više,
koliko i čovjeku koji je dobro obučen.

00:06:35.900 --> 00:06:37.116
Ali, možete pitati,

00:06:37.140 --> 00:06:39.756
što sa nevaljalim igračima,

00:06:39.780 --> 00:06:43.116
nekom dobro financiranom
nevladinom organizacijom?

00:06:43.140 --> 00:06:46.956
Ne bojim se umjetne inteligencije
u rukama vuka samotnjaka.

00:06:46.980 --> 00:06:51.516
Jasno, ne možemo se zaštititi
od svih nasumičnih činova nasilja,

00:06:51.540 --> 00:06:53.676
ali stvarnost je da takav sustav

00:06:53.700 --> 00:06:56.796
traži znatnu obuku
i finu obuku

00:06:56.820 --> 00:06:59.116
daleko onkraj resursa pojedinca.

00:06:59.140 --> 00:07:00.356
I nadalje,

00:07:00.380 --> 00:07:03.636
to je daleko više nego
samo ubaciti internetski virus u svijet,

00:07:03.660 --> 00:07:06.756
gdje pritisnete tipku,
odjednom je na milijun mjesta

00:07:06.780 --> 00:07:09.236
i laptopi posvuda
krenu eksplodirati.

00:07:09.260 --> 00:07:12.076
Sad, ove su vrste stvari
puno veće,

00:07:12.100 --> 00:07:13.815
i sigurno ćemo ih vidjeti kako dolaze.

00:07:14.340 --> 00:07:17.396
Bojim li se kako bi takva 
umjetna inteligencija

00:07:17.420 --> 00:07:19.380
mogla zaprijetiti cijelom čovječanstvu?

00:07:20.100 --> 00:07:24.476
Ako pogledate filmove
kao što su "Matrix" ili "Metropolis,"

00:07:24.500 --> 00:07:27.676
"Terminator,"
serije kao što je "Westworld,"

00:07:27.700 --> 00:07:29.836
svi oni pričaju o ovom tipu straha.

00:07:29.860 --> 00:07:34.156
Zbilja, u knjizi "Superinteligencija"
filozofa Nicka Bostroma,

00:07:34.180 --> 00:07:35.716
on polazi od ove teme

00:07:35.740 --> 00:07:39.756
te uočava kako superinteligencija
može biti ne samo opasna,

00:07:39.780 --> 00:07:43.636
mogla bi predstavljati egzistencijalnu 
opasnost svom čovječanstvu.

00:07:43.660 --> 00:07:45.876
Dr. Bostromov temeljni argument

00:07:45.900 --> 00:07:48.636
je kako će takvi sustavi s vremenom

00:07:48.660 --> 00:07:51.916
imati tako neutaživu žeđ
za informacijom

00:07:51.940 --> 00:07:54.836
da će možda naučiti kako učiti

00:07:54.860 --> 00:07:57.476
te s vremenom otkriti
kako mogu imati ciljeve

00:07:57.500 --> 00:07:59.796
koji su suprotni ljudskim potrebama.

00:07:59.820 --> 00:08:01.676
Dr. Bostrom ima neki broj sljedbenika.

00:08:01.700 --> 00:08:06.020
Podržavaju ga ljudi
poput Elona Muska i Stephena Hawkinga.

00:08:06.700 --> 00:08:09.100
Sa svim dužnim poštovanjem

00:08:09.980 --> 00:08:11.996
prema tim briljantnim umovima,

00:08:12.020 --> 00:08:14.276
vjerujem kako su oni
iz temelja u krivu.

00:08:14.300 --> 00:08:17.476
Sad, ima puno dijelova
dr. Bostromovog argumenta za maknuti,

00:08:17.500 --> 00:08:19.636
a nemam vremena kako bih ih maknuo sve,

00:08:19.660 --> 00:08:22.356
ali vrlo kratko, razmislite o ovome:

00:08:22.380 --> 00:08:26.116
super znanje je vrlo različito
od super djelovanja.

00:08:26.140 --> 00:08:28.036
HAL je bio prijetnja posadi Discoveryja

00:08:28.060 --> 00:08:32.476
samo stoga što je HAL
upravljao svime u Discoveryju.

00:08:32.500 --> 00:08:34.996
Tako bi trebalo biti
i sa superinteligencijom.

00:08:35.020 --> 00:08:37.516
Morala bi imati vlast
nad cijelim našim svijetom.

00:08:37.540 --> 00:08:40.356
Tako je sastavljen Skynet
iz filma "Terminator"

00:08:40.380 --> 00:08:42.236
u kojem smo imali superinteligenciju

00:08:42.260 --> 00:08:43.636
koja je upravljala ljudskom voljom,

00:08:43.660 --> 00:08:47.516
koja je vodila svaki uređaj
koji je bio u svakom kutku svijeta.

00:08:47.540 --> 00:08:48.996
Praktično govoreći,

00:08:49.020 --> 00:08:51.116
to se neće dogoditi.

00:08:51.140 --> 00:08:54.196
Ne gradimo UI-e
koji kontroliraju vrijeme,

00:08:54.220 --> 00:08:55.556
koji upravljaju plimom,

00:08:55.580 --> 00:08:58.956
koji upravljaju nama
jogunastima, kaotičnim ljudima.

00:08:58.980 --> 00:09:02.876
I nadalje, ako bi takva
umjetna superinteligencija postojala,

00:09:02.900 --> 00:09:05.836
morala bi se nadmetati
sa ljudskim ekonomijama,

00:09:05.860 --> 00:09:08.380
i time se nadmetati za resurse sa nama.

00:09:09.020 --> 00:09:10.236
I na kraju --

00:09:10.260 --> 00:09:11.500
nemojte reći Siri ovo --

00:09:12.260 --> 00:09:13.636
uvijek ih možemo ištekati.

00:09:13.660 --> 00:09:15.780
(Smijeh)

00:09:17.180 --> 00:09:19.636
Mi smo na nevjerojatnom putovanju

00:09:19.660 --> 00:09:22.156
koevolucije sa našim strojevima.

00:09:22.180 --> 00:09:24.676
Ljudi koji smo danas

00:09:24.700 --> 00:09:27.236
nisu ljudi koji ćemo biti tada.

00:09:27.260 --> 00:09:30.396
Brinuti sada o usponu
superinteligencije

00:09:30.420 --> 00:09:33.476
je na puno načina
opasno odvraćanje pažnje

00:09:33.500 --> 00:09:35.836
stoga što nam već uspon računarstva sam

00:09:35.860 --> 00:09:38.876
donosi brojna
ljudska i društvena pitanja

00:09:38.900 --> 00:09:40.540
kojima se moramo pozabaviti sada.

00:09:41.180 --> 00:09:43.996
Kako ću najbolje organizirati društvo

00:09:44.020 --> 00:09:46.356
kad se potreba za radom smanji?

00:09:46.380 --> 00:09:50.196
Kako mogu donijeti razumijevanje
i obrazovanje širom svijeta

00:09:50.220 --> 00:09:51.996
a i dalje uvažavati razlike?

00:09:52.020 --> 00:09:56.276
Kako mogu produljiti i poboljšati ljudski 
život kroz kognitivnu zdravstvenu skrb?

00:09:56.300 --> 00:09:59.156
Kako mogu koristiti računarstvo

00:09:59.180 --> 00:10:00.940
za pomoć kako bi došli do zvijezda?

00:10:01.580 --> 00:10:03.620
A to je uzbudljiva stvar.

00:10:04.220 --> 00:10:06.556
Prilike za korištenje računarstva

00:10:06.580 --> 00:10:08.116
kako bi unaprijedili ljudsko iskustvo

00:10:08.140 --> 00:10:09.556
su nam unutar dosega,

00:10:09.580 --> 00:10:11.436
sada i ovdje,

00:10:11.460 --> 00:10:13.140
a tek smo na početku.

00:10:14.100 --> 00:10:15.316
Hvala vam puno.

00:10:15.340 --> 00:10:19.626
(Pljesak)

