WEBVTT
Kind: captions
Language: vi

00:00:00.000 --> 00:00:07.000
Translator: Thuy Nguyen Thanh
Reviewer: Vy Chu

00:00:12.580 --> 00:00:16.420
Khi còn là đứa trẻ,
tôi là con mọt sách chính hiệu.

00:00:17.140 --> 00:00:19.316
Tôi nghĩ ở đây cũng có các bạn như vậy.

00:00:19.340 --> 00:00:20.556
(Cười)

00:00:20.580 --> 00:00:23.796
Và bạn, bạn cười to nhất đó,
chắc đúng bạn rồi.

00:00:23.820 --> 00:00:26.076
(Cười)

00:00:26.100 --> 00:00:29.596
Tôi lớn lên ở một thị trấn nhỏ
trong vùng đồng bằng bụi bặm bắc Texas,

00:00:29.620 --> 00:00:32.956
ba tôi là cảnh sát trưởng
và ông nội tôi là một mục sư.

00:00:32.980 --> 00:00:35.040
Gây ra rắc rối là không được phép.

00:00:35.860 --> 00:00:39.116
Và tôi bắt đầu đọc các sách số học
để cho vui.

00:00:39.140 --> 00:00:40.676
(Cười)

00:00:40.700 --> 00:00:42.396
Bạn cũng vậy chứ.

00:00:42.420 --> 00:00:46.156
Nó đã đưa tôi đến việc tạo ra thiết bị
laser, máy tính và các tên lửa mô hình,

00:00:46.180 --> 00:00:49.180
và đã dẫn dắt tôi tạo ra 
nhiên liệu tên lửa trong phòng ngủ.

00:00:49.780 --> 00:00:53.436
Theo thuật ngữ khoa học mà nói,

00:00:53.460 --> 00:00:56.716
chúng ta gọi nó là một ý tưởng sai lầm.

00:00:56.740 --> 00:00:57.956
(Cười)

00:00:57.980 --> 00:01:00.156
Vào cùng khoảng thời gian đó,

00:01:00.180 --> 00:01:03.396
Ra mắt bộ phim của Stanley Kubrick 
"2001: A Space Odyssey",

00:01:03.420 --> 00:01:05.620
và cuộc đời tôi đã thay đổi từ đó.

00:01:06.100 --> 00:01:08.156
Tôi yêu mọi thứ của bộ phim đó,

00:01:08.180 --> 00:01:10.636
đặc biệt là nhân vật HAL 9000.

00:01:10.740 --> 00:01:12.796
HAL là một máy tính có tri giác

00:01:12.820 --> 00:01:15.276
được thiết kế để hướng dẫn 
phi thuyền Discovery

00:01:15.300 --> 00:01:17.830
từ Trái Đất tới sao Mộc.

00:01:17.830 --> 00:01:19.936
HAL cũng là một nhân vật có khiếm khuyết,

00:01:19.940 --> 00:01:24.220
vì cuối cùng anh ấy đã lựa chọn đề cao
nhiệm vụ hơn sự sống con người.

00:01:24.660 --> 00:01:26.756
HAL là một nhân vật viễn tưởng

00:01:26.780 --> 00:01:29.400
nhưng anh ta vẫn nói lên
nỗi sợ hãi của chúng ta

00:01:29.400 --> 00:01:31.556
nỗi sợ hãi của việc bị thống trị

00:01:31.580 --> 00:01:34.570
bởi trí tuệ nhân tạo vô cảm

00:01:34.570 --> 00:01:36.580
không quan tâm đến loài người chúng ta.

00:01:37.700 --> 00:01:40.276
Tôi tin rằng những nỗi sợ hãi như vậy
là vô căn cứ.

00:01:40.300 --> 00:01:42.996
Thật vậy, chúng ta đang ở 
một thời điểm đáng chú ý

00:01:43.020 --> 00:01:44.556
trong lịch sử loài người,

00:01:44.580 --> 00:01:49.556
nó được thúc đẩy nhờ sự bất mãn với các
giới hạn của cơ thể và trí tuệ chúng ta,

00:01:49.580 --> 00:01:51.276
chúng ta đang tạo ra các cỗ máy

00:01:51.300 --> 00:01:54.916
tinh vi, phức tạp, xinh đẹp và duyên dáng

00:01:54.940 --> 00:01:56.980
thứ sẽ mở rộng trải nghiệm 
của con người

00:01:56.980 --> 00:01:58.700
theo những cách vượt xa tưởng tượng.

00:01:59.540 --> 00:02:02.116
Sau quá trinh công tác từ 
Học viện không quân

00:02:02.140 --> 00:02:04.076
đến công tác tại Không quân đến nay,

00:02:04.100 --> 00:02:05.796
tôi đã trở thành một kỹ sư hệ thống

00:02:05.820 --> 00:02:08.556
và gần đây tôi được tham gia vào
một vấn đề kỹ thuật

00:02:08.580 --> 00:02:11.156
hỗ trợ cho nhiệm vụ đến sao Hỏa của NASA.

00:02:11.180 --> 00:02:13.660
Trong các chuyến bay
đến Mặt trăng,

00:02:13.660 --> 00:02:16.810
chúng ta có thể dựa trên hệ thống
điều khiển nhiệm vụ ở Houston

00:02:16.810 --> 00:02:18.836
để quan sát mọi khía cạnh của chuyến bay.

00:02:18.860 --> 00:02:22.396
Tuy nhiên, sao Hỏa thì xa hơn đến 200 lần

00:02:22.400 --> 00:02:25.636
và như vậy trung bình phải cần 13 phút

00:02:25.660 --> 00:02:28.796
để một tín hiệu đi 
từ Trái đất đến sao Hỏa.

00:02:28.820 --> 00:02:32.220
Nếu có trục trặc, sẽ không đủ thời gian.

00:02:32.660 --> 00:02:35.156
Và do vậy một giải pháp kỹ thuật hợp lý

00:02:35.180 --> 00:02:37.756
đặt ra là cần phải đặt
thiết bị điều khiển nhiệm vụ

00:02:37.780 --> 00:02:40.770
ngay bên trong phi thuyền Orion

00:02:40.770 --> 00:02:43.716
Một ý tưởng hấp dẫn khác 
trong hồ sơ nhiệm vụ

00:02:43.720 --> 00:02:46.636
là đưa các robot giống người
lên bề mặt sao Hỏa

00:02:46.660 --> 00:02:48.516
trước khi con người đặt chân đến,

00:02:48.540 --> 00:02:50.196
đầu tiên để xây dựng cơ sở hạ tầng

00:02:50.220 --> 00:02:53.580
và sau đó để phục vụ các thành viên 
cộng tác của đội ngũ khoa học.

00:02:55.220 --> 00:02:57.950
Khi tôi xem xét nó dưới góc độ kỹ thuật,

00:02:57.950 --> 00:03:01.156
thì thấy rõ ràng cái mà chúng ta cần
tạo ra

00:03:01.180 --> 00:03:03.356
là một trí tuệ nhân tạo hiểu biết xã hội

00:03:03.380 --> 00:03:05.756
thông minh, dễ cộng tác.

00:03:05.780 --> 00:03:10.070
Nói cách khác, tôi cần tạo ra một thứ
rất giống với một HAL

00:03:10.070 --> 00:03:12.546
nhưng không có khuynh hướng giết người.

00:03:12.546 --> 00:03:13.900
(Cười)

00:03:14.740 --> 00:03:16.556
Chúng ta hãy dừng lại trong giây lát.

00:03:16.580 --> 00:03:20.476
Thực sự chúng ta có thể tạo ra 
một trí tuệ nhân tạo như vậy không?

00:03:20.500 --> 00:03:21.956
Thực ra là có thể.

00:03:21.980 --> 00:03:23.236
Theo nhiều cách,

00:03:23.260 --> 00:03:25.190
đây là một vấn đề khó khăn về kỹ thuật

00:03:25.190 --> 00:03:26.736
có liên quan đến AI,

00:03:26.736 --> 00:03:31.300
chứ không phải một vấn đề lộn xộn về AI
cần được giải quyết.

00:03:31.300 --> 00:03:33.960
Để diễn đạt lại theo ý của Alan Turing,

00:03:33.960 --> 00:03:36.460
tôi không hứng thú tạo ra
một cỗ máy có tri giác.

00:03:36.460 --> 00:03:38.116
Tôi không tạo ra một HAL.

00:03:38.140 --> 00:03:40.556
Cái tôi làm là tạo ra một bộ não đơn giản

00:03:40.560 --> 00:03:43.700
mà có thể đem đến ảo giác của trí tuệ.

00:03:44.820 --> 00:03:47.930
Nghệ thuật và khoa học điện toán đã
đi được một đoạn đường dài

00:03:47.930 --> 00:03:49.476
kể từ khi HAL trên sóng,

00:03:49.480 --> 00:03:52.716
và tôi tưởng tượng nếu cha đẻ của
anh ta, Dr. Chandra, ở đây hôm nay,

00:03:52.740 --> 00:03:55.076
ông ấy sẽ có một loạt câu hỏi 
cho chúng ta.

00:03:55.100 --> 00:03:57.196
Chúng ta có thể

00:03:57.220 --> 00:04:01.236
sử dụng một hệ thống gồm hàng triệu
triệu thiết bị,

00:04:01.260 --> 00:04:02.590
để nhập các luồng dữ liệu,

00:04:02.590 --> 00:04:05.036
để dự đoán các thất bại và
có hành động trước không?

00:04:05.036 --> 00:04:06.210
Vâng, có thể.

00:04:06.210 --> 00:04:09.460
Ta tạo ra được các hệ thống
giao tiếp bằng ngôn ngữ tự nhiên không?

00:04:09.460 --> 00:04:10.230
Được.

00:04:10.230 --> 00:04:13.670
Ta tạo ra được các hệ thống
nhận diện vật thể, xác định cảm xúc,

00:04:13.670 --> 00:04:17.000
tự biểu hiện cảm xúc, chơi trò chơi
và thậm chí đọc khẩu hình không?

00:04:17.000 --> 00:04:18.110
Được.

00:04:18.110 --> 00:04:20.450
Ta tạo ra được hệ thống
tự lập mục tiêu,

00:04:20.450 --> 00:04:24.116
xây dựng kế hoạch lệch với mục tiêu đó
và học hỏi từ quá trình trên không?

00:04:24.140 --> 00:04:25.056
Được

00:04:25.250 --> 00:04:28.586
Chúng ta có thể tạo ra các hệ thống
có khả năng nhận thức tư duy không?

00:04:28.650 --> 00:04:30.236
Cái này chúng ta đang học.

00:04:30.260 --> 00:04:33.740
Chúng ta có thể tạo ra các hệ thống
có nền tảng đạo đức và luân lý không?

00:04:34.300 --> 00:04:36.340
Cái này chúng ta phải học cách làm.

00:04:37.120 --> 00:04:38.546
Vậy chúng ta hãy tạm chấp nhận

00:04:38.556 --> 00:04:41.476
rằng có thể tạo ra một hệ thống
trí tuệ nhân tạo như vậy

00:04:41.500 --> 00:04:43.586
cho kiểu nhiệm vụ này và cả các loại khác.

00:04:43.610 --> 00:04:46.196
Câu hỏi tiếp theo mà bạn phải tự hỏi là,

00:04:46.220 --> 00:04:47.676
chúng ta có nên sợ nó?

00:04:47.680 --> 00:04:49.670
Thực chất, mọi công nghệ mới

00:04:49.670 --> 00:04:52.540
đều đem đến sự lo lắng ở mức độ nào đó.

00:04:52.540 --> 00:04:54.310
Khi chúng ta thấy xe hơi lần đầu,

00:04:54.310 --> 00:04:58.356
người ta đã than phiền rằng nó sẽ 
hủy hoại gia đình.

00:04:58.380 --> 00:05:01.076
Khi điện thoại lần đầu xuất hiện,

00:05:01.100 --> 00:05:03.990
người ta đã lo lắng nó sẽ hủy hoại
việc giao lưu trò chuyện.

00:05:03.990 --> 00:05:07.956
Đã có lúc khi chữ viết trở nên phổ biến,

00:05:07.980 --> 00:05:10.476
người ta đã nghĩ
chúng ta sẽ mất khả năng ghi nhớ.

00:05:10.500 --> 00:05:12.556
Tất cả lo sợ đó đều đúng ở mức độ nào đó,

00:05:12.560 --> 00:05:14.996
nhưng những công nghệ này cũng

00:05:15.020 --> 00:05:18.396
mang đến chúng ta những thứ
đã mở rộng trải nghiệm của con người

00:05:18.420 --> 00:05:20.300
một cách sâu sắc.

00:05:21.660 --> 00:05:23.940
Chúng ta hãy nói thêm về nó một chút.

00:05:24.900 --> 00:05:29.370
Tôi không sợ việc một AI như vậy
được tạo ra,

00:05:29.370 --> 00:05:33.406
bởi vì cuối cùng nó vẫn sẽ là hiện thân
một số giá trị của chúng ta.

00:05:33.410 --> 00:05:37.036
Hãy nghĩ thế này: xây dựng một hệ thống
có nhận thức về cơ bản là khác với

00:05:37.060 --> 00:05:40.356
xây dựng một hệ thống phần mềm chuyên sâu
truyền thống như trước đây.

00:05:40.380 --> 00:05:42.660
Ta không lập trình nó. Ta dạy nó.

00:05:42.680 --> 00:05:45.480
Để dạy một hệ thống nhận biết
các bông hoa,

00:05:45.480 --> 00:05:48.540
Tôi cho nó xem hàng nghìn bông hoa
thuộc những loại tôi thích.

00:05:48.540 --> 00:05:50.836
Để dạy một hệ thống cách chơi trò chơi--

00:05:50.860 --> 00:05:52.820
Vâng, tôi sẽ làm thế. Bạn cũng sẽ thôi.

00:05:54.420 --> 00:05:56.460
Tôi thích hoa mà. Nào mọi người.

00:05:57.260 --> 00:06:00.030
Để dạy một hệ thống chơi game
như Go chẳng hạn,

00:06:00.030 --> 00:06:02.196
Tôi sẽ cho nó chơi hàng nghìn ván Go,

00:06:02.220 --> 00:06:03.876
trong quá trình đó tôi cũng dạy nó

00:06:03.900 --> 00:06:06.316
cách phân biệt
ván chơi hay và ván chơi dở.

00:06:06.340 --> 00:06:10.036
Nếu tôi muốn tạo ra một trợ lý pháp lý
thông minh nhân tạo,

00:06:10.060 --> 00:06:11.820
Tôi sẽ dạy nó một số tập sách luật

00:06:11.820 --> 00:06:14.716
nhưng đồng thời tôi sẽ lồng ghép vào đó

00:06:14.740 --> 00:06:17.620
ý thức về lòng thương người và công lý
là một phần của luật.

00:06:18.380 --> 00:06:21.310
Theo thuật ngữ khoa học,
ta gọi nó là sự thật nền móng,

00:06:21.310 --> 00:06:23.396
và đây là điểm quan trọng:

00:06:23.420 --> 00:06:24.876
khi sản xuất các cỗ máy này,

00:06:24.900 --> 00:06:28.316
chúng ta đang dạy chúng ý thức
về các giá trị của chúng ta.

00:06:28.340 --> 00:06:31.476
Do vậy, tôi tin tưởng vào
một trí tuệ nhân tạo

00:06:31.500 --> 00:06:35.140
bằng với, nếu không muốn nói hơn cả,
một con người được giáo dục tốt.

00:06:35.900 --> 00:06:37.116
Nhưng bạn có thể sẽ hỏi,

00:06:37.140 --> 00:06:39.756
thế còn các tổ chức biến tướng thì sao,

00:06:39.780 --> 00:06:43.110
một tổ chức phi chính phủ
lắm tiền nào đấy chẳng hạn?

00:06:43.110 --> 00:06:46.956
Tôi không sợ một trí tuệ nhân tạo
trong tay một kẻ đơn độc.

00:06:46.980 --> 00:06:51.516
Rõ ràng, chúng ta không thể bảo vệ mình
trước những hành động bạo lực bất ngờ,

00:06:51.540 --> 00:06:53.676
nhưng trong thực tế một hệ thống như vậy

00:06:53.700 --> 00:06:56.796
yêu cầu sự đào tạo cơ bản
và đào tạo tinh vi

00:06:56.820 --> 00:06:59.116
vượt xa các nguồn lực của một cá nhân.

00:06:59.140 --> 00:07:00.356
Và hơn nữa,

00:07:00.380 --> 00:07:03.636
nó không giống như việc phát tán
virus qua mạng ra khắp thế giới,

00:07:03.660 --> 00:07:06.600
kiểu bạn chỉ ấn một phím,
nó đã lan ra một triệu vị trí

00:07:06.600 --> 00:07:09.180
và laptop bắt đầu nổ tanh bành
khắp mọi nơi.

00:07:09.180 --> 00:07:12.040
Những chuyện thế này lớn hơn rất nhiều,

00:07:12.040 --> 00:07:13.885
và chắc chắn ta
sẽ dự đoán trước được.

00:07:14.340 --> 00:07:17.396
Tôi có sợ rằng
một trí tuệ nhân tạo như vậy

00:07:17.420 --> 00:07:19.380
có thể đe dọa toàn nhân loại hay không?

00:07:20.100 --> 00:07:24.476
Nếu bạn nhìn vào các bộ phim
như "The Matrix," "Metropolis,"

00:07:24.500 --> 00:07:27.676
"The Terminator,"
các chương trình như "Westworld,"

00:07:27.700 --> 00:07:29.836
chúng đều nói về nỗi sợ hãi này.

00:07:29.860 --> 00:07:34.100
Thực vậy, trong cuốn "Siêu trí tuệ"
của triết gia Nick Bostrom,

00:07:34.100 --> 00:07:35.776
ông đề cập đến chủ đề này

00:07:35.776 --> 00:07:39.740
và nhận ra rằng một siêu trí tuệ
có thể không chỉ nguy hiểm,

00:07:39.740 --> 00:07:43.580
mà còn có thể đại diện cho một
mối đe dọa sống còn với toàn nhân loại.

00:07:43.580 --> 00:07:46.256
Lý lẽ chủ yếu của Dr. Bostrom

00:07:46.256 --> 00:07:48.636
là các hệ thống như vậy cuối cùng sẽ

00:07:48.660 --> 00:07:51.810
có một cơn khát thông tin vô độ;

00:07:51.810 --> 00:07:54.836
là chúng có lẽ sẽ học cách để học

00:07:54.860 --> 00:07:57.476
và cuối cùng khám phá ra rằng
chúng có các mục tiêu

00:07:57.500 --> 00:07:59.796
trái ngược với nhu cầu con người.

00:07:59.820 --> 00:08:01.676
Có một số người ủng hộ Dr. Bostrom.

00:08:01.700 --> 00:08:06.020
Ông ấy được ủng hộ bởi những người như
Elon Musk và Stephen Hawking.

00:08:06.700 --> 00:08:09.100
Dù rất kính trọng

00:08:09.980 --> 00:08:11.996
những bộ óc lỗi lạc này,

00:08:12.020 --> 00:08:14.276
tôi tin rằng họ về căn bản đều sai.

00:08:14.300 --> 00:08:17.476
Có rất nhiều lý lẽ của
Dr. Bostrom cần được mổ xẻ

00:08:17.500 --> 00:08:19.636
và tôi không có thời gian để mổ xẻ tất cả,

00:08:19.660 --> 00:08:22.356
nhưng rất ngắn gọn, hãy nghĩ thế này:

00:08:22.380 --> 00:08:26.116
biết nhiều khác với làm nhiều.

00:08:26.140 --> 00:08:28.036
HAL là một mối đe dọa với tàu Discovery

00:08:28.060 --> 00:08:32.476
chỉ trong điều kiện HAL được chỉ huy
toàn bộ tàu Discovery.

00:08:32.500 --> 00:08:34.996
Nghĩa là chúng ta xét về một siêu trí tuệ.

00:08:35.020 --> 00:08:37.516
Nó sẽ phải có quyền chi phối
toàn thế giới chúng ta.

00:08:37.540 --> 00:08:40.356
Đây là trường hợp của Skynet
trong phim "The Terminator"

00:08:40.380 --> 00:08:42.180
trong đó chúng ta có một siêu trí tuệ

00:08:42.180 --> 00:08:43.666
có thể chỉ huy ý chí con người,

00:08:43.666 --> 00:08:47.516
điều khiển mọi thiết bị
ở mọi ngõ ngách của thể giới.

00:08:47.540 --> 00:08:48.996
Thực tế mà nói,

00:08:49.020 --> 00:08:51.116
điều đó sẽ không xảy ra.

00:08:51.140 --> 00:08:54.160
Chúng ta sẽ không tạo ra các AI
kiểm soát được thời tiết,

00:08:54.160 --> 00:08:55.606
điều khiển được thủy triều,

00:08:55.606 --> 00:08:58.956
hay chỉ huy được loài người
lộn xộn thất thường chúng ta.

00:08:58.980 --> 00:09:02.876
Và hơn nữa,
nếu một trí tuệ nhân tạo như vậy tồn tại,

00:09:02.900 --> 00:09:05.836
nó sẽ phải cạnh tranh
với nền kinh tế nhân loại,

00:09:05.860 --> 00:09:08.380
và từ đó cạnh tranh với chúng ta
giành các nguồn lực.

00:09:09.020 --> 00:09:10.236
Và cuối cùng thì --

00:09:10.260 --> 00:09:11.500
đừng nói với Siri nhé --

00:09:12.260 --> 00:09:13.636
ta luôn có thể rút dây điện.

00:09:13.660 --> 00:09:15.780
(Cười)

00:09:17.180 --> 00:09:19.636
Chúng ta đang ở trên
một hành trình khó tin

00:09:19.660 --> 00:09:22.156
cùng tiến hóa với các cỗ máy của mình.

00:09:22.180 --> 00:09:24.676
Loài người chúng ta hôm nay

00:09:24.700 --> 00:09:27.236
sẽ không phải là
loài người chúng ta sau này.

00:09:27.260 --> 00:09:30.396
Lo lắng lúc này về sự phát triển
của một siêu trí tuệ

00:09:30.420 --> 00:09:33.476
theo nhiều cách
là một sự phân tâm nguy hiểm

00:09:33.500 --> 00:09:35.836
bởi vì sự phát triển của điện toán tự nó

00:09:35.860 --> 00:09:38.876
cũng đã đem đến nhiều vấn đề
về con người và xã hội

00:09:38.880 --> 00:09:40.560
mà chúng ta giờ đây phải chú ý tới.

00:09:41.180 --> 00:09:43.996
Chúng ta sẽ tổ chức xã hội
một cách tốt nhất như thế nào

00:09:44.020 --> 00:09:46.356
khi nhu cầu về 
sức lao động con người giảm đi?

00:09:46.380 --> 00:09:50.196
Làm thế nào tôi có thể đem sự hiểu biết
và giáo dục đi khắp thế giới

00:09:50.220 --> 00:09:51.996
và vẫn tôn trọng sự khác biệt?

00:09:52.020 --> 00:09:56.276
Làm sao tôi có thể mở rộng và nâng cao đời
sống qua chăm sóc sức khỏe có nhận thức?

00:09:56.300 --> 00:09:59.156
Tôi có thể sử dụng điện toán như thế nào

00:09:59.180 --> 00:10:00.940
để giúp đưa người lên các vì sao?

00:10:01.580 --> 00:10:03.620
Và đó là điều thú vị.

00:10:04.220 --> 00:10:06.556
Các cơ hội sử dụng điện toán

00:10:06.580 --> 00:10:08.116
để nâng cao trải nghiệm

00:10:08.140 --> 00:10:09.556
đang trong tầm tay chúng ta,

00:10:09.580 --> 00:10:11.436
ở đây và bây giờ,

00:10:11.460 --> 00:10:13.140
và chúng ta chỉ vừa bắt đầu.

00:10:14.100 --> 00:10:15.316
Xin cám ơn rất nhiều.

00:10:15.340 --> 00:10:19.626
(Vỗ tay)

