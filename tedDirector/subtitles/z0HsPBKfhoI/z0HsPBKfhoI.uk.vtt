WEBVTT
Kind: captions
Language: uk

00:00:00.000 --> 00:00:07.000
Перекладач: Olena Gapak
Утверджено: Hanna Leliv

00:00:12.580 --> 00:00:16.420
Коли я був малим,
я був еталонним "ботаніком".

00:00:17.140 --> 00:00:19.316
Думаю, деякі з вас 
теж такими були.

00:00:19.340 --> 00:00:20.556
(Сміх)

00:00:20.580 --> 00:00:23.796
А ви, пане, що сміявся найголосніше,
ним, мабуть, і залишились.

00:00:23.820 --> 00:00:26.076
(Сміх)

00:00:26.100 --> 00:00:29.596
Я ріс у маленькому містечку 
на запиленій рівнині північного Техасу,

00:00:29.620 --> 00:00:32.956
син шерифа, який був сином пастора.

00:00:32.980 --> 00:00:34.900
Вскочити в якусь халепу
було не варіант.

00:00:35.860 --> 00:00:39.116
І тому заради розваги
я почав читати книжки з обчислення.

00:00:39.140 --> 00:00:40.676
(Сміх)

00:00:40.700 --> 00:00:42.396
Ви також це робили?

00:00:42.420 --> 00:00:46.156
Це призвело до того, що я зробив лазер,
і комп'ютер, і модель ракети,

00:00:46.180 --> 00:00:49.180
і це призвело до того, що я зробив
ракетне паливо в спальні.

00:00:49.780 --> 00:00:53.436
Науковою мовою

00:00:53.460 --> 00:00:56.716
ми називаємо це 
дуже поганою ідеєю.

00:00:56.740 --> 00:00:57.956
(Сміх)

00:00:57.980 --> 00:01:00.156
Приблизно в той самий час

00:01:00.180 --> 00:01:03.396
"2001: Космічна Одісея" Стенлі Кубрика
з'явилась в кінотеатрах,

00:01:03.420 --> 00:01:05.620
і моє життя назавжди змінилось.

00:01:06.100 --> 00:01:08.156
Мені все подобалось у тому фільмі,

00:01:08.180 --> 00:01:10.716
особливо ХАЛ 9000.

00:01:10.740 --> 00:01:12.796
ХАЛ був кмітливим комп'ютером,

00:01:12.820 --> 00:01:15.346
призаченим направляти 
космічний корабель Дискавері

00:01:15.346 --> 00:01:17.836
з Землі на Юпітер.

00:01:17.860 --> 00:01:19.916
ХАЛ також був персонажем з недоліками,

00:01:19.940 --> 00:01:24.220
бо врешті вирішив, що місія
важливіша за людське життя.

00:01:24.660 --> 00:01:26.756
ХАЛ був вигаданим персонажем,

00:01:26.780 --> 00:01:29.436
проте він говорить з нашими страхами,

00:01:29.460 --> 00:01:31.556
зі страхом бути підкореним

00:01:31.580 --> 00:01:34.596
позбавленим почуттів штучним інтелектом,

00:01:34.620 --> 00:01:36.580
що є байдужим до людства.

00:01:37.700 --> 00:01:40.276
Я вважаю, що такі страхи безпідставні.

00:01:40.300 --> 00:01:42.996
Насправді ми живемо в неймовірний час

00:01:43.020 --> 00:01:44.556
людської історії,

00:01:44.580 --> 00:01:49.556
де, відмовляючись визнати 
обмеження свого тіла і розуму,

00:01:49.580 --> 00:01:51.276
ми будуємо машини

00:01:51.300 --> 00:01:54.916
фантастичної, немислимої 
складності та величі,

00:01:54.940 --> 00:01:56.996
які розширять досвід людини так,

00:01:57.020 --> 00:01:58.700
що зараз і не уявити.

00:01:59.300 --> 00:02:02.116
Після кар'єри, яка привела мене
з Академії повітряних сил

00:02:02.140 --> 00:02:04.076
до Космічного командування,

00:02:04.100 --> 00:02:05.846
я обійняв посаду
системного інженера

00:02:05.846 --> 00:02:08.556
і нещодавно мене залучили до 
інженерної проблеми,

00:02:08.580 --> 00:02:11.156
пов'язаною з місією на Марс НАСА.

00:02:11.180 --> 00:02:13.676
Під час польотів на Місяць

00:02:13.700 --> 00:02:16.820
ми можемо покладатись
на керування місією в Хьюстоні,

00:02:16.820 --> 00:02:19.026
що спостерігатиме
за всіма особливостями польоту.

00:02:19.026 --> 00:02:22.396
Однак Марс у 200 разів
віддаленіший,

00:02:22.420 --> 00:02:25.636
і тому потрібно в середньому
13 хвилин,

00:02:25.660 --> 00:02:28.796
щоб сигнал подолав відстань
від Землі до Марсу.

00:02:28.820 --> 00:02:32.220
Якщо виникне проблема, 
часу забракне.

00:02:32.660 --> 00:02:35.156
І тому розумним інженерним рішенням

00:02:35.180 --> 00:02:37.756
є розмістити контроль над місією

00:02:37.780 --> 00:02:40.796
всередині космічного корабля Оріон.

00:02:40.820 --> 00:02:43.716
Ще одна чудова ідея
в організації місії -

00:02:43.740 --> 00:02:46.636
помістити робота-гуманоїда
на поверхню Марсу

00:02:46.660 --> 00:02:48.516
перед тим, як туди прибуде людина, -

00:02:48.540 --> 00:02:50.196
спочатку 
побудувати споруди,

00:02:50.220 --> 00:02:53.580
а згодом служити одним із членів
наукової команди.

00:02:55.220 --> 00:02:57.956
Коли я подивився на це 
з точки зору інженерії,

00:02:57.980 --> 00:03:01.156
мені стало зрозуміло,
що мені потрібно створити

00:03:01.180 --> 00:03:03.356
розумний, здатний до співпраці,

00:03:03.380 --> 00:03:05.756
соціально розумний 
штучний інтелект.

00:03:05.780 --> 00:03:10.076
Іншими словами, мені потрібно побудувати
щось дуже схоже на ХАЛа,

00:03:10.100 --> 00:03:12.516
але без схильності до вбивств.

00:03:12.540 --> 00:03:13.900
(Сміх)

00:03:14.740 --> 00:03:16.556
Зупинімося на хвилину.

00:03:16.580 --> 00:03:20.476
Чи можливо ось так просто 
створити штучний інтелект?

00:03:20.500 --> 00:03:21.956
Власне, так.

00:03:21.980 --> 00:03:23.236
З багатьох міркувань

00:03:23.260 --> 00:03:25.236
це складне інженерне завдання

00:03:25.260 --> 00:03:26.796
стосовно штучного інтелекту,

00:03:26.796 --> 00:03:31.436
не якась скажена кулька,
що має бути спроектована.

00:03:31.460 --> 00:03:34.116
Якщо перефразувати Алана Тюрінґа,

00:03:34.140 --> 00:03:36.516
я не зацікавлений в будуванні
кмітливої машини.

00:03:36.540 --> 00:03:38.116
Я не будую ХАЛА.

00:03:38.140 --> 00:03:40.556
Все, що я хочу, - простий мозок,

00:03:40.580 --> 00:03:43.700
що може створювати
відчуття інтелекту.

00:03:44.820 --> 00:03:47.720
Мистецтво і наука програмування 
пройшли довгий шлях

00:03:47.720 --> 00:03:49.566
з того часу, як ХАЛ 
з'явився на екрані

00:03:49.566 --> 00:03:52.956
і я можу уявити, якщо б його винахідник
доктор Чандра був тут сьогодні,

00:03:52.956 --> 00:03:55.076
скільки він мав би до нас питань.

00:03:55.100 --> 00:03:57.196
Чи дійсно ми можемо

00:03:57.220 --> 00:04:01.236
змусити мільйони і мільйони 
приладів

00:04:01.260 --> 00:04:02.716
зчитувати потоки даних,

00:04:02.740 --> 00:04:05.126
передбачувати власні помилки 
і діяти заздалегідь?

00:04:05.126 --> 00:04:06.236
Так.

00:04:06.260 --> 00:04:09.436
Чи вміємо ми будувати системи, 
які спілкуються природною мовою?

00:04:09.460 --> 00:04:10.390
Так.

00:04:10.390 --> 00:04:13.676
Чи ми вміємо будувати системи,
які розрізняють об'єкти, емоції,

00:04:13.700 --> 00:04:17.076
відчувають емоції самі, 
грають в ігри чи навіть читають по губах?

00:04:17.100 --> 00:04:18.230
Так.

00:04:18.230 --> 00:04:20.996
Чи можемо збудувати систему, яка
ставить перед собою цілі,

00:04:20.996 --> 00:04:24.116
яка будує плани відносно цих цілей
і навчається одночасно?

00:04:24.140 --> 00:04:25.356
Так.

00:04:25.380 --> 00:04:28.716
Чи вміємо будувати системи, 
що моделюють психіку людини?

00:04:28.740 --> 00:04:30.236
Ми вчимося це робити.

00:04:30.260 --> 00:04:33.740
Чи вміємо будувати системи, 
які мають етичні та моральні засади?

00:04:34.300 --> 00:04:36.340
Ми маємо цього навчитись.

00:04:37.180 --> 00:04:38.556
Тож припустімо на мить,

00:04:38.580 --> 00:04:41.476
що можна побудувати
штучний інтелект

00:04:41.500 --> 00:04:43.636
для цього типу місії та подібних.

00:04:43.660 --> 00:04:46.196
Наступне питання,
яке ви маєте поставити:

00:04:46.220 --> 00:04:47.676
чи слід його боятися?

00:04:47.700 --> 00:04:49.676
Кожна нова технологія

00:04:49.700 --> 00:04:52.596
тягне за собою
певне занепокоєння.

00:04:52.620 --> 00:04:54.316
Коли ми вперше побачили авто,

00:04:54.340 --> 00:04:58.356
люди галасували, що з цього почнеться
руйнування сім'ї.

00:04:58.380 --> 00:05:01.076
Коли ми вперше побачили телефони,

00:05:01.100 --> 00:05:04.186
люди були стурбовані, що вони зруйнують
всі повсякденні розмови.

00:05:04.186 --> 00:05:07.956
Коли ми побачили, що
письмове слово стало поширеним,

00:05:07.980 --> 00:05:10.476
люди думали, що ми втратимо 
вміння запам'ятовувати.

00:05:10.500 --> 00:05:12.556
Усі ці речі до певної міри правдиві,

00:05:12.580 --> 00:05:14.996
але водночас ці технології

00:05:15.020 --> 00:05:18.396
принесли нам винаходи, 
які якісно поглибили людський досвід -

00:05:18.420 --> 00:05:20.300
до нового рівня.

00:05:21.660 --> 00:05:23.940
Тому міркуймо далі.

00:05:24.940 --> 00:05:29.676
Я не боюсь створення 
подібного штучного інтелекту,

00:05:29.700 --> 00:05:33.516
оскільки, зрештою,
він вбере в себе деякі наші цінності.

00:05:33.540 --> 00:05:37.036
Задумайтесь: побудова когнітивної 
системи фундаментально відрізняється

00:05:37.060 --> 00:05:40.356
від побудови традиційного
програмного забезпечення минулого.

00:05:40.380 --> 00:05:42.836
Ми не програмуємо їх. 
Ми їх вчимо.

00:05:42.860 --> 00:05:45.516
Щоб навчити систему 
розпізнавати квіти,

00:05:45.540 --> 00:05:48.556
я показую їй тисячу квітів,
які мені подобаються.

00:05:48.580 --> 00:05:50.836
Щоб навчити систему
грати у гру --

00:05:50.860 --> 00:05:52.820
Ну, я б показував. Ви б теж.

00:05:54.420 --> 00:05:56.460
Я люблю квіти. Ну справді.

00:05:57.260 --> 00:06:00.116
Щоб навчити систему, як
грати у гру - наприклад, го -

00:06:00.140 --> 00:06:02.196
мені б довелося зіграти 
тисячі партій в го,

00:06:02.220 --> 00:06:03.876
але в процесі я також навчаю її,

00:06:03.900 --> 00:06:06.316
як відрізнити 
хорошу партію від поганої.

00:06:06.340 --> 00:06:10.036
Якщо я хочу створити штучний інтелект,
який допомагає в правових питаннях,

00:06:10.060 --> 00:06:11.836
мені потрібно навчити його законам,

00:06:11.860 --> 00:06:14.716
але водночас я вливаю туди

00:06:14.740 --> 00:06:17.620
почуття милосердя і справедливості,
що є частиною закону.

00:06:18.380 --> 00:06:21.356
Науковці називають це
фундаментальною правдою,

00:06:21.380 --> 00:06:23.396
і тут є важливий момент:

00:06:23.420 --> 00:06:24.876
створюючи ці машини,

00:06:24.900 --> 00:06:28.316
ми водночас вчимо їх
нашим цінностям.

00:06:28.340 --> 00:06:31.476
І через це я довіряю
штучному інтелекту

00:06:31.500 --> 00:06:35.140
так само, якщо не більше,
ніж добре навченій людині.

00:06:35.900 --> 00:06:37.116
Але ви можете запитати,

00:06:37.140 --> 00:06:39.756
а що ж щодо шахраїв,

00:06:39.780 --> 00:06:43.116
якихось добре фінансованих
неурядових організацій?

00:06:43.140 --> 00:06:46.956
Я не боюсь штучного інтелекту 
в руках самотнього вовка.

00:06:46.980 --> 00:06:51.516
Зрозуміло, ми не можемо захистити себе 
від всіх актів насильства,

00:06:51.540 --> 00:06:53.676
але реальність є такою системою,

00:06:53.700 --> 00:06:56.796
яка вимагає суттєвої підготовки
і тонкого навчання,

00:06:56.820 --> 00:06:59.116
для яких окремій людині
забракне ресурсів.

00:06:59.140 --> 00:07:00.356
І більше того,

00:07:00.380 --> 00:07:03.636
це набагато складніше, ніж просто 
пустити інтернет-вірус у світ,

00:07:03.660 --> 00:07:06.756
де ти натискаєш кнопку - 
і ось він у мільйоні місць,

00:07:06.780 --> 00:07:09.236
і ноутбуки починають вибухати усюди.

00:07:09.260 --> 00:07:12.076
Але такі події
є значно масштабнішими,

00:07:12.100 --> 00:07:14.015
і ми обов'язково зможемо їх помітити.

00:07:14.340 --> 00:07:17.396
Чи я боюсь, що
такий штучний інтелект

00:07:17.420 --> 00:07:19.380
може загрожувати всьому людству?

00:07:20.100 --> 00:07:24.476
Якщо ви подивитесь такі фільми, 
як "Матриця", "Метрополіс",

00:07:24.500 --> 00:07:27.676
"Термінатор", серіал 
"Західний світ",

00:07:27.700 --> 00:07:29.836
вони всі розповідають про такий страх.

00:07:29.860 --> 00:07:33.986
Насправді, у своїй книжці "Суперрозум"
філософ Нік Бостром

00:07:34.036 --> 00:07:35.716
порушує цю тему

00:07:35.740 --> 00:07:39.756
і стверджує, що суперрозум може
не тільки бути небезпечним,

00:07:39.780 --> 00:07:43.636
він може становити екзистенційну 
загрозу людству.

00:07:43.660 --> 00:07:45.876
Головним аргументом доктора Бострома

00:07:45.900 --> 00:07:48.636
є те, що така система одного разу

00:07:48.660 --> 00:07:51.916
матиме таку ненаситну спрагу
до інформації,

00:07:51.940 --> 00:07:54.836
що вона, можливо, навчиться вчитися -

00:07:54.860 --> 00:07:57.476
і врешті зрозуміє, 
що вона може ставити цілі,

00:07:57.500 --> 00:07:59.796
які суперечать потребам людства.

00:07:59.820 --> 00:08:01.676
У доктора Бострома 
є багато прихильників.

00:08:01.700 --> 00:08:06.020
Його підтримують такі люди, 
як Ілон Маск і Стівен Гокінґ.

00:08:06.700 --> 00:08:09.100
З усією повагою

00:08:09.980 --> 00:08:11.996
до цих блискучих розумів,

00:08:12.020 --> 00:08:14.140
я думаю, що вони 
серйозно помиляються.

00:08:14.140 --> 00:08:17.476
Є багато аргументів доктора Бострома, 
що варто обговорити,

00:08:17.500 --> 00:08:19.636
і в мене немає часу розглядати їх усі,

00:08:19.660 --> 00:08:22.356
але дуже стисло:

00:08:22.380 --> 00:08:26.116
неймовірне знання дуже відрізняється 
від неймовірних дій.

00:08:26.140 --> 00:08:28.036
ХАЛ був загрозою для команди Дискавері,

00:08:28.060 --> 00:08:32.476
оскільки ХАЛ керував усіма 
аспектами Дискавері.

00:08:32.500 --> 00:08:34.996
Тому йому потрібно було мати
суперрозум.

00:08:35.020 --> 00:08:37.516
Йому довелось би панувати в усьому світі.

00:08:37.540 --> 00:08:40.356
Це випадок Скайнет
з фільму "Термінатор",

00:08:40.380 --> 00:08:42.236
в якому був суперрозум,

00:08:42.260 --> 00:08:43.756
який контролював людську волю,

00:08:43.756 --> 00:08:47.516
направляв кожен пристрій
в будь-якому куточку світу.

00:08:47.540 --> 00:08:48.996
З практичної точки зору,

00:08:49.020 --> 00:08:51.116
цього ніколи не станеться.

00:08:51.140 --> 00:08:54.196
Ми не будуємо штучних інтелектів,
які контролюють погоду,

00:08:54.220 --> 00:08:55.556
які направляють хвилі,

00:08:55.580 --> 00:08:58.956
які командують нами -
примхливими, хаотичними людьми.

00:08:58.980 --> 00:09:02.876
І більше того, якщо б такий 
штучний інтелект справді існував,

00:09:02.900 --> 00:09:05.836
йому б доводилось конкурувати 
з людською економікою,

00:09:05.860 --> 00:09:08.380
а значить, конкурувати з нами за ресурси.

00:09:09.020 --> 00:09:10.236
І зрештою -

00:09:10.260 --> 00:09:11.500
не кажіть Сірі цього -

00:09:11.750 --> 00:09:13.636
ми завжди можемо їх вимкнути.

00:09:13.660 --> 00:09:15.780
(Сміх)

00:09:17.180 --> 00:09:19.636
Ми в неймовірній подорожі

00:09:19.660 --> 00:09:22.156
співеволюції з нашими машинами.

00:09:22.180 --> 00:09:24.676
Люди, якими ми є сьогодні,

00:09:24.700 --> 00:09:27.236
відрізняються від людей, 
якими ми станемо завтра.

00:09:27.260 --> 00:09:30.396
Хвилюватися тепер 
про розвиток суперрозуму -

00:09:30.420 --> 00:09:33.476
досить небезпечне відволікання,

00:09:33.500 --> 00:09:35.836
бо розвиток комп'ютерів як такий

00:09:35.860 --> 00:09:38.876
порушує багато
гуманітарних і суспільних проблем,

00:09:38.900 --> 00:09:40.540
на які нам треба звернути увагу.

00:09:41.180 --> 00:09:43.790
Як краще організувати суспільство,

00:09:43.790 --> 00:09:46.356
коли потреба в людській праці
зменшується?

00:09:46.380 --> 00:09:50.196
Як донести порозуміння й освіту 
до всіх куточків планети

00:09:50.220 --> 00:09:52.046
і поважати при цьому наші відмінності?

00:09:52.046 --> 00:09:56.276
Як поліпшити і продовжити людське життя
за допомогою охорони когнітивного здоров'я?

00:09:56.300 --> 00:09:59.156
Як використати комп'ютери,

00:09:59.180 --> 00:10:00.940
щоб привели нас до зірок?

00:10:01.580 --> 00:10:03.620
Ось що цікаво.

00:10:04.220 --> 00:10:06.556
Можливості використати обчислювання

00:10:06.580 --> 00:10:08.116
для поліпшення людського досвіду

00:10:08.140 --> 00:10:09.556
доступні нам

00:10:09.580 --> 00:10:11.436
тут і тепер,

00:10:11.460 --> 00:10:13.140
і це тільки початок.

00:10:14.100 --> 00:10:15.316
Дуже дякую.

00:10:15.340 --> 00:10:19.626
(Оплески)

