WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:07.000
翻译人员: Weidi Liu
校对人员: Jiawei Ni

00:00:12.570 --> 00:00:16.480
在我还是孩子的时候 ，
我是一个典型的书呆子。

00:00:17.260 --> 00:00:19.366
我猜你们的一部分人和我一样。

00:00:19.366 --> 00:00:20.596
（笑声）

00:00:20.596 --> 00:00:23.846
还有你，那位先生，笑得最大声，
说不定你现在还是呢。

00:00:23.846 --> 00:00:26.126
（笑声）

00:00:26.126 --> 00:00:29.646
我成长在德克萨斯州北部
荒凉平原上的一个小镇。

00:00:29.646 --> 00:00:32.986
我的爸爸是警长，爷爷是一位牧师，

00:00:32.986 --> 00:00:35.620
所以自找麻烦从来不是一个好的选项。

00:00:35.860 --> 00:00:39.156
所以我开始看有关
微积分的书来打发时间。

00:00:39.156 --> 00:00:40.736
（笑声）

00:00:40.736 --> 00:00:42.446
你也是这样。

00:00:42.446 --> 00:00:46.206
于是我学着制作了一个激光器，
一台电脑，和一个火箭模型，

00:00:46.206 --> 00:00:49.180
并且在自己的卧室制取火箭燃料。

00:00:49.780 --> 00:00:53.476
现在，从科学的角度而言，

00:00:53.476 --> 00:00:56.756
我们把这个叫做
一个糟糕透顶的主意。

00:00:56.756 --> 00:00:57.996
（笑声）

00:00:57.996 --> 00:01:00.196
在差不多同一时间，

00:01:00.196 --> 00:01:03.436
Stanley Kubrick的
“2001：太空漫游”上映了，

00:01:03.436 --> 00:01:06.180
我的生活从此改变。

00:01:06.180 --> 00:01:08.186
我喜欢关于那部电影的一切，

00:01:08.186 --> 00:01:10.756
特别是 HAL 9000。

00:01:10.756 --> 00:01:12.836
HAL是一台有情感的电脑，

00:01:12.836 --> 00:01:15.316
为引导“发现一号”飞船从地球

00:01:15.316 --> 00:01:17.886
前往木星而设计出来。

00:01:17.886 --> 00:01:19.956
HAL 也是一个有缺陷的角色，

00:01:19.956 --> 00:01:24.840
因为在结尾他将任务的
价值置于生命之上。

00:01:24.840 --> 00:01:26.816
HAL 是一个虚构的角色，

00:01:26.816 --> 00:01:29.476
但尽管如此，他挑战了我们的恐惧，

00:01:29.476 --> 00:01:31.606
被一些冷漠无情的

00:01:31.606 --> 00:01:34.676
人工智能（AI）

00:01:34.676 --> 00:01:36.580
所统治的恐惧。

00:01:37.700 --> 00:01:40.270
但我相信这些恐惧只是杞人忧天。

00:01:40.270 --> 00:01:42.900
的确，我们正处于人类历史上

00:01:42.900 --> 00:01:44.540
一个值得铭记的时间点，

00:01:44.540 --> 00:01:49.540
不甘被自身肉体和头脑所局限，

00:01:49.540 --> 00:01:51.260
我们正在制造

00:01:51.260 --> 00:01:54.966
那些可以通过我们无法想象的方式

00:01:54.966 --> 00:01:57.046
来拓展人类体验的机器，

00:01:57.046 --> 00:01:58.700
它们精美，复杂，而且优雅。

00:01:59.540 --> 00:02:02.156
我在空军学院

00:02:02.156 --> 00:02:04.106
和航天司令部工作过，

00:02:04.106 --> 00:02:05.836
现在是一个系统工程师。

00:02:05.836 --> 00:02:08.616
最近我碰到了一个和
NASA火星任务相关的

00:02:08.616 --> 00:02:11.206
工程问题。

00:02:11.206 --> 00:02:13.766
当前，前往月球的宇宙飞行，

00:02:13.766 --> 00:02:16.866
我们可以依靠休斯顿的指挥中心

00:02:16.866 --> 00:02:18.856
来密切关注飞行的各个方面。

00:02:18.866 --> 00:02:22.476
但是，火星相比而言
多出了200倍的距离，

00:02:22.476 --> 00:02:25.726
这使得一个信号从地球到火星

00:02:25.726 --> 00:02:28.836
平均要花费13分钟。

00:02:28.836 --> 00:02:32.220
如果有了麻烦，
我们并没有足够的时间来解决。

00:02:32.660 --> 00:02:35.196
所以一个可靠的工程解决方案

00:02:35.196 --> 00:02:37.776
促使我们把一个指挥中枢

00:02:37.780 --> 00:02:40.866
放在“猎户星”飞船之中。

00:02:40.866 --> 00:02:43.756
在任务档案中的另一个创意

00:02:43.756 --> 00:02:46.666
是在人类抵达火星之前，把人形机器人

00:02:46.666 --> 00:02:48.606
先一步放在火星表面，

00:02:48.606 --> 00:02:50.190
它们可以先建造据点，

00:02:50.190 --> 00:02:53.580
然后作为科学团队的合作伙伴驻扎。

00:02:55.220 --> 00:02:58.006
当我从工程师的角度看待这个想法，

00:02:58.006 --> 00:03:01.176
对于建造一个聪明，懂得合作，

00:03:01.180 --> 00:03:03.386
擅长社交的人工智能的需求

00:03:03.386 --> 00:03:05.756
是十分明显的。

00:03:05.760 --> 00:03:10.116
换句话说，我需要建造一个和HAL一样，

00:03:10.116 --> 00:03:12.566
但是没有谋杀倾向的机器。

00:03:12.566 --> 00:03:13.900
（笑声）

00:03:14.740 --> 00:03:16.586
待会儿再回到这个话题。

00:03:16.586 --> 00:03:20.506
真的有可能打造一个
类似的人工智能吗？

00:03:20.506 --> 00:03:21.956
是的，当然可以。

00:03:21.960 --> 00:03:23.236
在许多方面，

00:03:23.240 --> 00:03:25.220
一个困难的工程问题来自于

00:03:25.220 --> 00:03:26.700
AI的各个零件，

00:03:26.700 --> 00:03:31.410
而不是什么琐碎的AI问题。

00:03:31.410 --> 00:03:34.090
借用Alan Turing的话来说，

00:03:34.090 --> 00:03:36.510
我没有兴趣建造一台有情感的机器。

00:03:36.510 --> 00:03:38.116
我不是要建造HAL。

00:03:38.120 --> 00:03:40.530
我只想要一个简单的大脑，

00:03:40.530 --> 00:03:43.700
一个能让你以为它有智能的东西。

00:03:44.820 --> 00:03:47.930
自从HAL登上荧幕，
关于编程的技术和艺术

00:03:47.930 --> 00:03:49.440
已经发展了许多，

00:03:49.440 --> 00:03:52.690
我能想象如果它的发明者
Chandra博士今天在这里的话，

00:03:52.690 --> 00:03:55.070
他会有很多的问题问我们。

00:03:55.070 --> 00:03:57.180
我们真的可能

00:03:57.180 --> 00:04:01.230
用一个连接了无数设备的系统，

00:04:01.230 --> 00:04:02.690
通过读取数据流，

00:04:02.690 --> 00:04:04.960
来预测它们的失败并提前行动吗？

00:04:04.960 --> 00:04:06.170
是的。

00:04:06.170 --> 00:04:09.380
我们能建造一个和人类
用语言交流的系统吗？

00:04:09.380 --> 00:04:10.300
能。

00:04:10.300 --> 00:04:13.670
我们能够打造一个能
辨识目标，鉴别情绪，

00:04:13.670 --> 00:04:17.050
表现自身情感，打游戏，
甚至读唇的系统吗？

00:04:17.050 --> 00:04:18.150
可以。

00:04:18.150 --> 00:04:20.420
我们可以建造一个能设定目标，

00:04:20.420 --> 00:04:24.050
通过各种达成目的的
方法来学习的系统吗？

00:04:24.050 --> 00:04:25.240
也可以。

00:04:25.240 --> 00:04:28.610
我们可以建造一个类似人脑的系统吗？

00:04:28.610 --> 00:04:30.150
这是我们正在努力做的。

00:04:30.150 --> 00:04:33.740
我们可以建造一个有道德
和感情基础的系统吗？

00:04:34.300 --> 00:04:36.340
这是我们必须要学习的。

00:04:37.180 --> 00:04:38.520
总而言之，

00:04:38.520 --> 00:04:41.440
建造一个类似的用于这类任务的

00:04:41.440 --> 00:04:43.600
人工智能是可行的。

00:04:43.600 --> 00:04:46.150
另一个你必须扪心自问的问题是，

00:04:46.150 --> 00:04:47.650
我们应该害怕它吗？

00:04:47.650 --> 00:04:49.650
毕竟，每一种新技术

00:04:49.650 --> 00:04:52.550
都给我们带来某种程度的不安。

00:04:52.550 --> 00:04:54.290
我们第一次看见汽车的时候，

00:04:54.290 --> 00:04:58.330
人们悲叹我们会看到家庭的毁灭。

00:04:58.330 --> 00:05:01.070
我们第一次看见电话的时候，

00:05:01.070 --> 00:05:03.970
人们担忧这会破坏所有的文明交流。

00:05:03.970 --> 00:05:07.930
在某个时间点我们看到书写文字的蔓延，

00:05:07.930 --> 00:05:10.470
人们认为我们会丧失记忆的能力。

00:05:10.470 --> 00:05:12.530
这些在某个程度上是合理的，

00:05:12.530 --> 00:05:14.980
但也正是这些技术

00:05:14.980 --> 00:05:18.360
给人类的生活在某些方面

00:05:18.360 --> 00:05:20.300
带来了前所未有的体验。

00:05:21.660 --> 00:05:23.940
我们再稍稍扩展一下。

00:05:24.940 --> 00:05:29.660
我并不畏惧这类人工智能的诞生，

00:05:29.660 --> 00:05:33.516
因为它最终会融入我们的部分价值观。

00:05:33.520 --> 00:05:37.030
想想这个：建造一个认知系统

00:05:37.030 --> 00:05:40.396
与建造一个以传统的软件为主的
系统有着本质的不同。

00:05:40.396 --> 00:05:42.830
我们不编译它们。我们教导它们。

00:05:42.830 --> 00:05:45.490
为了教会系统如何识别花朵，

00:05:45.490 --> 00:05:48.550
我给它看了上千种我喜欢的花。

00:05:48.550 --> 00:05:50.830
为了教会系统怎么打游戏——

00:05:50.830 --> 00:05:52.820
当然，我会。你们也会。

00:05:54.420 --> 00:05:56.460
我喜欢花。这没什么。

00:05:57.260 --> 00:06:00.100
为了教会系统如何玩像围棋一样的游戏，

00:06:00.100 --> 00:06:02.190
我玩了许多围棋的游戏，

00:06:02.190 --> 00:06:03.850
但是在这个过程中

00:06:03.850 --> 00:06:06.290
我也教会它如何分别差游戏和好游戏。

00:06:06.290 --> 00:06:10.030
如果我想要一个人工智能法律助手，

00:06:10.030 --> 00:06:11.836
我会给它一些法律文集，

00:06:11.840 --> 00:06:14.716
但同时我会将 怜悯和正义

00:06:14.720 --> 00:06:18.320
也是法律的一部分 这种观点融入其中。

00:06:18.320 --> 00:06:21.620
用一个术语来解释，
就是我们所说的真相，

00:06:21.620 --> 00:06:23.370
而关键在于：

00:06:23.370 --> 00:06:24.926
为了制造这些机器，

00:06:24.926 --> 00:06:28.366
我们正教给它们我们的价值观。

00:06:28.366 --> 00:06:31.470
正因如此，我相信一个人工智能

00:06:31.470 --> 00:06:35.140
绝不逊于一个经过良好训练的人类。

00:06:35.900 --> 00:06:37.166
但是，你或许会问，

00:06:37.166 --> 00:06:39.796
要是流氓组织，

00:06:39.796 --> 00:06:43.156
和资金充沛的无政府组织
也在利用它们呢？

00:06:43.156 --> 00:06:46.956
我并不害怕独狼掌控的人工智能。

00:06:46.960 --> 00:06:51.510
很明显，我们无法从
随机的暴力行为中保护自己，

00:06:51.510 --> 00:06:53.736
但是现实是，制造这样一个系统

00:06:53.736 --> 00:06:56.846
超越了个人所拥有资源的极限，

00:06:56.846 --> 00:06:59.216
因为它需要踏实细致的训练和培养。

00:06:59.216 --> 00:07:00.050
还有，

00:07:00.050 --> 00:07:03.600
这远比向世界散播一个网络病毒，

00:07:03.600 --> 00:07:06.570
比如你按下一个按钮，
瞬间全世界都被感染，

00:07:06.570 --> 00:07:09.210
并且在各处的笔记本电脑中
开始爆发来的复杂。

00:07:09.210 --> 00:07:12.156
这类东西正在越来越强大，

00:07:12.156 --> 00:07:13.815
我们必然会看到它们的来临。

00:07:14.340 --> 00:07:17.476
我会害怕一个有可能威胁所有人类的

00:07:17.476 --> 00:07:19.380
人工智能吗？

00:07:20.100 --> 00:07:24.450
如果你看过《黑客帝国》，《大都会》，

00:07:24.450 --> 00:07:27.650
《终结者》，或者
《西部世界》这类电视剧，

00:07:27.650 --> 00:07:29.820
它们都在表达这种恐惧。

00:07:29.820 --> 00:07:34.130
的确，在哲学家Nick Bostrom
写的《超级智能》中，

00:07:34.130 --> 00:07:35.700
他选择了这个主题，

00:07:35.700 --> 00:07:39.750
并且观察到超级智能不仅仅危险，

00:07:39.750 --> 00:07:43.620
它还对所有人类的存在造成了威胁。

00:07:43.620 --> 00:07:45.870
Bostrom博士的基础观点认为，

00:07:45.870 --> 00:07:48.620
这样的系统迟早会

00:07:48.620 --> 00:07:51.880
产生对信息的无止境渴求，

00:07:51.880 --> 00:07:54.810
也许它们会开始学习，

00:07:54.810 --> 00:07:57.430
并且最终发现它们的目的

00:07:57.430 --> 00:07:59.780
和人类的需求背道而驰。

00:07:59.780 --> 00:08:01.670
Bostrom博士有一群粉丝。

00:08:01.670 --> 00:08:06.020
Elon Musk和Stephen Hawking也支持他。

00:08:06.700 --> 00:08:09.100
虽然要向这些伟大的头脑

00:08:09.980 --> 00:08:11.960
致以崇高的敬意，

00:08:11.960 --> 00:08:14.270
但我还是相信他们从一开始就错了。

00:08:14.270 --> 00:08:17.816
Bostrom博士的观点
有很多地方可以细细体会，

00:08:17.816 --> 00:08:19.666
但现在我没有时间一一解读，

00:08:19.666 --> 00:08:22.386
简要而言，请考虑这句话：

00:08:22.386 --> 00:08:26.100
全知并非全能。

00:08:26.100 --> 00:08:28.030
HAL成为了对发现一号成员的威胁，

00:08:28.030 --> 00:08:32.470
只是因为它控制了
发现一号的各个方面。

00:08:32.470 --> 00:08:34.990
正因如此它才需要是一个人工智能。

00:08:34.990 --> 00:08:37.470
它需要对于我们世界的完全掌控。

00:08:37.470 --> 00:08:40.330
这就是《终结者》中的天网，

00:08:40.330 --> 00:08:42.200
一个控制了人们意志，

00:08:42.200 --> 00:08:43.776
控制了世界各处

00:08:43.776 --> 00:08:47.510
各个机器的超级智能。

00:08:47.510 --> 00:08:48.990
说实在的，

00:08:48.990 --> 00:08:51.100
这完全是杞人忧天。

00:08:51.100 --> 00:08:54.180
我们不是在制造可以控制天气，

00:08:54.180 --> 00:08:55.550
引导潮水，

00:08:55.550 --> 00:08:59.006
指挥我们这些
多变，混乱的人类的人工智能。

00:08:59.006 --> 00:09:02.870
另外，即使这类人工智能存在，

00:09:02.870 --> 00:09:05.830
它需要和人类的经济竞争，

00:09:05.830 --> 00:09:08.380
进而和我们拥有的资源竞争。

00:09:09.020 --> 00:09:10.210
最后——

00:09:10.210 --> 00:09:11.500
不要告诉Siri——

00:09:12.200 --> 00:09:13.716
我们可以随时拔掉电源。

00:09:13.716 --> 00:09:15.780
（笑声）

00:09:17.180 --> 00:09:19.706
我们正处于和机器共同演化的

00:09:19.706 --> 00:09:22.246
奇妙旅途之中。

00:09:22.246 --> 00:09:24.640
未来的人类

00:09:24.640 --> 00:09:27.230
将与今天的人类大相径庭。

00:09:27.230 --> 00:09:30.456
当前对人工智能崛起的担忧，

00:09:30.456 --> 00:09:33.440
从各方面来说都是
一个危险的错误指引，

00:09:33.440 --> 00:09:35.820
因为电脑的崛起

00:09:35.820 --> 00:09:38.926
带给了我们许多必须参与的

00:09:38.926 --> 00:09:40.540
关乎人类和社会的问题。

00:09:41.180 --> 00:09:43.950
我应该如何管理社会

00:09:43.950 --> 00:09:46.376
来应对人类劳工需求量的降低？

00:09:46.380 --> 00:09:50.190
我应该怎样在进行全球化
交流和教育的同时，

00:09:50.190 --> 00:09:51.990
依旧尊重彼此的不同？

00:09:51.990 --> 00:09:56.356
我应该如何通过可认知医疗
来延长并强化人类的生命？

00:09:56.356 --> 00:09:59.150
我应该如何通过计算机

00:09:59.150 --> 00:10:00.940
来帮助我们前往其他星球？

00:10:01.580 --> 00:10:03.620
这些都很令人兴奋。

00:10:04.220 --> 00:10:06.530
通过计算机来升级

00:10:06.530 --> 00:10:08.206
人类体验的机会

00:10:08.206 --> 00:10:09.550
就在我们手中，

00:10:09.550 --> 00:10:11.420
就在此时此刻，

00:10:11.420 --> 00:10:13.140
我们的旅途才刚刚开始。

00:10:14.100 --> 00:10:15.316
谢谢大家。

00:10:15.320 --> 00:10:19.626
（掌声）

