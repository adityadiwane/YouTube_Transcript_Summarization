WEBVTT
Kind: captions
Language: ar

00:00:00.000 --> 00:00:07.000
المترجم: Mohamad Alhaddad
المدقّق: Hussain Laghabi

00:00:12.580 --> 00:00:16.420
عندما كنت صغيراً، كنت شغوفاً بالدراسة

00:00:17.140 --> 00:00:19.316
أظن أن بعضاً منكم كان كذلك، أيضاًً.

00:00:19.340 --> 00:00:20.556
(ضحك)

00:00:20.580 --> 00:00:23.796
و أنت يا سيدي، الذي ضحكت بشدة،
من المحتمل أنك مازلت كذلك.

00:00:23.820 --> 00:00:26.076
(ضحك)

00:00:26.100 --> 00:00:29.596
لقد عشت في بلدة صغيرة
في أحد ضواحي نورث تكساس،

00:00:29.620 --> 00:00:32.956
كوني ولداً لضابط في الشرطة وهو حفيد لقس.

00:00:32.980 --> 00:00:34.900
فإن الوقوع في المشاكل لم يكن من خياراتي.

00:00:35.860 --> 00:00:39.116
فبدأت بقراءة كتب الرياضيات للترفيه.

00:00:39.140 --> 00:00:40.676
(ضحك)

00:00:40.700 --> 00:00:42.396
أنتم فعلتم ذلك، أيضاً.

00:00:42.420 --> 00:00:46.156
قادني ذلك لعمل ليزر وحاسوب ونماذج صواريخ،

00:00:46.180 --> 00:00:49.180
فأنشأت ورشة صواريخ في غرفة نومي.

00:00:49.780 --> 00:00:53.436
الآن، علمياً،

00:00:53.460 --> 00:00:56.716
نسمي هذه بالفكرة السيئة جداً.

00:00:56.740 --> 00:00:57.956
(ضحك)

00:00:57.980 --> 00:01:00.156
في ذلك الوقت،

00:01:00.180 --> 00:01:03.396
رواية ستاني كوبورز "ملحمة الفضاء"
أصبحت في دور العرض،

00:01:03.420 --> 00:01:05.710
وبذلك تغيرت حياتي للأبد.

00:01:06.100 --> 00:01:08.156
لقد أحببت كل شيء في ذلك الفلم،

00:01:08.180 --> 00:01:10.716
خصوصي "هال" 9000.

00:01:10.740 --> 00:01:12.796
"هال" كان حاسوباً واعياً

00:01:12.820 --> 00:01:15.276
مصمماً لقيادة سفينة الاستكشاف

00:01:15.300 --> 00:01:17.836
من الأرض إلى المشترى.

00:01:17.860 --> 00:01:19.916
"هال" كان شخصيةً سيئة،

00:01:19.940 --> 00:01:24.220
في النهاية لقد اختار إتمام المهمة عوضاً 
عن إنقاذ البشرية.

00:01:24.660 --> 00:01:26.756
"هال" كان شخصيةً خياليََة،

00:01:26.780 --> 00:01:29.436
لكن ومع ذلك
لقد تحاكى مع مخاوفنا،

00:01:29.460 --> 00:01:31.556
مخاوفنا في التعامل مع

00:01:31.580 --> 00:01:34.596
ذكاء اصطناعي عديم المشاعر

00:01:34.620 --> 00:01:36.580
لا يختلف عنا نحن البشر.

00:01:37.700 --> 00:01:40.276
أنا أؤمن بأن هذه المخاوف لا وجود لها.

00:01:40.300 --> 00:01:42.996
في الواقع، نحن نعيش وقتاً مميزاً

00:01:43.020 --> 00:01:44.556
في تاريخ البشريََة،

00:01:44.580 --> 00:01:49.556
حيث نسير رافضين وضع حدود لقدراتنا
العقلية والجسدية،

00:01:49.580 --> 00:01:51.276
نقوم ببناء آلات

00:01:51.300 --> 00:01:54.916
ذات تعقيد وأداء رائعين وجميلين

00:01:54.940 --> 00:01:56.996
ستسهم في توسيع المعرفة البشريََة

00:01:57.020 --> 00:01:58.700
لمراحل لا يمكن تصورها.

00:01:59.540 --> 00:02:02.116
عبر خبرتي المهنية من الكلية الجوية

00:02:02.140 --> 00:02:04.076
إلى توجيه سفن الفضاء،
إلى الوقت الحالي،

00:02:04.100 --> 00:02:05.796
أصبحت مهندس أنظمة،

00:02:05.820 --> 00:02:08.556
ومؤخراً تم توكيلي بمهمة حل مشكلة
هندسية

00:02:08.580 --> 00:02:11.156
متعلََقة بعملية وكالة الفضاء 
"ناسا" في المريخ.

00:02:11.180 --> 00:02:13.676
حالياً، في الرحلات الفضائية إلى القمر،

00:02:13.700 --> 00:02:16.836
يمكننا الإعتماد على 
نظام الملاحة المسمََى "هيوستون"

00:02:16.860 --> 00:02:18.836
لمتابعة كافة تفاصيل الرحلة.

00:02:18.860 --> 00:02:22.396
ولكن المريخ يبعد مئتي ضعف مقارنة بالقمر،

00:02:22.420 --> 00:02:25.636
وبالتالي نحتاج في المتوسط ل 13 دقيقة

00:02:25.660 --> 00:02:28.796
لتنتقل الإشارة 
من الأرض إلى المريخ،

00:02:28.820 --> 00:02:32.220
وإذا ما حدثت مشكلة،
لا يوجد وقت كاف لتفاديها.

00:02:32.660 --> 00:02:35.156
ولذلك فإن الحل الهندسي الأكثر فائدة

00:02:35.180 --> 00:02:37.756
يدعونا لوضع جهاز توجيه للحركة (جهاز ملاحة)

00:02:37.780 --> 00:02:40.796
داخل جدران المركبة الفضائية.

00:02:40.820 --> 00:02:43.716
وفكرة أخرى جميلة تصب في 
مجال أداء العملية

00:02:43.740 --> 00:02:46.636
هي إرسال رجال آليين ووضعهم فوق سطح المريخ

00:02:46.660 --> 00:02:48.516
قبل وصول البشر بنفسهم إلى هناك،

00:02:48.540 --> 00:02:50.196
أولاً لتقديم بعض التسهيلات

00:02:50.220 --> 00:02:53.580
ومن ثمََ كأعضاء متعاونين
في الفريق العلمي.

00:02:55.220 --> 00:02:57.956
حالياًً عندما أتمعن في هذه الفكرة
من منظور هندسي،

00:02:57.980 --> 00:03:01.156
فإنني أرى بشكل واضح 
أن ما أخطط له

00:03:01.180 --> 00:03:03.356
هو حل ذكي، متعاون،

00:03:03.380 --> 00:03:05.756
يتمتع بالذكاء الاصطناعي والاجتماعي.

00:03:05.780 --> 00:03:10.076
بعبارة أخرى، أريد بناء ما يشبه "هال"
إلى حد كبير

00:03:10.100 --> 00:03:12.516
ولكن بدون أن يكون له نزعات للقتل.

00:03:12.540 --> 00:03:13.900
(ضحك)

00:03:14.740 --> 00:03:16.556
لنتوقف للحظة.

00:03:16.580 --> 00:03:20.476
هل من الممكن بناء نظام ذكي مثل هذا؟

00:03:20.500 --> 00:03:21.956
في الواقع، أجل.

00:03:21.980 --> 00:03:23.236
بتصورات مختلفة،

00:03:23.260 --> 00:03:25.236
إن هذه مشكلة هندسية صعبة

00:03:25.260 --> 00:03:26.716
تضمن عناصر ذكاء اصطناعي،

00:03:26.740 --> 00:03:31.436
ليست مجرد مسألة ذكاء اصطناعي
بسيطة يمكن إنشاؤها.

00:03:31.460 --> 00:03:34.116
كما ورد عن "آلان تورنج"،

00:03:34.140 --> 00:03:36.516
أنا لست مهتما بآلة ذات إحساس.

00:03:36.540 --> 00:03:38.116
أنا لا أقوم بصناعة "هال".

00:03:38.140 --> 00:03:40.556
كل ما أريد إنجازه هو مجرد عقل بسيط،

00:03:40.580 --> 00:03:43.700
يمكنه تقديم هذا الذكاء الخيالي.

00:03:44.820 --> 00:03:47.956
إن علوم وفنون الحاسوب قد أثمرت
في نهاية المطاف

00:03:47.980 --> 00:03:49.476
منذ ظهور "هال" على الشاشة،

00:03:49.500 --> 00:03:52.716
وأظن في حال وجود المخترع 
الدكتور "شاندرو" هنا اليوم،

00:03:52.740 --> 00:03:55.076
فإنه سوف يكون لديه العديد من
التساؤلات.

00:03:55.100 --> 00:03:57.196
هل بالفعل يمكننا

00:03:57.220 --> 00:04:01.236
تكوين نظام يحوي ملايين ملايين
الأجهزة،

00:04:01.260 --> 00:04:02.716
يمكنه قراءة معطياتها،

00:04:02.740 --> 00:04:04.996
التنبؤ بحالات حصول الخطأ،
و التصرف قبل وقوعه؟

00:04:05.020 --> 00:04:06.236
أجل.

00:04:06.260 --> 00:04:09.436
هل يمكننا بناء نظام يمكنه التحاكي
مع البشر بلغاتهم؟

00:04:09.460 --> 00:04:10.676
أجل.

00:04:10.700 --> 00:04:13.676
هل يمكننا بناء أنظمة يمكنها التعرف على
الأجسام، وعلى حركتها،

00:04:13.700 --> 00:04:17.076
والشعور بأنفسها، أو اللعب بألعاب الحاسب
وقراءة حركة الشفاه؟

00:04:17.100 --> 00:04:18.316
أجل.

00:04:18.340 --> 00:04:20.476
هل يمكننا بناء نظام يضع أهدافاًً لنفسه،

00:04:20.500 --> 00:04:24.116
يتوجب الوصول إليها وضع خطط 
يتعلمها حتى يصل هدفه؟

00:04:24.140 --> 00:04:25.356
أجل.

00:04:25.380 --> 00:04:28.716
هل يمكننا بناء أنظمة تستطيع التفكير؟

00:04:28.740 --> 00:04:30.236
هذا ما نقوم بالتعلم لإنجازه.

00:04:30.260 --> 00:04:33.740
هل يمكننا بناء أنظمة ذات اعتقادات
أدبية وخُلقية؟

00:04:34.300 --> 00:04:36.340
هذا ما يتوجب علينا تعلم كيفية إنجازه.

00:04:37.180 --> 00:04:38.556
إذا، لنتقبَََل حالياً

00:04:38.580 --> 00:04:41.476
القدرة على بناء نظام كهذا

00:04:41.500 --> 00:04:43.636
من أجل مهمة كهذه وغيرها.

00:04:43.660 --> 00:04:46.196
السؤال الآخر الذي ينبغي عليكم سؤاله هو،

00:04:46.220 --> 00:04:47.676
هل ينبغي علينا الخوف منه؟

00:04:47.700 --> 00:04:49.676
حسناً، كل تقنية جديدة

00:04:49.700 --> 00:04:52.596
تجلب معها شيئاً من الريبة.

00:04:52.620 --> 00:04:54.316
عندما رأينا السيارات لأول مرة،

00:04:54.340 --> 00:04:58.356
توقع البشر حدوث دمار للعائلة.

00:04:58.380 --> 00:05:01.076
عندما رأينا الهواتف لأول مرة،

00:05:01.100 --> 00:05:03.996
قلق الناس من قيامها بإنهاء المحادثات
الشخصية.

00:05:04.020 --> 00:05:07.956
في الماضي عندما انتشرت الكلمة المكتوبة،

00:05:07.980 --> 00:05:10.476
ظن البشر أننا سنفقد قدرتنا على التذكر.

00:05:10.500 --> 00:05:12.556
هذه التنبؤات واقعية لحد معين،

00:05:12.580 --> 00:05:14.996
ولكن هذه التقنيات

00:05:15.020 --> 00:05:18.396
جلبت لنا ما زاد خبرتنا كبشر

00:05:18.420 --> 00:05:20.300
بطرقٍ عميقة.

00:05:21.660 --> 00:05:23.940
لنتوسع في هذه النقطة بعض الشيء.

00:05:24.940 --> 00:05:29.676
أنا لا أخاف من بناء أنظمة 
ذكاء اصطناعي كهذه،

00:05:29.700 --> 00:05:33.516
ﻷنها بالنتيجة ستقوم باستعمال معاييرنا.

00:05:33.540 --> 00:05:37.036
لنأخذ بعين الاعتبار أن
بناء نظام واع يختلف تماماً

00:05:37.060 --> 00:05:40.356
عن بناء النظام التقليدي المعقد في الماضي.

00:05:40.380 --> 00:05:42.836
نحن لا نقوم ببرمجتها، نحن نعلِِمها.

00:05:42.860 --> 00:05:45.516
لتعليم نظام كيف يتعرف على الورود،

00:05:45.540 --> 00:05:48.556
أُريه آلاف الورود، من الأنواع المفضلة لدي.

00:05:48.580 --> 00:05:50.836
لتعليم نظام كيف يلعب لعبة

00:05:50.860 --> 00:05:52.820
حسنا، أريد فعل ذلك.
و أنتم تريدون، أيضاً.

00:05:54.420 --> 00:05:56.460
أنا أحب الورود، صدقوني.

00:05:57.260 --> 00:06:00.116
لتعليم نظام كيفية لعب لعبة ك "جو"،

00:06:00.140 --> 00:06:02.196
أقوم بلعب ال "جو" آلاف المرات،

00:06:02.220 --> 00:06:03.876
ضمنياً أقوم بتعليمه التمييز

00:06:03.900 --> 00:06:06.316
بين اللعبة الجيدة من السيئة.

00:06:06.340 --> 00:06:10.036
إذا أردت إنشاء مساعد قانوني
ذكي،

00:06:10.060 --> 00:06:11.836
سوف أعلمه بعض قواعد القانون

00:06:11.860 --> 00:06:14.716
ولكن بنفس الوقت أقوم بتمرير

00:06:14.740 --> 00:06:17.620
إحساس الرحمة والعدل المتعلق
بهذا القانون.

00:06:18.380 --> 00:06:21.356
علمياً نطلق على هذه العملية
"القاعدة الحقيقية"،

00:06:21.380 --> 00:06:23.396
هنا تكمن النقطة المهمة:

00:06:23.420 --> 00:06:24.876
لإنشاء آلات كهذه،

00:06:24.900 --> 00:06:28.316
نحن نقوم بتعليمها الشعور بقيمنا.

00:06:28.340 --> 00:06:31.476
إلى هنا، أنا أثق بالذكاء الاصطناعي

00:06:31.500 --> 00:06:35.140
بما يساوي، إن لم يزِد،
إنساناً جيد التدريب.

00:06:35.900 --> 00:06:37.116
لكن، قد تسألون،

00:06:37.140 --> 00:06:39.756
ماذا عن العملاء المحتالين،

00:06:39.780 --> 00:06:43.116
المؤسسات غير الحكومية 
ذات الدعم المادي القوي؟

00:06:43.140 --> 00:06:46.956
أنا لا أخاف من الذكاء الاصطناعي في يد
ثعلب وحيد.

00:06:46.980 --> 00:06:51.516
بشكل صريح، لا يمكننا حماية أنفسنا
من كافة أشكال العنف،

00:06:51.540 --> 00:06:53.676
لكن في الواقع نظام كهذا

00:06:53.700 --> 00:06:56.796
يحتاج لإنجاز تدريبات كثيفة ومعقدة

00:06:56.820 --> 00:06:59.116
تفوق قدرة الفرد الواحد.

00:06:59.140 --> 00:07:00.356
أيضاًً،

00:07:00.380 --> 00:07:03.636
إنها أصعب من مجرد إدخال فايروس
إلى العالم عبر الإنترنت،

00:07:03.660 --> 00:07:06.756
حيث بضغطة زر، ينتشر في ملايين الأجهزة

00:07:06.780 --> 00:07:09.236
وتتعطل الحواسيب المحمولة في كل مكان.

00:07:09.260 --> 00:07:12.076
إن هذه الأمور أكبر بكثير،

00:07:12.100 --> 00:07:13.815
ونحن نراها في المستقبل.

00:07:14.340 --> 00:07:17.396
هل أخاف من هكذا ذكاء اصطناعي

00:07:17.420 --> 00:07:19.380
قد يهدد البشرية جمعاء؟

00:07:20.100 --> 00:07:24.476
إذا ما تابعتم أفلام ك "ماتريكس"، 
و"شرطة القطار"

00:07:24.500 --> 00:07:27.676
و"المدمر"، ومسلسلات مثل "العالم الغربي"

00:07:27.700 --> 00:07:29.836
فإن كلها تتحدث عن هذه المخاوف.

00:07:29.860 --> 00:07:34.156
في الواقع، في كتاب "الذكاء الخارق"
للفيلسوف "نيك بوستروم"،

00:07:34.180 --> 00:07:35.716
يتناول هذه النمطية

00:07:35.740 --> 00:07:39.756
ويرى أن الذكاء الخارق ليس مجرد
خطر فحسب،

00:07:39.780 --> 00:07:43.636
وإنما يمكن أن يمثل تهديداً حقيقياً
لكافة البشرية.

00:07:43.660 --> 00:07:45.876
الجدل الأساسي للدكتور "بوستروم"

00:07:45.900 --> 00:07:48.636
أن هذه الأنظمة بشكل مفاجيء

00:07:48.660 --> 00:07:51.916
ستصبح متعطشة بشكل شديد للمعلومات

00:07:51.940 --> 00:07:54.836
وبالتالي سوف تتعلم كيف تتم عملية التعلم

00:07:54.860 --> 00:07:57.476
ومن ثم تكتشف أن لديها أهدافاً

00:07:57.500 --> 00:07:59.796
تخالف ما يحتاجه البشر.

00:07:59.820 --> 00:08:01.676
الدكتور "بوستروم" لديه عدد من الأتباع.

00:08:01.700 --> 00:08:06.020
و يدعمه في تفكيره أشخاص ك 
"إيلون موسك" و "ستيفن هاوكنج".

00:08:06.700 --> 00:08:09.100
مع كل احترامي

00:08:09.980 --> 00:08:11.996
لهذه العقول الرائعة،

00:08:12.020 --> 00:08:14.276
إلا أني أؤمن بأنهم على خطأ.

00:08:14.300 --> 00:08:17.476
هناك العديد من جدليات الدكتور "بوستوم"
للمناقشة،

00:08:17.500 --> 00:08:19.636
وليس لدي وقت كاف لمناقشتها كلها،

00:08:19.660 --> 00:08:22.356
لكن بشكل مختصر، فكروا في هذه:

00:08:22.380 --> 00:08:26.116
المعرفة الفائقة تختلف بشكل أساسي عن
العمل الخارق.

00:08:26.140 --> 00:08:28.036
"هال" كان خطراً على فريق الاستكشاف

00:08:28.060 --> 00:08:32.476
فقط عندما قام "هال" بإصدار
التعليمات كافة في عملية الاستكشاف.

00:08:32.500 --> 00:08:34.996
لذا هذه الفكرة مرتبطة بالذكاء الخارق.

00:08:35.020 --> 00:08:37.516
من الممكن أن تسيطر على عالمنا.

00:08:37.540 --> 00:08:40.356
هذه الأشياء مقتبسة من فلم "المدمر"

00:08:40.380 --> 00:08:42.236
حيث لدينا ذكاء خارق

00:08:42.260 --> 00:08:43.636
يقود رغبة البشر،

00:08:43.660 --> 00:08:47.516
يتحكم بكل جهاز موجود في كل زاوية
في أنحاء العالم.

00:08:47.540 --> 00:08:48.996
للحديث بشكل عملي،

00:08:49.020 --> 00:08:51.116
هذا لن يحدث.

00:08:51.140 --> 00:08:54.196
نحن لا نقوم ببناء أنظمة ذكاء اصطناعي
تتحكم بحالة الطقس،

00:08:54.220 --> 00:08:55.556
وتوجه المد والجزر،

00:08:55.580 --> 00:08:58.956
وتأمرنا نحن البشر المتقلبين العشوائيين.

00:08:58.980 --> 00:09:02.876
وللمزيد، إذا وُجدت أنظمة ذكاء اصطناعي
كهذه،

00:09:02.900 --> 00:09:05.836
ستنافس اقتصاد البشر،

00:09:05.860 --> 00:09:08.380
وبالتالي تنافس البشر
في المصادر والثروات.

00:09:09.020 --> 00:09:10.236
وفي النهاية.

00:09:10.260 --> 00:09:11.500
لا تخبروا نظام "سيري" بهذا

00:09:12.260 --> 00:09:13.636
حيث يمكننا إطفاؤها دائماً.

00:09:13.660 --> 00:09:15.780
(ضحك)

00:09:17.180 --> 00:09:19.636
نحن في رحلة عظيمة

00:09:19.660 --> 00:09:22.156
من التطور مع آلاتنا.

00:09:22.180 --> 00:09:24.676
ما نحن عليه اليوم كبشر

00:09:24.700 --> 00:09:27.236
يختلف عن ما سنكون عليه في المستقبل.

00:09:27.260 --> 00:09:30.396
القلق الآن من نمو الذكاء الخارق

00:09:30.420 --> 00:09:33.476
هو إلهاء خطير بحالات متعددة

00:09:33.500 --> 00:09:35.836
لأن نمو الحوسبة بذاتها

00:09:35.860 --> 00:09:38.876
يجلب لنا عدداً من المشاكل
البشرية والاجتماعية

00:09:38.900 --> 00:09:40.540
ينبغي علينا حلها الآن.

00:09:41.180 --> 00:09:43.996
كيف يمكنني أن أنظم المجتمع
بالشكل الأفضل

00:09:44.020 --> 00:09:46.356
عندما يقل احتياج اليد العاملة البشرية؟

00:09:46.380 --> 00:09:50.196
كيف يمكنني أن أجلب الوعي والتعليم لكافة 
أنحاء الأرض

00:09:50.220 --> 00:09:51.996
مع احترام كافة أشكال اختلافاتنا؟

00:09:52.020 --> 00:09:56.276
كيف يمكنني أن أوسع وأحسن حياة الإنسان
من خلال العناية الإدراكية؟

00:09:56.300 --> 00:09:59.156
كيف يمكنني استعمال الحوسبة

00:09:59.180 --> 00:10:00.940
لتقوم بأخذنا إلى النجوم؟

00:10:01.580 --> 00:10:03.620
وهذا هو الشيء الممتع.

00:10:04.220 --> 00:10:06.556
إن الفرص لاستعمال الحوسبة

00:10:06.580 --> 00:10:08.116
لتطوير خبرة الإنسان

00:10:08.140 --> 00:10:09.556
في متناول يدنا،

00:10:09.580 --> 00:10:11.436
هنا والآن،

00:10:11.460 --> 00:10:13.140
نحن نبدأ فحسب.

00:10:14.100 --> 00:10:15.316
شكراً لكم.

00:10:15.340 --> 00:10:19.626
(تصفيق)

