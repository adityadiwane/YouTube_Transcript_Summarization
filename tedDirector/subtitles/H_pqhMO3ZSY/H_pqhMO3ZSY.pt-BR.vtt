WEBVTT
Kind: captions
Language: pt-BR

00:00:00.000 --> 00:00:12.000
Tradutor: Gustavo Rocha
Revisor: Leonardo Silva

00:00:12.641 --> 00:00:14.995
Eu gostaria de contar-lhes uma história

00:00:14.995 --> 00:00:18.171
que conecta o famoso
incidente de privacidade

00:00:18.171 --> 00:00:20.940
envolvendo Adão e Eva,

00:00:20.940 --> 00:00:24.386
e a notável mudança no limite

00:00:24.386 --> 00:00:27.072
entre o público e o privado
que vem acontecendo

00:00:27.072 --> 00:00:28.842
nos últimos 10 anos.

00:00:28.842 --> 00:00:30.140
Vocês conhecem o incidente.

00:00:30.140 --> 00:00:33.470
Adão e Eva, um dia, no Jardim do Éden,

00:00:33.470 --> 00:00:35.313
se dão conta de que estão nus.

00:00:35.313 --> 00:00:36.813
Eles se apavoram.

00:00:36.813 --> 00:00:39.570
E o resto vocês já conhecem.

00:00:39.570 --> 00:00:41.758
Hoje em dia, Adão e Eva

00:00:41.758 --> 00:00:44.119
provavelmente reagiriam diferente.

00:00:44.119 --> 00:00:46.387
[@Adão Ontem à noite foi show!
Curti akela maçã LOL]

00:00:46.387 --> 00:00:48.260
[@Eva pois é... gata,
cê viu minha calça por aí?]

00:00:48.260 --> 00:00:50.896
Nós publicamos muito mais informações

00:00:50.896 --> 00:00:54.230
sobre nós mesmos on-line
do que jamais antes,

00:00:54.230 --> 00:00:55.934
e tanta informação sobre nós

00:00:55.934 --> 00:00:58.158
está sendo coletada por organizações.

00:00:58.158 --> 00:01:01.920
Bem, essa análise massiva
de informações pessoais,

00:01:01.920 --> 00:01:05.832
ou <i>big data</i>, traz muitos ganhos e benefícios,

00:01:05.832 --> 00:01:08.470
mas há também desvantagens complexas

00:01:08.470 --> 00:01:11.568
ao abrir mão da privacidade.

00:01:11.568 --> 00:01:15.591
E minha história é sobre essas desvantagens.

00:01:15.591 --> 00:01:18.175
Começamos com uma observação
que, na minha mente,

00:01:18.175 --> 00:01:21.502
se tornou cada vez mais clara
nos últimos anos,

00:01:21.502 --> 00:01:23.599
de que qualquer informação pessoal

00:01:23.599 --> 00:01:25.884
pode se tornar delicada.

00:01:25.884 --> 00:01:30.009
No ano de 2000, cerca de
100 bilhões de fotos

00:01:30.009 --> 00:01:31.921
foram tiradas em todo o mundo,

00:01:31.921 --> 00:01:34.986
mas somente uma porção minúscula delas

00:01:34.986 --> 00:01:36.869
acabou sendo carregada na Internet.

00:01:36.869 --> 00:01:40.230
Em 2010, somente no Facebook,
em um único mês,

00:01:40.230 --> 00:01:43.500
2,5 bilhões de fotos foram carregadas,

00:01:43.500 --> 00:01:45.382
a maioria identificada.

00:01:45.382 --> 00:01:47.262
No mesmo período de tempo,

00:01:47.262 --> 00:01:52.132
a capacidade dos computadores
de reconhecerem pessoas em fotos

00:01:52.132 --> 00:01:55.740
melhorou em três ordens de grandeza.

00:01:55.740 --> 00:01:57.622
O que acontece quando colocamos

00:01:57.622 --> 00:01:59.123
essas tecnologias juntas:

00:01:59.123 --> 00:02:01.781
crescente disponibilidade
de informações faciais;

00:02:01.781 --> 00:02:05.429
crescente capacidade de reconhecimento
facial por computadores;

00:02:05.429 --> 00:02:07.611
mas também computação em nuvem,

00:02:07.611 --> 00:02:09.499
que oferece a qualquer um neste salão

00:02:09.499 --> 00:02:11.059
o tipo de poder computacional

00:02:11.059 --> 00:02:12.945
que há alguns anos era somente o domínio

00:02:12.945 --> 00:02:14.727
de agências de três letras;

00:02:14.727 --> 00:02:16.105
e computação ubíqua,

00:02:16.105 --> 00:02:18.997
que permite que meu celular,
que não é um supercomputador,

00:02:18.997 --> 00:02:20.668
conecte-se à Internet

00:02:20.668 --> 00:02:23.002
e faça lá centenas de milhares

00:02:23.002 --> 00:02:25.641
de correspondências faciais
em alguns segundos?

00:02:25.641 --> 00:02:28.269
Bem, nós conjecturamos que o resultado

00:02:28.269 --> 00:02:30.333
dessa combinação de tecnologias

00:02:30.333 --> 00:02:33.221
será uma mudança radical em nossas noções

00:02:33.221 --> 00:02:35.478
de privacidade e anonimato.

00:02:35.478 --> 00:02:37.471
Para testar isso, fizemos um experimento

00:02:37.471 --> 00:02:39.592
no campus da universidade
de Carnegie Mellon.

00:02:39.592 --> 00:02:41.691
Nós pedimos a estudantes
que estavam passando

00:02:41.691 --> 00:02:43.470
para participarem em um estudo,

00:02:43.470 --> 00:02:46.032
e tiramos uma foto com uma <i>webcam</i>,

00:02:46.032 --> 00:02:48.814
e pedimos que eles preenchessem
uma pesquisa em um laptop.

00:02:48.814 --> 00:02:50.793
Enquanto estavam preenchendo a pesquisa,

00:02:50.793 --> 00:02:53.590
nós enviamos a foto para um
<i>cluster</i> de computação em nuvem,

00:02:53.590 --> 00:02:55.317
e começamos a usar um reconhecedor facial

00:02:55.317 --> 00:02:57.722
para equiparar aquela foto
com um banco de dados

00:02:57.722 --> 00:03:00.115
de algumas centenas de milhares de fotos

00:03:00.115 --> 00:03:03.711
que tínhamos baixado de perfis do Facebook.

00:03:03.711 --> 00:03:06.970
No momento que o participante
atingia a última página

00:03:06.970 --> 00:03:10.317
na pesquisa, a página tinha sido
atualizada dinamicamente

00:03:10.317 --> 00:03:12.630
com as 10 fotos que melhor se ajustavam

00:03:12.630 --> 00:03:14.915
e que o reconhecedor tinha encontrado,

00:03:14.915 --> 00:03:16.653
e pedimos que os participantes indicassem

00:03:16.653 --> 00:03:20.773
se eles estavam ou não na foto.

00:03:20.773 --> 00:03:24.472
Você vê o participante?

00:03:24.472 --> 00:03:27.317
Bem, o computador viu e, de fato, viu

00:03:27.317 --> 00:03:29.466
para um em cada três participantes.

00:03:29.466 --> 00:03:32.650
Então essencialmente, podemos
partir de um rosto anônimo,

00:03:32.650 --> 00:03:36.134
off-line ou on-line, e podemos
usar reconhecimento facial

00:03:36.134 --> 00:03:38.494
para dar um nome a um rosto anônimo,

00:03:38.494 --> 00:03:40.602
graças a dados de mídia social.

00:03:40.602 --> 00:03:42.474
Mas há alguns anos,
nós fizemos algo diferente.

00:03:42.474 --> 00:03:44.297
Nós partimos de dados de mídia social,

00:03:44.297 --> 00:03:47.348
combinamos estatisticamente com dados

00:03:47.348 --> 00:03:49.450
da seguridade social do governo americano,

00:03:49.450 --> 00:03:52.774
e acabamos prevendo
números de seguridade social,

00:03:52.774 --> 00:03:54.286
que nos Estados Unidos

00:03:54.286 --> 00:03:56.326
são informações extremamente delicadas.

00:03:56.326 --> 00:03:58.419
Vocês veem onde estou indo com isso?

00:03:58.419 --> 00:04:01.341
Então, se combinarmos os dois estudos,

00:04:01.341 --> 00:04:02.853
a questão se torna:

00:04:02.853 --> 00:04:05.573
dá para partir de um rosto e,

00:04:05.573 --> 00:04:07.884
usando reconhecimento facial,
encontrar um nome

00:04:07.884 --> 00:04:10.553
e informação disponível publicamente

00:04:10.553 --> 00:04:12.485
sobre esse nome e essa pessoa,

00:04:12.485 --> 00:04:14.733
e dessa informação publicamente disponível

00:04:14.733 --> 00:04:16.775
inferir informação
indisponível publicamente,

00:04:16.775 --> 00:04:18.161
muito mais delicada,

00:04:18.161 --> 00:04:20.033
que podemos conectar de volta com o rosto?

00:04:20.033 --> 00:04:21.789
E a resposta é, sim,
dá para fazer, e nós fizemos.

00:04:21.789 --> 00:04:24.357
Claro, a precisão continua caindo.

00:04:24.357 --> 00:04:25.301
[27% dos cinco primeiros dígitos do SSN
do indivíduo identificados (em 4 tentativas)]

00:04:25.301 --> 00:04:29.128
Mas na verdade, nós até decidimos
desenvolver um aplicativo para iPhone

00:04:29.128 --> 00:04:31.843
que usa a câmera interna do celular

00:04:31.843 --> 00:04:33.443
para tirar a foto de um indivíduo

00:04:33.443 --> 00:04:34.930
e então envia para a nuvem

00:04:34.930 --> 00:04:37.592
e faz em tempo real
o que acabei de descrever:

00:04:37.592 --> 00:04:39.680
faz a correspondência,
encontra informação pública,

00:04:39.680 --> 00:04:41.410
tenta inferir informação delicada,

00:04:41.410 --> 00:04:44.001
e então envia de volta ao iPhone

00:04:44.001 --> 00:04:47.610
para que seja sobreposta
ao rosto do indivíduo,

00:04:47.610 --> 00:04:49.511
um exemplo de realidade aumentada,

00:04:49.511 --> 00:04:51.962
provavelmente um exemplo
assustador de realidade aumentada.

00:04:51.962 --> 00:04:55.301
Na verdade, não desenvolvemos
o aplicativo para disponibilizar,

00:04:55.301 --> 00:04:57.223
somente como prova de conceito.

00:04:57.223 --> 00:04:59.536
De fato, pegue essas tecnologias

00:04:59.536 --> 00:05:01.373
e coloque-as até seu extremo lógico.

00:05:01.373 --> 00:05:04.092
Imagine um futuro onde estranhos à sua volta

00:05:04.092 --> 00:05:06.403
olharão para você por seus <i>Google Glasses</i>

00:05:06.403 --> 00:05:08.710
ou, um dia, suas lentes de contato,

00:05:08.710 --> 00:05:12.730
e usar sete ou oito
pontos de dados sobre você

00:05:12.730 --> 00:05:15.312
para inferir qualquer outra coisa

00:05:15.312 --> 00:05:17.915
que se possa saber sobre você.

00:05:17.915 --> 00:05:22.200
Como será esse futuro sem segredos?

00:05:22.709 --> 00:05:24.673
E deveríamos nos preocupar?

00:05:24.673 --> 00:05:26.564
Nós talvez queiramos acreditar

00:05:26.564 --> 00:05:29.604
que o futuro com tanta riqueza de dados

00:05:29.604 --> 00:05:32.118
será um futuro sem preconceitos,

00:05:32.118 --> 00:05:35.701
mas na verdade, ter tanta informação

00:05:35.701 --> 00:05:37.892
não quer dizer que tomaremos decisões

00:05:37.892 --> 00:05:39.598
que sejam mais objetivas.

00:05:39.598 --> 00:05:42.158
Em outro experimento,
nós apresentamos aos participantes

00:05:42.158 --> 00:05:44.404
informações sobre um potencial
candidato de emprego.

00:05:44.404 --> 00:05:47.582
Incluímos nessas informações
algumas referências

00:05:47.582 --> 00:05:50.228
a informações engraçadas, totalmente legais,

00:05:50.228 --> 00:05:52.693
mas talvez levemente constrangedoras

00:05:52.693 --> 00:05:54.713
que o participante tinha postado <i>on-line</i>.

00:05:54.713 --> 00:05:57.079
Agora, curiosamente, entre os participantes,

00:05:57.079 --> 00:06:00.162
alguns tinham postado
informações comparáveis,

00:06:00.162 --> 00:06:02.160
e outros não.

00:06:02.524 --> 00:06:04.473
Que grupo vocês acham

00:06:04.473 --> 00:06:08.895
que estava mais suscetível a julgar
severamente nosso participante?

00:06:09.295 --> 00:06:10.982
Paradoxalmente, foi o grupo

00:06:10.982 --> 00:06:12.715
que tinha postado informação similar,

00:06:12.715 --> 00:06:15.657
um exemplo de dissonância moral.

00:06:15.657 --> 00:06:17.407
Você podem estar pensando.

00:06:17.407 --> 00:06:19.109
isso não se aplica a mim,

00:06:19.109 --> 00:06:21.271
porque não tenho nada a esconder.

00:06:21.271 --> 00:06:23.753
Mas na verdade, privacidade não se trata

00:06:23.753 --> 00:06:27.429
de ter algo negativo a esconder.

00:06:27.429 --> 00:06:29.783
Imagine que você é o diretor de RH

00:06:29.783 --> 00:06:32.730
de uma certa organização,
e você recebe currículos,

00:06:32.730 --> 00:06:35.203
e decide procurar mais informações
sobre os candidatos.

00:06:35.203 --> 00:06:37.663
Portanto, você pesquisa seus nomes

00:06:37.663 --> 00:06:39.903
e em um certo universo,

00:06:39.903 --> 00:06:41.911
você encontra esta informação.

00:06:41.911 --> 00:06:46.348
Ou em um universo paralelo,
você encontra esta informação.

00:06:46.348 --> 00:06:49.065
Você acha que estaria igualmente disposto

00:06:49.065 --> 00:06:51.868
a chamar o candidato para uma entrevista?

00:06:51.868 --> 00:06:54.150
Se você pensa que sim, então você não é

00:06:54.150 --> 00:06:56.732
como os empregadores
americanos que, na verdade,

00:06:56.732 --> 00:07:00.039
são parte de nosso experimento,
ou seja, fizemos exatamente isso.

00:07:00.039 --> 00:07:03.221
Criamos perfis no Facebook,
manipulando características,

00:07:03.221 --> 00:07:06.072
e começamos a enviar currículos
para empresas nos EUA,

00:07:06.072 --> 00:07:07.980
e detectamos, monitoramos,

00:07:07.980 --> 00:07:10.373
se estavam procurando
pelos nossos candidatos,

00:07:10.373 --> 00:07:12.205
e se estavam reagindo às informações

00:07:12.205 --> 00:07:14.143
que encontravam na mídia social. E estavam.

00:07:14.143 --> 00:07:16.244
Discriminação estava
acontecendo pela mídia social

00:07:16.244 --> 00:07:19.317
para candidatos igualmente capazes.

00:07:19.317 --> 00:07:23.892
Marqueteiros gostam que acreditemos

00:07:23.892 --> 00:07:26.161
que toda informação sobre nós sempre será

00:07:26.161 --> 00:07:29.434
usada de uma maneira a nosso favor.

00:07:29.434 --> 00:07:33.149
Mas pensem novamente. Por que
esse deve ser sempre o caso?

00:07:33.149 --> 00:07:35.813
Em um filme que saiu há alguns anos,

00:07:35.813 --> 00:07:38.366
"Minority Report", uma cena famosa

00:07:38.366 --> 00:07:40.942
tinha Tom Cruise entrando em um <i>shopping</i>

00:07:40.942 --> 00:07:44.718
e anúncios holográficos personalizados

00:07:44.718 --> 00:07:46.553
apareciam a sua volta.

00:07:46.553 --> 00:07:49.780
Agora, esse filme acontece em 2054,

00:07:49.780 --> 00:07:51.422
cerca de daqui a 40 anos,

00:07:51.422 --> 00:07:54.330
e por mais interessante
que pareça essa tecnologia,

00:07:54.330 --> 00:07:56.976
ela já subestima enormemente

00:07:56.976 --> 00:07:59.116
a quantidade de informação
que as organizações

00:07:59.116 --> 00:08:01.599
podem juntar sobre você,
e como podem usá-la

00:08:01.599 --> 00:08:04.997
para influenciá-lo de uma maneira
que você nem perceba.

00:08:04.997 --> 00:08:07.100
E como exemplo, este é um outro experimento

00:08:07.100 --> 00:08:09.373
que estamos fazendo,
ainda não terminamos.

00:08:09.373 --> 00:08:11.692
Imagine que uma organização tem acesso

00:08:11.692 --> 00:08:13.748
à sua lista de amigos no Facebook,

00:08:13.748 --> 00:08:15.520
e através de algum tipo de algoritmo

00:08:15.520 --> 00:08:19.254
eles conseguem detectar
seus dois amigos preferidos.

00:08:19.254 --> 00:08:21.534
E eles criam, em tempo real,

00:08:21.534 --> 00:08:24.376
uma composição facial desses dois amigos.

00:08:24.376 --> 00:08:27.445
E estudos anteriores ao nosso
mostraram que as pessoas

00:08:27.445 --> 00:08:30.330
não reconhecem mais, mesmo
se forem elas mesmas

00:08:30.330 --> 00:08:32.792
em composições faciais, mas elas reagem

00:08:32.792 --> 00:08:34.909
a essas composições de maneira positiva.

00:08:34.909 --> 00:08:38.324
Então, da próxima vez que você estiver
procurando por um certo produto,

00:08:38.324 --> 00:08:40.883
e houver um anúncio sugerindo
que você o compre,

00:08:40.883 --> 00:08:43.790
não será somente um porta-voz em geral.

00:08:43.790 --> 00:08:46.103
Será um de seus amigos,

00:08:46.103 --> 00:08:49.406
e você nem vai saber
que isso está acontecendo.

00:08:49.406 --> 00:08:51.819
E o problema é que

00:08:51.819 --> 00:08:54.338
os mecanismos de políticas
que temos atualmente

00:08:54.338 --> 00:08:57.776
para nos proteger dos abusos
de informações pessoais

00:08:57.776 --> 00:09:00.760
são como levar uma faca para um tiroteio.

00:09:00.760 --> 00:09:03.673
Um desses mecanismos é a transparência,

00:09:03.673 --> 00:09:06.873
dizer às pessoas o que se vai
fazer com os dados delas.

00:09:06.873 --> 00:09:08.979
E, em princípio, isso é uma coisa muito boa.

00:09:08.979 --> 00:09:12.646
É necessário, mas não é suficiente.

00:09:12.646 --> 00:09:16.344
A transparência pode ser mal direcionada.

00:09:16.344 --> 00:09:18.448
Você pode dizer às pessoas o que vai fazer,

00:09:18.448 --> 00:09:20.680
e você ainda avisa para revelar

00:09:20.680 --> 00:09:23.303
quantidades arbitrárias
de informações pessoais.

00:09:23.303 --> 00:09:26.189
E ainda em outro experimento,
este com estudantes,

00:09:26.189 --> 00:09:29.247
nós pedimos que eles
fornecessem informações

00:09:29.247 --> 00:09:31.060
a respeito de seu comportamento no campus,

00:09:31.060 --> 00:09:34.000
incluindo questões delicadas, como esta.

00:09:34.000 --> 00:09:34.621
[Você já colou em uma prova?]

00:09:34.621 --> 00:09:36.921
Agora para um grupo
de participantes nós dissemos:

00:09:36.921 --> 00:09:39.762
"Somente outros estudantes
verão suas respostas."

00:09:39.762 --> 00:09:41.341
Para outro grupo de
participantes, nós dissemos:

00:09:41.341 --> 00:09:44.902
"Estudantes e professores
verão suas respostas."

00:09:44.902 --> 00:09:47.493
Transparência. Notificação.
E com certeza, funcionou,

00:09:47.493 --> 00:09:48.900
no sentido de que o primeiro
grupo de participantes

00:09:48.900 --> 00:09:51.468
estava muito mais disposto
a se expor do que o segundo.

00:09:51.468 --> 00:09:52.988
Faz sentido, certo?

00:09:52.988 --> 00:09:54.478
Mas aí adicionamos um despiste.

00:09:54.478 --> 00:09:57.238
Nós repetimos o experimento
com os mesmos dois grupos,

00:09:57.238 --> 00:09:59.665
desta vez adicionando uma demora

00:09:59.665 --> 00:10:02.600
entre o momento quando
dizíamos aos participantes

00:10:02.600 --> 00:10:04.680
como usaríamos seus dados

00:10:04.680 --> 00:10:09.068
e o momento em que eles realmente
começaram a responder as perguntas.

00:10:09.068 --> 00:10:11.629
Quanto tempo vocês acham
que tivemos que adicionar

00:10:11.629 --> 00:10:16.242
para anular o efeito inibitório

00:10:16.242 --> 00:10:19.653
de saber que os professores
veriam suas respostas?

00:10:19.653 --> 00:10:21.433
Dez minutos?

00:10:21.433 --> 00:10:23.224
Cinco minutos?

00:10:23.224 --> 00:10:25.000
Um minuto?

00:10:25.000 --> 00:10:27.049
Que tal 15 segundos?

00:10:27.049 --> 00:10:29.717
15 segundos foram suficientes
para que os dois grupos

00:10:29.717 --> 00:10:31.285
expusessem a mesma
quantidade de informação,

00:10:31.285 --> 00:10:34.031
como se o segundo grupo
não se importasse mais

00:10:34.031 --> 00:10:36.687
com os professores lendo suas respostas.

00:10:36.687 --> 00:10:40.023
Bem, eu tenho que admitir
que, até agora, essa palestra

00:10:40.023 --> 00:10:42.503
pode parecer excessivamente sombria,

00:10:42.503 --> 00:10:44.224
mas não é esse meu argumento.

00:10:44.224 --> 00:10:46.923
Na verdade, quero compartilhar
com vocês o fato de que

00:10:46.923 --> 00:10:48.695
há alternativas.

00:10:48.695 --> 00:10:51.194
O jeito pelo qual estamos fazendo
as coisas agora não é o único

00:10:51.194 --> 00:10:54.231
pelo qual elas podem ser feitas,
e certamente não é o melhor

00:10:54.231 --> 00:10:56.258
pelo qual podem ser feitas.

00:10:56.258 --> 00:11:00.429
Quando alguém lhe diz: "As pessoas
não se importam com privacidade",

00:11:00.429 --> 00:11:03.071
pense que o jogo foi projetado

00:11:03.071 --> 00:11:06.185
e manipulado para que elas não possam
se importar com privacidade,

00:11:06.185 --> 00:11:09.057
e chegar à percepção de que
essas manipulações ocorrem

00:11:09.057 --> 00:11:10.664
já é metade do caminho

00:11:10.664 --> 00:11:12.922
para ser capaz de se proteger.

00:11:12.922 --> 00:11:16.632
Quando alguém lhe diz que
a privacidade é incompatível

00:11:16.632 --> 00:11:18.481
com os benefícios da <i>big data</i>,

00:11:18.481 --> 00:11:20.954
considere que nos últimos 20 anos,

00:11:20.954 --> 00:11:22.871
pesquisadores criaram tecnologias

00:11:22.871 --> 00:11:26.189
para permitir que praticamente
qualquer transação eletrônica

00:11:26.189 --> 00:11:29.938
aconteça protegendo mais a privacidade.

00:11:29.938 --> 00:11:32.493
Podemos navegar pela internet anonimamente.

00:11:32.493 --> 00:11:35.171
Podemos enviar <i>e-mails</i> que só podem ser lidos

00:11:35.171 --> 00:11:38.880
pelo destinatário desejado,
nem mesmo a NSA.

00:11:38.880 --> 00:11:41.877
Podemos até ter mineração de dados
preservando a privacidade.

00:11:41.877 --> 00:11:45.771
Em outras palavras, podemos
ter os benefícios da <i>big data</i>

00:11:45.771 --> 00:11:47.903
e proteger a privacidade ao mesmo tempo.

00:11:47.903 --> 00:11:51.694
Claro, essas tecnologias
implicam numa mudança

00:11:51.694 --> 00:11:53.240
de custos e receitas

00:11:53.240 --> 00:11:55.347
entre os detentores dos dados
e as pessoas referidas,

00:11:55.347 --> 00:11:58.800
que é o motivo, talvez,
por que não se fala mais disso.

00:11:58.800 --> 00:12:02.506
O que me leva de volta ao Jardim do Éden.

00:12:02.506 --> 00:12:05.286
Há uma segunda
interpretação de privacidade

00:12:05.286 --> 00:12:07.095
da história do Jardim do Éden

00:12:07.095 --> 00:12:09.191
que não tem a ver com a questão

00:12:09.191 --> 00:12:11.416
de Adão e Eva se sentirem nus

00:12:11.416 --> 00:12:13.797
e envergonhados.

00:12:13.797 --> 00:12:16.578
Dá para encontrar ecos desta interpretação

00:12:16.578 --> 00:12:19.360
no poema de John Milton, "Paraíso Perdido".

00:12:19.360 --> 00:12:23.557
No jardim, Adão e Eva estão
satisfeitos materialmente

00:12:23.557 --> 00:12:25.661
Estão felizes. Estão satisfeitos.

00:12:25.661 --> 00:12:27.954
Entretanto, eles também
não têm conhecimento

00:12:27.954 --> 00:12:29.594
e autoconsciência.

00:12:29.594 --> 00:12:31.303
No momento em que eles comem o que é

00:12:31.303 --> 00:12:34.206
chamado apropriadamente
de fruta do conhecimento,

00:12:34.206 --> 00:12:36.811
é aí que eles se descobrem.

00:12:36.811 --> 00:12:40.842
Eles se tornam cientes.
Eles atingem autonomia.

00:12:40.842 --> 00:12:43.968
O preço a se pagar, entretanto,
é abandonar o jardim.

00:12:43.968 --> 00:12:47.849
Então, privacidade,
de certa maneira, é tanto o meio

00:12:47.849 --> 00:12:50.811
quanto o preço a se pagar pela liberdade.

00:12:50.811 --> 00:12:53.581
Novamente, marqueteiros nos dizem

00:12:53.581 --> 00:12:56.600
que <i>big data</i> e mídia social

00:12:56.600 --> 00:12:59.579
não são somente um paraíso
de lucros para eles,

00:12:59.579 --> 00:13:02.036
mas um Jardim do Éden para o resto de nós.

00:13:02.036 --> 00:13:03.274
Nós temos conteúdo gratuito.

00:13:03.274 --> 00:13:06.397
Podemos jogar Angry Birds.
Temos aplicativos direcionados.

00:13:06.397 --> 00:13:09.294
Mas na verdade, em alguns anos, organizações

00:13:09.294 --> 00:13:10.903
saberão tanto sobre nós,

00:13:10.903 --> 00:13:13.613
que conseguirão inferir nossos desejos

00:13:13.613 --> 00:13:15.817
antes mesmo que os tenhamos e, talvez,

00:13:15.817 --> 00:13:18.264
comprar produtos em nosso nome

00:13:18.264 --> 00:13:20.538
antes mesmo de sabermos
que precisamos deles.

00:13:20.538 --> 00:13:23.775
Bom, havia um escritor inglês

00:13:23.775 --> 00:13:26.820
que antecipou esse tipo de futuro

00:13:26.820 --> 00:13:28.225
onde ofereceríamos

00:13:28.225 --> 00:13:31.773
nossa autonomia e liberdade
em troca de conforto.

00:13:31.773 --> 00:13:33.934
Mais ainda do que George Orwell,

00:13:33.934 --> 00:13:36.695
o escritor é, obviamente, Aldous Huxley.

00:13:36.695 --> 00:13:39.549
Em "Admirável Mundo Novo",
ele imagina uma sociedade

00:13:39.549 --> 00:13:41.720
onde as tecnologias que criamos

00:13:41.720 --> 00:13:43.579
originalmente por liberdade

00:13:43.579 --> 00:13:46.146
acabam nos reprimindo.

00:13:46.146 --> 00:13:50.937
Entretanto, no livro, ele também
nos oferece uma saída

00:13:50.937 --> 00:13:54.375
dessa sociedade, parecida com o caminho

00:13:54.375 --> 00:13:58.330
que Adão e Eva tiveram
que seguir para sair do jardim.

00:13:58.330 --> 00:14:00.477
No mundo dos Selvagens,

00:14:00.477 --> 00:14:03.546
recuperar a autonomia
e a liberdade é possível,

00:14:03.546 --> 00:14:06.225
porém o preço a se pagar é alto.

00:14:06.225 --> 00:14:11.940
Então eu acredito que
uma das lutas que definirão

00:14:11.940 --> 00:14:14.503
nossa época será a luta

00:14:14.503 --> 00:14:16.890
pelo controle de informações pessoais,

00:14:16.890 --> 00:14:20.397
a luta por se <i>big data</i>
vai se tornar um força

00:14:20.397 --> 00:14:21.686
pela liberdade,

00:14:21.686 --> 00:14:26.432
em vez de uma força que vai
nos manipular ocultamente.

00:14:26.432 --> 00:14:29.025
Agora mesmo, muitos de nós

00:14:29.025 --> 00:14:31.778
nem mesmo sabem
que a luta está acontecendo,

00:14:31.778 --> 00:14:34.450
mas ela está, quer vocês gostem ou não.

00:14:34.450 --> 00:14:37.254
E com o risco de fazer o papel da serpente,

00:14:37.254 --> 00:14:40.151
vou lhes dizer que
as ferramentas para a luta

00:14:40.151 --> 00:14:43.160
estão aqui, a consciência
do que está acontecendo,

00:14:43.160 --> 00:14:44.515
e em suas mãos,

00:14:44.515 --> 00:14:48.255
só a alguns cliques de distância.

00:14:48.255 --> 00:14:49.737
Obrigado.

00:14:49.737 --> 00:14:54.214
(Aplausos)

