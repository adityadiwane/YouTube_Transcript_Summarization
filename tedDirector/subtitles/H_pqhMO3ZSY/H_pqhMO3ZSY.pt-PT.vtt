WEBVTT
Kind: captions
Language: pt-PT

00:00:00.000 --> 00:00:07.000
Tradutor: Catarina Tavares Teles
Revisora: Margarida Ferreira

00:00:13.193 --> 00:00:15.299
Gostava de vos contar uma história

00:00:15.299 --> 00:00:18.580
que liga o conhecido
incidente de privacidade

00:00:18.580 --> 00:00:21.111
que envolveu Adão e Eva,

00:00:21.111 --> 00:00:24.576
e a mudança notável nas fronteiras

00:00:24.576 --> 00:00:26.443
entre o público e o privado,

00:00:26.443 --> 00:00:28.918
que ocorreu nos últimos 10 anos.

00:00:28.918 --> 00:00:30.530
Vocês conhecem o incidente.

00:00:30.530 --> 00:00:32.479
Um dia, no jardim do Éden,

00:00:32.479 --> 00:00:35.389
Adão e Eva apercebem-se de que estão nus.

00:00:35.389 --> 00:00:37.136
Ficam doidos.

00:00:37.136 --> 00:00:38.960
E o resto é história.

00:00:39.817 --> 00:00:44.176
Nos dias de hoje, provavelmente,
Adão e Eva agiriam de forma diferente

00:00:44.176 --> 00:00:46.634
[ @Adão A noite ontem foi baril! 
Adorei a maçã LOL]

00:00:46.634 --> 00:00:49.135
[@Eva ya... fofo, onde estão
as minhas calças?]

00:00:49.155 --> 00:00:51.486
Hoje, revelamos "online" mais informações

00:00:51.486 --> 00:00:54.230
sobre nós próprios do que nunca.

00:00:54.230 --> 00:00:56.372
Essas informações todas sobre nós

00:00:56.372 --> 00:00:58.519
estão a ser recolhidas por organizações.

00:00:58.519 --> 00:01:01.697
Hoje, há muito a ganhar e a beneficiar

00:01:01.697 --> 00:01:04.114
com esta análise maciça 
de informações pessoais,

00:01:04.114 --> 00:01:06.120
ou seja, os metadados,

00:01:06.120 --> 00:01:08.593
mas há também complexos inconvenientes,

00:01:08.593 --> 00:01:11.568
que advêm de abdicarmos 
da nossa privacidade.

00:01:12.250 --> 00:01:14.905
A minha história 
é sobre esses inconvenientes.

00:01:16.305 --> 00:01:18.670
Começamos com uma observação 
que, quanto a mim,

00:01:18.670 --> 00:01:21.654
se tornou cada vez mais clara 
nos últimos anos:

00:01:21.654 --> 00:01:23.865
quaisquer informações pessoais

00:01:23.865 --> 00:01:26.255
podem tornar-se em informações sensíveis.

00:01:26.255 --> 00:01:29.437
No ano 2000, tiraram-se em todo o mundo

00:01:29.437 --> 00:01:32.101
cerca de 100 mil milhões de fotos,

00:01:32.101 --> 00:01:35.100
mas apenas uma proporção 
minúscula dessas fotos

00:01:35.100 --> 00:01:37.164
foram carregadas na Internet.

00:01:37.164 --> 00:01:40.325
Em 2010, só no Facebook, num único mês,

00:01:40.325 --> 00:01:43.614
foram carregadas 2500 milhões de fotos,

00:01:43.614 --> 00:01:45.705
a maior parte delas identificadas.

00:01:45.705 --> 00:01:47.614
No mesmo período de tempo,

00:01:47.614 --> 00:01:52.255
a capacidade, por parte dos computadores,
de reconhecer pessoas em fotos

00:01:52.255 --> 00:01:55.178
aumentou para o triplo.

00:01:56.304 --> 00:01:57.907
O que acontece quando combinamos

00:01:57.907 --> 00:01:59.380
todas as tecnologias?

00:01:59.380 --> 00:02:01.904
a crescente disponibilidade 
de dados faciais;

00:02:01.904 --> 00:02:05.514
a crescente capacidade 
de os computadores reconhecerem caras;

00:02:05.514 --> 00:02:07.667
e também a computação em nuvem,

00:02:07.667 --> 00:02:09.765
que fornece a qualquer pessoa neste teatro

00:02:09.765 --> 00:02:11.430
o tipo de poder computacional

00:02:11.430 --> 00:02:14.897
que, há uns anos, estava apenas 
na posse de agências secretas;

00:02:14.897 --> 00:02:16.390
e a computação omnipresente,

00:02:16.390 --> 00:02:19.549
que permite que o meu telemóvel, 
— que não é um super computador —

00:02:19.549 --> 00:02:20.944
se ligue à Internet

00:02:20.944 --> 00:02:23.335
e realize centenas de milhares

00:02:23.335 --> 00:02:26.310
de medições faciais em poucos segundos.

00:02:26.310 --> 00:02:28.450
Bem, nós presumimos que o resultado

00:02:28.450 --> 00:02:30.542
desta combinação de tecnologias

00:02:30.542 --> 00:02:32.249
será uma mudança radical

00:02:32.249 --> 00:02:35.573
nas nossas noções 
de privacidade e anonimato.

00:02:35.762 --> 00:02:37.766
Para testar isso, fizemos uma experiência

00:02:37.766 --> 00:02:39.906
no campus da Universidade 
de Carnegie Mellon.

00:02:39.906 --> 00:02:42.262
Pedimos a estudantes, 
que estavam a passar,

00:02:42.262 --> 00:02:43.727
para participarem num estudo,

00:02:43.727 --> 00:02:46.412
e tirámos-lhes uma fotografia 
com uma "webcam".

00:02:46.412 --> 00:02:49.242
Pedimos-lhes para preencherem 
um questionário num portátil.

00:02:49.242 --> 00:02:51.193
Enquanto eles preenchiam o questionário,

00:02:51.193 --> 00:02:53.656
carregámos a fotografia numa
nuvem de computação

00:02:53.656 --> 00:02:55.964
e iniciámos um processo 
de reconhecimento facial

00:02:55.964 --> 00:02:58.160
para combinar essa foto 
com uma base de dados

00:02:58.160 --> 00:03:00.467
de algumas centenas de milhares de imagens

00:03:00.467 --> 00:03:03.349
que tínhamos descarregado 
dos perfis no Facebook.

00:03:04.440 --> 00:03:07.970
Quando os participantes chegavam 
à ultima página do questionário,

00:03:07.970 --> 00:03:10.612
a página tinha-se alterado dinamicamente

00:03:10.612 --> 00:03:12.804
com as dez fotografias mais compatíveis

00:03:12.804 --> 00:03:15.238
que o programa de reconhecimento 
tinha encontrado.

00:03:15.238 --> 00:03:17.720
Pedimos aos participantes para indicar

00:03:17.720 --> 00:03:20.163
se se conseguiam encontrar nas fotos.

00:03:21.192 --> 00:03:23.186
Veem o indivíduo?

00:03:24.843 --> 00:03:27.669
O computador conseguiu ver, 
e de facto fê-lo

00:03:27.669 --> 00:03:29.827
para um em cada três participantes.

00:03:29.827 --> 00:03:32.850
Portanto, podemos começar 
com uma face anónima

00:03:32.850 --> 00:03:36.372
"offline" ou "online", 
e podemos usar o reconhecimento facial

00:03:36.372 --> 00:03:38.798
para dar um nome a essa face anónima,

00:03:38.798 --> 00:03:40.859
graças aos dados das redes sociais.

00:03:40.859 --> 00:03:42.721
Aqui há uns anos, fizemos outra coisa.

00:03:42.731 --> 00:03:44.792
Começámos com dados de redes sociais,

00:03:44.792 --> 00:03:46.814
que combinámos estatisticamente

00:03:46.814 --> 00:03:49.659
com dados da Segurança Social 
do governo dos EUA,

00:03:49.659 --> 00:03:52.888
e acabámos por prever 
números da segurança social,

00:03:52.888 --> 00:03:56.286
que, nos EUA, são informações 
extremamente confidenciais.

00:03:56.326 --> 00:03:58.714
Percebem onde quero chegar com isto?

00:03:58.714 --> 00:04:01.750
Portanto, se combinarmos os dois estudos,

00:04:01.750 --> 00:04:03.291
a pergunta passa a ser:

00:04:03.291 --> 00:04:05.573
Podemos começar com uma face

00:04:05.573 --> 00:04:07.436
e, usando o reconhecimento facial,

00:04:07.436 --> 00:04:10.619
encontrar um nome e informações 
disponíveis publicamente

00:04:10.619 --> 00:04:12.837
acerca desse nome e dessa pessoa?

00:04:12.837 --> 00:04:15.040
A partir dessa informação 
pública disponível,

00:04:15.040 --> 00:04:17.146
inferir as que não estão disponíveis,

00:04:17.146 --> 00:04:18.581
muito mais confidenciais,

00:04:18.581 --> 00:04:20.349
que poderemos relacionar com a face?

00:04:20.349 --> 00:04:22.235
A resposta é: "Sim, podemos e fizemos".

00:04:22.235 --> 00:04:24.442
É claro, a precisão vai ficando pior.

00:04:24.442 --> 00:04:25.815
[27% (em 4 tentativas)]

00:04:25.815 --> 00:04:29.204
Mas, de facto, até decidimos desenvolver
uma aplicação para iPhone

00:04:29.204 --> 00:04:31.881
que usa a câmara interna do telemóvel

00:04:31.881 --> 00:04:35.509
para tirar uma fotografia do participante
e, depois, carregá-la para a nuvem.

00:04:35.509 --> 00:04:37.430
Faz em tempo real o que descrevi:

00:04:37.430 --> 00:04:39.841
procura uma compatibilidade, 
informações públicas,

00:04:39.841 --> 00:04:41.809
tenta inferir informações confidenciais,

00:04:41.809 --> 00:04:44.410
e depois envia de novo para o telemóvel

00:04:44.410 --> 00:04:47.695
para que fique sobreposto
na cara do participante.

00:04:47.695 --> 00:04:49.777
É um exemplo de "realidade aumentada",

00:04:49.777 --> 00:04:52.542
provavelmente um exemplo assustador
de realidade aumentada.

00:04:52.742 --> 00:04:55.396
Não desenvolvemos 
a aplicação para a tornar disponível,

00:04:55.396 --> 00:04:57.651
mas apenas como validação do conceito.

00:04:57.651 --> 00:04:59.717
Peguem nestas tecnologias

00:04:59.717 --> 00:05:01.668
e levem-nas até ao seu extremo lógico.

00:05:01.668 --> 00:05:04.273
Imaginem um futuro 
em que estranhos à nossa volta

00:05:04.273 --> 00:05:06.650
vão olhar para nós 
com os seus óculos Google

00:05:06.650 --> 00:05:08.967
ou, um dia, com as suas lentes de contacto,

00:05:08.967 --> 00:05:13.110
e vão usar sete ou oito dados sobre nós

00:05:13.110 --> 00:05:15.626
para deduzir tudo o resto

00:05:15.626 --> 00:05:18.276
que pode ser conhecida sobre nós.

00:05:18.276 --> 00:05:22.147
Com o que é que se vai parecer 
este futuro sem segredos?

00:05:22.861 --> 00:05:24.920
Devemo-nos importar?

00:05:25.358 --> 00:05:26.859
Podemos querer acreditar

00:05:26.859 --> 00:05:29.746
que o futuro, com tanta riqueza de dados,

00:05:29.746 --> 00:05:32.736
será um futuro sem mais preconceitos,

00:05:32.736 --> 00:05:35.948
mas o facto de termos tanta informação

00:05:35.948 --> 00:05:39.663
não significa que vamos tomar 
decisões mais objetivas.

00:05:39.663 --> 00:05:42.367
Noutra experiência, 
apresentámos aos participantes

00:05:42.367 --> 00:05:44.927
informações sobre candidatos 
potenciais a um emprego.

00:05:44.927 --> 00:05:47.848
Incluímos, nestas informações, 
algumas referências

00:05:47.848 --> 00:05:50.913
a informações engraçadas, 
absolutamente legais,

00:05:50.913 --> 00:05:52.969
mas talvez um pouco embaraçosas,

00:05:52.969 --> 00:05:55.292
que o indivíduo tinha 
publicado na Internet.

00:05:55.292 --> 00:05:57.307
Curiosamente, entre os participantes,

00:05:57.307 --> 00:06:00.390
alguns tinham publicado "online" 
informações semelhantes,

00:06:00.390 --> 00:06:02.247
outros não.

00:06:02.733 --> 00:06:04.634
Que grupo acham

00:06:04.634 --> 00:06:08.806
que avaliou mais severamente o indivíduo?

00:06:09.748 --> 00:06:11.305
Paradoxalmente, foi o grupo

00:06:11.305 --> 00:06:13.372
que tinha colocado 
informações semelhantes.

00:06:13.372 --> 00:06:15.904
Um exemplo de dissonância moral.

00:06:16.371 --> 00:06:17.835
Podem estar a pensar:

00:06:17.835 --> 00:06:19.366
"Isto não se aplica a mim,

00:06:19.366 --> 00:06:21.509
"porque eu não tenho nada a esconder."

00:06:21.509 --> 00:06:23.753
Mas, na realidade, a privacidade

00:06:23.753 --> 00:06:27.324
não se trata de ter coisas 
negativas a esconder.

00:06:27.648 --> 00:06:29.954
Imaginem que são o diretor 
de Recursos Humanos

00:06:29.954 --> 00:06:32.653
duma certa organização, 
recebem currículos,

00:06:32.653 --> 00:06:35.317
e decidem procurar mais informações
sobre os candidatos.

00:06:35.317 --> 00:06:37.939
Por isso, procuram os nomes no Google

00:06:37.939 --> 00:06:39.903
e, num determinado universo,

00:06:39.903 --> 00:06:42.340
encontram esta informação.

00:06:42.340 --> 00:06:46.348
Ou, num universo paralelo, 
encontram esta informação.

00:06:46.462 --> 00:06:49.322
Acham que a probabilidade 
de chamarem um ou outro candidato

00:06:49.322 --> 00:06:52.068
para uma entrevista seria a mesma?

00:06:52.468 --> 00:06:54.407
Se concordam, então não são

00:06:54.407 --> 00:06:56.465
como os empregadores norte-americanos

00:06:56.465 --> 00:07:00.200
que tomaram parte na nossa experiência
— fizemos isso mesmo.

00:07:00.200 --> 00:07:03.392
Criámos perfis no Facebook, 
manipulámos características

00:07:03.392 --> 00:07:06.186
e depois enviámos currículos 
para empresas nos EUA.

00:07:06.586 --> 00:07:08.256
Detetámos, monitorizámos

00:07:08.256 --> 00:07:10.553
se elas investigaram os nossos candidatos,

00:07:10.553 --> 00:07:12.471
e se agiram baseando-se nas informações

00:07:12.471 --> 00:07:14.666
que encontraram nas redes sociais. 
E agiram.

00:07:14.666 --> 00:07:17.910
A discriminação acontecia
a partir das redes sociais

00:07:17.910 --> 00:07:20.011
entre candidatos com habilitações iguais.

00:07:20.460 --> 00:07:24.149
Os publicitários gostariam 
que acreditássemos

00:07:24.149 --> 00:07:26.275
que todas as nossas informações pessoais

00:07:26.275 --> 00:07:29.148
serão sempre usadas em nosso favor.

00:07:29.986 --> 00:07:33.387
Mas pensem novamente. 
Por que razão será sempre assim?

00:07:33.387 --> 00:07:36.052
Num filme que saiu há uns anos,

00:07:36.052 --> 00:07:38.566
o "Relatório Minoritário", 
existe uma cena famosa

00:07:38.566 --> 00:07:41.332
em que Tom Cruise anda a passear 
num centro comercial

00:07:41.332 --> 00:07:43.689
e, à sua volta, aparece

00:07:43.689 --> 00:07:46.791
publicidade holográfica personalizada.

00:07:47.190 --> 00:07:50.180
Esse filme passa-se em 2054,

00:07:50.180 --> 00:07:51.841
daqui a cerca de 40 anos.

00:07:51.841 --> 00:07:54.520
Por mais empolgante 
que essa tecnologia pareça,

00:07:54.520 --> 00:07:57.137
subestima largamente

00:07:57.137 --> 00:08:00.544
a quantidade de informações 
que as organizações recolhem sobre nós

00:08:00.544 --> 00:08:02.132
e a maneira como a podem usar

00:08:02.132 --> 00:08:05.139
para nos influenciar duma maneira 
que nem sequer vamos detetar.

00:08:05.139 --> 00:08:07.118
Como exemplo, eis outra experiência

00:08:07.118 --> 00:08:09.649
que ainda está a decorrer, 
ainda não está finalizada.

00:08:09.649 --> 00:08:12.250
Imaginem que uma organização tem acesso

00:08:12.250 --> 00:08:13.928
à nossa lista de amigos no Facebook

00:08:13.928 --> 00:08:15.853
e, através de um qualquer algoritmo,

00:08:15.853 --> 00:08:19.149
consegue detetar dois 
dos nossos amigos mais chegados.

00:08:19.634 --> 00:08:21.714
Depois criam, em tempo real,

00:08:21.714 --> 00:08:24.490
um compósito facial desses dois amigos.

00:08:24.490 --> 00:08:27.654
Estudos anteriores ao nosso 
mostram que as pessoas

00:08:27.654 --> 00:08:32.150
já não se reconhecem a si mesmos
nesses compósitos faciais,

00:08:32.390 --> 00:08:35.230
mas reagem a eles duma maneira positiva.

00:08:35.230 --> 00:08:38.514
Por isso, da próxima vez que estiverem 
à procura de um certo produto

00:08:38.514 --> 00:08:41.397
e existir uma campanha 
sugerindo que compre aquele,

00:08:41.397 --> 00:08:44.208
não será um apresentador qualquer.

00:08:44.208 --> 00:08:46.331
será um dos nossos amigos,

00:08:46.331 --> 00:08:49.510
e nós nem percebemos 
que isso está a acontecer.

00:08:49.510 --> 00:08:51.571
Neste momento, o problema

00:08:51.571 --> 00:08:54.785
é que os atuais mecanismos 
políticos que temos

00:08:54.785 --> 00:08:57.976
para nos protegermos do abuso 
de informações pessoais

00:08:57.976 --> 00:09:01.830
funcionam como levar uma faca 
para um duelo de pistolas.

00:09:01.830 --> 00:09:03.987
Um destes mecanismos é a transparência,

00:09:03.987 --> 00:09:07.130
dizer às pessoas o que vão fazer 
com os seus dados.

00:09:07.130 --> 00:09:09.264
Em princípio, isso é uma coisa boa.

00:09:09.264 --> 00:09:12.941
É necessário, mas não é suficiente.

00:09:12.941 --> 00:09:16.486
A transparência pode ser mal direcionada.

00:09:16.486 --> 00:09:18.657
Podem dizer às pessoas o que vão fazer,

00:09:18.657 --> 00:09:20.680
e, mesmo assim, incitá-las

00:09:20.680 --> 00:09:23.541
a revelar quantidades arbitrárias 
de informações pessoais.

00:09:23.541 --> 00:09:26.912
Ainda noutra experiência, 
— esta com estudantes —

00:09:26.912 --> 00:09:29.428
pedimos para fornecerem informações

00:09:29.428 --> 00:09:31.364
sobre o seu comportamento no campus,

00:09:31.364 --> 00:09:33.761
incluindo algumas questões 
sensíveis, como esta.

00:09:33.761 --> 00:09:35.573
[Alguma vez copiaram num exame?]

00:09:35.573 --> 00:09:37.501
A um grupo dissemos:

00:09:37.501 --> 00:09:40.760
"As respostas só serão vistas 
por outros estudantes".

00:09:40.760 --> 00:09:41.864
A outro grupo dissemos:

00:09:41.864 --> 00:09:45.140
"As respostas vão ser vistas 
pelos outros estudantes e professores".

00:09:45.140 --> 00:09:47.635
Transparência. Notificação. 
Isso resultou.

00:09:47.635 --> 00:09:49.261
O primeiro grupo de participantes

00:09:49.261 --> 00:09:52.201
estava mais disposto a revelar
as informações do que o outro.

00:09:52.201 --> 00:09:53.426
Faz sentido, não faz?

00:09:53.426 --> 00:09:55.128
Mas depois induzimo-los em erro.

00:09:55.128 --> 00:09:57.533
Repetimos a experiência 
com os mesmos dois grupos,

00:09:57.533 --> 00:09:59.817
mas desta vez adicionando um atraso

00:09:59.817 --> 00:10:02.723
entre a altura em que lhes dissemos

00:10:02.723 --> 00:10:05.510
como iríamos usar os dados deles

00:10:05.510 --> 00:10:08.782
e a altura em que começámos
a fzer as perguntas.

00:10:09.382 --> 00:10:11.895
Que atraso acham que tivemos de adicionar

00:10:11.895 --> 00:10:16.508
no sentido de anular o efeito inibitório

00:10:16.508 --> 00:10:19.833
de saber que os membros da faculdade 
iriam ver as suas respostas?

00:10:20.810 --> 00:10:22.400
Dez minutos?

00:10:22.400 --> 00:10:23.700
Cinco minutos?

00:10:23.700 --> 00:10:25.380
Um minuto?

00:10:25.380 --> 00:10:27.248
Que tal quinze segundos?

00:10:27.248 --> 00:10:29.659
Quinze segundos chegaram
para que os dois grupos

00:10:29.659 --> 00:10:31.818
revelassem a mesma 
quantidade de informações,

00:10:31.818 --> 00:10:34.107
como se o segundo grupo 
já não se importasse

00:10:34.107 --> 00:10:37.229
com o facto de os membros da faculdade
irem ler as suas respostas.

00:10:37.229 --> 00:10:40.289
Tenho que reconhecer 
que esta palestra, até agora,

00:10:40.289 --> 00:10:42.674
pode ter soado excessivamente sombria

00:10:42.674 --> 00:10:44.662
mas não é isso que me interessa.

00:10:44.662 --> 00:10:46.923
Quero partilhar convosco o facto

00:10:46.923 --> 00:10:48.847
de que existem alternativas.

00:10:48.847 --> 00:10:50.755
A maneira como estamos a fazer as coisas 

00:10:50.755 --> 00:10:52.516
não é a única maneira de as fazer

00:10:52.516 --> 00:10:56.100
e certamente também não é 
a melhor maneira de serem feitas.

00:10:56.619 --> 00:11:00.590
Quando alguém vos diz:
"As pessoas não ligam à sua privacidade",

00:11:00.590 --> 00:11:03.213
considerem se a ideia foi desenvolvida

00:11:03.213 --> 00:11:06.330
e manipulada para que elas 
não se importem.

00:11:06.330 --> 00:11:09.171
Chegar à conclusão de que 
estas manipulações ocorrem

00:11:09.171 --> 00:11:13.160
é já meio caminho andado para 
sermos capazes de nos protegermos.

00:11:13.559 --> 00:11:16.784
Quando alguém nos diz 
que a privacidade é incompatível

00:11:16.784 --> 00:11:18.671
com os benefícios dos metadados,

00:11:18.671 --> 00:11:21.213
considerem que, nos últimos 20 anos,

00:11:21.213 --> 00:11:23.137
os investigadores criaram tecnologias

00:11:23.137 --> 00:11:26.331
que permitem que, virtualmente,
qualquer transação eletrónica

00:11:26.331 --> 00:11:29.938
ocorra de uma maneira que preserve
a privacidade dos envolvidos.

00:11:30.128 --> 00:11:32.597
Podemos navegar anonimamente 
pela Internet.

00:11:32.597 --> 00:11:35.418
Podemos enviar "emails" 
que só poderão ser lidos

00:11:35.418 --> 00:11:38.880
pelo destinatário pretendido, 
nem mesmo pela NSA.

00:11:39.129 --> 00:11:42.475
Podemos até realizar extração de dados
preservando a nossa privacidade.

00:11:42.475 --> 00:11:45.951
Em suma, podemos ter os benefícios 
das grandes base de dados

00:11:45.951 --> 00:11:48.160
enquanto protegemos a nossa privacidade.

00:11:48.160 --> 00:11:51.255
É claro, estas tecnologias implicam

00:11:51.255 --> 00:11:53.144
uma mudança de custos e receitas

00:11:53.144 --> 00:11:55.785
entre os detentores de dados 
e os titulares dos dados,

00:11:55.785 --> 00:11:58.780
o que é, talvez, a razão 
de não se ouvir falar mais sobre isto.

00:11:59.760 --> 00:12:02.506
O que me traz de novo ao jardim do Éden.

00:12:02.915 --> 00:12:05.619
Há uma segunda interpretação 
da privacidade

00:12:05.619 --> 00:12:07.418
na história do jardim do Éden

00:12:07.418 --> 00:12:09.495
que não tem a ver com o facto

00:12:09.495 --> 00:12:13.482
de Adão e Eva se sentirem 
nus e envergonhados.

00:12:13.987 --> 00:12:16.873
É possível encontrar 
ecos desta interpretação

00:12:16.873 --> 00:12:19.550
em "Paraíso Perdido", de John Milton.

00:12:19.550 --> 00:12:23.557
No jardim, Adão e Eva estavam 
materialmente contentes.

00:12:23.737 --> 00:12:25.861
Eram felizes. Estavam satisfeitos.

00:12:25.861 --> 00:12:29.496
Contudo, eles não possuíam 
conhecimento nem consciência.

00:12:29.851 --> 00:12:31.741
No momento em que comem 

00:12:31.741 --> 00:12:34.472
o apropriadamente chamado
"fruto do conhecimento",

00:12:34.472 --> 00:12:36.734
descobrem-se a eles próprios.

00:12:37.277 --> 00:12:41.320
Tornam-se conscientes. 
Alcançam a autonomia.

00:12:41.320 --> 00:12:44.720
O preço a pagar, porém, é sair do jardim.

00:12:44.720 --> 00:12:47.725
Daí que a privacidade engloba igualmente

00:12:47.725 --> 00:12:50.620
os meios e o preço a pagar pela liberdade.

00:12:51.680 --> 00:12:53.666
De novo, os publicitários dizem-nos

00:12:53.666 --> 00:12:56.800
que os metadados e as redes sociais

00:12:56.800 --> 00:12:59.702
não são apenas um paraíso 
de lucro para eles,

00:12:59.702 --> 00:13:02.255
mas um Jardim do Éden 
para todos nós.

00:13:02.255 --> 00:13:03.674
Temos um conteúdo livre.

00:13:03.674 --> 00:13:06.787
Podemos jogar Angry Birds. 
Temos aplicativos direcionados.

00:13:06.787 --> 00:13:08.732
Mas, na verdade, dentro de poucos anos,

00:13:08.732 --> 00:13:11.112
as organizações vão saber tanto sobre nós

00:13:11.112 --> 00:13:13.813
que serão capazes de inferir 
os nossos desejos

00:13:13.813 --> 00:13:15.369
antes de nós os formularmos,

00:13:15.369 --> 00:13:18.454
e talvez até comprar produtos 
em nosso nome

00:13:18.454 --> 00:13:20.699
antes sequer de sabermos 
que precisamos deles.

00:13:20.699 --> 00:13:24.203
Existiu um autor inglês

00:13:24.203 --> 00:13:26.991
que previu este tipo de futuro

00:13:26.991 --> 00:13:31.425
em que trocaremos a nossa autonomia 
e liberdade pelo conforto,

00:13:31.963 --> 00:13:34.248
ainda mais do que George Orwell,

00:13:34.248 --> 00:13:36.904
O autor é, naturalmente, Aldous Huxley.

00:13:36.904 --> 00:13:39.844
Em "Admirável Mundo Novo", 
ele imagina uma sociedade

00:13:39.844 --> 00:13:41.910
em que as tecnologias que criámos,

00:13:41.910 --> 00:13:43.845
inicialmente para a liberdade,

00:13:43.845 --> 00:13:45.869
acabam por nos controlar.

00:13:46.384 --> 00:13:52.432
No entanto, no livro, ele também 
nos oferece uma saída dessa sociedade,

00:13:52.432 --> 00:13:56.413
semelhante ao caminho 
que Adão e Eva tiveram de seguir

00:13:56.413 --> 00:13:58.820
para sair do jardim.

00:13:58.730 --> 00:14:00.924
Nas palavras da personagem Selvagem,

00:14:00.924 --> 00:14:03.765
é possível recuperar 
a autonomia e a liberdade ,

00:14:03.765 --> 00:14:06.472
embora o preço a pagar seja acentuado.

00:14:07.430 --> 00:14:13.768
Então, eu acredito que uma das lutas 
decisivas dos nossos tempos

00:14:13.768 --> 00:14:17.131
será a luta pelo controlo 
dessas informações pessoais,

00:14:17.131 --> 00:14:21.806
a luta sobre se os metadados 
vão tornar-se uma força pela liberdade,

00:14:21.828 --> 00:14:25.841
em vez duma força 
que nos manipula secretamente.

00:14:27.510 --> 00:14:29.710
Neste momento, muitos de nós

00:14:29.710 --> 00:14:31.978
nem sequer sabemos 
que a luta está a acontecer,

00:14:31.978 --> 00:14:34.345
mas está, gostem ou não.

00:14:34.840 --> 00:14:37.406
Correndo o risco de fazer 
o papel da serpente,

00:14:37.406 --> 00:14:40.398
vou dizer-vos que 
as ferramentas para a luta

00:14:40.398 --> 00:14:43.160
estão aqui, a consciência 
do que está a acontecer,

00:14:43.160 --> 00:14:44.791
e nas nossas mãos,

00:14:44.791 --> 00:14:48.255
apenas a alguns cliques de distância.

00:14:48.664 --> 00:14:49.956
Obrigado.

00:14:49.956 --> 00:14:53.471
(Aplausos)

