WEBVTT
Kind: captions
Language: uk

00:00:00.000 --> 00:00:07.000
Перекладач: Roksolana Berezhanska
Утверджено: Marta Oliynyk

00:00:12.641 --> 00:00:14.995
Я б хотів розповісти Вам історію,

00:00:14.995 --> 00:00:18.171
що містить загальновідомий випадок приватності,

00:00:18.171 --> 00:00:20.940
що трапився з Адамом та Євою,

00:00:20.940 --> 00:00:24.386
і про значні зміщення меж

00:00:24.386 --> 00:00:27.072
між публічним і приватним, що відбулися

00:00:27.072 --> 00:00:28.842
протягом останніх 10 років.

00:00:28.842 --> 00:00:30.140
Ви знаєте цей випадок.

00:00:30.140 --> 00:00:33.470
Одного дня в Едемському саду Адам і Єва

00:00:33.470 --> 00:00:35.313
усвідомлюють, що вони голі.

00:00:35.313 --> 00:00:36.813
Вони шоковані.

00:00:36.813 --> 00:00:39.570
А решта - це вже історія.

00:00:39.570 --> 00:00:41.758
Сьогодні Адам і Єва

00:00:41.758 --> 00:00:44.119
напевне повели б себе по-іншому.

00:00:44.119 --> 00:00:46.387
[@Адам: вчорашня ніч була супер! ]

00:00:46.387 --> 00:00:48.260
[@Єва: егеж, крихітко, не знаєш, де мої штани? ]

00:00:48.260 --> 00:00:50.896
Ми таки видаємо набагато більше інформації

00:00:50.896 --> 00:00:54.230
про нас в Інтернеті, ніж коли-небудь раніше,

00:00:54.230 --> 00:00:55.934
і дуже багато інформації про нас

00:00:55.934 --> 00:00:58.158
збирається різними структурами.

00:00:58.158 --> 00:01:01.440
Тепер є багато вигод

00:01:01.440 --> 00:01:03.886
з масового аналізу особистої інформації

00:01:03.886 --> 00:01:05.832
чи з баз даних,

00:01:05.832 --> 00:01:08.470
але ми також платимо певну ціну

00:01:08.470 --> 00:01:11.568
за викриття приватності.

00:01:11.568 --> 00:01:15.591
І в моїй розповіді ідеться про цю ціну.

00:01:15.591 --> 00:01:18.175
Почнемо зі спостереження, яке, на мою думку,

00:01:18.175 --> 00:01:21.502
стає все чіткішим за останні кілька років.

00:01:21.502 --> 00:01:23.599
Воно полягає в тому, що будь-яка особиста інформація

00:01:23.599 --> 00:01:25.884
може стати точкою ураження.

00:01:25.884 --> 00:01:30.009
Повертаючись у 2000 рік, близько 100 млрд фотографій

00:01:30.009 --> 00:01:31.921
було зроблено по всьому світу,

00:01:31.921 --> 00:01:34.986
проте лише крихітну їх частину

00:01:34.986 --> 00:01:36.869
завантажили в Інтернет.

00:01:36.869 --> 00:01:40.230
У 2010 році лише на Facebook, всього за один місяць

00:01:40.230 --> 00:01:43.500
було завантажено 2,5 млрд фотографій,

00:01:43.500 --> 00:01:45.382
і більшість з них ідентифіковані.

00:01:45.382 --> 00:01:47.262
За той самий проміжок часу

00:01:47.262 --> 00:01:52.132
можливість комп'ютера розпізнавати людей на фотографіях

00:01:52.132 --> 00:01:55.740
покращилася втричі.

00:01:55.740 --> 00:01:57.622
Що ж відбувається, коли ви поєднуєте

00:01:57.622 --> 00:01:59.123
ці технології:

00:01:59.123 --> 00:02:01.781
збільшення доступності до особистих даних;

00:02:01.781 --> 00:02:05.429
покращення можливості комп'ютера розпізнавати обличчя;

00:02:05.429 --> 00:02:07.611
але також "хмарні" обчислення,

00:02:07.611 --> 00:02:09.499
що дає кожному присутньому

00:02:09.499 --> 00:02:11.059
певну обчислювальну потужність,

00:02:11.059 --> 00:02:12.945
яка кілька років тому була доступна

00:02:12.945 --> 00:02:14.727
лише підпільним державним структурам;

00:02:14.727 --> 00:02:16.105
і глобальні обчислення,

00:02:16.105 --> 00:02:18.997
що дозволяють моєму телефону, що не є супер-комп'ютером,

00:02:18.997 --> 00:02:20.668
під'єднуватися до інтернету

00:02:20.668 --> 00:02:23.002
і виконувати там сотні тисяч

00:02:23.002 --> 00:02:25.641
співставлень облич всього за кілька секунд?

00:02:25.641 --> 00:02:28.269
Ну, як результат, ми припускаємо,

00:02:28.269 --> 00:02:30.333
що ця комбінація технологій

00:02:30.333 --> 00:02:33.221
здійснить радикальні зміни в нашому сприйнятті

00:02:33.221 --> 00:02:35.478
понять приватності та анонімності.

00:02:35.478 --> 00:02:37.471
Щоб це перевірити, ми здійснили експеримент

00:02:37.471 --> 00:02:39.592
у студмістечку університету Карнеґі-Меллон.

00:02:39.592 --> 00:02:41.691
Ми попросили студентів, які проходили повз,

00:02:41.691 --> 00:02:43.470
взяти участь в дослідженні.

00:02:43.470 --> 00:02:46.032
Ми фотографували їх на веб-камеру

00:02:46.032 --> 00:02:48.814
і просили пройти опитування на лептопі.

00:02:48.814 --> 00:02:50.793
Поки вони заповнювали анкети,

00:02:50.793 --> 00:02:53.590
ми завантажили їхні фото до кластера хмарних обчислень

00:02:53.590 --> 00:02:55.317
та застосували систему розпізнавання облич,

00:02:55.317 --> 00:02:57.722
щоб знайти їхні відповідники в базі даних

00:02:57.722 --> 00:03:00.115
сотень тисяч фотографій,

00:03:00.115 --> 00:03:03.711
які ми завантажили з профілів соцмережі Facebook.

00:03:03.711 --> 00:03:06.970
Коли учасник дійшов до останньої сторінки

00:03:06.970 --> 00:03:10.317
опитування, оновившись, сторінка динамічно

00:03:10.317 --> 00:03:12.630
підібрала 10 найбільш відповідних фотографій,

00:03:12.630 --> 00:03:14.915
які знайшов розпізнавач.

00:03:14.915 --> 00:03:16.653
Ми попросили учасників сказати,

00:03:16.653 --> 00:03:20.773
чи знайшли вони себе на фотографіях.

00:03:20.773 --> 00:03:24.472
Ви бачите респондента?

00:03:24.472 --> 00:03:27.317
Що ж, комп'ютер побачив. І побачив

00:03:27.317 --> 00:03:29.466
кожного третього з респондентів.

00:03:29.466 --> 00:03:32.650
По суті, ми можемо почати з лиця аноніма,

00:03:32.650 --> 00:03:36.134
в мережі чи ні, і використати систему розпізнавання облич,

00:03:36.134 --> 00:03:38.494
щоб ідентифікувати обличчя цього аноніма

00:03:38.494 --> 00:03:40.602
за допомогою даних соціальних мереж.

00:03:40.602 --> 00:03:42.474
Проте кілька років тому ми діяли по-іншому.

00:03:42.474 --> 00:03:44.297
Ми розпочинали з даних соціальних мереж

00:03:44.297 --> 00:03:47.348
та поєднували їх статистично з даними

00:03:47.348 --> 00:03:49.450
уряду соціальної безпеки США.

00:03:49.450 --> 00:03:52.774
В результаті ми спрогнозували номери соціального страхування,

00:03:52.774 --> 00:03:54.286
які в Сполучених Штатах

00:03:54.286 --> 00:03:56.326
є надзвичайно конфіденційною інформацією.

00:03:56.326 --> 00:03:58.419
Чи ви бачите, до чого я веду?

00:03:58.419 --> 00:04:01.341
Тобто якщо об'єднати ці два дослідження,

00:04:01.341 --> 00:04:02.853
виникає питання:

00:04:02.853 --> 00:04:05.573
чи можемо ми почати з обличчя

00:04:05.573 --> 00:04:07.884
і, використовуючи розпізнавач облич, знайти ім'я

00:04:07.884 --> 00:04:10.553
та публічно доступну інформацію

00:04:10.553 --> 00:04:12.485
про це ім'я і цю людину

00:04:12.485 --> 00:04:14.733
і, виходячи з цієї інформації,

00:04:14.733 --> 00:04:16.775
вивести таку, яка не є публічно доступною,

00:04:16.775 --> 00:04:18.381
більш конфіденційну,

00:04:18.381 --> 00:04:19.873
яку можна знову співставити з обличчям?

00:04:19.873 --> 00:04:21.789
Відповідь - так, ми можемо. І ми це зробили.

00:04:21.789 --> 00:04:24.357
Звісно, точність погіршується.

00:04:24.357 --> 00:04:25.301
[ у 27%респондентів визначено перших 5 символів номеру соціального страхування (з 4 спроб) ]

00:04:25.301 --> 00:04:29.128
Але насправді ми навіть вирішили розробити додаток для iPhone,

00:04:29.128 --> 00:04:31.843
який застосовує фронтальну камеру телефона,

00:04:31.843 --> 00:04:33.443
щоб сфотографувати респондента

00:04:33.443 --> 00:04:34.930
і завантажити її в хмару,

00:04:34.930 --> 00:04:37.592
а потім зробити все те, що я щойно описав:

00:04:37.592 --> 00:04:39.680
шукати співпадіння, знайти публічну інформацію,

00:04:39.680 --> 00:04:41.410
спробувати вивести конфіденційну

00:04:41.410 --> 00:04:44.001
і надіслати в телефон,

00:04:44.001 --> 00:04:47.610
щоб вона відобразилася на обличчі респондента,

00:04:47.610 --> 00:04:49.511
як приклад доповненої реальності,

00:04:49.511 --> 00:04:51.962
мабуть жахаючої доповненої реальності.

00:04:51.962 --> 00:04:55.301
Насправді, ми не розробляли додаток, щоб він був доступним,

00:04:55.301 --> 00:04:57.223
просто для підтвердження концепції.

00:04:57.223 --> 00:04:59.536
Візьмемо ці технології

00:04:59.536 --> 00:05:01.373
та доведемо їх до логічної межі.

00:05:01.373 --> 00:05:04.092
Уявіть майбутнє, в якому незнайомці навколо вас

00:05:04.092 --> 00:05:06.403
дивитимуться на вас крізь Google окуляри

00:05:06.403 --> 00:05:08.710
а, одного дня, й крізь контактні лінзи,

00:05:08.710 --> 00:05:12.730
і, використовуючи 7 чи 8 пунктів вихідних даних про Вас,

00:05:12.730 --> 00:05:15.312
виведуть ще щось,

00:05:15.312 --> 00:05:17.915
що може бути про Вас відомим.

00:05:17.915 --> 00:05:22.709
Як же виглядатиме таке майбутнє без таємниць?

00:05:22.709 --> 00:05:24.673
Чи повинно нас це хвилювати?

00:05:24.673 --> 00:05:26.564
Нам би хотілося вірити,

00:05:26.564 --> 00:05:29.604
що майбутнє з таким обсягом доступної інформації

00:05:29.604 --> 00:05:32.118
буде майбутнім без упереджень,

00:05:32.118 --> 00:05:35.701
але, насправді, володіння такою кількістю інформації

00:05:35.701 --> 00:05:37.892
не означає, що наші рішення

00:05:37.892 --> 00:05:39.598
будуть об'єктивнішими.

00:05:39.598 --> 00:05:42.158
В іншому експерименті ми запропонували респондентам

00:05:42.158 --> 00:05:44.404
інформацію про потенційного працівника.

00:05:44.404 --> 00:05:47.582
Ми включили до інформації про них кілька посилань

00:05:47.582 --> 00:05:50.228
на певну смішну, цілком доступну інформацію,

00:05:50.228 --> 00:05:52.693
проте дещо таку, що осоромлює,

00:05:52.693 --> 00:05:54.713
яку респондент виклав в мережі.

00:05:54.713 --> 00:05:57.079
Цікаво, що поміж респондентів

00:05:57.079 --> 00:06:00.162
дехто оприлюднив непевну інформацію,

00:06:00.162 --> 00:06:02.524
а дехто ні.

00:06:02.524 --> 00:06:04.473
Яка з груп, на Вашу думку,

00:06:04.473 --> 00:06:09.025
більш імовірно, суворо засудить респондента?

00:06:09.025 --> 00:06:10.982
Парадоксально, але це була група,

00:06:10.982 --> 00:06:12.715
що виклала ідентичну інформацію -

00:06:12.715 --> 00:06:15.657
приклад морального дисонансу.

00:06:15.657 --> 00:06:17.407
Як Ви, мабуть, думаєте,

00:06:17.407 --> 00:06:19.109
це не про мене,

00:06:19.109 --> 00:06:21.271
оскільки мені нічого приховувати.

00:06:21.271 --> 00:06:23.753
Та насправді приватність не обов'язково означає

00:06:23.753 --> 00:06:27.429
мати щось до приховування.

00:06:27.429 --> 00:06:29.783
Уявіть, що ви директор відділу кадрів

00:06:29.783 --> 00:06:32.730
певної організації. Ви отримуєте резюме

00:06:32.730 --> 00:06:35.203
і вирішуєте пошукати більше інформації про аплікантів.

00:06:35.203 --> 00:06:37.663
Таким чином, ви гуглите їхні імена

00:06:37.663 --> 00:06:39.903
і в якомусь із всесвітів

00:06:39.903 --> 00:06:41.911
знаходите цю інформацію.

00:06:41.911 --> 00:06:46.348
Чи навіть в паралельному всесвіті, ви все ж знаходите цю інформацію.

00:06:46.348 --> 00:06:49.065
Як Ви думаєте, Ви з однаковою впевненістю

00:06:49.065 --> 00:06:51.868
запросите обох кандидатів на співбесіду?

00:06:51.868 --> 00:06:54.150
Якщо так, то Ви не є

00:06:54.150 --> 00:06:56.732
такими, як роботодавці США, які

00:06:56.732 --> 00:07:00.039
є частиною нашого експерименту, і вважають, що так чинити правильно.

00:07:00.039 --> 00:07:03.221
Ми створили профілі на Facebook, засоби маніпуляції,

00:07:03.221 --> 00:07:06.072
і почали розсилати резюме в американські компанії.

00:07:06.072 --> 00:07:07.980
Ми провели моніторинг,

00:07:07.980 --> 00:07:10.373
чи шукали вони наших кандидатів

00:07:10.373 --> 00:07:12.205
та чи зважали вони на знайдену в соціальних

00:07:12.205 --> 00:07:14.143
мережах інформацію. І вони зважали.

00:07:14.143 --> 00:07:16.244
Дискримінація, що ґрунтується на інформації з соціальних мереж,

00:07:16.244 --> 00:07:19.317
застосовувалася щодо однаково кваліфікованих кандидатів.

00:07:19.317 --> 00:07:23.892
Сучасні маркетологи хочуть, щоб ми повірили,

00:07:23.892 --> 00:07:26.161
що вся інформація про нас завжди

00:07:26.161 --> 00:07:29.434
буде використана в нашу користь.

00:07:29.434 --> 00:07:33.149
Але подумайте ще раз. Чи завжди це так?

00:07:33.149 --> 00:07:35.813
У фільмі "Особлива думка", що вийшов

00:07:35.813 --> 00:07:38.366
кілька років тому, є сцена, де

00:07:38.366 --> 00:07:40.942
Том Круз іде в торговому центрі,

00:07:40.942 --> 00:07:44.718
а навколо нього з'являються

00:07:44.718 --> 00:07:46.553
персоналізовані рекламні голограми.

00:07:46.553 --> 00:07:49.780
Дія фільму відбувається у 2054 році

00:07:49.780 --> 00:07:51.422
близько 40 років від нашого часу,

00:07:51.422 --> 00:07:54.330
і хоч як захоплююче виглядають ці технології,

00:07:54.330 --> 00:07:56.976
вони вже значно недооцінюють

00:07:56.976 --> 00:07:59.116
кількості інформації, яку організації

00:07:59.116 --> 00:08:01.599
можуть про Вас зібрати і як вони можуть нею скористатись,

00:08:01.599 --> 00:08:04.997
щоб вплинути на Вас без Вашого відома.

00:08:04.997 --> 00:08:07.100
Отже, як приклад маємо інший експеримент,

00:08:07.100 --> 00:08:09.373
який ще триває.

00:08:09.373 --> 00:08:11.692
Уявіть, що організація має доступ

00:08:11.692 --> 00:08:13.748
до списку ваших друзів на Facebook

00:08:13.748 --> 00:08:15.520
і завдяки певному алгоритму дій

00:08:15.520 --> 00:08:19.254
вона може виокремити двох улюблених,

00:08:19.254 --> 00:08:21.534
а також в реальному часі скомпонувати

00:08:21.534 --> 00:08:24.376
з них двох фоторобот.

00:08:24.376 --> 00:08:27.445
Дослідження, що передували нашим, показують, що люди

00:08:27.445 --> 00:08:30.330
більше не розпізнають навіть самих себе

00:08:30.330 --> 00:08:32.792
на фотороботах, проте реагують

00:08:32.792 --> 00:08:34.909
на такі вельми позитивно.

00:08:34.909 --> 00:08:38.324
Тому наступного разу, шукаючи певний товар,

00:08:38.324 --> 00:08:40.883
в рекламі, що пропонуватиме вам його придбати

00:08:40.883 --> 00:08:43.790
буде не просто незнайома людина.

00:08:43.790 --> 00:08:46.103
Це буде один із ваших друзів,

00:08:46.103 --> 00:08:49.406
а ви навіть не здогадуватиметесь про це.

00:08:49.406 --> 00:08:51.819
Проблема полягає в тому,

00:08:51.819 --> 00:08:54.338
що сучасні механізми захисту

00:08:54.338 --> 00:08:57.776
від зловживання персональною інформацією

00:08:57.776 --> 00:09:00.760
виглядають як ніж, принесений на перестрілку.

00:09:00.760 --> 00:09:03.673
Один із цих механізмів - прозорість,

00:09:03.673 --> 00:09:06.873
яка каже людям, яким чином ви збираєтеся використовувати їхні особисті дані.

00:09:06.873 --> 00:09:08.979
Загалом це дуже хороша і необхідна річ.

00:09:08.979 --> 00:09:12.646
Але її недостатньо.

00:09:12.646 --> 00:09:16.344
Прозорість може бути неправильно адресована.

00:09:16.344 --> 00:09:18.448
Ви можете інформувати людей про те, що ви збираєтесь робити,

00:09:18.448 --> 00:09:20.680
але, все-таки, підштовхувати їх видавати

00:09:20.680 --> 00:09:23.303
довільну кількість персональної інформації.

00:09:23.303 --> 00:09:26.189
Ми здійснили ще один експеримент, цього разу зі студентами.

00:09:26.189 --> 00:09:29.247
Ми попросили їх надати інформацію

00:09:29.247 --> 00:09:31.060
про їхню поведінку в студентському містечку,

00:09:31.060 --> 00:09:34.000
включивши доволі делікатні питання, наприклад:

00:09:34.000 --> 00:09:34.621
[ Ви коли-небудь списували на іспиті?]

00:09:34.621 --> 00:09:36.921
Одній групі студентів ми сказали:

00:09:36.921 --> 00:09:39.762
"Тільки студенти бачитимуть ваші відповіді".

00:09:39.762 --> 00:09:41.341
Іншій групі ми сказали:

00:09:41.341 --> 00:09:44.902
"Ваші відповіді бачитимуть студенти і викладачі".

00:09:44.902 --> 00:09:47.493
Прозорість. Попередження. Звісно, це спрацювало.

00:09:47.493 --> 00:09:48.900
В тому сенсі, що у першій групі піддослідних

00:09:48.900 --> 00:09:51.468
була більша ймовірність правдивості інформації.

00:09:51.468 --> 00:09:52.988
Логічно, чи не так?

00:09:52.988 --> 00:09:54.478
Але тоді ми застосували зворотню вказівку.

00:09:54.478 --> 00:09:57.238
Ми повторили експеримент з тими ж групами,

00:09:57.238 --> 00:09:59.665
проте цього разу додали затримку

00:09:59.665 --> 00:10:02.600
в часі, між тим, щоб повідомити опонентам,

00:10:02.600 --> 00:10:04.680
як ми будемо використовувати їх дані,

00:10:04.680 --> 00:10:09.068
і часом, коли вони почали відповідати.

00:10:09.068 --> 00:10:11.629
Наскільки довгою, на Вашу думку, мала бути затримка,

00:10:11.629 --> 00:10:16.242
щоб звести нанівець ефект придушення

00:10:16.242 --> 00:10:19.653
факту, що викладачі побачать їхні відповіді?

00:10:19.653 --> 00:10:21.433
10 хвилин?

00:10:21.433 --> 00:10:23.224
5 хвилин?

00:10:23.224 --> 00:10:25.000
1 хвилина?

00:10:25.000 --> 00:10:27.049
Як щодо 15 секунд?

00:10:27.049 --> 00:10:29.717
15 секунд було достатньо, щоб дві групи опонентів

00:10:29.717 --> 00:10:31.285
розкрили таку ж кількість інформації,

00:10:31.285 --> 00:10:34.031
при тому, що другу групу не хвилювало те,

00:10:34.031 --> 00:10:36.687
що викладачі побачать відповіді.

00:10:36.687 --> 00:10:40.023
Мушу визнати, що моя промова

00:10:40.023 --> 00:10:42.503
мабуть звучить надзвичайно гнітюче,

00:10:42.503 --> 00:10:44.224
але це не моя точка зору.

00:10:44.224 --> 00:10:46.923
Насправді, я хочу сказати вам про те,

00:10:46.923 --> 00:10:48.695
що є альтернатива.

00:10:48.695 --> 00:10:51.194
Спосіб, до якого ми вдаємося, не єдиний

00:10:51.194 --> 00:10:54.231
і не найкращий, порівняно зі способом,

00:10:54.231 --> 00:10:56.258
яким ми можемо їх робити.

00:10:56.258 --> 00:11:00.429
Коли хтось каже вам: "Люди не зважають на приватність",

00:11:00.429 --> 00:11:03.071
подумайте про те, чи правилами гри було обумовлено,

00:11:03.071 --> 00:11:05.795
що вони не можуть турбуватися про приватність

00:11:05.795 --> 00:11:09.057
і усвідомлюючи, що такі маніпуляції тапляються

00:11:09.057 --> 00:11:10.664
на половині шляху до

00:11:10.664 --> 00:11:12.922
здатності захисти себе.

00:11:12.922 --> 00:11:16.632
Коли хтось каже вам, що приватність несумісна

00:11:16.632 --> 00:11:18.481
з перевагами великих баз даних,

00:11:18.481 --> 00:11:20.954
подумайте про те, що протягом останніх 20 років

00:11:20.954 --> 00:11:22.871
дослідники створили технології,

00:11:22.871 --> 00:11:26.189
що дозволяють віртуальним електронним транзакціям

00:11:26.189 --> 00:11:29.938
відбуватися в режимі збереження приватності.

00:11:29.938 --> 00:11:32.493
Ми можемо анонімно переглядати інтернет-сторінки.

00:11:32.493 --> 00:11:35.171
Ми можемо надсилати приватні електронні листи,

00:11:35.171 --> 00:11:38.880
до яких не має доступу навіть Управління національної безпеки.

00:11:38.880 --> 00:11:41.877
Ми навіть можемо мати глибокий аналіз приватних даних.

00:11:41.877 --> 00:11:45.771
Іншими словами, ми можемо отримувати вигоду від великих баз даних,

00:11:45.771 --> 00:11:47.903
одночасно захищаючи свою приватність.

00:11:47.903 --> 00:11:51.694
Звісно, такі технології передбачають рух

00:11:51.694 --> 00:11:53.240
вартості та доходів

00:11:53.240 --> 00:11:55.347
між сервісами зберігання та власниками даних,

00:11:55.347 --> 00:11:58.800
саме тому ви, мабуть, не чули багато про них.

00:11:58.800 --> 00:12:02.506
Це повертає мене до Едемського саду.

00:12:02.506 --> 00:12:05.286
Існує інше трактування приватності

00:12:05.286 --> 00:12:07.095
з історії Едемського саду,

00:12:07.095 --> 00:12:09.191
яке не має нічого спільного з тим,

00:12:09.191 --> 00:12:11.416
що Адам і Єва побачили, що вони голі

00:12:11.416 --> 00:12:13.797
і відчули сором.

00:12:13.797 --> 00:12:16.578
Ви можете знайти схоже трактування

00:12:16.578 --> 00:12:19.360
у "Втраченому раї" Джона Мільтона.

00:12:19.360 --> 00:12:23.557
В саду Адам і Єва природньо задоволені.

00:12:23.557 --> 00:12:25.661
Вони щасливі. Вони задоволені.

00:12:25.661 --> 00:12:27.954
Тим не менше, їм бракує знань

00:12:27.954 --> 00:12:29.594
та самосвідомості.

00:12:29.594 --> 00:12:32.913
У мить, коли вони куштують

00:12:32.913 --> 00:12:34.206
плід пізнання,

00:12:34.206 --> 00:12:36.811
вони усвідомлюють себе.

00:12:36.811 --> 00:12:40.842
Вони стають свідомими. Вони досягають автономії.

00:12:40.842 --> 00:12:43.968
Проте ціна розплати - вигнання з раю.

00:12:43.968 --> 00:12:47.849
Тому приватність є засобом і ціною,

00:12:47.849 --> 00:12:50.811
яку слід платити за свободу.

00:12:50.811 --> 00:12:53.581
Знову ж таки, маркетологи кажуть нам,

00:12:53.581 --> 00:12:56.600
що великі бази даних і соціальні медіа

00:12:56.600 --> 00:12:59.579
є не тільки раєм прибутків для них,

00:12:59.579 --> 00:13:02.036
але Едемським садом для всіх нас.

00:13:02.036 --> 00:13:03.274
Ми отримуємо безкоштовний контент.

00:13:03.274 --> 00:13:06.397
Ми отримуємо гру Angry Birds. Ми отримуємо цільові додатки.

00:13:06.397 --> 00:13:09.294
Але через кілька років організації

00:13:09.294 --> 00:13:10.903
знатимуть про нас стільки,

00:13:10.903 --> 00:13:13.613
що вони будуть здатні вивести наші бажання

00:13:13.613 --> 00:13:15.817
перед тим, як ми їх сформуємо, а можливо й

00:13:15.817 --> 00:13:18.264
придбати продукти від нашого імені

00:13:18.264 --> 00:13:20.538
до того, як ми знатимемо, що вони нам потрібні.

00:13:20.538 --> 00:13:23.775
Був один англійський письменник,

00:13:23.775 --> 00:13:26.820
що очікував майбутнього,

00:13:26.820 --> 00:13:28.225
в якому ми віддамо

00:13:28.225 --> 00:13:31.773
свою автономність і свободу заради комфорту.

00:13:31.773 --> 00:13:33.934
Навіть більше, ніж Джордж Орвелл,

00:13:33.934 --> 00:13:36.695
цей автор, звісно ж, Олдос Гакслі.

00:13:36.695 --> 00:13:39.549
У романі "Чудовий новий світ" він описав суспільство,

00:13:39.549 --> 00:13:41.720
в якому технології, що первинно створені

00:13:41.720 --> 00:13:43.579
для свободи,

00:13:43.579 --> 00:13:46.146
в результаті поневолюють нас.

00:13:46.146 --> 00:13:50.937
Проте в книзі він також пропонує нам вихід

00:13:50.937 --> 00:13:54.375
з цього суспільства, схожий до шляху,

00:13:54.375 --> 00:13:58.330
який Адам і Єва пішли, щоб покинути рай.

00:13:58.330 --> 00:14:00.477
Словами Дикуна (героя роману),

00:14:00.477 --> 00:14:03.546
відновлення автономності і свободи можливе,

00:14:03.546 --> 00:14:06.225
проте ціна надмірно висока.

00:14:06.225 --> 00:14:11.940
Тому я вважаю, що одним із вирішальних боїв

00:14:11.940 --> 00:14:14.503
нашого часу буде бій

00:14:14.503 --> 00:14:16.890
за отримання контролю над особистими даними,

00:14:16.890 --> 00:14:20.397
боротьба, в якій великі бази даних стануть силою

00:14:20.397 --> 00:14:21.686
для свободи,

00:14:21.686 --> 00:14:26.432
а не такою, що приховано маніпулюватиме нами.

00:14:26.432 --> 00:14:29.025
Саме зараз багато з нас

00:14:29.025 --> 00:14:31.778
навіть не усвідомлюють, що боротьба вже триває.

00:14:31.778 --> 00:14:34.450
Проте факт залишається фактом, подобається вам це чи ні.

00:14:34.450 --> 00:14:37.254
І, ризикуючи зіграти змія,

00:14:37.254 --> 00:14:40.151
скажу вам, що засоби боротьби

00:14:40.151 --> 00:14:43.160
є тут - усвідомлення того, що відбувається.

00:14:43.160 --> 00:14:44.515
І вони в ваших руках.

00:14:44.515 --> 00:14:48.255
Всього кілька кліків мишкою.

00:14:48.255 --> 00:14:49.737
Дякую.

00:14:49.737 --> 00:14:54.214
(Оплески)

