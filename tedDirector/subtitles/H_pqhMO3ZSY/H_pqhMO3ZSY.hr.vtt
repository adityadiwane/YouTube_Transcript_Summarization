WEBVTT
Kind: captions
Language: hr

00:00:00.000 --> 00:00:07.000
Prevoditelj: Davorin Jelačić
Recezent: Tilen Pigac - EFZG

00:00:12.641 --> 00:00:14.995
Želim vam ispričati priču

00:00:14.995 --> 00:00:18.171
koja povezuje slavan incident s privatnošću

00:00:18.171 --> 00:00:20.940
Adama i Eve,

00:00:20.940 --> 00:00:24.386
te izuzetno pomicanje granica

00:00:24.386 --> 00:00:27.072
između javnog i privatnog koje se dogodilo

00:00:27.072 --> 00:00:28.842
u zadnjih 10 godina.

00:00:28.842 --> 00:00:30.140
Znate za taj incident.

00:00:30.140 --> 00:00:33.470
Jednoga dana u Edenskom vrtu, Adam i Eva

00:00:33.470 --> 00:00:35.313
shvate da su goli.

00:00:35.313 --> 00:00:36.813
Izbezume se.

00:00:36.813 --> 00:00:39.570
I ostalo je povijest.

00:00:39.570 --> 00:00:41.758
Danas, Adam i Eva

00:00:41.758 --> 00:00:44.119
bi vjerojatno postupili drukčije.

00:00:44.119 --> 00:00:46.387
[@Adam Sinoć ludilo! jabuka bila njam LOL]

00:00:46.387 --> 00:00:48.260
[@Eva tako je.. dušo, imaš ideju gdje su mi nestale hlače?]

00:00:48.260 --> 00:00:50.896
Otkrivamo puno više informacija

00:00:50.896 --> 00:00:54.230
o sebi online nego ikada ranije,

00:00:54.230 --> 00:00:55.934
i mnoge informacije o nama

00:00:55.934 --> 00:00:58.158
prikupljaju razne organizacije.

00:00:58.158 --> 00:01:01.440
Koristi od ovakve masovne analize
osobnih informacija,

00:01:01.440 --> 00:01:03.886
velike količine podataka,

00:01:03.886 --> 00:01:05.832
su značajne

00:01:05.832 --> 00:01:08.470
ali puno toga i gubimo

00:01:08.470 --> 00:01:11.568
time što se odričemo svoje privatnosti.

00:01:11.568 --> 00:01:15.591
Moja priča je o toj transakciji.

00:01:15.591 --> 00:01:18.175
Počinjemo s opažanjem koje je, po meni,

00:01:18.175 --> 00:01:21.502
sve očiglednije u posljednjih
nekoliko godina,

00:01:21.502 --> 00:01:23.599
a to je da svaka osobna informacija

00:01:23.599 --> 00:01:25.884
može postati osjetljiva informacija.

00:01:25.884 --> 00:01:30.009
U 2000. godini, nekih
100 milijardi fotografija

00:01:30.009 --> 00:01:31.921
je uslikano širom svijeta,

00:01:31.921 --> 00:01:34.986
ali je samo maleni dio njih

00:01:34.986 --> 00:01:36.869
postavljen na internet.

00:01:36.869 --> 00:01:40.230
U 2010-toj, samo na Facebooku,
u jednom mjesecu,

00:01:40.230 --> 00:01:43.500
postavljeno je 2,5 milijardi slika,

00:01:43.500 --> 00:01:45.382
većina s identifikacijskim podatcima.

00:01:45.382 --> 00:01:47.262
U istom razdoblju,

00:01:47.262 --> 00:01:52.132
mogućnost računala da prepozna
ljude na fotografijama

00:01:52.132 --> 00:01:55.740
poboljšala se za tri reda veličine.

00:01:55.740 --> 00:01:57.622
Što se dogodi kad kombiniramo

00:01:57.622 --> 00:01:59.123
te tehnologije:

00:01:59.123 --> 00:02:01.781
povećana dostupnost podataka o licima;

00:02:01.781 --> 00:02:05.429
poboljšanje prepoznavanja lica od strane računala;

00:02:05.429 --> 00:02:07.611
ali također i računalnih oblaka,

00:02:07.611 --> 00:02:09.499
koji svakome u ovoj dvorani

00:02:09.499 --> 00:02:11.059
daju računalnu snagu

00:02:11.059 --> 00:02:12.945
kakva je prije nekoliko godina bila dostupna samo

00:02:12.945 --> 00:02:14.727
agencijama s 3 slova;

00:02:14.727 --> 00:02:16.105
zatim sveprisutno računalstvo,

00:02:16.105 --> 00:02:18.997
koje omogućava mom telefonu
koji nije superračunalo,

00:02:18.997 --> 00:02:20.668
da se spoji na internet

00:02:20.668 --> 00:02:23.002
i ondje obavi analizu

00:02:23.002 --> 00:02:25.641
stotine tisuća lica u nekoliko sekundi?

00:02:25.641 --> 00:02:28.269
Zaključujemo da će rezultat

00:02:28.269 --> 00:02:30.333
kombiniranja ovih tehnologija

00:02:30.333 --> 00:02:33.221
predstavljati radikalnu promjenu naših shvaćanja

00:02:33.221 --> 00:02:35.478
privatnosti i anonimnosti.

00:02:35.478 --> 00:02:37.471
Da to provjerimo, provelii smo eksperiment

00:02:37.471 --> 00:02:39.592
u kampusu sveučilišta Carnegie Mellon.

00:02:39.592 --> 00:02:41.691
Zamolili smo studente u prolazu

00:02:41.691 --> 00:02:43.470
za sudjelovanje u studiji,

00:02:43.470 --> 00:02:46.032
slikali ih web kamerom,

00:02:46.032 --> 00:02:48.814
i tražili da ispune anketu na laptopu.

00:02:48.814 --> 00:02:50.793
Dok su oni ispunjavali anketu,

00:02:50.793 --> 00:02:53.590
podigli smo njihovu sliku na klaster
mrežnog oblaka,

00:02:53.590 --> 00:02:55.317
i aktivirali prepoznavanje lica

00:02:55.317 --> 00:02:57.722
kako bismo spojili tu sliku

00:02:57.722 --> 00:03:00.115
s nekom od stotina tisuća drugih

00:03:00.115 --> 00:03:03.711
koje smo preuzeli s Facebook profila.

00:03:03.711 --> 00:03:06.970
Dok bi ispitanik došao do zadnje
stranice ankete

00:03:06.970 --> 00:03:10.317
stranica bi se dinamički ažurirala

00:03:10.317 --> 00:03:12.630
s 10 najsličnijih slika

00:03:12.630 --> 00:03:14.915
koje je program za prepoznavanje
lica pronašao,

00:03:14.915 --> 00:03:16.653
i tražili smo da nam ispitanici pokažu

00:03:16.653 --> 00:03:20.773
nalaze li sebe na nekoj od slika.

00:03:20.773 --> 00:03:24.472
Vidite li ispitanika?

00:03:24.472 --> 00:03:27.317
Pa, računalo ga je našlo i to je uspjelo

00:03:27.317 --> 00:03:29.466
za trećinu ispitanika.

00:03:29.466 --> 00:03:32.650
U biti, možemo početi od anonimnog lica,

00:03:32.650 --> 00:03:36.134
na mreži ili ne, i možemo upotrijebiti
prepoznavanje lica

00:03:36.134 --> 00:03:38.494
kako bismo otkrili ime toga lica,

00:03:38.494 --> 00:03:40.602
zahvaljujući podacima s društvenih medija.

00:03:40.602 --> 00:03:42.474
Ali prije nekoliko godina, učinili smo nešto drugo.

00:03:42.474 --> 00:03:44.297
Uzeli smo podatke s društvenih medija,

00:03:44.297 --> 00:03:47.348
kombinirali ih statistički s podatcima

00:03:47.348 --> 00:03:49.450
socijalnog osiguranja američke vlade,

00:03:49.450 --> 00:03:52.774
i mogli smo predvidjeti brojeve socijalnog osiguranja,

00:03:52.774 --> 00:03:54.286
koji su u Sjedinjenim Državama

00:03:54.286 --> 00:03:56.326
izuzetno osjetljiva informacija.

00:03:56.326 --> 00:03:58.419
Vidite li kamo vas vodim?

00:03:58.419 --> 00:04:01.341
Ako kombiniramo te dvije studije,

00:04:01.341 --> 00:04:02.853
postavljamo pitanje

00:04:02.853 --> 00:04:05.573
možemo li početi od lica,

00:04:05.573 --> 00:04:07.884
i, pomoću prepoznavanja lica, otkriti ime

00:04:07.884 --> 00:04:10.553
i javno dostupne informacije

00:04:10.553 --> 00:04:12.485
o tom imenu i toj osobi,

00:04:12.485 --> 00:04:14.733
i od tih javno dostupnih informacija

00:04:14.733 --> 00:04:16.775
zaključiti informacije koje nisu
javno dostupne,

00:04:16.775 --> 00:04:18.381
one osjetljivije,

00:04:18.381 --> 00:04:19.873
koje ćemo povezati s licem?

00:04:19.873 --> 00:04:21.789
Odgovor je da, možemo, i uspjeli smo.

00:04:21.789 --> 00:04:24.357
Naravno, točnost se pogoršava.

00:04:24.357 --> 00:04:25.301
[otkriveno 27% prvih 5 znamenki
socijalnog osiguranja (iz 4 pokušaja)]

00:04:25.301 --> 00:04:29.128
Čak smo odlučili razviti aplikaciju za iPhone

00:04:29.128 --> 00:04:31.843
koja koristi ugrađenu kameru telefona

00:04:31.843 --> 00:04:33.443
kako bi slikala osobu

00:04:33.443 --> 00:04:34.930
i zatim poslala sliku u oblak

00:04:34.930 --> 00:04:37.592
i odradila ono što sam vam opisao,
u stvarnom vremenu:

00:04:37.592 --> 00:04:39.680
izvršila prepoznavanje, otkrila javne informacije,

00:04:39.680 --> 00:04:41.410
probala pogoditi osjetljive informacije,

00:04:41.410 --> 00:04:44.001
i poslala ih natrag na mobitel

00:04:44.001 --> 00:04:47.610
gdje su ispisane preko lica osobe.

00:04:47.610 --> 00:04:49.511
To je primjer poboljšane stvarnosti,

00:04:49.511 --> 00:04:51.962
vjerojatno ljigav primjer poboljšane stvarnosti.

00:04:51.962 --> 00:04:55.301
Nismo izradili aplikaciju kako 
bismo je učinili javno dostupnom,

00:04:55.301 --> 00:04:57.223
već da bismo dokazali koncept.

00:04:57.223 --> 00:04:59.536
Uzmite te tehnologije

00:04:59.536 --> 00:05:01.373
i razvijte ih do njihovog logičnog ekstrema.

00:05:01.373 --> 00:05:04.092
Zamislite budućnost u kojoj će vas stranci uokolo

00:05:04.092 --> 00:05:06.403
gledati kroz svoje Google naočale

00:05:06.403 --> 00:05:08.710
ili, jednoga dana, kroz leće,

00:05:08.710 --> 00:05:12.730
i koristiti sedam ili osam podataka o vama

00:05:12.730 --> 00:05:15.312
kako bi otkrili sve drugo

00:05:15.312 --> 00:05:17.915
što se o vama može doznati.

00:05:17.915 --> 00:05:22.709
Kako će izgledati ta budućnost bez tajni?

00:05:22.709 --> 00:05:24.673
Trebamo li brinuti?

00:05:24.673 --> 00:05:26.564
Možda bismo rado povjerovali

00:05:26.564 --> 00:05:29.604
da će budućnost s toliko podataka

00:05:29.604 --> 00:05:32.118
biti budućnost bez predrasuda,

00:05:32.118 --> 00:05:35.701
no, zapravo, količina informacija

00:05:35.701 --> 00:05:37.892
ne znači da ćemo donositi

00:05:37.892 --> 00:05:39.598
objektivnije odluke.

00:05:39.598 --> 00:05:42.158
U drugom eksperimentu, ispitanicima smo predočili

00:05:42.158 --> 00:05:44.404
informacije o potencijalnom kandidatu za posao.

00:05:44.404 --> 00:05:47.582
Te informacije su sadržavale

00:05:47.582 --> 00:05:50.228
neke duhovite, savršeno zakonite,

00:05:50.228 --> 00:05:52.693
ali možda malo neugodne informacije

00:05:52.693 --> 00:05:54.713
koje je osoba objavila na internetu.

00:05:54.713 --> 00:05:57.079
Neki naši ispitanici

00:05:57.079 --> 00:06:00.162
su objavili usporedive informacije,

00:06:00.162 --> 00:06:02.524
a neki nisu.

00:06:02.524 --> 00:06:04.473
Što mislite, koja je skupina

00:06:04.473 --> 00:06:09.025
oštrije sudila našem kandidatu?

00:06:09.025 --> 00:06:10.982
Paradoksalno, bila je to skupina

00:06:10.982 --> 00:06:12.715
koja je o sebi objavila slične informacije,

00:06:12.715 --> 00:06:15.657
što je primjer moralnog odstupanja.

00:06:15.657 --> 00:06:17.407
Možda mislite,

00:06:17.407 --> 00:06:19.109
to se ne odnosi na mene,

00:06:19.109 --> 00:06:21.271
jer ja nemam što skrivati.

00:06:21.271 --> 00:06:23.753
Ali kod privatnosti nije riječ

00:06:23.753 --> 00:06:27.429
o nečemu negativnom što želimo sakriti.

00:06:27.429 --> 00:06:29.783
Zamislite da ste direktor ljudskih resursa

00:06:29.783 --> 00:06:32.730
u tvrtki, zaprimate životopise

00:06:32.730 --> 00:06:35.203
i odlučujete pronaći više informacija
o kandidatima.

00:06:35.203 --> 00:06:37.663
Upišete im imena u Google

00:06:37.663 --> 00:06:39.903
i u određenom svemiru,

00:06:39.903 --> 00:06:41.911
pronađete te informacije.

00:06:41.911 --> 00:06:46.348
Ili u paralelnom svemiru pronađete ove.

00:06:46.348 --> 00:06:49.065
Mislite li da je vjerojatnost jednaka

00:06:49.065 --> 00:06:51.868
kojeg kandidata ćete pozvati?

00:06:51.868 --> 00:06:54.150
Ako tako mislite, onda niste

00:06:54.150 --> 00:06:56.732
kao američki poslodavci koji su

00:06:56.732 --> 00:07:00.039
sudionici eksperimenta, odnosno, baš smo to uradili.

00:07:00.039 --> 00:07:03.221
Napravili smo Facebook profile, oblikovali karakteristike

00:07:03.221 --> 00:07:06.072
i počeli slati životopise tvrtkama u SAD-u;

00:07:06.072 --> 00:07:07.980
otkrivali smo, pratili,

00:07:07.980 --> 00:07:10.373
da li pretražuju naše kandidate

00:07:10.373 --> 00:07:12.205
i da li postupaju prema informacijama

00:07:12.205 --> 00:07:14.143
koje su našli na društvenim mrežama.
Činili su to.

00:07:14.143 --> 00:07:16.244
Društveni mediji su omogućili diskriminaciju

00:07:16.244 --> 00:07:19.317
između jednako stručnih kandidata.

00:07:19.317 --> 00:07:23.892
Marketeri žele da vjerujemo

00:07:23.892 --> 00:07:26.161
da će se naše informacije uvijek

00:07:26.161 --> 00:07:29.434
koristiti u našu korist.

00:07:29.434 --> 00:07:33.149
Razmislite malo. Zašto bi to uvijek bilo tako?

00:07:33.149 --> 00:07:35.813
U filmu otprije nekoliko godina,

00:07:35.813 --> 00:07:38.366
"Manjinsko izvješće", u slavnom prizoru

00:07:38.366 --> 00:07:40.942
Tom Cruise hoda trgovačkim centrom

00:07:40.942 --> 00:07:44.718
i holografska reklama prilagođena njemu

00:07:44.718 --> 00:07:46.553
pojavljuje se oko njega.

00:07:46.553 --> 00:07:49.780
Radnja filma je u 2054.,

00:07:49.780 --> 00:07:51.422
oko 40 godina od danas,

00:07:51.422 --> 00:07:54.330
i koliko god uzbudljivo ta tehnologija izgledala,

00:07:54.330 --> 00:07:56.976
već danas drastično podcjenjuje

00:07:56.976 --> 00:07:59.116
količinu informacija koje organizacije

00:07:59.116 --> 00:08:01.599
mogu prikupiti o vama i koristiti

00:08:01.599 --> 00:08:04.997
da utječu na vas na način koji nikada nećete otkriti.

00:08:04.997 --> 00:08:07.100
Na primjer, evo eksperimenta

00:08:07.100 --> 00:08:09.373
koji je u tijeku, nije još završen.

00:08:09.373 --> 00:08:11.692
Zamislite da organizacija ima pristup

00:08:11.692 --> 00:08:13.748
popisu vaših Facebook prijatelja,

00:08:13.748 --> 00:08:15.520
i da nekakvim algoritmom

00:08:15.520 --> 00:08:19.254
može otkriti vaša dva najdraža prijatelja.

00:08:19.254 --> 00:08:21.534
Zatim izrade, u stvarnom vremenu,

00:08:21.534 --> 00:08:24.376
kompozit lica to dvoje prijatelja.

00:08:24.376 --> 00:08:27.445
Ranije studije su pokazale

00:08:27.445 --> 00:08:30.330
da ljudi ne prepoznaju ni sebe

00:08:30.330 --> 00:08:32.792
u kompozitima lica, ali na njih

00:08:32.792 --> 00:08:34.909
reagiraju s naklonošću.

00:08:34.909 --> 00:08:38.324
I tako vi tražite neki proizvod,

00:08:38.324 --> 00:08:40.883
a u oglasu koji ga reklamira,

00:08:40.883 --> 00:08:43.790
nije bilo koji prezenter.

00:08:43.790 --> 00:08:46.103
To je jedan od vaših prijatelja,

00:08:46.103 --> 00:08:49.406
a vi nećete ni znati što se događa.

00:08:49.406 --> 00:08:51.819
Problem je u tome

00:08:51.819 --> 00:08:54.338
što su postojeći pravni mehanizmi

00:08:54.338 --> 00:08:57.776
za zaštitu od zloporabe osobnih informacija

00:08:57.776 --> 00:09:00.760
efikasni kao nož u dvoboju pištoljima.

00:09:00.760 --> 00:09:03.673
Jedan od tih mehanizama je transparentnost,

00:09:03.673 --> 00:09:06.873
da obavijestite ljude što ćete 
činiti s njihovim podatcima.

00:09:06.873 --> 00:09:08.979
U načelu, to je vrlo dobra stvar.

00:09:08.979 --> 00:09:12.646
To je potrebno, ali ne i dovoljno.

00:09:12.646 --> 00:09:16.344
Transparentnost može biti prikrivena.

00:09:16.344 --> 00:09:18.448
Kažete ljudima što ćete činiti s podatcima,

00:09:18.448 --> 00:09:20.680
a onda ih ipak navedete da otkriju

00:09:20.680 --> 00:09:23.303
arbitrarne količine osobnih informacija.

00:09:23.303 --> 00:09:26.189
U još jednom eksperimentu, ovoga
puta sa studentima,

00:09:26.189 --> 00:09:29.247
pitali smo ih za informacije

00:09:29.247 --> 00:09:31.060
o njihovom ponašanju na fakultetu,

00:09:31.060 --> 00:09:34.000
uključujući osjetljiva pitanja, poput ovog.

00:09:34.000 --> 00:09:34.621
[Jeste li ikada varali na ispitu?]

00:09:34.621 --> 00:09:36.921
Jednoj skupini smo rekli,

00:09:36.921 --> 00:09:39.762
"Samo će drugi studenti vidjeti vaše odgovore."

00:09:39.762 --> 00:09:41.341
Drugoj skupini smo rekli,

00:09:41.341 --> 00:09:44.902
"Studenti i profesori će
vidjeti vaše odgovore."

00:09:44.902 --> 00:09:47.493
Transparentnost. Obavijest.
Rezultat je bio taj

00:09:47.493 --> 00:09:48.900
da je prva skupina ispitanika

00:09:48.900 --> 00:09:51.468
priznala puno više od druge.

00:09:51.468 --> 00:09:52.988
Ima smisla, zar ne?

00:09:52.988 --> 00:09:54.478
Ali tada smo uveli prikrivanje.

00:09:54.478 --> 00:09:57.238
Ponovili smo eksperiment
s iste dvije skupine,

00:09:57.238 --> 00:09:59.665
samo smo dodali vremenski pomak

00:09:59.665 --> 00:10:02.600
između vremena u kojem smo im rekli

00:10:02.600 --> 00:10:04.680
kako ćemo koristiti njihove podatke

00:10:04.680 --> 00:10:09.068
i vremena početka odgovaranja na pitanja.

00:10:09.068 --> 00:10:11.629
Što mislite, koliki je pomak trebao

00:10:11.629 --> 00:10:16.242
da se poništi efekt inhibicije

00:10:16.242 --> 00:10:19.653
uslijed znanja da će profesori
vidjeti odgovore?

00:10:19.653 --> 00:10:21.433
10 minuta?

00:10:21.433 --> 00:10:23.224
5 minuta?

00:10:23.224 --> 00:10:25.000
Jednu minutu?

00:10:25.000 --> 00:10:27.049
Ili 15 sekundi?

00:10:27.049 --> 00:10:29.717
Bilo je dovoljno 15 sekundi da dvije skupine

00:10:29.717 --> 00:10:31.285
otkriju istu količinu informacija,

00:10:31.285 --> 00:10:34.031
kao da drugoj skupini više nije bilo važno

00:10:34.031 --> 00:10:36.687
što će profesori pročitati odgovore.

00:10:36.687 --> 00:10:40.023
Priznajem da do sada moj govor

00:10:40.023 --> 00:10:42.503
možda zvuči izrazito sumorno,

00:10:42.503 --> 00:10:44.224
ali nije mi to cilj.

00:10:44.224 --> 00:10:46.923
Želim s vama podijeliti činjenicu

00:10:46.923 --> 00:10:48.695
da postoje alternative.

00:10:48.695 --> 00:10:51.194
Način na koji sada obavljamo stvari nije jedini

00:10:51.194 --> 00:10:54.231
mogući način, a svakako nije najbolji

00:10:54.231 --> 00:10:56.258
mogući način.

00:10:56.258 --> 00:11:00.429
Kad vam kažu, "Ljudi ne mare za privatnost,"

00:11:00.429 --> 00:11:03.071
razmislite nije li sve možda osmišljeno

00:11:03.071 --> 00:11:05.795
tako da ne mogu voditi
računa o privatnosti,

00:11:05.795 --> 00:11:09.057
a shvaćanjem da se manipulacije događaju

00:11:09.057 --> 00:11:10.664
već smo na pola puta

00:11:10.664 --> 00:11:12.922
do mogućnosti da se zaštitimo.

00:11:12.922 --> 00:11:16.632
Kad vam kažu da je privatnost u suprotnosti

00:11:16.632 --> 00:11:18.481
s koristima od masovnih podataka,

00:11:18.481 --> 00:11:20.954
vodite računa da su u posljednjih 20 godina,

00:11:20.954 --> 00:11:22.871
otkrivene tehnologije

00:11:22.871 --> 00:11:26.189
koje omogućavaju praktički svim elektronskim transakcijama

00:11:26.189 --> 00:11:29.938
odvijanje u uvjetima zaštite privatnosti.

00:11:29.938 --> 00:11:32.493
Možemo se anonimno kretati po internetu.

00:11:32.493 --> 00:11:35.171
Možemo slati e-poštu koju može čitati

00:11:35.171 --> 00:11:38.880
samo onaj kome šaljemo,
a ne može ni NSA.

00:11:38.880 --> 00:11:41.877
Možemo imati čak i prikupljanje podataka
koje čuva privatnost.

00:11:41.877 --> 00:11:45.771
Drugim riječima, možemo imati
sve koristi od masovnih podataka,

00:11:45.771 --> 00:11:47.903
a ipak zaštititi privatnost.

00:11:47.903 --> 00:11:51.694
Naravno, te tehnologije traže seljenje

00:11:51.694 --> 00:11:53.240
troškova i prihoda

00:11:53.240 --> 00:11:55.347
između vlasnika podataka i
onih na koje se podaci odnose,

00:11:55.347 --> 00:11:58.800
i zato ne čujete puno o tome.

00:11:58.800 --> 00:12:02.506
Što me vraća u Edenski vrt.

00:12:02.506 --> 00:12:05.286
Ima još jedna interpretacija privatnosti

00:12:05.286 --> 00:12:07.095
u priči o rajskom vrtu

00:12:07.095 --> 00:12:09.191
koja se ne mora odnositi na pitanje

00:12:09.191 --> 00:12:11.416
osjećaja golotinje Adama i Eve

00:12:11.416 --> 00:12:13.797
i njihovog osjećaja stida.

00:12:13.797 --> 00:12:16.578
Odjek te interpretacije može se naći

00:12:16.578 --> 00:12:19.360
u "Izgubljenom raju" Johna Miltona.

00:12:19.360 --> 00:12:23.557
U vrtu, Adam i Eva su
materijalno zadovoljni.

00:12:23.557 --> 00:12:25.661
Sretni su. Zadovoljni su.

00:12:25.661 --> 00:12:27.954
Ali nedostaju im znanje

00:12:27.954 --> 00:12:29.594
i svjesnost o sebi.

00:12:29.594 --> 00:12:32.913
Kada pojedu odgovarajuće nazvanu

00:12:32.913 --> 00:12:34.206
voćku znanja,

00:12:34.206 --> 00:12:36.811
oni otkriju sebe.

00:12:36.811 --> 00:12:40.842
Spoznaju se. Ostvare autonomiju.

00:12:40.842 --> 00:12:43.968
Cijena, međutim, napuštanje je vrta.

00:12:43.968 --> 00:12:47.849
Privatnost je tako i sredstvo

00:12:47.849 --> 00:12:50.811
i cijena koju moramo platiti za slobodu.

00:12:50.811 --> 00:12:53.581
Marketeri nam kažu

00:12:53.581 --> 00:12:56.600
da masovni podaci i društveni mediji

00:12:56.600 --> 00:12:59.579
nisu samo rajski profit za njih,

00:12:59.579 --> 00:13:02.036
već i Edenski vrt za sve nas.

00:13:02.036 --> 00:13:03.274
Dobivamo besplatne sadržaje.

00:13:03.274 --> 00:13:06.397
Možemo igrati Angry Birds.
Dobivamo ciljane aplikacije.

00:13:06.397 --> 00:13:09.294
Međutim, za nekoliko godina će organizacije

00:13:09.294 --> 00:13:10.903
znati toliko mnogo o nama,

00:13:10.903 --> 00:13:13.613
da će moći predvidjeti naše želje

00:13:13.613 --> 00:13:15.817
i prije nego ih oblikujemo, a možda

00:13:15.817 --> 00:13:18.264
i naručiti proizvode u naše ime

00:13:18.264 --> 00:13:20.538
prije nego i shvatimo da ih trebamo.

00:13:20.538 --> 00:13:23.775
Jedan engleski pisac

00:13:23.775 --> 00:13:26.820
je predvidio ovakvu budućnost

00:13:26.820 --> 00:13:28.225
gdje ćemo se odreći

00:13:28.225 --> 00:13:31.773
neovisnosti i slobode za komfor.

00:13:31.773 --> 00:13:33.934
Još i više od Georgea Orwella,

00:13:33.934 --> 00:13:36.695
taj autor je, naravno, Aldous Huxley.

00:13:36.695 --> 00:13:39.549
U "Vrlom novom svijetu," zamišlja društvo

00:13:39.549 --> 00:13:41.720
u kojem tehnologije koje smo stvorili

00:13:41.720 --> 00:13:43.579
da nam donesu slobodu

00:13:43.579 --> 00:13:46.146
naposlijetku nas sputaju.

00:13:46.146 --> 00:13:50.937
Međutim, u knjizi nam on nudi i izlaz

00:13:50.937 --> 00:13:54.375
iz takvog društva, sličan onom

00:13:54.375 --> 00:13:58.330
Adama i Eve koji su napustili vrt.

00:13:58.330 --> 00:14:00.477
Kako je Savage rekao,

00:14:00.477 --> 00:14:03.546
moguće je povratiti neovisnost i slobodu,

00:14:03.546 --> 00:14:06.225
ali treba platiti visoku cijenu.

00:14:06.225 --> 00:14:11.940
Vjerujem da će jedna od ključnih bitaka

00:14:11.940 --> 00:14:14.503
našeg doba biti bitka

00:14:14.503 --> 00:14:16.890
za kontrolu nad osobnim informacijama,

00:14:16.890 --> 00:14:20.397
bitka oko toga hoće li masovni podaci

00:14:20.397 --> 00:14:21.686
biti sila oslobođenja,

00:14:21.686 --> 00:14:26.432
ili sila koja će potajno manipulirati nama.

00:14:26.432 --> 00:14:29.025
Danas mnogi od nas

00:14:29.025 --> 00:14:31.778
ni ne znaju da se ta bitka vodi,

00:14:31.778 --> 00:14:34.450
ali odvija se, sviđalo vam se to ili ne.

00:14:34.450 --> 00:14:37.254
I riskirajući da glumim zmiju,

00:14:37.254 --> 00:14:40.151
reći ću vam da su alati za tu bitku tu,

00:14:40.151 --> 00:14:43.160
svjesnost o onome što se događa,

00:14:43.160 --> 00:14:44.515
u vašim su rukama,

00:14:44.515 --> 00:14:48.255
samo nekoliko klikova od vas.

00:14:48.255 --> 00:14:49.737
Hvala vam.

00:14:49.737 --> 00:14:54.214
(Pljesak)

