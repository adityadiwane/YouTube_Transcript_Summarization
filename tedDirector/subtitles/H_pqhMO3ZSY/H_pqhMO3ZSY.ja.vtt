WEBVTT
Kind: captions
Language: ja

00:00:00.000 --> 00:00:07.000
翻訳: Shigeto Oeda
校正: Tomoyuki Suzuki

00:00:12.641 --> 00:00:14.995
今日は アダムとイブが巻き込まれた

00:00:14.995 --> 00:00:18.171
今日は アダムとイブが巻き込まれた

00:00:18.171 --> 00:00:20.940
有名なプライバシー事件と

00:00:20.940 --> 00:00:24.386
ここ10年間に起きた

00:00:24.386 --> 00:00:27.072
公私の境界の 大きな変化を関連付けて

00:00:27.072 --> 00:00:28.842
お話ししたいと思います

00:00:28.842 --> 00:00:30.140
この事件はご存知の通りです

00:00:30.140 --> 00:00:33.470
エデンの園にいる
アダムとイブはある日

00:00:33.470 --> 00:00:35.313
自分たちが 裸なのに気付きます

00:00:35.313 --> 00:00:36.813
彼らは ひどく あわてます

00:00:36.813 --> 00:00:39.570
その後の顛末は 
ご存知の通りです

00:00:39.570 --> 00:00:41.758
今日なら アダムとイブは

00:00:41.758 --> 00:00:44.119
恐らく 違った行動を取るでしょう

00:00:44.119 --> 00:00:46.387
[ @アダム ：昨夜は楽しかった！
あのリンゴは美味しかった（笑）]

00:00:46.387 --> 00:00:48.260
[ @イブ：本当に
でも ズボンどうしたんだろう？]

00:00:48.260 --> 00:00:50.896
私たちは今までになく多くの

00:00:50.896 --> 00:00:54.230
情報をオンラインで公開しています

00:00:54.230 --> 00:00:55.934
そして我々に関する情報は

00:00:55.934 --> 00:00:58.158
様々な組織によって収集されています

00:00:58.158 --> 00:01:01.440
この大量の個人情報

00:01:01.440 --> 00:01:03.886
あるいはビッグデータから

00:01:03.886 --> 00:01:05.832
非常に有用で 
価値のあるものが得られますが

00:01:05.832 --> 00:01:08.470
同時にプライバシーを手放すことに伴う

00:01:08.470 --> 00:01:11.568
複雑なトレードオフも存在します

00:01:11.568 --> 00:01:15.033
今日は このような
トレードオフについてお話しします

00:01:15.033 --> 00:01:18.175
まずここ数年の間に 私のなかで

00:01:18.175 --> 00:01:21.502
非常に明確になってきた

00:01:21.502 --> 00:01:23.599
どんな個人情報でもプライバシー情報になり得る

00:01:23.599 --> 00:01:25.884
という洞察から話したいと思います

00:01:25.884 --> 00:01:28.529
2000年の時点で 全世界で

00:01:28.529 --> 00:01:31.921
1000億枚ほどの写真が撮られましたが

00:01:31.921 --> 00:01:34.986
ウェブにアップロードされたものの割合は

00:01:34.986 --> 00:01:36.869
非常に小さなものでした

00:01:36.869 --> 00:01:40.230
2010年には ファイスブックだけでも

00:01:40.230 --> 00:01:43.500
25億枚の写真がひと月にアップされ

00:01:43.500 --> 00:01:45.382
そのほとんどが誰の写真か特定可能です

00:01:45.382 --> 00:01:47.262
同じ期間で

00:01:47.262 --> 00:01:52.132
写真に写っている人を認識する

00:01:52.132 --> 00:01:55.740
コンピュータの能力は３桁上がりました

00:01:55.740 --> 00:01:57.622
この２つの技術を組み合わせると

00:01:57.622 --> 00:01:59.123
何が起こるでしょうか

00:01:59.123 --> 00:02:01.781
顔写真がどんどん手に入るようになり

00:02:01.781 --> 00:02:05.429
コンピュータの顔認識の能力が向上する

00:02:05.429 --> 00:02:07.611
そしてクラウドコンピューティングが

00:02:07.611 --> 00:02:09.499
ここにいる誰に対しても

00:02:09.499 --> 00:02:11.059
数年前には政府の専門機関しか

00:02:11.059 --> 00:02:12.945
持てなかったような

00:02:12.945 --> 00:02:14.727
コンピュータ能力を提供します

00:02:14.727 --> 00:02:16.105
ユビキタスコンピューティングによって

00:02:16.105 --> 00:02:18.997
スーパーコンピュータではない私の携帯電話が

00:02:18.997 --> 00:02:20.668
インターネットに接続して

00:02:20.668 --> 00:02:23.002
数十万の顔認識を

00:02:23.002 --> 00:02:25.641
数秒で実行することを可能にします

00:02:25.641 --> 00:02:28.269
このような技術の組み合わせが

00:02:28.269 --> 00:02:30.333
プライバシーと匿名性に対する

00:02:30.333 --> 00:02:33.221
私たちの基本的な理解を

00:02:33.221 --> 00:02:35.478
大きく変えると推測します

00:02:35.478 --> 00:02:37.471
この考えを検証するために

00:02:37.471 --> 00:02:39.592
カーネギーメロン大学で
ある実験を行ないました

00:02:39.592 --> 00:02:41.691
キャンパスを歩いている学生に声を掛け

00:02:41.691 --> 00:02:43.470
実験に参加してもらい

00:02:43.470 --> 00:02:46.032
ウェブカメラで学生の顔写真を撮って

00:02:46.032 --> 00:02:48.814
ノートパソコンに用意した
アンケートに答えてもらいました

00:02:48.814 --> 00:02:50.793
彼らがアンケートに答えている間に

00:02:50.793 --> 00:02:53.590
クラウドコンピューティングのクラスタに
写真をアップし

00:02:53.590 --> 00:02:55.317
顔認識技術を使って

00:02:55.317 --> 00:02:57.722
フェイスブックから取ってきた

00:02:57.722 --> 00:03:00.115
数十万の画像データーベースと

00:03:00.115 --> 00:03:03.711
照合しました

00:03:03.711 --> 00:03:06.970
被験者の学生がアンケートの最後の

00:03:06.970 --> 00:03:10.317
ページに到達する頃には
そのページは動的に変更されていて

00:03:10.317 --> 00:03:12.630
顔認識エンジンが選んだ最も似ている

00:03:12.630 --> 00:03:14.915
10枚の写真が表示されます

00:03:14.915 --> 00:03:16.653
そこで 学生にその写真の中に

00:03:16.653 --> 00:03:20.773
自分が映っているかどうか尋ねました

00:03:20.773 --> 00:03:24.472
学生をみつけられますか？

00:03:24.472 --> 00:03:27.317
コンピュータは探せたわけです

00:03:27.317 --> 00:03:29.466
実際3人中1人の割合で学生の特定が可能でした

00:03:29.466 --> 00:03:32.650
基本的には 
不特定の顔写真から始めて

00:03:32.650 --> 00:03:36.134
オンラインかオフラインかに関わらず
顔認識技術を使って

00:03:36.134 --> 00:03:38.494
その写真の人物の名前を特定することが

00:03:38.494 --> 00:03:40.602
ソーシャルメディアのおかげで可能です

00:03:40.602 --> 00:03:42.474
数年前にこれとは異なる実験を行いました

00:03:42.474 --> 00:03:44.297
ソーシャルメディアのデータから始めて

00:03:44.297 --> 00:03:47.348
それを統計的に米国政府の

00:03:47.348 --> 00:03:49.450
ソーシャルセキュリティー関連の
データと組み合わせ

00:03:49.450 --> 00:03:52.774
最終的にソーシャルセキュリティー番号を
特定することが出来ました

00:03:52.774 --> 00:03:54.286
ソーシャルセキュリティー番号は米国では

00:03:54.286 --> 00:03:56.326
プライバシーに関する非常に重要な情報です

00:03:56.326 --> 00:03:58.419
この話の先に見えてくるものは何でしょう？

00:03:58.419 --> 00:04:01.341
この2つの実験を組み合わせると

00:04:01.341 --> 00:04:02.853
ある疑問が浮かびます

00:04:02.853 --> 00:04:05.573
顔の写真から始めて

00:04:05.573 --> 00:04:07.884
顔認識技術によって名前を特定し

00:04:07.884 --> 00:04:10.553
その名前からその個人に対する

00:04:10.553 --> 00:04:12.485
公開情報を取得し

00:04:12.485 --> 00:04:14.733
次に公開情報から

00:04:14.733 --> 00:04:16.775
よりプライベートな

00:04:16.775 --> 00:04:18.381
非公開情報を参照し

00:04:18.381 --> 00:04:19.873
それを元の顔と結びつけられるでしょうか？

00:04:19.873 --> 00:04:21.789
答えはイエスで
私たちはそれに成功しました

00:04:21.789 --> 00:04:23.926
もちろん精度は徐々に悪くなります
[27%の被験者のソーシャルセキュリティ番号の
頭5桁が特定できました]

00:04:23.926 --> 00:04:25.701
もちろん精度は徐々に悪くなります
[27%の被験者のソーシャルセキュリティ番号の
頭5桁が特定できました]

00:04:25.701 --> 00:04:29.128
iPhoneのアプリも作ることにしました
[27%の被験者のソーシャルセキュリティ番号の
頭5桁が特定できました]

00:04:29.128 --> 00:04:31.843
内蔵のカメラで

00:04:31.843 --> 00:04:33.443
人物の写真を撮って

00:04:33.443 --> 00:04:34.930
それをクラウドにアップして

00:04:34.930 --> 00:04:37.592
先ほど説明した処理をリアルタイムで実行して

00:04:37.592 --> 00:04:39.680
該当する写真を探し
公開されている情報を探し

00:04:39.680 --> 00:04:41.410
そこからプライバシー情報を探し

00:04:41.410 --> 00:04:44.001
それを携帯に送り返し

00:04:44.001 --> 00:04:47.610
その人物の写真と重ね合わせて表示する

00:04:47.610 --> 00:04:49.511
なんかゾッとする
拡張現実の例だと思います

00:04:49.511 --> 00:04:51.962
なんかゾッとする
拡張現実の例だと思います

00:04:51.962 --> 00:04:55.301
このアプリは公開していません

00:04:55.301 --> 00:04:57.223
単に可能かやってみただけです

00:04:57.223 --> 00:04:59.536
このような技術の応用が

00:04:59.536 --> 00:05:01.373
最大限に進んだ場合を考えると

00:05:01.373 --> 00:05:04.092
周りにいる他人が

00:05:04.092 --> 00:05:06.403
あなたの顔を
グーグルグラスや

00:05:06.403 --> 00:05:08.710
コンタクトレンズを通して見て

00:05:08.710 --> 00:05:12.730
あなたに関する７､８個のデータから

00:05:12.730 --> 00:05:15.312
知り得るどんな個人情報にも

00:05:15.312 --> 00:05:17.915
アクセスできるような未来が想像できます

00:05:17.915 --> 00:05:22.709
このような秘密のない世界はどんな感じでしょうか？

00:05:22.709 --> 00:05:24.673
このようなことに注意を払うべきでしょうか？

00:05:24.673 --> 00:05:26.564
私たちは

00:05:26.564 --> 00:05:29.604
多くのデータにアクセスできる未来が

00:05:29.604 --> 00:05:32.118
偏見のない世界だと信じる傾向にあります

00:05:32.118 --> 00:05:35.701
でも 多くの情報があるからといって

00:05:35.701 --> 00:05:37.892
より客観的な判断をするとは限りません

00:05:37.892 --> 00:05:39.598
より客観的な判断をするとは限りません

00:05:39.598 --> 00:05:42.158
もう1つの実験では 被験者に

00:05:42.158 --> 00:05:44.404
就職志望者の情報を見てもらいました

00:05:44.404 --> 00:05:47.582
この情報のなかに 
意図的に混ぜたのは

00:05:47.582 --> 00:05:50.228
被験者自身がネットに投稿した

00:05:50.228 --> 00:05:52.693
完全に合法でありながら
ちょっと具合の悪い

00:05:52.693 --> 00:05:54.713
情報に関することです

00:05:54.713 --> 00:05:57.079
被験者のなかには

00:05:57.079 --> 00:06:00.162
そのような情報を投稿したことがある人も

00:06:00.162 --> 00:06:02.524
ない人もいました

00:06:02.524 --> 00:06:04.473
どちらのグループが求職者を

00:06:04.473 --> 00:06:09.025
厳しく非難したと思いますか？

00:06:09.025 --> 00:06:10.982
逆説的ですが 自分自身もそのような

00:06:10.982 --> 00:06:12.715
情報を投稿したことがあるグループでした

00:06:12.715 --> 00:06:15.657
これは言動不一致の良い例です

00:06:15.657 --> 00:06:17.407
皆さんは これは自分は当てはまらない

00:06:17.407 --> 00:06:19.109
なぜなら隠しておきたいことは

00:06:19.109 --> 00:06:21.271
ないからと思っているかも知れません

00:06:21.271 --> 00:06:23.753
しかしプライバシーとは

00:06:23.753 --> 00:06:27.429
ネガティブなことを隠すことではありません

00:06:27.429 --> 00:06:29.783
あなたがどこかの組織の

00:06:29.783 --> 00:06:32.730
人事担当責任者で 就職希望者の
履歴書を受け取ったときに

00:06:32.730 --> 00:06:35.203
応募者に関してもう少し調べたいと思ったとしましょう

00:06:35.203 --> 00:06:37.663
候補者の名前をグーグルで検索したら

00:06:37.663 --> 00:06:39.903
あるサイトで こんな情報が
見つかったとします

00:06:39.903 --> 00:06:41.911
あるサイトで こんな情報が
見つかったとします

00:06:41.911 --> 00:06:46.348
一方 どこかのパラレルワールドで
この情報を見つけるとします

00:06:46.348 --> 00:06:49.065
あなたは両方の応募者と

00:06:49.065 --> 00:06:51.868
同じように面談をしたいと思うでしょうか

00:06:51.868 --> 00:06:54.150
もしそう思うのなら

00:06:54.150 --> 00:06:56.732
あなたは米国の雇用主とは違います

00:06:56.732 --> 00:07:00.039
私たちは実験の一部として
正にそのような状況を試してみました

00:07:00.039 --> 00:07:03.221
ファイスブックに人柄を操作した
架空のプロファイルを登録し

00:07:03.221 --> 00:07:06.072
米国内の様々な企業に
その人物の履歴書を送りました

00:07:06.072 --> 00:07:07.980
そして企業がその人物に関する情報を

00:07:07.980 --> 00:07:10.373
ソーシャルメディアで検索するかどうか

00:07:10.373 --> 00:07:12.205
また そこで得た情報に影響されるか
観察すると

00:07:12.205 --> 00:07:14.143
影響されると分かりました

00:07:14.143 --> 00:07:16.244
同じ能力の候補者間で

00:07:16.244 --> 00:07:19.317
ソーシャルメディアを介した差別が発生していました

00:07:19.317 --> 00:07:23.892
マーケティングの人たちは

00:07:23.892 --> 00:07:26.161
私たち自身に関する情報が

00:07:26.161 --> 00:07:29.434
私たちのために使われると
信じさせようとしています

00:07:29.434 --> 00:07:33.149
でもよく考えてみてください
常にそうであるはずがありません

00:07:33.149 --> 00:07:35.813
何年か前の「マイノリティー・レポート」という映画で

00:07:35.813 --> 00:07:38.366
トム・クルーズがショッピング・モールを歩くと

00:07:38.366 --> 00:07:40.942
個人に特化した広告がホログラフィックに

00:07:40.942 --> 00:07:44.718
彼を取り巻くように表示される

00:07:44.718 --> 00:07:46.553
有名な場面がありました

00:07:46.553 --> 00:07:49.780
映画の設定は2054年

00:07:49.780 --> 00:07:51.422
40年後の未来となっていて

00:07:51.422 --> 00:07:54.330
その技術は素晴らしいもののように見えますが

00:07:54.330 --> 00:07:56.976
それはすでに

00:07:56.976 --> 00:07:59.116
組織が集めることのできる個人情報と

00:07:59.116 --> 00:08:01.599
それが思いもよらない方法で利用され得る

00:08:01.599 --> 00:08:04.997
ということに関して過小評価をしています

00:08:04.997 --> 00:08:07.100
そのような例として
現在私たちが行なっている

00:08:07.100 --> 00:08:09.373
実験を紹介します

00:08:09.373 --> 00:08:11.692
企業があなたのフェイスブックの

00:08:11.692 --> 00:08:13.748
友達リストにアクセスできると仮定してみて下さい

00:08:13.748 --> 00:08:15.520
そして何らかのアルゴリズムで

00:08:15.520 --> 00:08:19.254
一番仲のいい友達を２人選ぶことが出来るとします

00:08:19.254 --> 00:08:21.534
そしてその２人の友達の顔の写真を

00:08:21.534 --> 00:08:24.365
リアルタイムにで合成出来るとします

00:08:24.365 --> 00:08:27.445
過去の研究から 合成された顔写真は

00:08:27.445 --> 00:08:30.330
本人自身もそれと認識は出来ないのですが

00:08:30.330 --> 00:08:32.792
何故かその写真の顔に親しみをもつ

00:08:32.792 --> 00:08:34.909
ということが分かっています

00:08:34.909 --> 00:08:38.324
次に何か特定の商品を探しているとき

00:08:38.324 --> 00:08:40.883
それを勧める広告があり

00:08:40.883 --> 00:08:43.790
勧めているのは見知らぬ人ではなく

00:08:43.790 --> 00:08:46.103
あなたの友達の合成イメージで

00:08:46.103 --> 00:08:49.406
それに 全く気付かないかもしれません

00:08:49.406 --> 00:08:51.819
問題は

00:08:51.819 --> 00:08:54.338
現在の法律では

00:08:54.338 --> 00:08:57.776
個人情報の悪用から
我々を保護する仕組みは

00:08:57.776 --> 00:09:00.760
銃撃戦にナイフで対抗するような
ものだということです

00:09:00.760 --> 00:09:03.673
このような仕組みの一つが透明性で

00:09:03.673 --> 00:09:06.873
入手した情報をどの様に使うかを
明確するものです

00:09:06.873 --> 00:09:08.979
原理として それは良いことです

00:09:08.979 --> 00:09:12.646
必要ですが 充分ではありません

00:09:12.646 --> 00:09:16.344
透明性は誤用される可能性があります

00:09:16.344 --> 00:09:18.448
情報をどのように使うかを伝えた上で

00:09:18.448 --> 00:09:20.680
個人情報を提供するように

00:09:20.680 --> 00:09:23.303
相手を誘導することが可能です

00:09:23.303 --> 00:09:26.189
学生を対象に
もう1つ実験を実施しました

00:09:26.189 --> 00:09:29.247
学生達にキャンパスでの行ないに関して

00:09:29.247 --> 00:09:31.060
情報を提供するように依頼しました

00:09:31.060 --> 00:09:33.810
試験でカンニングをしたことがありますか？

00:09:33.810 --> 00:09:35.311
といった非常にプライベートな質問を含んでいます

00:09:35.311 --> 00:09:36.921
一つ目のグループには

00:09:36.921 --> 00:09:39.762
回答は 他の学生だけが閲覧すると伝えました

00:09:39.762 --> 00:09:41.341
もう一つのグループには

00:09:41.341 --> 00:09:44.902
他の学生と教授陣が回答を閲覧すると伝えました

00:09:44.902 --> 00:09:47.493
透明性と留意点が明確であり
当然期待した結果になりました

00:09:47.493 --> 00:09:48.900
最初のグループの方が

00:09:48.900 --> 00:09:51.468
２番目のグループより情報を開示する可能性が
高いという結果です

00:09:51.468 --> 00:09:52.988
妥当な結果だと思います

00:09:52.988 --> 00:09:54.478
次に誤った誘導を加えました

00:09:54.478 --> 00:09:57.238
同じ２つのグループで実験を行ないましたが

00:09:57.238 --> 00:09:59.665
今度はどのように回答が利用されるかを

00:09:59.665 --> 00:10:02.600
伝えるタイミングと

00:10:02.600 --> 00:10:04.680
実際に質問をするタイミングを

00:10:04.680 --> 00:10:09.068
ずらしました

00:10:09.068 --> 00:10:11.629
教授陣も回答を閲覧するという

00:10:11.629 --> 00:10:16.242
抑止作用のある情報を

00:10:16.242 --> 00:10:19.653
忘れるのにどの位の時間が必要だったと思いますか？

00:10:19.653 --> 00:10:21.433
10分？

00:10:21.433 --> 00:10:23.224
５分？

00:10:23.224 --> 00:10:25.000
１分？

00:10:25.000 --> 00:10:27.049
15秒ではどうでしょう？

00:10:27.049 --> 00:10:29.717
両方のグループに同じ量の情報を開示させるのには

00:10:29.717 --> 00:10:31.285
15秒あれば充分でした

00:10:31.285 --> 00:10:34.031
２つ目のグループも教授陣が

00:10:34.031 --> 00:10:36.687
回答を見ることを気にしていないようでした

00:10:36.687 --> 00:10:40.023
ここまでの話で

00:10:40.023 --> 00:10:42.503
とても憂鬱な気分になるとは思います

00:10:42.503 --> 00:10:44.224
でもそれがポイントではないのです

00:10:44.224 --> 00:10:46.923
実際には別のアプローチがある

00:10:46.923 --> 00:10:48.695
ということをお話したいのです

00:10:48.695 --> 00:10:51.194
現在のやり方が唯一の方法ということは

00:10:51.194 --> 00:10:54.231
ありませんし 
ベストな方法でもありません

00:10:54.231 --> 00:10:56.258
ありませんし 
ベストな方法でもありません

00:10:56.258 --> 00:11:00.429
もし誰かが
「人々はプライバシーを気にしない」と言ったら

00:11:00.429 --> 00:11:03.071
やり方が巧妙にデザインされていて

00:11:03.071 --> 00:11:05.795
プライバシーを
気にさせなくしているか疑って下さい

00:11:05.795 --> 00:11:09.057
そのような操作が存在すると気付くことは

00:11:09.057 --> 00:11:10.664
すでに自分自身を守る道を

00:11:10.664 --> 00:11:12.922
半分進んでいることを示しています

00:11:12.922 --> 00:11:16.632
もし誰かが プライバシーと

00:11:16.632 --> 00:11:18.481
ビッグデータから得られるメリットは
両立しないなどと言い放ったら

00:11:18.481 --> 00:11:20.954
ここ20年間 研究者は

00:11:20.954 --> 00:11:22.871
理論上どんな電気的な通信に対しても

00:11:22.871 --> 00:11:26.189
プライバシーを強化できる技術を

00:11:26.189 --> 00:11:29.938
開発してきたことを思い出すべきです

00:11:29.938 --> 00:11:32.493
私たちは匿名のままインターネットを
ブラウズすることができます

00:11:32.493 --> 00:11:35.171
メールを指定した受信者だけが

00:11:35.171 --> 00:11:38.880
読めるように送ることもできます
米国家安全保障局でさえ読めません

00:11:38.880 --> 00:11:41.877
プライバシーに配慮したデータマイニングも可能です

00:11:41.877 --> 00:11:45.771
言い換えれば ビッグデータのメリットを享受しながら

00:11:45.771 --> 00:11:47.903
同時にプライバシーを守ることが可能です

00:11:47.903 --> 00:11:51.694
もちろんこのような技術は

00:11:51.694 --> 00:11:53.240
情報を持つ者と
情報を利用する者の間の

00:11:53.240 --> 00:11:55.347
コストと利益のあり方に
影響するかもしれません

00:11:55.347 --> 00:11:58.800
これが理由であまりこのような話を
聞かないのかもしれません

00:11:58.800 --> 00:12:02.506
ここからエデンの園に話に戻ります

00:12:02.506 --> 00:12:05.286
これはエデンの園の話を

00:12:05.286 --> 00:12:07.095
プライバシーの観点からみたもう1つの解釈です

00:12:07.095 --> 00:12:09.191
これはアダムとイブが裸であることに

00:12:09.191 --> 00:12:11.416
気付いて恥ずかしいと思ったこととは

00:12:11.416 --> 00:12:13.797
関係がありません

00:12:13.797 --> 00:12:16.578
同じような話を

00:12:16.578 --> 00:12:19.360
ジョン・ミルトンの「失楽園」にも
見ることができます

00:12:19.360 --> 00:12:23.557
エデンの園で アダムとイブは
物質的には不自由なく

00:12:23.557 --> 00:12:25.661
幸せで満足していました

00:12:25.661 --> 00:12:27.954
しかし同時に彼らは知識や

00:12:27.954 --> 00:12:29.594
自己認識に欠けていました

00:12:29.594 --> 00:12:32.913
彼らは 適切にも知恵の実と名付けられた

00:12:32.913 --> 00:12:34.206
実を食べた瞬間

00:12:34.206 --> 00:12:36.811
自分たちを発見しました

00:12:36.811 --> 00:12:40.842
彼らは自分達を認識し
自主性を獲得しました

00:12:40.842 --> 00:12:43.968
しかしその代償は
エデンの園を去ることでした

00:12:43.968 --> 00:12:47.849
プライバシーも自由のための

00:12:47.849 --> 00:12:50.811
手段でもあり
同時に払うべき代償でもあります

00:12:50.811 --> 00:12:53.581
マーケッティングの人々は

00:12:53.581 --> 00:12:56.600
ビッグデータとソーシャルメディアは

00:12:56.600 --> 00:12:59.579
彼らが利益を得るための場ではなく

00:12:59.579 --> 00:13:02.036
私たち全員のエデンの園だといいます

00:13:02.036 --> 00:13:03.274
私たちは無料でコンテンツを楽しみ

00:13:03.274 --> 00:13:06.397
アングリーバードを手に入れたり
ほしいアプリを手に入れたりできます

00:13:06.397 --> 00:13:09.294
しかし実際には 数年の内に企業は

00:13:09.294 --> 00:13:10.903
私たちを熟知し

00:13:10.903 --> 00:13:13.613
私たちの欲しいものを

00:13:13.613 --> 00:13:15.817
意識する前に察知し

00:13:15.817 --> 00:13:18.264
必要と思う前に 買い物まで
してくれるようになるかもしれません

00:13:18.264 --> 00:13:20.538
必要と思う前に 買い物まで
してくれるようになるかもしれません

00:13:20.538 --> 00:13:23.775
このように私たちが自立性と自由を

00:13:23.775 --> 00:13:26.820
快適さと引き換えに手放してしまう

00:13:26.820 --> 00:13:28.225
このような未来を憂慮した

00:13:28.225 --> 00:13:31.773
英国の作家がいました

00:13:31.773 --> 00:13:33.934
ジョージ・オーウェル以上に
これを描いたのは

00:13:33.934 --> 00:13:36.695
もちろん オルダス・ハクスリーです

00:13:36.695 --> 00:13:39.549
『すばらしい新世界』で彼は

00:13:39.549 --> 00:13:41.720
もともとは自由のために私たちが

00:13:41.720 --> 00:13:43.579
作り上げた技術が 私たちを

00:13:43.579 --> 00:13:46.146
支配する世界を創造しています

00:13:46.146 --> 00:13:50.937
しかし同時にその小説では その社会から

00:13:50.937 --> 00:13:54.375
逃げ出す経路 アダムとイブがエデンの園から

00:13:54.375 --> 00:13:58.330
出るために通らなければならなかった経路と
似たものを示しています

00:13:58.330 --> 00:14:00.477
小説のなかの未開人の言葉として

00:14:00.477 --> 00:14:03.546
代償は高いが

00:14:03.546 --> 00:14:06.225
自立性と自由を手に入れることは可能だと言っています

00:14:06.225 --> 00:14:11.940
我々の時代の独特の戦いは

00:14:11.940 --> 00:14:14.503
個人情報のコントロールに対する

00:14:14.503 --> 00:14:16.890
戦いであると私は信じています

00:14:16.890 --> 00:14:20.397
ビッグデータが自由のための力となるか

00:14:20.397 --> 00:14:21.686
私たちを陰で操作するような
力となるかの戦いです

00:14:21.686 --> 00:14:26.432
私たちを陰で操作するような
力となるかの戦いです

00:14:26.432 --> 00:14:29.025
現在はまだ 多くの人が

00:14:29.025 --> 00:14:31.778
そのような戦いが始まっていることすら知りません

00:14:31.778 --> 00:14:34.450
しかし好むと好まないとにかかわらず
戦いは始まっています

00:14:34.450 --> 00:14:37.254
蛇の役割を演じてしまう危険を承知で

00:14:37.254 --> 00:14:40.151
戦いに必要な道具はここにある

00:14:40.151 --> 00:14:43.160
何が起きているかを理解する力は

00:14:43.160 --> 00:14:44.515
あなたの手のなかに

00:14:44.515 --> 00:14:48.255
ほんの数クリックしか離れていないところに
あると伝えたいと思います

00:14:48.255 --> 00:14:49.737
ありがとうございます

00:14:49.737 --> 00:14:54.214
（拍手）

