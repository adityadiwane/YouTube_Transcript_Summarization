WEBVTT
Kind: captions
Language: it

00:00:00.000 --> 00:00:07.000
Traduttore: Alessandra Tadiotto
Revisore: Anna Cristiana Minoli

00:00:12.641 --> 00:00:14.995
Vorrei raccontarvi una storia

00:00:14.995 --> 00:00:18.171
che collega il famoso incidente sulla privacy

00:00:18.171 --> 00:00:20.940
di Adamo ed Eva

00:00:20.940 --> 00:00:24.386
e il notevole cambiamento nei confini

00:00:24.386 --> 00:00:27.072
tra pubblico e privato

00:00:27.072 --> 00:00:28.842
avvenuto negli ultimi 10 anni.

00:00:28.842 --> 00:00:30.140
Conoscete i fatti.

00:00:30.140 --> 00:00:33.470
Un giorno nel giardino dell'Eden 
Adamo ed Eva

00:00:33.470 --> 00:00:35.313
si accorgono di essere nudi.

00:00:35.313 --> 00:00:36.813
Vanno fuori di testa.

00:00:36.813 --> 00:00:39.570
E il resto è storia.

00:00:39.570 --> 00:00:41.758
Oggi, Adamo ed Eva probabilmente

00:00:41.758 --> 00:00:44.119
agirebbero in modo diverso

00:00:44.119 --> 00:00:46.387
[@Adam La scorsa notte è stato fantastico.
Ho adorato quella mela LOL]

00:00:46.387 --> 00:00:48.260
[@Eva sì... tesoro, sai dove sono finiti i miei slip?]

00:00:48.260 --> 00:00:50.896
Riveliamo più informazioni che mai

00:00:50.896 --> 00:00:54.230
su noi stessi on-line,

00:00:54.230 --> 00:00:55.934
e queste informazioni su di noi

00:00:55.934 --> 00:00:58.158
vengono raccolte da organizzazioni.

00:00:58.158 --> 00:01:01.440
C'è molto da guadagnarci e da fare profitto

00:01:01.440 --> 00:01:03.886
dall'analisi massiccia dei dati personali,

00:01:03.886 --> 00:01:05.832
o grandi dati,

00:01:05.832 --> 00:01:08.470
ma c'è anche una complessa rete di compromessi

00:01:08.470 --> 00:01:11.568
che deriva dal rinunciare alla nostra privacy.

00:01:11.568 --> 00:01:15.591
E la mia storia riguarda questi compromessi.

00:01:15.591 --> 00:01:18.175
Cominciamo con un'osservazione che, a mio avviso,

00:01:18.175 --> 00:01:21.502
è divenuta sempre più chiara negli ultimi anni,

00:01:21.502 --> 00:01:23.599
e cioé che ogni singola informazione personale

00:01:23.599 --> 00:01:25.884
può diventare un dato sensibile.

00:01:25.884 --> 00:01:30.009
Nel 2000, circa 100 miliardi di foto

00:01:30.009 --> 00:01:31.921
vennero scattate in tutto il mondo,

00:01:31.921 --> 00:01:34.986
ma solo una minuscola parte

00:01:34.986 --> 00:01:36.869
venne di fatto caricata online.

00:01:36.869 --> 00:01:40.230
Nel 2010, solo su Facebook, in un solo mese,

00:01:40.230 --> 00:01:43.500
ne sono state caricate 2,5 miliardi,

00:01:43.500 --> 00:01:45.382
e la maggior parte identificata.

00:01:45.382 --> 00:01:47.262
Nello stesso arco di tempo,

00:01:47.262 --> 00:01:52.132
la capacità dei computer
di riconoscere le persone nelle foto

00:01:52.132 --> 00:01:55.740
si è triplicata.

00:01:55.740 --> 00:01:57.622
Che cosa succede quando

00:01:57.622 --> 00:01:59.123
si mettono insieme tutte queste tecnologie?

00:01:59.123 --> 00:02:01.781
Un aumento della disponibilità di dati facciali,

00:02:01.781 --> 00:02:05.429
un miglioramento nella capacità dei computer
di riconoscere i volti

00:02:05.429 --> 00:02:07.611
ma anche il cloud computing,

00:02:07.611 --> 00:02:09.499
che offre a chiunque qui presente

00:02:09.499 --> 00:02:11.059
un potere di gestione 
della infrastruttura informatica

00:02:11.059 --> 00:02:12.945
che qualche anno fa era appannaggio solo

00:02:12.945 --> 00:02:14.727
delle agenzie di sicurezza:

00:02:14.727 --> 00:02:16.105
e l'onnipresenza dell'informatica,

00:02:16.105 --> 00:02:18.997
che permette al mio telefono,
che è anche un supercomputer,

00:02:18.997 --> 00:02:20.668
di connettersi a internet

00:02:20.668 --> 00:02:23.002
ed eseguire centinaia di migliaia

00:02:23.002 --> 00:02:25.641
di analisi facciali in pochi secondi.

00:02:25.641 --> 00:02:28.269
Riteniamo che il risultato di questa

00:02:28.269 --> 00:02:30.333
combinazione di tecnologie

00:02:30.333 --> 00:02:33.221
porterà a un cambio radicale 
della nostra concezione

00:02:33.221 --> 00:02:35.478
di privacy e anonimato.

00:02:35.478 --> 00:02:37.471
Per verificarlo, abbiamo fatto un esperimento

00:02:37.471 --> 00:02:39.592
al campus della Carnegie Mellon University.

00:02:39.592 --> 00:02:41.691
Abbiamo chiesto agli studenti 
che passavano di lì

00:02:41.691 --> 00:02:43.470
di partecipare a uno studio.

00:02:43.470 --> 00:02:46.032
Abbiamo scattato una foto con la webcam,

00:02:46.032 --> 00:02:48.814
e abbiamo chiesto loro di compilare un questionario
su un pc portatile.

00:02:48.814 --> 00:02:50.793
Mentre compilavano il questionario,

00:02:50.793 --> 00:02:53.590
abbiamo caricato le foto su un cluster di cloud-computing,

00:02:53.590 --> 00:02:55.317
e abbiamo iniziato a utilizzare
un riconoscitore facciale

00:02:55.317 --> 00:02:57.722
per incrociare quello scatto con un database

00:02:57.722 --> 00:03:00.115
contenente centinaia di migliaia di immagini

00:03:00.115 --> 00:03:03.711
che avevamo scaricato dai profili di Facebook.

00:03:03.711 --> 00:03:06.970
Quando il soggetto aveva compilato
l'ultima pagina del questionario

00:03:06.970 --> 00:03:10.317
la pagina era già stata aggiornata dinamicamente

00:03:10.317 --> 00:03:12.630
con le 10 foto che meglio combaciavano

00:03:12.630 --> 00:03:14.915
trovate dal sistema 
di riconoscimento facciale

00:03:14.915 --> 00:03:16.653
e abbiamo chiesto ai soggetti di indicare

00:03:16.653 --> 00:03:20.773
se si ritrovavano nella foto.

00:03:20.773 --> 00:03:24.472
Vedete il soggetto?

00:03:24.472 --> 00:03:27.317
Bene, il computer ci riusciva, 
riconosceva

00:03:27.317 --> 00:03:29.466
un soggetto su tre.

00:03:29.466 --> 00:03:32.650
Quindi, in sintesi, partendo da un volto anonimo,

00:03:32.650 --> 00:03:36.134
offline o online, siamo in grado di usare 
il riconoscimento facciale

00:03:36.134 --> 00:03:38.494
e dare un nome a quel volto anonimo

00:03:38.494 --> 00:03:40.602
grazie ai dati presenti sui social media.

00:03:40.602 --> 00:03:42.474
Ma qualche anno fa, abbiamo fatto quacosa in più.

00:03:42.474 --> 00:03:44.297
Partendo dai dati dei social media,

00:03:44.297 --> 00:03:47.348
li abbiamo combinati statisticamente con i dati

00:03:47.348 --> 00:03:49.450
del sistemi del sistame previdenziale americano,

00:03:49.450 --> 00:03:52.774
e siamo riusciti a prevedere i codici fiscali,

00:03:52.774 --> 00:03:54.286
che negli Stati Uniti

00:03:54.286 --> 00:03:56.326
sono dati estremamente sensibili.

00:03:56.326 --> 00:03:58.419
Capite dove voglio arrivare?

00:03:58.419 --> 00:04:01.341
Se uniamo i due studi,

00:04:01.341 --> 00:04:02.853
allora la domanda è:

00:04:02.853 --> 00:04:05.573
è possibile partire da un volto e,

00:04:05.573 --> 00:04:07.884
usando il riconoscimento facciale,
trovare un nome

00:04:07.884 --> 00:04:10.553
e altre informazioni 
disponibili pubblicamente

00:04:10.553 --> 00:04:12.485
sul quel nome e su quella persona,

00:04:12.485 --> 00:04:14.733
e da tali informazioni pubbliche

00:04:14.733 --> 00:04:16.775
dedurre informazioni non disponibili pubblicamente,

00:04:16.775 --> 00:04:18.381
dati molto più sensibili

00:04:18.381 --> 00:04:19.873
riconducibili a quel volto?

00:04:19.873 --> 00:04:21.789
E la risposta è sì, possiamo farlo, e l'abbiamo fatto.

00:04:21.789 --> 00:04:24.357
Naturalmente, il grado di accuratezza diminuisce.

00:04:24.357 --> 00:04:25.301
[abbiamo identificato il 27% delle prime 5 cifre 
del codice fiscale (con 4 tentativi)]

00:04:25.301 --> 00:04:29.128
Ma abbiamo anche deciso di sviluppare
un'applicazione iPhone

00:04:29.128 --> 00:04:31.843
che utilizza la fotocamera interna del telefono

00:04:31.843 --> 00:04:33.443
scatta una foto del soggetto,

00:04:33.443 --> 00:04:34.930
la carica sul cloud

00:04:34.930 --> 00:04:37.592
e poi fa in tempo reale
esattamente quello che vi ho descritto:

00:04:37.592 --> 00:04:39.680
cerca un raffronto, 
recupera informazioni pubbliche,

00:04:39.680 --> 00:04:41.410
cerca di desumere dati sensibili,

00:04:41.410 --> 00:04:44.001
e poi le rinvia al telefono

00:04:44.001 --> 00:04:47.610
così da sovrapporle al volto del soggetto:

00:04:47.610 --> 00:04:49.511
È un esempio di realtà aumentata,

00:04:49.511 --> 00:04:51.962
probabilmente ne è un esempio raccapricciante.

00:04:51.962 --> 00:04:55.301
Di fatto, non abbiamo sviluppato l'applicazione
per renderla disponibile sul mercato,

00:04:55.301 --> 00:04:57.223
ma solo come dimostrazione del concetto.

00:04:57.223 --> 00:04:59.536
Infatti, prendete queste tecnologie

00:04:59.536 --> 00:05:01.373
e portatele all'estremo.

00:05:01.373 --> 00:05:04.092
Immaginate un futuro dove estranei attorno a voi

00:05:04.092 --> 00:05:06.403
vi guarderanno attraverso i loro occhiali Google

00:05:06.403 --> 00:05:08.710
o, un giorno, attraverso le loro lenti a contatto

00:05:08.710 --> 00:05:12.730
e useranno sette od otto 
punti di infomazione su di voi

00:05:12.730 --> 00:05:15.312
per dedurre ulteriori informazioni

00:05:15.312 --> 00:05:17.915
su di voi.

00:05:17.915 --> 00:05:22.709
Come sarà questo futuro senza segreti?

00:05:22.709 --> 00:05:24.673
E dovremmo preoccuparcene?

00:05:24.673 --> 00:05:26.564
Ci piacerebbe credere

00:05:26.564 --> 00:05:29.604
che un futuro così, 
con un'enorme quantità di dati

00:05:29.604 --> 00:05:32.118
sarebbe un futuro senza pregiudizi,

00:05:32.118 --> 00:05:35.701
ma in verità, essere in possesso 
di così tante informazioni

00:05:35.701 --> 00:05:37.892
non significa prendere decisioni

00:05:37.892 --> 00:05:39.598
più obiettive.

00:05:39.598 --> 00:05:42.158
In un altro esperimento,
abbiamo presentato ai nostri soggetti

00:05:42.158 --> 00:05:44.404
informazioni su un potenziale candidato per un lavoro.

00:05:44.404 --> 00:05:47.582
Abbiamo incluso alcuni riferimenti

00:05:47.582 --> 00:05:50.228
ad alcune informazioni divertenti,
assolutamente legali,

00:05:50.228 --> 00:05:52.693
ma forse leggermente imbarazzanti

00:05:52.693 --> 00:05:54.713
che quel soggetto aveva pubblicato online

00:05:54.713 --> 00:05:57.079
Stranamente, tra i nostri soggetti,

00:05:57.079 --> 00:06:00.162
alcuni avevano pubblicato delle informazioni simili,

00:06:00.162 --> 00:06:02.524
altri no.

00:06:02.524 --> 00:06:04.473
Quale gruppo pensate

00:06:04.473 --> 00:06:09.025
era più propenso a giudicare 
severamente il nostro soggetto?

00:06:09.025 --> 00:06:10.982
Paradossalmente, il gruppo

00:06:10.982 --> 00:06:12.715
che aveva pubblicato informazioni simili:

00:06:12.715 --> 00:06:15.657
un esempio di dissonanza morale.

00:06:15.657 --> 00:06:17.407
Forse starete pensando:

00:06:17.407 --> 00:06:19.109
questo non mi riguarda,

00:06:19.109 --> 00:06:21.271
perché io non ho nulla da nascondere.

00:06:21.271 --> 00:06:23.753
Ma in realtà la privacy non riguarda

00:06:23.753 --> 00:06:27.429
qualcosa di negativo da nascondere.

00:06:27.429 --> 00:06:29.783
Immaginate di essere il direttore delle risorse umane

00:06:29.783 --> 00:06:32.730
di una certa organizzazione, 
ricevete dei CV,

00:06:32.730 --> 00:06:35.203
e decidete di trovare ulteriori informazioni
sui candidati.

00:06:35.203 --> 00:06:37.663
Allora, digitate su Google i loro nomi,

00:06:37.663 --> 00:06:39.903
e in un certo universo,

00:06:39.903 --> 00:06:41.911
trovate tali informazioni.

00:06:41.911 --> 00:06:46.348
O in un universo parallelo, 
trovate queste informazioni.

00:06:46.348 --> 00:06:49.065
Credete davvero che sarete obiettivi

00:06:49.065 --> 00:06:51.868
nel chiamare questo o quel candidato
per un colloquio?

00:06:51.868 --> 00:06:54.150
Se lo pensate, allora non siete

00:06:54.150 --> 00:06:56.732
come i datori di lavoro statunitensi

00:06:56.732 --> 00:07:00.039
che hanno partecipato al nostro esperimento.
Abbiamo fatto proprio questo.

00:07:00.039 --> 00:07:03.221
Abbiamo creato dei profili su facebook, 
manipolando le caratteristiche,

00:07:03.221 --> 00:07:06.072
poi abbiamo inviato dei CV a delle aziende negli Stati Uniti,

00:07:06.072 --> 00:07:07.980
e abbiamo rilevato e monitorato

00:07:07.980 --> 00:07:10.373
eventuali ricerche sui nostri candidati,

00:07:10.373 --> 00:07:12.205
e se ciò avveniva sulla base delle informazioni

00:07:12.205 --> 00:07:14.143
che si potevano reperire sui social media.
E accadeva proprio questo.

00:07:14.143 --> 00:07:16.244
Candidati ugualmente dotati 
venivano discriminati

00:07:16.244 --> 00:07:19.317
attraverso i social media.

00:07:19.317 --> 00:07:23.892
Gli esperti di marketing amano farci credere

00:07:23.892 --> 00:07:26.161
che tutte queste informazioni su di noi
verrano sempre usate

00:07:26.161 --> 00:07:29.434
in nostro favore.

00:07:29.434 --> 00:07:33.149
Ma pensateci bene. 
Perché dovrebbe essere sempre così?

00:07:33.149 --> 00:07:35.813
In un film uscito qualche anno fa,

00:07:35.813 --> 00:07:38.366
"Minority Report", c'è una scena famosa

00:07:38.366 --> 00:07:40.942
in cui Tom Cruise cammina 
in un centro commerciale

00:07:40.942 --> 00:07:44.718
e attorno a lui appaiono

00:07:44.718 --> 00:07:46.553
messagi pubblicitari personalizzati.

00:07:46.553 --> 00:07:49.780
Quel film è ambientato nel 2054,

00:07:49.780 --> 00:07:51.422
fra circa 40 anni,

00:07:51.422 --> 00:07:54.330
e per quanto affascinante appaia quella tecnologia,

00:07:54.330 --> 00:07:56.976
sottovaluta grandemente

00:07:56.976 --> 00:07:59.116
la quantità di informazioni che le organizzazioni

00:07:59.116 --> 00:08:01.599
possono raccogliere su di noi, 
e come le possono usare

00:08:01.599 --> 00:08:04.997
per influenzarci in modi 
di cui non siamo consapevoli.

00:08:04.997 --> 00:08:07.100
Ne è un esempio l'esperimento

00:08:07.100 --> 00:08:09.373
che stiamo conducendo, 
non ancora portato a termine.

00:08:09.373 --> 00:08:11.692
Immaginate che un'organizzazione abbia accesso

00:08:11.692 --> 00:08:13.748
alla lista dei vostri amici su Facebook,

00:08:13.748 --> 00:08:15.520
e che attraverso un algoritmo

00:08:15.520 --> 00:08:19.254
sia in grado di rilevare i due amici
che amate di più.

00:08:19.254 --> 00:08:21.534
Poi creano in tempo reale

00:08:21.534 --> 00:08:24.376
un composito facciale di questi due amici.

00:08:24.376 --> 00:08:27.445
Studi precedenti hanno dimostrato 
che le persone

00:08:27.445 --> 00:08:30.330
non riconoscono nemmeno più se stesse

00:08:30.330 --> 00:08:32.792
nei compositi facciali, 
ma che reagiscono

00:08:32.792 --> 00:08:34.909
positivamente a quei compositi.

00:08:34.909 --> 00:08:38.324
Così la prossima volta che cercate un prodotto

00:08:38.324 --> 00:08:40.883
e una pubblicità vi suggerisce 
di comprare quel prodotto,

00:08:40.883 --> 00:08:43.790
potrebbe non esserci un normale portavoce,

00:08:43.790 --> 00:08:46.103
potrebbe essere uno dei vostri amici,

00:08:46.103 --> 00:08:49.406
e voi non ne sarete nemmeno consapevoli.

00:08:49.406 --> 00:08:51.819
Il problema è che

00:08:51.819 --> 00:08:54.338
attualmente i meccanismi normativi che abbiamo

00:08:54.338 --> 00:08:57.776
a disposizione per difenderci agli abusi 
di dati personali

00:08:57.776 --> 00:09:00.760
sono totalmente inadeguati.

00:09:00.760 --> 00:09:03.673
Uno di questi è la trasparenza,

00:09:03.673 --> 00:09:06.873
ovvero comunicare alle persone
come verrano usati i loro dati.

00:09:06.873 --> 00:09:08.979
E in linea di principio, è una buona cosa.

00:09:08.979 --> 00:09:12.646
È necessario, ma non sufficiente.

00:09:12.646 --> 00:09:16.344
La trasparenza può essere manipolata.

00:09:16.344 --> 00:09:18.448
Potete dire alle persone cosa avete intenzione di fare,

00:09:18.448 --> 00:09:20.680
e in seguito spingerli a rivelare

00:09:20.680 --> 00:09:23.303
una gran quantità di dati personali.

00:09:23.303 --> 00:09:26.189
Così, in un altro esperimento

00:09:26.189 --> 00:09:29.247
abbiamo chiesto a degli studenti
di fornire informazioni

00:09:29.247 --> 00:09:31.060
inerenti il loro comportamento nel campus,

00:09:31.060 --> 00:09:34.000
comprese molte domande sensibili,
come questa:

00:09:34.000 --> 00:09:34.621
[Hai mai copiato ad un esame?]

00:09:34.621 --> 00:09:36.921
A un gruppo di soggetti abbiamo detto

00:09:36.921 --> 00:09:39.762
"Le tue risposte verrano viste solo da altri studenti."

00:09:39.762 --> 00:09:41.341
A un altro gruppo di soggetti, abbiamo detto,

00:09:41.341 --> 00:09:44.902
"Gli studenti e la facoltà vedranno le tue risposte".

00:09:44.902 --> 00:09:47.493
Trasparenza. Notifica. 
E certo, ha funzionato,

00:09:47.493 --> 00:09:48.900
nel senso che il primo gruppo

00:09:48.900 --> 00:09:51.468
era molto più propenso a fornire
dati sensibili rispetto al secondo gruppo.

00:09:51.468 --> 00:09:52.988
Ha senso, non è vero?

00:09:52.988 --> 00:09:54.478
Ma poi abbiamo aggiunto indicazioni fuorvianti.

00:09:54.478 --> 00:09:57.238
Abbiamo ripetuto l'esperimento 
con gli stessi due gruppi,

00:09:57.238 --> 00:09:59.665
questa volta facendo passare
un lasso di tempo

00:09:59.665 --> 00:10:02.600
tra il momento in cui abbiamo 
detto agli studenti

00:10:02.600 --> 00:10:04.680
come avremmo usato i loro dati

00:10:04.680 --> 00:10:09.068
e il momento in cui abbiamo cominciato
effettivamente a raccogliere le risposte.

00:10:09.068 --> 00:10:11.629
Quanto deve durare l'intervallo di tempo

00:10:11.629 --> 00:10:16.242
affinché si annulli l'effetto inibitorio

00:10:16.242 --> 00:10:19.653
di sapere che la facoltà leggerà le vostre risposte?

00:10:19.653 --> 00:10:21.433
Dieci minuti?

00:10:21.433 --> 00:10:23.224
Cinque minuti?

00:10:23.224 --> 00:10:25.000
Un minuto?

00:10:25.000 --> 00:10:27.049
Che ne dite di 15 secondi?

00:10:27.049 --> 00:10:29.717
Quindici secondi sono stati sufficienti
per far sì che entrambi i gruppi

00:10:29.717 --> 00:10:31.285
fornissero le stesse informazioni,

00:10:31.285 --> 00:10:34.031
come se ora al secondo gruppo 
non importasse più

00:10:34.031 --> 00:10:36.687
che la facoltà leggesse le loro risposte.

00:10:36.687 --> 00:10:40.023
Devo ammettere che il mio intervento finora

00:10:40.023 --> 00:10:42.503
può sembrare eccessivamente pessimista,

00:10:42.503 --> 00:10:44.224
ma non è questo il punto.

00:10:44.224 --> 00:10:46.923
In realtà, voglio condividere 
con voi il fatto che

00:10:46.923 --> 00:10:48.695
ci sono alternative.

00:10:48.695 --> 00:10:51.194
Il modo in cui stiamo facendo le cose 
non è l'unico modo,

00:10:51.194 --> 00:10:54.231
e certo non è il modo migliore

00:10:54.231 --> 00:10:56.258
con cui farle.

00:10:56.258 --> 00:11:00.429
Quando qualcuno vi dice, 
"Le persone non si preoccupano della privacy"

00:11:00.429 --> 00:11:03.071
considerate se il gioco è stato progettato

00:11:03.071 --> 00:11:05.795
e manipolato in modo che non possano preoccuparsi della privacy.

00:11:05.795 --> 00:11:09.057
Comprendere che queste manipolazioni si verificano

00:11:09.057 --> 00:11:10.664
è già metà della strada da compiere

00:11:10.664 --> 00:11:12.922
per poter essere in grado di proteggersi.

00:11:12.922 --> 00:11:16.632
Quando qualcuno vi dice 
che la riservatezza è incompatibile

00:11:16.632 --> 00:11:18.481
con i vantaggi dei grandi dati,

00:11:18.481 --> 00:11:20.954
considerate che negli ultimi 20 anni,

00:11:20.954 --> 00:11:22.871
i ricercatori hanno creato tecnologie

00:11:22.871 --> 00:11:26.189
per far sì che qualsiasi transazione elettronica

00:11:26.189 --> 00:11:29.938
avvenga in modo da garantire 
maggiormente la privacy.

00:11:29.938 --> 00:11:32.493
Possiamo navigare in Internet in modo anonimo.

00:11:32.493 --> 00:11:35.171
Possiamo inviare email che possono essere lette solo

00:11:35.171 --> 00:11:38.880
dal destinatario, nemmeno dalla NSA.

00:11:38.880 --> 00:11:41.877
Possiamo anche estrarre dati preservando la privacy.

00:11:41.877 --> 00:11:45.771
In altre parole, possiamo avere 
i benefici dei grandi dati

00:11:45.771 --> 00:11:47.903
proteggendo la privacy.

00:11:47.903 --> 00:11:51.694
Naturalmente, queste tecnologie implicano 
uno spostamento

00:11:51.694 --> 00:11:53.240
di costi e ricavi

00:11:53.240 --> 00:11:55.347
tra i titolari di dati e i soggetti dei dati,

00:11:55.347 --> 00:11:58.800
che è il motivo per cui non ne sentiamo parlare molto.

00:11:58.800 --> 00:12:02.506
Il che mi riporta al Giardino dell'Eden.

00:12:02.506 --> 00:12:05.286
C'è una seconda interpretazione sulla privacy

00:12:05.286 --> 00:12:07.095
nella storia del Giardino dell'Eden

00:12:07.095 --> 00:12:09.191
che non ha nulla a che fare con il fatto

00:12:09.191 --> 00:12:11.416
che Adamo ed Eva si sentono nudi

00:12:11.416 --> 00:12:13.797
e si vergognano.

00:12:13.797 --> 00:12:16.578
Si possono trovare echi di questa interpretazione

00:12:16.578 --> 00:12:19.360
nel "Paradiso Perduto" di John Milton.

00:12:19.360 --> 00:12:23.557
Nel giardino, Adamo ed Eva sono materialmente contenti.

00:12:23.557 --> 00:12:25.661
Sono felici. Sono soddisfatti.

00:12:25.661 --> 00:12:27.954
Tuttavia, mancano della conoscenza

00:12:27.954 --> 00:12:29.594
e della consapevolezza di sé.

00:12:29.594 --> 00:12:32.913
Il momento in cui mangiano 
il giustamente chiamato

00:12:32.913 --> 00:12:34.206
frutto della conoscenza,

00:12:34.206 --> 00:12:36.811
è il momento in cui scoprono se stessi.

00:12:36.811 --> 00:12:40.842
Diventano consapevoli. 
Raggiungono l'autonomia.

00:12:40.842 --> 00:12:43.968
Tuttavia, Il prezzo da pagare 
è lasciare il giardino.

00:12:43.968 --> 00:12:47.849
Così la privacy, in un certo senso, è sia il mezzo

00:12:47.849 --> 00:12:50.811
sia il prezzo da pagare per la libertà.

00:12:50.811 --> 00:12:53.581
Ancora una volta, il marketing ci dice

00:12:53.581 --> 00:12:56.600
che i big data e i social media

00:12:56.600 --> 00:12:59.579
non sono solo un paradiso di profitto per loro,

00:12:59.579 --> 00:13:02.036
ma un Giardino dell'Eden per il resto di noi.

00:13:02.036 --> 00:13:03.274
Otteniamo contenuti gratuiti.

00:13:03.274 --> 00:13:06.397
Possiamo giocare a Angry Birds. 
Scarichiamo applicazioni mirate.

00:13:06.397 --> 00:13:09.294
Ma in realtà, in pochi anni, le organizzazioni

00:13:09.294 --> 00:13:10.903
socpriranno così tanto di noi,

00:13:10.903 --> 00:13:13.613
che saranno in grado di dedurre i nostri desideri

00:13:13.613 --> 00:13:15.817
prima ancora che si formulino

00:13:15.817 --> 00:13:18.264
e forse acquisteranno prodotti per nostro conto

00:13:18.264 --> 00:13:20.538
prima ancora che ci rendiamo conto 
di averne bisogno.

00:13:20.538 --> 00:13:23.775
C'è stato un autore inglese

00:13:23.775 --> 00:13:26.820
che ha previsto questo tipo di futuro

00:13:26.820 --> 00:13:28.225
dove avremmo venduto

00:13:28.225 --> 00:13:31.773
la nostra autonomia e libertà per il comfort.

00:13:31.773 --> 00:13:33.934
Ancora più che George Orwell,

00:13:33.934 --> 00:13:36.695
l'autore è, naturalmente, Aldous Huxley.

00:13:36.695 --> 00:13:39.549
In "Il Mondo Nuovo", immagina una società

00:13:39.549 --> 00:13:41.720
dove le tecnologie che abbiamo creato

00:13:41.720 --> 00:13:43.579
originariamente per la libertà

00:13:43.579 --> 00:13:46.146
finiscono per controllarci.

00:13:46.146 --> 00:13:50.937
Tuttavia, nel libro, ci offre anche una via d'uscita

00:13:50.937 --> 00:13:54.375
da quella società, simile al percorso

00:13:54.375 --> 00:13:58.330
che Adamo ed Eva hanno dovuto seguire 
per lasciare il giardino.

00:13:58.330 --> 00:14:00.477
Nelle parole del Selvaggio,

00:14:00.477 --> 00:14:03.546
riconquistare l'autonomia 
e la libertà è possibile,

00:14:03.546 --> 00:14:06.225
anche se il prezzo da pagare è caro.

00:14:06.225 --> 00:14:11.940
Quindi credo che una delle lotte principali

00:14:11.940 --> 00:14:14.503
dei nostri tempi sarà la lotta

00:14:14.503 --> 00:14:16.890
per il controllo delle informazioni personali,

00:14:16.890 --> 00:14:20.397
la lotta per determinare se i big data diventeranno una forza

00:14:20.397 --> 00:14:21.686
per la libertà,

00:14:21.686 --> 00:14:26.432
o una forza che ci manipolerà di nascosto.

00:14:26.432 --> 00:14:29.025
Ora, molti di noi

00:14:29.025 --> 00:14:31.778
non sanno nemmeno che la lotta è in corso,

00:14:31.778 --> 00:14:34.450
ma è così, che vi piaccia o no.

00:14:34.450 --> 00:14:37.254
E a rischio di fare la parte del serpente,

00:14:37.254 --> 00:14:40.151
vi dirò che gli strumenti per la lotta

00:14:40.151 --> 00:14:43.160
sono qui, la consapevolezza di ciò che sta succedendo,

00:14:43.160 --> 00:14:44.515
e nelle vostre mani,

00:14:44.515 --> 00:14:48.255
a pochi clic di distanza.

00:14:48.255 --> 00:14:49.737
Grazie.

00:14:49.737 --> 00:14:54.214
(Applausi)

