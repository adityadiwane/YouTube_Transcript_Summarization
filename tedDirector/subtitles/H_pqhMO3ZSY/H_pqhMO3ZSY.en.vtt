WEBVTT
Kind: captions
Language: en

00:00:12.641 --> 00:00:14.995
I would like to tell you a story

00:00:14.995 --> 00:00:18.171
connecting the notorious privacy incident

00:00:18.171 --> 00:00:20.940
involving Adam and Eve,

00:00:20.940 --> 00:00:24.386
and the remarkable shift in the boundaries

00:00:24.386 --> 00:00:27.072
between public and private which has occurred

00:00:27.072 --> 00:00:28.842
in the past 10 years.

00:00:28.842 --> 00:00:30.140
You know the incident.

00:00:30.140 --> 00:00:33.470
Adam and Eve one day in the Garden of Eden

00:00:33.470 --> 00:00:35.313
realize they are naked.

00:00:35.313 --> 00:00:36.813
They freak out.

00:00:36.813 --> 00:00:39.570
And the rest is history.

00:00:39.570 --> 00:00:41.758
Nowadays, Adam and Eve

00:00:41.758 --> 00:00:44.119
would probably act differently.

00:00:44.119 --> 00:00:46.387
[@Adam Last nite was a blast! loved dat apple LOL]

00:00:46.387 --> 00:00:48.260
[@Eve yep.. babe, know what happened to my pants tho?]

00:00:48.260 --> 00:00:50.896
We do reveal so much more information

00:00:50.896 --> 00:00:54.230
about ourselves online than ever before,

00:00:54.230 --> 00:00:55.934
and so much information about us

00:00:55.934 --> 00:00:58.158
is being collected by organizations.

00:00:58.158 --> 00:01:01.440
Now there is much to gain and benefit

00:01:01.440 --> 00:01:03.886
from this massive analysis of personal information,

00:01:03.886 --> 00:01:05.832
or big data,

00:01:05.832 --> 00:01:08.470
but there are also complex tradeoffs that come

00:01:08.470 --> 00:01:11.568
from giving away our privacy.

00:01:11.568 --> 00:01:15.591
And my story is about these tradeoffs.

00:01:15.591 --> 00:01:18.175
We start with an observation which, in my mind,

00:01:18.175 --> 00:01:21.502
has become clearer and clearer in the past few years,

00:01:21.502 --> 00:01:23.599
that any personal information

00:01:23.599 --> 00:01:25.884
can become sensitive information.

00:01:25.884 --> 00:01:30.009
Back in the year 2000, about 100 billion photos

00:01:30.009 --> 00:01:31.921
were shot worldwide,

00:01:31.921 --> 00:01:34.986
but only a minuscule proportion of them

00:01:34.986 --> 00:01:36.869
were actually uploaded online.

00:01:36.869 --> 00:01:40.230
In 2010, only on Facebook, in a single month,

00:01:40.230 --> 00:01:43.500
2.5 billion photos were uploaded,

00:01:43.500 --> 00:01:45.382
most of them identified.

00:01:45.382 --> 00:01:47.262
In the same span of time,

00:01:47.262 --> 00:01:52.132
computers' ability to recognize people in photos

00:01:52.132 --> 00:01:55.740
improved by three orders of magnitude.

00:01:55.740 --> 00:01:57.622
What happens when you combine

00:01:57.622 --> 00:01:59.123
these technologies together:

00:01:59.123 --> 00:02:01.781
increasing availability of facial data;

00:02:01.781 --> 00:02:05.429
improving facial recognizing ability by computers;

00:02:05.429 --> 00:02:07.611
but also cloud computing,

00:02:07.611 --> 00:02:09.499
which gives anyone in this theater

00:02:09.499 --> 00:02:11.059
the kind of computational power

00:02:11.059 --> 00:02:12.945
which a few years ago was only the domain

00:02:12.945 --> 00:02:14.727
of three-letter agencies;

00:02:14.727 --> 00:02:16.105
and ubiquitous computing,

00:02:16.105 --> 00:02:18.997
which allows my phone, which is not a supercomputer,

00:02:18.997 --> 00:02:20.668
to connect to the Internet

00:02:20.668 --> 00:02:23.002
and do there hundreds of thousands

00:02:23.002 --> 00:02:25.641
of face metrics in a few seconds?

00:02:25.641 --> 00:02:28.269
Well, we conjecture that the result

00:02:28.269 --> 00:02:30.333
of this combination of technologies

00:02:30.333 --> 00:02:33.221
will be a radical change in our very notions

00:02:33.221 --> 00:02:35.478
of privacy and anonymity.

00:02:35.478 --> 00:02:37.471
To test that, we did an experiment

00:02:37.471 --> 00:02:39.592
on Carnegie Mellon University campus.

00:02:39.592 --> 00:02:41.691
We asked students who were walking by

00:02:41.691 --> 00:02:43.470
to participate in a study,

00:02:43.470 --> 00:02:46.032
and we took a shot with a webcam,

00:02:46.032 --> 00:02:48.814
and we asked them to fill out a survey on a laptop.

00:02:48.814 --> 00:02:50.793
While they were filling out the survey,

00:02:50.793 --> 00:02:53.590
we uploaded their shot to a cloud-computing cluster,

00:02:53.590 --> 00:02:55.317
and we started using a facial recognizer

00:02:55.317 --> 00:02:57.722
to match that shot to a database

00:02:57.722 --> 00:03:00.115
of some hundreds of thousands of images

00:03:00.115 --> 00:03:03.711
which we had downloaded from Facebook profiles.

00:03:03.711 --> 00:03:06.970
By the time the subject reached the last page

00:03:06.970 --> 00:03:10.317
on the survey, the page had been dynamically updated

00:03:10.317 --> 00:03:12.630
with the 10 best matching photos

00:03:12.630 --> 00:03:14.915
which the recognizer had found,

00:03:14.915 --> 00:03:16.653
and we asked the subjects to indicate

00:03:16.653 --> 00:03:20.773
whether he or she found themselves in the photo.

00:03:20.773 --> 00:03:24.472
Do you see the subject?

00:03:24.472 --> 00:03:27.317
Well, the computer did, and in fact did so

00:03:27.317 --> 00:03:29.466
for one out of three subjects.

00:03:29.466 --> 00:03:32.650
So essentially, we can start from an anonymous face,

00:03:32.650 --> 00:03:36.134
offline or online, and we can use facial recognition

00:03:36.134 --> 00:03:38.494
to give a name to that anonymous face

00:03:38.494 --> 00:03:40.602
thanks to social media data.

00:03:40.602 --> 00:03:42.474
But a few years back, we did something else.

00:03:42.474 --> 00:03:44.297
We started from social media data,

00:03:44.297 --> 00:03:47.348
we combined it statistically with data

00:03:47.348 --> 00:03:49.450
from U.S. government social security,

00:03:49.450 --> 00:03:52.774
and we ended up predicting social security numbers,

00:03:52.774 --> 00:03:54.286
which in the United States

00:03:54.286 --> 00:03:56.326
are extremely sensitive information.

00:03:56.326 --> 00:03:58.419
Do you see where I'm going with this?

00:03:58.419 --> 00:04:01.341
So if you combine the two studies together,

00:04:01.341 --> 00:04:02.853
then the question becomes,

00:04:02.853 --> 00:04:05.573
can you start from a face and,

00:04:05.573 --> 00:04:07.884
using facial recognition, find a name

00:04:07.884 --> 00:04:10.553
and publicly available information

00:04:10.553 --> 00:04:12.485
about that name and that person,

00:04:12.485 --> 00:04:14.733
and from that publicly available information

00:04:14.733 --> 00:04:16.775
infer non-publicly available information,

00:04:16.775 --> 00:04:18.381
much more sensitive ones

00:04:18.381 --> 00:04:19.873
which you link back to the face?

00:04:19.873 --> 00:04:21.789
And the answer is, yes, we can, and we did.

00:04:21.789 --> 00:04:24.357
Of course, the accuracy keeps getting worse.

00:04:24.357 --> 00:04:25.301
[27% of subjects' first 5 SSN digits identified (with 4 attempts)]

00:04:25.301 --> 00:04:29.128
But in fact, we even decided to develop an iPhone app

00:04:29.128 --> 00:04:31.843
which uses the phone's internal camera

00:04:31.843 --> 00:04:33.443
to take a shot of a subject

00:04:33.443 --> 00:04:34.930
and then upload it to a cloud

00:04:34.930 --> 00:04:37.592
and then do what I just described to you in real time:

00:04:37.592 --> 00:04:39.680
looking for a match, finding public information,

00:04:39.680 --> 00:04:41.410
trying to infer sensitive information,

00:04:41.410 --> 00:04:44.001
and then sending back to the phone

00:04:44.001 --> 00:04:47.610
so that it is overlaid on the face of the subject,

00:04:47.610 --> 00:04:49.511
an example of augmented reality,

00:04:49.511 --> 00:04:51.962
probably a creepy example of augmented reality.

00:04:51.962 --> 00:04:55.301
In fact, we didn't develop the app to make it available,

00:04:55.301 --> 00:04:57.223
just as a proof of concept.

00:04:57.223 --> 00:04:59.536
In fact, take these technologies

00:04:59.536 --> 00:05:01.373
and push them to their logical extreme.

00:05:01.373 --> 00:05:04.092
Imagine a future in which strangers around you

00:05:04.092 --> 00:05:06.403
will look at you through their Google Glasses

00:05:06.403 --> 00:05:08.710
or, one day, their contact lenses,

00:05:08.710 --> 00:05:12.730
and use seven or eight data points about you

00:05:12.730 --> 00:05:15.312
to infer anything else

00:05:15.312 --> 00:05:17.915
which may be known about you.

00:05:17.915 --> 00:05:22.709
What will this future without secrets look like?

00:05:22.709 --> 00:05:24.673
And should we care?

00:05:24.673 --> 00:05:26.564
We may like to believe

00:05:26.564 --> 00:05:29.604
that the future with so much wealth of data

00:05:29.604 --> 00:05:32.118
would be a future with no more biases,

00:05:32.118 --> 00:05:35.701
but in fact, having so much information

00:05:35.701 --> 00:05:37.892
doesn't mean that we will make decisions

00:05:37.892 --> 00:05:39.598
which are more objective.

00:05:39.598 --> 00:05:42.158
In another experiment, we presented to our subjects

00:05:42.158 --> 00:05:44.404
information about a potential job candidate.

00:05:44.404 --> 00:05:47.582
We included in this information some references

00:05:47.582 --> 00:05:50.228
to some funny, absolutely legal,

00:05:50.228 --> 00:05:52.693
but perhaps slightly embarrassing information

00:05:52.693 --> 00:05:54.713
that the subject had posted online.

00:05:54.713 --> 00:05:57.079
Now interestingly, among our subjects,

00:05:57.079 --> 00:06:00.162
some had posted comparable information,

00:06:00.162 --> 00:06:02.524
and some had not.

00:06:02.524 --> 00:06:04.473
Which group do you think

00:06:04.473 --> 00:06:09.025
was more likely to judge harshly our subject?

00:06:09.025 --> 00:06:10.982
Paradoxically, it was the group

00:06:10.982 --> 00:06:12.715
who had posted similar information,

00:06:12.715 --> 00:06:15.657
an example of moral dissonance.

00:06:15.657 --> 00:06:17.407
Now you may be thinking,

00:06:17.407 --> 00:06:19.109
this does not apply to me,

00:06:19.109 --> 00:06:21.271
because I have nothing to hide.

00:06:21.271 --> 00:06:23.753
But in fact, privacy is not about

00:06:23.753 --> 00:06:27.429
having something negative to hide.

00:06:27.429 --> 00:06:29.783
Imagine that you are the H.R. director

00:06:29.783 --> 00:06:32.730
of a certain organization, and you receive résumés,

00:06:32.730 --> 00:06:35.203
and you decide to find more information about the candidates.

00:06:35.203 --> 00:06:37.663
Therefore, you Google their names

00:06:37.663 --> 00:06:39.903
and in a certain universe,

00:06:39.903 --> 00:06:41.911
you find this information.

00:06:41.911 --> 00:06:46.348
Or in a parallel universe, you find this information.

00:06:46.348 --> 00:06:49.065
Do you think that you would be equally likely

00:06:49.065 --> 00:06:51.868
to call either candidate for an interview?

00:06:51.868 --> 00:06:54.150
If you think so, then you are not

00:06:54.150 --> 00:06:56.732
like the U.S. employers who are, in fact,

00:06:56.732 --> 00:07:00.039
part of our experiment, meaning we did exactly that.

00:07:00.039 --> 00:07:03.221
We created Facebook profiles, manipulating traits,

00:07:03.221 --> 00:07:06.072
then we started sending out résumés to companies in the U.S.,

00:07:06.072 --> 00:07:07.980
and we detected, we monitored,

00:07:07.980 --> 00:07:10.373
whether they were searching for our candidates,

00:07:10.373 --> 00:07:12.205
and whether they were acting on the information

00:07:12.205 --> 00:07:14.143
they found on social media. And they were.

00:07:14.143 --> 00:07:16.244
Discrimination was happening through social media

00:07:16.244 --> 00:07:19.317
for equally skilled candidates.

00:07:19.317 --> 00:07:23.892
Now marketers like us to believe

00:07:23.892 --> 00:07:26.161
that all information about us will always

00:07:26.161 --> 00:07:29.434
be used in a manner which is in our favor.

00:07:29.434 --> 00:07:33.149
But think again. Why should that be always the case?

00:07:33.149 --> 00:07:35.813
In a movie which came out a few years ago,

00:07:35.813 --> 00:07:38.366
"Minority Report," a famous scene

00:07:38.366 --> 00:07:40.942
had Tom Cruise walk in a mall

00:07:40.942 --> 00:07:44.718
and holographic personalized advertising

00:07:44.718 --> 00:07:46.553
would appear around him.

00:07:46.553 --> 00:07:49.780
Now, that movie is set in 2054,

00:07:49.780 --> 00:07:51.422
about 40 years from now,

00:07:51.422 --> 00:07:54.330
and as exciting as that technology looks,

00:07:54.330 --> 00:07:56.976
it already vastly underestimates

00:07:56.976 --> 00:07:59.116
the amount of information that organizations

00:07:59.116 --> 00:08:01.599
can gather about you, and how they can use it

00:08:01.599 --> 00:08:04.997
to influence you in a way that you will not even detect.

00:08:04.997 --> 00:08:07.100
So as an example, this is another experiment

00:08:07.100 --> 00:08:09.373
actually we are running, not yet completed.

00:08:09.373 --> 00:08:11.692
Imagine that an organization has access

00:08:11.692 --> 00:08:13.748
to your list of Facebook friends,

00:08:13.748 --> 00:08:15.520
and through some kind of algorithm

00:08:15.520 --> 00:08:19.254
they can detect the two friends that you like the most.

00:08:19.254 --> 00:08:21.534
And then they create, in real time,

00:08:21.534 --> 00:08:24.376
a facial composite of these two friends.

00:08:24.376 --> 00:08:27.445
Now studies prior to ours have shown that people

00:08:27.445 --> 00:08:30.330
don't recognize any longer even themselves

00:08:30.330 --> 00:08:32.792
in facial composites, but they react

00:08:32.792 --> 00:08:34.909
to those composites in a positive manner.

00:08:34.909 --> 00:08:38.324
So next time you are looking for a certain product,

00:08:38.324 --> 00:08:40.883
and there is an ad suggesting you to buy it,

00:08:40.883 --> 00:08:43.790
it will not be just a standard spokesperson.

00:08:43.790 --> 00:08:46.103
It will be one of your friends,

00:08:46.103 --> 00:08:49.406
and you will not even know that this is happening.

00:08:49.406 --> 00:08:51.819
Now the problem is that

00:08:51.819 --> 00:08:54.338
the current policy mechanisms we have

00:08:54.338 --> 00:08:57.776
to protect ourselves from the abuses of personal information

00:08:57.776 --> 00:09:00.760
are like bringing a knife to a gunfight.

00:09:00.760 --> 00:09:03.673
One of these mechanisms is transparency,

00:09:03.673 --> 00:09:06.873
telling people what you are going to do with their data.

00:09:06.873 --> 00:09:08.979
And in principle, that's a very good thing.

00:09:08.979 --> 00:09:12.646
It's necessary, but it is not sufficient.

00:09:12.646 --> 00:09:16.344
Transparency can be misdirected.

00:09:16.344 --> 00:09:18.448
You can tell people what you are going to do,

00:09:18.448 --> 00:09:20.680
and then you still nudge them to disclose

00:09:20.680 --> 00:09:23.303
arbitrary amounts of personal information.

00:09:23.303 --> 00:09:26.189
So in yet another experiment, this one with students,

00:09:26.189 --> 00:09:29.247
we asked them to provide information

00:09:29.247 --> 00:09:31.060
about their campus behavior,

00:09:31.060 --> 00:09:34.000
including pretty sensitive questions, such as this one.

00:09:34.000 --> 00:09:34.621
[Have you ever cheated in an exam?]

00:09:34.621 --> 00:09:36.921
Now to one group of subjects, we told them,

00:09:36.921 --> 00:09:39.762
"Only other students will see your answers."

00:09:39.762 --> 00:09:41.341
To another group of subjects, we told them,

00:09:41.341 --> 00:09:44.902
"Students and faculty will see your answers."

00:09:44.902 --> 00:09:47.493
Transparency. Notification. And sure enough, this worked,

00:09:47.493 --> 00:09:48.900
in the sense that the first group of subjects

00:09:48.900 --> 00:09:51.468
were much more likely to disclose than the second.

00:09:51.468 --> 00:09:52.988
It makes sense, right?

00:09:52.988 --> 00:09:54.478
But then we added the misdirection.

00:09:54.478 --> 00:09:57.238
We repeated the experiment with the same two groups,

00:09:57.238 --> 00:09:59.665
this time adding a delay

00:09:59.665 --> 00:10:02.600
between the time we told subjects

00:10:02.600 --> 00:10:04.680
how we would use their data

00:10:04.680 --> 00:10:09.068
and the time we actually started answering the questions.

00:10:09.068 --> 00:10:11.629
How long a delay do you think we had to add

00:10:11.629 --> 00:10:16.242
in order to nullify the inhibitory effect

00:10:16.242 --> 00:10:19.653
of knowing that faculty would see your answers?

00:10:19.653 --> 00:10:21.433
Ten minutes?

00:10:21.433 --> 00:10:23.224
Five minutes?

00:10:23.224 --> 00:10:25.000
One minute?

00:10:25.000 --> 00:10:27.049
How about 15 seconds?

00:10:27.049 --> 00:10:29.717
Fifteen seconds were sufficient to have the two groups

00:10:29.717 --> 00:10:31.285
disclose the same amount of information,

00:10:31.285 --> 00:10:34.031
as if the second group now no longer cares

00:10:34.031 --> 00:10:36.687
for faculty reading their answers.

00:10:36.687 --> 00:10:40.023
Now I have to admit that this talk so far

00:10:40.023 --> 00:10:42.503
may sound exceedingly gloomy,

00:10:42.503 --> 00:10:44.224
but that is not my point.

00:10:44.224 --> 00:10:46.923
In fact, I want to share with you the fact that

00:10:46.923 --> 00:10:48.695
there are alternatives.

00:10:48.695 --> 00:10:51.194
The way we are doing things now is not the only way

00:10:51.194 --> 00:10:54.231
they can done, and certainly not the best way

00:10:54.231 --> 00:10:56.258
they can be done.

00:10:56.258 --> 00:11:00.429
When someone tells you, "People don't care about privacy,"

00:11:00.429 --> 00:11:03.071
consider whether the game has been designed

00:11:03.071 --> 00:11:05.795
and rigged so that they cannot care about privacy,

00:11:05.795 --> 00:11:09.057
and coming to the realization that these manipulations occur

00:11:09.057 --> 00:11:10.664
is already halfway through the process

00:11:10.664 --> 00:11:12.922
of being able to protect yourself.

00:11:12.922 --> 00:11:16.632
When someone tells you that privacy is incompatible

00:11:16.632 --> 00:11:18.481
with the benefits of big data,

00:11:18.481 --> 00:11:20.954
consider that in the last 20 years,

00:11:20.954 --> 00:11:22.871
researchers have created technologies

00:11:22.871 --> 00:11:26.189
to allow virtually any electronic transactions

00:11:26.189 --> 00:11:29.938
to take place in a more privacy-preserving manner.

00:11:29.938 --> 00:11:32.493
We can browse the Internet anonymously.

00:11:32.493 --> 00:11:35.171
We can send emails that can only be read

00:11:35.171 --> 00:11:38.880
by the intended recipient, not even the NSA.

00:11:38.880 --> 00:11:41.877
We can have even privacy-preserving data mining.

00:11:41.877 --> 00:11:45.771
In other words, we can have the benefits of big data

00:11:45.771 --> 00:11:47.903
while protecting privacy.

00:11:47.903 --> 00:11:51.694
Of course, these technologies imply a shifting

00:11:51.694 --> 00:11:53.240
of cost and revenues

00:11:53.240 --> 00:11:55.347
between data holders and data subjects,

00:11:55.347 --> 00:11:58.800
which is why, perhaps, you don't hear more about them.

00:11:58.800 --> 00:12:02.506
Which brings me back to the Garden of Eden.

00:12:02.506 --> 00:12:05.286
There is a second privacy interpretation

00:12:05.286 --> 00:12:07.095
of the story of the Garden of Eden

00:12:07.095 --> 00:12:09.191
which doesn't have to do with the issue

00:12:09.191 --> 00:12:11.416
of Adam and Eve feeling naked

00:12:11.416 --> 00:12:13.797
and feeling ashamed.

00:12:13.797 --> 00:12:16.578
You can find echoes of this interpretation

00:12:16.578 --> 00:12:19.360
in John Milton's "Paradise Lost."

00:12:19.360 --> 00:12:23.557
In the garden, Adam and Eve are materially content.

00:12:23.557 --> 00:12:25.661
They're happy. They are satisfied.

00:12:25.661 --> 00:12:27.954
However, they also lack knowledge

00:12:27.954 --> 00:12:29.594
and self-awareness.

00:12:29.594 --> 00:12:32.913
The moment they eat the aptly named

00:12:32.913 --> 00:12:34.206
fruit of knowledge,

00:12:34.206 --> 00:12:36.811
that's when they discover themselves.

00:12:36.811 --> 00:12:40.842
They become aware. They achieve autonomy.

00:12:40.842 --> 00:12:43.968
The price to pay, however, is leaving the garden.

00:12:43.968 --> 00:12:47.849
So privacy, in a way, is both the means

00:12:47.849 --> 00:12:50.811
and the price to pay for freedom.

00:12:50.811 --> 00:12:53.581
Again, marketers tell us

00:12:53.581 --> 00:12:56.600
that big data and social media

00:12:56.600 --> 00:12:59.579
are not just a paradise of profit for them,

00:12:59.579 --> 00:13:02.036
but a Garden of Eden for the rest of us.

00:13:02.036 --> 00:13:03.274
We get free content.

00:13:03.274 --> 00:13:06.397
We get to play Angry Birds. We get targeted apps.

00:13:06.397 --> 00:13:09.294
But in fact, in a few years, organizations

00:13:09.294 --> 00:13:10.903
will know so much about us,

00:13:10.903 --> 00:13:13.613
they will be able to infer our desires

00:13:13.613 --> 00:13:15.817
before we even form them, and perhaps

00:13:15.817 --> 00:13:18.264
buy products on our behalf

00:13:18.264 --> 00:13:20.538
before we even know we need them.

00:13:20.538 --> 00:13:23.775
Now there was one English author

00:13:23.775 --> 00:13:26.820
who anticipated this kind of future

00:13:26.820 --> 00:13:28.225
where we would trade away

00:13:28.225 --> 00:13:31.773
our autonomy and freedom for comfort.

00:13:31.773 --> 00:13:33.934
Even more so than George Orwell,

00:13:33.934 --> 00:13:36.695
the author is, of course, Aldous Huxley.

00:13:36.695 --> 00:13:39.549
In "Brave New World," he imagines a society

00:13:39.549 --> 00:13:41.720
where technologies that we created

00:13:41.720 --> 00:13:43.579
originally for freedom

00:13:43.579 --> 00:13:46.146
end up coercing us.

00:13:46.146 --> 00:13:50.937
However, in the book, he also offers us a way out

00:13:50.937 --> 00:13:54.375
of that society, similar to the path

00:13:54.375 --> 00:13:58.330
that Adam and Eve had to follow to leave the garden.

00:13:58.330 --> 00:14:00.477
In the words of the Savage,

00:14:00.477 --> 00:14:03.546
regaining autonomy and freedom is possible,

00:14:03.546 --> 00:14:06.225
although the price to pay is steep.

00:14:06.225 --> 00:14:11.940
So I do believe that one of the defining fights

00:14:11.940 --> 00:14:14.503
of our times will be the fight

00:14:14.503 --> 00:14:16.890
for the control over personal information,

00:14:16.890 --> 00:14:20.397
the fight over whether big data will become a force

00:14:20.397 --> 00:14:21.686
for freedom,

00:14:21.686 --> 00:14:26.432
rather than a force which will hiddenly manipulate us.

00:14:26.432 --> 00:14:29.025
Right now, many of us

00:14:29.025 --> 00:14:31.778
do not even know that the fight is going on,

00:14:31.778 --> 00:14:34.450
but it is, whether you like it or not.

00:14:34.450 --> 00:14:37.254
And at the risk of playing the serpent,

00:14:37.254 --> 00:14:40.151
I will tell you that the tools for the fight

00:14:40.151 --> 00:14:43.160
are here, the awareness of what is going on,

00:14:43.160 --> 00:14:44.515
and in your hands,

00:14:44.515 --> 00:14:48.255
just a few clicks away.

00:14:48.255 --> 00:14:49.737
Thank you.

00:14:49.737 --> 00:14:54.214
(Applause)

