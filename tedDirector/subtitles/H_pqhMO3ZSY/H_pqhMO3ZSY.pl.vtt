WEBVTT
Kind: captions
Language: pl

00:00:00.000 --> 00:00:07.000
Tłumaczenie: Joanna Zając
Korekta: Agata Leśnicka

00:00:12.641 --> 00:00:14.995
Opowiem historię,

00:00:14.995 --> 00:00:18.171
związaną z głośnym przypadkiem
dotyczącym prywatności

00:00:18.171 --> 00:00:20.940
z Adamem i Ewą w roli głównej,

00:00:20.940 --> 00:00:24.386
i znaczącym przesunięciem granicy

00:00:24.386 --> 00:00:27.072
pomiędzy sferą publiczną i prywatną

00:00:27.072 --> 00:00:28.842
w ciągu ostatnich 10 lat.

00:00:28.842 --> 00:00:30.140
Znacie tę historię.

00:00:30.140 --> 00:00:33.470
Pewnego dnia w raju, Adam i Ewa

00:00:33.470 --> 00:00:35.313
spostrzegli, że są nadzy.

00:00:35.313 --> 00:00:36.813
Spanikowali.

00:00:36.813 --> 00:00:39.570
Reszta jest historią.

00:00:39.570 --> 00:00:41.758
Obecnie, Adam i Ewa

00:00:41.758 --> 00:00:44.119
pewnie zachowaliby się inaczej.

00:00:44.119 --> 00:00:46.387
[@Adam Super ubaw wczoraj! jabłko ekstra LOL]

00:00:46.387 --> 00:00:48.260
[@Ewa no... kotku, wiesz co się stało z moimi spodniami?]

00:00:48.260 --> 00:00:50.896
Ujawniamy w internecie znacznie więcej informacji

00:00:50.896 --> 00:00:54.230
niż kiedyś,

00:00:54.230 --> 00:00:55.934
a organizacje zbierają o nas

00:00:55.934 --> 00:00:58.158
coraz więcej danych.

00:00:58.158 --> 00:01:01.440
Można wiele skorzystać

00:01:01.440 --> 00:01:03.886
z masowej analizy informacji osobistych

00:01:03.886 --> 00:01:05.832
i dużej ilości danych,

00:01:05.832 --> 00:01:08.470
lecz oddawanie prywatności

00:01:08.470 --> 00:01:11.568
odbywa się kosztem złożonych kompromisów.

00:01:11.568 --> 00:01:15.591
O tym jest ta historia.

00:01:15.591 --> 00:01:18.175
Zacznijmy od pewnej obserwacji, która dla mnie

00:01:18.175 --> 00:01:21.502
stawała się coraz wyraźniejsza w ostatnich latach.

00:01:21.502 --> 00:01:23.599
Każdy rodzaj danych osobowych

00:01:23.599 --> 00:01:25.884
może stać się informacją wrażliwą.

00:01:25.884 --> 00:01:30.009
W roku 2000 wykonano na całym świecie

00:01:30.009 --> 00:01:31.921
około 100 miliardów zdjęć,

00:01:31.921 --> 00:01:34.986
ale tylko minimalny ich procent

00:01:34.986 --> 00:01:36.869
znalazł się w Internecie.

00:01:36.869 --> 00:01:40.230
W roku 2010 tylko na Facebooku, w ciągu miesiąca,

00:01:40.230 --> 00:01:43.500
umieszczono 2,5 miliarda zdjęć,

00:01:43.500 --> 00:01:45.382
w większości opisanych.

00:01:45.382 --> 00:01:47.262
W tym samym przedziale czasu

00:01:47.262 --> 00:01:52.132
zdolność komputerów do rozpoznania fotografii ludzi

00:01:52.132 --> 00:01:55.740
poprawiła się o trzy rzędy wielkości.

00:01:55.740 --> 00:01:57.622
Co się dzieje

00:01:57.622 --> 00:01:59.123
po połączeniu tych technologii:

00:01:59.123 --> 00:02:01.781
wzrasta dostępność do zdjęć twarzy,

00:02:01.781 --> 00:02:05.429
poprawia się zdolność komputerów do ich rozpoznania

00:02:05.429 --> 00:02:07.611
a chmury obliczeniowe,

00:02:07.611 --> 00:02:09.499
dają każdemu użytkownikowi

00:02:09.499 --> 00:02:11.059
moc obliczeniową,

00:02:11.059 --> 00:02:12.945
która jeszcze kilka lat temu

00:02:12.945 --> 00:02:14.727
była domeną tylko trzyliterowych agencji,

00:02:14.727 --> 00:02:16.105
plus wszechobecne obliczenia,

00:02:16.105 --> 00:02:18.997
pozwalające mojemu telefonowi, który nie jest superkomputerem,

00:02:18.997 --> 00:02:20.668
na połączenie z Internetem

00:02:20.668 --> 00:02:23.002
i zebranie setek tysięcy danych

00:02:23.002 --> 00:02:25.641
o twarzach w ciągu sekund?

00:02:25.641 --> 00:02:28.269
Podejrzewamy,

00:02:28.269 --> 00:02:30.333
że połączenie tych technologii

00:02:30.333 --> 00:02:33.221
spowoduje znaczącą zmianę

00:02:33.221 --> 00:02:35.478
w postrzeganiu prywatności i anonimowości.

00:02:35.478 --> 00:02:37.471
By to sprawdzić, przeprowadziliśmy eksperyment

00:02:37.471 --> 00:02:39.592
na kampusie Carnegie Mellon University.

00:02:39.592 --> 00:02:41.691
Poprosiliśmy przechodzących studentów

00:02:41.691 --> 00:02:43.470
o udziału w badaniu.

00:02:43.470 --> 00:02:46.032
Zrobiliśmy im zdjęcia kamerą internetową,

00:02:46.032 --> 00:02:48.814
i poprosiliśmy o wypełnienie ankiety na laptopie.

00:02:48.814 --> 00:02:50.793
W międzyczasie

00:02:50.793 --> 00:02:53.590
przesłaliśmy ich zdjęcia na klaster chmur obliczeniowych

00:02:53.590 --> 00:02:55.317
i przy użyciu rozpoznawania twarzy

00:02:55.317 --> 00:02:57.722
zaczęliśmy szukać dopasowań

00:02:57.722 --> 00:03:00.115
wśród setek tysięcy zdjęć

00:03:00.115 --> 00:03:03.711
ściągniętych z profili na Facebooku.

00:03:03.711 --> 00:03:06.970
Zanim badany dotarł do ostatniej strony ankiety,

00:03:06.970 --> 00:03:10.317
ona już zawierała

00:03:10.317 --> 00:03:12.630
10 najbardziej pasujących zdjęć,

00:03:12.630 --> 00:03:14.915
które znalazł identyfikator.

00:03:14.915 --> 00:03:16.653
Poprosiliśmy badanych

00:03:16.653 --> 00:03:20.773
o stwierdzenie,czy odnaleźli się na tych zdjęciach.

00:03:20.773 --> 00:03:24.472
Widzicie badanego?

00:03:24.472 --> 00:03:27.317
Komputer go zobaczył,

00:03:27.317 --> 00:03:29.466
i było tak dla jednej trzeciej badanych.

00:03:29.466 --> 00:03:32.650
A więc możemy zacząć od anonimowej twarzy,

00:03:32.650 --> 00:03:36.134
offline, czy online, użyć identyfikacji twarzy,

00:03:36.134 --> 00:03:38.494
aby dzięki danym z mediów społecznościowych

00:03:38.494 --> 00:03:40.602
nadać jej imię.

00:03:40.602 --> 00:03:42.474
Kilka lat temu zrobiliśmy coś innego.

00:03:42.474 --> 00:03:44.297
Zaczęliśmy od danych z portali społecznościowych,

00:03:44.297 --> 00:03:47.348
które połączyliśmy zależnościami statystycznymi

00:03:47.348 --> 00:03:49.450
z danymi z amerykańskiego rządowego systemu ubezpieczeń społecznych.

00:03:49.450 --> 00:03:52.774
Udało nam się przewidzieć numery ubezpieczeń społecznych,

00:03:52.774 --> 00:03:54.286
które w USA

00:03:54.286 --> 00:03:56.326
są niezwykle wrażliwymi danymi.

00:03:56.326 --> 00:03:58.419
Rozumiecie, dokąd zmierzam?

00:03:58.419 --> 00:04:01.341
Jeśli połączymy te dwa badania,

00:04:01.341 --> 00:04:02.853
powstaje pytanie,

00:04:02.853 --> 00:04:05.573
czy można zacząć od twarzy,

00:04:05.573 --> 00:04:07.884
i używając identyfikatora twarzy,

00:04:07.884 --> 00:04:10.553
znaleźć imię i informacje publicznie dostępne

00:04:10.553 --> 00:04:12.485
na temat danej osoby,

00:04:12.485 --> 00:04:14.733
a potem, używając tych informacji,

00:04:14.733 --> 00:04:16.775
dotrzeć do danych publicznie niedostępnych,

00:04:16.775 --> 00:04:18.381
które są

00:04:18.381 --> 00:04:19.873
znacznie bardziej wrażliwe?

00:04:19.873 --> 00:04:21.789
Odpowiedź brzmi: tak i to właśnie zrobiliśmy.

00:04:21.789 --> 00:04:24.357
Oczywiście przy kolejnych dopasowaniach spada dokładność.

00:04:24.357 --> 00:04:25.301
[zidentyfikowano 5 pierwszych cyfr numeru ubezpieczenia w 27% (po 4 próbach)]

00:04:25.301 --> 00:04:29.128
Stworzyliśmy aplikację na iPhone'a,

00:04:29.128 --> 00:04:31.843
która wykorzystuje wbudowaną kamerę

00:04:31.843 --> 00:04:33.443
do zrobienia zdjęcia obiektu,

00:04:33.443 --> 00:04:34.930
a potem wysyła je do chmury

00:04:34.930 --> 00:04:37.592
i robi opisywane rzeczy w czasie rzeczywistym:

00:04:37.592 --> 00:04:39.680
dopasowuje, wyszukuje dane publiczne,

00:04:39.680 --> 00:04:41.410
próbuje pozyskać poufne dane,

00:04:41.410 --> 00:04:44.001
a potem przesyła je z powrotem

00:04:44.001 --> 00:04:47.610
i nakłada na twarz obiektu,

00:04:47.610 --> 00:04:49.511
dając, pewnie trochę przerażający,

00:04:49.511 --> 00:04:51.962
przykład rzeczywistości rozszerzonej.

00:04:51.962 --> 00:04:55.301
Ta aplikacja nie powstała do użytku publicznego,

00:04:55.301 --> 00:04:57.223
tylko żeby udowodnić, że to możliwe.

00:04:57.223 --> 00:04:59.536
Spójrzmy na te technologie

00:04:59.536 --> 00:05:01.373
i wyobraźmy sobie ich ekstremalne zastosowania.

00:05:01.373 --> 00:05:04.092
Wyobraźmy sobie przyszłość,

00:05:04.092 --> 00:05:06.403
w której obcy ludzie patrzą na nas przez Google Glass,

00:05:06.403 --> 00:05:08.710
a kiedyś, przez soczewki kontaktowe,

00:05:08.710 --> 00:05:12.730
i używając siedmiu czy ośmiu informacji

00:05:12.730 --> 00:05:15.312
docierają do wszelkich innych

00:05:15.312 --> 00:05:17.915
dostępnych o nas danych.

00:05:17.915 --> 00:05:22.709
Jak będzie wyglądać przyszłość bez tajemnic?

00:05:22.709 --> 00:05:24.673
I czy powinno nas to obchodzić?

00:05:24.673 --> 00:05:26.564
Możemy chcieć wierzyć,

00:05:26.564 --> 00:05:29.604
że przyszłość z taką ilością dostępnych danych

00:05:29.604 --> 00:05:32.118
będzie pozbawiona zniekształceń,

00:05:32.118 --> 00:05:35.701
ale w zasadzie

00:05:35.701 --> 00:05:37.892
posiadanie tak wielu danych nie oznacza,

00:05:37.892 --> 00:05:39.598
że nasze decyzje będą bardziej obiektywne.

00:05:39.598 --> 00:05:42.158
W kolejnym eksperymencie dostarczyliśmy badanym

00:05:42.158 --> 00:05:44.404
dane o potencjalnym kandydacie do pracy.

00:05:44.404 --> 00:05:47.582
Były tam wzmianki

00:05:47.582 --> 00:05:50.228
o pewnych zabawnych, całkowicie legalnych,

00:05:50.228 --> 00:05:52.693
ale trochę żenujących sprawach,

00:05:52.693 --> 00:05:54.713
które kandydat umieścił w Internecie.

00:05:54.713 --> 00:05:57.079
Co ciekawe,

00:05:57.079 --> 00:06:00.162
niektórzy z naszych badanych

00:06:00.162 --> 00:06:02.524
zamieszczali podobne informacje, a niektórzy nie.

00:06:02.524 --> 00:06:04.473
Jak myślicie,

00:06:04.473 --> 00:06:09.025
która grupa była bardziej surowa w ocenie?

00:06:09.025 --> 00:06:10.982
Paradoksalnie, to ci,

00:06:10.982 --> 00:06:12.715
którzy umieszczali w sieci podobne informacje.

00:06:12.715 --> 00:06:15.657
To przykład dysonansu moralnego.

00:06:15.657 --> 00:06:17.407
Możecie myśleć,

00:06:17.407 --> 00:06:19.109
że was to nie dotyczy,

00:06:19.109 --> 00:06:21.271
bo nie macie nic do ukrycia.

00:06:21.271 --> 00:06:23.753
Ale faktycznie, w prywatności nie chodzi o to,

00:06:23.753 --> 00:06:27.429
że mamy coś negatywnego do ukrycia.

00:06:27.429 --> 00:06:29.783
Wyobraźcie sobie, że jesteście dyrektorami personalnymi

00:06:29.783 --> 00:06:32.730
w pewnej organizacji i dostajecie CV.

00:06:32.730 --> 00:06:35.203
Postanawiacie dowiedzieć się więcej o kandydatach.

00:06:35.203 --> 00:06:37.663
Szukacie ich w Google,

00:06:37.663 --> 00:06:39.903
i w jednym przypadku

00:06:39.903 --> 00:06:41.911
znajdujecie określoną informację.

00:06:41.911 --> 00:06:46.348
A drugim, inną informację.

00:06:46.348 --> 00:06:49.065
Sądzicie, że bylibyście tak samo skłonni

00:06:49.065 --> 00:06:51.868
zaprosić każdego z kandydatów na rozmowę?

00:06:51.868 --> 00:06:54.150
Jeśli tak, to nie jesteście podobni

00:06:54.150 --> 00:06:56.732
do amerykańskich pracodawców,

00:06:56.732 --> 00:07:00.039
którzy uczestniczyli w naszym badaniu.

00:07:00.039 --> 00:07:03.221
Stworzyliśmy profile na Facebooku, manipulując cechami,

00:07:03.221 --> 00:07:06.072
a potem porozsyłaliśmy CV do amerykańskich firm.

00:07:06.072 --> 00:07:07.980
Obserwowaliśmy

00:07:07.980 --> 00:07:10.373
czy szukają naszych kandydatów,

00:07:10.373 --> 00:07:12.205
i czy wykorzystują informacje,

00:07:12.205 --> 00:07:14.143
z portali społecznościowych. I wykorzystywali.

00:07:14.143 --> 00:07:16.244
Dla tak samo wykwalifikowanych kandydatów

00:07:16.244 --> 00:07:19.317
media te powodowały dyskryminację.

00:07:19.317 --> 00:07:23.892
Marketingowcy chcą nas przekonać,

00:07:23.892 --> 00:07:26.161
że każda informacja o nas

00:07:26.161 --> 00:07:29.434
zawsze będzie wykorzystana na naszą korzyść.

00:07:29.434 --> 00:07:33.149
Ale zastanówcie się. Dlaczego miałoby tak być?

00:07:33.149 --> 00:07:35.813
W filmie sprzed kilku lat

00:07:35.813 --> 00:07:38.366
"Raport Mniejszości", w słynnej scenie

00:07:38.366 --> 00:07:40.942
Tom Cruise idzie przez centrum handlowe,

00:07:40.942 --> 00:07:44.718
a obok niego pojawia się

00:07:44.718 --> 00:07:46.553
hologram ze spersonalizowaną reklamą.

00:07:46.553 --> 00:07:49.780
Akcja filmu dzieje się w 2054 roku,

00:07:49.780 --> 00:07:51.422
za jakieś 40 lat,

00:07:51.422 --> 00:07:54.330
i jakkolwiek ekscytująco wygląda ta technologia,

00:07:54.330 --> 00:07:56.976
to już znacznie nie docenia

00:07:56.976 --> 00:07:59.116
ilości informacji, które organizacje

00:07:59.116 --> 00:08:01.599
są w stanie o nas zebrać,

00:08:01.599 --> 00:08:04.997
i jak niezauważalnie mogą na nas wpływać.

00:08:04.997 --> 00:08:07.100
I kolejne badanie,

00:08:07.100 --> 00:08:09.373
jeszcze nie jest skończone.

00:08:09.373 --> 00:08:11.692
Wyobraźcie sobie, że organizacja ma dostęp

00:08:11.692 --> 00:08:13.748
do listy waszych przyjaciół na Facebooku,

00:08:13.748 --> 00:08:15.520
i dzięki jakiemuś algorytmowi

00:08:15.520 --> 00:08:19.254
potrafi stwierdzić, których dwóch lubicie najbardziej.

00:08:19.254 --> 00:08:21.534
Potrafi w czasie rzeczywistym

00:08:21.534 --> 00:08:24.376
stworzyć połączenie twarzy tych osób.

00:08:24.376 --> 00:08:27.445
Poprzednie badania wskazują,

00:08:27.445 --> 00:08:30.330
że w przypadku kombinacji rysów twarzy,

00:08:30.330 --> 00:08:32.792
ludzie nie rozpoznają nawet siebie,

00:08:32.792 --> 00:08:34.909
ale reagują w pozytywny sposób.

00:08:34.909 --> 00:08:38.324
Zatem następnym razem, poszukując jakiegoś produktu,

00:08:38.324 --> 00:08:40.883
zobaczycie reklamę, w której nie wystąpi

00:08:40.883 --> 00:08:43.790
zwykły aktor,

00:08:43.790 --> 00:08:46.103
ale jeden z waszych przyjaciół,

00:08:46.103 --> 00:08:49.406
a wy nawet nie będziecie tego świadomi.

00:08:49.406 --> 00:08:51.819
Problem w tym,

00:08:51.819 --> 00:08:54.338
że obecne mechanizmy

00:08:54.338 --> 00:08:57.776
chroniące przed nadużyciami danych osobowych,

00:08:57.776 --> 00:09:00.760
są jak przyniesienie noża na strzelaninę.

00:09:00.760 --> 00:09:03.673
Jednym z nich jest przejrzystość,

00:09:03.673 --> 00:09:06.873
nakazująca informowanie ludzi, co zrobisz z ich danymi.

00:09:06.873 --> 00:09:08.979
W teorii to dobra rzecz.

00:09:08.979 --> 00:09:12.646
Potrzebna, ale niewystarczająca.

00:09:12.646 --> 00:09:16.344
Przejrzystość można przeinaczyć.

00:09:16.344 --> 00:09:18.448
Możesz powiedzieć, co chcesz zrobić,

00:09:18.448 --> 00:09:20.680
a potem nadal przekonywać ich

00:09:20.680 --> 00:09:23.303
do ujawnienia kolejnych prywatnych danych.

00:09:23.303 --> 00:09:26.189
W innym eksperymencie poproszono studentów

00:09:26.189 --> 00:09:29.247
o dostarczenie informacji

00:09:29.247 --> 00:09:31.060
na temat ich zachowań studenckich,

00:09:31.060 --> 00:09:34.000
między innymi dość wrażliwych kwestii, na przykład:

00:09:34.000 --> 00:09:34.621
["Czy kiedyś ściągałeś na egzaminie?"]

00:09:34.621 --> 00:09:36.921
Jednej grupie powiedzieliśmy:

00:09:36.921 --> 00:09:39.762
"Tylko inni studenci zobaczą wasze odpowiedzi".

00:09:39.762 --> 00:09:41.341
Drugiej zaś:

00:09:41.341 --> 00:09:44.902
"Wasze odpowiedzi zobaczą studenci i wykładowcy".

00:09:44.902 --> 00:09:47.493
Przejrzystość. Powiadomienie. I zadziałało.

00:09:47.493 --> 00:09:48.900
Pierwsza grupa badanych

00:09:48.900 --> 00:09:51.468
znacznie chętniej ujawniała informacje.

00:09:51.468 --> 00:09:52.988
Ma to sens, prawda?

00:09:52.988 --> 00:09:54.478
Potem odwróciliśmy kota ogonem.

00:09:54.478 --> 00:09:57.238
Powtórzyliśmy eksperyment z tymi samymi grupami,

00:09:57.238 --> 00:09:59.665
tym razem robiąc przerwę

00:09:59.665 --> 00:10:02.600
pomiędzy informacją,

00:10:02.600 --> 00:10:04.680
jak wykorzystamy dane,

00:10:04.680 --> 00:10:09.068
a faktycznym czasem odpowiadania na pytania.

00:10:09.068 --> 00:10:11.629
Jak długa musiała być przerwa,

00:10:11.629 --> 00:10:16.242
by usunąć zahamowania

00:10:16.242 --> 00:10:19.653
powstałe z wiedzy o pokazaniu odpowiedzi wykładowcom?

00:10:19.653 --> 00:10:21.433
Dziesięć minut?

00:10:21.433 --> 00:10:23.224
Pięć?

00:10:23.224 --> 00:10:25.000
Jedną minutę?

00:10:25.000 --> 00:10:27.049
Może 15 sekund?

00:10:27.049 --> 00:10:29.717
Wystarczyło 15 sekund,

00:10:29.717 --> 00:10:31.285
żeby obie grupy ujawniły tyle samo informacji,

00:10:31.285 --> 00:10:34.031
tak jakby drugiej grupy już nie obchodziło,

00:10:34.031 --> 00:10:36.687
że wykładowcy zobaczą ich odpowiedzi.

00:10:36.687 --> 00:10:40.023
Muszę przyznać, że na razie moje wystąpienie

00:10:40.023 --> 00:10:42.503
brzmi strasznie przygnębiająco,

00:10:42.503 --> 00:10:44.224
a nie o to mi chodzi.

00:10:44.224 --> 00:10:46.923
Naprawdę chcę powiedzieć,

00:10:46.923 --> 00:10:48.695
że są inne rozwiązania.

00:10:48.695 --> 00:10:51.194
Sposób, w jaki działamy teraz,

00:10:51.194 --> 00:10:54.231
to nie jedyna możliwość,

00:10:54.231 --> 00:10:56.258
a na pewno nie najlepsza.

00:10:56.258 --> 00:11:00.429
Kiedy ktoś wam mówi: "Ludzi nie obchodzi prywatność",

00:11:00.429 --> 00:11:03.071
zastanówcie się, czy nie zaprojektowano tego tak,

00:11:03.071 --> 00:11:05.795
by nie byli w stanie o tym pomyśleć.

00:11:05.795 --> 00:11:09.057
Zrozumienie takiej manipulacji

00:11:09.057 --> 00:11:10.664
to połowa drogi

00:11:10.664 --> 00:11:12.922
do ochrony siebie.

00:11:12.922 --> 00:11:16.632
Kiedy mówią wam, że prywatność nie idzie w parze

00:11:16.632 --> 00:11:18.481
z korzyściami, które dają wielkie zbiory danych,

00:11:18.481 --> 00:11:20.954
pomyślcie, że za 20 lat

00:11:20.954 --> 00:11:22.871
badacze stworzą technologie,

00:11:22.871 --> 00:11:26.189
które pozwolą dokonywać transakcji elektronicznych

00:11:26.189 --> 00:11:29.938
w sposób bardziej chroniący prywatność.

00:11:29.938 --> 00:11:32.493
Możemy anonimowo przeglądać internet.

00:11:32.493 --> 00:11:35.171
Możemy wysyłać e-maile, czytane przez adresata

00:11:35.171 --> 00:11:38.880
a nie przez agencję bezpieczeństwa wewnętrznego.

00:11:38.880 --> 00:11:41.877
Możemy nawet mieć chroniącą prywatność eksplorację danych.

00:11:41.877 --> 00:11:45.771
Innymi słowy, możemy mieć korzyści z wielkich zbiorów danych,

00:11:45.771 --> 00:11:47.903
chroniąc jednocześnie prywatność.

00:11:47.903 --> 00:11:51.694
Oczywiście, wprowadzenie tych technologii

00:11:51.694 --> 00:11:53.240
oznacza przesunięcie kosztu i przychodu

00:11:53.240 --> 00:11:55.347
pomiędzy właścicielami danych i ich podmiotami

00:11:55.347 --> 00:11:58.800
i pewnie dlatego nie słyszy się o tym więcej.

00:11:58.800 --> 00:12:02.506
Wracam więc do raju.

00:12:02.506 --> 00:12:05.286
Jest druga interpretacja tej historii,

00:12:05.286 --> 00:12:07.095
jeśli chodzi o prywatność.

00:12:07.095 --> 00:12:09.191
Nie chodzi o to,

00:12:09.191 --> 00:12:11.416
że Adam i Ewa poczuli się nadzy

00:12:11.416 --> 00:12:13.797
i zawstydzeni.

00:12:13.797 --> 00:12:16.578
Echo tej interpretacji mamy

00:12:16.578 --> 00:12:19.360
w "Raju utraconym" Johna Miltona.

00:12:19.360 --> 00:12:23.557
W ogrodzie Adam i Ewa są naprawdę zadowoleni.

00:12:23.557 --> 00:12:25.661
Szczęśliwi. Usatysfakcjonowani.

00:12:25.661 --> 00:12:27.954
Jednak brak im wiedzy

00:12:27.954 --> 00:12:29.594
i samoświadomości.

00:12:29.594 --> 00:12:32.913
Kiedy zjadają, trafnie nazwany,

00:12:32.913 --> 00:12:34.206
owoc poznania,

00:12:34.206 --> 00:12:36.811
odkrywają samych siebie.

00:12:36.811 --> 00:12:40.842
Zyskują świadomość i niezależność.

00:12:40.842 --> 00:12:43.968
Ceną jest opuszczenie ogrodu.

00:12:43.968 --> 00:12:47.849
Prywatność zatem,

00:12:47.849 --> 00:12:50.811
jest zarówno drogą do wolności, jak i jej kosztem.

00:12:50.811 --> 00:12:53.581
Marketingowcy mówią nam,

00:12:53.581 --> 00:12:56.600
że wielkie zbiory danych i media społecznościowe

00:12:56.600 --> 00:12:59.579
to nie tylko raj korzyści dla nich,

00:12:59.579 --> 00:13:02.036
ale i dla nas.

00:13:02.036 --> 00:13:03.274
Dostajemy darmowe udogodnienia:

00:13:03.274 --> 00:13:06.397
Angry Birds czy spersonalizowane aplikacje.

00:13:06.397 --> 00:13:09.294
Tak naprawdę, za kilka lat

00:13:09.294 --> 00:13:10.903
organizacje będą wiedziały o nas tak wiele,

00:13:10.903 --> 00:13:13.613
że będą znać nasze potrzeby,

00:13:13.613 --> 00:13:15.817
zanim my je odkryjemy,

00:13:15.817 --> 00:13:18.264
kupować w naszym imieniu produkty,

00:13:18.264 --> 00:13:20.538
zanim zorientujemy się, że ich potrzebujemy.

00:13:20.538 --> 00:13:23.775
Był pewien angielski pisarz,

00:13:23.775 --> 00:13:26.820
który przewidział taką przyszłość,

00:13:26.820 --> 00:13:28.225
w której oddawalibyśmy

00:13:28.225 --> 00:13:31.773
naszą niezależność i wolność za wygodę.

00:13:31.773 --> 00:13:33.934
Bardziej niż George Orwell

00:13:33.934 --> 00:13:36.695
był nim Aldous Huxley.

00:13:36.695 --> 00:13:39.549
W "Nowym wspaniałym świecie" opisuje społeczeństwo,

00:13:39.549 --> 00:13:41.720
w którym nacisk wywierają technologie,

00:13:41.720 --> 00:13:43.579
pierwotnie stworzone

00:13:43.579 --> 00:13:46.146
dla wolności.

00:13:46.146 --> 00:13:50.937
Jednak proponuje także

00:13:50.937 --> 00:13:54.375
sposób ucieczki z tego społeczeństwa,

00:13:54.375 --> 00:13:58.330
podobny do opuszczenia raju przez Adama i Ewę.

00:13:58.330 --> 00:14:00.477
Savage twierdzi,

00:14:00.477 --> 00:14:03.546
że odzyskanie niezależności i wolności jest możliwe,

00:14:03.546 --> 00:14:06.225
jakkolwiek cena jest wygórowana.

00:14:06.225 --> 00:14:11.940
Wierzę zatem, że jedną z decydujących wojen

00:14:11.940 --> 00:14:14.503
naszych czasów,

00:14:14.503 --> 00:14:16.890
będzie walka o kontrolę nad danymi osobowymi.

00:14:16.890 --> 00:14:20.397
Walka o to, czy wielkie zbiory danych

00:14:20.397 --> 00:14:21.686
staną po stronie wolności,

00:14:21.686 --> 00:14:26.432
a nie posłużą tajnym manipulacjom.

00:14:26.432 --> 00:14:29.025
Obecnie większość z nas

00:14:29.025 --> 00:14:31.778
nawet nie wie, że ta walka trwa,

00:14:31.778 --> 00:14:34.450
ale tak jest czy nam się to podoba, czy nie.

00:14:34.450 --> 00:14:37.254
Ryzykując, że odegram rolę biblijnego węża

00:14:37.254 --> 00:14:40.151
powiem, że narzędzia tej walki

00:14:40.151 --> 00:14:43.160
są dostępne, a świadomość tej sytuacji

00:14:43.160 --> 00:14:44.515
jest w waszych rękach,

00:14:44.515 --> 00:14:48.255
w odległości kilku kliknięć.

00:14:48.255 --> 00:14:49.737
Dziękuję.

00:14:49.737 --> 00:14:54.214
(Oklaski)

