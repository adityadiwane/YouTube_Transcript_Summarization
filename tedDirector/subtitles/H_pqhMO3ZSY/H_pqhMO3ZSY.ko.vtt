WEBVTT
Kind: captions
Language: ko

00:00:00.000 --> 00:00:07.000
번역: Wooran Lee
검토: Gemma Lee

00:00:12.641 --> 00:00:14.995
저는 아담과 이브와 관련된 

00:00:14.995 --> 00:00:18.171
악명 높은

00:00:18.171 --> 00:00:20.940
사생활 사건과 관련된 이야기와

00:00:20.940 --> 00:00:24.386
지난 10년 간 사생활과 

00:00:24.386 --> 00:00:27.072
공개된 생활의 경계가
얼마나 놀랍게 바뀌었는지

00:00:27.072 --> 00:00:28.842
말씀드리겠습니다.

00:00:28.842 --> 00:00:30.140
여러분도 아는 일입니다.

00:00:30.140 --> 00:00:33.470
어느 날 아담과 이브는 에덴 동산에서

00:00:33.470 --> 00:00:35.313
그들이 알몸이라는 것을 
인지합니다.

00:00:35.313 --> 00:00:36.813
매우 놀라죠.

00:00:36.813 --> 00:00:39.570
나머지는 성경에 
나오는 그대로입니다.

00:00:39.570 --> 00:00:41.758
만약 지금이었다면 아담과 이브는

00:00:41.758 --> 00:00:44.119
아마 다르게 행동했을 겁니다.

00:00:44.119 --> 00:00:46.387
[@아담 어제 밤은 폭발적이었어! 
그 사과 최고야. LOL]

00:00:46.387 --> 00:00:48.260
[@이브 응... 자기, 근데 
내 바지 어디간 거야?]

00:00:48.260 --> 00:00:50.896
우리는 스스로에 관해서 
전례가 없을 만큼

00:00:50.896 --> 00:00:54.230
훨씬 많은 정보를 온라인에 공개하고

00:00:54.230 --> 00:00:55.934
우리에 관한 많은 정보를

00:00:55.934 --> 00:00:58.158
여러 기관들이 수집하고 있습니다.

00:00:58.158 --> 00:01:01.440
이제 대량의 개인 정보 분석자료와

00:01:01.440 --> 00:01:03.886
빅데이터를 통해

00:01:03.886 --> 00:01:05.832
얻을 수 있는 이득은 매우 많습니다

00:01:05.832 --> 00:01:08.470
하지만 우리의 개인정보를 넘김으로써

00:01:08.470 --> 00:01:11.568
발생하는 복잡한 댓가가 있습니다.

00:01:11.568 --> 00:01:15.591
그리고 제 이야기는
그 댓가에 관한 것입니다.

00:01:15.591 --> 00:01:18.175
지난 수년간 제 머리속에서

00:01:18.175 --> 00:01:21.502
더 명확해진 관찰로 시작하겠습니다

00:01:21.502 --> 00:01:23.599
어떤 개인 정보도

00:01:23.599 --> 00:01:25.884
민감한 정보가 될 수 있습니다

00:01:25.884 --> 00:01:30.009
2000년에 전세계에서

00:01:30.009 --> 00:01:31.921
천억명의 사람들이 사진을 찍혔고

00:01:31.921 --> 00:01:34.986
하지만 그 중 아주 작은 비율만

00:01:34.986 --> 00:01:36.869
실제로 온라인에 실렸습니다.

00:01:36.869 --> 00:01:40.230
2010년엔 단 한 달간 페이스북에서만

00:01:40.230 --> 00:01:43.500
25억 장의 사진이 올려졌고

00:01:43.500 --> 00:01:45.382
대부분이 신원 확인되었죠.

00:01:45.382 --> 00:01:47.262
같은 기간동안

00:01:47.262 --> 00:01:52.132
사진 속의 사람을 인식할 수 있는

00:01:52.132 --> 00:01:55.740
컴퓨터의 능력은
천배 가량 증가했습니다.

00:01:55.740 --> 00:01:57.622
그 기술들을 모두 합치면

00:01:57.622 --> 00:01:59.123
어떤 일이 일어날까요.

00:01:59.123 --> 00:02:01.781
안면 정보 이용율의 증가,

00:02:01.781 --> 00:02:05.429
컴퓨터의 안면 인식 능력 개선,

00:02:05.429 --> 00:02:07.611
또한 클라우드 컴퓨팅을 
가능하게 합니다

00:02:07.611 --> 00:02:09.499
그래서 이 극장에 계신 어떤 분이라도

00:02:09.499 --> 00:02:11.059
몇년 전에는 세글자를 가진 조직만이

00:02:11.059 --> 00:02:12.945
할 수 있었던

00:02:12.945 --> 00:02:14.727
계산 능력을 가질 수 있습니다.

00:02:14.727 --> 00:02:16.105
그리고 유비쿼터스 컴퓨팅은

00:02:16.105 --> 00:02:18.997
슈퍼컴퓨터가 아닌 제 휴대전화를

00:02:18.997 --> 00:02:20.668
인터넷에 연결해서

00:02:20.668 --> 00:02:23.002
몇 초 만에 수백, 수천장의

00:02:23.002 --> 00:02:25.641
안면인식을 가능하도록 합니다.

00:02:25.641 --> 00:02:28.269
우리는 이 기술 조합의 결과가 

00:02:28.269 --> 00:02:30.333
사생활과 익명성에 관한

00:02:30.333 --> 00:02:33.221
개념에 급속한 변화를

00:02:33.221 --> 00:02:35.478
가져오리라 추측합니다.

00:02:35.478 --> 00:02:37.471
그것을 시험하기 위해

00:02:37.471 --> 00:02:39.592
카네기 멜론 대학교에서 
실험을 진행했습니다.

00:02:39.592 --> 00:02:41.691
우리는 걸어 지나가는 학생들에게

00:02:41.691 --> 00:02:43.470
연구에 참여할 수 있냐고 묻고

00:02:43.470 --> 00:02:46.032
웹캠으로 사진을 찍고

00:02:46.032 --> 00:02:48.814
노트북의 설문조사에
응해달라고 했습니다.

00:02:48.814 --> 00:02:50.793
그들이 설문에 답하는 동안

00:02:50.793 --> 00:02:53.590
우린 그들의 사진을 
클라우드 컴퓨팅군에 올렸고

00:02:53.590 --> 00:02:55.317
안면 인식기를 이용해서 그 사진과

00:02:55.317 --> 00:02:57.722
우리가 페이스북 프로필에서 내려받은

00:02:57.722 --> 00:03:00.115
수많은 사진들이 있는 데이터베이스를

00:03:00.115 --> 00:03:03.711
맞춰보기 시작했습니다.

00:03:03.711 --> 00:03:06.970
참가자가 설문지의 
마지막장에 도달했을 때

00:03:06.970 --> 00:03:10.317
실시간으로 업데이트 되는 그 페이지는

00:03:10.317 --> 00:03:12.630
안면 인식프로그램이 찾은

00:03:12.630 --> 00:03:14.915
가장 비슷한 사진을 10장 표시했고

00:03:14.915 --> 00:03:16.653
우리는 참가자들에게

00:03:16.653 --> 00:03:20.773
자신들의 사진이 있는지
표기하도록 했습니다.

00:03:20.773 --> 00:03:24.472
설문 참가자가 보이시나요?

00:03:24.472 --> 00:03:27.317
컴퓨터는 보았습니다.

00:03:27.317 --> 00:03:29.466
사실 3명 중 1명을 찾아냈습니다.

00:03:29.466 --> 00:03:32.650
기본적으로 우리는 
오프라인 또는 온라인에 있는

00:03:32.650 --> 00:03:36.134
익명의 얼굴로 시작해서
안면 인식 기술을 통해

00:03:36.134 --> 00:03:38.494
그 익명의 얼굴에 
이름을 찾을 수 있습니다.

00:03:38.494 --> 00:03:40.602
소셜 미디어 데이터 덕분이지요.

00:03:40.602 --> 00:03:42.474
하지만 몇년 전
우린 다른 일을 했습니다.

00:03:42.474 --> 00:03:44.297
우리는 소셜 미디어 데이터를

00:03:44.297 --> 00:03:47.348
미국 정부의 사회 보장 정보와

00:03:47.348 --> 00:03:49.450
통계적으로 합쳐

00:03:49.450 --> 00:03:52.774
사회 보장 번호를 
예상하기에 이르렀습니다

00:03:52.774 --> 00:03:54.286
미국내에서는

00:03:54.286 --> 00:03:56.326
매우 민감한 정보입니다.

00:03:56.326 --> 00:03:58.419
제가 무슨 말을 하는지 아시겠습니까?

00:03:58.419 --> 00:04:01.341
만약 여러분이 두 연구를 합친다면

00:04:01.341 --> 00:04:02.853
의문을 가지게 될 것 입니다.

00:04:02.853 --> 00:04:05.573
얼굴로 시작해서

00:04:05.573 --> 00:04:07.884
안면 인식 기술을 이용해서
이름을 알아내고

00:04:07.884 --> 00:04:10.553
그 이름과 그 사람에 대해

00:04:10.553 --> 00:04:12.485
공개된 정보를 얻고

00:04:12.485 --> 00:04:14.733
그렇게 공개된 정보로부터

00:04:14.733 --> 00:04:16.775
훨씬 더 민감한

00:04:16.775 --> 00:04:18.381
비공개 정보를 추론해서

00:04:18.381 --> 00:04:19.873
그 얼굴에 연결할 수 있을까?

00:04:19.873 --> 00:04:21.789
답은 예이고 우리는 해냈습니다.

00:04:21.789 --> 00:04:24.357
물론 정확도는 계속 안 좋아지죠.

00:04:24.357 --> 00:04:25.301
[네 번의 시도로 대상의 27%의 
사회 보장 번호 첫 다섯 자리가 확인]

00:04:25.301 --> 00:04:29.128
하지만 우리는 사실 아이폰 앱을 
개발하기로 결정했습니다.

00:04:29.128 --> 00:04:31.843
이앱은 전화기의 내부 카메라를 사용하여

00:04:31.843 --> 00:04:33.443
대상의 사진을 찍어

00:04:33.443 --> 00:04:34.930
클라우드에 올려서

00:04:34.930 --> 00:04:37.592
제가 방금 설명드린 작업을 
실시간으로 진행합니다.

00:04:37.592 --> 00:04:39.680
일치하는 것을 찾고 
공개된 정보를 찾으며

00:04:39.680 --> 00:04:41.410
민감한 정보를 추론하여

00:04:41.410 --> 00:04:44.001
그리고 전화기의 사진위에

00:04:44.001 --> 00:04:47.610
찾은 정보를 덧씌워 표기됩니다.

00:04:47.610 --> 00:04:49.511
증강 현실의 예처럼요.

00:04:49.511 --> 00:04:51.962
아마 소름끼치는 
증강 현실의 예일 것입니다.

00:04:51.962 --> 00:04:55.301
사실 우리는 그 앱을 
사용하기 위해서가 아니라

00:04:55.301 --> 00:04:57.223
단지 개념을 증명해 
보이기 위해서 만들었죠.

00:04:57.223 --> 00:04:59.536
실제로 이 기술을 이용해서

00:04:59.536 --> 00:05:01.373
극한의 논리를 적용해보세요.

00:05:01.373 --> 00:05:04.092
미래에 여러분 주위에서 낯선 사람들이

00:05:04.092 --> 00:05:06.403
구글 글래스나 콘텍츠 렌즈로

00:05:06.403 --> 00:05:08.710
여러분을 쳐다본다고 상상해 보세요.

00:05:08.710 --> 00:05:12.730
여러분에 대한 
일곱 여덟 가지 데이터를 사용해

00:05:12.730 --> 00:05:15.312
여러분에 대해 알려진

00:05:15.312 --> 00:05:17.915
어떠한 정보든지 추론합니다.

00:05:17.915 --> 00:05:22.709
비밀이 없는 미래는 어떤 모습일까요?

00:05:22.709 --> 00:05:24.673
그리고 우리가 그걸 신경써야 할까요?

00:05:24.673 --> 00:05:26.564
우리는 풍부한 데이터가 
존재하는 그 미래가

00:05:26.564 --> 00:05:29.604
편견이 사라진 미래라고

00:05:29.604 --> 00:05:32.118
믿고 싶어할 수도 있지만

00:05:32.118 --> 00:05:35.701
사실 많은 정보를 갖는다는 것이

00:05:35.701 --> 00:05:37.892
우리가 보다 객관적인 결정을

00:05:37.892 --> 00:05:39.598
내린다는 뜻은 아닙니다.

00:05:39.598 --> 00:05:42.158
또 다른 실험에서 우린 피실험자들에게

00:05:42.158 --> 00:05:44.404
잠재적 입사지원자들에
대한 정보를 제공하였습니다

00:05:44.404 --> 00:05:47.582
우린 그 정보 속에

00:05:47.582 --> 00:05:50.228
피실험자들이 인터넷에 올렸던

00:05:50.228 --> 00:05:52.693
조금 웃기고 아주 합법적이지만

00:05:52.693 --> 00:05:54.713
아마 조금은 창피할 수 있는 내용을 
포함했습니다.

00:05:54.713 --> 00:05:57.079
자 흥미롭게도, 피실험자들 중에서

00:05:57.079 --> 00:06:00.162
어떤 이들은 비슷한 정보를 올렸고

00:06:00.162 --> 00:06:02.524
다른 이들은 그러지 않았습니다.

00:06:02.524 --> 00:06:04.473
여러분 생각엔 어떤 그룹이

00:06:04.473 --> 00:06:09.025
입사지원자들을 더욱 냉정하게
평가했을까요?

00:06:09.025 --> 00:06:10.982
역설적으로 그 그룹은

00:06:10.982 --> 00:06:12.715
비슷한 정보를 올렸던 그룹이었습니다.

00:06:12.715 --> 00:06:15.657
도덕적 부조화의 한 예입니다.

00:06:15.657 --> 00:06:17.407
여러분은 아마 숨길게 없기 때문에

00:06:17.407 --> 00:06:19.109
이건 나와는 상관없는 일이야

00:06:19.109 --> 00:06:21.271
라고 생각하실지 모릅니다.

00:06:21.271 --> 00:06:23.753
하지만 사실, 사생활은 감춰야 할

00:06:23.753 --> 00:06:27.429
부정적인 것에 관한 것이 아닙니다.

00:06:27.429 --> 00:06:29.783
여러분이 어느 단체의 
인사 담당자라고 가정해보세요.

00:06:29.783 --> 00:06:32.730
여러분은 이력서를 받고

00:06:32.730 --> 00:06:35.203
지원자에 대한 더 많은 정보를 
찾기로 결정합니다

00:06:35.203 --> 00:06:37.663
그래서 여러분은 그 이름들을
구글에 검색하고

00:06:37.663 --> 00:06:39.903
어느 한 곳에서

00:06:39.903 --> 00:06:41.911
그 정보를 찾습니다.

00:06:41.911 --> 00:06:46.348
혹은 평행 세계에서
이런 정보를 찾습니다.

00:06:46.348 --> 00:06:49.065
여러분은 두 지원자 모두 동등하게

00:06:49.065 --> 00:06:51.868
면접에 부를 것이라 생각하세요?

00:06:51.868 --> 00:06:54.150
만약 그렇다면

00:06:54.150 --> 00:06:56.732
여러분은 우리 실험에 참가했던

00:06:56.732 --> 00:07:00.039
미국의 고용자와 다른 겁니다.
우린 그렇게 했어요.

00:07:00.039 --> 00:07:03.221
우린 조작된 페이스북 프로필을 만들고

00:07:03.221 --> 00:07:06.072
미국 회사에 이력서를 보냈습니다.

00:07:06.072 --> 00:07:07.980
그리고 그 회사에서

00:07:07.980 --> 00:07:10.373
우리의 지원자를 찾는지
감시하였습니다.

00:07:10.373 --> 00:07:12.205
또한 그들이 소셜 미디어에서 
찾은 정보에 따라

00:07:12.205 --> 00:07:14.143
행동하는지 지켜보았어요.
그들은 그렇게 했습니다.

00:07:14.143 --> 00:07:16.244
소셜 미디어로 인해
동등한 기술을 지닌

00:07:16.244 --> 00:07:19.317
지원자들을 두고
차별이 생겼습니다.

00:07:19.317 --> 00:07:23.892
마케팅 담당자들은

00:07:23.892 --> 00:07:26.161
우리에 대한 정보는 항상 우리에게

00:07:26.161 --> 00:07:29.434
좋은 쪽으로 사용될거라고
믿게 합니다.

00:07:29.434 --> 00:07:33.149
하지만 다시 생각해보세요.
왜 항상 그래야 할까요?

00:07:33.149 --> 00:07:35.813
몇년 전에 개봉한 영화,

00:07:35.813 --> 00:07:38.366
"마이너리티 리포트"에는

00:07:38.366 --> 00:07:40.942
톰 크루즈가 백화점을 걸을 때
홀로그래픽 개인 맞춤 광고가

00:07:40.942 --> 00:07:44.718
그의 주위에 나타나는

00:07:44.718 --> 00:07:46.553
유명한 장면이 있습니다.

00:07:46.553 --> 00:07:49.780
그 영화의 배경은 2054년입니다.

00:07:49.780 --> 00:07:51.422
지금으로부터 40년 후죠.

00:07:51.422 --> 00:07:54.330
그 기술이 아주 멋지게 보이듯

00:07:54.330 --> 00:07:56.976
단체들이 모을 수 있는
여러분에 대한 정보의 양과

00:07:56.976 --> 00:07:59.116
또 그 정보를 이용해 
여러분이 알아챌 수 없도록

00:07:59.116 --> 00:08:01.599
여러분에게 영향을 
줄 수 있다는 사실은

00:08:01.599 --> 00:08:04.997
매우 과소평가돼있습니다.

00:08:04.997 --> 00:08:07.100
예로, 이 실험은
현재 우리가 진행 중인

00:08:07.100 --> 00:08:09.373
또다른 실험이고
아직 끝나지 않았습니다.

00:08:09.373 --> 00:08:11.692
어느 단체가 당신의
페이스북 친구 목록에

00:08:11.692 --> 00:08:13.748
접근할 수 있다고 상상해보세요.

00:08:13.748 --> 00:08:15.520
그리고 어떠한 알고리즘을 통해

00:08:15.520 --> 00:08:19.254
여러분이 가장 좋아하는
친구 2명을 알아낼 수 있습니다.

00:08:19.254 --> 00:08:21.534
그리고 그들은 실시간으로

00:08:21.534 --> 00:08:24.376
그 두 친구의 안면 합성을
만들어냅니다.

00:08:24.376 --> 00:08:27.445
이 전의 연구들에서 사람들은

00:08:27.445 --> 00:08:30.330
합성된 얼굴에서 자신조차도 
인식할 수 없지만

00:08:32.792 --> 00:08:34.909
긍정적인 반응을 보인다고 했습니다.

00:08:34.909 --> 00:08:38.324
그러니 다음에 만약 여러분이
어떤 제품을 찾을 때

00:08:38.324 --> 00:08:40.883
만약 특정 제품을 권하는 광고가 있으면

00:08:40.883 --> 00:08:43.790
일반적인 대변인이 아닐겁니다.

00:08:43.790 --> 00:08:46.103
아마 여러분의 친구 중 한명일 거에요.

00:08:46.103 --> 00:08:49.406
그리고 여러분은 이런 일이
일어나는지도 모르겠죠.

00:08:49.406 --> 00:08:51.819
이제 문제는

00:08:51.819 --> 00:08:54.338
현재 우리가 가진
개인 정보의 남용을

00:08:54.338 --> 00:08:57.776
막기 위한 정책 구조는

00:08:57.776 --> 00:09:00.760
마치 총싸움에 칼을 
가지고 가는 것과 같습니다.

00:09:00.760 --> 00:09:03.673
그 구조 중 하나는 투명성입니다.

00:09:03.673 --> 00:09:06.873
사람들의 정보를 가지고
무엇을 할지 말하는 것이죠.

00:09:06.873 --> 00:09:08.979
원칙대로라면 아주 좋은 거에요.

00:09:08.979 --> 00:09:12.646
필요하지만 충분하지 않습니다.

00:09:12.646 --> 00:09:16.344
투명성은 잘못 사용될 수 있습니다.

00:09:16.344 --> 00:09:18.448
사람들에게 무엇을 할지 말을 하고

00:09:18.448 --> 00:09:20.680
계속 그들에게 확실하지 않은 양의

00:09:20.680 --> 00:09:23.303
정보를 밝히라고 부추길 수 있어요.

00:09:23.303 --> 00:09:26.189
그래서 또 다른 학생들과 
진행한 실험에서는

00:09:26.189 --> 00:09:29.247
교정 안에서의 행동에 대한

00:09:29.247 --> 00:09:31.060
정보를 요구했습니다.

00:09:31.060 --> 00:09:34.000
꽤나 민감한 이런 질문도 있었어요.

00:09:34.000 --> 00:09:34.621
[시험 도중 컨닝한 적이 있습니까?]

00:09:34.621 --> 00:09:36.921
우린 한 피실험자 그룹에게
이렇게 얘기했습니다.

00:09:36.921 --> 00:09:39.762
"다른 학생들만 답을 볼거에요."

00:09:39.762 --> 00:09:41.341
또 다른 그룹에게는 이렇게 얘기했죠.

00:09:41.341 --> 00:09:44.902
"학생들과 교수진들 모두
응답을 볼겁니다."

00:09:44.902 --> 00:09:47.493
투명성. 알림. 이건 
당연하게도 통했습니다.

00:09:47.493 --> 00:09:48.900
첫번재 그룹이
두번째 그룹보다

00:09:48.900 --> 00:09:51.468
더 많은 정보를 밝혔어요.

00:09:51.468 --> 00:09:52.988
납득이 되지요. 그렇죠?

00:09:52.988 --> 00:09:54.478
그리고 잘못된 지시를 더했습니다.

00:09:54.478 --> 00:09:57.238
같은 그룹에게 실험을 반복했지만

00:09:57.238 --> 00:09:59.665
이번엔 피실험자들에게

00:09:59.665 --> 00:10:02.600
우리가 정보를 어떻게 
사용할지 말한 때와

00:10:02.600 --> 00:10:04.680
질문에 답하기 시작한 때 사이에

00:10:04.680 --> 00:10:09.068
시간 간격을 두었어요.

00:10:09.068 --> 00:10:11.629
교수진이 응답을 볼거란 사실에 의한

00:10:11.629 --> 00:10:16.242
억제 영향을 무효화시키기 위해

00:10:16.242 --> 00:10:19.653
얼마나 긴 간격을 두어야 했을까요?

00:10:19.653 --> 00:10:21.433
10분?

00:10:21.433 --> 00:10:23.224
5분?

00:10:23.224 --> 00:10:25.000
1분?

00:10:25.000 --> 00:10:27.049
15초는 어떻습니까?

00:10:27.049 --> 00:10:29.717
두 그룹이 같은 양의
정보를 밝히기 위해선

00:10:29.717 --> 00:10:31.285
15초면 충분했습니다.

00:10:31.285 --> 00:10:34.031
두번째 그룹이 교수진이 
응답을 볼거란 사실을

00:10:34.031 --> 00:10:36.687
신경 쓰지 않은 것처럼요.

00:10:36.687 --> 00:10:40.023
저는 지금껏 이 강연이

00:10:40.023 --> 00:10:42.503
매우 우울하게 들렸다는 걸 인정하지만

00:10:42.503 --> 00:10:44.224
그게 중점은 아닙니다.

00:10:44.224 --> 00:10:46.923
사실 전 대안이 있다는 것을

00:10:46.923 --> 00:10:48.695
알려드리고 싶습니다.

00:10:48.695 --> 00:10:51.194
지금 저희가 하는 방식이

00:10:51.194 --> 00:10:54.231
유일한 방법이 아니고
또한 최상의 방법은

00:10:54.231 --> 00:10:56.258
더더욱 아닙니다.

00:10:56.258 --> 00:11:00.429
누군가 여러분에게
"사람들은 사생활에 신경쓰지 않아."

00:11:00.429 --> 00:11:03.071
라고 말할 때 
모든게 설계되고 조작되어

00:11:03.071 --> 00:11:05.795
사생활에 신경 쓸 수 
없게 됬는지 고려해보세요.

00:11:05.795 --> 00:11:09.057
그리고 그러한 조작들이
일어나고 있음을 깨닫는 순간이

00:11:09.057 --> 00:11:10.664
이미 여러분 자신을 
보호하는 과정의

00:11:10.664 --> 00:11:12.922
절반에 왔습니다.

00:11:12.922 --> 00:11:16.632
누군가 여러분에게 사생활과

00:11:16.632 --> 00:11:18.481
빅 데이터의 장점은
공존할 수 없다 말할 때

00:11:18.481 --> 00:11:20.954
지난 20년 간 연구진들이

00:11:20.954 --> 00:11:22.871
기술을 개발해서

00:11:22.871 --> 00:11:26.189
거의 모든 전자 거래가

00:11:26.189 --> 00:11:29.938
사생활을 더 보호하는 방식으로 
이뤄지고 있습니다.

00:11:29.938 --> 00:11:32.493
우린 익명으로 인터넷을 
볼 수 있습니다.

00:11:32.493 --> 00:11:35.171
우린 오직 지정된 
수신자만 읽을 수 있는

00:11:35.171 --> 00:11:38.880
이메일을 보낼 수 있습니다.
NSA도 읽을 수 없어요.

00:11:38.880 --> 00:11:41.877
우린 사생활을 보호하며
데이터 마이닝을 할 수 있습니다.

00:11:41.877 --> 00:11:45.771
다른 말로 우리는 사생활을 보호하며

00:11:45.771 --> 00:11:47.903
빅 데이터의 이점을 가질 수 있습니다.

00:11:47.903 --> 00:11:51.694
물론 그 기술들은

00:11:51.694 --> 00:11:53.240
데이터 보유자와 대상자 사이의

00:11:53.240 --> 00:11:55.347
비용과 수익의 변화를 의미합니다.

00:11:55.347 --> 00:11:58.800
어쩌면 여러분이 그것에 대해
더이상 듣지 못하는 이유일겁니다.

00:11:58.800 --> 00:12:02.506
다시 에덴의 동산으로 돌아가보죠.

00:12:02.506 --> 00:12:05.286
에덴의 동산 이야기엔

00:12:05.286 --> 00:12:07.095
두번째 사생활에 관련된
해석이 있습니다.

00:12:07.095 --> 00:12:09.191
이건 아담과 이브가

00:12:09.191 --> 00:12:11.416
알몸인 채

00:12:11.416 --> 00:12:13.797
창피해 하는 것과는 상관 없어요.

00:12:13.797 --> 00:12:16.578
이 해석의 메아리는

00:12:16.578 --> 00:12:19.360
존 밀튼의 "실낙원"에서
찾으실 수 있습니다.

00:12:19.360 --> 00:12:23.557
동산에서 아담과 이브는
물질적으로 만족합니다.

00:12:23.557 --> 00:12:25.661
그들은 행복합니다.
만족하고 있어요.

00:12:25.661 --> 00:12:27.954
하지만 그들은

00:12:27.954 --> 00:12:29.594
지식과 자기 인식이 없습니다.

00:12:29.594 --> 00:12:32.913
그들이 적절하게 이름 지어진

00:12:32.913 --> 00:12:34.206
선악과를 먹는 순간

00:12:34.206 --> 00:12:36.811
자기 자신을 발견합니다.

00:12:36.811 --> 00:12:40.842
그들은 의식하게 되요.
자주성을 얻습니다.

00:12:40.842 --> 00:12:43.968
하지만 그에 대한 대가는
동산을 떠나는 것입니다.

00:12:43.968 --> 00:12:47.849
사생활이란 한편으로

00:12:47.849 --> 00:12:50.811
자유를 위해 지불하는 대가입니다.

00:12:50.811 --> 00:12:53.581
다시 한번, 마케팅 담당자들은

00:12:53.581 --> 00:12:56.600
빅 데이터와 소셜 미디어가

00:12:56.600 --> 00:12:59.579
그들을 위한 이익의 낙원일 뿐만 아니라

00:12:59.579 --> 00:13:02.036
우리를 위한 에덴 동산이라고 말합니다.

00:13:02.036 --> 00:13:03.274
우린 무료 콘텐츠를 얻죠.

00:13:03.274 --> 00:13:06.397
앵그리 버드를 하게 되고요.
특정한 앱을 얻습니다.

00:13:06.397 --> 00:13:09.294
하지만 사실 몇년 후 단체들은

00:13:09.294 --> 00:13:10.903
우리에 대해 아주 많이 
알게 될 겁니다.

00:13:10.903 --> 00:13:13.613
그들은 우리가 욕구를 가지기도 전에

00:13:13.613 --> 00:13:15.817
그 욕구를 추론할 수 있고

00:13:15.817 --> 00:13:18.264
우리가 필요를 느끼기도 전에

00:13:18.264 --> 00:13:20.538
제품을 사게 할 수도 있을 겁니다.

00:13:20.538 --> 00:13:23.775
한 영국 작가가

00:13:23.775 --> 00:13:26.820
이런 미래를 예상했어요.

00:13:26.820 --> 00:13:28.225
편의를 위해 

00:13:28.225 --> 00:13:31.773
우리의 자주성과 자유를 판다고 말이죠.

00:13:31.773 --> 00:13:33.934
조지 오웰보다 더한

00:13:33.934 --> 00:13:36.695
자가는 물론 올더스 헉슬리입니다.

00:13:36.695 --> 00:13:39.549
"멋진 신세계"에서 그는

00:13:39.549 --> 00:13:41.720
우리가 창조한 
자유를 위한 기술들이

00:13:41.720 --> 00:13:43.579
우리를 강압하는

00:13:43.579 --> 00:13:46.146
사회를 그립니다.

00:13:46.146 --> 00:13:50.937
하지만 책에서 그는

00:13:50.937 --> 00:13:54.375
그런 사회에서 벗어나는
방법도 권합니다.

00:13:54.375 --> 00:13:58.330
아담과 이브가 동산을
떠나야 했던 것과 비슷하게요.

00:13:58.330 --> 00:14:00.477
새비지의 말로

00:14:00.477 --> 00:14:03.546
자주성과 자유를 
되찾는 것은 가능합니다.

00:14:03.546 --> 00:14:06.225
그에 대한 대가는 비싸지만요.

00:14:06.225 --> 00:14:11.940
그래서 저는

00:14:11.940 --> 00:14:14.503
우리 시대의 중요한 싸움은

00:14:14.503 --> 00:14:16.890
개인정보 통제를 위한 싸움,

00:14:16.890 --> 00:14:20.397
빅 데이터가 자유를 위한

00:14:20.397 --> 00:14:21.686
힘이 될지의 싸움이라 믿습니다.

00:14:21.686 --> 00:14:26.432
우리를 몰래 조종하는 힘 대신에요.

00:14:26.432 --> 00:14:29.025
지금 많은 사람들은

00:14:29.025 --> 00:14:31.778
그 싸움이 진행 중인지도 모를거에요.

00:14:31.778 --> 00:14:34.450
하지만 여러분이
좋든 싫든 진행 중입니다.

00:14:34.450 --> 00:14:37.254
그리고 뱀을 갖고 노는 위험을 무릅쓰고

00:14:37.254 --> 00:14:40.151
싸움을 위한 도구는

00:14:40.151 --> 00:14:43.160
여기 있습니다.
현 상황의 자각과

00:14:43.160 --> 00:14:44.515
여러분의 손에 있는

00:14:44.515 --> 00:14:48.255
몇 번의 클릭입니다.

00:14:48.255 --> 00:14:49.737
감사합니다.

00:14:49.737 --> 00:14:54.214
(박수)

