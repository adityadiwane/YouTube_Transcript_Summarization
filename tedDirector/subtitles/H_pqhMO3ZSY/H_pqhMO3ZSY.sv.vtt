WEBVTT
Kind: captions
Language: sv

00:00:00.000 --> 00:00:07.000
Översättare: Jenny Lillie
Granskare: Stephanie Green

00:00:12.641 --> 00:00:14.995
Jag vill berätta en historia för er,

00:00:14.995 --> 00:00:18.171
genom att länka samman
det berömda integritetsmissödet

00:00:18.171 --> 00:00:20.940
som involverade Adam och Eva,

00:00:20.940 --> 00:00:24.386
med ett anmärkningsvärt skifte
som skett i gränsen

00:00:24.386 --> 00:00:27.072
mellan det offentliga 
och det privata livet

00:00:27.072 --> 00:00:28.842
under de senaste tio åren.

00:00:28.842 --> 00:00:30.140
Ni känner till missödet.

00:00:30.140 --> 00:00:33.470
Adam och Eva är i Edens lustgård

00:00:33.470 --> 00:00:35.313
och inser att de är nakna.

00:00:35.313 --> 00:00:36.813
De blir utom sig.

00:00:36.813 --> 00:00:39.027
Och resten är historia.

00:00:39.570 --> 00:00:41.758
I dagens värld skulle Adam och Eva

00:00:41.758 --> 00:00:44.119
troligen agera på ett annat sätt.

00:00:44.119 --> 00:00:46.387
[@Adam Igår kväll var helskoj! 
Älskade äpplet LOL]

00:00:46.387 --> 00:00:48.260
[@Eva japp..älskling, 
vad hände med mina byxor??]

00:00:48.260 --> 00:00:50.896
Vi avslöjar mer information

00:00:50.896 --> 00:00:54.230
om oss själva på nätet än någonsin förut

00:00:54.230 --> 00:00:55.934
och så mycket information om oss

00:00:55.934 --> 00:00:58.158
samlas in av olika organisationer.

00:00:58.158 --> 00:01:01.250
Det finns mycket att vinna

00:01:01.260 --> 00:01:03.886
från denna massiva analys 
av personlig information,

00:01:03.886 --> 00:01:05.832
eller stora datamängd,

00:01:05.832 --> 00:01:08.470
men det för också med sig
komplexa kostnader

00:01:08.470 --> 00:01:12.148
när man ger bort sitt privatliv. (Skratt)

00:01:12.158 --> 00:01:15.163
Min historia handlar om dessa kostnader.

00:01:15.591 --> 00:01:18.175
Vi börjar med en observation 
som enligt mig

00:01:18.175 --> 00:01:21.502
har klarnat alltmer under de senaste åren;

00:01:21.502 --> 00:01:23.599
all personlig information

00:01:23.599 --> 00:01:25.884
kan bli känslig information.

00:01:25.884 --> 00:01:30.719
År 2000 togs det omkring 
100 miljarder foton

00:01:30.729 --> 00:01:31.921
i hela världen

00:01:31.921 --> 00:01:34.986
men enbart en minimal del av dem

00:01:34.986 --> 00:01:36.869
lades ut på nätet.

00:01:36.869 --> 00:01:40.230
År 2010 lades det bara på Facebook ut

00:01:40.230 --> 00:01:43.500
2,5 miljarder foton under en månad,

00:01:43.500 --> 00:01:45.382
varav de flesta var identifierade.

00:01:45.382 --> 00:01:47.262
Inom samma tidsram

00:01:47.262 --> 00:01:52.132
blev datorernas förmåga
att känna igen människor på foton

00:01:52.132 --> 00:01:55.740
ungefär 1000 gånger bättre.

00:01:55.740 --> 00:01:57.622
Vad händer när du kombinerar

00:01:57.622 --> 00:01:59.123
dessa två teknologier:

00:01:59.123 --> 00:02:01.781
en ökad tillgång av ansiktsdata;

00:02:01.781 --> 00:02:05.429
en förbättrad förmåga hos datorer
att känna igen ansikten;

00:02:05.429 --> 00:02:07.611
men också hos molntjänster

00:02:07.611 --> 00:02:09.499
som ger vem som helst inom denna arena

00:02:09.499 --> 00:02:11.059
den typen av datormakt

00:02:11.059 --> 00:02:12.945
som för några år sedan enbart förbehölls

00:02:12.945 --> 00:02:14.727
trebokstaviga regeringsorgan;

00:02:14.727 --> 00:02:16.105
och heltäckande datornät

00:02:16.105 --> 00:02:18.997
som tillåter min mobil, som 
inte är någon superdator,

00:02:18.997 --> 00:02:20.668
att kopplas till Internet

00:02:20.668 --> 00:02:23.002
och där göra hundratusentals

00:02:23.002 --> 00:02:25.641
ansiktskontroller på några sekunder?

00:02:25.641 --> 00:02:28.269
Vi kan förmoda att resultatet

00:02:28.269 --> 00:02:30.333
av att kombinera dessa teknologier

00:02:30.333 --> 00:02:33.221
kommer att vara en radikalt ändrad syn på

00:02:33.221 --> 00:02:35.478
privatliv och anonymitet.

00:02:35.478 --> 00:02:37.651
För att testa detta 
gjorde vi ett experiment

00:02:37.651 --> 00:02:39.592
på Carnegie Mellons universitetsområde.

00:02:39.592 --> 00:02:41.691
Vi tillfrågade studenter som gick förbi

00:02:41.691 --> 00:02:43.470
om att delta i en studie.

00:02:43.470 --> 00:02:46.032
Vi tog ett foto av dem med en webbkamera

00:02:46.032 --> 00:02:48.814
och bad dem fylla i en enkät på en laptop.

00:02:48.814 --> 00:02:50.793
Medan de fyllde i enkäten

00:02:50.793 --> 00:02:53.590
laddade vi upp fotona av 
dem i ett datormoln,

00:02:53.590 --> 00:02:55.457
och vi använde
ett igenkänningsprogram

00:02:55.457 --> 00:02:57.722
för att matcha bilderna med en databas

00:02:57.722 --> 00:03:00.115
bestående av hundratusentals bilder

00:03:00.115 --> 00:03:03.711
som vi hade laddat ner 
från Facebook-profiler.

00:03:03.711 --> 00:03:06.970
När den tillfrågade personen 
kommit till sista sidan

00:03:06.970 --> 00:03:10.317
av enkäten, så hade sidan 
dynamiskt uppdaterats

00:03:10.317 --> 00:03:12.630
med de tio bäst matchande bilderna

00:03:12.630 --> 00:03:14.915
som programmet hade hittat

00:03:14.915 --> 00:03:16.863
och vi bad personen att ange om

00:03:16.863 --> 00:03:20.186
han eller hon var med på bilderna.

00:03:20.773 --> 00:03:23.674
Ser du personen?

00:03:24.472 --> 00:03:27.317
Datorn gjorde det - och det samma gällde

00:03:27.317 --> 00:03:29.466
en av tre personer i studien.

00:03:29.466 --> 00:03:32.650
Så i princip kan vi börja med 
ett helt anonymt ansikte,

00:03:32.650 --> 00:03:36.134
off-line eller på nätet, och 
sedan kan ansiktsigenkänning

00:03:36.134 --> 00:03:38.494
ge ett namn till det anonyma ansiktet

00:03:38.494 --> 00:03:40.512
med hjälp av sociala medier.

00:03:40.512 --> 00:03:42.754
Men för några år sedan 
gjorde vi något annat.

00:03:42.754 --> 00:03:44.297
Vi utgick från sociala medier

00:03:44.297 --> 00:03:47.348
och kombinerade datan 
statistiskt med information

00:03:47.348 --> 00:03:49.560
från amerikanska 
socialförsäkringsmyndigheten

00:03:49.580 --> 00:03:54.274
och det slutade med att vi kunde förutse
socialförsäkringsnummer, som i USA är

00:03:54.286 --> 00:03:56.326
extremt känslig information.

00:03:56.326 --> 00:03:58.419
Ser ni vart jag är på väg med detta?

00:03:58.419 --> 00:04:01.341
Om man kombinerar dessa två studier

00:04:01.341 --> 00:04:02.853
så uppkommer frågan;

00:04:02.853 --> 00:04:05.573
kan du utgå från ett 
ansikte och genom att

00:04:05.573 --> 00:04:07.884
använda ansiktsigenkänning, hitta namn

00:04:07.884 --> 00:04:10.553
och offentligt tillgänglig information

00:04:10.553 --> 00:04:12.485
om det namnet och den personen

00:04:12.485 --> 00:04:14.733
och utifrån den offentliga informationen

00:04:14.733 --> 00:04:16.775
få fram icke offentlig information,

00:04:16.775 --> 00:04:18.381
mycket känsligare sådan

00:04:18.381 --> 00:04:19.873
som sammanlänkas till ansiktet?

00:04:19.873 --> 00:04:22.089
Och svaret är ja, vi kan, 
och vi gjorde det.

00:04:22.099 --> 00:04:24.177
Självklart blir precisionen bara värre.

00:04:24.187 --> 00:04:26.661
[27 % av personernas
5 första siffror identifierade]

00:04:26.661 --> 00:04:29.128
Vi beslöt oss att utveckla en iPhone-app

00:04:29.128 --> 00:04:31.843
som använder mobilens inbyggda kamera

00:04:31.843 --> 00:04:33.363
för att ta en bild av en person

00:04:33.373 --> 00:04:34.990
och ladda upp den i ett datormoln

00:04:34.990 --> 00:04:37.592
och göra exakt det jag 
beskrivit för er i realtid:

00:04:37.592 --> 00:04:39.680
matcha bilden, söka
offentlig information,

00:04:39.680 --> 00:04:41.410
försöka få fram känslig information,

00:04:41.410 --> 00:04:44.001
och sedan skicka den tillbaks till mobilen

00:04:44.001 --> 00:04:47.610
så att den ligger ovanpå 
bilden av personen -

00:04:47.610 --> 00:04:49.511
ett exempel på en utvidgad verklighet,

00:04:49.511 --> 00:04:52.092
förmodligen ett skräckexempel 
på utvidgad verklighet.

00:04:52.102 --> 00:04:55.301
Vi utvecklade inte appen för att 
göra den tillgänglig,

00:04:55.301 --> 00:04:57.223
utan enbart för att bevisa något.

00:04:57.223 --> 00:04:59.536
Faktum är att om man tar dessa teknologier

00:04:59.536 --> 00:05:01.373
och drar dem till sin spets,

00:05:01.373 --> 00:05:04.092
tänk dig en framtid där 
främlingar omkring dig

00:05:04.092 --> 00:05:06.403
tittar på dig genom sina Google-glasögon

00:05:06.403 --> 00:05:08.710
eller kanske en dag genom 
sina kontaktlinser,

00:05:08.710 --> 00:05:12.730
som tar fram sju eller åtta 
tillgängliga uppgifter om dig

00:05:12.730 --> 00:05:15.312
som sedan antyder allt möjligt annat

00:05:15.312 --> 00:05:17.915
som kan tas reda på om dig.

00:05:18.305 --> 00:05:22.275
Hur kommer denna framtid 
utan hemligheter att se ut?

00:05:22.709 --> 00:05:24.673
Borde vi bry oss?

00:05:24.673 --> 00:05:26.564
Vi vill gärna tro

00:05:26.564 --> 00:05:29.604
att en framtid som är så 
rik på information

00:05:29.604 --> 00:05:32.118
skulle vara en framtid utan fördomar,

00:05:32.118 --> 00:05:35.701
men att ha tillgång till så 
mycket information

00:05:35.701 --> 00:05:37.892
innebär inte att vi kommer ta beslut

00:05:37.892 --> 00:05:39.598
som är mer objektiva.

00:05:39.598 --> 00:05:41.638
I ett annat experiment presenterade vi

00:05:41.658 --> 00:05:44.404
information om en arbetssökande
för testpersonerna.

00:05:44.404 --> 00:05:47.582
Vi inkluderade hänvisningar till

00:05:47.582 --> 00:05:50.228
skojig, absolut laglig

00:05:50.228 --> 00:05:52.693
men kanske rätt pinsam information

00:05:52.693 --> 00:05:54.573
som kandidaten lagt upp på nätet.

00:05:54.573 --> 00:05:57.159
Det intressanta var att en del
av våra försökspersoner

00:05:57.159 --> 00:06:00.162
hade lagt upp liknande information,

00:06:00.162 --> 00:06:02.524
och andra hade inte gjort det.

00:06:02.524 --> 00:06:05.603
Vilken grupp tror du var mest benägen

00:06:05.613 --> 00:06:09.025
att döma vår kandidat hårdare?

00:06:09.025 --> 00:06:10.982
Paradoxalt nog var det gruppen

00:06:10.982 --> 00:06:12.865
som hade lagt upp liknande information,

00:06:12.865 --> 00:06:15.657
ett exempel på moralisk dissonans.

00:06:15.657 --> 00:06:17.407
Nu kanske du tänker

00:06:17.407 --> 00:06:19.109
att detta inte gäller dig

00:06:19.109 --> 00:06:21.271
eftersom du inte har något att gömma.

00:06:21.271 --> 00:06:23.753
Men faktum är att ditt privatliv

00:06:23.753 --> 00:06:27.429
inte handlar om att gömma något negativt.

00:06:27.429 --> 00:06:29.783
Tänk dig att du är rekryteringschef

00:06:29.783 --> 00:06:32.440
på en organisation, du får CV:n,

00:06:32.440 --> 00:06:35.313
och du bestämmer dig för
att ta reda på mer om kandidaterna.

00:06:35.323 --> 00:06:37.663
Du googlar därför deras namn

00:06:37.663 --> 00:06:39.903
och i ett visst universum

00:06:39.903 --> 00:06:41.911
hittar du denna information.

00:06:41.911 --> 00:06:46.022
I ett annat universum, 
hittar du denna information.

00:06:46.348 --> 00:06:49.065
Tror du att du skulle vara lika benägen

00:06:49.065 --> 00:06:51.868
att kalla båda kandidaterna 
till en intervju?

00:06:51.868 --> 00:06:54.150
Om du tror det, så är du inte

00:06:54.150 --> 00:06:56.732
som de amerikanska arbetsgivarna som var

00:06:56.732 --> 00:07:00.039
delaktiga i vårt experiment. 
Vi gjorde exakt detta.

00:07:00.039 --> 00:07:03.221
Vi skapade Facebookprofiler, 
manipulerade grunddragen,

00:07:03.221 --> 00:07:06.072
och började skicka ut 
CV:n till olika företag i USA,

00:07:06.072 --> 00:07:07.980
och sedan bevakade vi

00:07:07.980 --> 00:07:10.203
om de sökte efter våra kandidater

00:07:10.203 --> 00:07:12.085
och om de agerade utifrån informationen

00:07:12.085 --> 00:07:14.493
som de fann på sociala medier.
Och det gjorde de.

00:07:14.493 --> 00:07:16.574
Diskriminering skedde 
genom sociala medier

00:07:16.584 --> 00:07:19.317
bland kandidater med likvärdiga meriter.

00:07:19.317 --> 00:07:23.892
Marknadsförare vill få oss att tro

00:07:23.892 --> 00:07:26.911
att all information om oss 
alltid kommer att användas

00:07:26.911 --> 00:07:29.434
på ett sätt som gynnar oss.

00:07:29.434 --> 00:07:33.149
Men tänk om. Varför 
skulle det alltid vara så?

00:07:33.149 --> 00:07:35.813
I en film som kom ut för några år sedan -

00:07:35.813 --> 00:07:38.366
"Minority Report" - i en berömd scen

00:07:38.366 --> 00:07:40.942
gick Tom Cruise omkring i 
ett shoppingcenter

00:07:40.942 --> 00:07:44.718
och holografisk, personlig reklam

00:07:44.718 --> 00:07:46.553
uppkom omkring honom.

00:07:46.553 --> 00:07:49.780
Den filmen utspelar sig år 2054,

00:07:49.780 --> 00:07:51.422
omkring 40 år framåt,

00:07:51.422 --> 00:07:54.330
och hur spännande den 
teknologin än ser ut,

00:07:54.330 --> 00:07:56.976
så underskattar den avsevärt

00:07:56.976 --> 00:07:59.116
den mängd information som organisationer

00:07:59.116 --> 00:08:01.599
kan samla in om dig och 
hur de kan använda den

00:08:01.599 --> 00:08:04.997
för att influera dig på 
sätt du inte märker.

00:08:04.997 --> 00:08:07.100
Här är ett exempel på ett annat experiment

00:08:07.100 --> 00:08:09.373
som vi håller på med just nu.

00:08:09.373 --> 00:08:11.692
Tänk dig att en organisation 
har tillgång till

00:08:11.692 --> 00:08:13.748
din lista av Facebookvänner

00:08:13.748 --> 00:08:15.520
och genom någon algoritm

00:08:15.520 --> 00:08:19.254
kan de upptäcka de två 
vänner du gillar mest.

00:08:19.254 --> 00:08:21.534
Sedan kan de, i realtid, skapa

00:08:21.534 --> 00:08:24.376
ett sammansatt ansikte av 
dessa två vänner.

00:08:24.376 --> 00:08:27.445
Studier gjorda innan dem vi gjort, 
har visat att folk

00:08:27.445 --> 00:08:30.330
inte ens känner igen sig själva i

00:08:30.330 --> 00:08:32.792
ett sammansatt ansikte 
men de reagerar

00:08:32.792 --> 00:08:34.909
på bilden på ett positivt sätt.

00:08:34.909 --> 00:08:38.324
Så nästa gång du söker 
efter en specifik produkt

00:08:38.324 --> 00:08:40.883
och du ser en reklamannons för produkten,

00:08:40.883 --> 00:08:43.790
så kommer det inte vara vilket 
ansikte som helst i reklamen.

00:08:43.790 --> 00:08:46.103
Det kommer vara en av dina vänner

00:08:46.103 --> 00:08:49.406
och du kommer inte veta att det är så.

00:08:49.406 --> 00:08:51.819
Problemet just nu är

00:08:51.819 --> 00:08:54.338
att de aktuella mekanismer vi har

00:08:54.338 --> 00:08:57.776
för att skydda oss från att personlig 
information om oss missbrukas,

00:08:57.776 --> 00:09:00.760
är som att ta med sig en kniv 
till en skottlossning.

00:09:00.760 --> 00:09:03.673
En av dessa mekanismer är öppenhet;

00:09:03.673 --> 00:09:06.873
låta folk veta hur informationen 
om dem kommer att användas.

00:09:06.873 --> 00:09:08.979
Och i princip är det en bra sak.

00:09:08.979 --> 00:09:12.646
Det är nödvändigt, men inte tillräckligt.

00:09:12.646 --> 00:09:16.344
Öppenhet kan vara vilseledande.

00:09:16.344 --> 00:09:18.448
Man kan säga till folk 
vad man tänker göra

00:09:18.448 --> 00:09:20.680
och sedan ändå påverka dem att avslöja

00:09:20.680 --> 00:09:23.303
godtyckliga mängder av 
personlig information.

00:09:23.303 --> 00:09:26.189
Så i ett annat experiment - 
denna gång bland studenter -

00:09:26.189 --> 00:09:29.247
bad vi dem att förse oss med information

00:09:29.247 --> 00:09:31.060
om deras vanor på universitetet,

00:09:31.060 --> 00:09:33.577
bland annat rätt känsliga 
frågor som de här:

00:09:33.597 --> 00:09:34.934
[Har du fuskat på ett prov?]

00:09:34.934 --> 00:09:36.921
Till en grupp av studenter sade vi att

00:09:36.921 --> 00:09:39.762
"Enbart andra studenter 
kommer se dina svar".

00:09:39.762 --> 00:09:41.341
Till en annan grupp sade vi att

00:09:41.341 --> 00:09:44.596
"Studenter och lärare 
kommer se dina svar".

00:09:44.902 --> 00:09:47.493
Öppenhet. Och helt klart så fungerade det

00:09:47.493 --> 00:09:48.900
genom att den första gruppen

00:09:48.900 --> 00:09:51.468
var mycket mer benägen 
att avslöja sig än den andra.

00:09:51.468 --> 00:09:52.918
Verkar vettigt, eller hur?

00:09:52.918 --> 00:09:54.678
Men sedan lade vi till vilseledning.

00:09:54.678 --> 00:09:57.238
Vi upprepade experimentet 
med samma två grupper,

00:09:57.238 --> 00:09:59.665
denna gång med en fördröjning

00:09:59.665 --> 00:10:02.600
mellan tidpunkten då vi berättade

00:10:02.600 --> 00:10:04.680
hur vi skulle använda informationen

00:10:04.680 --> 00:10:09.068
och tidpunkten då de faktiskt 
började besvara frågorna.

00:10:09.068 --> 00:10:11.629
Hur lång fördröjning tror ni 
vi behövde lägga till

00:10:11.629 --> 00:10:16.242
för att omintetgöra den hämmande effekten

00:10:16.242 --> 00:10:19.653
av att veta att lärare
skulle se dina svar?

00:10:19.653 --> 00:10:21.433
Tio minuter?

00:10:21.433 --> 00:10:23.224
Fem minuter?

00:10:23.224 --> 00:10:25.000
En minut?

00:10:25.000 --> 00:10:26.869
Vad sägs om 15 sekunder?

00:10:26.869 --> 00:10:29.617
Femton sekunder var tillräckligt 
för att de två grupperna

00:10:29.617 --> 00:10:31.595
skulle avslöja samma mängd information,

00:10:31.615 --> 00:10:34.275
som om den andra gruppen 
inte längre brydde sig om

00:10:34.275 --> 00:10:36.687
att lärarna skulle läsa svaren.

00:10:36.687 --> 00:10:40.023
Jag måste erkänna att detta tal hittills

00:10:40.023 --> 00:10:42.503
låter överdrivet dystert,

00:10:42.503 --> 00:10:44.224
men det är inte min avsikt.

00:10:44.224 --> 00:10:46.923
Jag vill dela med mig av vetskapen

00:10:46.923 --> 00:10:48.695
att det finns alternativ.

00:10:48.695 --> 00:10:51.194
Det sätt vi gör saker på nu 
är inte det enda sättet

00:10:51.194 --> 00:10:54.231
och definitivt inte 
det bästa sättet

00:10:54.231 --> 00:10:56.258
att göra dem på.

00:10:56.258 --> 00:11:00.429
Om någon säger till dig, 
"Folk bryr sig inte om sitt privatliv",

00:11:00.429 --> 00:11:02.861
begrunda då om spelets regler 
har designats

00:11:02.871 --> 00:11:05.935
och manipulerats så att de inte 
kan bry sig om sina privatliv

00:11:05.935 --> 00:11:09.027
och att när de kommer underfund med 
att denna manipulering sker

00:11:09.027 --> 00:11:10.824
är de redan halvvägs genom processen

00:11:10.824 --> 00:11:12.922
av att kunna skydda sig själva.

00:11:12.922 --> 00:11:16.632
Om någon säger till dig att ett 
privatliv inte är kompatibelt

00:11:16.632 --> 00:11:18.481
med förmånerna av big data,

00:11:18.481 --> 00:11:20.954
ta då i beaktande att 
under de senaste 20 åren

00:11:20.954 --> 00:11:22.871
har forskare skapat teknologier

00:11:22.871 --> 00:11:26.189
som möjliggör vilken elektronisk 
transaktion som helst

00:11:26.189 --> 00:11:29.938
att äga rum på ett sätt 
som skyddar individen.

00:11:29.938 --> 00:11:32.493
Vi kan surfa på nätet anonymt.

00:11:32.493 --> 00:11:35.171
Vi kan skicka mejl som enbart kan läsas

00:11:35.171 --> 00:11:38.453
av den avsedda mottagaren, 
inte ens av NSA.

00:11:38.880 --> 00:11:41.877
Vi kan till och med skydda 
individen inom datautvinning.

00:11:41.877 --> 00:11:45.771
Med andra ord kan vi ha förmånerna 
av stora mängder data

00:11:45.771 --> 00:11:47.903
på samma gång som vi skyddar individen.

00:11:47.903 --> 00:11:51.694
Dessa teknologier innebär 
förstås ett skifte

00:11:51.694 --> 00:11:53.240
i kostnader och intäkter

00:11:53.240 --> 00:11:55.807
mellan informationsinnehavare
och privatpersoner

00:11:55.807 --> 00:11:58.800
vilket kanske är orsaken till 
att vi inte hör mer om dem.

00:11:58.800 --> 00:12:02.506
Detta för mig tillbaka
till Edens lustgård.

00:12:02.506 --> 00:12:05.286
Det finns en annan tolkning

00:12:05.286 --> 00:12:07.095
av historien om Edens lustgård

00:12:07.095 --> 00:12:09.191
som inte har något att göra med att

00:12:09.191 --> 00:12:11.416
Adam och Eva känner sig nakna

00:12:11.416 --> 00:12:13.797
och skamfyllda.

00:12:13.797 --> 00:12:16.578
Du kan höra ekon av denna tolkning

00:12:16.578 --> 00:12:19.360
i John Miltons "Det förlorade paradiset".

00:12:19.360 --> 00:12:23.557
I lustgården så är Adam 
och Eva materiellt nöjda.

00:12:23.557 --> 00:12:25.661
De är lyckliga. De är nöjda.

00:12:25.661 --> 00:12:27.954
Men, de saknar också kunskap

00:12:27.954 --> 00:12:29.594
och självmedvetenhet.

00:12:29.594 --> 00:12:32.913
I samma stund som de äter av

00:12:32.913 --> 00:12:34.206
kunskapens frukt,

00:12:34.206 --> 00:12:36.811
upptäcker de sig själva.

00:12:36.811 --> 00:12:40.842
De blir medvetna. De blir självständiga.

00:12:40.842 --> 00:12:43.968
Priset de får betala är att 
de måste lämna lustgården.

00:12:43.968 --> 00:12:47.849
Så ett privatliv innebär både tillgången

00:12:47.849 --> 00:12:50.811
och priset att betala för friheten.

00:12:50.811 --> 00:12:53.581
Återigen säger marknadsförare

00:12:53.581 --> 00:12:56.600
att big data och sociala medier

00:12:56.600 --> 00:12:59.579
inte bara är ett paradis 
av vinster för dem

00:12:59.579 --> 00:13:02.036
men också en Edens lustgård för oss andra.

00:13:02.036 --> 00:13:03.274
Vi får ett fritt innehåll.

00:13:03.274 --> 00:13:06.397
Vi får spela Angry Birds. 
Vi får målinriktade appar.

00:13:06.397 --> 00:13:09.294
Men inom några år så vet organisationer

00:13:09.294 --> 00:13:10.903
så mycket om oss

00:13:10.903 --> 00:13:13.613
att de kan dra slutsatsen om våra önskemål

00:13:13.613 --> 00:13:15.817
innan vi ens hunnit forma dem och kanske

00:13:15.817 --> 00:13:18.264
köpa produkter på våra vägnar

00:13:18.264 --> 00:13:20.538
innan vi ens visste att vi behövde dem.

00:13:20.538 --> 00:13:23.775
Det fanns en engelsk författare

00:13:23.775 --> 00:13:26.820
som anade en sådan framtid

00:13:26.820 --> 00:13:28.225
där vi skulle byta ut vår

00:13:28.225 --> 00:13:31.773
självständighet och frihet
för bekvämlighet.

00:13:31.773 --> 00:13:33.934
Ännu mer än författaren George Orwell,

00:13:33.934 --> 00:13:36.695
jag talar förstås om Aldous Huxley.

00:13:36.695 --> 00:13:39.549
I "Du sköna nya värld" föreställer 
han sig ett samhälle

00:13:39.549 --> 00:13:41.720
där de teknologier som vi skapat,

00:13:41.720 --> 00:13:43.579
ursprungligen för frihet,

00:13:43.579 --> 00:13:46.146
till slut förtrycker oss.

00:13:46.146 --> 00:13:50.937
I boken erbjuder han 
emellertid en väg ut ur

00:13:50.937 --> 00:13:54.375
det samhället, som liknar den väg

00:13:54.375 --> 00:13:58.330
som Adam och Eva måste följa 
för att lämna lustgården.

00:13:58.330 --> 00:14:00.477
Enligt Vildens ord

00:14:00.477 --> 00:14:03.546
är det möjligt att återfå 
självstyre och frihet,

00:14:03.546 --> 00:14:06.225
men priset att betala är svindlande.

00:14:06.865 --> 00:14:11.430
Så jag tror att en av vår tids

00:14:11.450 --> 00:14:14.503
viktigaste kamper kommer att vara kampen

00:14:14.503 --> 00:14:16.890
om kontroll över personlig information,

00:14:16.890 --> 00:14:20.397
kampen om huruvida big data 
kommer bli en kraft

00:14:20.397 --> 00:14:22.586
för frihet, istället för en kraft

00:14:22.586 --> 00:14:25.985
som manipulerar oss i det fördolda.

00:14:26.432 --> 00:14:29.025
Just nu är det många av oss

00:14:29.025 --> 00:14:31.778
som inte ens vet om att kampen pågår,

00:14:31.778 --> 00:14:34.450
men det gör den, vare sig 
du vill det eller inte.

00:14:34.450 --> 00:14:37.254
Och med risken att ta ormens roll,

00:14:37.254 --> 00:14:40.151
vill jag säga till er att 
redskapen för kampen är,

00:14:40.151 --> 00:14:43.160
vetskapen om vad som pågår,

00:14:43.160 --> 00:14:44.785
och att det ligger i era händer,

00:14:44.785 --> 00:14:48.255
bara några klick bort.

00:14:48.255 --> 00:14:50.147
Tack.

00:14:50.167 --> 00:14:53.164
(Applåder)

