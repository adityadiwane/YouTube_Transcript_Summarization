WEBVTT
Kind: captions
Language: el

00:00:00.000 --> 00:00:07.000
Μετάφραση: Chryssa Takahashi
Επιμέλεια: Miriela Patrikiadou

00:00:12.641 --> 00:00:14.995
Θα ήθελα να σας πω μια ιστορία

00:00:14.995 --> 00:00:18.171
που συνδέει το διαβόητο περιστατικό 
για την προστασία της ιδιωτικής ζωής

00:00:18.171 --> 00:00:20.940
που είχε να κάνει 
με τον Αδάμ και την Εύα

00:00:20.940 --> 00:00:24.386
και την αξιοσημείωτη μετατόπιση στα όρια

00:00:24.386 --> 00:00:27.072
μεταξύ του δημοσίου και του ιδιωτικoύ,

00:00:27.072 --> 00:00:28.842
η οποία έγινε τα τελευταία 10 χρόνια.

00:00:28.842 --> 00:00:30.140
Ξέρετε το συμβάν.

00:00:30.140 --> 00:00:33.470
Ο Αδάμ και η Εύα, 
μία μέρα στον Κήπο της Εδέμ

00:00:33.470 --> 00:00:35.313
συνειδητοποιούν ότι είναι γυμνοί.

00:00:35.313 --> 00:00:36.813
Φρικάρουν.

00:00:36.813 --> 00:00:39.570
Και τα υπόλοιπα είναι ιστορία.

00:00:39.570 --> 00:00:41.758
Σήμερα, ο Αδάμ και η Εύα

00:00:41.758 --> 00:00:44.119
μάλλον θα συμπεριφερόντουσαν
διαφορετικά.

00:00:44.119 --> 00:00:46.387
[@Adam Χτες ήταν τζαμάτα! 
Το μήλο ήταν τέλειο LOL]

00:00:46.387 --> 00:00:48.260
[@Eve ναι.. μωρό, ξέρεις μήπως
τι έγινε με το παντελόνι μου;]

00:00:48.260 --> 00:00:50.896
Αποκαλύπτουμε πολύ
περισσότερες πληροφορίες

00:00:50.896 --> 00:00:54.230
για τους εαυτούς μας στο Διαδίκτυο
παρά ποτέ

00:00:54.230 --> 00:00:55.934
και τόσες πληροφορίες για μας

00:00:55.934 --> 00:00:58.158
συλλέγονται από εταιρίες.

00:00:58.158 --> 00:01:01.440
Είναι πολλά τα κέρδη και τα οφέλη

00:01:01.440 --> 00:01:03.886
από αυτή την μαζική ανάλυση
προσωπικών πληροφοριών

00:01:03.886 --> 00:01:05.832
ή τα μεγάλα δεδομένα,

00:01:05.832 --> 00:01:08.470
αλλά υπάρχουν και πολύπλοκα 
ανταλλάγματα που έρχονται

00:01:08.470 --> 00:01:11.568
με το να χαρίζουμε την προστασία
των προσωπικών μας δεδομένων.

00:01:11.568 --> 00:01:15.591
Και η ιστορία μου έχει να κάνει
με αυτά τα ανταλλάγματα.

00:01:15.591 --> 00:01:18.175
Ξεκινάμε με μία παρατήρηση,
η οποία, στο μυαλό μου,

00:01:18.175 --> 00:01:21.502
έχει γίνει όλο και πιο ξεκάθαρη
τα τελευταία χρόνια,

00:01:21.502 --> 00:01:23.599
ότι οποιαδήποτε προσωπική πληροφορία

00:01:23.599 --> 00:01:25.884
μπορεί να γίνει ευαίσθητη πληροφορία.

00:01:25.884 --> 00:01:30.009
Το 2000, περίπου 100
δισεκατομμύρια φωτογραφίες

00:01:30.009 --> 00:01:31.921
τραβήχτηκαν σε όλο τον κόσμο,

00:01:31.921 --> 00:01:34.986
αλλά μόνο ένα μικροσκοπικό ποσοστό τους

00:01:34.986 --> 00:01:36.869
ανέβηκαν στο διαδίκτυο.

00:01:36.869 --> 00:01:40.230
Το 2010, μόνο στο Facebook, 
σε έναν μόνο μήνα,

00:01:40.230 --> 00:01:43.500
ανεβήκαν 2,5 δισεκατομμύρια
φωτογραφίες,

00:01:43.500 --> 00:01:45.382
οι περισσότερες από αυτές 
έχουν εντοπιστεί.

00:01:45.382 --> 00:01:47.262
Στο ίδιο χρονικό διάστημα,

00:01:47.262 --> 00:01:52.132
η ικανότητα των υπολογιστών
να αναγνωρίζουν άτομα σε φωτογραφίες

00:01:52.132 --> 00:01:55.740
βελτιώθηκε κατά τρεις τάξεις μεγέθους.

00:01:55.740 --> 00:01:57.622
Τι συμβαίνει όταν συνδυάσετε

00:01:57.622 --> 00:01:59.123
αυτές τις τεχνολογίες:

00:01:59.123 --> 00:02:01.781
αύξηση της διαθεσιμότητας 
των δεδομένων του προσώπου,

00:02:01.781 --> 00:02:05.429
βελτίωση της ικανότητας αναγνώρισης 
προσώπου από υπολογιστές,

00:02:05.429 --> 00:02:07.611
αλλά και το υπολογιστικό νέφος,

00:02:07.611 --> 00:02:09.499
το οποίο δίνει σε οποιονδήποτε
σε αυτή την αίθουσα

00:02:09.499 --> 00:02:11.059
το είδος της υπολογιστικής δύναμης

00:02:11.059 --> 00:02:12.945
η οποία πριν από λίγα χρόνια 
ήταν μόνο ο τομέας

00:02:12.945 --> 00:02:14.727
οργανισμών με τρία γράμματα

00:02:14.727 --> 00:02:16.105
και πανταχού παρούσα 
υπολογιστική τεχνολογία,

00:02:16.105 --> 00:02:18.997
η οποία επιτρέπει στο τηλέφωνό μου, 
το οποίο δεν είναι ένας υπερ-υπολογιστής

00:02:18.997 --> 00:02:20.668
να συνδέεται στο διαδίκτυο

00:02:20.668 --> 00:02:23.002
και να κάνει εκεί εκατοντάδες χιλιάδες

00:02:23.002 --> 00:02:25.641
μετρήσεις προσώπων σε λίγα δευτερόλεπτα.

00:02:25.641 --> 00:02:28.269
Λοιπόν, εικάζουμε ότι το αποτέλεσμα

00:02:28.269 --> 00:02:30.333
αυτού του συνδυασμού τεχνολογιών

00:02:30.333 --> 00:02:33.221
θα είναι μία ριζική αλλαγή 
στις αντιλήψεις μας

00:02:33.221 --> 00:02:35.478
σχετικά με την προστασία προσωπικών 
δεδομένων και της ανωνυμίας.

00:02:35.478 --> 00:02:37.471
Για να το ελέγξουμε αυτό,
κάναμε ένα πείραμα

00:02:37.471 --> 00:02:39.592
στην πανεπιστημιούπολη
του Carnegie Mellon.

00:02:39.592 --> 00:02:41.691
Ρωτήσαμε φοιτητές που περνούσαν

00:02:41.691 --> 00:02:43.470
να πάρουν μέρος σε μία έρευνα

00:02:43.470 --> 00:02:46.032
και τραβήξαμε μία φωτογραφία
με μία διαδικτυακή κάμερα

00:02:46.032 --> 00:02:48.814
και τους ζητήσαμε να συμπληρώσουν
μία έρευνα σε έναν φορητό υπολογιστή.

00:02:48.814 --> 00:02:50.793
Καθώς συμπλήρωναν την έρευνα,

00:02:50.793 --> 00:02:53.590
ανεβάσαμε τη φωτογραφία τους
σε ένα σύμπλεγμα υπολογιστικού νέφους

00:02:53.590 --> 00:02:55.317
και αρχίσαμε να χρησιμοποιούμε
αναγνώριση προσώπου

00:02:55.317 --> 00:02:57.722
για να ταιριάξουμε αυτή τη φωτογραφία
σε μία βάση δεδομένων

00:02:57.722 --> 00:03:00.115
με μερικές εκατοντάδες χιλιάδες
φωτογραφίες

00:03:00.115 --> 00:03:03.711
τις οποίες είχαμε κατεβάσει
από προφίλ στο Facebook.

00:03:03.711 --> 00:03:06.970
Μέχρι να φτάσει το άτομο
στην τελευταία σελίδα της έρευνας,

00:03:06.970 --> 00:03:10.317
η σελίδα είχε ανανεωθεί δυναμικά

00:03:10.317 --> 00:03:12.630
με τις 10 φωτογραφίες
που ταίριαζαν καλύτερα

00:03:12.630 --> 00:03:14.915
τις οποίες είχε βρεί 
ο αναγνωριστής

00:03:14.915 --> 00:03:16.653
και τους ζητήσαμε να υποδηλώσουν

00:03:16.653 --> 00:03:20.773
αν έβλεπαν τον εαυτό τους 
στη φωτογραφία.

00:03:20.773 --> 00:03:24.472
Βλέπετε το άτομο;

00:03:24.472 --> 00:03:27.317
Λοιπόν, ο υπολογίστής το είδε
και στην πραγματικότητα το είδε

00:03:27.317 --> 00:03:29.466
για ένα από τα τρία άτομα.

00:03:29.466 --> 00:03:32.650
Έτσι, ουσιαστικά, μπορούμε να ξεκινήσουμε 
από ένα ανώνυμο πρόσωπο,

00:03:32.650 --> 00:03:36.134
στο διαδίκτυο ή εκτός, και να 
χρησιμοποιήσουμε αναγνώριση προσώπου

00:03:36.134 --> 00:03:38.494
για να δώσουμε ένα όνομα
σε αυτό το ανώνυμο πρόσωπο

00:03:38.494 --> 00:03:40.602
χάρη στα δεδομένα κοινωνικών μέσων.

00:03:40.602 --> 00:03:42.474
Αλλά πριν από μερικά χρόνια,
κάναμε κάτι άλλο.

00:03:42.474 --> 00:03:44.297
Ξεκινήσαμε από δεδομένα
κοινωνικών μέσων,

00:03:44.297 --> 00:03:47.348
τα συνδυάσαμε στατιστικά
με δεδομένα

00:03:47.348 --> 00:03:49.450
από την κοινωνική ασφάλιση
της κυβέρνησης των ΗΠΑ

00:03:49.450 --> 00:03:52.774
και καταλήξαμε να προβλέπουμε 
αριθμούς κοινωνικών ασφαλίσεων,

00:03:52.774 --> 00:03:54.286
οι οποίοι στις Ηνωμένες Πολιτείες

00:03:54.286 --> 00:03:56.326
είναι εξαιρετικά ευαίσθητη πληροφορία.

00:03:56.326 --> 00:03:58.419
Βλέπετε που το πάω;

00:03:58.419 --> 00:04:01.341
Έτσι, αν συνδυάσετε τις δύο μελέτες,

00:04:01.341 --> 00:04:02.853
το ερώτημα που προκύπτει είναι,

00:04:02.853 --> 00:04:05.573
μπορείτε να ξεκινήσετε 
από ένα πρόσωπο και

00:04:05.573 --> 00:04:07.884
χρησιμοποιώντας αναγνώριση προσώπου,
να βρείτε ένα όνομα

00:04:07.884 --> 00:04:10.553
και δημόσια διαθέσιμες πληροφορίες

00:04:10.553 --> 00:04:12.485
σχετικά με αυτό το όνομα
και αυτό το άτομο

00:04:12.485 --> 00:04:14.733
και από αυτήν την δημόσια διαθέσιμη
πληροφορία

00:04:14.733 --> 00:04:16.775
να εντοπίσουν μη-δημόσια
διαθέσιμες πληροφορίες,

00:04:16.775 --> 00:04:18.381
πολύ πιο ευαίσθητες

00:04:18.381 --> 00:04:19.873
τις οποίες συνδέετε με το πρόσωπο;

00:04:19.873 --> 00:04:21.789
Και η απάντηση είναι, ναι, 
μπορούμε και το κάναμε.

00:04:21.789 --> 00:04:24.357
Φυσικά, η ακρίβεια όλο και χειροτερεύει.

00:04:24.357 --> 00:04:25.301
[Βρέθηκαν τα 5 πρώτα ψηφία του αριθμού ΙΚΑ
στο 27% των ατόμων (με 4 προσπάθειες)]

00:04:25.301 --> 00:04:29.128
Αλλά στην πραγματικότητα, αποφασίσαμε
να αναπτύξουμε μία εφαρμογή για iPhone

00:04:29.128 --> 00:04:31.843
η οποία χρησιμοποιεί την εσωτερική
κάμερα του τηλεφώνου

00:04:31.843 --> 00:04:33.443
για να βγάλει μία φωτογραφία
του ατόμου

00:04:33.443 --> 00:04:34.930
και να την ανεβάσει στο σύννεφο

00:04:34.930 --> 00:04:37.592
και μετά να κάνει αυτό που μόλις σας
περιέγραψα σε πραγματικό χρόνο:

00:04:37.592 --> 00:04:39.680
αναζήτηση ταύτισης,
εύρεση δημοσίων πληροφοριών,

00:04:39.680 --> 00:04:41.410
προσπάθεια εντοπισμού
ευαίσθητων πληροφοριών

00:04:41.410 --> 00:04:44.001
και μετά αποστολή τους πίσω
στο τηλέφωνο

00:04:44.001 --> 00:04:47.610
έτσι ώστε να επικαλύψει
το πρόσωπο του ατόμου.

00:04:47.610 --> 00:04:49.511
Ένα παράδειγμα επαυξημένης 
πραγματικότητας,

00:04:49.511 --> 00:04:51.962
μάλλον ένα ανατριχιαστικό παράδειγμα
επαυξημένης πραγματικότητας.

00:04:51.962 --> 00:04:55.301
Στην πραγματικότητα, δεν αναπτύξαμε
την εφαρμογή για να την διαθέσουμε,

00:04:55.301 --> 00:04:57.223
απλώς ως απόδειξη της έννοιας.

00:04:57.223 --> 00:04:59.536
Βασικά, πάρτε αυτές τις τεχνολογίες

00:04:59.536 --> 00:05:01.373
και ωθήστε τις στα λογικά τους άκρα.

00:05:01.373 --> 00:05:04.092
Φανταστείτε ένα μέλλον
όπου οι άγνωστοι γύρω σας

00:05:04.092 --> 00:05:06.403
θα σας κοιτάνε μέσω 
των γυαλιών της Google

00:05:06.403 --> 00:05:08.710
ή, μία μέρα, τους φακούς επαφής τους

00:05:08.710 --> 00:05:12.730
και θα χρησιμοποιούν επτά
ή οκτώ σημεία δεδομένων για εσάς

00:05:12.730 --> 00:05:15.312
για να βρουν οτιδήποτε άλλο

00:05:15.312 --> 00:05:17.915
μπορεί να είναι γνωστό για σας.

00:05:17.915 --> 00:05:22.709
Πώς θα είναι αυτό το μέλλον 
χωρίς μυστικά;

00:05:22.709 --> 00:05:24.673
Και πρέπει να νοιαστούμε;

00:05:24.673 --> 00:05:26.564
Ίσως θέλουμε να πιστεύουμε

00:05:26.564 --> 00:05:29.604
ότι το μέλλον με τόσα πολλά δεδομένα

00:05:29.604 --> 00:05:32.118
θα είναι ένα μέλλον χωρίς προκαταλήψεις,

00:05:32.118 --> 00:05:35.701
αλλά στην πραγματικότητα, 
το να έχεις τόσες πολλές πληροφορίες

00:05:35.701 --> 00:05:37.892
δεν σημαίνει ότι θα κάνουμε επιλογές

00:05:37.892 --> 00:05:39.598
οι οποίες είναι πιο αντικειμενικές.

00:05:39.598 --> 00:05:42.158
Σε ένα άλλο πείραμα, 
παρουσιάσαμε στους συμμετέχοντες

00:05:42.158 --> 00:05:44.404
πληροφορίες σχετικά 
με έναν πιθανό υποψήφιο για δουλειά.

00:05:44.404 --> 00:05:47.582
Συμπεριλάβαμε μερικές συστάσεις 
σε αυτές τις πληροφορίες

00:05:47.582 --> 00:05:50.228
μερικές αστείες, απολύτως νόμιμες,

00:05:50.228 --> 00:05:52.693
αλλά ίσως ελαφρώς 
ντροπιαστικές πληροφορίες

00:05:52.693 --> 00:05:54.713
τις οποίες είχε ανεβάσει 
στο διαδίκτυο ο συμμετέχων.

00:05:54.713 --> 00:05:57.079
Τώρα είναι πολύ ενδιαφέρον, ότι
ανάμεσα στους συμμετέχοντές μας,

00:05:57.079 --> 00:06:00.162
μερικοί είχαν ανεβάσει 
αντίστοιχες πληροφορίες,

00:06:00.162 --> 00:06:02.524
και μερικοί όχι.

00:06:02.524 --> 00:06:04.473
Ποια ομάδα νομίζετε

00:06:04.473 --> 00:06:09.025
ότι ήταν πιο πιθανόν να κρίνει
πιο σκληρά τον συμμετέχοντα?

00:06:09.025 --> 00:06:10.982
Παραδόξως, ήταν η ομάδα

00:06:10.982 --> 00:06:12.715
που είχε ανεβάσει
παρόμοιες πληροφορίες,

00:06:12.715 --> 00:06:15.657
ένα παράδειγμα ηθικής παραφωνίας.

00:06:15.657 --> 00:06:17.407
Τώρα ίσως να σκέφτεστε,

00:06:17.407 --> 00:06:19.109
αυτό δεν με αφορά,

00:06:19.109 --> 00:06:21.271
επειδή δεν έχω τίποτα να κρύψω.

00:06:21.271 --> 00:06:23.753
Όμως, στην πραγματικότητα, η προστασία
των προσωπικών δεδομένων δεν έχει να κάνει

00:06:23.753 --> 00:06:27.429
με το να έχεις κάτι αρνητικό να κρύψεις.

00:06:27.429 --> 00:06:29.783
Φανταστείτε ότι είστε
ο διευθυντής ανθρώπινου δυναμικού

00:06:29.783 --> 00:06:32.730
μίας συγκεκριμένης εταιρίας
και λαμβάνετε βιογραφικά

00:06:32.730 --> 00:06:35.203
και αποφασίζετε να βρείτε περισσότερες
πληροφορίες για τους υποψήφιους.

00:06:35.203 --> 00:06:37.663
Έτσι, γκουγκλάρετε τα ονόματά τους

00:06:37.663 --> 00:06:39.903
και σε κάποιο σύμπαν,

00:06:39.903 --> 00:06:41.911
βρίσκετε αυτές τις πληροφορίες.

00:06:41.911 --> 00:06:46.348
Ή σε ένα παράλληλο σύμπαν,
βρίσκετε αυτές τις πληροφορίες.

00:06:46.348 --> 00:06:49.065
Πιστεύετε ότι θα ήταν εξίσου πιθανό

00:06:49.065 --> 00:06:51.868
να καλέσετε κάποιον από τους υποψήφιους
για μία συνέντευξη;

00:06:51.868 --> 00:06:54.150
Αν το νομίζετε,
τότε δεν είσαστε

00:06:54.150 --> 00:06:56.732
σαν τους Αμερικανούς εργοδότες,
οι οποίοι, στην πραγματικότητα,

00:06:56.732 --> 00:07:00.039
είναι μέρος του πειράματός μας, 
που σημαίνει ότι κάναμε αυτό ακριβώς.

00:07:00.039 --> 00:07:03.221
Δημιουργήσαμε προφίλ στο Facebook, 
χειραγωγώντας χαρακτηριστικά,

00:07:03.221 --> 00:07:06.072
μετά αρχίσαμε να στέλνουμε βιογραφικά
σε εταιρίες στις Η.Π.Α.

00:07:06.072 --> 00:07:07.980
και εντοπίσαμε, παρακολουθήσαμε,

00:07:07.980 --> 00:07:10.373
αν έψαχναν για τους υποψηφίους μας,

00:07:10.373 --> 00:07:12.205
και αν ενεργούσαν 
σύμφωνα με τις πληροφορίες

00:07:12.205 --> 00:07:14.143
που έβρισκαν στα κοινωνικά μέσα.
Και το έκαναν.

00:07:14.143 --> 00:07:16.244
Γινόντουσαν διακρίσεις μέσω 
των κοινωνικών μέσων

00:07:16.244 --> 00:07:19.317
για εξίσου έμπειρους υποψήφιους.

00:07:19.317 --> 00:07:23.892
Τώρα οι μαρκετίστες σαν κι εμάς 
θέλουν να πιστεύουν

00:07:23.892 --> 00:07:26.161
ότι όλες οι πληροφορίες για μας

00:07:26.161 --> 00:07:29.434
θα χρησιμοποιούνται πάντα
με έναν ευνοϊκό για μας τρόπο.

00:07:29.434 --> 00:07:33.149
Ξανασκεφτείτε το. 
Γιατί να είναι πάντα έτσι;

00:07:33.149 --> 00:07:35.813
Σε μία ταινία που βγήκε 
πριν από μερικά χρόνια,

00:07:35.813 --> 00:07:38.366
το «Minority Report»,
σε μία διάσημη σκηνή

00:07:38.366 --> 00:07:40.942
ο Τομ Κρουζ περπατούσε 
σε ένα εμπορικό κέντρο

00:07:40.942 --> 00:07:44.718
και εμφανιζόταν γύρω του

00:07:44.718 --> 00:07:46.553
προσωποποιημένες διαφημίσεις
σε ολόγραμμα.

00:07:46.553 --> 00:07:49.780
Τώρα, αυτή η ταινία 
διαδραματίζεται στο 2054,

00:07:49.780 --> 00:07:51.422
σε περίπου 40 χρόνια από τώρα

00:07:51.422 --> 00:07:54.330
και όσο συναρπαστική 
κι αν φαίνεται αυτή η τεχνολογία,

00:07:54.330 --> 00:07:56.976
ήδη υποτιμά αφάνταστα

00:07:56.976 --> 00:07:59.116
τον όγκο των πληροφοριών 
που οι εταιρίες

00:07:59.116 --> 00:08:01.599
μπορούν να συλλέξουν για σας
και πώς μπορούν να τις χρησιμοποιήσουν

00:08:01.599 --> 00:08:04.997
για να σας επηρεάσουν με ένα τρόπο
που δεν θα τον καταλάβετε καν.

00:08:04.997 --> 00:08:07.100
Έτσι, σαν παράδειγμα, 
αυτό είναι ένα άλλο πείραμα

00:08:07.100 --> 00:08:09.373
που κάνουμε τώρα, 
που δεν έχει ολοκληρωθεί ακόμη.

00:08:09.373 --> 00:08:11.692
Φανταστείτε πως μία εταιρία
έχει πρόσβαση

00:08:11.692 --> 00:08:13.748
στη λίστα με τους φίλους σας
στο Facebook

00:08:13.748 --> 00:08:15.520
και μέσω κάποιου αλγόριθμου

00:08:15.520 --> 00:08:19.254
μπορούν να εντοπίσουν ποιοι δύο φίλοι
σας αρέσουν περισσότερο.

00:08:19.254 --> 00:08:21.534
Και μετά δημιουργούν, 
σε πραγματικό χρόνο,

00:08:21.534 --> 00:08:24.376
μία σύνθεση από τα πρόσωπα
των δύο αυτών φίλων.

00:08:24.376 --> 00:08:27.445
Μελέτες, πριν από την δική μας,
έχουν δείξει ότι οι άνθρωποι

00:08:27.445 --> 00:08:30.330
δεν αναγνωρίζουν ούτε τους εαυτούς τους

00:08:30.330 --> 00:08:32.792
σε συνθέσεις προσώπων, αλλά αντιδρούν

00:08:32.792 --> 00:08:34.909
σε αυτές τις συνθέσεις 
με θετικό τρόπο.

00:08:34.909 --> 00:08:38.324
Έτσι την επόμενη φορά που θα ψάχνετε
για ένα συγκεκριμένο προϊόν

00:08:38.324 --> 00:08:40.883
και μία διαφήμηση σας προτείνει 
να το αγοράσετε,

00:08:40.883 --> 00:08:43.790
δεν θα είναι απλώς 
ένας τυπικός εκπρόσωπος.

00:08:43.790 --> 00:08:46.103
Θα είναι ένας από τους φίλους σας

00:08:46.103 --> 00:08:49.406
και δεν θα ξέρετε καν ότι συμβαίνει αυτό.

00:08:49.406 --> 00:08:51.819
Τώρα το πρόβλημα είναι ότι

00:08:51.819 --> 00:08:54.338
οι υπάρχοντες μηχανισμοί πολιτικής
που έχουμε

00:08:54.338 --> 00:08:57.776
για να προστατεύσουμε τους εαυτούς μας 
από καταχρήσεις των προσωπικών πληροφοριών

00:08:57.776 --> 00:09:00.760
είναι σα να φέρνουμε μαχαίρι
σε ένα πιστολίδι.

00:09:00.760 --> 00:09:03.673
Ο ένας από αυτούς τους μηχανισμούς
είναι η διαφάνεια,

00:09:03.673 --> 00:09:06.873
το να λες στον κόσμο τι θα κάνεις
με τα δεδομένα τους.

00:09:06.873 --> 00:09:08.979
Κατ 'αρχήν, αυτό είναι 
ένα πολύ καλό πράγμα.

00:09:08.979 --> 00:09:12.646
Είναι απαραίτητο,
αλλά δεν είναι αρκετό.

00:09:12.646 --> 00:09:16.344
Η διαφάνεια μπορεί να πάει
σε λανθασμένη κατεύθυνση.

00:09:16.344 --> 00:09:18.448
Μπορείτε να πείτε στον κόσμο
τι θα κάνετε,

00:09:18.448 --> 00:09:20.680
αλλά μετά συνεχίζετε να τους ωθείτε
να αποκαλύψουν

00:09:20.680 --> 00:09:23.303
αυθαίρετες ποσότητες 
προσωπικών πληροφοριών.

00:09:23.303 --> 00:09:26.189
Σε ένα ακόμη πείραμα,
αυτή τη φορά με φοιτητές,

00:09:26.189 --> 00:09:29.247
τους ζητήσαμε να παρέχουν πληροφορίες

00:09:29.247 --> 00:09:31.060
σχετικά με την συμπεριφορά τους
στην πανεπιστημιούπολη,

00:09:31.060 --> 00:09:34.000
συμπεριλαμβανομένων πολύ 
ευαίσθητων ερωτήσεων, όπως αυτή.

00:09:34.000 --> 00:09:34.621
[Έχεις αντιγράψει ποτέ σε εξετάσεις;]

00:09:34.621 --> 00:09:36.921
Στην μία ομάδα είπαμε,

00:09:36.921 --> 00:09:39.762
«Μόνο άλλοι φοιτητές 
θα δούνε τις απαντήσεις σας.»

00:09:39.762 --> 00:09:41.341
Στην άλλη ομάδα είπαμε,

00:09:41.341 --> 00:09:44.902
«Φοιτητές και καθηγητές θα δούνε
τις απαντήσεις σας.»

00:09:44.902 --> 00:09:47.493
Διαφάνεια. Ειδοποίηση.
Και φυσικά, δούλεψε,

00:09:47.493 --> 00:09:48.900
από την άποψη ότι η πρώτη ομάδα

00:09:48.900 --> 00:09:51.468
ήταν πολύ πιο πιθανό 
να αποκαλύψει από ότι η δεύτερη.

00:09:51.468 --> 00:09:52.988
Κατανοητό, έτσι δεν είναι;

00:09:52.988 --> 00:09:54.478
Αλλα μετά προσθέσαμε την παραπλάνηση.

00:09:54.478 --> 00:09:57.238
Επαναλάβαμε το πείραμα
με τις δύο ίδιες ομάδες,

00:09:57.238 --> 00:09:59.665
αλλά αυτή τη φορά 
προσθέσαμε μία καθυστέρηση

00:09:59.665 --> 00:10:02.600
μεταξύ της στιγμής που είπαμε
στους συμμετέχοντες

00:10:02.600 --> 00:10:04.680
πώς θα χρησιμοποιήσουμε
τα δεδομένα τους

00:10:04.680 --> 00:10:09.068
και την ώρα που ξεκινήσαμε 
να απαντάμε στις ερωτήσεις.

00:10:09.068 --> 00:10:11.629
Πόση νομίζετε πως ήταν η καθυστέρηση
που έπρεπε να προσθέσουμε

00:10:11.629 --> 00:10:16.242
για να μηδενίσουμε 
την ανασταλτική επίδραση

00:10:16.242 --> 00:10:19.653
της γνώσης ότι οι καθηγητές
θα δουν τις απαντήσεις σας;

00:10:19.653 --> 00:10:21.433
Δέκα λεπτά;

00:10:21.433 --> 00:10:23.224
Πέντε λεπτά;

00:10:23.224 --> 00:10:25.000
Ένα λεπτό;

00:10:25.000 --> 00:10:27.049
Τι λέτε για 15 δευτερόλεπτα;

00:10:27.049 --> 00:10:29.717
Δεκαπέντε δευτερόλεπττα ήταν αρκετά
για να κάνουμε τις δύο ομάδες

00:10:29.717 --> 00:10:31.285
να αποκαλύψουν τον ίδιο
όγκο πληροφοριών,

00:10:31.285 --> 00:10:34.031
λες και η δεύτερη ομάδα
δεν νοιαζόταν πια

00:10:34.031 --> 00:10:36.687
για το αν οι καθηγητές 
θα διάβαζαν τις απαντήσεις τους.

00:10:36.687 --> 00:10:40.023
Τώρα πρέπει να ομολογήσω,
ότι αυτή η ομιλία μέχρι τώρα

00:10:40.023 --> 00:10:42.503
μπορεί να ακούγεται 
ιδιαίτερα απαισιόδοξη,

00:10:42.503 --> 00:10:44.224
αλλά δεν είναι αυτό το θέμα μου.

00:10:44.224 --> 00:10:46.923
Στην πραγματικότητα, θέλω να μοιραστώ
μαζί σας το γεγονός

00:10:46.923 --> 00:10:48.695
ότι υπάρχουν εναλλακτικές.

00:10:48.695 --> 00:10:51.194
Ο τρόπος με τον οποίο κάνουμε πράγματα 
τώρα δεν είναι ο μοναδικός τρόπος

00:10:51.194 --> 00:10:54.231
που μπορούν να γίνουν,
και σίγουρα όχι ο καλύτερος τρόπος

00:10:54.231 --> 00:10:56.258
που μπορούν να γίνουν.

00:10:56.258 --> 00:11:00.429
Όταν σας πουν, «Ο κόσμος δεν ενδιαφέρεται για
την προστασία των προσωπικών του δεδομένων,»

00:11:00.429 --> 00:11:03.071
αναρωτηθείτε αν το παιχνίδι
έχει σχεδιαστεί

00:11:03.071 --> 00:11:05.795
και στηθεί έτσι ώστε να μην νοιάζονται για 
την προστασία των προσωπικών τους δεδομένων

00:11:05.795 --> 00:11:09.057
και όταν συνειδητοποιήσουν ότι 
γίνονται αυτοί οι χειρισμοί

00:11:09.057 --> 00:11:10.664
είναι ήδη στο μέσον της διαδικασίας

00:11:10.664 --> 00:11:12.922
της προστασίας του εαυτού σας.

00:11:12.922 --> 00:11:16.632
Αν σας πουν ότι η προστασία 
των προσωπικών δεδομένων είναι ασύμβατη

00:11:16.632 --> 00:11:18.481
με τα οφέλη των μεγάλων δεδομένων,

00:11:18.481 --> 00:11:20.954
αναλογιστείτε ότι τα τελευταία 20 χρόνια,

00:11:20.954 --> 00:11:22.871
οι ερευνητές δημιούργησαν τεχνόλογίες

00:11:22.871 --> 00:11:26.189
που επιτρέπουν σχεδόν κάθε
ηλεκτρονική συναλλαγή

00:11:26.189 --> 00:11:29.938
να γίνεται με τρόπο που προστατεύει
τα πρσωπικά δεδομένα περισσότερο.

00:11:29.938 --> 00:11:32.493
Μπορούμε να πλοηγηθούμε 
στο διαδίκτυο ανώνυμα.

00:11:32.493 --> 00:11:35.171
Μπορούμε να στείλουμε email 
που μπορουν να διαβαστούν μόνο

00:11:35.171 --> 00:11:38.880
από τον σκοπούμενο αποδέκτη, 
ούτε καν από την Yπηρεσία Εθνικής Ασφαλείας.

00:11:38.880 --> 00:11:41.877
Μπορούμε να έχουμε ακόμη και εξόρυξη 
δεδομένων με διαφύλαξη της ιδιωτικής ζωής.

00:11:41.877 --> 00:11:45.771
Με άλλα λόγια, μπορούμε να έχουμε 
τα οφέλη των μεγάλων δεδομένων

00:11:45.771 --> 00:11:47.903
καθώς προστατεύουμε
τα προσωπικά δεδομένα.

00:11:47.903 --> 00:11:51.694
Φυσικά, αυτές οι τεχνολογίες
υποδηλώνουν μία μετατόπιση

00:11:51.694 --> 00:11:53.240
κόστους και τζίρου

00:11:53.240 --> 00:11:55.347
μεταξύ των κατόχων των δεδομένων και 
των υποκειμένων των δεδομένων,

00:11:55.347 --> 00:11:58.800
και γι'αυτό, ίσως,
να μην ακούτε πιο πολλά γι'αυτά.

00:11:58.800 --> 00:12:02.506
Κι επιστρέφω στον Κήπο της Εδέμ.

00:12:02.506 --> 00:12:05.286
Υπάρχει και μία δεύτερη ερμηνεία
για την προστασία προσωπικών δεδομένων

00:12:05.286 --> 00:12:07.095
στην ιστορία του Κήπου της Εδέμ

00:12:07.095 --> 00:12:09.191
η οποία δεν έχει να κάνει 
με το γεγονός ότι

00:12:09.191 --> 00:12:11.416
ο Αδάμ και η Εύα νιώθουν γυμνοί

00:12:11.416 --> 00:12:13.797
και νιώθουν ντροπή.

00:12:13.797 --> 00:12:16.578
Μπορείτε να βρείτε την ηχώ
αυτής της ερμηνείας

00:12:16.578 --> 00:12:19.360
στο «Χαμένος Παράδεισος» του Τζον Μίλτον.

00:12:19.360 --> 00:12:23.557
Στον κήπο, ο Αδάμ και η Εύα
είναι υλικά ικανοποιημένοι.

00:12:23.557 --> 00:12:25.661
Είναι ευτυχισμένοι. 
Είναι ικανοποιημένοι.

00:12:25.661 --> 00:12:27.954
Όμως δεν έχουν γνώση

00:12:27.954 --> 00:12:29.594
και αυτογνωσία.

00:12:29.594 --> 00:12:32.913
Τη στιγμή που τρώνε 
αυτό που εύστοχα ονομάστηκε

00:12:32.913 --> 00:12:34.206
φρούτο της γνώσης,

00:12:34.206 --> 00:12:36.811
τότε ανακάλυψαν τους εαυτούς τους.

00:12:36.811 --> 00:12:40.842
Έχουν συνείδηση. 
Καταφέρνουν να έχουν αυτονομία.

00:12:40.842 --> 00:12:43.968
Όμως το τίμημα, 
είναι η δίωξη από τον κήπο.

00:12:43.968 --> 00:12:47.849
Έτσι, η προστασία των προσωπικών δεδομένων,
κατά κάποιο τρόπο, είναι και ο τρόπος

00:12:47.849 --> 00:12:50.811
και το τίμημα της ελευθερίας.

00:12:50.811 --> 00:12:53.581
Και πάλι, οι μαρκετίστες μας λένε

00:12:53.581 --> 00:12:56.600
ότι τα μεγάλα δεδομένα
και τα κοινωνικά μέσα

00:12:56.600 --> 00:12:59.579
δεν είναι απλώς ένας παράδεισος 
από κέρδη γι'αυτούς,

00:12:59.579 --> 00:13:02.036
αλλά ένας Κήπος της Εδέμ
για τους υπόλοιπους από εμας.

00:13:02.036 --> 00:13:03.274
Παίρνουμε δωρεάν περιεχόμενο.

00:13:03.274 --> 00:13:06.397
Μπορούμε να παίξουμε Angry Birds. 
Παίρνουμε στοχευμένες εφαρμογές.

00:13:06.397 --> 00:13:09.294
Αλλά στην πραγματικότητα,
σε μερικά χρόνια, οι εταιρίες

00:13:09.294 --> 00:13:10.903
θα γνωρίζουν τόσα πολλά για μας,

00:13:10.903 --> 00:13:13.613
που θα μπορούν να συμπεραίνουν 
τις επιθυμίες μας

00:13:13.613 --> 00:13:15.817
πριν ακόμη τις σχηματίσουμε,

00:13:15.817 --> 00:13:18.264
και ίσως να αγοράζουν προϊόντα 
για λογαριασμό μας

00:13:18.264 --> 00:13:20.538
πριν καν ξέρουμε ότι τα χρειαζόμαστε.

00:13:20.538 --> 00:13:23.775
Υπήρχε ένας Άγγλος συγγραφέας

00:13:23.775 --> 00:13:26.820
ο οποίος προέβλεψε ένα τέτοιο μέλλον

00:13:26.820 --> 00:13:28.225
όπου θα ανταλλάζαμε
την αυτονομία μας

00:13:28.225 --> 00:13:31.773
και την ελευθερία μας
για την άνεσή μας.

00:13:31.773 --> 00:13:33.934
Ακόμη περισσότερο από
τον Τζώρτζ Όργουελ,

00:13:33.934 --> 00:13:36.695
ο συγγραφέας είναι, φυσικά,
ο Άλντους Χάξλεϋ.

00:13:36.695 --> 00:13:39.549
Στο «Θαυμαστός Καινούργιος Κόσμος»,
φαντάζεται μία κοινωνία

00:13:39.549 --> 00:13:41.720
όπου οι τεχνολογίες που δημιουργήσαμε

00:13:41.720 --> 00:13:43.579
αρχικά για ελευθερία

00:13:43.579 --> 00:13:46.146
κατέληξαν να μας εξαναγκάζουν.

00:13:46.146 --> 00:13:50.937
Όμως, στο βιβλίο, μας προσφέρει 
επίσης έναν τρόπο να ξεφύγουμε

00:13:50.937 --> 00:13:54.375
από αυτή την κοινωνία,
παρόμοιο με το μονοπάτι

00:13:54.375 --> 00:13:58.330
που έπρεπε να ακολουθήσει ο Αδάμ 
και η Εύα για να φύγουν από τον κήπο.

00:13:58.330 --> 00:14:00.477
Σύμφωνα με τα λόγια του Άγριου,

00:14:00.477 --> 00:14:03.546
είναι δυνατή η επανάκτηση 
της αυτονομίας και της ελευθερίας,

00:14:03.546 --> 00:14:06.225
αλλά το τίμημα είναι υψηλό.

00:14:06.225 --> 00:14:11.940
Έτσι πιστεύω ότι μία από
τις καθοριστικές μάχες

00:14:11.940 --> 00:14:14.503
των καιρών μας θα είναι η μάχη

00:14:14.503 --> 00:14:16.890
για το έλεγχο των προσωπικών πληροφοριών,

00:14:16.890 --> 00:14:20.397
η μάχη για το αν τα μεγάλα δεδομένα
θα γίνουν μία δύναμη

00:14:20.397 --> 00:14:21.686
για ελευθερία,

00:14:21.686 --> 00:14:26.432
αντί για μία δύναμη που θα μας 
χειρίζεται στα κρυφά.

00:14:26.432 --> 00:14:29.025
Τώρα, πολλοί από εμάς

00:14:29.025 --> 00:14:31.778
δεν ξέρουν καν ότι γίνεται
αυτή η μάχη,

00:14:31.778 --> 00:14:34.450
αλλά γίνεται, είτε σας αρέσει, είτε όχι.

00:14:34.450 --> 00:14:37.254
Και με το ρίσκο του να παίξω τον όφι,

00:14:37.254 --> 00:14:40.151
θα σας πω ότι τα εργαλεία για τη μάχη

00:14:40.151 --> 00:14:43.160
είναι εδώ, η επίγνωση του τι συμβαίνει,

00:14:43.160 --> 00:14:44.515
στα χέρια σας,

00:14:44.515 --> 00:14:48.255
απλώς λίγα κλικ μακρυά.

00:14:48.255 --> 00:14:49.737
Σας ευχαριστώ.

00:14:49.737 --> 00:14:54.214
(Χειροκρότημα)

