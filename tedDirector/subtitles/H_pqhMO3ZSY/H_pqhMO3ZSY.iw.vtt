WEBVTT
Kind: captions
Language: iw

00:00:00.000 --> 00:00:07.000
מתרגם: Shlomo Adam
מבקר: Ido Dekkers

00:00:12.641 --> 00:00:14.995
ברצוני לספר לכם סיפור

00:00:14.995 --> 00:00:18.171
המקשר את תקרית הפרטיות
הידועה לשמצה

00:00:18.171 --> 00:00:20.940
בין אדם לחווה,

00:00:20.940 --> 00:00:24.386
עם התזוזה הבולטת בגבולות

00:00:24.386 --> 00:00:27.072
שבין הפרטי לציבורי, שהתרחשה

00:00:27.072 --> 00:00:28.842
ב-10 השנים האחרונות.

00:00:28.842 --> 00:00:30.140
התקרית ההיא מוכרת לכם.

00:00:30.140 --> 00:00:33.470
אדם וחווה,
יום אחד בגן-עדן,

00:00:33.470 --> 00:00:35.313
תופשים שהם עירומים.

00:00:35.313 --> 00:00:36.813
הם מתחרפנים.

00:00:36.813 --> 00:00:39.570
והשאר כתוב בדברי הימים.

00:00:39.570 --> 00:00:41.758
היום, אדם וחווה

00:00:41.758 --> 00:00:44.119
היו ודאי נוהגים אחרת.

00:00:44.119 --> 00:00:46.387
[@אדם אתמול בלילה היה פיצוץ!
היה אחלה תפוח:)))]

00:00:46.387 --> 00:00:48.260
[@חווה כן... מותק, יודעת איפה
המכנסיים שלי?]

00:00:48.260 --> 00:00:50.896
אנו חושפים מידע רב

00:00:50.896 --> 00:00:54.230
על עצמנו ברשת,
יותר מאי-פעם,

00:00:54.230 --> 00:00:55.934
והמון מידע אודותינו

00:00:55.934 --> 00:00:58.158
נאסף ע"י ארגונים.

00:00:58.158 --> 00:01:01.440
יש הרבה יתרונות ותועלת

00:01:01.440 --> 00:01:03.886
בניתוח המסיבי הזה
של מידע אישי,

00:01:03.886 --> 00:01:05.832
או צבירת הנתונים,

00:01:05.832 --> 00:01:08.470
אבל יש גם חסרונות מורכבים

00:01:08.470 --> 00:01:11.568
שנלווים לוויתור על פרטיותנו.
[הסוכנות לבטחון לאומי בין החברים ב"פייסבוק"]

00:01:11.568 --> 00:01:15.591
והסיפור שלי עוסק בחסרונות האלה.

00:01:15.591 --> 00:01:18.175
נפתח בהבחנה, שלדעתי,

00:01:18.175 --> 00:01:21.502
הפכה ברורה יותר ויותר
בשנים האחרונות,

00:01:21.502 --> 00:01:23.599
והיא שכל מידע אישי

00:01:23.599 --> 00:01:25.884
עלול להפוך למידע רגיש.

00:01:25.884 --> 00:01:30.009
בשנת 2000, כ-100 מיליארד תמונות

00:01:30.009 --> 00:01:31.921
צולמו בכל העולם,

00:01:31.921 --> 00:01:34.986
אבל רק חלקיק זעיר מהן

00:01:34.986 --> 00:01:36.869
הועלו לרשת.

00:01:36.869 --> 00:01:40.230
ב-2010, רק ב"פייסבוק"
ורק בחודש אחד,

00:01:40.230 --> 00:01:43.500
הועלו 2.5 מיליארד צילומים,

00:01:43.500 --> 00:01:45.382
רובם מזוהים.

00:01:45.382 --> 00:01:47.262
באותה תקופה,

00:01:47.262 --> 00:01:52.132
יכולת המחשבים לזהות אנשים בצילומים

00:01:52.132 --> 00:01:55.740
השתפרה בשלושה סדרי-גודל.

00:01:55.740 --> 00:01:57.622
מה קורה כשמשלבים

00:01:57.622 --> 00:01:59.123
בין שתי הטכנולוגיות האלה:

00:01:59.123 --> 00:02:01.781
עליה בזמינות נתוני הפנים

00:02:01.781 --> 00:02:05.429
ושיפור יכולת זיהוי הפנים
ע"י מחשבים,

00:02:05.429 --> 00:02:07.611
אבל גם מיחשוב הענן

00:02:07.611 --> 00:02:09.499
שמעניק לכל אדם בזירה הזו

00:02:09.499 --> 00:02:11.059
כוח מיחשוב מהסוג

00:02:11.059 --> 00:02:12.945
שלפני שנים ספורות היה
שייך בלעדית

00:02:12.945 --> 00:02:14.727
לסוכנויות ששמן מורכב מ-3 אותיות,

00:02:14.727 --> 00:02:16.105
והמיחשוב שנמצא בכל,

00:02:16.105 --> 00:02:18.997
שמאפשר לטלפון שלי,
שאיננו מחשב-על,

00:02:18.997 --> 00:02:20.668
להתחבר לאינטרנט

00:02:20.668 --> 00:02:23.002
ולבצע שם מאות אלפי

00:02:23.002 --> 00:02:25.641
מדידות פנים בכמה שניות?

00:02:25.641 --> 00:02:28.269
ובכן, אנו משערים שהתוצאה

00:02:28.269 --> 00:02:30.333
של שילוב הטכנולוגיות הזה

00:02:30.333 --> 00:02:33.221
תהיה שינוי קיצוני
במושגים שלנו עצמם

00:02:33.221 --> 00:02:35.478
בקשר לפרטיות ואלמוניות.

00:02:35.478 --> 00:02:37.471
כדי לבחון זאת, ערכנו ניסוי

00:02:37.471 --> 00:02:39.592
בקמפוס של אוניברסיטת
קרנגי מלון.

00:02:39.592 --> 00:02:41.691
ביקשנו מסטודנטים עוברים ושבים

00:02:41.691 --> 00:02:43.470
להשתתף במחקר מסוים,

00:02:43.470 --> 00:02:46.032
צילמנו אותם במצלמת רשת,

00:02:46.032 --> 00:02:48.814
וביקשנו מהם למלא טופס
של סקר במחשב אישי.

00:02:48.814 --> 00:02:50.793
ובזמן שהם מילאו את הטופס,

00:02:50.793 --> 00:02:53.590
העלינו את תמונתם למיחשוב הענן,

00:02:53.590 --> 00:02:55.317
והתחלנו להשתמש בתוכנת זיהוי פנים

00:02:55.317 --> 00:02:57.722
כדי להתאים את התצלום
לבסיס נתונים

00:02:57.722 --> 00:03:00.115
של כמה מאות אלפי תמונות

00:03:00.115 --> 00:03:03.711
שהורדנו מפרופילים ב"פייסבוק".

00:03:03.711 --> 00:03:06.970
עד שהנבדק סיים למלא
את העמוד האחרון בטופס הסקר,

00:03:06.970 --> 00:03:10.317
הדף התעדכן באופן דינמי

00:03:10.317 --> 00:03:12.630
עם 10 התמונות התואמות ביותר

00:03:12.630 --> 00:03:14.915
שתוכנת הזיהוי מצאה,

00:03:14.915 --> 00:03:16.653
ואז ביקשנו מהנבדקים לציין

00:03:16.653 --> 00:03:20.773
אם הוא או היא
מצאו את עצמם ביניהם.

00:03:20.773 --> 00:03:24.472
אתם מזהים את הנבדק?

00:03:24.472 --> 00:03:27.317
המחשב בהחלט זיהה,
ולמעשה הצליח בכך

00:03:27.317 --> 00:03:29.466
עם אחד מכל שלושה נבדקים.

00:03:29.466 --> 00:03:32.650
כך שבעצם, אנו יכולים להתחיל
מפרצוף אלמוני,

00:03:32.650 --> 00:03:36.134
מקוון או לא מקוון,
ולהשתמש בזיהוי פנים

00:03:36.134 --> 00:03:38.494
כדי להצמיד שם
לאותו פרצוף אלמוני

00:03:38.494 --> 00:03:40.602
הודות לנתוני המדיה החברתית.

00:03:40.602 --> 00:03:42.474
אך לפני כמה שנים
עשינו משהו אחר.

00:03:42.474 --> 00:03:44.297
התחלנו מנתוני המדיה החברתית,

00:03:44.297 --> 00:03:47.348
שילבנו אותם באופן סטטיסטי
עם נתונים

00:03:47.348 --> 00:03:49.450
מהביטוח הלאומי של ממשלת ארה"ב,

00:03:49.450 --> 00:03:52.774
והצלחנו לגלות מראש
מספרי ביטוח לאומי,

00:03:52.774 --> 00:03:54.286
ובארה"ב,

00:03:54.286 --> 00:03:56.326
זהו מידע רגיש ביותר.

00:03:56.326 --> 00:03:58.419
אתם מבינים מה אני אומר כאן?

00:03:58.419 --> 00:04:01.341
אם משלבים בין שני המחקרים,

00:04:01.341 --> 00:04:02.853
אז השאלה היא,

00:04:02.853 --> 00:04:05.573
האם אפשר להתחיל מפרצוף,

00:04:05.573 --> 00:04:07.884
ובעזרת זיהוי פנים, למצוא שם

00:04:07.884 --> 00:04:10.553
ומידע זמין לציבור

00:04:10.553 --> 00:04:12.485
על השם והאדם האלה,

00:04:12.485 --> 00:04:14.733
ומתוך אותו מידע זמין לציבור,

00:04:14.733 --> 00:04:16.775
להפיק מידע שאיננו זמין לציבור,

00:04:16.775 --> 00:04:18.381
מידע הרבה יותר רגיש

00:04:18.381 --> 00:04:19.873
שניתן לקשרו עם אותו פרצוף?

00:04:19.873 --> 00:04:21.789
והתשובה היא כן, אנו יכולים,
ואנו עשינו זאת.

00:04:21.789 --> 00:04:24.357
כמובן, שהדיוק הולך ויורד.

00:04:24.357 --> 00:04:25.301
[זוהו 27% ממספרי הביטוח הלאומי
של הנבדקים (ב-4 נסיונות)]

00:04:25.301 --> 00:04:29.128
אך למעשה החלטנו אפילו
לפתח יישומון ל"אייפון"

00:04:29.128 --> 00:04:31.843
שמנצל את המצלמה
הפנימית של הטלפון

00:04:31.843 --> 00:04:33.443
כדי לצלם את הנבדק,

00:04:33.443 --> 00:04:34.930
להעלות את זה לענן

00:04:34.930 --> 00:04:37.592
ואז לעשות את מה
שזה עתה תיארתי, בזמן אמת:

00:04:37.592 --> 00:04:39.680
לחפש התאמה, לאתר מידע גלוי,

00:04:39.680 --> 00:04:41.410
לנסות להסיק ממנו מידע רגיש,

00:04:41.410 --> 00:04:44.001
ולשלוח את זה בחזרה
אל הטלפון

00:04:44.001 --> 00:04:47.610
כדי לרבד את זה
על פרצופו של הנבדק,

00:04:47.610 --> 00:04:49.511
כדוגמה של מציאות מורחבת,

00:04:49.511 --> 00:04:51.962
אולי דוגמה חולנית
של מציאות מורחבת.

00:04:51.962 --> 00:04:55.301
לא פיתחנו את היישומון
כדי שיהיה זמין לציבור,

00:04:55.301 --> 00:04:57.223
אלא רק כדי להוכיח רעיון.

00:04:57.223 --> 00:04:59.536
בעצם, כדי לקחת את
הטכנולוגיות האלה

00:04:59.536 --> 00:05:01.373
עד לקיצוניות הלוגית שלהן.

00:05:01.373 --> 00:05:04.092
תארו לעצמכן עתיד
שבו אנשים זרים סביבכם

00:05:04.092 --> 00:05:06.403
יביטו בכם דרך
משקפי ה"גוגל" שלהם

00:05:06.403 --> 00:05:08.710
או, יום אחד,
דרך עדשות המגע שלהם,

00:05:08.710 --> 00:05:12.730
וישתמשו ב-7 או 8
נקודות מידע אודותיכם

00:05:12.730 --> 00:05:15.312
כדי להסיק את כל יתר המידע

00:05:15.312 --> 00:05:17.915
שאפשר למצוא אודותיכם.

00:05:17.915 --> 00:05:22.709
איך ייראה עתיד ללא סודות?

00:05:22.709 --> 00:05:24.673
והאם זה צריך להיות
חשוב לנו?

00:05:24.673 --> 00:05:26.564
אולי אנו אוהבים להאמין

00:05:26.564 --> 00:05:29.604
שעתיד שיש בו
עושר כזה של נתונים

00:05:29.604 --> 00:05:32.118
יהיה עתיד שבו כבר
לא יהיו דעות מוטות,

00:05:32.118 --> 00:05:35.701
אך למעשה,
זמינותו של מידע כה רב

00:05:35.701 --> 00:05:37.892
אינה אומרת שאנו נקבל החלטות

00:05:37.892 --> 00:05:39.598
אובייקטיביות יותר.

00:05:39.598 --> 00:05:42.158
בניסוי אחר הצגנו לנבדקים שלנו

00:05:42.158 --> 00:05:44.404
מידע אודות מועמד פוטנציאלי
למשרה כלשהי.

00:05:44.404 --> 00:05:47.582
כללנו במידע זה כמה התייחסויות

00:05:47.582 --> 00:05:50.228
למידע קצת משעשע,
חוקי לגמרי,

00:05:50.228 --> 00:05:52.693
אך אולי מעט מביך

00:05:52.693 --> 00:05:54.713
שהנבדק העלה לרשת.

00:05:54.713 --> 00:05:57.079
המעניין הוא
שבין הנבדקים שלנו,

00:05:57.079 --> 00:06:00.162
אחדים העלו מידע דומה,

00:06:00.162 --> 00:06:02.524
ואחרים לא.

00:06:02.524 --> 00:06:04.473
איזו קבוצה לדעתכם

00:06:04.473 --> 00:06:09.025
נטתה לשפוט בחומרה
את הנבדק שלנו?

00:06:09.025 --> 00:06:10.982
באופן פרדוקסלי, היתה זו הקבוצה

00:06:10.982 --> 00:06:12.715
שהעלתה לרשת מידע דומה,

00:06:12.715 --> 00:06:15.657
דוגמה לעיוות מוסרי.

00:06:15.657 --> 00:06:17.407
אולי תחשבו:

00:06:17.407 --> 00:06:19.109
"זה לא נוגע לי,

00:06:19.109 --> 00:06:21.271
"כי אין לי דבר להסתיר."

00:06:21.271 --> 00:06:23.753
אך האמת היא שפרטיות איננה

00:06:23.753 --> 00:06:27.429
הסתרה של משהו שלילי.

00:06:27.429 --> 00:06:29.783
תארו לעצמכם שאתם
מנהלי משאבי-אנוש

00:06:29.783 --> 00:06:32.730
בארגון כלשהו,
ואתם מקבלים קורות חיים

00:06:32.730 --> 00:06:35.203
ומחליטים לחפש מידע נוסף
אודות המועמדים שלכם.

00:06:35.203 --> 00:06:37.663
אז אתם "מגגלים" את שמותיהם

00:06:37.663 --> 00:06:39.903
ובמציאות מסוימת,

00:06:39.903 --> 00:06:41.911
אתם מגלים את המידע הזה.

00:06:41.911 --> 00:06:46.348
או במציאות מקבילה אחרת,
את המידע הזה.

00:06:46.348 --> 00:06:49.065
האם נראה לכם סביר
שתזמינו באופן שוויוני

00:06:49.065 --> 00:06:51.868
את המועמדות האלה לראיון?

00:06:51.868 --> 00:06:54.150
אם אתם חושבים שכן,
הרי שאינכם

00:06:54.150 --> 00:06:56.732
כמו המעסיקים בארה"ב, שלמעשה,

00:06:56.732 --> 00:07:00.039
משתתפים בניסוי שלנו, כלומר:
זה בדיוק מה שעשינו.

00:07:00.039 --> 00:07:03.221
יצרנו פרופילי "פייסבוק",
המצאנו תכונות אופי,

00:07:03.221 --> 00:07:06.072
ואז התחלנו לשלוח קורות חיים
לחברות בארה"ב,

00:07:06.072 --> 00:07:07.980
וגילינו, עקבנו,

00:07:07.980 --> 00:07:10.373
אם הם חיפשו מידע
אודות מועמדינו,

00:07:10.373 --> 00:07:12.205
ואם הם פעלו על סמך המידע

00:07:12.205 --> 00:07:14.143
שמצאו במדיה החברתית.
והם בהחלט כן.

00:07:14.143 --> 00:07:16.244
התרחשה אפליה בכל המדיה החברתית

00:07:16.244 --> 00:07:19.317
לגבי מועמדים זהים מבחינת כישורים.

00:07:19.317 --> 00:07:23.892
המשווקים רוצים שנאמין

00:07:23.892 --> 00:07:26.161
שכל המידע עלינו ישמש תמיד

00:07:26.161 --> 00:07:29.434
לתועלתנו.

00:07:29.434 --> 00:07:33.149
אבל אם תחשבו על זה,
למה שזה תמיד יהיה כך?

00:07:33.149 --> 00:07:35.813
בסרט שיצא לפני כמה שנים,

00:07:35.813 --> 00:07:38.366
"דוח מיוחד", בסצנה מפורסמת

00:07:38.366 --> 00:07:40.942
רואים את טום קרוז עובר בקניון

00:07:40.942 --> 00:07:44.718
ופרסום הולוגרפי מותאם-אישית

00:07:44.718 --> 00:07:46.553
מופיע סביבו.

00:07:46.553 --> 00:07:49.780
הסרט מתרחש ב-2054,

00:07:49.780 --> 00:07:51.422
כ-40 שנה מהיום,

00:07:51.422 --> 00:07:54.330
וככל שהטכנולוגיה הזו
נראית מלהיבה,

00:07:54.330 --> 00:07:56.976
היא כבר ממעיטה מאד באומדן

00:07:56.976 --> 00:07:59.116
כמות המידע שארגונים

00:07:59.116 --> 00:08:01.599
יכולים לאסוף אודותיכם,
וכיצד הם יכולים להשתמש בו

00:08:01.599 --> 00:08:04.997
כדי להשפיע עליכם בדרך
שאפילו לא תגלו.

00:08:04.997 --> 00:08:07.100
בתור דוגמה, זהו ניסוי נוסף

00:08:07.100 --> 00:08:09.373
שאנו עורכים בימים אלה,
הוא טרם הושלם.

00:08:09.373 --> 00:08:11.692
תארו לעצמכם שלארגון כלשהו
יש גישה

00:08:11.692 --> 00:08:13.748
לרשימת חבריכם ב"פייסבוק",

00:08:13.748 --> 00:08:15.520
ובאמצעות אלגוריתם מסוים

00:08:15.520 --> 00:08:19.254
הוא יכול לזהות את שני החברים
הכי אהובים שלכם.

00:08:19.254 --> 00:08:21.534
ואז הוא יוצר, בזמן אמת,

00:08:21.534 --> 00:08:24.376
קלסתרון משולב של
שני החברים האלה.

00:08:24.376 --> 00:08:27.445
מחקרים שנערכו לפנינו הראו שאנשים

00:08:27.445 --> 00:08:30.330
כבר לא מזהים אפילו את עצמם

00:08:30.330 --> 00:08:32.792
בקלסתרונים, אבל הם מגיבים

00:08:32.792 --> 00:08:34.909
לקלסתרונים אלה באופן חיובי.

00:08:34.909 --> 00:08:38.324
כך שבפעם הבאה
שתחפשו מוצר כלשהו,

00:08:38.324 --> 00:08:40.883
ותהיה איזו פרסומת שתציע לכם
לקנות אותו,

00:08:40.883 --> 00:08:43.790
המציג לא יהיה דובר רגיל,

00:08:43.790 --> 00:08:46.103
אלא אחד מחבריכם,

00:08:46.103 --> 00:08:49.406
ואתם אפילו לא תדעו
שזה מה שקורה.

00:08:49.406 --> 00:08:51.819
הבעיה היא,

00:08:51.819 --> 00:08:54.338
שמנגנוני המדיניות הנוכחיים שלנו

00:08:54.338 --> 00:08:57.776
שמטרתם להגן על עצמנו
מניצול לרעה של מידע אישי

00:08:57.776 --> 00:09:00.760
הם כמו להשתמש בסכין
בקרב אקדחים.

00:09:00.760 --> 00:09:03.673
אחד המנגנונים האלה הוא שקיפות,

00:09:03.673 --> 00:09:06.873
לומר לאנשים מה תעשה
עם המידע אודותיהם,

00:09:06.873 --> 00:09:08.979
ועקרונית, זה טוב מאד.

00:09:08.979 --> 00:09:12.646
זה הכרחי,
אבל זה לא מספיק.

00:09:12.646 --> 00:09:16.344
אפשר להשתמש בשקיפות באופן מטעה.

00:09:16.344 --> 00:09:18.448
אפשר לומר לאנשים
מה עומדים לעשות,

00:09:18.448 --> 00:09:20.680
ולהמשיך להציק להם בדרישות למסור

00:09:20.680 --> 00:09:23.303
כמויות שרירותיות של מידע אישי.

00:09:23.303 --> 00:09:26.189
אז בניסוי נוסף,
הפעם בהשתתפות סטודנטים,

00:09:26.189 --> 00:09:29.247
ביקשנו מהם לספק מידע

00:09:29.247 --> 00:09:31.060
לגבי התנהלותם בקמפוס,

00:09:31.060 --> 00:09:34.000
כולל שאלות רגישות למדי,
כמו זאת.

00:09:34.000 --> 00:09:34.621
[האם אי-פעם רימית במבחן?]
לקבוצה אחת של נבדקים אמרנו:

00:09:34.621 --> 00:09:36.921
[האם אי-פעם רימית במבחן?]
לקבוצה אחת של נבדקים אמרנו:

00:09:36.921 --> 00:09:39.762
"רק סטודנטים אחרים
יראו את תשובותיכם."

00:09:39.762 --> 00:09:41.341
לקבוצת נבדקים שניה אמרנו:

00:09:41.341 --> 00:09:44.902
"הסטודנטים והסגל יראו
את תשובותיכם."

00:09:44.902 --> 00:09:47.493
שקיפות. הודעה מראש.
וזה בהחלט עבד,

00:09:47.493 --> 00:09:48.900
במובן זה שקבוצת הנבדקים הראשונה

00:09:48.900 --> 00:09:51.468
נטתה הרבה יותר למסור מידע
מאשר השניה.

00:09:51.468 --> 00:09:52.988
הגיוני, נכון?

00:09:52.988 --> 00:09:54.478
אבל הוספנו הטעיה.

00:09:54.478 --> 00:09:57.238
חזרנו על הניסוי
עם אותן שתי קבוצות,

00:09:57.238 --> 00:09:59.665
והפעם הוספנו השהיה

00:09:59.665 --> 00:10:02.600
בין הזמן בו הודענו לנבדקים

00:10:02.600 --> 00:10:04.680
איך בדעתנו להשתמש בנתונים

00:10:04.680 --> 00:10:09.068
ובין הזמן שבו הם החלו בפועל
לענות על השאלות.

00:10:09.068 --> 00:10:11.629
מה אורך ההשהייה, לדעתכם,
היה עלינו להכניס

00:10:11.629 --> 00:10:16.242
כדי לבטל את האפקט המעכב

00:10:16.242 --> 00:10:19.653
שבידיעה שהסגל
עתיד לראות את התשובות?

00:10:19.653 --> 00:10:21.433
10 דקות?

00:10:21.433 --> 00:10:23.224
5 דקות?

00:10:23.224 --> 00:10:25.000
דקה אחת?

00:10:25.000 --> 00:10:27.049
מה תאמרו על 15 שניות?

00:10:27.049 --> 00:10:29.717
די היה ב-15 שניות
כדי ששתי הקבוצות

00:10:29.717 --> 00:10:31.285
תמסורנה את אותה כמות מידע,

00:10:31.285 --> 00:10:34.031
כאילו שלחברי הקבוצה השניה
כבר לא היה איכפת

00:10:34.031 --> 00:10:36.687
אם הסגל יראה את תשובותיהם.

00:10:36.687 --> 00:10:40.023
עלי להודות שההרצאה הזו
נראית עד עתה

00:10:40.023 --> 00:10:42.503
קודרת ביותר,

00:10:42.503 --> 00:10:44.224
אבל לא זה המסר שלי.

00:10:44.224 --> 00:10:46.923
למעשה, אני רוצה לגלות לכם
את העובדה

00:10:46.923 --> 00:10:48.695
שיש חלופות.

00:10:48.695 --> 00:10:51.194
הדרך בה אנו עושים דברים כיום
אינה הדרך היחידה

00:10:51.194 --> 00:10:54.231
בה ניתן לעשותם, ואין ספק
שהיא לא הדרך הכי טובה

00:10:54.231 --> 00:10:56.258
בה ניתן לעשותם.

00:10:56.258 --> 00:11:00.429
כשמישהו אומר לכם:
"לאנשים לא איכפת מפרטיות",

00:11:00.429 --> 00:11:03.071
שאלו את עצמכם
האם המשחק לא תוכנן וטופל מראש

00:11:03.071 --> 00:11:05.795
כך שלא יהיה להם איכפת
לגבי הפרטיות,

00:11:05.795 --> 00:11:09.057
וההבנה שנעשות מניפולציות כאלה

00:11:09.057 --> 00:11:10.664
היא כבר חצי מהפתרון

00:11:10.664 --> 00:11:12.922
של היכולת להגן על עצמכם.

00:11:12.922 --> 00:11:16.632
כשמישהו אומר לכם
שהפרטיות אינה עולה בקנה אחד

00:11:16.632 --> 00:11:18.481
עם התועלת שבצבירת נתונים,

00:11:18.481 --> 00:11:20.954
חישבו על כך
שב-20 השנה האחרונות,

00:11:20.954 --> 00:11:22.871
חוקרים יצרו טכנולוגיות

00:11:22.871 --> 00:11:26.189
שתאפשרנה לבצע למעשה
כל עיסקה אלקטרונית

00:11:26.189 --> 00:11:29.938
באופן שמגן יותר על הפרטיות.

00:11:29.938 --> 00:11:32.493
אנו יכולים לגלוש באינטרנט באלמוניות.

00:11:32.493 --> 00:11:35.171
אנו יכולים לשלוח דוא"ל
שיכול להיקרא

00:11:35.171 --> 00:11:38.880
רק ע"י הנמען, אפילו לא
ע"י הסוכנות לבטחון לאומי.

00:11:38.880 --> 00:11:41.877
אנו יכולים לבצע אפילו
כריית-נתונים מכבדת פרטיות.

00:11:41.877 --> 00:11:45.771
במלים אחרות, אנו יכולים לקבל
את יתרונות צבירת הנתונים

00:11:45.771 --> 00:11:47.903
תוך שמירה על הפרטיות.

00:11:47.903 --> 00:11:51.694
מובן שמהטכנולוגיות האלה
משתמע שינוי

00:11:51.694 --> 00:11:53.240
מבחינת עלות והכנסות

00:11:53.240 --> 00:11:55.347
בין בעלי הנתונים
לבין נושאי המידע,

00:11:55.347 --> 00:11:58.800
ואולי בגלל זה
לא מרבים לשמוע עליהן.

00:11:58.800 --> 00:12:02.506
וזה מחזיר אותי לגן-עדן.

00:12:02.506 --> 00:12:05.286
יש פרשנות נוספת, שקשורה בפרטיות,

00:12:05.286 --> 00:12:07.095
לסיפור גן העדן

00:12:07.095 --> 00:12:09.191
שלא בהכרח מתעסק עם הסוגיה

00:12:09.191 --> 00:12:11.416
של עירומם של אדם וחווה

00:12:11.416 --> 00:12:13.797
ותחושת הבושה שלהם.

00:12:13.797 --> 00:12:16.578
אפשר למצוא הדים לפרשנות זו

00:12:16.578 --> 00:12:19.360
ב"גן העדן האבוד"
של ג'ון מילטון.

00:12:19.360 --> 00:12:23.557
בגן העדן, אדם וחווה מסופקים
מבחינה חומרית.

00:12:23.557 --> 00:12:25.661
הם מאושרים. הם שבעי-רצון.

00:12:25.661 --> 00:12:27.954
אבל גם אין להם ידע

00:12:27.954 --> 00:12:29.594
ומודעות עצמית.

00:12:29.594 --> 00:12:32.913
ברגע שהם אוכלים מהפרי
בעל השם ההולם,

00:12:32.913 --> 00:12:34.206
פרי עץ הדעת,

00:12:34.206 --> 00:12:36.811
הם מגלים את עצמם.

00:12:36.811 --> 00:12:40.842
הם נעשים מודעים.
הם רוכשים עצמאות.

00:12:40.842 --> 00:12:43.968
אבל המחיר שעליהם לשלם
הוא עזיבת גן העדן.

00:12:43.968 --> 00:12:47.849
כך שהפרטיות, במובן מסוים,
היא גם האמצעי

00:12:47.849 --> 00:12:50.811
וגם המחיר של החופש.

00:12:50.811 --> 00:12:53.581
שוב, המשווקים מספרים לנו

00:12:53.581 --> 00:12:56.600
שצבירת נתונים והמידה החברתית

00:12:56.600 --> 00:12:59.579
הם לא רק
גן-עדן של רווחים עבורם,

00:12:59.579 --> 00:13:02.036
אלא גן-עדן עבור כולנו.

00:13:02.036 --> 00:13:03.274
אנו מקבלים תוכן בחינם.

00:13:03.274 --> 00:13:06.397
אנו זוכים לשחק "אנגרי בירדס",
אנו מקבלים יישומונים ייעודיים.

00:13:06.397 --> 00:13:09.294
אך האמת היא
שתוך שנים ספורות, הארגונים

00:13:09.294 --> 00:13:10.903
ידעו כל-כך הרבה עלינו,

00:13:10.903 --> 00:13:13.613
שהם יוכלו להסיק מהם רצונותינו

00:13:13.613 --> 00:13:15.817
עוד לפני שאנו נדע מהם, ואולי

00:13:15.817 --> 00:13:18.264
לרכוש מוצרים בשמנו

00:13:18.264 --> 00:13:20.538
עוד לפני שנדע
שאנו זקוקים להם.

00:13:20.538 --> 00:13:23.775
היה סופר אנגלי,

00:13:23.775 --> 00:13:26.820
שצפה עתיד מעין זה,

00:13:26.820 --> 00:13:28.225
שבו נוכל לסחור

00:13:28.225 --> 00:13:31.773
בעצמאותנו ובחירותנו תמורת נוחות.

00:13:31.773 --> 00:13:33.934
יותר מכפי שחזה ג'ורג' אורוול,

00:13:33.934 --> 00:13:36.695
הסופר הוא כמובן אלדוס הקסלי.

00:13:36.695 --> 00:13:39.549
ב"עולם חדש מופלא"
הוא תיאר חברה

00:13:39.549 --> 00:13:41.720
שבה הטכנולוגיות שיצרנו

00:13:41.720 --> 00:13:43.579
במקור למען החופש,

00:13:43.579 --> 00:13:46.146
בסופו של דבר מדכאות אותנו.

00:13:46.146 --> 00:13:50.937
אבל באותו ספר הוא גם
הציע דרך מוצא

00:13:50.937 --> 00:13:54.375
מאותה חברה, בדומה לדרך

00:13:54.375 --> 00:13:58.330
שאדם וחווה נאלצו ללכת בה
כדי לעזוב את הגן.

00:13:58.330 --> 00:14:00.477
כפי שאומר הפרא, מתוך הספר,

00:14:00.477 --> 00:14:03.546
השבת העצמאות והחירות היא אפשרית,

00:14:03.546 --> 00:14:06.225
אם כי המחיר שיש לשלם
הוא גבוה.

00:14:06.225 --> 00:14:11.940
כך שאני אכן מאמין
שאחד הקרבות

00:14:11.940 --> 00:14:14.503
שיגדירו את תקופתנו יהיה הקרב

00:14:14.503 --> 00:14:16.890
על השליטה במידע האישי שלנו,

00:14:16.890 --> 00:14:20.397
הקרב שיקבע אם צבירת הנתונים
תהפוך לכוח

00:14:20.397 --> 00:14:21.686
למען החירות,

00:14:21.686 --> 00:14:26.432
ולא לכוח שיפעיל עלינו
מניפולציות סמויות.

00:14:26.432 --> 00:14:29.025
לעת עתה, רבים מאיתנו

00:14:29.025 --> 00:14:31.778
אפילו לא יודעים
שהקרב הזה בכלל מתחולל,

00:14:31.778 --> 00:14:34.450
אך הוא בהחלט ניטש,
אם זה מוצא חן בעיניכם ואם לא.

00:14:34.450 --> 00:14:37.254
ומתוך הסתכנות בגילום דמות הנחש,

00:14:37.254 --> 00:14:40.151
אגלה לכם שהכלים לניהול הקרב

00:14:40.151 --> 00:14:43.160
הם כאן - המודעות למה שקורה,

00:14:43.160 --> 00:14:44.515
וכאן, בקצות אצבעותיכם.

00:14:44.515 --> 00:14:48.255
נחוצות רק כמה הקשות מקש.

00:14:48.255 --> 00:14:49.737
תודה לכם.

00:14:49.737 --> 00:14:54.214
[מחיאות כפיים]

