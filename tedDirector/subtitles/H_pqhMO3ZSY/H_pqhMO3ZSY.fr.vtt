WEBVTT
Kind: captions
Language: fr

00:00:00.000 --> 00:00:07.000
Traducteur: Pierre Granchamp
Relecteur: Nhu PHAM

00:00:12.641 --> 00:00:14.995
Je voudrais vous raconter une histoire

00:00:14.995 --> 00:00:18.171
qui fait le lien
entre le célèbre incident qui s'est produit

00:00:18.171 --> 00:00:20.940
dans la vie privée d'Adam et Eve,

00:00:20.940 --> 00:00:24.386
et le remarquable
déplacement de la frontière

00:00:24.386 --> 00:00:27.072
entre vie publique et vie privée
qui s'est produit

00:00:27.072 --> 00:00:28.842
dans les 10 dernières années.

00:00:28.842 --> 00:00:30.140
Vous connaissez l'incident.

00:00:30.140 --> 00:00:33.470
Adam et Eve, un jour,
au Jardin d'Eden,

00:00:33.470 --> 00:00:35.313
réalisent qu'ils sont nus.

00:00:35.313 --> 00:00:36.813
Ils paniquent.

00:00:36.813 --> 00:00:39.570
On connait la suite.

00:00:39.570 --> 00:00:41.758
De nos jours, Adam et Eve

00:00:41.758 --> 00:00:44.119
se comporteraient sans doute
différemment.

00:00:44.119 --> 00:00:46.387
[@Adam Cette nuit était trop top ! Adoré 7 pom LOL]

00:00:46.387 --> 00:00:48.260
[@Eve Ok...Baby, T sais ce ki est arrivé à mon pantalon ?

00:00:48.260 --> 00:00:50.896
Nous révélons tellement plus d'informations

00:00:50.896 --> 00:00:54.230
sur nous-mêmes, en ligne,
que jamais auparavant,

00:00:54.230 --> 00:00:55.934
et tant d'informations
qui nous concernent

00:00:55.934 --> 00:00:58.158
sont collectées
par des organisations.

00:00:58.158 --> 00:01:01.440
Nous avons certes
beaucoup à gagner et à bénéficier

00:01:01.440 --> 00:01:03.886
de cette analyse géante
d'informations personnelles,

00:01:03.886 --> 00:01:05.832
ou Big Data,

00:01:05.832 --> 00:01:08.470
mais il y a aussi
des contreparties complexes

00:01:08.470 --> 00:01:11.568
à abandonner notre vie privée.

00:01:11.568 --> 00:01:15.591
Mon histoire
parle de ces contreparties.

00:01:15.591 --> 00:01:18.175
Commençons par une observation qui,
dans mon esprit,

00:01:18.175 --> 00:01:21.502
est devenue de plus en plus claire
ces dernières années :

00:01:21.502 --> 00:01:23.599
toute information personnelle

00:01:23.599 --> 00:01:25.884
peut devenir une information sensible.

00:01:25.884 --> 00:01:30.009
En 2000,
environ 100 milliards de photos

00:01:30.009 --> 00:01:31.921
ont été prises dans le monde,

00:01:31.921 --> 00:01:34.986
mais seule une infime proportion
de celles-ci

00:01:34.986 --> 00:01:36.869
ont été mises en ligne.

00:01:36.869 --> 00:01:40.230
En 2010, rien que sur Facebook,
en un seul mois,

00:01:40.230 --> 00:01:43.500
2,5 milliards de photos
ont été mises en ligne,

00:01:43.500 --> 00:01:45.382
la plupart identifiées.

00:01:45.382 --> 00:01:47.262
Pendant ce temps,

00:01:47.262 --> 00:01:52.132
la capacité des ordinateurs
à reconnaître des personnes sur photo

00:01:52.132 --> 00:01:55.740
s'est améliorée de trois ordres de grandeur.

00:01:55.740 --> 00:01:57.622
Que se passe-t-il quand on combine

00:01:57.622 --> 00:01:59.123
ces technologies ?

00:01:59.123 --> 00:02:01.781
Disponibilité croissante
des données faciales ;

00:02:01.781 --> 00:02:05.429
capacité améliorée de reconnaissance faciale
par les ordinateurs ;

00:02:05.429 --> 00:02:07.611
mais aussi le cloud computing,

00:02:07.611 --> 00:02:09.499
qui donne à chacun de nous
dans cette salle

00:02:09.499 --> 00:02:11.059
la puissance de calcul

00:02:11.059 --> 00:02:12.945
qui, il y a quelques années à peine,
était du registre

00:02:12.945 --> 00:02:14.727
des services spéciaux ;

00:02:14.727 --> 00:02:16.105
et l'informatique omniprésente,

00:02:16.105 --> 00:02:18.997
qui permet à mon téléphone,
pourtant pas un super-ordinateur,

00:02:18.997 --> 00:02:20.668
de se connecter à Internet

00:02:20.668 --> 00:02:23.002
et d'y réaliser des centaines de milliers

00:02:23.002 --> 00:02:25.641
de mesures faciales
en quelques secondes.

00:02:25.641 --> 00:02:28.269
Et bien, nous émettons l'hypothèse

00:02:28.269 --> 00:02:30.333
que le résultat
de cette combinaison de technologies

00:02:30.333 --> 00:02:33.221
sera un changement radical
de nos conceptions mêmes

00:02:33.221 --> 00:02:35.478
de la vie privée et de l'anonymat.

00:02:35.478 --> 00:02:37.471
Pour tester cela,
on a réalisé une expérience

00:02:37.471 --> 00:02:39.592
sur le campus
de l'Université Carnegie Mellon.

00:02:39.592 --> 00:02:41.691
On a demandé
à des étudiants qui passaient

00:02:41.691 --> 00:02:43.470
de participer à une étude,

00:02:43.470 --> 00:02:46.032
on a pris une photo d'eux
avec une webcam,

00:02:46.032 --> 00:02:48.814
et on leur a demandé de répondre à un sondage
sur un PC portable.

00:02:48.814 --> 00:02:50.793
Pendant qu'ils répondaient au sondage,

00:02:50.793 --> 00:02:53.590
on a téléchargé leur photo
sur un groupe de cloud-computing,

00:02:53.590 --> 00:02:55.317
et on a utilisé
un outil de reconnaissance faciale

00:02:55.317 --> 00:02:57.722
pour faire correspondre cette photo
à une base de données

00:02:57.722 --> 00:03:00.115
de centaines de milliers d'images,

00:03:00.115 --> 00:03:03.711
qu'on avait téléchargée
depuis des profils Facebook.

00:03:03.711 --> 00:03:06.970
Avant que le sujet
n'ait atteint la dernière page du sondage,

00:03:06.970 --> 00:03:10.317
celle-ci avait été mise à jour
de façon dynamique

00:03:10.317 --> 00:03:12.630
avec les 10 meilleures photos correspondantes

00:03:12.630 --> 00:03:14.915
que l'outil de reconnaissance avait trouvées,

00:03:14.915 --> 00:03:16.653
et on a demandé aux sujets d'indiquer

00:03:16.653 --> 00:03:20.773
si ils ou elles s'étaient reconnus sur la photo.

00:03:20.773 --> 00:03:24.472
Vous voyez le sujet ?

00:03:24.472 --> 00:03:27.317
Et bien, l'ordinateur l'avait vu,
et en fait, il l'a vu

00:03:27.317 --> 00:03:29.466
chez un sujet sur trois.

00:03:29.466 --> 00:03:32.650
On peut donc partir d'un visage anonyme,

00:03:32.650 --> 00:03:36.134
en ligne ou hors ligne,
et on peut utiliser la reconnaissance faciale

00:03:36.134 --> 00:03:38.494
pour donner un nom à ce visage anonyme

00:03:38.494 --> 00:03:40.602
grâce aux données des réseaux sociaux.

00:03:40.602 --> 00:03:42.474
Mais quelques années auparavant,
on a fait quelque chose d'autre.

00:03:42.474 --> 00:03:44.297
A partir des données des réseaux sociaux,

00:03:44.297 --> 00:03:47.348
combinées statistiquement avec les données

00:03:47.348 --> 00:03:49.450
de la Sécurité Sociale
du gouvernement américain,

00:03:49.450 --> 00:03:52.774
on a réussi à déduire
les numéros de sécurité sociale,

00:03:52.774 --> 00:03:54.286
ce qui est, aux Etats-Unis,

00:03:54.286 --> 00:03:56.326
une information extrêmement sensible.

00:03:56.326 --> 00:03:58.419
Vous voyez où je veux en venir ?

00:03:58.419 --> 00:04:01.341
Si vous combinez les deux études,

00:04:01.341 --> 00:04:02.853
la question devient :

00:04:02.853 --> 00:04:05.573
peut-on partir d'un visage et,

00:04:05.573 --> 00:04:07.884
en utilisant la reconnaissance faciale,
trouver le nom

00:04:07.884 --> 00:04:10.553
et les informations disponibles
de façon publique

00:04:10.553 --> 00:04:12.485
sur ce nom et sur cette personne,

00:04:12.485 --> 00:04:14.733
puis, à partir de ces informations publiques,

00:04:14.733 --> 00:04:16.775
en déduire des informations
non publiques,

00:04:16.775 --> 00:04:18.381
beaucoup plus sensibles,

00:04:18.381 --> 00:04:19.873
que l'on peut relier au visage ?

00:04:19.873 --> 00:04:21.789
Et la réponse est, oui,
on peut, et on l'a fait.

00:04:21.789 --> 00:04:24.357
Bien sûr,
la précision est de pire en pire.

00:04:24.357 --> 00:04:25.301
[27% des 5 premiers chiffres
du numéro de SS identifiés (après 4 essais)]

00:04:25.301 --> 00:04:29.128
En fait, nous avons même décidé
de développer une application iPhone

00:04:29.128 --> 00:04:31.843
qui utilise la caméra interne du téléphone

00:04:31.843 --> 00:04:33.443
pour prendre un sujet en photo

00:04:33.443 --> 00:04:34.930
et la télécharger sur le cloud

00:04:34.930 --> 00:04:37.592
et puis faire ce que je vous ai décrit
en temps réel :

00:04:37.592 --> 00:04:39.680
chercher une correspondance,
trouver des informations publiques,

00:04:39.680 --> 00:04:41.410
essayer d'en déduire
des informations sensibles,

00:04:41.410 --> 00:04:44.001
et la renvoyer sur le téléphone

00:04:44.001 --> 00:04:47.610
pour qu'elle s'affiche
sur le visage du sujet,

00:04:47.610 --> 00:04:49.511
un exemple de réalité augmentée,

00:04:49.511 --> 00:04:51.962
probablement un exemple effrayant
de réalité augmentée.

00:04:51.962 --> 00:04:55.301
En fait, nous n'avons pas développé l'appli
pour la rendre publique,

00:04:55.301 --> 00:04:57.223
mais seulement
comme une preuve du concept.

00:04:57.223 --> 00:04:59.536
En fait, prenez ces technologies,

00:04:59.536 --> 00:05:01.373
et poussez-les
jusqu'à leur extrémité logique.

00:05:01.373 --> 00:05:04.092
Imaginez un futur
où des inconnus autour de vous

00:05:04.092 --> 00:05:06.403
vous regarderont
à travers leurs Google Glasses

00:05:06.403 --> 00:05:08.710
ou bien, un jour, leurs lentilles de contact,

00:05:08.710 --> 00:05:12.730
et utiliseront 7 ou 8 données sur vous

00:05:12.730 --> 00:05:15.312
pour en déduire n'importe quoi d'autre

00:05:15.312 --> 00:05:17.915
qui pourrait être connu à votre sujet.

00:05:17.915 --> 00:05:22.709
A quoi ressemblera ce futur
sans secrets ?

00:05:22.709 --> 00:05:24.673
Et devons-nous
nous en préoccuper ?

00:05:24.673 --> 00:05:26.564
On pourrait aimer croire

00:05:26.564 --> 00:05:29.604
qu'un futur avec une telle richesse de données

00:05:29.604 --> 00:05:32.118
serait un futur sans plus de parti-pris,

00:05:32.118 --> 00:05:35.701
mais en fait, avoir autant d'informations

00:05:35.701 --> 00:05:37.892
ne veut pas dire
que nous prendrons des décisions

00:05:37.892 --> 00:05:39.598
plus objectives.

00:05:39.598 --> 00:05:42.158
Dans une autre expérience,
nous avons présenté à nos sujets

00:05:42.158 --> 00:05:44.404
des informations à propos
d'un candidat potentiel à un emploi.

00:05:44.404 --> 00:05:47.582
Nous avons inclus dans ces informations
des références à des choses

00:05:47.582 --> 00:05:50.228
plutôt drôles, absolument légales,

00:05:50.228 --> 00:05:52.693
mais peut-être un peu embarrassantes

00:05:52.693 --> 00:05:54.713
que le sujet avait postées en ligne.

00:05:54.713 --> 00:05:57.079
De façon intéressante,
parmi nos sujets,

00:05:57.079 --> 00:06:00.162
certains avaient posté
des choses de même nature,

00:06:00.162 --> 00:06:02.524
et d'autres non.

00:06:02.524 --> 00:06:04.473
Quel groupe, d'après vous,

00:06:04.473 --> 00:06:09.025
a été le plus enclin
à juger durement notre sujet ?

00:06:09.025 --> 00:06:10.982
De façon paradoxale,
c'est le groupe

00:06:10.982 --> 00:06:12.715
qui avait posté des choses similaires,

00:06:12.715 --> 00:06:15.657
un exemple de dissonance morale.

00:06:15.657 --> 00:06:17.407
Vous pourriez vous dire,

00:06:17.407 --> 00:06:19.109
ceci ne s'applique pas à moi,

00:06:19.109 --> 00:06:21.271
parce que je n'ai rien à cacher.

00:06:21.271 --> 00:06:23.753
Mais en réalité,
la vie privée n'a rien à voir

00:06:23.753 --> 00:06:27.429
avec le fait d'avoir quelque chose
de négatif à cacher.

00:06:27.429 --> 00:06:29.783
Imaginez que vous êtes le directeur des R.H.

00:06:29.783 --> 00:06:32.730
d'une certaine organisation,
et que vous receviez des CV,

00:06:32.730 --> 00:06:35.203
et que vous décidiez de trouver plus d'informations
au sujet de vos candidats.

00:06:35.203 --> 00:06:37.663
Pour cela, vous Googlez leur noms

00:06:37.663 --> 00:06:39.903
et dans un certain univers,

00:06:39.903 --> 00:06:41.911
vous trouvez ces informations.

00:06:41.911 --> 00:06:46.348
Ou, dans un univers parallèle,
vous trouvez celles-ci.

00:06:46.348 --> 00:06:49.065
Pensez-vous que chaque candidat
aurait autant de chance

00:06:49.065 --> 00:06:51.868
que vous l'appeliez
pour un entretien ?

00:06:51.868 --> 00:06:54.150
SI vous pensez ça,
alors vous n'êtes pas

00:06:54.150 --> 00:06:56.732
comme les employeurs américains
qui, en fait, font

00:06:56.732 --> 00:07:00.039
partie de notre expérience,
je veux dire que c'est exactement ce qu'on a fait.

00:07:00.039 --> 00:07:03.221
On a créé des profils Facebook,
déformé certains faits,

00:07:03.221 --> 00:07:06.072
et on a commencé à envoyer nos CV
à des sociétés aux Etats-Unis,

00:07:06.072 --> 00:07:07.980
puis nous avons détecté et suivi

00:07:07.980 --> 00:07:10.373
s'ils faisaient des recherches
sur nos candidats,

00:07:10.373 --> 00:07:12.205
et s'ils agissaient
en fonction des informations

00:07:12.205 --> 00:07:14.143
qu'ils avaient trouvées sur les réseaux sociaux.
Et ils le faisaient.

00:07:14.143 --> 00:07:16.244
La discrimination se faisait à travers
les réseaux sociaux

00:07:16.244 --> 00:07:19.317
pour des candidats
de même niveau de qualification.

00:07:19.317 --> 00:07:23.892
Les spécialistes du marketing
voudraient nous faire croire

00:07:23.892 --> 00:07:26.161
que toutes les informations nous concernant
seront toujours

00:07:26.161 --> 00:07:29.434
utilisées pour notre bénéfice.

00:07:29.434 --> 00:07:33.149
Mais pensez-y.
Pourquoi devrait-ce toujours être le cas ?

00:07:33.149 --> 00:07:35.813
Dans un film sorti
il y a quelques années,

00:07:35.813 --> 00:07:38.366
« Minority Report »,
une scène célèbre

00:07:38.366 --> 00:07:40.942
montre Tom Cruise
marchant dans un centre commercial

00:07:40.942 --> 00:07:44.718
alors qu'une une publicité holographique
personnalisée

00:07:44.718 --> 00:07:46.553
apparaît autour de lui.

00:07:46.553 --> 00:07:49.780
Ce film était censé se passer en 2054,

00:07:49.780 --> 00:07:51.422
dans environ 40 ans,

00:07:51.422 --> 00:07:54.330
et aussi excitante que semble cette technologie,

00:07:54.330 --> 00:07:56.976
elle sous-estime déjà largement

00:07:56.976 --> 00:07:59.116
la quantité d'informations
que les organisations

00:07:59.116 --> 00:08:01.599
peuvent recueillir à votre sujet,
et comment elles peuvent les utiliser

00:08:01.599 --> 00:08:04.997
pour vous influencer d'une manière
que vous ne détecterez même pas.

00:08:04.997 --> 00:08:07.100
A titre d'exemple,
voici une autre expérience

00:08:07.100 --> 00:08:09.373
qui est encore en cours,
pas encore terminée.

00:08:09.373 --> 00:08:11.692
Imaginez qu'une organisation ait accès

00:08:11.692 --> 00:08:13.748
à votre liste d'amis sur Facebook,

00:08:13.748 --> 00:08:15.520
et grâce à une sorte d'algorithme

00:08:15.520 --> 00:08:19.254
elle peut détecter les deux amis
que vous aimez le plus.

00:08:19.254 --> 00:08:21.534
Puis elle crée, en temps réel,

00:08:21.534 --> 00:08:24.376
un composé du visage de ces deux amis.

00:08:24.376 --> 00:08:27.445
Des études avant les nôtres
ont démontré que les gens

00:08:27.445 --> 00:08:30.330
ne se reconnaissent même plus eux-mêmes

00:08:30.330 --> 00:08:32.792
dans des visages composés,
mais ils réagissent

00:08:32.792 --> 00:08:34.909
envers ces composés d'une manière favorable.

00:08:34.909 --> 00:08:38.324
Ainsi, la prochaine fois
que vous chercherez un produit donné,

00:08:38.324 --> 00:08:40.883
et qu'il y aura une pub
vous proposant de l'acheter,

00:08:40.883 --> 00:08:43.790
ce ne sera pas juste un acteur standard.

00:08:43.790 --> 00:08:46.103
Ce sera l'un de vos amis,

00:08:46.103 --> 00:08:49.406
et vous ne remarquerez même pas
que cela se passe comme ça.

00:08:49.406 --> 00:08:51.819
Aujourd'hui, le problème est que

00:08:51.819 --> 00:08:54.338
les mécanismes de régulations actuels

00:08:54.338 --> 00:08:57.776
pour nous protéger contre les abus
liés à l'utilisation des informations personnelles

00:08:57.776 --> 00:09:00.760
sont comme affronter une mitraillette
armé seulement d'un couteau.

00:09:00.760 --> 00:09:03.673
L'un de ces mécanismes
est la transparence,

00:09:03.673 --> 00:09:06.873
on doit informer les personnes
de ce que l'on va faire avec leurs données.

00:09:06.873 --> 00:09:08.979
Et, en principe,
c'est une très bonne chose.

00:09:08.979 --> 00:09:12.646
C'est nécessaire,
mais pas suffisant.

00:09:12.646 --> 00:09:16.344
La transparence peut être détournée.

00:09:16.344 --> 00:09:18.448
Vous pouvez dire aux gens
ce que vous allez faire,

00:09:18.448 --> 00:09:20.680
et les pousser encore à divulguer

00:09:20.680 --> 00:09:23.303
des quantités arbitraires
d'informations personnelles.

00:09:23.303 --> 00:09:26.189
Ainsi, dans une autre expérience,
menée avec des étudiants,

00:09:26.189 --> 00:09:29.247
nous leur avons demandé
de fournir des informations

00:09:29.247 --> 00:09:31.060
sur leur comportement
sur le campus,

00:09:31.060 --> 00:09:34.000
y compris des questions très sensibles,
comme celle-ci :

00:09:34.000 --> 00:09:34.621
[Avez-vous déjà triché à un examen ? ]

00:09:34.621 --> 00:09:36.921
Nous avons dit à un groupe de sujets :

00:09:36.921 --> 00:09:39.762
«Seuls d'autres étudiants
vont voir vos réponses.»

00:09:39.762 --> 00:09:41.341
Nous avons dit à un autre groupe :

00:09:41.341 --> 00:09:44.902
«Les étudiants et les professeurs
vont voir vos réponses.»

00:09:44.902 --> 00:09:47.493
Transparence. Notification.
Et, bien sûr, ça a marché,

00:09:47.493 --> 00:09:48.900
dans le sens où
le premier groupe de sujets

00:09:48.900 --> 00:09:51.468
était beaucoup plus enclin à donner des informations
que le second.

00:09:51.468 --> 00:09:52.988
C'est logique, n'est-ce pas ?

00:09:52.988 --> 00:09:54.478
Mais nous avons ensuite
ajouté le détournement.

00:09:54.478 --> 00:09:57.238
Nous avons répété l'expérience
avec les deux mêmes groupes,

00:09:57.238 --> 00:09:59.665
cette fois-ci, en ajoutant un délai

00:09:59.665 --> 00:10:02.600
entre le moment où on a dit aux sujets

00:10:02.600 --> 00:10:04.680
comment nous utiliserions leurs données

00:10:04.680 --> 00:10:09.068
et le moment où on a commencé
à leur poser des questions.

00:10:09.068 --> 00:10:11.629
Combien de temps pensez-vous
que nous ayonsdû ajouter

00:10:11.629 --> 00:10:16.242
pour neutraliser l'effet inhibiteur

00:10:16.242 --> 00:10:19.653
de savoir que les professeurs
verraient vos réponses ?

00:10:19.653 --> 00:10:21.433
Dix minutes ?

00:10:21.433 --> 00:10:23.224
Cinq minutes ?

00:10:23.224 --> 00:10:25.000
Une minute ?

00:10:25.000 --> 00:10:27.049
Que diriez-vous de 15 secondes ?

00:10:27.049 --> 00:10:29.717
Quinze secondes ont suffit
pour que les deux groupes

00:10:29.717 --> 00:10:31.285
divulguent la même quantité d'informations,

00:10:31.285 --> 00:10:34.031
comme si le deuxième groupe
ne se souciait plus

00:10:34.031 --> 00:10:36.687
que les professeurs
puissent lire ses réponses.

00:10:36.687 --> 00:10:40.023
Je dois reconnaître que cette conférence,
jusqu'ici,

00:10:40.023 --> 00:10:42.503
peut sembler excessivement sombre,

00:10:42.503 --> 00:10:44.224
mais ce n'est pas l'essentiel
de mon message.

00:10:44.224 --> 00:10:46.923
En fait, je veux vous montrer

00:10:46.923 --> 00:10:48.695
qu'il existe des alternatives.

00:10:48.695 --> 00:10:51.194
La façon dont on fait les choses aujourd'hui

00:10:51.194 --> 00:10:54.231
n'est pas la seule manière de faire,

00:10:54.231 --> 00:10:56.258
ni certainement la meilleure.

00:10:56.258 --> 00:11:00.429
Si quelqu'un vous dit,
« Les gens se fichent de préserver leur vie privée »,

00:11:00.429 --> 00:11:03.071
demandez-vous si le jeu n'a pas été conçu

00:11:03.071 --> 00:11:05.795
et truqué dans le but
qu'ils ne puissent pas s'en soucier.

00:11:05.795 --> 00:11:09.057
Réaliser que ces manipulations
se produisent,

00:11:09.057 --> 00:11:10.664
c'est déjà être à mi-chemin
du processus

00:11:10.664 --> 00:11:12.922
qui nous rend capables
de nous protéger nous-mêmes.

00:11:12.922 --> 00:11:16.632
Si quelqu'un vous dit que la protection
de la vie privée est incompatible

00:11:16.632 --> 00:11:18.481
avec les avantages du Big Data,

00:11:18.481 --> 00:11:20.954
considérez qu'au cours
des 20 dernières années,

00:11:20.954 --> 00:11:22.871
les chercheurs ont créé des technologies

00:11:22.871 --> 00:11:26.189
qui permettent à virtuellement
toutes les transactions électroniques

00:11:26.189 --> 00:11:29.938
de se dérouler d'une manière
plus respectueuse de la vie privée.

00:11:29.938 --> 00:11:32.493
On peut naviguer sur Internet
de façon anonyme.

00:11:32.493 --> 00:11:35.171
On peut envoyer des mails
qui ne pourront être lus

00:11:35.171 --> 00:11:38.880
que par le destinataire,
pas même par la NSA.

00:11:38.880 --> 00:11:41.877
Il peut même y avoir une exploitation des bases de données respectueuse de la vie privée.

00:11:41.877 --> 00:11:45.771
En d'autres termes, nous pouvons avoir
les avantages du Big Data

00:11:45.771 --> 00:11:47.903
tout en protégeant la vie privée.

00:11:47.903 --> 00:11:51.694
Bien sûr, ces technologies
impliquent un changement

00:11:51.694 --> 00:11:53.240
de la répartition des coûts
et des revenus

00:11:53.240 --> 00:11:55.347
entre les détenteurs de données
et les personnes concernées,

00:11:55.347 --> 00:11:58.800
ce qui explique peut-être pourquoi
vous n'en entendez pas beaucoup parler.

00:11:58.800 --> 00:12:02.506
Ce qui me ramène au Jardin d'Eden.

00:12:02.506 --> 00:12:05.286
Il y a une seconde interprétation
à propos de la vie privée

00:12:05.286 --> 00:12:07.095
dans l'histoire du Jardin d'Eden

00:12:07.095 --> 00:12:09.191
qui n'a rien à voir avec le fait

00:12:09.191 --> 00:12:11.416
qu'Adam et Eve se sentent nus

00:12:11.416 --> 00:12:13.797
et honteux.

00:12:13.797 --> 00:12:16.578
Vous pouvez trouver les échos
de cette interprétation

00:12:16.578 --> 00:12:19.360
dans "Le Paradis perdu"
de John Milton.

00:12:19.360 --> 00:12:23.557
Dans le jardin, Adam et Eve
sont contentés sur le plan matériel.

00:12:23.557 --> 00:12:25.661
Ils sont heureux.
Ils sont satisfaits.

00:12:25.661 --> 00:12:27.954
Cependant, ils n'ont ni la connaissance,

00:12:27.954 --> 00:12:29.594
ni la conscience d'eux-mêmes.

00:12:29.594 --> 00:12:32.913
Au moment où ils mangent
le bien nommé

00:12:32.913 --> 00:12:34.206
fruit de la connaissance,

00:12:34.206 --> 00:12:36.811
c'est là qu'ils se découvrent eux-mêmes.

00:12:36.811 --> 00:12:40.842
Ils prennent conscience.
Ils parviennent à l'autonomie.

00:12:40.842 --> 00:12:43.968
Cependant, le prix à payer,
c'est de quitter le jardin.

00:12:43.968 --> 00:12:47.849
La vie privée, d'une certaine manière,
c'est à la fois le moyen

00:12:47.849 --> 00:12:50.811
et le prix à payer pour la liberté.

00:12:50.811 --> 00:12:53.581
Encore une fois,
les spécialistes du marketing nous disent

00:12:53.581 --> 00:12:56.600
que le Big Data
et les réseaux sociaux

00:12:56.600 --> 00:12:59.579
ne sont pas seulement pour eux
un paradis du profit,

00:12:59.579 --> 00:13:02.036
mais aussi un Jardin d'Eden
pour nous autres.

00:13:02.036 --> 00:13:03.274
Nous bénéficions de contenus gratuits.

00:13:03.274 --> 00:13:06.397
Nous avons la chance de jouer à Angry Birds.
Nous avons des applications ciblées.

00:13:06.397 --> 00:13:09.294
Mais en fait, dans quelques années,
ces organisations

00:13:09.294 --> 00:13:10.903
sauront tant de choses sur nous,

00:13:10.903 --> 00:13:13.613
qu'ils seront en mesure
de déduire nos désirs

00:13:13.613 --> 00:13:15.817
avant que nous les ayons formés,
et peut-être

00:13:15.817 --> 00:13:18.264
d'acheter des produits en notre nom

00:13:18.264 --> 00:13:20.538
avant qu'on ne réalise
qu'on en a besoin.

00:13:20.538 --> 00:13:23.775
Il y a un auteur anglais

00:13:23.775 --> 00:13:26.820
qui a anticipé ce genre de futur

00:13:26.820 --> 00:13:28.225
dans lequel nous abandonnerions

00:13:28.225 --> 00:13:31.773
notre autonomie et notre liberté
pour du confort.

00:13:31.773 --> 00:13:33.934
Plus encore que George Orwell,

00:13:33.934 --> 00:13:36.695
cet auteur est, bien sûr,
Aldous Huxley.

00:13:36.695 --> 00:13:39.549
Dans « Le Meilleur des Mondes »,
il imagine une société

00:13:39.549 --> 00:13:41.720
dans laquelle les technologies
que nous avons créées

00:13:41.720 --> 00:13:43.579
à l'origine pour la liberté

00:13:43.579 --> 00:13:46.146
finissent par nous contraindre.

00:13:46.146 --> 00:13:50.937
Toutefois, dans ce livre,
il nous offre aussi une porte de sortie

00:13:50.937 --> 00:13:54.375
de cette société,
semblable au chemin

00:13:54.375 --> 00:13:58.330
qu'Adam et Ève ont eu à suivre
pour quitter le jardin.

00:13:58.330 --> 00:14:00.477
Selon les termes du Sauvage,

00:14:00.477 --> 00:14:03.546
retrouver l'autonomie et la liberté
est possible,

00:14:03.546 --> 00:14:06.225
bien que le prix à payer soit élevé.

00:14:06.225 --> 00:14:11.940
Je crois fermement
que l'un des combats décisifs

00:14:11.940 --> 00:14:14.503
de notre époque sera le combat

00:14:14.503 --> 00:14:16.890
pour le contrôle des informations personnelles,

00:14:16.890 --> 00:14:20.397
le combat pour savoir si les Big Data
peuvent devenir un vecteur

00:14:20.397 --> 00:14:21.686
de liberté,

00:14:21.686 --> 00:14:26.432
plutôt qu'un moyen
de nous manipuler à notre insu.

00:14:26.432 --> 00:14:29.025
À l'heure actuelle,
bon nombre d'entre nous

00:14:29.025 --> 00:14:31.778
ne savent même pas
que le combat a commencé,

00:14:31.778 --> 00:14:34.450
mais c'est le cas,
que ça vous plaise ou non.

00:14:34.450 --> 00:14:37.254
Et au risque de jouer les serpents,

00:14:37.254 --> 00:14:40.151
je vous dirais
que les outils pour ce combat

00:14:40.151 --> 00:14:43.160
sont là,
la conscience de ce qui est en train de se passer,

00:14:43.160 --> 00:14:44.515
et dans vos mains,

00:14:44.515 --> 00:14:48.255
à quelques clics seulement.

00:14:48.255 --> 00:14:49.737
Merci.

00:14:49.737 --> 00:14:54.214
(Applaudissements)

