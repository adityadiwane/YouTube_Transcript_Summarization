WEBVTT
Kind: captions
Language: de

00:00:00.000 --> 00:00:07.000
Übersetzung: Philipp Bock
Lektorat: Judith Matz

00:00:12.641 --> 00:00:14.995
Ich möchte eine Geschichte erzählen,

00:00:14.995 --> 00:00:18.171
die den berüchtigten Datenschutz-Zwischenfall

00:00:18.171 --> 00:00:20.940
von Adam und Eva

00:00:20.940 --> 00:00:24.386
und die bemerkenswerte 
Verschiebung der Grenzen

00:00:24.386 --> 00:00:27.072
zwischen „öffentlich“ und „privat“ in den

00:00:27.072 --> 00:00:28.842
letzten 10 Jahren vereint.

00:00:28.842 --> 00:00:30.140
Sie kennen den Fall.

00:00:30.140 --> 00:00:33.470
Eines Tages bemerken 
Adam und Eva im Garten Eden,

00:00:33.470 --> 00:00:35.313
dass sie nackt sind.

00:00:35.313 --> 00:00:36.813
Sie drehen durch.

00:00:36.813 --> 00:00:39.570
Und der Rest ist Geschichte.

00:00:39.570 --> 00:00:41.758
Heute würden sich Adam und Eva

00:00:41.758 --> 00:00:44.119
wahrscheinlich anders verhalten.

00:00:44.119 --> 00:00:46.387
[@Adam Letzte Nacht war wahnsinn! 
leckerer Apfel LOL]

00:00:46.387 --> 00:00:48.260
[@Eva ja.. schatz, was ist denn 
mit meiner Hose passiert?]

00:00:48.260 --> 00:00:50.896
Im Internet geben wir wirklich so viel mehr

00:00:50.896 --> 00:00:54.230
Informationen über uns preis als je zuvor,

00:00:54.230 --> 00:00:55.934
und so viel mehr davon

00:00:55.934 --> 00:00:58.158
wird von Organisationen gesammelt.

00:00:58.158 --> 00:01:01.440
Diese massive Auswertung persönlicher Daten

00:01:01.440 --> 00:01:03.886
– oder „Big Data“ – kann eine große

00:01:03.886 --> 00:01:05.832
Bereicherung sein,

00:01:05.832 --> 00:01:08.470
aber wir lassen uns 
auf komplexe Kompromisse ein,

00:01:08.470 --> 00:01:11.568
wenn wir unsere Privatsphäre aufgeben.

00:01:11.568 --> 00:01:15.591
Und davon handelt meine Geschichte.

00:01:15.591 --> 00:01:18.175
Wir beginnen mit einer Beobachtung, 
die für mich

00:01:18.175 --> 00:01:21.502
in den letzten Jahren 
immer eindeutiger geworden ist,

00:01:21.502 --> 00:01:23.599
dass jede persönliche Information

00:01:23.599 --> 00:01:25.884
zu sensibler Information werden kann.

00:01:25.884 --> 00:01:30.009
Im Jahr 2000 wurden 
weltweit etwa 100 Milliarden

00:01:30.009 --> 00:01:31.921
Fotos geschossen,

00:01:31.921 --> 00:01:34.986
aber nur ein winziger Teil davon

00:01:34.986 --> 00:01:36.869
wurde ins Netz geladen.

00:01:36.869 --> 00:01:40.230
2010 wurden allein auf Facebook in einem Monat

00:01:40.230 --> 00:01:43.500
2,5 Milliarden Fotos hochgeladen,

00:01:43.500 --> 00:01:45.382
die meisten mit Namen beschriftet.

00:01:45.382 --> 00:01:47.262
In derselben Zeit

00:01:47.262 --> 00:01:52.132
hat sich die Gesichtserkennung 
von Computern

00:01:52.132 --> 00:01:55.740
um drei Größenordnungen verbessert.

00:01:55.740 --> 00:01:57.622
Was passiert, wenn man diese beiden

00:01:57.622 --> 00:01:59.123
Technologien kombiniert:

00:01:59.123 --> 00:02:01.781
Erhöhte Verfügbarkeit von Gesichtsdaten;

00:02:01.781 --> 00:02:05.429
verbesserte Gesichtserkennung 
durch Computer;

00:02:05.429 --> 00:02:07.611
aber auch Cloud-Computing,

00:02:07.611 --> 00:02:09.499
das jedem in diesem Saal

00:02:09.499 --> 00:02:11.059
Rechenkapazitäten in die Hände legt,

00:02:11.059 --> 00:02:12.945
die vor ein paar Jahren nur Behörden

00:02:12.945 --> 00:02:14.727
mit drei Buchstaben hatten;

00:02:14.727 --> 00:02:16.105
und allgegenwärtige Computer,

00:02:16.105 --> 00:02:18.997
die es meinem Telefon, 
das nun kein Supercomputer ist,

00:02:18.997 --> 00:02:20.668
ermöglichen, ins Internet zu gehen

00:02:20.668 --> 00:02:23.002
und dort hunderttausende Gesichter

00:02:23.002 --> 00:02:25.641
in ein paar Sekunden zu vergleichen?

00:02:25.641 --> 00:02:28.269
Wir mutmaßen, dass das Ergebnis

00:02:28.269 --> 00:02:30.333
dieser Kombination von Technologien

00:02:30.333 --> 00:02:33.221
eine radikaler Wandel in unserer Auffassung

00:02:33.221 --> 00:02:35.478
von Privatsphäre und Anonymität sein wird.

00:02:35.478 --> 00:02:37.471
Mit einem Experiment auf dem Campus

00:02:37.471 --> 00:02:39.592
unserer Universität haben wir das getestet.

00:02:39.592 --> 00:02:41.691
Wir baten vorbeilaufende Studenten,

00:02:41.691 --> 00:02:43.470
an einer Studie teilzunehmen,

00:02:43.470 --> 00:02:46.032
machten ein Foto mit einer Webcam,

00:02:46.032 --> 00:02:48.814
und baten sie, auf einem Laptop 
einen Fragebogen auszufüllen.

00:02:48.814 --> 00:02:50.793
Während sie das taten,

00:02:50.793 --> 00:02:53.590
schickten wir ihr Foto an einen Großrechner

00:02:53.590 --> 00:02:55.317
und fingen an, dieses Foto

00:02:55.317 --> 00:02:57.722
per Gesichtserkennung mit einer Datenbank

00:02:57.722 --> 00:03:00.115
von ein paar hunderttausend Fotos 
abzugleichen,

00:03:00.115 --> 00:03:03.711
die wir von Facebook-Profilen 
gesammelt hatten.

00:03:03.711 --> 00:03:06.970
Bevor die Testperson die letzte Seite erreichte,

00:03:06.970 --> 00:03:10.317
wurde sie automatisch 
mit den zehn Fotos aktualisiert,

00:03:10.317 --> 00:03:12.630
die laut Gesichtserkennung

00:03:12.630 --> 00:03:14.915
die größte Übereinstimmung hatten,

00:03:14.915 --> 00:03:16.653
und wir fragten die Teilnehmer,

00:03:16.653 --> 00:03:20.773
ob sie sich selbst auf einem Foto erkannten.

00:03:20.773 --> 00:03:24.472
Erkennen Sie die Testperson?

00:03:24.472 --> 00:03:27.317
Der Computer schon, und zwar

00:03:27.317 --> 00:03:29.466
bei einem von drei Teilnehmern.

00:03:29.466 --> 00:03:32.650
Im Grunde können wir also 
ein unbekanntes Gesicht nehmen,

00:03:32.650 --> 00:03:36.134
offline oder online, 
und ihm durch Gesichtserkennung

00:03:36.134 --> 00:03:38.494
dank Daten aus den sozialen Medien

00:03:38.494 --> 00:03:40.602
einen Namen geben.

00:03:40.602 --> 00:03:42.474
Aber vor ein paar Jahren
taten wir etwas anderes.

00:03:42.474 --> 00:03:44.297
Wir nahmen Daten aus sozialen Medien,

00:03:44.297 --> 00:03:47.348
kombinierten sie statistisch mit Daten

00:03:47.348 --> 00:03:49.450
der US-amerikanischen Sozialversicherung,

00:03:49.450 --> 00:03:52.774
und konnten damit 
Sozialversicherungsnummern vorhersagen –

00:03:52.774 --> 00:03:54.286
in den USA sind das

00:03:54.286 --> 00:03:56.326
extrem sensible Daten.

00:03:56.326 --> 00:03:58.419
Sehen Sie, worauf ich hinauswill?

00:03:58.419 --> 00:04:01.341
Wenn man diese beiden Studien kombiniert,

00:04:01.341 --> 00:04:02.853
ist die Frage, ob man

00:04:02.853 --> 00:04:05.573
von einem Gesicht ausgehen kann,

00:04:05.573 --> 00:04:07.884
dazu per Gesichtserkennung einen Namen

00:04:07.884 --> 00:04:10.553
und anhand des Namens

00:04:10.553 --> 00:04:12.485
öffentliche Informationen finden kann,

00:04:12.485 --> 00:04:14.733
und aus diesen öffentlichen Informationen

00:04:14.733 --> 00:04:16.775
auf nicht-öffentliche schließen kann,

00:04:16.775 --> 00:04:18.381
die viel sensibler sind,

00:04:18.381 --> 00:04:19.873
die man dann zum Gesicht zurückführt?

00:04:19.873 --> 00:04:21.789
Die Antwort ist: Ja, können und haben wir.

00:04:21.789 --> 00:04:24.357
Natürlich leidet darunter die Genauigkeit.

00:04:24.357 --> 00:04:25.301
[27&nbsp;% der ersten 5 Ziffern 
der SV-Nummern gefunden (bei 4 Versuchen)]

00:04:25.301 --> 00:04:29.128
Aber wir haben sogar eine App 
fürs iPhone entwickelt,

00:04:29.128 --> 00:04:31.843
die mit der Kamera des Geräts ein Foto

00:04:31.843 --> 00:04:33.443
von einer Person macht,

00:04:33.443 --> 00:04:34.930
es an die Cloud schickt,

00:04:34.930 --> 00:04:37.592
und dann in Echtzeit tut, 
was ich gerade beschrieben habe:

00:04:37.592 --> 00:04:39.680
Sie vergleicht Gesichter, 
findet öffentliche Informationen,

00:04:39.680 --> 00:04:41.410
versucht, auf sensible Informationen zu schließen,

00:04:41.410 --> 00:04:44.001
sendet sie an das Telefon zurück,

00:04:44.001 --> 00:04:47.610
und zeigt sie über dem Gesicht der Person an;

00:04:47.610 --> 00:04:49.511
Ein Beispiel für erweiterte Realität,

00:04:49.511 --> 00:04:51.962
und wohl eher ein unheimliches Beispiel.

00:04:51.962 --> 00:04:55.301
Wir haben diese App nicht entwickelt, 
um sie zu veröffentlichen,

00:04:55.301 --> 00:04:57.223
sondern um die Machbarkeit zu beweisen.

00:04:57.223 --> 00:04:59.536
Aber nehmen wir diese Technologien

00:04:59.536 --> 00:05:01.373
und treiben wir sie auf die logische Spitze.

00:05:01.373 --> 00:05:04.092
Stellen Sie sich eine Zukunft vor, in der Fremde

00:05:04.092 --> 00:05:06.403
Sie durch ihre Google-Brille sehen,

00:05:06.403 --> 00:05:08.710
oder irgendwann durch ihre Kontaktlinsen,

00:05:08.710 --> 00:05:12.730
und mit sieben, acht Datenpunkten über Sie

00:05:12.730 --> 00:05:15.312
auf alles andere schließen,

00:05:15.312 --> 00:05:17.915
das man über Sie wissen kann.

00:05:17.915 --> 00:05:22.709
Wie sieht diese Zukunft
ohne Geheimnisse aus?

00:05:22.709 --> 00:05:24.673
Und spielt das für uns eine Rolle?

00:05:24.673 --> 00:05:26.564
Vielleicht glauben wir gerne,

00:05:26.564 --> 00:05:29.604
dass eine Zukunft mit solchen Datenreichtümern

00:05:29.604 --> 00:05:32.118
eine Zukunft ohne Vorurteile ist,

00:05:32.118 --> 00:05:35.701
aber tatsächlich bedeutet diese Fülle

00:05:35.701 --> 00:05:37.892
an Information nicht, dass wir objektivere

00:05:37.892 --> 00:05:39.598
Entscheidungen treffen.

00:05:39.598 --> 00:05:42.158
In einem anderen Experiment 
gaben wir den Teilnehmern

00:05:42.158 --> 00:05:44.404
Informationen über einen Bewerber 
auf einen Job.

00:05:44.404 --> 00:05:47.582
Ein Teil dieser Informationen 
betraf Situationen,

00:05:47.582 --> 00:05:50.228
die zwar lustig und völlig legal,

00:05:50.228 --> 00:05:52.693
aber vielleicht etwas peinlich waren,

00:05:52.693 --> 00:05:54.713
die die „Bewerber“ ins Netz gestellt hatten.

00:05:54.713 --> 00:05:57.079
Interessant war, dass manche Teilnehmer

00:05:57.079 --> 00:06:00.162
so etwas auch schon einmal gepostet hatten

00:06:00.162 --> 00:06:02.524
und andere nicht.

00:06:02.524 --> 00:06:04.473
Welche Gruppe, glauben Sie,

00:06:04.473 --> 00:06:09.025
hat die Bewerber strenger beurteilt?

00:06:09.025 --> 00:06:10.982
Paradoxerweise war es die Gruppe,

00:06:10.982 --> 00:06:12.715
die selbst ähnliche Posts geschrieben hatte&nbsp;–

00:06:12.715 --> 00:06:15.657
ein Beispiel für moralische Dissonanz.

00:06:15.657 --> 00:06:17.407
Vielleicht denken Sie jetzt:

00:06:17.407 --> 00:06:19.109
„Das betrifft mich nicht,

00:06:19.109 --> 00:06:21.271
weil ich nichts zu verbergen habe.“

00:06:21.271 --> 00:06:23.753
Aber Privatsphäre bedeutet nicht,

00:06:23.753 --> 00:06:27.429
etwas Negatives verbergen zu wollen.

00:06:27.429 --> 00:06:29.783
Stellen Sie sich vor, 
Sie seien Personalchef

00:06:29.783 --> 00:06:32.730
einer Organisation. 
Man schickt Ihnen Lebensläufe

00:06:32.730 --> 00:06:35.203
und Sie wollen mehr Informationen 
über die Bewerber finden.

00:06:35.203 --> 00:06:37.663
Also googlen Sie ihre Namen

00:06:37.663 --> 00:06:39.903
und in einem gewissen Universum

00:06:39.903 --> 00:06:41.911
finden Sie diese Information.

00:06:41.911 --> 00:06:46.348
Oder in einem Paralleluniversum
finden Sie diese.

00:06:46.348 --> 00:06:49.065
Glauben Sie, Sie würden beide Bewerberinnen

00:06:49.065 --> 00:06:51.868
mit derselben Wahrscheinlichkeit
zum Gespräch einladen?

00:06:51.868 --> 00:06:54.150
Wenn ja, dann denken Sie anders

00:06:54.150 --> 00:06:56.732
als die Arbeitgeber der USA, die tatsächlich

00:06:56.732 --> 00:07:00.039
Teil unseres Experimentes waren, 
denn genau das taten wir:

00:07:00.039 --> 00:07:03.221
Wir erstellten Facebook-Profile 
mit verschiedenen Eigenschaften

00:07:03.221 --> 00:07:06.072
und schickten Bewerbungen
an Firmen in den USA,

00:07:06.072 --> 00:07:07.980
wir zeichneten auf,

00:07:07.980 --> 00:07:10.373
ob sie nach unseren Bewerbern suchten,

00:07:10.373 --> 00:07:12.205
und ob sie anhand der Informationen

00:07:12.205 --> 00:07:14.143
aus sozialen Medien handelten.
Und das taten sie.

00:07:14.143 --> 00:07:16.244
Durch soziale Medien wurden Bewerber

00:07:16.244 --> 00:07:19.317
trotz gleicher Fähigkeiten diskriminiert.

00:07:19.317 --> 00:07:23.892
Vermarkter hätten gerne, dass wir glauben,

00:07:23.892 --> 00:07:26.161
dass Informationen über uns immer

00:07:26.161 --> 00:07:29.434
zu unseren Gunsten genutzt werden.

00:07:29.434 --> 00:07:33.149
Aber denken Sie noch mal nach. 
Warum sollten sie?

00:07:33.149 --> 00:07:35.813
In einem Film, der vor ein paar Jahren lief,

00:07:35.813 --> 00:07:38.366
„Minority Report“, läuft Tom Cruise

00:07:38.366 --> 00:07:40.942
durch ein Einkaufszentrum

00:07:40.942 --> 00:07:44.718
und um ihn herum taucht personalisierte

00:07:44.718 --> 00:07:46.553
holografische Werbung auf.

00:07:46.553 --> 00:07:49.780
Dieser Film spielt im Jahr 2054,

00:07:49.780 --> 00:07:51.422
also in ungefähr 40 Jahren,

00:07:51.422 --> 00:07:54.330
und so spannnend diese Technologie aussieht,

00:07:54.330 --> 00:07:56.976
unterschätzt sie schon jetzt zutiefst,

00:07:56.976 --> 00:07:59.116
wie viele Informationen Organisationen

00:07:59.116 --> 00:08:01.599
über Sie sammeln können, 
und wie sie diese nutzen können,

00:08:01.599 --> 00:08:04.997
um Sie so zu beeinflussen,
dass Sie es nicht einmal merken.

00:08:04.997 --> 00:08:07.100
Als Beispiel ist hier noch ein Experiment,

00:08:07.100 --> 00:08:09.373
das wir gerade durchführen, es läuft noch.

00:08:09.373 --> 00:08:11.692
Stellen Sie sich vor, eine Organisation

00:08:11.692 --> 00:08:13.748
hat eine Liste Ihrer Facebook-Freunde,

00:08:13.748 --> 00:08:15.520
und mit irgendeinem Algorithmus

00:08:15.520 --> 00:08:19.254
können sie Ihre zwei besten Freunde herausfinden.

00:08:19.254 --> 00:08:21.534
Und dann vermischen sie in Echtzeit

00:08:21.534 --> 00:08:24.376
die Gesichter dieser beiden Freunde.

00:08:24.376 --> 00:08:27.445
Frühere Studien haben gezeigt, 
dass Menschen

00:08:27.445 --> 00:08:30.330
sich in diesen digital vermischten Gesichtern

00:08:30.330 --> 00:08:32.792
nicht einmal selbst erkennen, dass sie aber

00:08:32.792 --> 00:08:34.909
auf diese Bilder positiv reagieren.

00:08:34.909 --> 00:08:38.324
Wenn Sie also das nächste Mal 
ein Produkt suchen

00:08:38.324 --> 00:08:40.883
und Sie Werbung dafür sehen,

00:08:40.883 --> 00:08:43.790
dann wird darin nicht irgendein Model sein.

00:08:43.790 --> 00:08:46.103
Es wird einer Ihrer Freunde sein,

00:08:46.103 --> 00:08:49.406
und Sie werden es nicht einmal merken.

00:08:49.406 --> 00:08:51.819
Das Problem ist,

00:08:51.819 --> 00:08:54.338
dass die aktuellen Datenschutzmechanismen

00:08:54.338 --> 00:08:57.776
gegen den Missbrauch persönlicher Informationen

00:08:57.776 --> 00:09:00.760
so nützlich sind wie ein Messer 
bei einer Schießerei.

00:09:00.760 --> 00:09:03.673
Ein solcher Mechanismus ist Transparenz,

00:09:03.673 --> 00:09:06.873
man sagt Menschen, 
was man mit ihren Daten vorhat.

00:09:06.873 --> 00:09:08.979
Und im Prinzip ist das etwas sehr Gutes.

00:09:08.979 --> 00:09:12.646
Es ist notwendig, aber nicht hinreichend.

00:09:12.646 --> 00:09:16.344
Transparenz kann in die Irre führen.

00:09:16.344 --> 00:09:18.448
Sie können Menschen sagen, 
was Sie vorhaben,

00:09:18.448 --> 00:09:20.680
und ihnen dann doch beliebige Mengen

00:09:20.680 --> 00:09:23.303
persönlicher Informationen herauslocken.

00:09:23.303 --> 00:09:26.189
In noch einem anderen Experiment 
baten wir Studierende,

00:09:26.189 --> 00:09:29.247
uns Informationen über ihr Verhalten

00:09:29.247 --> 00:09:31.060
an der Uni zu geben,

00:09:31.060 --> 00:09:34.000
darunter ziemlich sensible Fragen, 
so wie diese hier.

00:09:34.000 --> 00:09:34.621
[Haben Sie je in einer Prüfung betrogen?]

00:09:34.621 --> 00:09:36.921
Einer Gruppe der Teilnehmer sagten wir:

00:09:36.921 --> 00:09:39.762
„Nur Kommilitonen lesen Ihre Antworten.“

00:09:39.762 --> 00:09:41.341
Einer zweiten Gruppe sagten wir:

00:09:41.341 --> 00:09:44.902
„Studierende und Lehrende 
lesen Ihre Antworten.“

00:09:44.902 --> 00:09:47.493
Transparenz. Aufklärung. 
Und natürlich funktionierte das,

00:09:47.493 --> 00:09:48.900
nämlich so, dass die erste Gruppe

00:09:48.900 --> 00:09:51.468
viel eher „Ja“ antwortete als die zweite.

00:09:51.468 --> 00:09:52.988
Das ist logisch, oder?

00:09:52.988 --> 00:09:54.478
Aber dann führten wir sie in die Irre.

00:09:54.478 --> 00:09:57.238
Wir wiederholten das Experiment 
mit den gleichen Gruppen,

00:09:57.238 --> 00:09:59.665
fügten aber eine Verzögerung ein

00:09:59.665 --> 00:10:02.600
zwischen der Aufklärung darüber,

00:10:02.600 --> 00:10:04.680
wie wir mit den Daten umgehen,

00:10:04.680 --> 00:10:09.068
und dem eigentlichen Beginn des Fragebogens.

00:10:09.068 --> 00:10:11.629
Wie lange musste die Verzögerung sein,

00:10:11.629 --> 00:10:16.242
um die Hemmungen 
der Teilnehmer aufzuheben,

00:10:16.242 --> 00:10:19.653
deren Antworten 
auch Dozenten lesen würden?

00:10:19.653 --> 00:10:21.433
Zehn Minuten?

00:10:21.433 --> 00:10:23.224
Fünf Minuten?

00:10:23.224 --> 00:10:25.000
Eine Minute?

00:10:25.000 --> 00:10:27.049
Wie wär’s mit 15 Sekunden?

00:10:27.049 --> 00:10:29.717
15 Sekunden waren genug, 
damit die beiden Gruppen

00:10:29.717 --> 00:10:31.285
dieselbe Menge an Informationen preisgaben,

00:10:31.285 --> 00:10:34.031
als wäre es der zweiten Gruppe plötzlich egal,

00:10:34.031 --> 00:10:36.687
dass Dozenten ihre Antworten lesen.

00:10:36.687 --> 00:10:40.023
Ich muss zugeben, 
dass meine Rede bis hierhin

00:10:40.023 --> 00:10:42.503
vielleicht äußerst finster klang,

00:10:42.503 --> 00:10:44.224
aber darum geht es mir nicht.

00:10:44.224 --> 00:10:46.923
Ich will mit Ihnen über die Tatsache reden,

00:10:46.923 --> 00:10:48.695
dass es Alternativen gibt.

00:10:48.695 --> 00:10:51.194
Unsere jetzige Vorgehensweise 
ist nicht die einzige

00:10:51.194 --> 00:10:54.231
und bestimmt nicht die beste Weise,

00:10:54.231 --> 00:10:56.258
die Dinge anzugehen.

00:10:56.258 --> 00:11:00.429
Wenn Ihnen jemand sagt: 
„Den Leuten ist Datenschutz egal,“

00:11:00.429 --> 00:11:03.071
dann überlegen Sie, ob die Würfel

00:11:03.071 --> 00:11:05.795
so gezinkt sind, dass ihnen 
Datenschutz egal sein muss,

00:11:05.795 --> 00:11:09.057
und mit der Einsicht, dass 
diese Manipulationen geschehen,

00:11:09.057 --> 00:11:10.664
haben Sie die halbe Strecke

00:11:10.664 --> 00:11:12.922
zum Schutz Ihrer Daten schon geschafft.

00:11:12.922 --> 00:11:16.632
Wenn Ihnen jemand sagt, 
Datenschutz sei unvereinbar

00:11:16.632 --> 00:11:18.481
mit den Vorteilen von Big Data,

00:11:18.481 --> 00:11:20.954
überlegen Sie, dass Wissenschaftler

00:11:20.954 --> 00:11:22.871
seit 20 Jahren Technologien geschaffen haben,

00:11:22.871 --> 00:11:26.189
mit sich denen nahezu 
jeder elektronische Vorgang

00:11:26.189 --> 00:11:29.938
datenschutzfreundlicher abwickeln lässt.

00:11:29.938 --> 00:11:32.493
Wir können anonym im Internet surfen.

00:11:32.493 --> 00:11:35.171
Wir können E-Mails verschicken, die nur

00:11:35.171 --> 00:11:38.880
der Empfänger lesen kann, nicht einmal die NSA.

00:11:38.880 --> 00:11:41.877
Und sogar datenschutzfreundlich 
nach Daten schürfen.

00:11:41.877 --> 00:11:45.771
Mit anderen Worten, wir können 
die Vorteile von Big Data genießen

00:11:45.771 --> 00:11:47.903
und dabei die Privatsphäre schützen.

00:11:47.903 --> 00:11:51.694
Voraussetzung für diese Technologien
ist eine Verschiebung

00:11:51.694 --> 00:11:53.240
von Kosten und Nutzen

00:11:53.240 --> 00:11:55.347
zwischen Inhaber und Gegenstand von Daten,

00:11:55.347 --> 00:11:58.800
was erklären mag, warum Sie 
nicht viel davon hören.

00:11:58.800 --> 00:12:02.506
Und das bringt mich zurück 
zum Garten Eden.

00:12:02.506 --> 00:12:05.286
Es gibt eine zweite Datenschutz-Interpretation

00:12:05.286 --> 00:12:07.095
der Geschichte aus dem Garten Eden,

00:12:07.095 --> 00:12:09.191
die nichts damit zu tun hat,

00:12:09.191 --> 00:12:11.416
dass Adam und Eva sich nackt fühlen

00:12:11.416 --> 00:12:13.797
und schämen.

00:12:13.797 --> 00:12:16.578
Ein Echo dieser Interpretation finden Sie

00:12:16.578 --> 00:12:19.360
in John Miltons „Paradise Lost“.

00:12:19.360 --> 00:12:23.557
Im Paradies sind Adam und Eva materiell zufrieden.

00:12:23.557 --> 00:12:25.661
Sie sind glücklich. Sie sind zufrieden.

00:12:25.661 --> 00:12:27.954
Gleichzeitig fehlt ihnen aber Wissen

00:12:27.954 --> 00:12:29.594
und Selbstwahrnehmung.

00:12:29.594 --> 00:12:32.913
In dem Moment, in dem sie 
die treffend benannte

00:12:32.913 --> 00:12:34.206
„Frucht der Erkenntnis“ essen,

00:12:34.206 --> 00:12:36.811
genau dann erkennen sie sich selbst.

00:12:36.811 --> 00:12:40.842
Sie erreichen Bewusstsein und Selbständigkeit.

00:12:40.842 --> 00:12:43.968
Der Preis dafür ist die Vertreibung 
aus dem Paradies.

00:12:43.968 --> 00:12:47.849
Privatsphäre ist also gewissermaßen

00:12:47.849 --> 00:12:50.811
zugleich Mittel und Preis der Freiheit.

00:12:50.811 --> 00:12:53.581
Vermarkter erzählen uns,

00:12:53.581 --> 00:12:56.600
Big Data und Soziale Medien

00:12:56.600 --> 00:12:59.579
seien nicht nur ein Profit-Paradies für sie,

00:12:59.579 --> 00:13:02.036
sondern auch ein Garten Eden für alle.

00:13:02.036 --> 00:13:03.274
Wir bekommen kostenlose Inhalte,

00:13:03.274 --> 00:13:06.397
dürfen Angry Birds spielen, 
bekommen bessere Werbung.

00:13:06.397 --> 00:13:09.294
Aber in ein paar Jahren 
werden Organisationen

00:13:09.294 --> 00:13:10.903
so viel über uns wissen,

00:13:10.903 --> 00:13:13.613
dass sie unsere Wünsche ablesen können,

00:13:13.613 --> 00:13:15.817
bevor wir sie überhaupt geformt haben,

00:13:15.817 --> 00:13:18.264
und vielleicht Produkte für uns kaufen,

00:13:18.264 --> 00:13:20.538
bevor wir wissen, dass wir sie brauchen.

00:13:20.538 --> 00:13:23.775
Es gibt einen englischen Schriftsteller,

00:13:23.775 --> 00:13:26.820
der diese Zukunft vorhergesehen hat,

00:13:26.820 --> 00:13:28.225
in der wir Freiheit und

00:13:28.225 --> 00:13:31.773
Selbständigkeit gegen Bequemlichkeit tauschen.

00:13:31.773 --> 00:13:33.934
Viel mehr noch als George Orwell,

00:13:33.934 --> 00:13:36.695
ich meine natürlich Aldous Huxley.

00:13:36.695 --> 00:13:39.549
In „Schöne neue Welt“ 
beschreibt er eine Gesellschaft,

00:13:39.549 --> 00:13:41.720
in der Technologien, die ursprünglich

00:13:41.720 --> 00:13:43.579
unserer Freiheit dienten,

00:13:43.579 --> 00:13:46.146
uns schließlich unterdrücken.

00:13:46.146 --> 00:13:50.937
Aber im Buch schlägt er auch einen Ausweg

00:13:50.937 --> 00:13:54.375
aus dieser Gesellschaft vor, ähnlich dem Weg

00:13:54.375 --> 00:13:58.330
dem Adam und Eva 
aus dem Paradies folgen mussten.

00:13:58.330 --> 00:14:00.477
In den Worten des Wilden,

00:14:00.477 --> 00:14:03.546
man kann Autonomie 
und Freiheit zurück erobern,

00:14:03.546 --> 00:14:06.225
aber der Preis dafür ist hoch.

00:14:06.225 --> 00:14:11.940
Ich glaube, einer der entscheidenden Kämpfe

00:14:11.940 --> 00:14:14.503
unserer Zeit wird der Kampf

00:14:14.503 --> 00:14:16.890
um die Kontrolle 
über persönliche Informationen sein,

00:14:16.890 --> 00:14:20.397
der Kampf darüber, ob Big Data eine Macht

00:14:20.397 --> 00:14:21.686
der Freiheit wird,

00:14:21.686 --> 00:14:26.432
und nicht eine Macht, 
die uns heimlich manipuliert.

00:14:26.432 --> 00:14:29.025
Noch wissen viele von uns nicht,

00:14:29.025 --> 00:14:31.778
dass dieser Kampf gerade geführt wird,

00:14:31.778 --> 00:14:34.450
aber das wird er, ob Sie wollen oder nicht.

00:14:34.450 --> 00:14:37.254
Und auf die Gefahr hin, die Schlange zu sein,

00:14:37.254 --> 00:14:40.151
sage ich Ihnen, dass die Waffen für den Kampf

00:14:40.151 --> 00:14:43.160
hier drin sind – 
die Erkenntnis darüber, was geschieht –,

00:14:43.160 --> 00:14:44.515
und in Ihren Händen –

00:14:44.515 --> 00:14:48.255
nur ein paar Klicks entfernt.

00:14:48.255 --> 00:14:49.737
Vielen Dank.

00:14:49.737 --> 00:14:54.214
(Applaus)

