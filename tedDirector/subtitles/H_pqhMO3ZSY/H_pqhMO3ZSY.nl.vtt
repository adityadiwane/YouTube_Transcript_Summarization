WEBVTT
Kind: captions
Language: nl

00:00:00.000 --> 00:00:07.000
Vertaald door: Michael van Rhee
Nagekeken door: Christel Foncke

00:00:12.641 --> 00:00:14.995
Ik wil jullie graag een verhaal vertellen

00:00:14.995 --> 00:00:18.171
dat het beruchte privacy-incident

00:00:18.171 --> 00:00:20.940
van Adam en Eva

00:00:20.940 --> 00:00:24.386
verbindt met de merkwaardige verschuiving 
van de grenzen

00:00:24.386 --> 00:00:27.072
tussen openbaar en privé

00:00:27.072 --> 00:00:28.842
in de laatste 10 jaar.

00:00:28.842 --> 00:00:30.140
Je kent het voorval.

00:00:30.140 --> 00:00:33.470
Op een dag realiseren Adam en Eva zich 
in het Hof van Eden

00:00:33.470 --> 00:00:35.313
dat ze naakt zijn.

00:00:35.313 --> 00:00:36.813
Ze schrikken zich rot ...

00:00:36.813 --> 00:00:39.570
en de rest is geschiedenis.

00:00:39.570 --> 00:00:41.758
Vandaag zouden Adam en Eva

00:00:41.758 --> 00:00:44.119
waarschijnlijk anders reageren.

00:00:44.119 --> 00:00:46.387
[@Adam Gisteravond was geweldig!
heerlijk die appel LOL]

00:00:46.387 --> 00:00:48.260
[@Eva yep.. schatje, maar weet je 
waar mijn onderbroek is?]

00:00:48.260 --> 00:00:50.896
We geven veel meer informatie

00:00:50.896 --> 00:00:54.230
over onszelf prijs op het internet dan ooit tevoren,

00:00:54.230 --> 00:00:55.934
en er wordt heel veel informatie over ons

00:00:55.934 --> 00:00:58.158
verzameld door organisaties.

00:00:58.158 --> 00:01:01.440
Er valt veel winst te halen

00:01:01.440 --> 00:01:03.886
uit deze gigantische analyse
van persoonlijke informatie,

00:01:03.886 --> 00:01:05.832
of grote hoeveelheden data,

00:01:05.832 --> 00:01:08.470
maar er komen ook complexe afwegingen kijken

00:01:08.470 --> 00:01:11.568
bij het weggeven van onze privacy.

00:01:11.568 --> 00:01:15.591
Mijn verhaal gaat over die afwegingen.

00:01:15.591 --> 00:01:18.175
We beginnen met een vaststelling 
die voor mij

00:01:18.175 --> 00:01:21.502
steeds duidelijker werd in de afgelopen jaren:

00:01:21.502 --> 00:01:23.599
alle persoonlijke informatie kan

00:01:23.599 --> 00:01:25.884
ook gevoelige informatie worden.

00:01:25.884 --> 00:01:30.009
In het jaar 2000 werden ongeveer 100 miljard foto's

00:01:30.009 --> 00:01:31.921
genomen over de hele wereld,

00:01:31.921 --> 00:01:34.986
maar slechts een minuscuul deel daarvan

00:01:34.986 --> 00:01:36.869
werd daadwerkelijk online gezet.

00:01:36.869 --> 00:01:40.230
In 2010 werden alleen op Facebook al,
in één maand,

00:01:40.230 --> 00:01:43.500
2,5 miljard foto's geüpload,

00:01:43.500 --> 00:01:45.382
waarvan de meeste geïdentificeerd waren.

00:01:45.382 --> 00:01:47.262
In dezelfde periode

00:01:47.262 --> 00:01:52.132
werd het vermogen van computers
voor gezichtsherkenning

00:01:52.132 --> 00:01:55.740
drie ordes van grootte beter.

00:01:55.740 --> 00:01:57.622
Wat gebeurt er zodra je de volgende

00:01:57.622 --> 00:01:59.123
technologieën combineert:

00:01:59.123 --> 00:02:01.781
grotere beschikbaarheid van gezichtsdata,

00:02:01.781 --> 00:02:05.429
verbeterde gezichtsherkenning,

00:02:05.429 --> 00:02:07.611
én cloud computing,

00:02:07.611 --> 00:02:09.499
wat iedereen in deze zaal

00:02:09.499 --> 00:02:11.059
de rekenkracht geeft

00:02:11.059 --> 00:02:12.945
waarover een paar jaar geleden

00:02:12.945 --> 00:02:14.727
enkel agentschappen met drie letters beschikten,

00:02:14.727 --> 00:02:16.105
en alomtegenwoordig computergebruik,

00:02:16.105 --> 00:02:18.997
wat mijn telefoon in staat stelt,
en dat is geen supercomputer,

00:02:18.997 --> 00:02:20.668
om verbinding te maken met het internet

00:02:20.668 --> 00:02:23.002
en daar honderdduizenden

00:02:23.002 --> 00:02:25.641
gezichtsberekeningen te maken 
in een paar seconden?

00:02:25.641 --> 00:02:28.269
We vermoeden dat als gevolg

00:02:28.269 --> 00:02:30.333
van de combinatie van deze technologieën

00:02:30.333 --> 00:02:33.221
onze opvattingen over privacy en anonimiteit 
radicaal zullen veranderen

00:02:33.221 --> 00:02:35.478
onze opvattingen over privacy en anonimiteit 
radicaal zullen veranderen

00:02:35.478 --> 00:02:37.471
Als test voerden we een experiment uit

00:02:37.471 --> 00:02:39.592
op de campus van Carnegie Mellon University.

00:02:39.592 --> 00:02:41.691
We vroegen studenten

00:02:41.691 --> 00:02:43.470
om deel te nemen aan een onderzoek,

00:02:43.470 --> 00:02:46.032
we namen hun foto met een webcam.

00:02:46.032 --> 00:02:48.814
Daarna moesten ze een vragenlijst invullen
op een laptop.

00:02:48.814 --> 00:02:50.793
Terwijl zij de vragenlijst invulden,

00:02:50.793 --> 00:02:53.590
uploadden wij hun foto naar 
een cloud computing wolk.

00:02:53.590 --> 00:02:55.317
We gebruikten een gezichtsherkenner

00:02:55.317 --> 00:02:57.722
om de foto te vergelijken

00:02:57.722 --> 00:03:00.115
met honderdduizenden foto's

00:03:00.115 --> 00:03:03.711
die we hadden gedownload van Facebookprofielen.

00:03:03.711 --> 00:03:06.970
Toen de deelnemer de laatste pagina

00:03:06.970 --> 00:03:10.317
van de vragenlijst bereikte,
was de pagina dynamisch geüpdatet

00:03:10.317 --> 00:03:12.630
met de 10 best overeenstemmende foto's

00:03:12.630 --> 00:03:14.915
die de gezichtsherkenner had gevonden.

00:03:14.915 --> 00:03:16.653
We vroegen de deelnemers

00:03:16.653 --> 00:03:20.773
of ze zichzelf herkenden in één van de foto's.

00:03:20.773 --> 00:03:24.472
Zie je de deelnemer?

00:03:24.472 --> 00:03:27.317
De computer in ieder geval wel, en die deed dat zelfs

00:03:27.317 --> 00:03:29.466
in één op de drie gevallen.

00:03:29.466 --> 00:03:32.650
We kunnen dus gewoon beginnen
met een anoniem gezicht,

00:03:32.650 --> 00:03:36.134
offline of online, en gezichtsherkenning gebruiken

00:03:36.134 --> 00:03:38.494
om een naam te plakken op dat anonieme gezicht

00:03:38.494 --> 00:03:40.602
dankzij gegevens van sociale media.

00:03:40.602 --> 00:03:42.474
Een paar jaar geleden deden we iets anders.

00:03:42.474 --> 00:03:44.297
We begonnen met data van sociale media,

00:03:44.297 --> 00:03:47.348
en combineerden die met statistisch data

00:03:47.348 --> 00:03:49.450
van de Amerikaanse overheid over sociale zekerheid,

00:03:49.450 --> 00:03:52.774
en uiteindelijk konden we sofinummers voorspellen,

00:03:52.774 --> 00:03:54.286
wat in de Verenigde Staten

00:03:54.286 --> 00:03:56.326
extreem gevoelige informatie is.

00:03:56.326 --> 00:03:58.419
Volgen jullie mij?

00:03:58.419 --> 00:04:01.341
Als je de twee onderzoeken samenvoegt,

00:04:01.341 --> 00:04:02.853
dan wordt de vraag:

00:04:02.853 --> 00:04:05.573
kun je met een foto van een gezicht,

00:04:05.573 --> 00:04:07.884
met behulp van gezichtsherkenning een naam

00:04:07.884 --> 00:04:10.553
en openbaar beschikbare informatie

00:04:10.553 --> 00:04:12.485
over die naam en die persoon vinden,

00:04:12.485 --> 00:04:14.733
en uit die openbare informatie

00:04:14.733 --> 00:04:16.775
afgeschermde informatie afleiden,

00:04:16.775 --> 00:04:18.381
veel gevoeligere informatie,

00:04:18.381 --> 00:04:19.873
die je weer koppelt aan het gezicht?

00:04:19.873 --> 00:04:21.789
Het antwoord is ja, dat kunnen we,
en dat hebben we ook gedaan.

00:04:21.789 --> 00:04:24.357
Natuurlijk neemt de nauwkeurigheid steeds meer af.

00:04:24.357 --> 00:04:25.301
[27% van de eerste 5 cijfers van de sofinummers van de deelnemers geïdentificeerd (met 4 pogingen)]

00:04:25.301 --> 00:04:29.128
We ontwikkelden zelfs een iPhone app

00:04:29.128 --> 00:04:31.843
die gebruikmaakt van de camera van de telefoon

00:04:31.843 --> 00:04:33.443
om een foto te nemen van een deelnemer,

00:04:33.443 --> 00:04:34.930
die uploadt naar een wolk,

00:04:34.930 --> 00:04:37.592
en vervolgens doet wat ik jullie zojuist heb uitgelegd:

00:04:37.592 --> 00:04:39.680
zoeken naar een overeenkomst,
openbare informatie opsporen,

00:04:39.680 --> 00:04:41.410
gevoelige informatie afleiden,

00:04:41.410 --> 00:04:44.001
en vervolgens terugsturen naar de telefoon,

00:04:44.001 --> 00:04:47.610
en bij het gezicht voegen.

00:04:47.610 --> 00:04:49.511
Een voorbeeld van toegevoegde realiteit,

00:04:49.511 --> 00:04:51.962
een eng voorbeeld van toegevoegde realiteit.

00:04:51.962 --> 00:04:55.301
We hebben deze app niet ontwikkeld 
om ze beschikbaar te maken,

00:04:55.301 --> 00:04:57.223
maar alleen om te bewijzen dat het kan.

00:04:57.223 --> 00:04:59.536
Stel dat je deze technologieën neemt

00:04:59.536 --> 00:05:01.373
en ze tot het uiterste drijft.

00:05:01.373 --> 00:05:04.092
Stel je eens een toekomst voor waarin onbekenden

00:05:04.092 --> 00:05:06.403
je aankijken door hun Google-bril

00:05:06.403 --> 00:05:08.710
of, ooit, hun contactlenzen,

00:05:08.710 --> 00:05:12.730
en zeven of acht datapunten van jou gebruiken

00:05:12.730 --> 00:05:15.312
om alles af te leiden

00:05:15.312 --> 00:05:17.915
wat er bekend is over jou.

00:05:17.915 --> 00:05:22.709
Hoe zal deze toekomst zonder geheimen eruit zien?

00:05:22.709 --> 00:05:24.673
Moeten we ons daarover zorgen maken?

00:05:24.673 --> 00:05:26.564
We geloven misschien graag

00:05:26.564 --> 00:05:29.604
dat de toekomst, met die overvloed aan data,

00:05:29.604 --> 00:05:32.118
een toekomst zonder vooroordelen zal zijn.

00:05:32.118 --> 00:05:35.701
Beschikken over een grote hoeveelheid informatie,

00:05:35.701 --> 00:05:37.892
betekent niet dat we beslissingen gaan nemen

00:05:37.892 --> 00:05:39.598
die objectiever zijn.

00:05:39.598 --> 00:05:42.158
In een ander experiment 
legden we onze deelnemers

00:05:42.158 --> 00:05:44.404
informatie voor over een sollicitant.

00:05:44.404 --> 00:05:47.582
Deze informatie bevatte wat verwijzingen

00:05:47.582 --> 00:05:50.228
naar wat grappige, volledig juiste,

00:05:50.228 --> 00:05:52.693
maar wellicht lichtelijk gênante informatie

00:05:52.693 --> 00:05:54.713
die de deelnemer online had gedeeld.

00:05:54.713 --> 00:05:57.079
Het was interessant dat, van onze deelnemers,

00:05:57.079 --> 00:06:00.162
sommigen vergelijkbare informatie hadden gedeeld,

00:06:00.162 --> 00:06:02.524
en sommigen niet.

00:06:02.524 --> 00:06:04.473
Welke groep denk je,

00:06:04.473 --> 00:06:09.025
beoordeelde onze deelnemer strenger?

00:06:09.025 --> 00:06:10.982
Paradoxaal genoeg was het de groep

00:06:10.982 --> 00:06:12.715
die vergelijkbare informatie had gedeeld,

00:06:12.715 --> 00:06:15.657
een voorbeeld van morele dissonantie.

00:06:15.657 --> 00:06:17.407
Nu denk je misschien,

00:06:17.407 --> 00:06:19.109
dit gaat niet op voor mij,

00:06:19.109 --> 00:06:21.271
want ik heb niets te verbergen.

00:06:21.271 --> 00:06:23.753
Maar privacy gaat niet echt over

00:06:23.753 --> 00:06:27.429
het verbergen van iets negatiefs.

00:06:27.429 --> 00:06:29.783
Stel je eens voor dat jij de H.R. directeur bent

00:06:29.783 --> 00:06:32.730
van een bepaalde organisatie, en je ontvangt cv's,

00:06:32.730 --> 00:06:35.203
en je besluit meer informatie te zoeken 
over de kandidaten.

00:06:35.203 --> 00:06:37.663
Om die reden google je hun namen,

00:06:37.663 --> 00:06:39.903
en in een bepaalde werkelijkheid

00:06:39.903 --> 00:06:41.911
vind je déze informatie.

00:06:41.911 --> 00:06:46.348
In een alternatieve werkelijkheid
vind je déze informatie.

00:06:46.348 --> 00:06:49.065
Denk je dat het je niet zou uitmaken

00:06:49.065 --> 00:06:51.868
welke kandidaat je uitnodigt
voor een sollicitatiegesprek?

00:06:51.868 --> 00:06:54.150
Als je dat denkt, dan ben je niet

00:06:54.150 --> 00:06:56.732
zoals Amerikaanse werkgevers,

00:06:56.732 --> 00:07:00.039
die deel uitmaken van exact ditzelfde experiment.

00:07:00.039 --> 00:07:03.221
We creëerden Facebookprofielen,
manipuleerden eigenschappen,

00:07:03.221 --> 00:07:06.072
waarna we cv's begonnen te verzenden
naar bedrijven in de V.S.,

00:07:06.072 --> 00:07:07.980
en we achterhaalden, we controleerden,

00:07:07.980 --> 00:07:10.373
of ze op zoek waren naar onze kandidaten,

00:07:10.373 --> 00:07:12.205
en of ze reageerden op de informatie

00:07:12.205 --> 00:07:14.143
die ze vonden in sociale media. 
Dat deden ze.

00:07:14.143 --> 00:07:16.244
Ze discrimineerden op basis van sociale media

00:07:16.244 --> 00:07:19.317
bij even bekwame kandidaten.

00:07:19.317 --> 00:07:23.892
Marketeers doen ons graag geloven

00:07:23.892 --> 00:07:26.161
dat alle informatie over ons

00:07:26.161 --> 00:07:29.434
altijd in ons voordeel gebruikt zal worden.

00:07:29.434 --> 00:07:33.149
Maar denk eens na: 
waarom zou dat altijd het geval zijn?

00:07:33.149 --> 00:07:35.813
'Minority Report', een film van een paar jaar 
geleden, heeft een beroemde scène

00:07:35.813 --> 00:07:38.366
'Minority Report', een film van een paar jaar 
geleden, heeft een beroemde scène

00:07:38.366 --> 00:07:40.942
waarin Tom Cruise in een winkelcentrum loopt

00:07:40.942 --> 00:07:44.718
en er holografische, gepersonaliseerde reclames

00:07:44.718 --> 00:07:46.553
om hem heen verschijnen.

00:07:46.553 --> 00:07:49.780
Die film speelde zich af in 2054,

00:07:49.780 --> 00:07:51.422
binnen ongeveer 40 jaar dus,

00:07:51.422 --> 00:07:54.330
en hoe opwindend die technologie ook lijkt,

00:07:54.330 --> 00:07:56.976
het onderschat al enorm

00:07:56.976 --> 00:07:59.116
hoeveel informatie organisaties

00:07:59.116 --> 00:08:01.599
over jou te weten komen,
en hoe ze het kunnen gebruiken

00:08:01.599 --> 00:08:04.997
om je te beïnvloeden op zó'n manier,
dat je het niet eens doorhebt.

00:08:04.997 --> 00:08:07.100
Ter illustratie nog een lopend,

00:08:07.100 --> 00:08:09.373
nog niet voltooid experiment.

00:08:09.373 --> 00:08:11.692
Stel je een organisatie voor die toegang heeft

00:08:11.692 --> 00:08:13.748
tot je vriendenlijst op Facebook.

00:08:13.748 --> 00:08:15.520
Met een bepaald algoritme

00:08:15.520 --> 00:08:19.254
kunnen ze achterhalen 
wie je twee beste vrienden zijn.

00:08:19.254 --> 00:08:21.534
Vervolgens creëren ze, in het echt,

00:08:21.534 --> 00:08:24.376
een samenstelling van de gezichten van je vrienden.

00:08:24.376 --> 00:08:27.445
Eerder onderzoeken toonden aan dat mensen

00:08:27.445 --> 00:08:30.330
zichzelf dan niet meer herkennen

00:08:30.330 --> 00:08:32.792
in de samenstelling, maar ze reageren er wel

00:08:32.792 --> 00:08:34.909
op een positieve manier op.

00:08:34.909 --> 00:08:38.324
Als je binnenkort op zoek bent
naar een bepaald product,

00:08:38.324 --> 00:08:40.883
en reclame ziet die je aanmoedigt om het te kopen,

00:08:40.883 --> 00:08:43.790
dan zal dat niet een gemiddelde woordvoerder zijn,

00:08:43.790 --> 00:08:46.103
maar één van je vrienden,

00:08:46.103 --> 00:08:49.406
en je zal het niet eens doorhebben dat dat gebeurt.

00:08:49.406 --> 00:08:51.819
Het probleem is dat

00:08:51.819 --> 00:08:54.338
onze huidige beleidsmechanismen

00:08:54.338 --> 00:08:57.776
om onszelf te beschermen tegen
misbruik van persoonlijke informatie,

00:08:57.776 --> 00:09:00.760
volstrekt nutteloos zijn.

00:09:00.760 --> 00:09:03.673
Eén van deze mechanismen is transparantie:

00:09:03.673 --> 00:09:06.873
aan mensen vertellen
wat je gaat doen met hun data.

00:09:06.873 --> 00:09:08.979
In principe is dat iets heel goeds.

00:09:08.979 --> 00:09:12.646
Het is noodzakelijk, maar het is niet voldoende.

00:09:12.646 --> 00:09:16.344
Transparantie kan de verkeerde kant uitgaan.

00:09:16.344 --> 00:09:18.448
Je kunt mensen wel vertellen wat je gaat doen,

00:09:18.448 --> 00:09:20.680
en ze toch porren willekeurige hoeveelheden 
persoonlijke informatie prijs te geven.

00:09:20.680 --> 00:09:23.303
en ze toch porren willekeurige hoeveelheden 
persoonlijke informatie prijs te geven.

00:09:23.303 --> 00:09:26.189
In nog een ander experiment, ditmaal met studenten,

00:09:26.189 --> 00:09:29.247
vroegen we om informatie te geven

00:09:29.247 --> 00:09:31.060
over hun gedrag op de campus,

00:09:31.060 --> 00:09:34.000
inclusief vrij gevoelige vragen, zoals de volgende.

00:09:34.000 --> 00:09:34.621
[Spiekte je ooit op een examen?]

00:09:34.621 --> 00:09:36.921
De ene groep vertelden we:

00:09:36.921 --> 00:09:39.762
"Alleen medestudenten zullen
je antwoorden te zien krijgen."

00:09:39.762 --> 00:09:41.341
Een andere groep vertelden we:

00:09:41.341 --> 00:09:44.902
"Zowel studenten als de faculteit zullen
je antwoorden te zien krijgen."

00:09:44.902 --> 00:09:47.493
Transparantie. Bekendmaking.
En inderdaad, het werkte:

00:09:47.493 --> 00:09:48.900
de eerste deelnemersgroep

00:09:48.900 --> 00:09:51.468
gaf veel sneller informatie prijs.

00:09:51.468 --> 00:09:52.988
Dat is logisch, toch?

00:09:52.988 --> 00:09:54.478
Maar toen voegden we de misleiding toe.

00:09:54.478 --> 00:09:57.238
We herhaalden het experiment
met dezelfde groepen,

00:09:57.238 --> 00:09:59.665
maar ditmaal met een vertraging

00:09:59.665 --> 00:10:02.600
tussen het moment dat we de deelnemers vertelden

00:10:02.600 --> 00:10:04.680
hoe we hun data zouden gebruiken

00:10:04.680 --> 00:10:09.068
en het moment waarop we daadwerkelijk
de vragen afnamen.

00:10:09.068 --> 00:10:11.629
Hoeveel vertraging denk je dat er nodig was

00:10:11.629 --> 00:10:16.242
om het remmende effect teniet te doen

00:10:16.242 --> 00:10:19.653
van de kennis dat de faculteit je antwoorden
te zien krijgt?

00:10:19.653 --> 00:10:21.433
Tien minuten?

00:10:21.433 --> 00:10:23.224
Vijf minuten?

00:10:23.224 --> 00:10:25.000
Eén minuut?

00:10:25.000 --> 00:10:27.049
Wat dacht je van 15 seconden?

00:10:27.049 --> 00:10:29.717
Vijftien seconden was genoeg om de twee groepen

00:10:29.717 --> 00:10:31.285
evenveel informatie te laten onthullen,

00:10:31.285 --> 00:10:34.031
alsof het de tweede groep niet meer uitmaakte

00:10:34.031 --> 00:10:36.687
dat de faculteit hun antwoorden te lezen zou krijgen.

00:10:36.687 --> 00:10:40.023
Ik moet toegeven dat deze toespraak tot nu toe

00:10:40.023 --> 00:10:42.503
misschien buitengewoon somber klinkt,

00:10:42.503 --> 00:10:44.224
maar dat is niet mijn punt.

00:10:44.224 --> 00:10:46.923
Ik wil dat jullie weten dat er alternatieven zijn.

00:10:46.923 --> 00:10:48.695
Ik wil dat jullie weten dat er alternatieven zijn.

00:10:48.695 --> 00:10:51.194
De manier waarop we dingen nu doen, 
is niet de enige mogelijke manier,

00:10:51.194 --> 00:10:54.231
en zeker niet de beste manier

00:10:54.231 --> 00:10:56.258
waarop het kan gedaan worden.

00:10:56.258 --> 00:11:00.429
Als iemand tegen je zegt:
"Mensen geven niet om privacy,"

00:11:00.429 --> 00:11:03.071
ga dan na of het spelletje dusdanig ontworpen

00:11:03.071 --> 00:11:05.795
en gemanipuleerd is, 
dat ze niet om privacy kunnen geven.

00:11:05.795 --> 00:11:09.057
Als je tot het besef komt dat
deze manipulaties gebeuren,

00:11:09.057 --> 00:11:10.664
ben je al halverwege het proces

00:11:10.664 --> 00:11:12.922
om jezelf te beschermen.

00:11:12.922 --> 00:11:16.632
Als iemand je vertelt dat privacy onverenigbaar is

00:11:16.632 --> 00:11:18.481
met de voordelen van grote hoeveelheden data,

00:11:18.481 --> 00:11:20.954
bedenk dan dat onderzoekers in de laatste 20 jaar

00:11:20.954 --> 00:11:22.871
technologieën hebben gecreëerd

00:11:22.871 --> 00:11:26.189
die het mogelijk maken dat vrijwel
elke elektronische transactie

00:11:26.189 --> 00:11:29.938
gebeurt op een
privacybeschermende manier.

00:11:29.938 --> 00:11:32.493
We kunnen anoniem surfen op het internet.

00:11:32.493 --> 00:11:35.171
We kunnen e-mails versturen die alleen
gelezen kunnen worden

00:11:35.171 --> 00:11:38.880
door de bedoelde ontvanger, niet eens de NSA.

00:11:38.880 --> 00:11:41.877
We kunnen zelfs op privacybeschermende wijze
doen aan datamining.

00:11:41.877 --> 00:11:45.771
Met andere woorden, we kunnen profiteren
van grote hoeveelheden data,

00:11:45.771 --> 00:11:47.903
terwijl we onze privacy beschermen.

00:11:47.903 --> 00:11:51.694
Deze technologieën betekenen wel
een verschuiving

00:11:51.694 --> 00:11:53.240
van kosten en opbrengsten

00:11:53.240 --> 00:11:55.347
tussen gegevenshouders en de betrokkenen,

00:11:55.347 --> 00:11:58.800
wat misschien de reden is dat
je niet erg veel over ze hoort.

00:11:58.800 --> 00:12:02.506
Dat brengt me terug bij het Hof van Eden.

00:12:02.506 --> 00:12:05.286
Er is nog een tweede interpretatie van de privacy

00:12:05.286 --> 00:12:07.095
in het verhaal van het Hof van Eden,

00:12:07.095 --> 00:12:09.191
en die heeft niets te maken met het feit dat

00:12:09.191 --> 00:12:11.416
Adam en Eva zich naakt voelen

00:12:11.416 --> 00:12:13.797
en zich schamen.

00:12:13.797 --> 00:12:16.578
Je kunt de naklank van deze interpretatie vinden

00:12:16.578 --> 00:12:19.360
in John Milton's "Paradise Lost".

00:12:19.360 --> 00:12:23.557
In de tuin worden Adam en Eva
voorzien in al hun behoeftes.

00:12:23.557 --> 00:12:25.661
Ze zijn blij. Ze zijn tevreden.

00:12:25.661 --> 00:12:27.954
Maar het ontbreekt ze aan kennis

00:12:27.954 --> 00:12:29.594
en zelfbewustzijn.

00:12:29.594 --> 00:12:32.913
Pas op het moment dat ze 
de zo treffend genoemde

00:12:32.913 --> 00:12:34.206
'vrucht van kennis' opeten,

00:12:34.206 --> 00:12:36.811
ontdekken ze zichzelf.

00:12:36.811 --> 00:12:40.842
Ze worden bewust. Ze worden autonoom.

00:12:40.842 --> 00:12:43.968
Daarvoor moeten ze echter wel de tuin verlaten.

00:12:43.968 --> 00:12:47.849
Privacy is dus zowel het middel

00:12:47.849 --> 00:12:50.811
als de prijs die we betalen voor vrijheid.

00:12:50.811 --> 00:12:53.581
Nogmaals, marketeers vertellen ons

00:12:53.581 --> 00:12:56.600
dat grote hoeveelheden data en sociale media

00:12:56.600 --> 00:12:59.579
niet alleen een winstparadijs voor hen is,

00:12:59.579 --> 00:13:02.036
maar ook een Hof van Eden voor de rest van ons.

00:13:02.036 --> 00:13:03.274
We krijgen gratis content.

00:13:03.274 --> 00:13:06.397
We kunnen Angry Birds spelen.
We krijgen doelgerichte apps.

00:13:06.397 --> 00:13:09.294
Maar in feite zullen organisaties over een paar jaar

00:13:09.294 --> 00:13:10.903
zoveel over ons weten,

00:13:10.903 --> 00:13:13.613
dat ze onze wensen zullen kunnen afleiden

00:13:13.613 --> 00:13:15.817
nog voor we ze hebben gevormd, en misschien

00:13:15.817 --> 00:13:18.264
namens ons producten kopen,

00:13:18.264 --> 00:13:20.538
nog voordat we weten dat we ze nodig hebben.

00:13:20.538 --> 00:13:23.775
Er was één Engelse auteur

00:13:23.775 --> 00:13:26.820
die anticipeerde op het soort toekomst

00:13:26.820 --> 00:13:28.225
waarin we onze autonomie en vrijheid

00:13:28.225 --> 00:13:31.773
zouden verhandelen voor comfort.

00:13:31.773 --> 00:13:33.934
Meer nog dan George Orwell,

00:13:33.934 --> 00:13:36.695
want ik heb het natuurlijk over Aldous Huxley.

00:13:36.695 --> 00:13:39.549
In "Brave New World"
beeldt hij zich een maatschappij in

00:13:39.549 --> 00:13:41.720
waarin technologieën die we oorspronkelijk

00:13:41.720 --> 00:13:43.579
creëerden voor vrijheid,

00:13:43.579 --> 00:13:46.146
ons uiteindelijk gaan bedwingen.

00:13:46.146 --> 00:13:50.937
Hij biedt ons in het boek ook een uitweg

00:13:50.937 --> 00:13:54.375
van die maatschappij, vergelijkbaar met het pad

00:13:54.375 --> 00:13:58.330
dat Adam en Eva moesten volgen
om het Hof te verlaten.

00:13:58.330 --> 00:14:00.477
In de woorden van de Savage,

00:14:00.477 --> 00:14:03.546
het is mogelijk om autonomie en vrijheid
terug te krijgen,

00:14:03.546 --> 00:14:06.225
maar daar hangt een duur prijskaartje aan.

00:14:06.225 --> 00:14:11.940
Ik geloof dus dat één van de bepalende gevechten

00:14:11.940 --> 00:14:14.503
van onze tijd, het gevecht

00:14:14.503 --> 00:14:16.890
om de controle van persoonlijke informatie zal zijn,

00:14:16.890 --> 00:14:20.397
het gevecht dat bepaalt of
grote hoeveelheden data een kracht worden

00:14:20.397 --> 00:14:21.686
voor vrijheid,

00:14:21.686 --> 00:14:26.432
in plaats van een kracht die ons
ongemerkt manipuleert.

00:14:26.432 --> 00:14:29.025
Op dit moment weten velen van ons

00:14:29.025 --> 00:14:31.778
niet eens dat dit gevecht gaande is,

00:14:31.778 --> 00:14:34.450
maar dat is het wel, of je dat nou leuk vindt of niet.

00:14:34.450 --> 00:14:37.254
Om te voorkomen dat wij de slang gaan spelen,

00:14:37.254 --> 00:14:40.151
zal ik je vertellen dat de middelen voor dit gevecht

00:14:40.151 --> 00:14:43.160
in je hoofd zitten, 
het besef van wat er gaande is,

00:14:43.160 --> 00:14:44.515
en in je handen,

00:14:44.515 --> 00:14:48.255
want het scheelt slechts een paar klikken.

00:14:48.255 --> 00:14:49.737
Dankjewel.

00:14:49.737 --> 00:14:54.214
(Applaus)

