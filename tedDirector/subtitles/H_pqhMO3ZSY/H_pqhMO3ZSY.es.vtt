WEBVTT
Kind: captions
Language: es

00:00:00.000 --> 00:00:07.000
Traductor: Sebastian Betti
Revisor: Carlos Arturo Morales

00:00:12.000 --> 00:00:14.540
Quiero contarles una historia

00:00:15.702 --> 00:00:18.171
que conecta el célebre 
incidente de privacidad

00:00:18.171 --> 00:00:20.940
que involucró a Adán y Eva,

00:00:20.940 --> 00:00:24.386
y el cambio notorio de los límites

00:00:24.386 --> 00:00:27.072
entre lo público y lo privado

00:00:27.072 --> 00:00:28.842
ocurrido en los últimos 10 años.

00:00:28.842 --> 00:00:30.140
Ya conocen el incidente.

00:00:30.140 --> 00:00:33.470
Adán y Eva, un día
en el Jardín del Edén,

00:00:33.470 --> 00:00:35.313
se dan cuenta de 
que están desnudos.

00:00:35.313 --> 00:00:36.813
Se asustan.

00:00:36.813 --> 00:00:39.570
Y el resto es historia.

00:00:39.570 --> 00:00:41.758
Hoy en día, Adán y Eva

00:00:41.758 --> 00:00:44.119
probablemente actuarían diferente.

00:00:44.119 --> 00:00:46.387
[@Adan ¡Lo de anoche fue genial!
Me encantó la manzana LOL] (Risas)

00:00:46.387 --> 00:00:48.260
(Risas) [@Eva Sip... bebé, ¿sabes 
qué le pasó a mis pantalones?]

00:00:48.260 --> 00:00:50.896
Revelamos mucha más información

00:00:50.896 --> 00:00:54.230
personal en línea que nunca antes,

00:00:54.230 --> 00:00:55.934
y las empresas recolectan mucha más

00:00:55.934 --> 00:00:58.158
información sobre nosotros.

00:00:58.158 --> 00:01:01.440
Podemos ganar mucho 
y beneficiarnos

00:01:01.440 --> 00:01:03.886
del análisis masivo 
de esta información personal,

00:01:03.886 --> 00:01:05.832
o "big data",

00:01:05.832 --> 00:01:07.744
pero también hay un precio grande 
que pagar en términos de privacidad,

00:01:07.744 --> 00:01:09.656
[Salí por una pizza]
[Mis amigos y la NSA]

00:01:09.656 --> 00:01:11.568
en contraprestación.

00:01:11.568 --> 00:01:15.591
Mi historia trata de esas 
contraprestaciones.

00:01:15.591 --> 00:01:18.175
Empecemos con un verdad
que, en mi opinión,

00:01:18.175 --> 00:01:21.502
se ha hecho cada vez más evidente
en los últimos años:

00:01:21.502 --> 00:01:23.599
toda información personal

00:01:23.599 --> 00:01:25.884
puede volverse sensible.

00:01:25.884 --> 00:01:30.009
En el 2000, se tomaron
unos 100 000 millones

00:01:30.009 --> 00:01:31.921
de fotos en el mundo,

00:01:31.921 --> 00:01:34.986
pero solo una ínfima parte de ellas

00:01:34.986 --> 00:01:36.869
se subíeron a la Web.

00:01:36.869 --> 00:01:40.230
En el 2010, solo en Facebook, 
en un solo mes,

00:01:40.230 --> 00:01:43.500
se subieron 2500 millones de fotos,

00:01:43.500 --> 00:01:45.382
la mayoría identificadas.

00:01:45.382 --> 00:01:47.262
En el mismo lapso de tiempo,

00:01:47.262 --> 00:01:52.132
la capacidad de las computadoras 
para reconocer personas en las fotos

00:01:52.132 --> 00:01:55.740
mejoró en tres órdenes de magnitud.

00:01:55.740 --> 00:01:57.622
¿Qué ocurre cuando uno combina

00:01:57.622 --> 00:01:59.123
estas tecnologías:

00:01:59.123 --> 00:02:01.781
aumento de la disponibilidad 
de datos faciales;

00:02:01.781 --> 00:02:05.429
mejora en el reconocimiento 
facial informático;

00:02:05.429 --> 00:02:07.611
y también la computación en la nube,

00:02:07.611 --> 00:02:09.499
que nos da a todos 
los presentes en la sala

00:02:09.499 --> 00:02:11.059
el poder computacional

00:02:11.059 --> 00:02:12.945
que hace unos años 
estaba disponible

00:02:12.945 --> 00:02:14.727
solo para agencias gubernamentales;

00:02:14.727 --> 00:02:16.105
y la computación ubicua,

00:02:16.105 --> 00:02:18.997
que le permite a mi móvil, 
que no es una supercomputadora,

00:02:18.997 --> 00:02:20.668
conectarse a Internet

00:02:20.668 --> 00:02:23.002
y tomar allí cientos de miles

00:02:23.002 --> 00:02:25.641
de medidas faciales 
en unos pocos segundos?

00:02:25.641 --> 00:02:28.269
Bueno, pensamos que el resultado

00:02:28.269 --> 00:02:30.333
de esta combinación de tecnologías

00:02:30.333 --> 00:02:33.221
será un cambio radical 
en nuestras nociones

00:02:33.221 --> 00:02:35.478
de privacidad y anonimato.

00:02:35.478 --> 00:02:37.471
Para comprobalo 
hicimos un experimento

00:02:37.471 --> 00:02:39.592
en el campus del Carnegie Mellon.

00:02:39.592 --> 00:02:41.691
Le pedimos a los estudiantes 
que pasaban por allí

00:02:41.691 --> 00:02:43.470
que participaran en un estudio,

00:02:43.470 --> 00:02:46.032
les tomamos una foto 
con una cámara web

00:02:46.032 --> 00:02:48.814
y les pedimos que completaran 
una encuesta en el portátil.

00:02:48.814 --> 00:02:50.793
Mientras completaban la encuesta,

00:02:50.793 --> 00:02:53.590
subimos la foto a un clúster 
de computación en la nube,

00:02:53.590 --> 00:02:55.317
y usamos 
un reconocedor facial

00:02:55.317 --> 00:02:57.722
para cotejar esa foto 
con una base de datos

00:02:57.722 --> 00:03:00.115
de cientos de miles 
de imágenes

00:03:00.115 --> 00:03:03.711
que habíamos bajado 
de perfiles de Facebook.

00:03:03.711 --> 00:03:06.970
Para cuando el sujeto 
llegaba al final

00:03:06.970 --> 00:03:10.317
de la encuesta, la página se había 
actualizado en forma dinámica

00:03:10.317 --> 00:03:12.630
con las 10 fotos 
encontradas por el reconocedor

00:03:12.630 --> 00:03:14.915
que mejor concordaban,

00:03:14.915 --> 00:03:16.653
y le pedímos al sujeto que indicara

00:03:16.653 --> 00:03:20.773
si se encontraba en la foto.

00:03:20.773 --> 00:03:24.472
¿Ven al sujeto?

00:03:24.472 --> 00:03:27.317
Bueno, la computadora sí, 
y de hecho,

00:03:27.317 --> 00:03:29.466
reconoció
1 de cada 3 sujetos.

00:03:29.466 --> 00:03:32.650
En esencia, podemos partir 
de un rostro anónimo,

00:03:32.650 --> 00:03:36.134
en disco o en la web, y usar 
el reconocimiento facial

00:03:36.134 --> 00:03:38.494
para ponerle nombre 
a ese rostro anónimo

00:03:38.494 --> 00:03:40.602
gracias a las redes sociales.

00:03:40.602 --> 00:03:42.474
Pero hace unos años hicimos algo más.

00:03:42.474 --> 00:03:44.297
Partimos de datos de redes sociales,

00:03:44.297 --> 00:03:47.348
los combinamos estadísticamente 
con datos

00:03:47.348 --> 00:03:49.450
de la seguridad social 
del gobierno de EE.UU.

00:03:49.450 --> 00:03:52.774
y terminamos prediciendo los
números de la seguridad social,

00:03:52.774 --> 00:03:54.286
que en Estados Unidos

00:03:54.286 --> 00:03:56.326
son una información 
extremadamente sensible.

00:03:56.326 --> 00:03:58.419
¿Ven a dónde quiero llegar con esto?

00:03:58.419 --> 00:04:01.341
Si combinan los dos estudios,

00:04:01.341 --> 00:04:02.853
entonces la pregunta pasa a ser:

00:04:02.853 --> 00:04:05.573
¿Podemos partir de un rostro

00:04:05.573 --> 00:04:07.884
y mediante reconocimiento 
facial, hallar un nombre

00:04:07.884 --> 00:04:10.553
e información pública

00:04:10.553 --> 00:04:12.485
sobre ese nombre y esa persona,

00:04:12.485 --> 00:04:14.733
y a partir de esa información pública

00:04:14.733 --> 00:04:16.775
inferir información no pública,

00:04:16.775 --> 00:04:18.381
mucho más sensible,

00:04:18.381 --> 00:04:19.873
para luego asociarla 
a aquel rostro?

00:04:19.873 --> 00:04:21.789
La respuesta es sí se puede, 
y lo hicimos.

00:04:21.789 --> 00:04:24.357
Por supuesto, la precisión 
va desmejorando a cada paso.

00:04:24.357 --> 00:04:25.301
[Se identificó el 27% de los 5 primeros dígitos
del SSN de los sujetos (con 4 intentos)]

00:04:25.301 --> 00:04:29.128
Decidimos incluso, 
desarrollar una app para iPhone,

00:04:29.128 --> 00:04:31.843
que usa la cámara interna del móvil

00:04:31.843 --> 00:04:33.443
para tomar una foto del sujeto

00:04:33.443 --> 00:04:34.930
y subirla a la nube,

00:04:34.930 --> 00:04:37.592
y luego hacer lo que les describí
en tiempo real:

00:04:37.592 --> 00:04:39.680
cotejarla, encontrar 
información pública,

00:04:39.680 --> 00:04:41.410
tratar de inferir 
información sensible,

00:04:41.410 --> 00:04:44.001
y luego enviarla 
nuevamente al móvil

00:04:44.001 --> 00:04:47.610
para ser superpuesta en 
el rostro del sujeto,

00:04:47.610 --> 00:04:49.511
un ejemplo de realidad aumentada,

00:04:49.511 --> 00:04:51.962
quizá un ejemplo escalofriante 
de realidad aumentada.

00:04:51.962 --> 00:04:55.301
De hecho, no desarrollamos la app 
para que estuviera disponible,

00:04:55.301 --> 00:04:57.223
sino como una prueba de concepto.

00:04:57.223 --> 00:04:59.536
Incluso tomamos estas tecnologías

00:04:59.536 --> 00:05:01.373
y las llevamos al extremo lógico.

00:05:01.373 --> 00:05:04.092
Imaginen un futuro en el que 
los extraños que los rodeen

00:05:04.092 --> 00:05:06.403
los miren con sus gafas Google

00:05:06.403 --> 00:05:08.710
o, algún día, con sus lentes de contacto,

00:05:08.710 --> 00:05:12.730
y usen 7 o 8 datos de ustedes

00:05:12.730 --> 00:05:15.312
para inferir todo lo demás

00:05:15.312 --> 00:05:17.915
que pueda saberse.

00:05:17.915 --> 00:05:22.709
¿Cómo será 
ese futuro sin secretos?

00:05:22.709 --> 00:05:24.673
¿Debería importarnos?

00:05:24.673 --> 00:05:26.564
Nos gustaría creer

00:05:26.564 --> 00:05:29.604
que un futuro con tanta 
riqueza de datos

00:05:29.604 --> 00:05:32.118
sería un futuro sin más prejuicios,

00:05:32.118 --> 00:05:35.701
pero, de hecho, contar 
con tanta información

00:05:35.701 --> 00:05:37.892
no significa que 
tomaremos decisiones

00:05:37.892 --> 00:05:39.598
más objetivas.

00:05:39.598 --> 00:05:42.158
En otro experimento, 
presentamos a nuestros sujetos

00:05:42.158 --> 00:05:44.404
información sobre un 
potencial candidato laboral.

00:05:44.404 --> 00:05:47.582
incluimos algunas referencias

00:05:47.582 --> 00:05:50.228
a cierta información 
totalmente legal,

00:05:50.228 --> 00:05:52.693
divertida pero 
un poco embarazosa,

00:05:52.693 --> 00:05:54.713
que el candidato había 
publicado en línea.

00:05:54.713 --> 00:05:57.079
Curiosamente, 
entre nuestros sujetos

00:05:57.079 --> 00:06:00.162
algunos habían publicado 
información similar

00:06:00.162 --> 00:06:02.524
y otros no.

00:06:02.524 --> 00:06:04.473
¿Qué grupo ceeen

00:06:04.473 --> 00:06:09.025
que mostró propensión a juzgar 
con más severidad a nuestro sujeto?

00:06:09.025 --> 00:06:10.982
Paradójicamente, fue el grupo

00:06:10.982 --> 00:06:12.715
que había publicado 
información similar,

00:06:12.715 --> 00:06:15.657
un ejemplo de disonancia moral.

00:06:15.657 --> 00:06:17.407
Quizá estén pensando

00:06:17.407 --> 00:06:19.109
que eso no los afecta a Uds.

00:06:19.109 --> 00:06:21.271
porque no tienen nada que ocultar.

00:06:21.271 --> 00:06:23.753
Pero, de hecho, 
la privacidad no tiene que ver

00:06:23.753 --> 00:06:27.429
con tener algo
negativo que ocultar.

00:06:27.429 --> 00:06:29.783
Imaginen que son 
el director de RR.HH.

00:06:29.783 --> 00:06:32.730
de cierta empresa, 
que reciben unas hojas de vida

00:06:32.730 --> 00:06:35.203
y deciden buscar más información 
sobre los candidatos.

00:06:35.203 --> 00:06:37.663
Entonces, googlean sus nombres

00:06:37.663 --> 00:06:39.903
y en determinado universo

00:06:39.903 --> 00:06:41.911
encuentran esta información--

00:06:41.911 --> 00:06:46.348
O en un universo paralelo, 
encuentran esta información.

00:06:46.348 --> 00:06:49.065
¿Creen que todos los candidatos 
tendrían con Uds.

00:06:49.065 --> 00:06:51.868
la misma oportunidad de ser 
llamados para una entrevista?

00:06:51.868 --> 00:06:54.150
Si piensan que sí, 
entonces no son

00:06:54.150 --> 00:06:56.732
como los empleadores de 
EE.UU. ya que, de hecho,

00:06:56.732 --> 00:07:00.039
en algún punto de nuestro experimento 
hicimos exactamente eso.

00:07:00.039 --> 00:07:03.221
Creamos perfiles de Facebook, 
manipulamos los rasgos,

00:07:03.221 --> 00:07:06.072
y luego empezamos a enviar 
hojas de vida a empresas en EE.UU.,

00:07:06.072 --> 00:07:07.980
y detectamos, monitoreamos,

00:07:07.980 --> 00:07:10.373
si estaban buscando información
de nuestros candidatos,

00:07:10.373 --> 00:07:12.205
y si actuaban con base 
en la información

00:07:12.205 --> 00:07:14.143
que encontraban en los 
medios sociales. Y lo hicieron.

00:07:14.143 --> 00:07:16.244
Se discriminó con base 
en los medios sociales

00:07:16.244 --> 00:07:19.317
a candidatos 
con iguales habilidades.

00:07:19.317 --> 00:07:23.892
Los vendedores quieren 
que creamos

00:07:23.892 --> 00:07:26.161
que toda la información 
sobre nosotros siempre

00:07:26.161 --> 00:07:29.434
será usada a nuestro favor.

00:07:29.434 --> 00:07:33.149
Pero piensen de nuevo. ¿Por qué 
habría de ser siempre así?

00:07:33.149 --> 00:07:35.813
En una película 
de hace unos años,

00:07:35.813 --> 00:07:38.366
"Minority Report", 
en una escena famosa

00:07:38.366 --> 00:07:40.942
Tom Cruise camina 
por un centro comercial

00:07:40.942 --> 00:07:44.718
rodeado de publicidad

00:07:44.718 --> 00:07:46.553
holográfica personalizada.

00:07:46.553 --> 00:07:49.780
La película transcurre en el 2054,

00:07:49.780 --> 00:07:51.422
dentro de unos 40 años,

00:07:51.422 --> 00:07:54.330
y aunque la tecnología 
luce emocionante,

00:07:54.330 --> 00:07:56.976
subestima en mucho

00:07:56.976 --> 00:07:59.116
la cantidad de información 
que las organizaciones

00:07:59.116 --> 00:08:01.599
pueden recolectar sobre nosotros, 
y la forma de usarla

00:08:01.599 --> 00:08:04.997
para influir en nosotros 
de maneras imperceptibles.

00:08:04.997 --> 00:08:07.100
Como ejemplo, 
está este otro experimento

00:08:07.100 --> 00:08:09.373
que estamos haciendo
y que todavía no terminamos.

00:08:09.373 --> 00:08:11.692
Imaginemos que una 
organización tiene acceso

00:08:11.692 --> 00:08:13.748
a tu lista de amigos de Facebook,

00:08:13.748 --> 00:08:15.520
y mediante algún tipo de algoritmo

00:08:15.520 --> 00:08:19.254
puede identificar 
sus dos mejores amigos.

00:08:19.254 --> 00:08:21.534
Que luego crean, en tiempo real,

00:08:21.534 --> 00:08:24.376
un rostro compuesto 
de estos dos amigos.

00:08:24.376 --> 00:08:27.445
Estudios previos al nuestro 
han demostrado que las personas

00:08:27.445 --> 00:08:30.330
no se reconocen ni a sí mismas

00:08:30.330 --> 00:08:32.792
en rostros compuestos, 
pero reaccionan

00:08:32.792 --> 00:08:34.909
a esas composiciones 
de manera positiva.

00:08:34.909 --> 00:08:38.324
Y entonces, la próxima vez 
que busquen algún producto

00:08:38.324 --> 00:08:40.883
y que haya una publicidad 
sugiriéndoles que lo compren,

00:08:40.883 --> 00:08:43.790
no será un vendedor común.

00:08:43.790 --> 00:08:46.103
Será uno de sus amigos,

00:08:46.103 --> 00:08:49.406
y ni siquiera sabrán 
lo que está pasando.

00:08:49.406 --> 00:08:51.819
El problema es que

00:08:51.819 --> 00:08:54.338
los mecanismos que tenemos
en la política acutal

00:08:54.338 --> 00:08:57.776
para la protección contra los abusos 
de la información personal

00:08:57.776 --> 00:09:00.760
son como llevar un cuchillo 
a un tiroteo.

00:09:00.760 --> 00:09:03.673
Uno de estos mecanismos 
es la transparencia,

00:09:03.673 --> 00:09:06.873
decirle a las personas 
lo que uno va a hacer con sus datos.

00:09:06.873 --> 00:09:08.979
En principio, 
eso es algo muy bueno.

00:09:08.979 --> 00:09:12.646
Es necesario, 
pero no es suficiente.

00:09:12.646 --> 00:09:16.344
La transparencia 
puede estar mal dirigida.

00:09:16.344 --> 00:09:18.448
Uno puede contarle
a la gente lo que hará,

00:09:18.448 --> 00:09:20.680
y luego empujarlos a revelar

00:09:20.680 --> 00:09:23.303
cantidades arbitrarias 
de información personal.

00:09:23.303 --> 00:09:26.189
Por eso en un experimento más, 
este con estudiantes,

00:09:26.189 --> 00:09:29.247
les pedimos que 
nos dieran información

00:09:29.247 --> 00:09:31.060
sobre su comportamiento 
en el campus,

00:09:31.060 --> 00:09:34.000
formulándoles preguntas 
tan sensibles como estas.

00:09:34.000 --> 00:09:34.621
[¿Alguna vez te copiaste en un examen?]

00:09:34.621 --> 00:09:36.921
A un grupo de sujetos les dijimos:

00:09:36.921 --> 00:09:39.762
"Solo otro grupo de estudiantes 
verá sus respuestas".

00:09:39.762 --> 00:09:41.341
A otro grupo de sujetos 
les dijimos:

00:09:41.341 --> 00:09:44.902
"Sus respuestas serán vistas 
por estudiantes y profesores".

00:09:44.902 --> 00:09:47.493
Transparencia. Notificación. 
Y por supuesto, esto funcionó,

00:09:47.493 --> 00:09:48.900
en el sentido de que el 
primer grupo de sujetos

00:09:48.900 --> 00:09:51.468
fue mucho más propenso a revelar 
información que el segundo.

00:09:51.468 --> 00:09:52.988
Tiene sentido, ¿no?

00:09:52.988 --> 00:09:54.478
Pero luego añadimos 
un distractor.

00:09:54.478 --> 00:09:57.238
Repetimos el experimento 
con los mismos dos grupos,

00:09:57.238 --> 00:09:59.665
esta vez añadiendo una demora

00:09:59.665 --> 00:10:02.600
entre el tiempo en que 
le dijimos a los sujetos

00:10:02.600 --> 00:10:04.680
cómo usaríamos sus datos

00:10:04.680 --> 00:10:09.068
y el tiempo en que empezamos
a [formular] las preguntas.

00:10:09.068 --> 00:10:11.629
¿Cuánta demora creen 
que tuvimos que añadir

00:10:11.629 --> 00:10:16.242
para anular el efecto inhibidor

00:10:16.242 --> 00:10:19.653
de saber que los profesores 
verían sus respuestas?

00:10:19.653 --> 00:10:21.433
¿10 minutos?

00:10:21.433 --> 00:10:23.224
¿5 minutos?

00:10:23.224 --> 00:10:25.000
¿1 minuto?

00:10:25.000 --> 00:10:27.049
¿Qué tal 15 segundos?

00:10:27.049 --> 00:10:29.717
Quince segundos fueron suficientes 
para que ambos grupos

00:10:29.717 --> 00:10:31.285
revelaran la misma cantidad 
de información,

00:10:31.285 --> 00:10:34.031
como si al segundo grupo 
no le importara más

00:10:34.031 --> 00:10:36.687
que los profesores 
leyeran sus respuestas.

00:10:36.687 --> 00:10:40.023
Tengo que admitir que 
esta charla hasta ahora

00:10:40.023 --> 00:10:42.503
puede sonar en extremo negativa,

00:10:42.503 --> 00:10:44.224
pero esa no es mi intención.

00:10:44.224 --> 00:10:46.923
De hecho, quiero compartir con Uds.

00:10:46.923 --> 00:10:48.695
las alternativas que hay.

00:10:48.695 --> 00:10:51.194
La forma en que hacemos 
las cosas ahora no es la única

00:10:51.194 --> 00:10:54.231
forma de hacerlas,
y ciertamente no es la mejor

00:10:54.231 --> 00:10:56.258
forma en que pueden ser hechas.

00:10:56.258 --> 00:11:00.429
Cuando alguien les diga: "La gente no se 
preocupa por la privacidad",

00:11:00.429 --> 00:11:03.071
piensen si el juego 
no ha sido diseñado

00:11:03.071 --> 00:11:05.795
y manipulado para que no se 
preocupen por la privacidad,

00:11:05.795 --> 00:11:09.057
y cuando concluyamos que 
que estas manipulaciones ocurren,

00:11:09.057 --> 00:11:10.664
ya estaremos a mitad de camino

00:11:10.664 --> 00:11:12.922
de poder autoprotegernos.

00:11:12.922 --> 00:11:16.632
Si alguien les dice que 
la privacidad es incompatible

00:11:16.632 --> 00:11:18.481
con los beneficios del "big data",

00:11:18.481 --> 00:11:20.954
piensen que en los últimos 20 años,

00:11:20.954 --> 00:11:22.871
los investigadores 
han creado tecnologías

00:11:22.871 --> 00:11:26.189
que permiten que virtualmente
cualquier transacción electrónica

00:11:26.189 --> 00:11:29.938
se realice en formas más 
preservadoras de la privacidad.

00:11:29.938 --> 00:11:32.493
Podemos navegar Internet 
en forma anónima.

00:11:32.493 --> 00:11:35.171
Podemos enviar correos electrónicos 
que solo pueda leer

00:11:35.171 --> 00:11:38.880
el destinatario, 
y no la NSA [agencia de seguridad].

00:11:38.880 --> 00:11:41.877
Podemos proteger incluso la privacidad 
de la minería de datos.

00:11:41.877 --> 00:11:45.771
En otras palabras, podemos tener 
los beneficios del "big data"

00:11:45.771 --> 00:11:47.903
y proteger la privacidad.

00:11:47.903 --> 00:11:51.694
Estas tecnologías, claro está,
implican una inversión

00:11:51.694 --> 00:11:53.240
de la relacón costo- beneficio

00:11:53.240 --> 00:11:55.347
para los tenedores de los datos,

00:11:55.347 --> 00:11:58.800
razón por la cual, quizá, 
no escuchamos mucho de ellas.

00:11:58.800 --> 00:12:02.506
Eso me lleva de vuelta 
al Jardín del Edén.

00:12:02.506 --> 00:12:05.286
Hay una segunda interpretación 
de la privacidad

00:12:05.286 --> 00:12:07.095
en la historia del Jardín del Edén

00:12:07.095 --> 00:12:09.191
que no tiene que ver con el tema

00:12:09.191 --> 00:12:11.416
del desnudo de Adán y Eva

00:12:11.416 --> 00:12:13.797
ni con sentir vergüenza.

00:12:13.797 --> 00:12:16.578
Pueden encontrar ecos 
de esta interpretación

00:12:16.578 --> 00:12:19.360
en "El paraíso perdido" 
de John Milton.

00:12:19.360 --> 00:12:23.557
En el jardín, Adán y Eva están 
materialmente contentos.

00:12:23.557 --> 00:12:25.661
Están felices. Están satisfechos.

00:12:25.661 --> 00:12:27.954
No obstante, carecen de conocimiento

00:12:27.954 --> 00:12:29.594
y de autoconciencia.

00:12:29.594 --> 00:12:32.913
En el momento en que 
comen el bien llamado

00:12:32.913 --> 00:12:34.206
fruto del conocimiento,

00:12:34.206 --> 00:12:36.811
es cuando se descubren a sí mismos.

00:12:36.811 --> 00:12:40.842
Se hacen conscientes. 
Logran autonomía.

00:12:40.842 --> 00:12:43.968
El precio a pagar, sin embargo, 
es abandonar el jardín.

00:12:43.968 --> 00:12:47.849
La privacidad, en cierto modo, 
es tanto el medio

00:12:47.849 --> 00:12:50.811
como el precio a pagar 
por la libertad.

00:12:50.811 --> 00:12:53.581
Y los vendedores
otra vez nos dicen

00:12:53.581 --> 00:12:56.600
que el "big data" 
y los medios sociales

00:12:56.600 --> 00:12:59.579
no son solo un paraíso 
de ganancias para ellos,

00:12:59.579 --> 00:13:02.036
sino el Jardín del Edén 
para el resto de nosotros.

00:13:02.036 --> 00:13:03.274
Recibimos contenido gratis.

00:13:03.274 --> 00:13:06.397
Podemos jugar a Angry Birds. 
Tenemos aplicaciones específicas.

00:13:06.397 --> 00:13:09.294
Pero, de hecho, en unos años, 
las organizaciones

00:13:09.294 --> 00:13:10.903
sabrán tanto de nosotros

00:13:10.903 --> 00:13:13.613
que podrán inferir 
nuestros deseos

00:13:13.613 --> 00:13:15.817
incluso antes de formularlos 
y quizá hasta

00:13:15.817 --> 00:13:18.264
compren productos en nuestro nombre

00:13:18.264 --> 00:13:20.538
antes de que sepamos 
que los necesitamos.

00:13:20.538 --> 00:13:23.775
Hay un escritor inglés

00:13:23.775 --> 00:13:26.820
que anticipó esta especie de futuro

00:13:26.820 --> 00:13:28.225
en el que cambiaríamos

00:13:28.225 --> 00:13:31.773
nuestra autonomía y libertad 
por comodidad--

00:13:31.773 --> 00:13:33.934
Incluso más que George Orwell.

00:13:33.934 --> 00:13:36.695
El autor es, 
por supuesto, Aldous Huxley.

00:13:36.695 --> 00:13:39.549
En "Un mundo feliz", 
él imagina una sociedad

00:13:39.549 --> 00:13:41.720
en la que las tecnologías que creamos

00:13:41.720 --> 00:13:43.579
en principio para la libertad

00:13:43.579 --> 00:13:46.146
terminan coaccionándonos.

00:13:46.146 --> 00:13:50.937
Sin embargo, en el libro, él también
nos ofrece una salida

00:13:50.937 --> 00:13:54.375
de esa sociedad, similar al sendero

00:13:54.375 --> 00:13:58.330
que Adán y Eva tuvieron que 
seguir para salir del jardín.

00:13:58.330 --> 00:14:00.477
En palabras de Savage,

00:14:00.477 --> 00:14:03.546
recuperar la autonomía 
y la libertad es posible,

00:14:03.546 --> 00:14:06.225
aunque el precio 
a pagar es elevado.

00:14:06.225 --> 00:14:11.940
Por eso creo que una 
de las peleas decisivas

00:14:11.940 --> 00:14:14.503
de nuestros tiempos 
será la pelea

00:14:14.503 --> 00:14:16.890
por el control de la 
información personal,

00:14:16.890 --> 00:14:20.397
la pelea porque el "big data" 
se vuelva una fuerza

00:14:20.397 --> 00:14:21.686
de libertad,

00:14:21.686 --> 00:14:26.432
en lugar de una fuerza que nos 
manipule desde las sombras.

00:14:26.432 --> 00:14:29.025
Muchos de nosotros

00:14:29.025 --> 00:14:31.778
ni siquiera sabemos que 
se está dando esta pelea,

00:14:31.778 --> 00:14:34.450
pero es así, nos guste o no.

00:14:34.450 --> 00:14:37.254
Y a riesgo de interpretar 
a la serpiente,

00:14:37.254 --> 00:14:40.151
les diré que las 
herramientas para pelear

00:14:40.151 --> 00:14:43.160
están aquí, la conciencia 
de lo que ocurre,

00:14:43.160 --> 00:14:44.515
y en sus manos,

00:14:44.515 --> 00:14:48.255
a unos pocos clics de distancia.

00:14:48.255 --> 00:14:49.737
Gracias.

00:14:49.737 --> 00:14:54.214
(Aplausos)

