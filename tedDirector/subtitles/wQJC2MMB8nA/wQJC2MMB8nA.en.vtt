WEBVTT
Kind: captions
Language: en

00:00:16.010 --> 00:00:18.286
So, security is two different things:

00:00:18.310 --> 00:00:20.836
it's a feeling, and it's a reality.

00:00:20.860 --> 00:00:22.285
And they're different.

00:00:22.309 --> 00:00:25.736
You could feel secure even if you're not.

00:00:25.760 --> 00:00:27.736
And you can be secure

00:00:27.760 --> 00:00:29.610
even if you don't feel it.

00:00:29.634 --> 00:00:31.751
Really, we have two separate concepts

00:00:31.775 --> 00:00:33.427
mapped onto the same word.

00:00:33.960 --> 00:00:37.586
And what I want to do in this talk
is to split them apart --

00:00:37.610 --> 00:00:41.220
figuring out when they diverge
and how they converge.

00:00:41.711 --> 00:00:43.986
And language is actually a problem here.

00:00:44.010 --> 00:00:46.086
There aren't a lot of good words

00:00:46.110 --> 00:00:48.171
for the concepts
we're going to talk about.

00:00:49.295 --> 00:00:53.415
So if you look at security
from economic terms,

00:00:53.439 --> 00:00:55.086
it's a trade-off.

00:00:55.110 --> 00:00:59.242
Every time you get some security,
you're always trading off something.

00:00:59.266 --> 00:01:01.111
Whether this is a personal decision --

00:01:01.135 --> 00:01:04.147
whether you're going to install
a burglar alarm in your home --

00:01:04.171 --> 00:01:05.328
or a national decision,

00:01:05.352 --> 00:01:07.662
where you're going to invade
a foreign country --

00:01:07.686 --> 00:01:11.468
you're going to trade off something:
money or time, convenience, capabilities,

00:01:11.492 --> 00:01:13.494
maybe fundamental liberties.

00:01:13.518 --> 00:01:16.792
And the question to ask
when you look at a security anything

00:01:16.816 --> 00:01:20.198
is not whether this makes us safer,

00:01:20.222 --> 00:01:22.437
but whether it's worth the trade-off.

00:01:22.461 --> 00:01:25.690
You've heard in the past
several years, the world is safer

00:01:25.714 --> 00:01:27.604
because Saddam Hussein is not in power.

00:01:27.628 --> 00:01:30.231
That might be true,
but it's not terribly relevant.

00:01:30.255 --> 00:01:33.086
The question is: Was it worth it?

00:01:33.110 --> 00:01:35.569
And you can make your own decision,

00:01:35.593 --> 00:01:38.326
and then you'll decide
whether the invasion was worth it.

00:01:38.350 --> 00:01:41.911
That's how you think about security:
in terms of the trade-off.

00:01:41.935 --> 00:01:44.545
Now, there's often no right or wrong here.

00:01:45.208 --> 00:01:48.516
Some of us have a burglar alarm
system at home and some of us don't.

00:01:48.540 --> 00:01:51.271
And it'll depend on where we live,

00:01:51.295 --> 00:01:53.221
whether we live alone or have a family,

00:01:53.245 --> 00:01:54.913
how much cool stuff we have,

00:01:54.937 --> 00:01:58.102
how much we're willing
to accept the risk of theft.

00:01:58.943 --> 00:02:01.891
In politics also,
there are different opinions.

00:02:02.459 --> 00:02:06.894
A lot of times, these trade-offs
are about more than just security,

00:02:06.918 --> 00:02:08.783
and I think that's really important.

00:02:08.807 --> 00:02:12.115
Now, people have a natural intuition
about these trade-offs.

00:02:12.588 --> 00:02:14.144
We make them every day.

00:02:14.807 --> 00:02:18.340
Last night in my hotel room,
when I decided to double-lock the door,

00:02:18.364 --> 00:02:20.364
or you in your car when you drove here;

00:02:21.191 --> 00:02:22.669
when we go eat lunch

00:02:22.693 --> 00:02:25.301
and decide the food's not
poison and we'll eat it.

00:02:25.325 --> 00:02:28.486
We make these trade-offs again and again,

00:02:28.510 --> 00:02:30.086
multiple times a day.

00:02:30.110 --> 00:02:31.699
We often won't even notice them.

00:02:31.723 --> 00:02:34.349
They're just part
of being alive; we all do it.

00:02:34.373 --> 00:02:35.928
Every species does it.

00:02:36.474 --> 00:02:39.336
Imagine a rabbit in a field, eating grass.

00:02:39.360 --> 00:02:41.303
And the rabbit sees a fox.

00:02:41.856 --> 00:02:43.905
That rabbit will make
a security trade-off:

00:02:43.929 --> 00:02:45.833
"Should I stay, or should I flee?"

00:02:46.380 --> 00:02:47.999
And if you think about it,

00:02:48.023 --> 00:02:50.578
the rabbits that are good
at making that trade-off

00:02:50.602 --> 00:02:52.580
will tend to live and reproduce,

00:02:52.604 --> 00:02:54.911
and the rabbits that are bad at it

00:02:54.935 --> 00:02:56.409
will get eaten or starve.

00:02:56.958 --> 00:02:58.566
So you'd think

00:02:59.573 --> 00:03:03.882
that us, as a successful species
on the planet -- you, me, everybody --

00:03:03.906 --> 00:03:06.479
would be really good
at making these trade-offs.

00:03:07.126 --> 00:03:10.230
Yet it seems, again and again,
that we're hopelessly bad at it.

00:03:11.768 --> 00:03:14.570
And I think that's a fundamentally
interesting question.

00:03:14.594 --> 00:03:16.467
I'll give you the short answer.

00:03:16.491 --> 00:03:19.142
The answer is, we respond
to the feeling of security

00:03:19.166 --> 00:03:20.684
and not the reality.

00:03:21.864 --> 00:03:24.560
Now, most of the time, that works.

00:03:25.538 --> 00:03:27.041
Most of the time,

00:03:27.065 --> 00:03:29.198
feeling and reality are the same.

00:03:30.776 --> 00:03:34.292
Certainly that's true
for most of human prehistory.

00:03:35.633 --> 00:03:38.341
We've developed this ability

00:03:38.365 --> 00:03:40.949
because it makes evolutionary sense.

00:03:41.985 --> 00:03:45.259
One way to think of it
is that we're highly optimized

00:03:45.283 --> 00:03:47.086
for risk decisions

00:03:47.110 --> 00:03:49.653
that are endemic to living
in small family groups

00:03:49.677 --> 00:03:52.213
in the East African Highlands
in 100,000 BC.

00:03:52.792 --> 00:03:55.451
2010 New York, not so much.

00:03:56.879 --> 00:04:00.085
Now, there are several biases
in risk perception.

00:04:00.109 --> 00:04:01.850
A lot of good experiments in this.

00:04:01.874 --> 00:04:05.477
And you can see certain biases
that come up again and again.

00:04:05.501 --> 00:04:06.854
I'll give you four.

00:04:06.878 --> 00:04:10.086
We tend to exaggerate
spectacular and rare risks

00:04:10.110 --> 00:04:12.086
and downplay common risks --

00:04:12.110 --> 00:04:13.628
so, flying versus driving.

00:04:14.451 --> 00:04:18.245
The unknown is perceived
to be riskier than the familiar.

00:04:21.470 --> 00:04:22.909
One example would be:

00:04:22.933 --> 00:04:25.546
people fear kidnapping by strangers,

00:04:25.570 --> 00:04:29.206
when the data supports that kidnapping
by relatives is much more common.

00:04:29.230 --> 00:04:30.804
This is for children.

00:04:30.828 --> 00:04:34.868
Third, personified risks
are perceived to be greater

00:04:34.892 --> 00:04:36.395
than anonymous risks.

00:04:36.419 --> 00:04:39.206
So, Bin Laden is scarier
because he has a name.

00:04:40.182 --> 00:04:41.545
And the fourth is:

00:04:41.569 --> 00:04:46.324
people underestimate risks
in situations they do control

00:04:46.348 --> 00:04:49.311
and overestimate them
in situations they don't control.

00:04:49.335 --> 00:04:52.719
So once you take up skydiving or smoking,

00:04:52.743 --> 00:04:54.367
you downplay the risks.

00:04:55.037 --> 00:04:58.090
If a risk is thrust upon you --
terrorism is a good example --

00:04:58.114 --> 00:04:59.271
you'll overplay it,

00:04:59.295 --> 00:05:01.634
because you don't feel
like it's in your control.

00:05:02.157 --> 00:05:05.650
There are a bunch
of other of these cognitive biases,

00:05:05.674 --> 00:05:08.013
that affect our risk decisions.

00:05:08.832 --> 00:05:11.086
There's the availability heuristic,

00:05:11.110 --> 00:05:15.290
which basically means we estimate
the probability of something

00:05:15.314 --> 00:05:18.653
by how easy it is to bring
instances of it to mind.

00:05:19.831 --> 00:05:21.608
So you can imagine how that works.

00:05:21.632 --> 00:05:25.260
If you hear a lot about tiger attacks,
there must be a lot of tigers around.

00:05:25.284 --> 00:05:28.628
You don't hear about lion attacks,
there aren't a lot of lions around.

00:05:28.652 --> 00:05:30.949
This works, until you invent newspapers,

00:05:30.973 --> 00:05:35.379
because what newspapers do
is repeat again and again

00:05:35.403 --> 00:05:36.809
rare risks.

00:05:36.833 --> 00:05:39.698
I tell people: if it's in the news,
don't worry about it,

00:05:39.722 --> 00:05:43.997
because by definition, news is something
that almost never happens.

00:05:44.021 --> 00:05:45.790
(Laughter)

00:05:45.814 --> 00:05:48.737
When something is so common,
it's no longer news.

00:05:48.761 --> 00:05:50.959
Car crashes, domestic violence --

00:05:50.983 --> 00:05:52.973
those are the risks you worry about.

00:05:53.713 --> 00:05:55.861
We're also a species of storytellers.

00:05:55.885 --> 00:05:58.000
We respond to stories more than data.

00:05:58.514 --> 00:06:00.920
And there's some basic
innumeracy going on.

00:06:00.944 --> 00:06:04.086
I mean, the joke "One, two,
three, many" is kind of right.

00:06:04.110 --> 00:06:06.432
We're really good at small numbers.

00:06:06.456 --> 00:06:08.792
One mango, two mangoes, three mangoes,

00:06:08.816 --> 00:06:10.793
10,000 mangoes, 100,000 mangoes --

00:06:10.817 --> 00:06:13.794
it's still more mangoes
you can eat before they rot.

00:06:13.818 --> 00:06:17.086
So one half, one quarter,
one fifth -- we're good at that.

00:06:17.110 --> 00:06:19.086
One in a million, one in a billion --

00:06:19.110 --> 00:06:20.685
they're both almost never.

00:06:21.546 --> 00:06:24.960
So we have trouble with the risks
that aren't very common.

00:06:25.760 --> 00:06:27.737
And what these cognitive biases do

00:06:27.761 --> 00:06:30.737
is they act as filters
between us and reality.

00:06:31.284 --> 00:06:35.157
And the result is that feeling
and reality get out of whack,

00:06:35.181 --> 00:06:36.565
they get different.

00:06:37.370 --> 00:06:41.301
Now, you either have a feeling --
you feel more secure than you are,

00:06:41.325 --> 00:06:43.010
there's a false sense of security.

00:06:43.034 --> 00:06:46.408
Or the other way, and that's a false
sense of insecurity.

00:06:47.015 --> 00:06:49.895
I write a lot about "security theater,"

00:06:49.919 --> 00:06:52.599
which are products
that make people feel secure,

00:06:52.623 --> 00:06:54.600
but don't actually do anything.

00:06:54.624 --> 00:06:57.181
There's no real word for stuff
that makes us secure,

00:06:57.205 --> 00:06:59.086
but doesn't make us feel secure.

00:06:59.110 --> 00:07:01.830
Maybe it's what the CIA
is supposed to do for us.

00:07:03.539 --> 00:07:05.707
So back to economics.

00:07:05.731 --> 00:07:09.387
If economics, if the market,
drives security,

00:07:09.411 --> 00:07:14.258
and if people make trade-offs
based on the feeling of security,

00:07:14.282 --> 00:07:18.962
then the smart thing for companies to do
for the economic incentives

00:07:18.986 --> 00:07:21.043
is to make people feel secure.

00:07:21.942 --> 00:07:24.272
And there are two ways to do this.

00:07:24.296 --> 00:07:27.086
One, you can make people actually secure

00:07:27.110 --> 00:07:28.573
and hope they notice.

00:07:28.597 --> 00:07:31.441
Or two, you can make people
just feel secure

00:07:31.465 --> 00:07:33.337
and hope they don't notice.

00:07:34.401 --> 00:07:35.776
Right?

00:07:35.800 --> 00:07:38.941
So what makes people notice?

00:07:39.500 --> 00:07:40.882
Well, a couple of things:

00:07:40.906 --> 00:07:43.172
understanding of the security,

00:07:43.196 --> 00:07:45.086
of the risks, the threats,

00:07:45.110 --> 00:07:46.984
the countermeasures, how they work.

00:07:47.008 --> 00:07:49.298
But if you know stuff, you're more likely

00:07:50.155 --> 00:07:52.381
to have your feelings match reality.

00:07:53.110 --> 00:07:56.255
Enough real-world examples helps.

00:07:56.279 --> 00:07:58.838
We all know the crime rate
in our neighborhood,

00:07:58.862 --> 00:08:01.663
because we live there,
and we get a feeling about it

00:08:01.687 --> 00:08:03.556
that basically matches reality.

00:08:05.038 --> 00:08:07.245
Security theater is exposed

00:08:07.269 --> 00:08:10.055
when it's obvious
that it's not working properly.

00:08:11.209 --> 00:08:13.879
OK. So what makes people not notice?

00:08:14.443 --> 00:08:15.935
Well, a poor understanding.

00:08:16.642 --> 00:08:19.786
If you don't understand the risks,
you don't understand the costs,

00:08:19.810 --> 00:08:21.967
you're likely to get the trade-off wrong,

00:08:21.991 --> 00:08:24.479
and your feeling doesn't match reality.

00:08:24.503 --> 00:08:26.240
Not enough examples.

00:08:26.879 --> 00:08:30.385
There's an inherent problem
with low-probability events.

00:08:30.919 --> 00:08:34.732
If, for example, terrorism
almost never happens,

00:08:34.756 --> 00:08:39.360
it's really hard to judge the efficacy
of counter-terrorist measures.

00:08:40.523 --> 00:08:44.086
This is why you keep sacrificing virgins,

00:08:44.110 --> 00:08:46.785
and why your unicorn defenses
are working just great.

00:08:46.809 --> 00:08:49.366
There aren't enough examples of failures.

00:08:51.109 --> 00:08:53.896
Also, feelings that cloud the issues --

00:08:53.920 --> 00:08:57.948
the cognitive biases I talked
about earlier: fears, folk beliefs --

00:08:58.727 --> 00:09:01.472
basically, an inadequate model of reality.

00:09:03.403 --> 00:09:05.574
So let me complicate things.

00:09:05.598 --> 00:09:07.575
I have feeling and reality.

00:09:07.599 --> 00:09:10.395
I want to add a third element.
I want to add "model."

00:09:10.839 --> 00:09:13.189
Feeling and model are in our head,

00:09:13.213 --> 00:09:16.665
reality is the outside world;
it doesn't change, it's real.

00:09:17.800 --> 00:09:20.014
Feeling is based on our intuition,

00:09:20.038 --> 00:09:21.664
model is based on reason.

00:09:22.383 --> 00:09:24.422
That's basically the difference.

00:09:24.446 --> 00:09:26.423
In a primitive and simple world,

00:09:26.447 --> 00:09:28.584
there's really no reason for a model,

00:09:30.253 --> 00:09:32.548
because feeling is close to reality.

00:09:32.572 --> 00:09:33.945
You don't need a model.

00:09:34.596 --> 00:09:36.746
But in a modern and complex world,

00:09:37.556 --> 00:09:41.206
you need models to understand
a lot of the risks we face.

00:09:42.362 --> 00:09:44.646
There's no feeling about germs.

00:09:45.110 --> 00:09:47.226
You need a model to understand them.

00:09:48.157 --> 00:09:51.835
This model is an intelligent
representation of reality.

00:09:52.411 --> 00:09:57.162
It's, of course, limited
by science, by technology.

00:09:58.249 --> 00:10:00.575
We couldn't have a germ theory of disease

00:10:00.599 --> 00:10:03.133
before we invented
the microscope to see them.

00:10:04.316 --> 00:10:06.959
It's limited by our cognitive biases.

00:10:08.110 --> 00:10:11.101
But it has the ability
to override our feelings.

00:10:11.507 --> 00:10:14.611
Where do we get these models?
We get them from others.

00:10:14.635 --> 00:10:19.854
We get them from religion,
from culture, teachers, elders.

00:10:20.298 --> 00:10:23.724
A couple years ago,
I was in South Africa on safari.

00:10:23.748 --> 00:10:26.510
The tracker I was with grew up
in Kruger National Park.

00:10:26.534 --> 00:10:29.287
He had some very complex
models of how to survive.

00:10:29.800 --> 00:10:33.713
And it depended on if you were attacked
by a lion, leopard, rhino, or elephant --

00:10:33.737 --> 00:10:36.471
and when you had to run away,
when you couldn't run away,

00:10:36.495 --> 00:10:39.578
when you had to climb a tree,
when you could never climb a tree.

00:10:39.602 --> 00:10:40.951
I would have died in a day.

00:10:42.160 --> 00:10:45.942
But he was born there,
and he understood how to survive.

00:10:46.490 --> 00:10:48.086
I was born in New York City.

00:10:48.110 --> 00:10:51.361
I could have taken him to New York,
and he would have died in a day.

00:10:51.385 --> 00:10:52.386
(Laughter)

00:10:52.410 --> 00:10:56.554
Because we had different models
based on our different experiences.

00:10:58.291 --> 00:11:00.760
Models can come from the media,

00:11:00.784 --> 00:11:02.547
from our elected officials ...

00:11:03.234 --> 00:11:06.315
Think of models of terrorism,

00:11:06.339 --> 00:11:08.536
child kidnapping,

00:11:08.560 --> 00:11:10.885
airline safety, car safety.

00:11:11.539 --> 00:11:13.532
Models can come from industry.

00:11:14.348 --> 00:11:17.566
The two I'm following
are surveillance cameras,

00:11:17.590 --> 00:11:19.086
ID cards,

00:11:19.110 --> 00:11:22.240
quite a lot of our computer
security models come from there.

00:11:22.264 --> 00:11:24.491
A lot of models come from science.

00:11:24.515 --> 00:11:26.352
Health models are a great example.

00:11:26.376 --> 00:11:29.402
Think of cancer, bird flu,
swine flu, SARS.

00:11:29.942 --> 00:11:34.812
All of our feelings of security
about those diseases

00:11:34.836 --> 00:11:39.491
come from models given to us, really,
by science filtered through the media.

00:11:41.038 --> 00:11:42.758
So models can change.

00:11:43.482 --> 00:11:45.585
Models are not static.

00:11:45.609 --> 00:11:48.849
As we become more comfortable
in our environments,

00:11:48.873 --> 00:11:52.475
our model can move closer to our feelings.

00:11:53.965 --> 00:11:56.305
So an example might be,

00:11:56.329 --> 00:11:57.925
if you go back 100 years ago,

00:11:57.949 --> 00:12:01.377
when electricity was first
becoming common,

00:12:01.401 --> 00:12:03.104
there were a lot of fears about it.

00:12:03.128 --> 00:12:05.606
There were people who were afraid
to push doorbells,

00:12:05.630 --> 00:12:08.635
because there was electricity
in there, and that was dangerous.

00:12:08.659 --> 00:12:11.528
For us, we're very facile
around electricity.

00:12:11.552 --> 00:12:14.370
We change light bulbs
without even thinking about it.

00:12:14.948 --> 00:12:21.111
Our model of security around electricity
is something we were born into.

00:12:21.735 --> 00:12:24.249
It hasn't changed as we were growing up.

00:12:24.273 --> 00:12:25.838
And we're good at it.

00:12:27.380 --> 00:12:31.879
Or think of the risks on the Internet
across generations --

00:12:31.903 --> 00:12:34.000
how your parents approach
Internet security,

00:12:34.024 --> 00:12:35.640
versus how you do,

00:12:35.664 --> 00:12:37.206
versus how our kids will.

00:12:38.300 --> 00:12:40.850
Models eventually fade
into the background.

00:12:42.427 --> 00:12:44.920
"Intuitive" is just
another word for familiar.

00:12:45.887 --> 00:12:49.737
So as your model is close to reality
and it converges with feelings,

00:12:49.761 --> 00:12:51.679
you often don't even know it's there.

00:12:53.239 --> 00:12:57.056
A nice example of this came
from last year and swine flu.

00:12:58.281 --> 00:13:00.281
When swine flu first appeared,

00:13:00.305 --> 00:13:02.923
the initial news caused
a lot of overreaction.

00:13:03.562 --> 00:13:05.540
Now, it had a name,

00:13:05.564 --> 00:13:07.614
which made it scarier
than the regular flu,

00:13:07.638 --> 00:13:09.205
even though it was more deadly.

00:13:09.784 --> 00:13:12.992
And people thought doctors
should be able to deal with it.

00:13:13.459 --> 00:13:15.983
So there was that feeling
of lack of control.

00:13:16.007 --> 00:13:19.116
And those two things
made the risk more than it was.

00:13:19.140 --> 00:13:22.697
As the novelty wore off
and the months went by,

00:13:22.721 --> 00:13:25.564
there was some amount of tolerance;
people got used to it.

00:13:26.355 --> 00:13:29.025
There was no new data,
but there was less fear.

00:13:29.681 --> 00:13:31.855
By autumn,

00:13:31.879 --> 00:13:35.261
people thought the doctors
should have solved this already.

00:13:35.722 --> 00:13:37.682
And there's kind of a bifurcation:

00:13:37.706 --> 00:13:43.457
people had to choose
between fear and acceptance --

00:13:44.512 --> 00:13:46.156
actually, fear and indifference --

00:13:46.180 --> 00:13:47.975
and they kind of chose suspicion.

00:13:49.110 --> 00:13:52.221
And when the vaccine appeared last winter,

00:13:52.245 --> 00:13:54.756
there were a lot of people --
a surprising number --

00:13:54.780 --> 00:13:56.577
who refused to get it.

00:13:58.777 --> 00:14:02.433
And it's a nice example of how
people's feelings of security change,

00:14:02.457 --> 00:14:04.060
how their model changes,

00:14:04.084 --> 00:14:05.752
sort of wildly,

00:14:05.776 --> 00:14:08.546
with no new information,
with no new input.

00:14:10.327 --> 00:14:12.135
This kind of thing happens a lot.

00:14:13.199 --> 00:14:15.170
I'm going to give one more complication.

00:14:15.194 --> 00:14:17.890
We have feeling, model, reality.

00:14:18.640 --> 00:14:21.150
I have a very relativistic
view of security.

00:14:21.174 --> 00:14:22.988
I think it depends on the observer.

00:14:23.695 --> 00:14:28.861
And most security decisions
have a variety of people involved.

00:14:29.792 --> 00:14:36.331
And stakeholders with specific trade-offs
will try to influence the decision.

00:14:36.355 --> 00:14:38.036
And I call that their agenda.

00:14:39.512 --> 00:14:43.003
And you see agenda --
this is marketing, this is politics --

00:14:43.481 --> 00:14:46.520
trying to convince you to have
one model versus another,

00:14:46.544 --> 00:14:48.528
trying to convince you to ignore a model

00:14:48.552 --> 00:14:51.224
and trust your feelings,

00:14:51.248 --> 00:14:53.752
marginalizing people
with models you don't like.

00:14:54.744 --> 00:14:56.260
This is not uncommon.

00:14:57.610 --> 00:15:00.839
An example, a great example,
is the risk of smoking.

00:15:02.196 --> 00:15:03.979
In the history of the past 50 years,

00:15:04.003 --> 00:15:06.616
the smoking risk shows
how a model changes,

00:15:06.640 --> 00:15:10.998
and it also shows how an industry fights
against a model it doesn't like.

00:15:11.983 --> 00:15:15.086
Compare that to the secondhand
smoke debate --

00:15:15.110 --> 00:15:17.063
probably about 20 years behind.

00:15:17.982 --> 00:15:19.597
Think about seat belts.

00:15:19.621 --> 00:15:21.645
When I was a kid, no one wore a seat belt.

00:15:21.669 --> 00:15:25.210
Nowadays, no kid will let you drive
if you're not wearing a seat belt.

00:15:26.633 --> 00:15:29.086
Compare that to the airbag debate,

00:15:29.110 --> 00:15:30.777
probably about 30 years behind.

00:15:32.006 --> 00:15:34.094
All examples of models changing.

00:15:36.855 --> 00:15:39.651
What we learn is that changing
models is hard.

00:15:40.334 --> 00:15:42.387
Models are hard to dislodge.

00:15:42.411 --> 00:15:44.086
If they equal your feelings,

00:15:44.110 --> 00:15:46.009
you don't even know you have a model.

00:15:47.110 --> 00:15:48.996
And there's another cognitive bias

00:15:49.020 --> 00:15:51.086
I'll call confirmation bias,

00:15:51.110 --> 00:15:55.471
where we tend to accept data
that confirms our beliefs

00:15:55.495 --> 00:15:57.932
and reject data
that contradicts our beliefs.

00:15:59.490 --> 00:16:03.425
So evidence against our model,
we're likely to ignore,

00:16:03.449 --> 00:16:04.697
even if it's compelling.

00:16:04.721 --> 00:16:07.726
It has to get very compelling
before we'll pay attention.

00:16:08.990 --> 00:16:11.587
New models that extend
long periods of time are hard.

00:16:11.611 --> 00:16:13.365
Global warming is a great example.

00:16:13.389 --> 00:16:16.831
We're terrible at models
that span 80 years.

00:16:16.855 --> 00:16:18.918
We can do "to the next harvest."

00:16:18.942 --> 00:16:21.248
We can often do "until our kids grow up."

00:16:21.760 --> 00:16:23.961
But "80 years," we're just not good at.

00:16:24.975 --> 00:16:27.277
So it's a very hard model to accept.

00:16:27.999 --> 00:16:30.945
We can have both models
in our head simultaneously --

00:16:31.912 --> 00:16:38.860
that kind of problem where
we're holding both beliefs together,

00:16:38.884 --> 00:16:40.254
the cognitive dissonance.

00:16:40.278 --> 00:16:43.434
Eventually, the new model
will replace the old model.

00:16:44.164 --> 00:16:46.354
Strong feelings can create a model.

00:16:47.411 --> 00:16:52.774
September 11 created a security model
in a lot of people's heads.

00:16:52.798 --> 00:16:56.086
Also, personal experiences
with crime can do it,

00:16:56.110 --> 00:16:57.489
personal health scare,

00:16:57.513 --> 00:16:58.979
a health scare in the news.

00:17:00.198 --> 00:17:03.543
You'll see these called
"flashbulb events" by psychiatrists.

00:17:04.183 --> 00:17:06.644
They can create a model instantaneously,

00:17:06.668 --> 00:17:08.429
because they're very emotive.

00:17:09.908 --> 00:17:11.496
So in the technological world,

00:17:11.520 --> 00:17:14.757
we don't have experience to judge models.

00:17:15.124 --> 00:17:18.057
And we rely on others. We rely on proxies.

00:17:18.081 --> 00:17:20.700
And this works, as long as
it's the correct others.

00:17:21.183 --> 00:17:23.865
We rely on government agencies

00:17:23.889 --> 00:17:28.293
to tell us what pharmaceuticals are safe.

00:17:28.317 --> 00:17:30.230
I flew here yesterday.

00:17:30.254 --> 00:17:32.016
I didn't check the airplane.

00:17:32.699 --> 00:17:35.294
I relied on some other group

00:17:35.318 --> 00:17:37.755
to determine whether
my plane was safe to fly.

00:17:37.779 --> 00:17:41.077
We're here, none of us fear the roof
is going to collapse on us,

00:17:41.101 --> 00:17:43.306
not because we checked,

00:17:43.330 --> 00:17:47.002
but because we're pretty sure
the building codes here are good.

00:17:48.442 --> 00:17:51.431
It's a model we just accept

00:17:51.455 --> 00:17:52.815
pretty much by faith.

00:17:53.331 --> 00:17:54.689
And that's OK.

00:17:57.966 --> 00:18:03.839
Now, what we want is people
to get familiar enough with better models,

00:18:03.863 --> 00:18:05.983
have it reflected in their feelings,

00:18:06.007 --> 00:18:09.009
to allow them to make security trade-offs.

00:18:10.110 --> 00:18:13.829
When these go out of whack,
you have two options.

00:18:13.853 --> 00:18:18.086
One, you can fix people's feelings,
directly appeal to feelings.

00:18:18.110 --> 00:18:20.516
It's manipulation, but it can work.

00:18:21.173 --> 00:18:23.364
The second, more honest way

00:18:23.388 --> 00:18:25.161
is to actually fix the model.

00:18:26.720 --> 00:18:28.821
Change happens slowly.

00:18:28.845 --> 00:18:33.223
The smoking debate took 40 years --
and that was an easy one.

00:18:35.195 --> 00:18:37.008
Some of this stuff is hard.

00:18:37.496 --> 00:18:41.252
Really, though, information
seems like our best hope.

00:18:41.276 --> 00:18:42.548
And I lied.

00:18:42.572 --> 00:18:46.592
Remember I said feeling, model, reality;
reality doesn't change?

00:18:46.616 --> 00:18:47.991
It actually does.

00:18:48.015 --> 00:18:49.729
We live in a technological world;

00:18:49.753 --> 00:18:52.091
reality changes all the time.

00:18:52.887 --> 00:18:55.864
So we might have,
for the first time in our species:

00:18:55.888 --> 00:18:59.071
feeling chases model, model chases
reality, reality's moving --

00:18:59.095 --> 00:19:00.628
they might never catch up.

00:19:02.180 --> 00:19:03.508
We don't know.

00:19:05.614 --> 00:19:07.217
But in the long term,

00:19:07.241 --> 00:19:09.445
both feeling and reality are important.

00:19:09.469 --> 00:19:12.702
And I want to close with two quick
stories to illustrate this.

00:19:12.726 --> 00:19:15.205
1982 -- I don't know if people
will remember this --

00:19:15.229 --> 00:19:18.599
there was a short epidemic
of Tylenol poisonings

00:19:18.623 --> 00:19:19.819
in the United States.

00:19:19.843 --> 00:19:21.204
It's a horrific story.

00:19:21.228 --> 00:19:23.307
Someone took a bottle of Tylenol,

00:19:23.331 --> 00:19:26.333
put poison in it, closed it up,
put it back on the shelf,

00:19:26.357 --> 00:19:27.915
someone else bought it and died.

00:19:27.939 --> 00:19:29.612
This terrified people.

00:19:29.636 --> 00:19:31.863
There were a couple of copycat attacks.

00:19:31.887 --> 00:19:34.732
There wasn't any real risk,
but people were scared.

00:19:34.756 --> 00:19:38.632
And this is how the tamper-proof
drug industry was invented.

00:19:38.656 --> 00:19:40.885
Those tamper-proof caps?
That came from this.

00:19:40.909 --> 00:19:42.480
It's complete security theater.

00:19:42.504 --> 00:19:45.395
As a homework assignment,
think of 10 ways to get around it.

00:19:45.419 --> 00:19:47.310
I'll give you one: a syringe.

00:19:47.334 --> 00:19:50.115
But it made people feel better.

00:19:50.744 --> 00:19:54.446
It made their feeling of security
more match the reality.

00:19:55.390 --> 00:19:58.324
Last story: a few years ago,
a friend of mine gave birth.

00:19:58.348 --> 00:19:59.745
I visit her in the hospital.

00:19:59.769 --> 00:20:01.692
It turns out, when a baby's born now,

00:20:01.716 --> 00:20:05.279
they put an RFID bracelet on the baby,
a corresponding one on the mother,

00:20:05.303 --> 00:20:08.923
so if anyone other than the mother takes
the baby out of the maternity ward,

00:20:08.947 --> 00:20:10.105
an alarm goes off.

00:20:10.129 --> 00:20:11.858
I said, "Well, that's kind of neat.

00:20:11.882 --> 00:20:15.852
I wonder how rampant
baby snatching is out of hospitals."

00:20:15.876 --> 00:20:17.112
I go home, I look it up.

00:20:17.136 --> 00:20:18.661
It basically never happens.

00:20:18.685 --> 00:20:20.529
(Laughter)

00:20:20.553 --> 00:20:23.396
But if you think about it,
if you are a hospital,

00:20:23.420 --> 00:20:25.800
and you need to take a baby
away from its mother,

00:20:25.824 --> 00:20:27.605
out of the room to run some tests,

00:20:27.629 --> 00:20:29.679
you better have some good
security theater,

00:20:29.703 --> 00:20:31.648
or she's going to rip your arm off.

00:20:31.672 --> 00:20:33.205
(Laughter)

00:20:34.161 --> 00:20:35.878
So it's important for us,

00:20:35.902 --> 00:20:38.037
those of us who design security,

00:20:38.061 --> 00:20:40.092
who look at security policy --

00:20:40.946 --> 00:20:44.254
or even look at public policy
in ways that affect security.

00:20:45.006 --> 00:20:48.422
It's not just reality;
it's feeling and reality.

00:20:48.446 --> 00:20:50.311
What's important

00:20:50.335 --> 00:20:51.880
is that they be about the same.

00:20:51.904 --> 00:20:54.435
It's important that,
if our feelings match reality,

00:20:54.459 --> 00:20:56.332
we make better security trade-offs.

00:20:56.711 --> 00:20:57.864
Thank you.

00:20:57.888 --> 00:21:00.021
(Applause)

