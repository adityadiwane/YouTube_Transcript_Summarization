WEBVTT
Kind: captions
Language: en

00:00:12.160 --> 00:00:15.160
Hello! My name is Golan Levin.

00:00:15.160 --> 00:00:17.160
I'm an artist and an engineer,

00:00:17.160 --> 00:00:19.160
which is, increasingly, a more common kind of hybrid.

00:00:19.160 --> 00:00:22.160
But I still fall into this weird crack

00:00:22.160 --> 00:00:24.160
where people don't seem to understand me.

00:00:24.160 --> 00:00:28.160
And I was looking around and I found this wonderful picture.

00:00:28.160 --> 00:00:31.160
It's a letter from "Artforum" in 1967

00:00:31.160 --> 00:00:34.160
saying "We can't imagine ever doing a special issue

00:00:34.160 --> 00:00:37.160
on electronics or computers in art." And they still haven't.

00:00:37.160 --> 00:00:42.160
And lest you think that you all, as the digerati, are more enlightened,

00:00:42.160 --> 00:00:45.160
I went to the Apple iPhone app store the other day.

00:00:45.160 --> 00:00:49.160
Where's art? I got productivity. I got sports.

00:00:49.160 --> 00:00:53.160
And somehow the idea that one would want to make art for the iPhone,

00:00:53.160 --> 00:00:55.160
which my friends and I are doing now,

00:00:55.160 --> 00:00:58.160
is still not reflected in our understanding

00:00:58.160 --> 00:01:00.160
of what computers are for.

00:01:00.160 --> 00:01:02.160
So, from both directions, there is kind of, I think, a lack of understanding

00:01:02.160 --> 00:01:04.160
about what it could mean to be an artist who uses the materials

00:01:04.160 --> 00:01:06.160
of his own day, or her own day,

00:01:06.160 --> 00:01:08.160
which I think artists are obliged to do,

00:01:08.160 --> 00:01:12.160
is to really explore the expressive potential of the new tools that we have.

00:01:12.160 --> 00:01:14.160
In my own case, I'm an artist,

00:01:14.160 --> 00:01:16.160
and I'm really interested in

00:01:16.160 --> 00:01:18.160
expanding the vocabulary of human action,

00:01:18.160 --> 00:01:21.160
and basically empowering people through interactivity.

00:01:21.160 --> 00:01:24.160
I want people to discover themselves as actors,

00:01:24.160 --> 00:01:28.160
as creative actors, by having interactive experiences.

00:01:28.160 --> 00:01:31.160
A lot of my work is about trying to get away from this.

00:01:31.160 --> 00:01:33.160
This a photograph of the desktop of a student of mine.

00:01:33.160 --> 00:01:35.160
And when I say desktop, I don't just mean

00:01:35.160 --> 00:01:38.160
the actual desk where his mouse has worn away the surface of the desk.

00:01:38.160 --> 00:01:40.160
If you look carefully, you can even see

00:01:40.160 --> 00:01:43.160
a hint of the Apple menu, up here in the upper left,

00:01:43.160 --> 00:01:45.160
where the virtual world has literally

00:01:45.160 --> 00:01:47.160
punched through to the physical.

00:01:47.160 --> 00:01:51.160
So this is, as Joy Mountford once said,

00:01:51.160 --> 00:01:53.160
"The mouse is probably the narrowest straw

00:01:53.160 --> 00:01:55.160
you could try to suck all of human expression through."

00:01:55.160 --> 00:01:58.160
(Laughter)

00:01:58.160 --> 00:02:01.160
And the thing I'm really trying to do is enabling people to have more rich

00:02:01.160 --> 00:02:03.160
kinds of interactive experiences.

00:02:03.160 --> 00:02:05.160
How can we get away from the mouse and use our full bodies

00:02:05.160 --> 00:02:08.160
as a way of exploring aesthetic experiences,

00:02:08.160 --> 00:02:10.160
not necessarily utilitarian ones.

00:02:10.160 --> 00:02:13.160
So I write software. And that's how I do it.

00:02:13.160 --> 00:02:15.160
And a lot of my experiences

00:02:15.160 --> 00:02:17.160
resemble mirrors in some way.

00:02:17.160 --> 00:02:19.160
Because this is, in some sense, the first way,

00:02:19.160 --> 00:02:21.160
that people discover their own potential as actors,

00:02:21.160 --> 00:02:23.160
and discover their own agency.

00:02:23.160 --> 00:02:26.160
By saying "Who is that person in the mirror? Oh it's actually me."

00:02:26.160 --> 00:02:28.160
And so, to give an example,

00:02:28.160 --> 00:02:30.160
this is a project from last year,

00:02:30.160 --> 00:02:32.160
which is called the Interstitial Fragment Processor.

00:02:32.160 --> 00:02:36.160
And it allows people to explore the negative shapes that they create

00:02:36.160 --> 00:02:39.160
when they're just going about their everyday business.

00:02:53.160 --> 00:02:55.160
So as people make shapes with their hands or their heads

00:02:55.160 --> 00:02:57.160
and so forth, or with each other,

00:02:57.160 --> 00:03:00.160
these shapes literally produce sounds and drop out of thin air --

00:03:00.160 --> 00:03:04.160
basically taking what's often this, kind of, unseen space,

00:03:04.160 --> 00:03:07.160
or this undetected space, and making it something real,

00:03:07.160 --> 00:03:10.160
that people then can appreciate and become creative with.

00:03:10.160 --> 00:03:13.160
So again, people discover their creative agency in this way.

00:03:13.160 --> 00:03:15.160
And their own personalities come out

00:03:15.160 --> 00:03:18.160
in totally unique ways.

00:03:18.160 --> 00:03:21.160
So in addition to using full-body input,

00:03:21.160 --> 00:03:23.160
something that I've explored now, for a while,

00:03:23.160 --> 00:03:25.160
has been the use of the voice,

00:03:25.160 --> 00:03:29.160
which is an immensely expressive system for us, vocalizing.

00:03:29.160 --> 00:03:31.160
Song is one of our oldest ways

00:03:31.160 --> 00:03:34.160
of making ourselves heard and understood.

00:03:34.160 --> 00:03:36.160
And I came across this fantastic research by Wolfgang KÃ¶hler,

00:03:36.160 --> 00:03:40.160
the so-called father of gestalt psychology, from 1927,

00:03:40.160 --> 00:03:42.160
who submitted to an audience like yourselves

00:03:42.160 --> 00:03:44.160
the following two shapes.

00:03:44.160 --> 00:03:46.160
And he said one of them is called Maluma.

00:03:46.160 --> 00:03:48.160
And one of them is called Taketa. Which is which?

00:03:48.160 --> 00:03:52.160
Anyone want to hazard a guess?

00:03:52.160 --> 00:03:54.160
Maluma is on top. Yeah. So.

00:03:54.160 --> 00:03:57.160
As he says here, most people answer without any hesitation.

00:03:57.160 --> 00:03:59.160
So what we're really seeing here is a phenomenon

00:03:59.160 --> 00:04:01.160
called phonaesthesia,

00:04:01.160 --> 00:04:03.160
which is a kind of synesthesia that all of you have.

00:04:03.160 --> 00:04:05.160
And so, whereas Dr. Oliver Sacks has talked about

00:04:05.160 --> 00:04:07.160
how perhaps one person in a million

00:04:07.160 --> 00:04:09.160
actually has true synesthesia,

00:04:09.160 --> 00:04:11.160
where they hear colors or taste shapes, and things like this,

00:04:11.160 --> 00:04:13.160
phonaesthesia is something we can all experience to some extent.

00:04:13.160 --> 00:04:16.160
It's about mappings between different perceptual domains,

00:04:16.160 --> 00:04:19.160
like hardness, sharpness, brightness and darkness,

00:04:19.160 --> 00:04:21.160
and the phonemes that we're able to speak with.

00:04:21.160 --> 00:04:23.160
So 70 years on, there's been some research where

00:04:23.160 --> 00:04:25.160
cognitive psychologists have actually sussed out

00:04:25.160 --> 00:04:27.160
the extent to which, you know,

00:04:27.160 --> 00:04:31.160
L, M and B are more associated with shapes that look like this,

00:04:31.160 --> 00:04:35.160
and P, T and K are perhaps more associated with shapes like this.

00:04:35.160 --> 00:04:37.160
And here we suddenly begin to have a mapping between curvature

00:04:37.160 --> 00:04:39.160
that we can exploit numerically,

00:04:39.160 --> 00:04:42.160
a relative mapping between curvature and shape.

00:04:42.160 --> 00:04:45.160
So it occurred to me, what happens if we could run these backwards?

00:04:45.160 --> 00:04:47.160
And thus was born the project called Remark,

00:04:47.160 --> 00:04:49.160
which is a collaboration with Zachary Lieberman

00:04:49.160 --> 00:04:51.160
and the Ars Electronica Futurelab.

00:04:51.160 --> 00:04:53.160
And this is an interactive installation which presents

00:04:53.160 --> 00:04:55.160
the fiction that speech casts visible shadows.

00:04:55.160 --> 00:04:58.160
So the idea is you step into a kind of a magic light.

00:04:58.160 --> 00:05:01.160
And as you do, you see the shadows of your own speech.

00:05:01.160 --> 00:05:03.160
And they sort of fly away, out of your head.

00:05:03.160 --> 00:05:06.160
If a computer speech recognition system

00:05:06.160 --> 00:05:10.160
is able to recognize what you're saying, then it spells it out.

00:05:10.160 --> 00:05:12.160
And if it isn't then it produces a shape which is very phonaesthetically

00:05:12.160 --> 00:05:14.160
tightly coupled to the sounds you made.

00:05:14.160 --> 00:05:17.160
So let's bring up a video of that.

00:06:03.160 --> 00:06:05.160
(Applause)

00:06:05.160 --> 00:06:08.160
Thanks. So. And this project here,

00:06:08.160 --> 00:06:11.160
I was working with the great abstract vocalist, Jaap Blonk.

00:06:11.160 --> 00:06:14.160
And he is a world expert in performing "The Ursonate,"

00:06:14.160 --> 00:06:16.160
which is a half-an-hour nonsense poem

00:06:16.160 --> 00:06:18.160
by Kurt Schwitters, written in the 1920s,

00:06:18.160 --> 00:06:22.160
which is half an hour of very highly patterned nonsense.

00:06:22.160 --> 00:06:24.160
And it's almost impossible to perform.

00:06:24.160 --> 00:06:27.160
But Jaap is one of the world experts in performing it.

00:06:27.160 --> 00:06:29.160
And in this project we've developed

00:06:29.160 --> 00:06:32.160
a form of intelligent real-time subtitles.

00:06:32.160 --> 00:06:35.160
So these are our live subtitles,

00:06:35.160 --> 00:06:38.160
that are being produced by a computer that knows the text of "The Ursonate" --

00:06:38.160 --> 00:06:41.160
fortunately Jaap does too, very well --

00:06:41.160 --> 00:06:46.160
and it is delivering that text at the same time as Jaap is.

00:06:53.160 --> 00:06:55.160
So all the text you're going to see

00:06:55.160 --> 00:06:57.160
is real-time generated by the computer,

00:06:57.160 --> 00:07:00.160
visualizing what he's doing with his voice.

00:08:10.160 --> 00:08:13.160
Here you can see the set-up where there is a screen with the subtitles behind him.

00:08:34.160 --> 00:08:36.160
Okay. So ...

00:08:36.160 --> 00:08:41.160
(Applause)

00:08:41.160 --> 00:08:43.160
The full videos are online if you are interested.

00:08:43.160 --> 00:08:45.160
I got a split reaction to that during the live performance,

00:08:45.160 --> 00:08:47.160
because there is some people who understand

00:08:47.160 --> 00:08:49.160
live subtitles are a kind of an oxymoron,

00:08:49.160 --> 00:08:52.160
because usually there is someone making them afterwards.

00:08:52.160 --> 00:08:55.160
And then a bunch of people who were like, "What's the big deal?

00:08:55.160 --> 00:08:57.160
I see subtitles all the time on television."

00:08:57.160 --> 00:09:00.160
You know? They don't imagine the person in the booth, typing it all.

00:09:00.160 --> 00:09:03.160
So in addition to the full body, and in addition to the voice,

00:09:03.160 --> 00:09:05.160
another thing that I've been really interested in,

00:09:05.160 --> 00:09:07.160
most recently, is the use of the eyes,

00:09:07.160 --> 00:09:11.160
or the gaze, in terms of how people relate to each other.

00:09:11.160 --> 00:09:13.160
It's a really profound amount of nonverbal information

00:09:13.160 --> 00:09:15.160
that's communicated with the eyes.

00:09:15.160 --> 00:09:17.160
And it's one of the most interesting technical challenges

00:09:17.160 --> 00:09:19.160
that's very currently active in the computer sciences:

00:09:19.160 --> 00:09:21.160
being able to have a camera that can understand,

00:09:21.160 --> 00:09:23.160
from a fairly big distance away,

00:09:23.160 --> 00:09:26.160
how these little tiny balls are actually pointing in one way or another

00:09:26.160 --> 00:09:28.160
to reveal what you're interested in,

00:09:28.160 --> 00:09:30.160
and where your attention is directed.

00:09:30.160 --> 00:09:33.160
So there is a lot of emotional communication that happens there.

00:09:33.160 --> 00:09:37.160
And so I've been beginning, with a variety of different projects,

00:09:37.160 --> 00:09:40.160
to understand how people can relate to machines with their eyes.

00:09:40.160 --> 00:09:43.160
And basically to ask the questions:

00:09:43.160 --> 00:09:48.160
What if art was aware that we were looking at it?

00:09:48.160 --> 00:09:50.160
How could it respond, in a way,

00:09:50.160 --> 00:09:53.160
to acknowledge or subvert the fact that we're looking at it?

00:09:53.160 --> 00:09:56.160
And what could it do if it could look back at us?

00:09:56.160 --> 00:09:58.160
And so those are the questions that are happening in the next projects.

00:09:58.160 --> 00:10:01.160
In the first one which I'm going to show you, called Eyecode,

00:10:01.160 --> 00:10:03.160
it's a piece of interactive software

00:10:03.160 --> 00:10:05.160
in which, if we read this little circle,

00:10:05.160 --> 00:10:08.160
"the trace left by the looking of the previous observer

00:10:08.160 --> 00:10:11.160
looks at the trace left by the looking of previous observer."

00:10:11.160 --> 00:10:13.160
The idea is that it's an image wholly constructed

00:10:13.160 --> 00:10:15.160
from its own history of being viewed

00:10:15.160 --> 00:10:17.160
by different people in an installation.

00:10:17.160 --> 00:10:22.160
So let me just switch over so we can do the live demo.

00:10:22.160 --> 00:10:26.160
So let's run this and see if it works.

00:10:26.160 --> 00:10:29.160
Okay. Ah, there is lots of nice bright video.

00:10:29.160 --> 00:10:31.160
There is just a little test screen that shows that it's working.

00:10:31.160 --> 00:10:33.160
And what I'm just going to do is -- I'm going to hide that.

00:10:33.160 --> 00:10:35.160
And you can see here that what it's doing

00:10:35.160 --> 00:10:38.160
is it's recording my eyes every time I blink.

00:10:44.160 --> 00:10:48.160
Hello? And I can ... hello ... okay.

00:10:48.160 --> 00:10:50.160
And no matter where I am, what's really going on here

00:10:50.160 --> 00:10:53.160
is that it's an eye-tracking system that tries to locate my eyes.

00:10:53.160 --> 00:10:55.160
And if I get really far away I'm blurry.

00:10:55.160 --> 00:10:57.160
You know, you're going to have these kind of blurry spots like this

00:10:57.160 --> 00:11:00.160
that maybe only resemble eyes in a very very abstract way.

00:11:00.160 --> 00:11:03.160
But if I come up really close and stare directly at the camera

00:11:03.160 --> 00:11:05.160
on this laptop then you'll see these nice crisp eyes.

00:11:05.160 --> 00:11:09.160
You can think of it as a way of, sort of, typing, with your eyes.

00:11:09.160 --> 00:11:11.160
And what you're typing are recordings of your eyes

00:11:11.160 --> 00:11:13.160
as you're looking at other peoples' eyes.

00:11:13.160 --> 00:11:16.160
So each person is looking at the looking

00:11:16.160 --> 00:11:18.160
of everyone else before them.

00:11:18.160 --> 00:11:20.160
And this exists in larger installations

00:11:20.160 --> 00:11:22.160
where there are thousands and thousands of eyes

00:11:22.160 --> 00:11:24.160
that people could be staring at,

00:11:24.160 --> 00:11:26.160
as you see who's looking at the people looking

00:11:26.160 --> 00:11:28.160
at the people looking before them.

00:11:28.160 --> 00:11:31.160
So I'll just add a couple more. Blink. Blink.

00:11:31.160 --> 00:11:34.160
And you can see, just once again, how it's sort of finding my eyes

00:11:34.160 --> 00:11:37.160
and doing its best to estimate when it's blinking.

00:11:37.160 --> 00:11:39.160
Alright. Let's leave that.

00:11:39.160 --> 00:11:42.160
So that's this kind of recursive observation system.

00:11:42.160 --> 00:11:44.160
(Applause)

00:11:44.160 --> 00:11:46.160
Thank you.

00:11:46.160 --> 00:11:48.160
The last couple pieces I'm going to show

00:11:48.160 --> 00:11:50.160
are basically in the new realm of robotics -- for me, new for me.

00:11:50.160 --> 00:11:52.160
It's called Opto-Isolator.

00:11:52.160 --> 00:11:55.160
And I'm going to show a video of the older version of it,

00:11:55.160 --> 00:11:57.160
which is just a minute long. Okay.

00:12:06.160 --> 00:12:08.160
In this case, the Opto-Isolator is blinking

00:12:08.160 --> 00:12:10.160
in response to one's own blinks.

00:12:10.160 --> 00:12:13.160
So it blinks one second after you do.

00:12:13.160 --> 00:12:16.160
This is a device which is intended to reduce

00:12:16.160 --> 00:12:19.160
the phenomenon of gaze down to the simplest possible materials.

00:12:19.160 --> 00:12:21.160
Just one eye,

00:12:21.160 --> 00:12:23.160
looking at you, and eliminating everything else about a face,

00:12:23.160 --> 00:12:26.160
but just to consider gaze in an isolated way

00:12:26.160 --> 00:12:29.160
as a kind of, as an element.

00:12:29.160 --> 00:12:32.160
And at the same time, it attempts to engage in what you might call

00:12:32.160 --> 00:12:34.160
familiar psycho-social gaze behaviors.

00:12:34.160 --> 00:12:36.160
Like looking away if you look at it too long

00:12:36.160 --> 00:12:38.160
because it gets shy,

00:12:38.160 --> 00:12:41.160
or things like that.

00:12:41.160 --> 00:12:44.160
Okay. So the last project I'm going to show

00:12:44.160 --> 00:12:47.160
is this new one called Snout.

00:12:47.160 --> 00:12:49.160
(Laughter)

00:12:49.160 --> 00:12:51.160
It's an eight-foot snout,

00:12:51.160 --> 00:12:53.160
with a googly eye.

00:12:53.160 --> 00:12:54.160
(Laughter)

00:12:54.160 --> 00:12:57.160
And inside it's got an 800-pound robot arm

00:12:57.160 --> 00:12:59.160
that I borrowed,

00:12:59.160 --> 00:13:00.160
(Laughter)

00:13:00.160 --> 00:13:02.160
from a friend.

00:13:02.160 --> 00:13:03.160
(Laughter)

00:13:03.160 --> 00:13:05.160
It helps to have good friends.

00:13:05.160 --> 00:13:08.160
I'm at Carnegie Mellon; we've got a great Robotics Institute there.

00:13:08.160 --> 00:13:10.160
I'd like to show you thing called Snout, which is --

00:13:10.160 --> 00:13:12.160
The idea behind this project is to

00:13:12.160 --> 00:13:16.160
make a robot that appears as if it's continually surprised to see you.

00:13:16.160 --> 00:13:20.160
(Laughter)

00:13:20.160 --> 00:13:22.160
The idea is that basically --

00:13:22.160 --> 00:13:24.160
if it's constantly like "Huh? ... Huh?"

00:13:24.160 --> 00:13:28.160
That's why its other name is Doubletaker, Taker of Doubles.

00:13:28.160 --> 00:13:30.160
It's always kind of doing a double take: "What?"

00:13:30.160 --> 00:13:32.160
And the idea is basically, can it look at you

00:13:32.160 --> 00:13:34.160
and make you feel as if like,

00:13:34.160 --> 00:13:36.160
"What? Is it my shoes?"

00:13:36.160 --> 00:13:39.160
"Got something on my hair?" Here we go. Alright.

00:14:10.160 --> 00:14:12.160
Checking him out ...

00:14:20.160 --> 00:14:22.160
For you nerds, here's a little behind-the-scenes.

00:14:22.160 --> 00:14:24.160
It's got a computer vision system,

00:14:24.160 --> 00:14:27.160
and it tries to look at the people who are moving around the most.

00:14:39.160 --> 00:14:41.160
Those are its targets.

00:14:42.160 --> 00:14:44.160
Up there is the skeleton,

00:14:44.160 --> 00:14:47.160
which is actually what it's trying to do.

00:14:54.160 --> 00:14:57.160
It's really about trying to create a novel body language for a new creature.

00:14:57.160 --> 00:14:59.160
Hollywood does this all the time, of course.

00:14:59.160 --> 00:15:01.160
But also have the body language communicate something

00:15:01.160 --> 00:15:03.160
to the person who is looking at it.

00:15:03.160 --> 00:15:05.160
This language is communicating that it is surprised to see you,

00:15:05.160 --> 00:15:08.160
and it's interested in looking at you.

00:15:08.160 --> 00:15:10.160
(Laughter)

00:15:10.160 --> 00:15:19.160
(Applause)

00:15:19.160 --> 00:15:21.160
Thank you very much. That's all I've got for today.

00:15:21.160 --> 00:15:24.160
And I'm really happy to be here. Thank you so much.

00:15:24.160 --> 00:15:27.160
(Applause)

