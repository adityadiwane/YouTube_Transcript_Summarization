WEBVTT
Kind: captions
Language: en

00:00:25.000 --> 00:00:27.548
What I'm going to show you first,
as quickly as I can,

00:00:27.572 --> 00:00:31.341
is some foundational work,
some new technology

00:00:31.365 --> 00:00:33.976
that we brought to Microsoft
as part of an acquisition

00:00:34.000 --> 00:00:35.821
almost exactly a year ago.

00:00:35.845 --> 00:00:38.213
This is Seadragon, and it's an environment

00:00:38.237 --> 00:00:40.713
in which you can either
locally or remotely interact

00:00:40.737 --> 00:00:42.856
with vast amounts of visual data.

00:00:43.165 --> 00:00:46.569
We're looking at many, many gigabytes
of digital photos here

00:00:46.593 --> 00:00:49.508
and kind of seamlessly
and continuously zooming in,

00:00:49.532 --> 00:00:52.077
panning through it,
rearranging it in any way we want.

00:00:52.389 --> 00:00:55.976
And it doesn't matter how much
information we're looking at,

00:00:56.000 --> 00:00:58.976
how big these collections are
or how big the images are.

00:00:59.000 --> 00:01:01.286
Most of them are ordinary
digital camera photos,

00:01:01.310 --> 00:01:04.454
but this one, for example,
is a scan from the Library of Congress,

00:01:04.478 --> 00:01:07.296
and it's in the 300 megapixel range.

00:01:07.320 --> 00:01:08.976
It doesn't make any difference

00:01:09.000 --> 00:01:13.144
because the only thing that ought to limit
the performance of a system like this one

00:01:13.168 --> 00:01:15.945
is the number of pixels on your screen
at any given moment.

00:01:15.969 --> 00:01:17.939
It's also very flexible architecture.

00:01:17.963 --> 00:01:21.690
This is an entire book,
so this is an example of non-image data.

00:01:21.714 --> 00:01:24.501
This is "Bleak House" by Dickens.

00:01:24.525 --> 00:01:27.309
Every column is a chapter.

00:01:27.333 --> 00:01:30.976
To prove to you that it's really text,
and not an image,

00:01:31.000 --> 00:01:33.048
we can do something
like so, to really show

00:01:33.072 --> 00:01:36.264
that this is a real representation
of the text; it's not a picture.

00:01:36.288 --> 00:01:38.952
Maybe this is an artificial way
to read an e-book.

00:01:38.976 --> 00:01:40.176
I wouldn't recommend it.

00:01:40.200 --> 00:01:43.048
This is a more realistic case,
an issue of The Guardian.

00:01:43.072 --> 00:01:45.358
Every large image
is the beginning of a section.

00:01:45.382 --> 00:01:48.286
And this really gives you
the joy and the good experience

00:01:48.310 --> 00:01:53.493
of reading the real paper version
of a magazine or a newspaper,

00:01:53.517 --> 00:01:55.952
which is an inherently
multi-scale kind of medium.

00:01:55.976 --> 00:01:56.976
We've done something

00:01:57.000 --> 00:01:59.976
with the corner of this particular
issue of The Guardian.

00:02:00.000 --> 00:02:02.976
We've made up a fake ad
that's very high resolution --

00:02:03.000 --> 00:02:05.198
much higher than in an ordinary ad --

00:02:05.222 --> 00:02:06.976
and we've embedded extra content.

00:02:07.000 --> 00:02:10.048
If you want to see the features
of this car, you can see it here.

00:02:10.072 --> 00:02:14.252
Or other models,
or even technical specifications.

00:02:14.276 --> 00:02:17.591
And this really gets
at some of these ideas

00:02:17.615 --> 00:02:22.276
about really doing away
with those limits on screen real estate.

00:02:22.300 --> 00:02:24.411
We hope that this means no more pop-ups

00:02:24.435 --> 00:02:26.976
and other rubbish like that --
shouldn't be necessary.

00:02:27.000 --> 00:02:29.658
Of course, mapping is one
of those obvious applications

00:02:29.682 --> 00:02:30.976
for a technology like this.

00:02:31.000 --> 00:02:33.191
And this one I really
won't spend any time on,

00:02:33.215 --> 00:02:36.549
except to say that we have things
to contribute to this field as well.

00:02:37.213 --> 00:02:39.071
But those are all the roads in the U.S.

00:02:39.095 --> 00:02:43.660
superimposed on top
of a NASA geospatial image.

00:02:44.000 --> 00:02:45.976
So let's pull up, now, something else.

00:02:46.000 --> 00:02:48.976
This is actually live on the Web now;
you can go check it out.

00:02:49.000 --> 00:02:52.704
This is a project called Photosynth,
which marries two different technologies.

00:02:52.728 --> 00:02:53.976
One of them is Seadragon

00:02:54.000 --> 00:02:56.906
and the other is some very
beautiful computer-vision research

00:02:56.930 --> 00:03:00.392
done by Noah Snavely, a graduate student
at the University of Washington,

00:03:00.416 --> 00:03:02.245
co-advised by Steve Seitz at U.W.

00:03:02.269 --> 00:03:04.247
and Rick Szeliski at Microsoft Research.

00:03:04.271 --> 00:03:06.004
A very nice collaboration.

00:03:06.412 --> 00:03:09.520
And so this is live on the Web.
It's powered by Seadragon.

00:03:09.544 --> 00:03:12.048
You can see that
when we do these sorts of views,

00:03:12.072 --> 00:03:13.795
where we can dive through images

00:03:13.819 --> 00:03:16.153
and have this kind
of multi-resolution experience.

00:03:16.177 --> 00:03:19.976
But the spatial arrangement of the images
here is actually meaningful.

00:03:20.000 --> 00:03:23.191
The computer vision algorithms
have registered these images together

00:03:23.215 --> 00:03:26.976
so that they correspond to the real
space in which these shots --

00:03:27.000 --> 00:03:30.300
all taken near Grassi Lakes
in the Canadian Rockies --

00:03:30.324 --> 00:03:31.987
all these shots were taken.

00:03:32.011 --> 00:03:33.478
So you see elements here

00:03:33.502 --> 00:03:39.515
of stabilized slide-show
or panoramic imaging,

00:03:39.539 --> 00:03:41.976
and these things have
all been related spatially.

00:03:42.000 --> 00:03:45.000
I'm not sure if I have time
to show you any other environments.

00:03:45.024 --> 00:03:46.455
Some are much more spatial.

00:03:46.479 --> 00:03:50.424
I would like to jump straight
to one of Noah's original data-sets --

00:03:50.448 --> 00:03:54.000
this is from an early prototype
that we first got working this summer --

00:03:54.024 --> 00:03:55.918
to show you what I think

00:03:55.942 --> 00:03:59.780
is really the punch line
behind the Photosynth technology,

00:03:59.804 --> 00:04:01.365
It's not necessarily so apparent

00:04:01.389 --> 00:04:04.284
from looking at the environments
we've put up on the website.

00:04:04.308 --> 00:04:06.485
We had to worry
about the lawyers and so on.

00:04:06.509 --> 00:04:08.810
This is a reconstruction
of Notre Dame Cathedral

00:04:08.834 --> 00:04:12.291
that was done entirely computationally
from images scraped from Flickr.

00:04:12.315 --> 00:04:14.334
You just type Notre Dame into Flickr,

00:04:14.358 --> 00:04:18.212
and you get some pictures of guys
in T-shirts, and of the campus and so on.

00:04:18.236 --> 00:04:21.382
And each of these orange cones
represents an image

00:04:21.406 --> 00:04:24.640
that was discovered
to belong to this model.

00:04:26.000 --> 00:04:27.976
And so these are all Flickr images,

00:04:28.000 --> 00:04:30.976
and they've all been related
spatially in this way.

00:04:31.000 --> 00:04:33.334
We can just navigate
in this very simple way.

00:04:35.000 --> 00:04:38.920
(Applause)

00:04:42.557 --> 00:04:43.571
(Applause ends)

00:04:43.595 --> 00:04:46.549
You know, I never thought
that I'd end up working at Microsoft.

00:04:46.573 --> 00:04:49.573
It's very gratifying to have
this kind of reception here.

00:04:49.597 --> 00:04:52.976
(Laughter)

00:04:53.000 --> 00:04:58.048
I guess you can see this is
lots of different types of cameras:

00:04:58.072 --> 00:05:01.233
it's everything from cell-phone cameras
to professional SLRs,

00:05:01.257 --> 00:05:04.448
quite a large number of them,
stitched together in this environment.

00:05:04.472 --> 00:05:07.104
If I can find some
of the sort of weird ones --

00:05:08.000 --> 00:05:11.322
So many of them are occluded
by faces, and so on.

00:05:12.595 --> 00:05:16.872
Somewhere in here there is actually
a series of photographs -- here we go.

00:05:16.896 --> 00:05:20.197
This is actually a poster of Notre Dame
that registered correctly.

00:05:20.221 --> 00:05:23.437
We can dive in from the poster

00:05:23.461 --> 00:05:27.271
to a physical view of this environment.

00:05:31.421 --> 00:05:33.287
What the point here really is

00:05:33.311 --> 00:05:35.902
is that we can do things
with the social environment.

00:05:35.926 --> 00:05:38.928
This is now taking data from everybody --

00:05:38.952 --> 00:05:42.823
from the entire collective memory,
visually, of what the Earth looks like --

00:05:42.847 --> 00:05:44.596
and link all of that together.

00:05:44.620 --> 00:05:47.459
Those photos become linked,
and they make something emergent

00:05:47.483 --> 00:05:49.436
that's greater than the sum of the parts.

00:05:49.460 --> 00:05:51.816
You have a model that emerges
of the entire Earth.

00:05:51.840 --> 00:05:55.917
Think of this as the long tail
to Stephen Lawler's Virtual Earth work.

00:05:55.941 --> 00:05:59.141
And this is something that grows
in complexity as people use it,

00:05:59.165 --> 00:06:02.976
and whose benefits become greater
to the users as they use it.

00:06:03.000 --> 00:06:06.692
Their own photos are getting tagged
with meta-data that somebody else entered.

00:06:06.716 --> 00:06:10.076
If somebody bothered
to tag all of these saints

00:06:10.100 --> 00:06:13.053
and say who they all are,
then my photo of Notre Dame Cathedral

00:06:13.077 --> 00:06:15.175
suddenly gets enriched
with all of that data,

00:06:15.199 --> 00:06:17.976
and I can use it as an entry point
to dive into that space,

00:06:18.000 --> 00:06:20.681
into that meta-verse,
using everybody else's photos,

00:06:20.705 --> 00:06:24.006
and do a kind of a cross-modal

00:06:24.030 --> 00:06:27.781
and cross-user social experience that way.

00:06:27.805 --> 00:06:31.976
And of course, a by-product of all of that
is immensely rich virtual models

00:06:32.000 --> 00:06:33.968
of every interesting part of the Earth,

00:06:33.992 --> 00:06:38.479
collected not just from overhead flights
and from satellite images

00:06:38.503 --> 00:06:40.555
and so on, but from the collective memory.

00:06:40.579 --> 00:06:41.673
Thank you so much.

00:06:41.697 --> 00:06:48.560
(Applause)

00:06:51.967 --> 00:06:52.968
(Applause ends)

00:06:52.992 --> 00:06:55.318
Chris Anderson:
Do I understand this right?

00:06:55.342 --> 00:06:57.839
What your software is going to allow,

00:06:57.863 --> 00:07:01.339
is that at some point,
really within the next few years,

00:07:01.363 --> 00:07:05.598
all the pictures that are shared
by anyone across the world

00:07:05.622 --> 00:07:07.183
are going to link together?

00:07:07.207 --> 00:07:09.594
BAA: Yes. What this is really
doing is discovering,

00:07:09.618 --> 00:07:11.976
creating hyperlinks,
if you will, between images.

00:07:12.000 --> 00:07:14.584
It's doing that based on the content
inside the images.

00:07:14.608 --> 00:07:17.630
And that gets really exciting
when you think about the richness

00:07:17.654 --> 00:07:19.958
of the semantic information
a lot of images have.

00:07:19.982 --> 00:07:21.942
Like when you do a web search for images,

00:07:21.966 --> 00:07:23.211
you type in phrases,

00:07:23.235 --> 00:07:26.135
and the text on the web page is carrying
a lot of information

00:07:26.159 --> 00:07:27.661
about what that picture is of.

00:07:27.685 --> 00:07:30.076
What if that picture links
to all of your pictures?

00:07:30.100 --> 00:07:32.513
The amount of semantic
interconnection and richness

00:07:32.537 --> 00:07:34.391
that comes out of that is really huge.

00:07:34.415 --> 00:07:35.864
It's a classic network effect.

00:07:35.888 --> 00:07:37.912
CA: Truly incredible. Congratulations.

