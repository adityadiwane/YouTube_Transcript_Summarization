WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:07.000
翻译人员: Geng Luo
校对人员: dahong zhang

00:00:25.000 --> 00:00:27.000
首先，我要用最快的速度为大家演示

00:00:27.000 --> 00:00:31.000
一些新技术的基础研究成果。

00:00:31.000 --> 00:00:34.000
正好是一年前，微软收购了我们公司，

00:00:34.000 --> 00:00:37.000
而我们为微软带来了这项技术，它就是Seadragon。

00:00:37.000 --> 00:00:40.000
Seadragon是一个软件环境，你可以通过它以近景或远景的方式

00:00:40.000 --> 00:00:43.000
浏览浩瀚的可视化数据。

00:00:43.000 --> 00:00:46.000
我们这里看到的是许多许多GB（千兆字节）级别的数码照片，

00:00:46.000 --> 00:00:49.000
对它们可以进行持续并且平滑的放大，

00:00:50.000 --> 00:00:52.000
可以通过全景的方式浏览它们，还可以对它们进行重新排列。

00:00:52.000 --> 00:00:56.000
不管所见到的数据有多少、

00:00:56.000 --> 00:00:59.000
图像集有多大以及图像本身有多大，Seadragon都拥有这样的处理能力。

00:00:59.000 --> 00:01:01.000
以上展示的图片大部分都是由数码相机拍摄的照片，

00:01:01.000 --> 00:01:04.000
但这个例子则不同，它是一张来自国会图书馆的扫描图片，

00:01:05.000 --> 00:01:07.000
拥有3亿个像素。

00:01:08.000 --> 00:01:09.000
然而，浏览它并没有什么区别，

00:01:09.000 --> 00:01:12.000
因为限制系统性能的唯一因素只是：

00:01:12.000 --> 00:01:15.000
你所使用的屏幕的像素数。

00:01:15.000 --> 00:01:18.000
Seadragon同时也是一个非常灵活的架构。

00:01:18.000 --> 00:01:21.000
举个例子，这是一本完整的书，它的数据是非图像的(文本)。

00:01:22.000 --> 00:01:27.000
这是狄更斯所著的《荒凉山庄》，一列就是一章的内容。

00:01:27.000 --> 00:01:31.000
我给大家证明一下这真的是文本而非图片，

00:01:31.000 --> 00:01:33.000
我们可以这样操作，

00:01:33.000 --> 00:01:36.000
大家可以看出这真的是文本，而不是一幅图片。

00:01:37.000 --> 00:01:39.000
也许这会是一种阅读电子书的方式，

00:01:39.000 --> 00:01:40.000
但是我可不推荐这么做。

00:01:40.000 --> 00:01:43.000
接下来是一个更加实际的例子，这是一期《卫报》。

00:01:43.000 --> 00:01:45.000
每一张大图片是一版开篇，

00:01:45.000 --> 00:01:48.000
而报纸或者杂志的纸质版本本身就包含了多种比例的图片，

00:01:48.000 --> 00:01:53.000
在阅读的时候，读者会得到更好的阅读体验，

00:01:54.000 --> 00:01:55.000
从而享受阅读的乐趣。

00:01:56.000 --> 00:01:57.000
我们在这里做了小小的改动

00:01:57.000 --> 00:02:00.000
在这一期《卫报》得角上。

00:02:00.000 --> 00:02:03.000
我们虚构了一个高分辨率的广告图片——

00:02:03.000 --> 00:02:05.000
这比你平常看到的普通广告的分辨率要高很多，

00:02:05.000 --> 00:02:07.000
在图片中嵌入了额外的内容。

00:02:07.000 --> 00:02:09.000
如果你希望看到这辆车的特性，你可以看这里。

00:02:10.000 --> 00:02:14.000
你还能看到其他的型号，甚至技术规格。

00:02:15.000 --> 00:02:17.000
这种方式在一定程度上

00:02:18.000 --> 00:02:22.000
避免了屏幕实际使用面积的限制。

00:02:22.000 --> 00:02:24.000
我们希望这个技术能够减少不必要的弹出窗口

00:02:24.000 --> 00:02:26.000
以及类似的垃圾信息。

00:02:27.000 --> 00:02:29.000
显然，对于这项技术的应用，

00:02:29.000 --> 00:02:31.000
数字地图也是显而易见的应用之一。

00:02:31.000 --> 00:02:33.000
对此，我真的不想花费太多的时间进行介绍，

00:02:33.000 --> 00:02:35.000
我只想告诉大家我们已经对这个领域做出了自己的贡献。

00:02:37.000 --> 00:02:39.000
这些只是在NASA的地理空间图片基础上

00:02:39.000 --> 00:02:43.000
进行叠加处理而得到的美国的道路地图。

00:02:44.000 --> 00:02:46.000
现在，我们先放下这些，看看其他的。

00:02:46.000 --> 00:02:49.000
实际上，这项技术已经放到网上了，大家可以自己去体验一下。

00:02:49.000 --> 00:02:50.000
这个项目叫Photosynth，

00:02:51.000 --> 00:02:52.000
它实际上融合了两个不同的技术：

00:02:52.000 --> 00:02:53.000
一个是Seadragon，

00:02:54.000 --> 00:02:56.000
而另一个则是源自华盛顿大学的研究生Noah Snavely

00:02:57.000 --> 00:02:59.000
所进行的计算机视觉研究的成果。

00:03:00.000 --> 00:03:02.000
这项研究还得到了华盛顿大学Steve Seitz

00:03:02.000 --> 00:03:06.000
和微软研究院Rick Szeliski的协助。这是一个非常漂亮的合作成果。

00:03:07.000 --> 00:03:09.000
这个项目在互联网上已经得到应用了，它是基于Seadragon技术构建的。

00:03:09.000 --> 00:03:11.000
你可以看到，我们轻松地对图片进行多种方式的查看，

00:03:12.000 --> 00:03:13.000
从而能够对图片进行细致的剖析

00:03:14.000 --> 00:03:15.000
并且拥有多分辨率的浏览体验。

00:03:16.000 --> 00:03:20.000
不过，这些图片在三维空间的排列事实上是非常有意义的。

00:03:20.000 --> 00:03:23.000
计算机视觉算法将这些图片联系到一起，

00:03:23.000 --> 00:03:27.000
那么这些图片就能够将真实空间呈现出来了，

00:03:27.000 --> 00:03:29.000
而我们正是在这个空间里拍下了上述的照片——这些照片都是在

00:03:31.000 --> 00:03:33.000
加拿大落基山脉的格拉西湖（Grassi Lakes）附近拍下的——（所有照片）都是在这里拍下的。

00:03:33.000 --> 00:03:37.000
因此你可以看到这里的元素是稳定的幻灯放映或者全景成像，

00:03:40.000 --> 00:03:42.000
而这些内容在空间上都是关联的。

00:03:42.000 --> 00:03:45.000
我不确定我们是否有时间来展示更多的环境全景。

00:03:45.000 --> 00:03:46.000
有很多例子比这个的空间感还要强。

00:03:47.000 --> 00:03:50.000
下面让我们来看一下去年夏天，

00:03:50.000 --> 00:03:52.000
我们利用Noah早期的数据库之一

00:03:52.000 --> 00:03:54.000
所Photosynth的初期模型的建立。

00:03:54.000 --> 00:03:55.000
我认为

00:03:55.000 --> 00:03:58.000
这可谓是我们这项技术的最抢眼之处。

00:03:59.000 --> 00:04:01.000
这项技术不单单像我们在

00:04:01.000 --> 00:04:04.000
网站上展示得那么简单明了。

00:04:04.000 --> 00:04:06.000
主要因为我们制作网站时，要顾及到很多法律问题。

00:04:07.000 --> 00:04:08.000
这里是利用Flickr网站上

00:04:09.000 --> 00:04:11.000
的图像重建的巴黎圣母院。

00:04:11.000 --> 00:04:14.000
你所要做的只是在Flickr网站上输入“巴黎圣母院”

00:04:14.000 --> 00:04:17.000
然后便能看到很多图片，包括留影的游人等等。

00:04:17.000 --> 00:04:21.000
所有这些橘黄颜色的锥形都代表了一张

00:04:22.000 --> 00:04:24.000
用来建立模型的图片。

00:04:26.000 --> 00:04:28.000
这些全部是来自Flickr的图片，

00:04:28.000 --> 00:04:31.000
被这样在空间里被串联起来。

00:04:31.000 --> 00:04:33.000
接着，我们便可如此自如的进行浏览。

00:04:35.000 --> 00:04:44.000
（鼓掌）

00:04:44.000 --> 00:04:46.000
说实话，我从来没想过我会最后来为微软工作

00:04:46.000 --> 00:04:50.000
受到这样欢迎，真挺令人高兴的。

00:04:50.000 --> 00:04:53.000
（笑声）

00:04:53.000 --> 00:04:56.000
我想你们可以看出

00:04:56.000 --> 00:04:58.000
这些图片原自很多不同的相机：

00:04:58.000 --> 00:05:01.000
从手机摄像头到专业单反。

00:05:02.000 --> 00:05:03.000
如此大量的不同质量的照片，全被在这个环境下

00:05:03.000 --> 00:05:04.000
拼合在了一起

00:05:04.000 --> 00:05:06.000
让我来找些比较诡异的图片。

00:05:08.000 --> 00:05:11.000
看，不少照片包含了游客的大头照等等。

00:05:13.000 --> 00:05:14.000
我记得这儿应该有

00:05:15.000 --> 00:05:16.000
一个系列的照片 - 啊，在这儿。

00:05:17.000 --> 00:05:20.000
这个是巴黎圣母院的海报。

00:05:21.000 --> 00:05:23.000
我们可以钻到海报里

00:05:24.000 --> 00:05:27.000
去看整个重建的环境。

00:05:31.000 --> 00:05:34.000
这里的重点呢便是我们可以

00:05:34.000 --> 00:05:39.000
有效地利用网络社区。我们可以从每个人那里得到数据

00:05:39.000 --> 00:05:40.000
将每个人对不同环境

00:05:40.000 --> 00:05:42.000
的记忆收集在一起，

00:05:43.000 --> 00:05:44.000
共建成模型。

00:05:44.000 --> 00:05:46.000
当所有这些图片交织在一起时，

00:05:46.000 --> 00:05:47.000
所衍生出的

00:05:47.000 --> 00:05:49.000
要远远超过单单收集起全部。

00:05:49.000 --> 00:05:51.000
这个模型所衍生出的，是整个地球。

00:05:51.000 --> 00:05:56.000
这如同是Stephen Lawler的《虚拟地球》的长尾市场。（Stephen Lawler 微软Virtual Earth项目主管）（见Long tail 长尾市场 TED: Chris Anderson ）

00:05:56.000 --> 00:05:58.000
这类模型，会随着人们的

00:05:58.000 --> 00:06:01.000
使用而不断变的复杂，

00:06:01.000 --> 00:06:03.000
变得更加有价值。

00:06:03.000 --> 00:06:05.000
用户的照片，会被大家

00:06:05.000 --> 00:06:06.000
注上标签。

00:06:07.000 --> 00:06:10.000
如果有人愿意为所有这些圣母院里的圣贤注上标签，

00:06:10.000 --> 00:06:13.000
表明他们是谁，那我们的圣母院照片便会

00:06:13.000 --> 00:06:15.000
一下子丰富起来，

00:06:15.000 --> 00:06:18.000
然后呢，我们便能以这张照片为起点，进入这个空间，

00:06:18.000 --> 00:06:20.000
这个由很多人的照片所搭建的虚拟世界，

00:06:21.000 --> 00:06:23.000
从而得到一种跨越模型，

00:06:25.000 --> 00:06:28.000
跨越用户的交互体验。

00:06:28.000 --> 00:06:29.000
当然了，这一切所带来另外一个宝贵产物便是

00:06:30.000 --> 00:06:32.000
一个非常丰富的模型 - 充斥

00:06:32.000 --> 00:06:34.000
这地球每个角落里有趣的景观。这些景观不再

00:06:35.000 --> 00:06:38.000
局限于航空和卫星图片，

00:06:38.000 --> 00:06:40.000
而是实实在在的人们按下快门一刻所收藏的记忆的集合。

00:06:40.000 --> 00:06:42.000
非常感谢！

00:06:42.000 --> 00:06:53.000
（掌声）

00:06:53.000 --> 00:06:57.000
Chris Anderson: 如果我理解正确的话，你们的这个软件将能够

00:06:58.000 --> 00:07:00.000
在未来的几年内

00:07:01.000 --> 00:07:05.000
将来自全球的图片

00:07:05.000 --> 00:07:07.000
接合在一起？

00:07:07.000 --> 00:07:09.000
BAA:是的。这个软件的真正意义便是去探索。

00:07:09.000 --> 00:07:12.000
它在图片间构建起超链接。

00:07:12.000 --> 00:07:13.000
这个接合的过程

00:07:13.000 --> 00:07:14.000
完全是基于图片的内容。

00:07:14.000 --> 00:07:17.000
更令人兴奋的

00:07:17.000 --> 00:07:19.000
在于图片所包含的大量文字语义信息。

00:07:19.000 --> 00:07:21.000
比如，你在网上所以一张图片，

00:07:22.000 --> 00:07:24.000
键入关键词后，网页上的文字内容

00:07:24.000 --> 00:07:27.000
将包含大量与这个图片相关的信息。

00:07:27.000 --> 00:07:29.000
现在，假设这些图片全都与你的图片相连，那将会怎样？

00:07:29.000 --> 00:07:31.000
那时，所以这些语义信息的相互链接

00:07:31.000 --> 00:07:32.000
以及内容量将是

00:07:32.000 --> 00:07:35.000
巨大的。这将是非常典型的网络效应。

00:07:35.000 --> 00:07:37.000
CA:Blaise，太难以置信了。祝贺你们！

00:07:37.000 --> 00:07:38.000
BAA:非常感谢各位！

