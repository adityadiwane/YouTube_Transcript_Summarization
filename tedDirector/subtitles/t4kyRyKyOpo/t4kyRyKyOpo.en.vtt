WEBVTT
Kind: captions
Language: en

00:00:12.880 --> 00:00:16.893
It used to be that if you wanted
to get a computer to do something new,

00:00:16.893 --> 00:00:18.447
you would have to program it.

00:00:18.447 --> 00:00:21.858
Now, programming, for those of you here
that haven't done it yourself,

00:00:21.858 --> 00:00:25.360
requires laying out in excruciating detail

00:00:25.360 --> 00:00:28.727
every single step that you want
the computer to do

00:00:28.727 --> 00:00:31.089
in order to achieve your goal.

00:00:31.089 --> 00:00:34.585
Now, if you want to do something
that you don't know how to do yourself,

00:00:34.585 --> 00:00:36.648
then this is going
to be a great challenge.

00:00:36.648 --> 00:00:40.131
So this was the challenge faced
by this man, Arthur Samuel.

00:00:40.131 --> 00:00:44.208
In 1956, he wanted to get this computer

00:00:44.208 --> 00:00:46.548
to be able to beat him at checkers.

00:00:46.548 --> 00:00:48.588
How can you write a program,

00:00:48.588 --> 00:00:52.394
lay out in excruciating detail,
how to be better than you at checkers?

00:00:52.394 --> 00:00:54.116
So he came up with an idea:

00:00:54.116 --> 00:00:57.840
he had the computer play
against itself thousands of times

00:00:57.840 --> 00:01:00.364
and learn how to play checkers.

00:01:00.364 --> 00:01:03.544
And indeed it worked,
and in fact, by 1962,

00:01:03.544 --> 00:01:07.561
this computer had beaten
the Connecticut state champion.

00:01:07.561 --> 00:01:10.534
So Arthur Samuel was
the father of machine learning,

00:01:10.534 --> 00:01:12.251
and I have a great debt to him,

00:01:12.251 --> 00:01:15.014
because I am a machine
learning practitioner.

00:01:15.014 --> 00:01:16.479
I was the president of Kaggle,

00:01:16.479 --> 00:01:19.867
a community of over 200,000
machine learning practictioners.

00:01:19.867 --> 00:01:21.925
Kaggle puts up competitions

00:01:21.925 --> 00:01:25.633
to try and get them to solve
previously unsolved problems,

00:01:25.633 --> 00:01:29.470
and it's been successful 
hundreds of times.

00:01:29.470 --> 00:01:31.940
So from this vantage point,
I was able to find out

00:01:31.940 --> 00:01:35.890
a lot about what machine learning
can do in the past, can do today,

00:01:35.890 --> 00:01:38.252
and what it could do in the future.

00:01:38.252 --> 00:01:42.675
Perhaps the first big success of 
machine learning commercially was Google.

00:01:42.675 --> 00:01:45.784
Google showed that it is
possible to find information

00:01:45.784 --> 00:01:47.536
by using a computer algorithm,

00:01:47.536 --> 00:01:50.437
and this algorithm is based
on machine learning.

00:01:50.437 --> 00:01:54.323
Since that time, there have been many
commercial successes of machine learning.

00:01:54.323 --> 00:01:56.160
Companies like Amazon and Netflix

00:01:56.160 --> 00:01:59.876
use machine learning to suggest
products that you might like to buy,

00:01:59.876 --> 00:02:01.896
movies that you might like to watch.

00:02:01.896 --> 00:02:03.703
Sometimes, it's almost creepy.

00:02:03.703 --> 00:02:05.657
Companies like LinkedIn and Facebook

00:02:05.657 --> 00:02:08.251
sometimes will tell you about
who your friends might be

00:02:08.251 --> 00:02:10.228
and you have no idea how it did it,

00:02:10.228 --> 00:02:13.195
and this is because it's using
the power of machine learning.

00:02:13.195 --> 00:02:16.152
These are algorithms that have
learned how to do this from data

00:02:16.152 --> 00:02:19.399
rather than being programmed by hand.

00:02:19.399 --> 00:02:21.877
This is also how IBM was successful

00:02:21.877 --> 00:02:25.739
in getting Watson to beat
the two world champions at "Jeopardy,"

00:02:25.739 --> 00:02:28.964
answering incredibly subtle
and complex questions like this one.

00:02:28.964 --> 00:02:31.799
["The ancient 'Lion of Nimrud' went missing
from this city's national museum in 2003 
(along with a lot of other stuff)"]

00:02:31.799 --> 00:02:35.034
This is also why we are now able
to see the first self-driving cars.

00:02:35.034 --> 00:02:37.856
If you want to be able to tell
the difference between, say,

00:02:37.856 --> 00:02:40.488
a tree and a pedestrian,
well, that's pretty important.

00:02:40.488 --> 00:02:43.075
We don't know how to write
those programs by hand,

00:02:43.075 --> 00:02:46.072
but with machine learning,
this is now possible.

00:02:46.072 --> 00:02:48.680
And in fact, this car has driven 
over a million miles

00:02:48.680 --> 00:02:52.186
without any accidents on regular roads.

00:02:52.196 --> 00:02:56.110
So we now know that computers can learn,

00:02:56.110 --> 00:02:58.010
and computers can learn to do things

00:02:58.010 --> 00:03:00.848
that we actually sometimes
don't know how to do ourselves,

00:03:00.848 --> 00:03:03.733
or maybe can do them better than us.

00:03:03.733 --> 00:03:07.928
One of the most amazing examples
I've seen of machine learning

00:03:07.928 --> 00:03:10.320
happened on a project that I ran at Kaggle

00:03:10.320 --> 00:03:13.911
where a team run by a guy
called Geoffrey Hinton

00:03:13.911 --> 00:03:15.463
from the University of Toronto

00:03:15.463 --> 00:03:18.140
won a competition for
automatic drug discovery.

00:03:18.140 --> 00:03:20.987
Now, what was extraordinary here
is not just that they beat

00:03:20.987 --> 00:03:25.000
all of the algorithms developed by Merck
or the international academic community,

00:03:25.000 --> 00:03:30.061
but nobody on the team had any background
in chemistry or biology or life sciences,

00:03:30.061 --> 00:03:32.230
and they did it in two weeks.

00:03:32.230 --> 00:03:33.611
How did they do this?

00:03:34.421 --> 00:03:37.342
They used an extraordinary algorithm
called deep learning.

00:03:37.342 --> 00:03:40.291
So important was this that in fact
the success was covered

00:03:40.291 --> 00:03:43.412
in The New York Times in a front page
article a few weeks later.

00:03:43.412 --> 00:03:46.147
This is Geoffrey Hinton
here on the left-hand side.

00:03:46.147 --> 00:03:50.488
Deep learning is an algorithm
inspired by how the human brain works,

00:03:50.488 --> 00:03:52.300
and as a result it's an algorithm

00:03:52.300 --> 00:03:56.141
which has no theoretical limitations
on what it can do.

00:03:56.141 --> 00:03:58.964
The more data you give it and the more
computation time you give it,

00:03:58.964 --> 00:04:00.276
the better it gets.

00:04:00.276 --> 00:04:02.615
The New York Times also
showed in this article

00:04:02.615 --> 00:04:04.857
another extraordinary
result of deep learning

00:04:04.857 --> 00:04:07.569
which I'm going to show you now.

00:04:07.569 --> 00:04:12.510
It shows that computers 
can listen and understand.

00:04:12.510 --> 00:04:15.221
(Video) Richard Rashid: Now, the last step

00:04:15.221 --> 00:04:18.246
that I want to be able
to take in this process

00:04:18.246 --> 00:04:22.961
is to actually speak to you in Chinese.

00:04:22.961 --> 00:04:25.596
Now the key thing there is,

00:04:25.596 --> 00:04:30.598
we've been able to take a large amount 
of information from many Chinese speakers

00:04:30.598 --> 00:04:33.128
and produce a text-to-speech system

00:04:33.128 --> 00:04:37.801
that takes Chinese text
and converts it into Chinese language,

00:04:37.801 --> 00:04:41.929
and then we've taken
an hour or so of my own voice

00:04:41.929 --> 00:04:43.820
and we've used that to modulate

00:04:43.820 --> 00:04:48.364
the standard text-to-speech system
so that it would sound like me.

00:04:48.364 --> 00:04:50.904
Again, the result's not perfect.

00:04:50.904 --> 00:04:53.552
There are in fact quite a few errors.

00:04:53.552 --> 00:04:56.036
(In Chinese)

00:04:56.036 --> 00:04:59.403
(Applause)

00:05:01.446 --> 00:05:05.022
There's much work to be done in this area.

00:05:05.022 --> 00:05:08.667
(In Chinese)

00:05:08.667 --> 00:05:12.100
(Applause)

00:05:13.345 --> 00:05:16.744
Jeremy Howard: Well, that was at
a machine learning conference in China.

00:05:16.744 --> 00:05:19.114
It's not often, actually,
at academic conferences

00:05:19.114 --> 00:05:21.011
that you do hear spontaneous applause,

00:05:21.011 --> 00:05:24.687
although of course sometimes
at TEDx conferences, feel free.

00:05:24.687 --> 00:05:27.482
Everything you saw there
was happening with deep learning.

00:05:27.482 --> 00:05:29.007
(Applause) Thank you.

00:05:29.007 --> 00:05:31.289
The transcription in English
was deep learning.

00:05:31.289 --> 00:05:34.701
The translation to Chinese and the text
in the top right, deep learning,

00:05:34.701 --> 00:05:38.008
and the construction of the voice
was deep learning as well.

00:05:38.008 --> 00:05:41.242
So deep learning is
this extraordinary thing.

00:05:41.242 --> 00:05:44.341
It's a single algorithm that
can seem to do almost anything,

00:05:44.341 --> 00:05:47.452
and I discovered that a year earlier,
it had also learned to see.

00:05:47.452 --> 00:05:49.628
In this obscure competition from Germany

00:05:49.628 --> 00:05:52.225
called the German Traffic Sign 
Recognition Benchmark,

00:05:52.225 --> 00:05:55.618
deep learning had learned
to recognize traffic signs like this one.

00:05:55.618 --> 00:05:57.712
Not only could it
recognize the traffic signs

00:05:57.712 --> 00:05:59.470
better than any other algorithm,

00:05:59.470 --> 00:06:02.189
the leaderboard actually showed
it was better than people,

00:06:02.189 --> 00:06:04.041
about twice as good as people.

00:06:04.041 --> 00:06:06.037
So by 2011, we had the first example

00:06:06.037 --> 00:06:09.442
of computers that can see
better than people.

00:06:09.442 --> 00:06:11.491
Since that time, a lot has happened.

00:06:11.491 --> 00:06:15.005
In 2012, Google announced that
they had a deep learning algorithm

00:06:15.005 --> 00:06:16.420
watch YouTube videos

00:06:16.420 --> 00:06:19.857
and crunched the data
on 16,000 computers for a month,

00:06:19.857 --> 00:06:24.218
and the computer independently learned
about concepts such as people and cats

00:06:24.218 --> 00:06:26.027
just by watching the videos.

00:06:26.027 --> 00:06:28.379
This is much like the way
that humans learn.

00:06:28.379 --> 00:06:31.119
Humans don't learn
by being told what they see,

00:06:31.119 --> 00:06:34.450
but by learning for themselves
what these things are.

00:06:34.450 --> 00:06:37.819
Also in 2012, Geoffrey Hinton,
who we saw earlier,

00:06:37.819 --> 00:06:40.677
won the very popular ImageNet competition,

00:06:40.677 --> 00:06:44.818
looking to try to figure out 
from one and a half million images

00:06:44.818 --> 00:06:46.256
what they're pictures of.

00:06:46.256 --> 00:06:49.789
As of 2014, we're now down
to a six percent error rate

00:06:49.789 --> 00:06:51.242
in image recognition.

00:06:51.242 --> 00:06:53.268
This is better than people, again.

00:06:53.268 --> 00:06:57.037
So machines really are doing
an extraordinarily good job of this,

00:06:57.037 --> 00:06:59.306
and it is now being used in industry.

00:06:59.306 --> 00:07:02.348
For example, Google announced last year

00:07:02.348 --> 00:07:06.933
that they had mapped every single
location in France in two hours,

00:07:06.933 --> 00:07:10.380
and the way they did it was
that they fed street view images

00:07:10.380 --> 00:07:14.699
into a deep learning algorithm
to recognize and read street numbers.

00:07:14.699 --> 00:07:16.919
Imagine how long
it would have taken before:

00:07:16.919 --> 00:07:20.274
dozens of people, many years.

00:07:20.274 --> 00:07:22.185
This is also happening in China.

00:07:22.185 --> 00:07:26.221
Baidu is kind of 
the Chinese Google, I guess,

00:07:26.221 --> 00:07:28.504
and what you see here in the top left

00:07:28.504 --> 00:07:32.478
is an example of a picture that I uploaded
to Baidu's deep learning system,

00:07:32.478 --> 00:07:36.247
and underneath you can see that the system
has understood what that picture is

00:07:36.247 --> 00:07:38.483
and found similar images.

00:07:38.483 --> 00:07:41.219
The similar images actually
have similar backgrounds,

00:07:41.219 --> 00:07:42.877
similar directions of the faces,

00:07:42.877 --> 00:07:44.665
even some with their tongue out.

00:07:44.665 --> 00:07:47.695
This is not clearly looking
at the text of a web page.

00:07:47.695 --> 00:07:49.107
All I uploaded was an image.

00:07:49.107 --> 00:07:53.128
So we now have computers which
really understand what they see

00:07:53.128 --> 00:07:54.752
and can therefore search databases

00:07:54.752 --> 00:07:58.306
of hundreds of millions
of images in real time.

00:07:58.306 --> 00:08:01.536
So what does it mean
now that computers can see?

00:08:01.536 --> 00:08:03.553
Well, it's not just 
that computers can see.

00:08:03.553 --> 00:08:05.622
In fact, deep learning
has done more than that.

00:08:05.622 --> 00:08:08.570
Complex, nuanced sentences like this one

00:08:08.570 --> 00:08:11.394
are now understandable
with deep learning algorithms.

00:08:11.394 --> 00:08:12.697
As you can see here,

00:08:12.697 --> 00:08:15.465
this Stanford-based system
showing the red dot at the top

00:08:15.465 --> 00:08:19.384
has figured out that this sentence
is expressing negative sentiment.

00:08:19.384 --> 00:08:22.790
Deep learning now in fact
is near human performance

00:08:22.802 --> 00:08:27.923
at understanding what sentences are about
and what it is saying about those things.

00:08:27.923 --> 00:08:30.651
Also, deep learning has
been used to read Chinese,

00:08:30.651 --> 00:08:33.807
again at about native
Chinese speaker level.

00:08:33.807 --> 00:08:35.975
This algorithm developed
out of Switzerland

00:08:35.975 --> 00:08:39.331
by people, none of whom speak
or understand any Chinese.

00:08:39.331 --> 00:08:41.382
As I say, using deep learning

00:08:41.382 --> 00:08:43.601
is about the best system
in the world for this,

00:08:43.601 --> 00:08:48.718
even compared to native
human understanding.

00:08:48.718 --> 00:08:51.682
This is a system that we
put together at my company

00:08:51.682 --> 00:08:53.728
which shows putting
all this stuff together.

00:08:53.728 --> 00:08:56.189
These are pictures which
have no text attached,

00:08:56.189 --> 00:08:58.541
and as I'm typing in here sentences,

00:08:58.541 --> 00:09:01.510
in real time it's understanding
these pictures

00:09:01.510 --> 00:09:03.189
and figuring out what they're about

00:09:03.189 --> 00:09:06.352
and finding pictures that are similar
to the text that I'm writing.

00:09:06.352 --> 00:09:09.108
So you can see, it's actually
understanding my sentences

00:09:09.108 --> 00:09:11.332
and actually understanding these pictures.

00:09:11.332 --> 00:09:13.891
I know that you've seen
something like this on Google,

00:09:13.891 --> 00:09:16.666
where you can type in things
and it will show you pictures,

00:09:16.666 --> 00:09:20.090
but actually what it's doing is it's
searching the webpage for the text.

00:09:20.090 --> 00:09:23.091
This is very different from actually
understanding the images.

00:09:23.091 --> 00:09:25.843
This is something that computers
have only been able to do

00:09:25.843 --> 00:09:29.091
for the first time in the last few months.

00:09:29.091 --> 00:09:33.182
So we can see now that computers
can not only see but they can also read,

00:09:33.182 --> 00:09:36.947
and, of course, we've shown that they
can understand what they hear.

00:09:36.947 --> 00:09:40.389
Perhaps not surprising now that
I'm going to tell you they can write.

00:09:40.389 --> 00:09:45.172
Here is some text that I generated
using a deep learning algorithm yesterday.

00:09:45.172 --> 00:09:49.096
And here is some text that an algorithm
out of Stanford generated.

00:09:49.096 --> 00:09:50.860
Each of these sentences was generated

00:09:50.860 --> 00:09:55.109
by a deep learning algorithm
to describe each of those pictures.

00:09:55.109 --> 00:09:59.581
This algorithm before has never seen
a man in a black shirt playing a guitar.

00:09:59.581 --> 00:10:01.801
It's seen a man before,
it's seen black before,

00:10:01.801 --> 00:10:03.400
it's seen a guitar before,

00:10:03.400 --> 00:10:07.694
but it has independently generated
this novel description of this picture.

00:10:07.694 --> 00:10:11.196
We're still not quite at human
performance here, but we're close.

00:10:11.196 --> 00:10:15.264
In tests, humans prefer
the computer-generated caption

00:10:15.264 --> 00:10:16.791
one out of four times.

00:10:16.791 --> 00:10:18.855
Now this system is now only two weeks old,

00:10:18.855 --> 00:10:20.701
so probably within the next year,

00:10:20.701 --> 00:10:23.502
the computer algorithm will be
well past human performance

00:10:23.502 --> 00:10:25.364
at the rate things are going.

00:10:25.364 --> 00:10:28.413
So computers can also write.

00:10:28.413 --> 00:10:31.888
So we put all this together and it leads
to very exciting opportunities.

00:10:31.888 --> 00:10:33.380
For example, in medicine,

00:10:33.380 --> 00:10:35.905
a team in Boston announced
that they had discovered

00:10:35.905 --> 00:10:38.854
dozens of new clinically relevant features

00:10:38.854 --> 00:10:43.120
of tumors which help doctors
make a prognosis of a cancer.

00:10:44.220 --> 00:10:46.516
Very similarly, in Stanford,

00:10:46.516 --> 00:10:50.179
a group there announced that,
looking at tissues under magnification,

00:10:50.179 --> 00:10:52.560
they've developed 
a machine learning-based system

00:10:52.560 --> 00:10:55.142
which in fact is better
than human pathologists

00:10:55.142 --> 00:10:59.519
at predicting survival rates
for cancer sufferers.

00:10:59.519 --> 00:11:02.764
In both of these cases, not only
were the predictions more accurate,

00:11:02.764 --> 00:11:05.266
but they generated new insightful science.

00:11:05.276 --> 00:11:06.781
In the radiology case,

00:11:06.781 --> 00:11:09.876
they were new clinical indicators
that humans can understand.

00:11:09.876 --> 00:11:11.668
In this pathology case,

00:11:11.668 --> 00:11:16.168
the computer system actually discovered
that the cells around the cancer

00:11:16.168 --> 00:11:19.508
are as important as
the cancer cells themselves

00:11:19.508 --> 00:11:21.260
in making a diagnosis.

00:11:21.260 --> 00:11:26.621
This is the opposite of what pathologists
had been taught for decades.

00:11:26.621 --> 00:11:29.913
In each of those two cases,
they were systems developed

00:11:29.913 --> 00:11:33.534
by a combination of medical experts
and machine learning experts,

00:11:33.534 --> 00:11:36.275
but as of last year,
we're now beyond that too.

00:11:36.275 --> 00:11:39.824
This is an example of
identifying cancerous areas

00:11:39.824 --> 00:11:42.354
of human tissue under a microscope.

00:11:42.354 --> 00:11:46.967
The system being shown here
can identify those areas more accurately,

00:11:46.967 --> 00:11:49.742
or about as accurately,
as human pathologists,

00:11:49.742 --> 00:11:53.134
but was built entirely with deep learning
using no medical expertise

00:11:53.134 --> 00:11:55.660
by people who have
no background in the field.

00:11:56.730 --> 00:11:59.285
Similarly, here, this neuron segmentation.

00:11:59.285 --> 00:12:02.953
We can now segment neurons
about as accurately as humans can,

00:12:02.953 --> 00:12:05.670
but this system was developed
with deep learning

00:12:05.670 --> 00:12:08.921
using people with no previous 
background in medicine.

00:12:08.921 --> 00:12:12.148
So myself, as somebody with
no previous background in medicine,

00:12:12.148 --> 00:12:15.875
I seem to be entirely well qualified
to start a new medical company,

00:12:15.875 --> 00:12:18.021
which I did.

00:12:18.021 --> 00:12:19.761
I was kind of terrified of doing it,

00:12:19.761 --> 00:12:22.650
but the theory seemed to suggest
that it ought to be possible

00:12:22.650 --> 00:12:28.142
to do very useful medicine
using just these data analytic techniques.

00:12:28.142 --> 00:12:30.622
And thankfully, the feedback
has been fantastic,

00:12:30.622 --> 00:12:32.978
not just from the media
but from the medical community,

00:12:32.978 --> 00:12:35.322
who have been very supportive.

00:12:35.322 --> 00:12:39.471
The theory is that we can take
the middle part of the medical process

00:12:39.471 --> 00:12:42.364
and turn that into data analysis
as much as possible,

00:12:42.364 --> 00:12:45.429
leaving doctors to do
what they're best at.

00:12:45.429 --> 00:12:47.031
I want to give you an example.

00:12:47.031 --> 00:12:51.975
It now takes us about 15 minutes
to generate a new medical diagnostic test

00:12:51.975 --> 00:12:53.929
and I'll show you that in real time now,

00:12:53.929 --> 00:12:57.416
but I've compressed it down to 
three minutes by cutting some pieces out.

00:12:57.416 --> 00:13:00.477
Rather than showing you
creating a medical diagnostic test,

00:13:00.477 --> 00:13:03.846
I'm going to show you 
a diagnostic test of car images,

00:13:03.846 --> 00:13:06.068
because that's something
we can all understand.

00:13:06.068 --> 00:13:09.269
So here we're starting with 
about 1.5 million car images,

00:13:09.269 --> 00:13:12.475
and I want to create something
that can split them into the angle

00:13:12.475 --> 00:13:14.698
of the photo that's being taken.

00:13:14.698 --> 00:13:18.586
So these images are entirely unlabeled,
so I have to start from scratch.

00:13:18.586 --> 00:13:20.451
With our deep learning algorithm,

00:13:20.451 --> 00:13:24.158
it can automatically identify
areas of structure in these images.

00:13:24.158 --> 00:13:27.778
So the nice thing is that the human
and the computer can now work together.

00:13:27.778 --> 00:13:29.956
So the human, as you can see here,

00:13:29.956 --> 00:13:32.631
is telling the computer
about areas of interest

00:13:32.631 --> 00:13:37.281
which it wants the computer then
to try and use to improve its algorithm.

00:13:37.281 --> 00:13:41.577
Now, these deep learning systems actually
are in 16,000-dimensional space,

00:13:41.577 --> 00:13:45.009
so you can see here the computer
rotating this through that space,

00:13:45.009 --> 00:13:47.001
trying to find new areas of structure.

00:13:47.001 --> 00:13:48.782
And when it does so successfully,

00:13:48.782 --> 00:13:52.786
the human who is driving it can then
point out the areas that are interesting.

00:13:52.786 --> 00:13:55.208
So here, the computer has
successfully found areas,

00:13:55.208 --> 00:13:57.770
for example, angles.

00:13:57.770 --> 00:13:59.376
So as we go through this process,

00:13:59.376 --> 00:14:01.716
we're gradually telling
the computer more and more

00:14:01.716 --> 00:14:04.144
about the kinds of structures
we're looking for.

00:14:04.144 --> 00:14:05.916
You can imagine in a diagnostic test

00:14:05.916 --> 00:14:09.266
this would be a pathologist identifying
areas of pathosis, for example,

00:14:09.266 --> 00:14:14.292
or a radiologist indicating
potentially troublesome nodules.

00:14:14.292 --> 00:14:16.851
And sometimes it can be
difficult for the algorithm.

00:14:16.851 --> 00:14:18.815
In this case, it got kind of confused.

00:14:18.815 --> 00:14:21.365
The fronts and the backs
of the cars are all mixed up.

00:14:21.365 --> 00:14:23.437
So here we have to be a bit more careful,

00:14:23.437 --> 00:14:26.669
manually selecting these fronts
as opposed to the backs,

00:14:26.669 --> 00:14:32.175
then telling the computer
that this is a type of group

00:14:32.175 --> 00:14:33.523
that we're interested in.

00:14:33.523 --> 00:14:36.200
So we do that for a while,
we skip over a little bit,

00:14:36.200 --> 00:14:38.446
and then we train the
machine learning algorithm

00:14:38.446 --> 00:14:40.420
based on these couple of hundred things,

00:14:40.420 --> 00:14:42.445
and we hope that it's gotten a lot better.

00:14:42.445 --> 00:14:45.518
You can see, it's now started to fade
some of these pictures out,

00:14:45.518 --> 00:14:50.226
showing us that it already is recognizing
how to understand some of these itself.

00:14:50.226 --> 00:14:53.128
We can then use this concept
of similar images,

00:14:53.128 --> 00:14:55.222
and using similar images, you can now see,

00:14:55.222 --> 00:14:59.241
the computer at this point is able to
entirely find just the fronts of cars.

00:14:59.241 --> 00:15:02.189
So at this point, the human
can tell the computer,

00:15:02.189 --> 00:15:04.482
okay, yes, you've done
a good job of that.

00:15:05.652 --> 00:15:07.837
Sometimes, of course, even at this point

00:15:07.837 --> 00:15:11.511
it's still difficult
to separate out groups.

00:15:11.511 --> 00:15:15.395
In this case, even after we let the
computer try to rotate this for a while,

00:15:15.399 --> 00:15:18.744
we still find that the left sides
and the right sides pictures

00:15:18.744 --> 00:15:20.222
are all mixed up together.

00:15:20.222 --> 00:15:22.362
So we can again give
the computer some hints,

00:15:22.362 --> 00:15:25.338
and we say, okay, try and find
a projection that separates out

00:15:25.338 --> 00:15:27.945
the left sides and the right sides
as much as possible

00:15:27.945 --> 00:15:30.067
using this deep learning algorithm.

00:15:30.067 --> 00:15:33.009
And giving it that hint --
ah, okay, it's been successful.

00:15:33.009 --> 00:15:35.891
It's managed to find a way
of thinking about these objects

00:15:35.891 --> 00:15:38.271
that's separated out these together.

00:15:38.271 --> 00:15:40.709
So you get the idea here.

00:15:40.709 --> 00:15:48.906
This is a case not where the human
is being replaced by a computer,

00:15:48.906 --> 00:15:51.546
but where they're working together.

00:15:51.546 --> 00:15:55.096
What we're doing here is we're replacing
something that used to take a team

00:15:55.096 --> 00:15:57.098
of five or six people about seven years

00:15:57.098 --> 00:15:59.703
and replacing it with something
that takes 15 minutes

00:15:59.703 --> 00:16:02.208
for one person acting alone.

00:16:02.208 --> 00:16:06.158
So this process takes about
four or five iterations.

00:16:06.158 --> 00:16:08.017
You can see we now have 62 percent

00:16:08.017 --> 00:16:10.976
of our 1.5 million images 
classified correctly.

00:16:10.976 --> 00:16:13.448
And at this point, we
can start to quite quickly

00:16:13.448 --> 00:16:14.745
grab whole big sections,

00:16:14.745 --> 00:16:17.664
check through them to make sure
that there's no mistakes.

00:16:17.664 --> 00:16:21.616
Where there are mistakes, we can
let the computer know about them.

00:16:21.616 --> 00:16:24.661
And using this kind of process
for each of the different groups,

00:16:24.661 --> 00:16:27.148
we are now up to
an 80 percent success rate

00:16:27.148 --> 00:16:29.563
in classifying the 1.5 million images.

00:16:29.563 --> 00:16:31.641
And at this point, it's just a case

00:16:31.641 --> 00:16:35.220
of finding the small number
that aren't classified correctly,

00:16:35.220 --> 00:16:38.108
and trying to understand why.

00:16:38.108 --> 00:16:39.851
And using that approach,

00:16:39.851 --> 00:16:43.972
by 15 minutes we get
to 97 percent classification rates.

00:16:43.972 --> 00:16:48.572
So this kind of technique
could allow us to fix a major problem,

00:16:48.578 --> 00:16:51.614
which is that there's a lack
of medical expertise in the world.

00:16:51.614 --> 00:16:55.103
The World Economic Forum says
that there's between a 10x and a 20x

00:16:55.103 --> 00:16:57.727
shortage of physicians
in the developing world,

00:16:57.727 --> 00:16:59.840
and it would take about 300 years

00:16:59.840 --> 00:17:02.734
to train enough people
to fix that problem.

00:17:02.734 --> 00:17:05.619
So imagine if we can help
enhance their efficiency

00:17:05.619 --> 00:17:08.458
using these deep learning approaches?

00:17:08.458 --> 00:17:10.690
So I'm very excited
about the opportunities.

00:17:10.690 --> 00:17:13.279
I'm also concerned about the problems.

00:17:13.279 --> 00:17:16.403
The problem here is that
every area in blue on this map

00:17:16.403 --> 00:17:20.172
is somewhere where services
are over 80 percent of employment.

00:17:20.172 --> 00:17:21.959
What are services?

00:17:21.959 --> 00:17:23.473
These are services.

00:17:23.473 --> 00:17:27.627
These are also the exact things that
computers have just learned how to do.

00:17:27.627 --> 00:17:31.431
So 80 percent of the world's employment
in the developed world

00:17:31.431 --> 00:17:33.963
is stuff that computers 
have just learned how to do.

00:17:33.963 --> 00:17:35.403
What does that mean?

00:17:35.403 --> 00:17:37.986
Well, it'll be fine.
They'll be replaced by other jobs.

00:17:37.986 --> 00:17:40.693
For example, there will be
more jobs for data scientists.

00:17:40.693 --> 00:17:41.510
Well, not really.

00:17:41.510 --> 00:17:44.628
It doesn't take data scientists 
very long to build these things.

00:17:44.628 --> 00:17:47.880
For example, these four algorithms
were all built by the same guy.

00:17:47.880 --> 00:17:50.318
So if you think, oh, 
it's all happened before,

00:17:50.318 --> 00:17:54.126
we've seen the results in the past
of when new things come along

00:17:54.126 --> 00:17:56.378
and they get replaced by new jobs,

00:17:56.378 --> 00:17:58.494
what are these new jobs going to be?

00:17:58.494 --> 00:18:00.365
It's very hard for us to estimate this,

00:18:00.365 --> 00:18:03.104
because human performance
grows at this gradual rate,

00:18:03.104 --> 00:18:05.666
but we now have a system, deep learning,

00:18:05.666 --> 00:18:08.893
that we know actually grows
in capability exponentially.

00:18:08.893 --> 00:18:10.498
And we're here.

00:18:10.498 --> 00:18:12.559
So currently, we see the things around us

00:18:12.559 --> 00:18:15.235
and we say, "Oh, computers
are still pretty dumb." Right?

00:18:15.235 --> 00:18:18.664
But in five years' time,
computers will be off this chart.

00:18:18.664 --> 00:18:22.529
So we need to be starting to think
about this capability right now.

00:18:22.529 --> 00:18:24.579
We have seen this once before, of course.

00:18:24.579 --> 00:18:25.966
In the Industrial Revolution,

00:18:25.966 --> 00:18:28.817
we saw a step change
in capability thanks to engines.

00:18:29.667 --> 00:18:32.805
The thing is, though,
that after a while, things flattened out.

00:18:32.805 --> 00:18:34.507
There was social disruption,

00:18:34.507 --> 00:18:37.946
but once engines were used 
to generate power in all the situations,

00:18:37.946 --> 00:18:40.300
things really settled down.

00:18:40.300 --> 00:18:41.773
The Machine Learning Revolution

00:18:41.773 --> 00:18:44.682
is going to be very different
from the Industrial Revolution,

00:18:44.682 --> 00:18:47.632
because the Machine Learning Revolution,
it never settles down.

00:18:47.632 --> 00:18:50.614
The better computers get
at intellectual activities,

00:18:50.614 --> 00:18:54.862
the more they can build better computers
to be better at intellectual capabilities,

00:18:54.862 --> 00:18:56.770
so this is going to be a kind of change

00:18:56.770 --> 00:18:59.248
that the world has actually
never experienced before,

00:18:59.248 --> 00:19:02.554
so your previous understanding
of what's possible is different.

00:19:02.974 --> 00:19:04.754
This is already impacting us.

00:19:04.754 --> 00:19:08.384
In the last 25 years,
as capital productivity has increased,

00:19:08.400 --> 00:19:12.588
labor productivity has been flat,
in fact even a little bit down.

00:19:13.408 --> 00:19:16.149
So I want us to start
having this discussion now.

00:19:16.149 --> 00:19:19.176
I know that when I often tell people
about this situation,

00:19:19.176 --> 00:19:20.666
people can be quite dismissive.

00:19:20.666 --> 00:19:22.339
Well, computers can't really think,

00:19:22.339 --> 00:19:25.367
they don't emote,
they don't understand poetry,

00:19:25.367 --> 00:19:27.888
we don't really understand how they work.

00:19:27.888 --> 00:19:29.374
So what?

00:19:29.374 --> 00:19:31.178
Computers right now can do the things

00:19:31.178 --> 00:19:33.897
that humans spend most
of their time being paid to do,

00:19:33.897 --> 00:19:35.628
so now's the time to start thinking

00:19:35.628 --> 00:19:40.015
about how we're going to adjust our
social structures and economic structures

00:19:40.015 --> 00:19:41.855
to be aware of this new reality.

00:19:41.855 --> 00:19:43.388
Thank you.

00:19:43.388 --> 00:19:44.190
(Applause)

