WEBVTT
Kind: captions
Language: en

00:00:15.260 --> 00:00:17.260
This is a photograph

00:00:17.260 --> 00:00:19.260
by the artist Michael Najjar,

00:00:19.260 --> 00:00:21.260
and it's real,

00:00:21.260 --> 00:00:23.260
in the sense that he went there to Argentina

00:00:23.260 --> 00:00:25.260
to take the photo.

00:00:25.260 --> 00:00:28.260
But it's also a fiction. There's a lot of work that went into it after that.

00:00:28.260 --> 00:00:30.260
And what he's done

00:00:30.260 --> 00:00:32.260
is he's actually reshaped, digitally,

00:00:32.260 --> 00:00:34.260
all of the contours of the mountains

00:00:34.260 --> 00:00:37.260
to follow the vicissitudes of the Dow Jones index.

00:00:37.260 --> 00:00:39.260
So what you see,

00:00:39.260 --> 00:00:41.260
that precipice, that high precipice with the valley,

00:00:41.260 --> 00:00:43.260
is the 2008 financial crisis.

00:00:43.260 --> 00:00:45.260
The photo was made

00:00:45.260 --> 00:00:47.260
when we were deep in the valley over there.

00:00:47.260 --> 00:00:49.260
I don't know where we are now.

00:00:49.260 --> 00:00:51.260
This is the Hang Seng index

00:00:51.260 --> 00:00:53.260
for Hong Kong.

00:00:53.260 --> 00:00:55.260
And similar topography.

00:00:55.260 --> 00:00:57.260
I wonder why.

00:00:57.260 --> 00:01:00.260
And this is art. This is metaphor.

00:01:00.260 --> 00:01:02.260
But I think the point is

00:01:02.260 --> 00:01:04.260
that this is metaphor with teeth,

00:01:04.260 --> 00:01:07.260
and it's with those teeth that I want to propose today

00:01:07.260 --> 00:01:09.260
that we rethink a little bit

00:01:09.260 --> 00:01:12.260
about the role of contemporary math --

00:01:12.260 --> 00:01:15.260
not just financial math, but math in general.

00:01:15.260 --> 00:01:17.260
That its transition

00:01:17.260 --> 00:01:20.260
from being something that we extract and derive from the world

00:01:20.260 --> 00:01:23.260
to something that actually starts to shape it --

00:01:23.260 --> 00:01:26.260
the world around us and the world inside us.

00:01:26.260 --> 00:01:28.260
And it's specifically algorithms,

00:01:28.260 --> 00:01:30.260
which are basically the math

00:01:30.260 --> 00:01:33.260
that computers use to decide stuff.

00:01:33.260 --> 00:01:35.260
They acquire the sensibility of truth

00:01:35.260 --> 00:01:37.260
because they repeat over and over again,

00:01:37.260 --> 00:01:40.260
and they ossify and calcify,

00:01:40.260 --> 00:01:42.260
and they become real.

00:01:42.260 --> 00:01:45.260
And I was thinking about this, of all places,

00:01:45.260 --> 00:01:48.260
on a transatlantic flight a couple of years ago,

00:01:48.260 --> 00:01:50.260
because I happened to be seated

00:01:50.260 --> 00:01:52.260
next to a Hungarian physicist about my age

00:01:52.260 --> 00:01:54.260
and we were talking

00:01:54.260 --> 00:01:56.260
about what life was like during the Cold War

00:01:56.260 --> 00:01:58.260
for physicists in Hungary.

00:01:58.260 --> 00:02:00.260
And I said, "So what were you doing?"

00:02:00.260 --> 00:02:02.260
And he said, "Well we were mostly breaking stealth."

00:02:02.260 --> 00:02:04.260
And I said, "That's a good job. That's interesting.

00:02:04.260 --> 00:02:06.260
How does that work?"

00:02:06.260 --> 00:02:08.260
And to understand that,

00:02:08.260 --> 00:02:11.260
you have to understand a little bit about how stealth works.

00:02:11.260 --> 00:02:14.260
And so -- this is an over-simplification --

00:02:14.260 --> 00:02:16.260
but basically, it's not like

00:02:16.260 --> 00:02:18.260
you can just pass a radar signal

00:02:18.260 --> 00:02:21.260
right through 156 tons of steel in the sky.

00:02:21.260 --> 00:02:24.260
It's not just going to disappear.

00:02:24.260 --> 00:02:27.260
But if you can take this big, massive thing,

00:02:27.260 --> 00:02:30.260
and you could turn it into

00:02:30.260 --> 00:02:32.260
a million little things --

00:02:32.260 --> 00:02:34.260
something like a flock of birds --

00:02:34.260 --> 00:02:36.260
well then the radar that's looking for that

00:02:36.260 --> 00:02:38.260
has to be able to see

00:02:38.260 --> 00:02:40.260
every flock of birds in the sky.

00:02:40.260 --> 00:02:44.260
And if you're a radar, that's a really bad job.

00:02:44.260 --> 00:02:47.260
And he said, "Yeah." He said, "But that's if you're a radar.

00:02:47.260 --> 00:02:49.260
So we didn't use a radar;

00:02:49.260 --> 00:02:52.260
we built a black box that was looking for electrical signals,

00:02:52.260 --> 00:02:55.260
electronic communication.

00:02:55.260 --> 00:02:58.260
And whenever we saw a flock of birds that had electronic communication,

00:02:58.260 --> 00:03:01.260
we thought, 'Probably has something to do with the Americans.'"

00:03:01.260 --> 00:03:03.260
And I said, "Yeah.

00:03:03.260 --> 00:03:05.260
That's good.

00:03:05.260 --> 00:03:07.260
So you've effectively negated

00:03:07.260 --> 00:03:09.260
60 years of aeronautic research.

00:03:09.260 --> 00:03:11.260
What's your act two?

00:03:11.260 --> 00:03:13.260
What do you do when you grow up?"

00:03:13.260 --> 00:03:15.260
And he said,

00:03:15.260 --> 00:03:17.260
"Well, financial services."

00:03:17.260 --> 00:03:19.260
And I said, "Oh."

00:03:19.260 --> 00:03:22.260
Because those had been in the news lately.

00:03:22.260 --> 00:03:24.260
And I said, "How does that work?"

00:03:24.260 --> 00:03:26.260
And he said, "Well there's 2,000 physicists on Wall Street now,

00:03:26.260 --> 00:03:28.260
and I'm one of them."

00:03:28.260 --> 00:03:31.260
And I said, "What's the black box for Wall Street?"

00:03:31.260 --> 00:03:33.260
And he said, "It's funny you ask that,

00:03:33.260 --> 00:03:36.260
because it's actually called black box trading.

00:03:36.260 --> 00:03:38.260
And it's also sometimes called algo trading,

00:03:38.260 --> 00:03:41.260
algorithmic trading."

00:03:41.260 --> 00:03:44.260
And algorithmic trading evolved in part

00:03:44.260 --> 00:03:47.260
because institutional traders have the same problems

00:03:47.260 --> 00:03:50.260
that the United States Air Force had,

00:03:50.260 --> 00:03:53.260
which is that they're moving these positions --

00:03:53.260 --> 00:03:55.260
whether it's Proctor &amp; Gamble or Accenture, whatever --

00:03:55.260 --> 00:03:57.260
they're moving a million shares of something

00:03:57.260 --> 00:03:59.260
through the market.

00:03:59.260 --> 00:04:01.260
And if they do that all at once,

00:04:01.260 --> 00:04:03.260
it's like playing poker and going all in right away.

00:04:03.260 --> 00:04:05.260
You just tip your hand.

00:04:05.260 --> 00:04:07.260
And so they have to find a way --

00:04:07.260 --> 00:04:09.260
and they use algorithms to do this --

00:04:09.260 --> 00:04:11.260
to break up that big thing

00:04:11.260 --> 00:04:13.260
into a million little transactions.

00:04:13.260 --> 00:04:15.260
And the magic and the horror of that

00:04:15.260 --> 00:04:17.260
is that the same math

00:04:17.260 --> 00:04:19.260
that you use to break up the big thing

00:04:19.260 --> 00:04:21.260
into a million little things

00:04:21.260 --> 00:04:23.260
can be used to find a million little things

00:04:23.260 --> 00:04:25.260
and sew them back together

00:04:25.260 --> 00:04:27.260
and figure out what's actually happening in the market.

00:04:27.260 --> 00:04:29.260
So if you need to have some image

00:04:29.260 --> 00:04:32.260
of what's happening in the stock market right now,

00:04:32.260 --> 00:04:34.260
what you can picture is a bunch of algorithms

00:04:34.260 --> 00:04:37.260
that are basically programmed to hide,

00:04:37.260 --> 00:04:40.260
and a bunch of algorithms that are programmed to go find them and act.

00:04:40.260 --> 00:04:43.260
And all of that's great, and it's fine.

00:04:43.260 --> 00:04:45.260
And that's 70 percent

00:04:45.260 --> 00:04:47.260
of the United States stock market,

00:04:47.260 --> 00:04:49.260
70 percent of the operating system

00:04:49.260 --> 00:04:52.260
formerly known as your pension,

00:04:52.260 --> 00:04:55.260
your mortgage.

00:04:55.260 --> 00:04:57.260
And what could go wrong?

00:04:57.260 --> 00:04:59.260
What could go wrong

00:04:59.260 --> 00:05:01.260
is that a year ago,

00:05:01.260 --> 00:05:04.260
nine percent of the entire market just disappears in five minutes,

00:05:04.260 --> 00:05:07.260
and they called it the Flash Crash of 2:45.

00:05:07.260 --> 00:05:10.260
All of a sudden, nine percent just goes away,

00:05:10.260 --> 00:05:12.260
and nobody to this day

00:05:12.260 --> 00:05:14.260
can even agree on what happened

00:05:14.260 --> 00:05:17.260
because nobody ordered it, nobody asked for it.

00:05:17.260 --> 00:05:20.260
Nobody had any control over what was actually happening.

00:05:20.260 --> 00:05:22.260
All they had

00:05:22.260 --> 00:05:24.260
was just a monitor in front of them

00:05:24.260 --> 00:05:26.260
that had the numbers on it

00:05:26.260 --> 00:05:28.260
and just a red button

00:05:28.260 --> 00:05:30.260
that said, "Stop."

00:05:30.260 --> 00:05:32.260
And that's the thing,

00:05:32.260 --> 00:05:34.260
is that we're writing things,

00:05:34.260 --> 00:05:37.260
we're writing these things that we can no longer read.

00:05:37.260 --> 00:05:39.260
And we've rendered something

00:05:39.260 --> 00:05:41.260
illegible,

00:05:41.260 --> 00:05:44.260
and we've lost the sense

00:05:44.260 --> 00:05:46.260
of what's actually happening

00:05:46.260 --> 00:05:48.260
in this world that we've made.

00:05:48.260 --> 00:05:50.260
And we're starting to make our way.

00:05:50.260 --> 00:05:53.260
There's a company in Boston called Nanex,

00:05:53.260 --> 00:05:55.260
and they use math and magic

00:05:55.260 --> 00:05:57.260
and I don't know what,

00:05:57.260 --> 00:05:59.260
and they reach into all the market data

00:05:59.260 --> 00:06:02.260
and they find, actually sometimes, some of these algorithms.

00:06:02.260 --> 00:06:05.260
And when they find them they pull them out

00:06:05.260 --> 00:06:08.260
and they pin them to the wall like butterflies.

00:06:08.260 --> 00:06:10.260
And they do what we've always done

00:06:10.260 --> 00:06:13.260
when confronted with huge amounts of data that we don't understand --

00:06:13.260 --> 00:06:15.260
which is that they give them a name

00:06:15.260 --> 00:06:17.260
and a story.

00:06:17.260 --> 00:06:19.260
So this is one that they found,

00:06:19.260 --> 00:06:23.260
they called the Knife,

00:06:23.260 --> 00:06:25.260
the Carnival,

00:06:25.260 --> 00:06:29.260
the Boston Shuffler,

00:06:29.260 --> 00:06:31.260
Twilight.

00:06:31.260 --> 00:06:33.260
And the gag is

00:06:33.260 --> 00:06:36.260
that, of course, these aren't just running through the market.

00:06:36.260 --> 00:06:39.260
You can find these kinds of things wherever you look,

00:06:39.260 --> 00:06:41.260
once you learn how to look for them.

00:06:41.260 --> 00:06:44.260
You can find it here: this book about flies

00:06:44.260 --> 00:06:46.260
that you may have been looking at on Amazon.

00:06:46.260 --> 00:06:48.260
You may have noticed it

00:06:48.260 --> 00:06:50.260
when its price started at 1.7 million dollars.

00:06:50.260 --> 00:06:52.260
It's out of print -- still ...

00:06:52.260 --> 00:06:54.260
(Laughter)

00:06:54.260 --> 00:06:57.260
If you had bought it at 1.7, it would have been a bargain.

00:06:57.260 --> 00:06:59.260
A few hours later, it had gone up

00:06:59.260 --> 00:07:01.260
to 23.6 million dollars,

00:07:01.260 --> 00:07:03.260
plus shipping and handling.

00:07:03.260 --> 00:07:05.260
And the question is:

00:07:05.260 --> 00:07:07.260
Nobody was buying or selling anything; what was happening?

00:07:07.260 --> 00:07:09.260
And you see this behavior on Amazon

00:07:09.260 --> 00:07:11.260
as surely as you see it on Wall Street.

00:07:11.260 --> 00:07:13.260
And when you see this kind of behavior,

00:07:13.260 --> 00:07:15.260
what you see is the evidence

00:07:15.260 --> 00:07:17.260
of algorithms in conflict,

00:07:17.260 --> 00:07:19.260
algorithms locked in loops with each other,

00:07:19.260 --> 00:07:21.260
without any human oversight,

00:07:21.260 --> 00:07:24.260
without any adult supervision

00:07:24.260 --> 00:07:27.260
to say, "Actually, 1.7 million is plenty."

00:07:27.260 --> 00:07:30.260
(Laughter)

00:07:30.260 --> 00:07:33.260
And as with Amazon, so it is with Netflix.

00:07:33.260 --> 00:07:35.260
And so Netflix has gone through

00:07:35.260 --> 00:07:37.260
several different algorithms over the years.

00:07:37.260 --> 00:07:40.260
They started with Cinematch, and they've tried a bunch of others --

00:07:40.260 --> 00:07:42.260
there's Dinosaur Planet; there's Gravity.

00:07:42.260 --> 00:07:44.260
They're using Pragmatic Chaos now.

00:07:44.260 --> 00:07:46.260
Pragmatic Chaos is, like all of Netflix algorithms,

00:07:46.260 --> 00:07:48.260
trying to do the same thing.

00:07:48.260 --> 00:07:50.260
It's trying to get a grasp on you,

00:07:50.260 --> 00:07:52.260
on the firmware inside the human skull,

00:07:52.260 --> 00:07:54.260
so that it can recommend what movie

00:07:54.260 --> 00:07:56.260
you might want to watch next --

00:07:56.260 --> 00:07:59.260
which is a very, very difficult problem.

00:07:59.260 --> 00:08:01.260
But the difficulty of the problem

00:08:01.260 --> 00:08:04.260
and the fact that we don't really quite have it down,

00:08:04.260 --> 00:08:06.260
it doesn't take away

00:08:06.260 --> 00:08:08.260
from the effects Pragmatic Chaos has.

00:08:08.260 --> 00:08:11.260
Pragmatic Chaos, like all Netflix algorithms,

00:08:11.260 --> 00:08:13.260
determines, in the end,

00:08:13.260 --> 00:08:15.260
60 percent

00:08:15.260 --> 00:08:17.260
of what movies end up being rented.

00:08:17.260 --> 00:08:19.260
So one piece of code

00:08:19.260 --> 00:08:22.260
with one idea about you

00:08:22.260 --> 00:08:25.260
is responsible for 60 percent of those movies.

00:08:25.260 --> 00:08:27.260
But what if you could rate those movies

00:08:27.260 --> 00:08:29.260
before they get made?

00:08:29.260 --> 00:08:31.260
Wouldn't that be handy?

00:08:31.260 --> 00:08:34.260
Well, a few data scientists from the U.K. are in Hollywood,

00:08:34.260 --> 00:08:36.260
and they have "story algorithms" --

00:08:36.260 --> 00:08:38.260
a company called Epagogix.

00:08:38.260 --> 00:08:41.260
And you can run your script through there,

00:08:41.260 --> 00:08:43.260
and they can tell you, quantifiably,

00:08:43.260 --> 00:08:45.260
that that's a 30 million dollar movie

00:08:45.260 --> 00:08:47.260
or a 200 million dollar movie.

00:08:47.260 --> 00:08:49.260
And the thing is, is that this isn't Google.

00:08:49.260 --> 00:08:51.260
This isn't information.

00:08:51.260 --> 00:08:53.260
These aren't financial stats; this is culture.

00:08:53.260 --> 00:08:55.260
And what you see here,

00:08:55.260 --> 00:08:57.260
or what you don't really see normally,

00:08:57.260 --> 00:09:01.260
is that these are the physics of culture.

00:09:01.260 --> 00:09:03.260
And if these algorithms,

00:09:03.260 --> 00:09:05.260
like the algorithms on Wall Street,

00:09:05.260 --> 00:09:08.260
just crashed one day and went awry,

00:09:08.260 --> 00:09:10.260
how would we know?

00:09:10.260 --> 00:09:12.260
What would it look like?

00:09:12.260 --> 00:09:15.260
And they're in your house. They're in your house.

00:09:15.260 --> 00:09:17.260
These are two algorithms competing for your living room.

00:09:17.260 --> 00:09:19.260
These are two different cleaning robots

00:09:19.260 --> 00:09:22.260
that have very different ideas about what clean means.

00:09:22.260 --> 00:09:24.260
And you can see it

00:09:24.260 --> 00:09:27.260
if you slow it down and attach lights to them,

00:09:27.260 --> 00:09:30.260
and they're sort of like secret architects in your bedroom.

00:09:30.260 --> 00:09:33.260
And the idea that architecture itself

00:09:33.260 --> 00:09:35.260
is somehow subject to algorithmic optimization

00:09:35.260 --> 00:09:37.260
is not far-fetched.

00:09:37.260 --> 00:09:40.260
It's super-real and it's happening around you.

00:09:40.260 --> 00:09:42.260
You feel it most

00:09:42.260 --> 00:09:44.260
when you're in a sealed metal box,

00:09:44.260 --> 00:09:46.260
a new-style elevator;

00:09:46.260 --> 00:09:48.260
they're called destination-control elevators.

00:09:48.260 --> 00:09:51.260
These are the ones where you have to press what floor you're going to go to

00:09:51.260 --> 00:09:53.260
before you get in the elevator.

00:09:53.260 --> 00:09:55.260
And it uses what's called a bin-packing algorithm.

00:09:55.260 --> 00:09:57.260
So none of this mishegas

00:09:57.260 --> 00:09:59.260
of letting everybody go into whatever car they want.

00:09:59.260 --> 00:10:01.260
Everybody who wants to go to the 10th floor goes into car two,

00:10:01.260 --> 00:10:04.260
and everybody who wants to go to the third floor goes into car five.

00:10:04.260 --> 00:10:06.260
And the problem with that

00:10:06.260 --> 00:10:08.260
is that people freak out.

00:10:08.260 --> 00:10:10.260
People panic.

00:10:10.260 --> 00:10:12.260
And you see why. You see why.

00:10:12.260 --> 00:10:14.260
It's because the elevator

00:10:14.260 --> 00:10:17.260
is missing some important instrumentation, like the buttons.

00:10:17.260 --> 00:10:19.260
(Laughter)

00:10:19.260 --> 00:10:21.260
Like the things that people use.

00:10:21.260 --> 00:10:23.260
All it has

00:10:23.260 --> 00:10:26.260
is just the number that moves up or down

00:10:26.260 --> 00:10:29.260
and that red button that says, "Stop."

00:10:29.260 --> 00:10:32.260
And this is what we're designing for.

00:10:32.260 --> 00:10:34.260
We're designing

00:10:34.260 --> 00:10:36.260
for this machine dialect.

00:10:36.260 --> 00:10:39.260
And how far can you take that? How far can you take it?

00:10:39.260 --> 00:10:41.260
You can take it really, really far.

00:10:41.260 --> 00:10:44.260
So let me take it back to Wall Street.

00:10:45.260 --> 00:10:47.260
Because the algorithms of Wall Street

00:10:47.260 --> 00:10:50.260
are dependent on one quality above all else,

00:10:50.260 --> 00:10:52.260
which is speed.

00:10:52.260 --> 00:10:55.260
And they operate on milliseconds and microseconds.

00:10:55.260 --> 00:10:57.260
And just to give you a sense of what microseconds are,

00:10:57.260 --> 00:10:59.260
it takes you 500,000 microseconds

00:10:59.260 --> 00:11:01.260
just to click a mouse.

00:11:01.260 --> 00:11:03.260
But if you're a Wall Street algorithm

00:11:03.260 --> 00:11:05.260
and you're five microseconds behind,

00:11:05.260 --> 00:11:07.260
you're a loser.

00:11:07.260 --> 00:11:09.260
So if you were an algorithm,

00:11:09.260 --> 00:11:12.260
you'd look for an architect like the one that I met in Frankfurt

00:11:12.260 --> 00:11:14.260
who was hollowing out a skyscraper --

00:11:14.260 --> 00:11:17.260
throwing out all the furniture, all the infrastructure for human use,

00:11:17.260 --> 00:11:20.260
and just running steel on the floors

00:11:20.260 --> 00:11:23.260
to get ready for the stacks of servers to go in --

00:11:23.260 --> 00:11:25.260
all so an algorithm

00:11:25.260 --> 00:11:28.260
could get close to the Internet.

00:11:28.260 --> 00:11:31.260
And you think of the Internet as this kind of distributed system.

00:11:31.260 --> 00:11:34.260
And of course, it is, but it's distributed from places.

00:11:34.260 --> 00:11:36.260
In New York, this is where it's distributed from:

00:11:36.260 --> 00:11:38.260
the Carrier Hotel

00:11:38.260 --> 00:11:40.260
located on Hudson Street.

00:11:40.260 --> 00:11:43.260
And this is really where the wires come right up into the city.

00:11:43.260 --> 00:11:47.260
And the reality is that the further away you are from that,

00:11:47.260 --> 00:11:49.260
you're a few microseconds behind every time.

00:11:49.260 --> 00:11:51.260
These guys down on Wall Street,

00:11:51.260 --> 00:11:53.260
Marco Polo and Cherokee Nation,

00:11:53.260 --> 00:11:55.260
they're eight microseconds

00:11:55.260 --> 00:11:57.260
behind all these guys

00:11:57.260 --> 00:12:01.260
going into the empty buildings being hollowed out

00:12:01.260 --> 00:12:03.260
up around the Carrier Hotel.

00:12:03.260 --> 00:12:06.260
And that's going to keep happening.

00:12:06.260 --> 00:12:08.260
We're going to keep hollowing them out,

00:12:08.260 --> 00:12:11.260
because you, inch for inch

00:12:11.260 --> 00:12:14.260
and pound for pound and dollar for dollar,

00:12:14.260 --> 00:12:17.260
none of you could squeeze revenue out of that space

00:12:17.260 --> 00:12:20.260
like the Boston Shuffler could.

00:12:20.260 --> 00:12:22.260
But if you zoom out,

00:12:22.260 --> 00:12:24.260
if you zoom out,

00:12:24.260 --> 00:12:28.260
you would see an 825-mile trench

00:12:28.260 --> 00:12:30.260
between New York City and Chicago

00:12:30.260 --> 00:12:32.260
that's been built over the last few years

00:12:32.260 --> 00:12:35.260
by a company called Spread Networks.

00:12:35.260 --> 00:12:37.260
This is a fiber optic cable

00:12:37.260 --> 00:12:39.260
that was laid between those two cities

00:12:39.260 --> 00:12:42.260
to just be able to traffic one signal

00:12:42.260 --> 00:12:45.260
37 times faster than you can click a mouse --

00:12:45.260 --> 00:12:48.260
just for these algorithms,

00:12:48.260 --> 00:12:51.260
just for the Carnival and the Knife.

00:12:51.260 --> 00:12:53.260
And when you think about this,

00:12:53.260 --> 00:12:55.260
that we're running through the United States

00:12:55.260 --> 00:12:58.260
with dynamite and rock saws

00:12:58.260 --> 00:13:00.260
so that an algorithm can close the deal

00:13:00.260 --> 00:13:03.260
three microseconds faster,

00:13:03.260 --> 00:13:05.260
all for a communications framework

00:13:05.260 --> 00:13:09.260
that no human will ever know,

00:13:09.260 --> 00:13:12.260
that's a kind of manifest destiny;

00:13:12.260 --> 00:13:15.260
and we'll always look for a new frontier.

00:13:15.260 --> 00:13:18.260
Unfortunately, we have our work cut out for us.

00:13:18.260 --> 00:13:20.260
This is just theoretical.

00:13:20.260 --> 00:13:22.260
This is some mathematicians at MIT.

00:13:22.260 --> 00:13:24.260
And the truth is I don't really understand

00:13:24.260 --> 00:13:26.260
a lot of what they're talking about.

00:13:26.260 --> 00:13:29.260
It involves light cones and quantum entanglement,

00:13:29.260 --> 00:13:31.260
and I don't really understand any of that.

00:13:31.260 --> 00:13:33.260
But I can read this map,

00:13:33.260 --> 00:13:35.260
and what this map says

00:13:35.260 --> 00:13:38.260
is that, if you're trying to make money on the markets where the red dots are,

00:13:38.260 --> 00:13:40.260
that's where people are, where the cities are,

00:13:40.260 --> 00:13:43.260
you're going to have to put the servers where the blue dots are

00:13:43.260 --> 00:13:45.260
to do that most effectively.

00:13:45.260 --> 00:13:48.260
And the thing that you might have noticed about those blue dots

00:13:48.260 --> 00:13:51.260
is that a lot of them are in the middle of the ocean.

00:13:51.260 --> 00:13:54.260
So that's what we'll do: we'll build bubbles or something,

00:13:54.260 --> 00:13:56.260
or platforms.

00:13:56.260 --> 00:13:58.260
We'll actually part the water

00:13:58.260 --> 00:14:00.260
to pull money out of the air,

00:14:00.260 --> 00:14:02.260
because it's a bright future

00:14:02.260 --> 00:14:04.260
if you're an algorithm.

00:14:04.260 --> 00:14:06.260
(Laughter)

00:14:06.260 --> 00:14:09.260
And it's not the money that's so interesting actually.

00:14:09.260 --> 00:14:11.260
It's what the money motivates,

00:14:11.260 --> 00:14:13.260
that we're actually terraforming

00:14:13.260 --> 00:14:15.260
the Earth itself

00:14:15.260 --> 00:14:17.260
with this kind of algorithmic efficiency.

00:14:17.260 --> 00:14:19.260
And in that light,

00:14:19.260 --> 00:14:21.260
you go back

00:14:21.260 --> 00:14:23.260
and you look at Michael Najjar's photographs,

00:14:23.260 --> 00:14:26.260
and you realize that they're not metaphor, they're prophecy.

00:14:26.260 --> 00:14:28.260
They're prophecy

00:14:28.260 --> 00:14:32.260
for the kind of seismic, terrestrial effects

00:14:32.260 --> 00:14:34.260
of the math that we're making.

00:14:34.260 --> 00:14:37.260
And the landscape was always made

00:14:37.260 --> 00:14:40.260
by this sort of weird, uneasy collaboration

00:14:40.260 --> 00:14:43.260
between nature and man.

00:14:43.260 --> 00:14:46.260
But now there's this third co-evolutionary force: algorithms --

00:14:46.260 --> 00:14:49.260
the Boston Shuffler, the Carnival.

00:14:49.260 --> 00:14:52.260
And we will have to understand those as nature,

00:14:52.260 --> 00:14:54.260
and in a way, they are.

00:14:54.260 --> 00:14:56.260
Thank you.

00:14:56.260 --> 00:15:16.260
(Applause)

