WEBVTT
Kind: captions
Language: pt

00:00:00.000 --> 00:00:07.000
Tradutor: Claudia Sander
Revisor: Leonardo Silva

00:00:12.760 --> 00:00:16.296
Quando as pessoas dizem
que têm medo da inteligência artificial,

00:00:16.320 --> 00:00:20.296
com frequência elas evocam imagens
de robôs humanoides cometendo massacres.

00:00:20.320 --> 00:00:22.000
Sabem? "O Exterminador do Futuro"?

00:00:22.400 --> 00:00:24.736
Bem, isso pode ser algo a ser considerado,

00:00:24.760 --> 00:00:26.616
mas é uma ameaça distante.

00:00:26.640 --> 00:00:30.096
Ou nos preocupamos com vigilância digital,

00:00:30.120 --> 00:00:31.896
com metáforas do passado.

00:00:31.920 --> 00:00:34.576
O livro "1984", de George Orwell,

00:00:34.600 --> 00:00:37.080
está de volta ao topo
das listas dos mais vendidos.

00:00:37.960 --> 00:00:39.376
É um livro ótimo,

00:00:39.400 --> 00:00:43.280
mas não é a distopia correta
para o século 21.

00:00:44.080 --> 00:00:45.496
O que mais devemos temer

00:00:45.520 --> 00:00:50.296
não é o que a inteligência artificial
em si vai fazer conosco,

00:00:50.320 --> 00:00:55.056
mas como as pessoas no poder
vão usar a inteligência artificial

00:00:55.080 --> 00:00:57.896
para nos controlar e nos manipular

00:00:57.920 --> 00:01:01.056
de formas novas, às vezes ocultas,

00:01:01.080 --> 00:01:04.096
sutis e inesperadas.

00:01:04.120 --> 00:01:05.976
Muita da tecnologia

00:01:06.000 --> 00:01:10.336
que ameaça nossa liberdade
e dignidade num futuro a curto prazo

00:01:10.360 --> 00:01:12.216
está sendo desenvolvida por empresas

00:01:12.240 --> 00:01:17.176
que estão no negócio de captura
e venda de nossos dados e nossa atenção

00:01:17.200 --> 00:01:19.456
para anunciantes e outros;

00:01:19.480 --> 00:01:22.896
Facebook, Google, Amazon,

00:01:22.920 --> 00:01:24.800
Alibaba, Tencent.

00:01:26.040 --> 00:01:31.536
A inteligência artificial começou
a dar suporte a esses negócios, também.

00:01:31.560 --> 00:01:33.656
E pode parecer
que a inteligência artificial

00:01:33.680 --> 00:01:36.536
é apenas o próximo passo
em anúncios on-line.

00:01:36.560 --> 00:01:37.776
Não é.

00:01:37.800 --> 00:01:40.256
É um salto de categoria.

00:01:40.280 --> 00:01:42.856
É um mundo totalmente novo,

00:01:42.880 --> 00:01:45.496
e tem muito potencial.

00:01:45.520 --> 00:01:52.440
Ela pode acelerar nosso entendimento
em muitas áreas de estudo e pesquisa.

00:01:53.120 --> 00:01:56.616
Mas, parafraseando
um famoso filósofo de Hollywood:

00:01:56.640 --> 00:02:00.280
"Junto com um potencial imenso,
vem um risco imenso".

00:02:01.120 --> 00:02:05.056
Vamos olhar um fato básico
de nossa vida digital: anúncios on-line.

00:02:05.080 --> 00:02:07.976
Certo? Nós meio que os dispensamos.

00:02:08.000 --> 00:02:09.976
Eles parecem toscos, não efetivos.

00:02:10.000 --> 00:02:14.256
Todos nós já tivemos a experiência
de sermos seguidos na web

00:02:14.280 --> 00:02:17.056
por um anúncio baseado
em algo que pesquisamos ou lemos.

00:02:17.080 --> 00:02:18.936
Sabe? Você pesquisa um par de botas

00:02:18.960 --> 00:02:22.336
e, por uma semana, essas botas
o perseguem onde quer que você vá.

00:02:22.360 --> 00:02:26.016
Mesmo depois de sucumbir e comprá-las,
elas continuam seguindo você.

00:02:26.040 --> 00:02:29.056
Nós nos acostumamos a esse tipo
de manipulação básica e barata.

00:02:29.080 --> 00:02:32.480
Reviramos os olhos e pensamos:
"Quer saber? Essas coisas não funcionam".

00:02:33.720 --> 00:02:39.476
Mas, on-line, as tecnologias digitais
não são só anúncios.

00:02:40.240 --> 00:02:43.360
Para entender isso, vamos pensar
em um exemplo do mundo real.

00:02:43.840 --> 00:02:48.496
Sabem quando, perto
dos caixas do supermercado,

00:02:48.520 --> 00:02:52.000
estão expostas balas e chicletes
na altura dos olhos das crianças?

00:02:52.800 --> 00:02:56.296
Isso é projetado para fazê-las
choramingar para os pais

00:02:56.320 --> 00:02:59.400
bem na hora em que eles
estão encerrando a compra.

00:03:00.040 --> 00:03:02.680
Isso é uma arquitetura de persuasão.

00:03:03.160 --> 00:03:06.256
Não é legal, mas funciona.

00:03:06.280 --> 00:03:08.320
E você vê isso em qualquer supermercado.

00:03:08.720 --> 00:03:10.416
Agora, no mundo físico,

00:03:10.416 --> 00:03:12.982
essas arquiteturas de persuasão
são um pouco limitadas,

00:03:12.982 --> 00:03:17.776
porque só se consegue colocar um certo
número de coisas perto do caixa, certo?

00:03:17.800 --> 00:03:22.096
E as balas e chicletes
são iguais para todos,

00:03:22.120 --> 00:03:24.976
mesmo que funcionem mais para as pessoas

00:03:24.976 --> 00:03:28.070
que estão com pequenos
humanos choraminguentos.

00:03:29.160 --> 00:03:33.080
No mundo físico,
vivemos com essas limitações.

00:03:34.280 --> 00:03:36.216
No mundo digital, no entanto,

00:03:36.240 --> 00:03:40.560
arquiteturas de persuasão
podem ser construídas aos bilhões

00:03:41.840 --> 00:03:45.696
e podem mirar, inferir, entender

00:03:45.720 --> 00:03:48.616
e ser aplicadas individualmente,

00:03:48.640 --> 00:03:49.856
um a um,

00:03:49.880 --> 00:03:52.016
ao entender nossas fraquezas,

00:03:52.040 --> 00:03:57.656
e podem ser enviadas de forma privada
para a tela do telefone de cada um,

00:03:57.680 --> 00:03:59.936
então não são visíveis para todos.

00:03:59.960 --> 00:04:01.216
E isso é diferente.

00:04:01.240 --> 00:04:04.730
Essa é só uma das coisas básicas
que a inteligência artificial pode fazer.

00:04:04.730 --> 00:04:08.890
Por exemplo, digamos que você queira
vender passagens aéreas para Las Vegas.

00:04:08.920 --> 00:04:12.506
No mundo antigo, você poderia pensar
em mirar em alguns setores da população

00:04:12.506 --> 00:04:14.960
baseado na experiência
e no que você puder imaginar.

00:04:15.560 --> 00:04:18.376
Você pode tentar anunciar para...

00:04:18.400 --> 00:04:20.896
homens de 25 a 35 anos,

00:04:20.920 --> 00:04:24.856
ou pessoas que têm limite alto
no cartão de crédito,

00:04:24.880 --> 00:04:26.286
ou casais aposentados, certo?

00:04:26.286 --> 00:04:28.096
É isso que você faria no passado.

00:04:28.120 --> 00:04:31.016
Com "big data" e aprendizado de máquina,

00:04:31.040 --> 00:04:32.564
não funciona mais assim.

00:04:33.320 --> 00:04:35.496
Para imaginar isso,

00:04:35.520 --> 00:04:39.376
pense em todos os dados
que o Facebook tem sobre você:

00:04:39.400 --> 00:04:41.936
cada atualização de status
que você já digitou,

00:04:41.960 --> 00:04:43.976
cada conversa no Messenger,

00:04:44.000 --> 00:04:45.880
cada local de onde você se conectou,

00:04:48.400 --> 00:04:51.576
todas as fotos que você publicou.

00:04:51.600 --> 00:04:55.376
Se você começa a digitar algo,
muda de ideia e deleta,

00:04:55.400 --> 00:04:58.600
o Facebook guarda isso e analisa, também.

00:04:59.160 --> 00:05:03.096
Além disso, ele tenta combinar você
com seus dados fora da rede.

00:05:03.120 --> 00:05:06.296
E também compra muitos dados
de corretores de dados.

00:05:06.320 --> 00:05:09.736
Pode ser qualquer coisa,
desde seus registros financeiros

00:05:09.760 --> 00:05:12.130
a uma boa parte
do seu histórico de navegação.

00:05:12.140 --> 00:05:13.130
Certo?

00:05:13.150 --> 00:05:17.776
Nos EUA esses dados
são coletados rotineiramente,

00:05:17.800 --> 00:05:19.760
organizados e vendidos.

00:05:20.320 --> 00:05:22.760
Na Europa, há regras mais rígidas.

00:05:23.680 --> 00:05:25.880
O que acontece então

00:05:26.920 --> 00:05:30.936
é que, vasculhando todos esses dados,
os algoritmos de aprendizado de máquina,

00:05:30.960 --> 00:05:33.856
e é por isso que são chamados
de algoritmos de aprendizado,

00:05:33.880 --> 00:05:37.976
aprendem a entender
as características das pessoas

00:05:38.000 --> 00:05:40.520
que já compraram passagens
para Las Vegas antes.

00:05:41.760 --> 00:05:45.296
Quando eles aprendem isso
a partir de dados existentes,

00:05:45.320 --> 00:05:49.136
eles também aprendem
a aplicar isso a outras pessoas.

00:05:49.160 --> 00:05:52.216
Então, se uma nova pessoa
é apresentada a eles,

00:05:52.240 --> 00:05:54.350
eles podem classificar se essa pessoa

00:05:54.350 --> 00:05:57.720
é do tipo que compra passagem
para Las Vegas ou não.

00:05:57.720 --> 00:06:03.176
Bem. Você está pensando:
"Uma oferta de passagem pra Las Vegas.

00:06:03.200 --> 00:06:04.656
Consigo ignorar isso".

00:06:04.680 --> 00:06:06.896
Mas o problema não é esse.

00:06:06.920 --> 00:06:12.646
O problema é que não entendemos mais
como esses algoritmos complexos trabalham.

00:06:12.680 --> 00:06:16.136
Não entendemos como eles estão
fazendo essa categorização.

00:06:16.160 --> 00:06:20.576
São matrizes gigantes,
milhares de linhas e colunas,

00:06:20.600 --> 00:06:22.560
talvez milhões,

00:06:23.320 --> 00:06:25.960
e nem os programadores

00:06:26.760 --> 00:06:28.440
nem ninguém que olhe isso,

00:06:29.440 --> 00:06:30.936
mesmo tendo todos os dados,

00:06:30.960 --> 00:06:35.576
entende exatamente
como eles estão operando;

00:06:35.600 --> 00:06:39.376
não mais do que você saberia
sobre o que estou pensando agora,

00:06:39.376 --> 00:06:43.336
se visse um corte transversal
do meu cérebro.

00:06:44.360 --> 00:06:46.936
É como se não estivéssemos
mais programando,

00:06:46.960 --> 00:06:51.360
estamos criando inteligência
que não entendemos verdadeiramente.

00:06:52.520 --> 00:06:56.496
E essas coisas só funcionam
se existir uma quantidade enorme de dados,

00:06:56.520 --> 00:07:01.616
então eles também encorajam
uma grande vigilância sobre nós,

00:07:01.616 --> 00:07:04.372
para que os algoritmos
de aprendizado de máquina funcionem.

00:07:04.372 --> 00:07:07.176
Por isso o Facebook quer
coletar tudo que pode sobre você.

00:07:07.200 --> 00:07:08.776
Os algoritmos funcionam melhor.

00:07:08.800 --> 00:07:11.496
Vamos explorar um pouco mais
o exemplo sobre Las vegas.

00:07:11.520 --> 00:07:15.200
E se o sistema que nós não entendemos

00:07:16.200 --> 00:07:21.336
estivesse detectando que é mais fácil
vender passagens para Las Vegas

00:07:21.360 --> 00:07:25.120
para pessoas bipolares
próximas de entrar em sua fase maníaca?

00:07:25.640 --> 00:07:27.680
Essas pessoas tendem a se tornar

00:07:27.680 --> 00:07:31.280
gastadores excessivos,
jogadores compulsivos.

00:07:31.280 --> 00:07:35.736
Eles podem fazer isso sem tenhamos a menor
ideia de que eles estão detectando isso.

00:07:35.760 --> 00:07:39.376
Uma vez dei esse exemplo
a alguns cientistas da computação

00:07:39.400 --> 00:07:41.456
e, no fim, um deles veio falar comigo.

00:07:41.480 --> 00:07:45.000
Ele estava perturbado e disse:
"Por isso eu não pude publicar".

00:07:45.600 --> 00:07:47.775
Eu fiquei tipo: "Não pôde publicar o quê?"

00:07:47.800 --> 00:07:53.656
Ele tinha tentado identificar
o início da fase maníaca,

00:07:53.676 --> 00:07:56.962
a partir de publicações de redes sociais,
antes dos sintomas clínicos,

00:07:56.962 --> 00:07:58.696
e tinha funcionado,

00:07:58.720 --> 00:08:00.776
tinha funcionado muito bem,

00:08:00.800 --> 00:08:05.680
e ele não tinha ideia sobre como tinha
funcionado ou o que ele estava detectando.

00:08:06.840 --> 00:08:11.256
Mas não publicar não resolve o problema,

00:08:11.280 --> 00:08:13.176
porque já existem empresas

00:08:13.200 --> 00:08:15.736
que estão desenvolvendo
esse tipo de tecnologia,

00:08:15.760 --> 00:08:18.560
e muita coisa simplesmente já existe.

00:08:19.240 --> 00:08:21.816
Isso não é mais tão difícil.

00:08:21.840 --> 00:08:25.296
Você já entrou no YouTube
para assistir a um vídeo

00:08:25.320 --> 00:08:27.680
e uma hora depois já assistiu a 27?

00:08:28.760 --> 00:08:31.256
Sabe aquela coluna à direita do YouTube

00:08:31.280 --> 00:08:35.346
que diz: "Próximo" e reproduz
automaticamente alguma coisa?

00:08:35.360 --> 00:08:36.576
É um algoritmo

00:08:36.600 --> 00:08:40.216
que escolhe o que ele acha
que pode interessar a você,

00:08:40.240 --> 00:08:41.776
não é você que escolhe.

00:08:41.800 --> 00:08:43.056
Não é um editor humano.

00:08:43.056 --> 00:08:44.562
É isso que os algoritmos fazem.

00:08:44.562 --> 00:08:49.256
Eles detectam o que você viu
e o que as pessoas como você viram,

00:08:49.280 --> 00:08:53.496
e infere que deve ser nisso
que você está interessado,

00:08:53.520 --> 00:08:56.135
que você quer consumir mais,
e apresenta mais para você.

00:08:56.159 --> 00:08:58.360
Parece uma característica benigna e útil,

00:08:59.280 --> 00:09:00.480
mas não é.

00:09:01.640 --> 00:09:08.600
Em 2016, participei de comícios
do então candidato Donald Trump

00:09:09.840 --> 00:09:13.176
para estudar o movimento de apoio a ele.

00:09:13.200 --> 00:09:16.656
Eu estudo movimentos sociais,
então estava estudando esse também.

00:09:16.680 --> 00:09:20.016
Eu queria escrever algo
sobre um de seus comícios,

00:09:20.040 --> 00:09:22.210
então assisti a ele
algumas vezes no YouTube.

00:09:23.240 --> 00:09:26.336
O YouTube começou a me recomendar

00:09:26.360 --> 00:09:30.616
e a reproduzir automaticamente
vídeos de supremacistas brancos

00:09:30.640 --> 00:09:33.296
em ordem crescente de extremismo.

00:09:33.320 --> 00:09:35.136
Se eu assistia a um,

00:09:35.160 --> 00:09:38.136
ele me apresentava outro
ainda mais radical

00:09:38.160 --> 00:09:40.014
e reproduzia-o automaticamente, também.

00:09:40.320 --> 00:09:44.856
Se você assistir a conteúdo
da Hillary Clinton ou do Bernie Sanders,

00:09:44.880 --> 00:09:49.576
o YouTube recomenda e reproduz 
conspiração de esquerda,

00:09:49.600 --> 00:09:51.360
e daí pra baixo.

00:09:52.480 --> 00:09:55.536
Bem, você pode pensar que isso
tem a ver com política, mas não.

00:09:55.544 --> 00:09:56.840
Não tem a ver com política.

00:09:56.840 --> 00:09:59.936
Isso é só o algoritmo descobrindo
sobre o comportamento humano.

00:09:59.960 --> 00:10:04.736
Uma vez assisti a um vídeo
sobre vegetarianismo no YouTube

00:10:04.760 --> 00:10:09.696
e o YouTube recomendou e reproduziu
automaticamente um vídeo sobre ser vegano.

00:10:09.720 --> 00:10:12.736
É como se você nunca fosse
radical o suficiente para o YouTube.

00:10:12.760 --> 00:10:14.336
(Risos)

00:10:14.360 --> 00:10:15.920
Então o que está havendo?

00:10:16.520 --> 00:10:20.056
O algoritmo do YouTube é privado,

00:10:20.080 --> 00:10:22.440
mas acho que acontece o seguinte.

00:10:23.360 --> 00:10:25.456
O algoritmo descobriu

00:10:25.480 --> 00:10:29.176
que, se conseguir persuadir as pessoas

00:10:29.200 --> 00:10:32.936
a pensarem que você pode mostrar
algo ainda mais extremo,

00:10:32.960 --> 00:10:35.376
é mais provável que elas fiquem no site

00:10:35.400 --> 00:10:39.816
assistindo a vídeo após vídeo
e se afundando cada vez mais,

00:10:39.840 --> 00:10:41.910
enquanto o Google mostra anúncios a elas.

00:10:43.760 --> 00:10:46.880
Se ninguém cuidar da questão ética,

00:10:47.720 --> 00:10:51.960
esses sites podem
traçar o perfil de pessoas

00:10:53.680 --> 00:10:55.600
que odeiam judeus,

00:10:56.360 --> 00:10:58.840
que acham que judeus são parasitas,

00:11:00.320 --> 00:11:05.240
e que têm conteúdo
explicitamente antissemita,

00:11:06.080 --> 00:11:08.080
e deixar que você os atinja com anúncios.

00:11:09.200 --> 00:11:12.736
Eles também podem mobilizar algoritmos

00:11:12.760 --> 00:11:15.896
para encontrar para você
plateias similares,

00:11:15.920 --> 00:11:21.496
pessoas que não têm conteúdo antissemita
tão explícito em seu perfil,

00:11:21.520 --> 00:11:27.696
mas que o algoritmo detecta que podem ser
suscetíveis a esse tipo de mensagem,

00:11:27.720 --> 00:11:30.110
e deixar que você os atinja 
com anúncios, também.

00:11:30.680 --> 00:11:33.416
Esse exemplo pode não parecer plausível,

00:11:33.440 --> 00:11:34.760
mas é real.

00:11:35.480 --> 00:11:37.616
A ProPublica investigou isso

00:11:37.640 --> 00:11:41.256
e descobriu que, sem dúvida,
isso pode ser feito no Facebook,

00:11:41.280 --> 00:11:43.696
e o Facebook prestativamente
ofereceu sugestões

00:11:43.720 --> 00:11:45.550
sobre como aumentar esse público.

00:11:46.720 --> 00:11:49.976
O BuzzFeed tentou isso com o Google,
e rapidamente descobriu que sim,

00:11:49.976 --> 00:11:51.756
pode-se fazer isso no Google, também.

00:11:51.756 --> 00:11:53.216
E não foi muito caro.

00:11:53.240 --> 00:11:57.656
O repórter da ProPublica
gastou em torno de US$ 30

00:11:57.680 --> 00:11:59.920
para atingir esse público-alvo.

00:12:02.600 --> 00:12:07.896
No último ano, o gerente
de mídias sociais do Donald Trump revelou

00:12:07.920 --> 00:12:13.256
que eles estavam usando "dark posts"
do Facebook para desmobilizar pessoas,

00:12:13.280 --> 00:12:14.656
não para persuadi-las,

00:12:14.680 --> 00:12:17.480
mas para convencê-las a não votar.

00:12:18.520 --> 00:12:22.096
E para isso eles miraram especificamente,

00:12:22.120 --> 00:12:26.016
por exemplo, homens afro-americanos
em cidades-chave como a Filadélfia,

00:12:26.040 --> 00:12:28.496
e vou ler exatamente o que ele disse.

00:12:28.520 --> 00:12:29.736
Estou citando.

00:12:29.760 --> 00:12:32.776
Eles estavam usando "publicações fechadas,

00:12:32.776 --> 00:12:35.152
com visualização controlada pela campanha,

00:12:35.152 --> 00:12:38.776
para que sejam vistas apenas pelas pessoas
que queremos que os vejam.

00:12:38.800 --> 00:12:40.016
Nós modelamos isso.

00:12:40.040 --> 00:12:44.760
E vai afetar drasticamente a capacidade
dela de mobilizar essas pessoas".

00:12:45.720 --> 00:12:48.000
O que há nesses "dark posts"?

00:12:48.480 --> 00:12:50.136
Não temos ideia.

00:12:50.160 --> 00:12:51.690
O Facebook não vai nos contar.

00:12:52.480 --> 00:12:56.856
O Facebook também organiza
algoritmicamente as publicações

00:12:56.880 --> 00:13:00.616
de seus amigos
ou das páginas que você segue.

00:13:00.640 --> 00:13:02.856
Ele não mostra tudo em ordem cronológica.

00:13:02.880 --> 00:13:07.696
Ele coloca na ordem em que o algoritmo
pensa que vai induzir você

00:13:07.720 --> 00:13:09.560
a ficar mais tempo no site.

00:13:11.040 --> 00:13:14.416
Isso tem várias consequências.

00:13:14.440 --> 00:13:18.240
Você pode estar pensando que alguém
está desprezando você no Facebook.

00:13:18.800 --> 00:13:22.056
Mas o algoritmo pode não estar
mostrando suas publicações a ele.

00:13:22.080 --> 00:13:28.040
O algoritmo está priorizando
alguns e encobrindo os outros.

00:13:29.320 --> 00:13:30.616
Experimentos mostram

00:13:30.640 --> 00:13:35.160
que o que o algoritmo escolhe para mostrar
a você pode afetar suas emoções.

00:13:36.600 --> 00:13:37.800
Mas não é só isso.

00:13:38.280 --> 00:13:40.640
Ele também afeta o comportamento político.

00:13:41.360 --> 00:13:46.016
Então, em 2010, nas eleições intercalares,

00:13:46.040 --> 00:13:51.936
o Facebook fez um experimento
com 61 milhões de pessoas nos EUA;

00:13:51.960 --> 00:13:53.856
isso foi revelado após o fato.

00:13:53.880 --> 00:13:57.296
Para algumas pessoas ele mostrou:
"Hoje é o dia das eleições",

00:13:57.320 --> 00:13:58.696
a versão mais simples,

00:13:58.720 --> 00:14:02.616
e para outras pessoas mostrou a versão
com aquela pequena barra

00:14:02.640 --> 00:14:07.706
com as fotinhos dos seus amigos
que clicaram "Eu votei".

00:14:09.000 --> 00:14:10.400
Esta barra simples.

00:14:11.710 --> 00:14:15.816
Então, as fotos eram a única diferença,

00:14:15.840 --> 00:14:19.096
e essa publicação,
mostrada apenas uma vez,

00:14:19.120 --> 00:14:25.176
levou 340 mil eleitores a mais

00:14:25.200 --> 00:14:26.896
nessa eleição,

00:14:26.920 --> 00:14:28.616
de acordo com uma pesquisa

00:14:28.640 --> 00:14:31.160
e confirmada nas listas de eleitores.

00:14:32.920 --> 00:14:34.576
Acaso? Não.

00:14:34.600 --> 00:14:39.960
Porque, em 2012, eles repetiram
o mesmo experimento.

00:14:40.840 --> 00:14:42.576
E, dessa vez,

00:14:42.600 --> 00:14:45.896
aquela mensagem cívica
apresentada apenas uma vez

00:14:45.920 --> 00:14:50.360
se converteu em 270 mil eleitores a mais.

00:14:51.160 --> 00:14:56.376
Para referência, as eleições
presidenciais de 2016 nos EUA

00:14:56.400 --> 00:14:59.920
foi decidida por cerca de 100 mil votos.

00:15:01.360 --> 00:15:06.096
Agora, o Facebook pode facilmente 
inferir suas preferências políticas,

00:15:06.120 --> 00:15:08.376
mesmo que você nunca
as tenha revelado no site.

00:15:08.400 --> 00:15:10.920
Certo? Esses algoritmos
fazem isso facilmente.

00:15:11.960 --> 00:15:15.856
E se uma plataforma com esse tipo de poder

00:15:15.880 --> 00:15:20.920
decidir mobilizar apoiadores
de um candidato e não do outro?

00:15:21.680 --> 00:15:24.120
Como vamos ao menos saber disso?

00:15:25.560 --> 00:15:29.696
Vejam, começamos por algo
aparentemente inócuo:

00:15:29.720 --> 00:15:31.936
anúncios on-line que nos perseguem;

00:15:31.960 --> 00:15:33.800
e fomos parar em outro lugar.

00:15:35.480 --> 00:15:37.936
Como público e como cidadãos,

00:15:37.960 --> 00:15:41.376
não sabemos mais se estamos
vendo a mesma informação

00:15:41.400 --> 00:15:42.880
ou o que os outros estão vendo,

00:15:43.680 --> 00:15:46.256
e sem uma base comum de informação,

00:15:46.280 --> 00:15:47.896
aos poucos

00:15:47.920 --> 00:15:51.136
o debate público está ficando impossível,

00:15:51.160 --> 00:15:54.136
e estamos apenas
nos primeiros estágios disso.

00:15:54.160 --> 00:15:57.616
Esses algoritmos podem inferir facilmente

00:15:57.640 --> 00:16:00.896
coisas como sua etnia,

00:16:00.896 --> 00:16:03.442
posição religiosa e política,
traços de personalidade,

00:16:03.442 --> 00:16:06.656
inteligência, felicidade,
uso de substâncias viciantes,

00:16:06.680 --> 00:16:09.816
separação dos pais, idade e gênero,

00:16:09.840 --> 00:16:11.800
só a partir das curtidas no Facebook.

00:16:13.440 --> 00:16:17.496
Esses algoritmos podem
identificar manifestantes

00:16:17.520 --> 00:16:20.280
mesmo que seus rostos
estejam parcialmente ocultos.

00:16:21.720 --> 00:16:28.336
Esses algoritmos podem detectar
a orientação sexual das pessoas

00:16:28.360 --> 00:16:31.560
só pelas fotos de perfil
de seus relacionamentos.

00:16:33.560 --> 00:16:36.176
Essas são inferências probabilísticas,

00:16:36.200 --> 00:16:39.096
então não estarão 100% corretas,

00:16:39.120 --> 00:16:44.016
mas não vejo os poderosos resistindo
à tentação de usar essas tecnologias

00:16:44.040 --> 00:16:46.216
só porque há alguns falsos positivos,

00:16:46.240 --> 00:16:49.496
que certamente vão criar
outra camada de problemas.

00:16:49.520 --> 00:16:52.456
Imagine o que um Estado pode fazer

00:16:52.480 --> 00:16:56.040
com a imensa quantidade de informação
que tem de seus cidadãos.

00:16:56.680 --> 00:17:01.456
A China já usa tecnologia
de detecção facial

00:17:01.480 --> 00:17:04.360
para identificar e prender pessoas.

00:17:05.280 --> 00:17:07.416
E a tragédia é a seguinte:

00:17:07.440 --> 00:17:12.976
estamos construindo infraestrutura
de vigilância autoritária

00:17:13.000 --> 00:17:15.960
só para que as pessoas
cliquem em anúncios.

00:17:17.240 --> 00:17:19.816
E esse não será o autoritarismo do Orwell.

00:17:19.839 --> 00:17:21.736
Isso não é "1984".

00:17:21.760 --> 00:17:26.336
Agora, se o autoritarismo usar
o medo explícito para nos aterrorizar,

00:17:26.358 --> 00:17:29.256
estaremos todos com medo,
mas saberemos disso,

00:17:29.280 --> 00:17:31.480
vamos odiá-lo e resistir a ele.

00:17:32.880 --> 00:17:37.296
Mas, se as pessoas no poder
estiverem usando esses algoritmos

00:17:37.319 --> 00:17:40.696
para nos observar discretamente,

00:17:40.720 --> 00:17:42.800
para nos julgar e para nos incitar,

00:17:43.720 --> 00:17:47.896
para prever e identificar
os encrenqueiros e os rebeldes,

00:17:47.920 --> 00:17:51.816
para aplicar arquiteturas
de persuasão em larga escala

00:17:51.840 --> 00:17:55.976
e para manipular as pessoas uma a uma

00:17:56.000 --> 00:18:01.440
usando suas fraquezas e vulnerabilidades
pessoais e individuais,

00:18:02.720 --> 00:18:04.920
e se estiverem fazendo
isso em larga escala,

00:18:06.080 --> 00:18:07.816
através de nossas telas privadas

00:18:07.840 --> 00:18:09.496
de forma que nem saibamos

00:18:09.520 --> 00:18:12.280
o que nossos colegas
cidadãos e vizinhos estão vendo,

00:18:13.560 --> 00:18:18.376
esse autoritarismo vai nos envolver
como uma teia de aranha

00:18:18.400 --> 00:18:20.880
e podemos nem saber que estamos nela.

00:18:22.440 --> 00:18:25.376
A capitalização de mercado do Facebook

00:18:25.400 --> 00:18:28.696
está perto de US$ 500 trilhões.

00:18:28.720 --> 00:18:32.230
É assim porque ele funciona muito bem
como uma arquitetura de persuasão.

00:18:33.760 --> 00:18:36.576
Mas a estrutura dessa arquitetura

00:18:36.600 --> 00:18:39.816
é a mesma se você vende sapatos

00:18:39.840 --> 00:18:42.336
ou política.

00:18:42.360 --> 00:18:45.480
O algoritmo não sabe a diferença.

00:18:46.240 --> 00:18:49.536
O mesmo algoritmo
que faz o que quiser conosco

00:18:49.560 --> 00:18:52.736
para nos tornar mais
influenciáveis pelos anúncios

00:18:52.760 --> 00:18:59.496
também organiza o fluxo de informações
políticas, pessoais e sociais,

00:18:59.520 --> 00:19:01.360
e é isso que precisa mudar.

00:19:02.240 --> 00:19:04.536
Não me entendam mal,

00:19:04.560 --> 00:19:08.240
usamos plataformas digitais
porque elas são valiosas para nós.

00:19:09.120 --> 00:19:12.870
Eu uso o Facebook para manter contato
com amigos e parentes no mundo inteiro.

00:19:14.000 --> 00:19:19.776
Já escrevi sobre como as mídias sociais
são cruciais para os movimentos sociais.

00:19:19.800 --> 00:19:22.816
Tenho estudado como essas
tecnologias podem ser usadas

00:19:22.840 --> 00:19:25.320
para contornar a censura em todo o mundo.

00:19:27.280 --> 00:19:33.696
Não é que as pessoas
que dirigem o Facebook ou Google

00:19:33.720 --> 00:19:35.900
estejam maliciosamente e deliberadamente

00:19:35.920 --> 00:19:40.896
tentando tornar o país
ou o mundo mais polarizado

00:19:40.920 --> 00:19:42.600
e encorajando o extremismo.

00:19:43.440 --> 00:19:47.416
Li as diversas declarações
bem-intencionadas

00:19:47.440 --> 00:19:50.760
que essas pessoas expressam.

00:19:51.600 --> 00:19:57.656
Mas não são as intenções ou declarações
das pessoas de tecnologia que importam,

00:19:57.680 --> 00:20:01.240
são as estruturas e modelos de negócio
que elas estão criando.

00:20:02.360 --> 00:20:04.456
E esse é o cerne do problema.

00:20:04.480 --> 00:20:09.200
Ou o Facebook é uma enorme fraude
de US$ 500 bilhões

00:20:10.200 --> 00:20:12.096
e os anúncios não funcionam no site

00:20:12.120 --> 00:20:14.816
e ele não funciona
como uma arquitetura de persuasão,

00:20:14.840 --> 00:20:18.960
ou seu poder de influência
é muito preocupante.

00:20:20.560 --> 00:20:22.336
Ou um ou outro.

00:20:22.360 --> 00:20:23.960
E o mesmo para o Google.

00:20:24.880 --> 00:20:27.336
Então o que podemos fazer?

00:20:27.360 --> 00:20:29.296
Isso precisa mudar.

00:20:29.320 --> 00:20:31.896
Eu não tenho uma receita simples,

00:20:31.920 --> 00:20:34.176
porque precisamos reestruturar

00:20:34.200 --> 00:20:37.216
toda a forma como opera
a nossa tecnologia digital.

00:20:37.240 --> 00:20:41.336
Tudo, desde a forma
como a tecnologia é desenvolvida

00:20:41.360 --> 00:20:45.216
até a forma como os incentivos,
econômicos e outros,

00:20:45.240 --> 00:20:47.520
são construídos no sistema.

00:20:48.480 --> 00:20:51.936
Precisamos encarar e tentar lidar

00:20:51.960 --> 00:20:56.616
com a falta de transparência criada
pelos proprietários dos algoritmos,

00:20:56.640 --> 00:21:00.456
o desafio estrutural da opacidade
do aprendizado de máquina,

00:21:00.480 --> 00:21:03.880
e de todos esses dados sendo coletados
sobre nós indiscriminadamente.

00:21:05.000 --> 00:21:07.520
Temos uma tarefa enorme diante de nós.

00:21:08.160 --> 00:21:10.840
Temos que mobilizar nossa tecnologia,

00:21:11.760 --> 00:21:13.336
nossa criatividade

00:21:13.360 --> 00:21:15.240
e, sim, nossos políticos,

00:21:16.240 --> 00:21:18.896
para que possamos criar
inteligência artificial

00:21:18.920 --> 00:21:22.040
que dê suporte a nossos objetivos humanos,

00:21:22.800 --> 00:21:26.720
mas que também seja limitada
por nossos valores humanos.

00:21:27.600 --> 00:21:29.760
E eu entendo que isso não será fácil.

00:21:30.360 --> 00:21:33.960
Podemos não concordar facilmente
com o significado desses termos.

00:21:34.920 --> 00:21:37.320
Mas, se levarmos a sério

00:21:38.240 --> 00:21:44.216
a forma como operam esses sistemas
dos quais tanto dependemos,

00:21:44.240 --> 00:21:48.360
não vejo como postergar
mais essa discussão.

00:21:49.200 --> 00:21:51.736
Essas estruturas

00:21:51.760 --> 00:21:55.856
estão organizando o modo como funcionamos

00:21:55.880 --> 00:21:58.176
e estão controlando

00:21:58.200 --> 00:22:00.816
o que podemos ou não fazer.

00:22:00.840 --> 00:22:03.296
E muitas dessas plataformas
financiadas por anúncios

00:22:03.320 --> 00:22:04.896
se vangloriam de serem gratuitas.

00:22:04.920 --> 00:22:09.480
Nesse contexto, o produto
que está sendo vendido somos nós.

00:22:10.840 --> 00:22:13.576
Precisamos de uma economia digital

00:22:13.600 --> 00:22:17.096
em que nossos dados e nossa atenção

00:22:17.120 --> 00:22:22.200
não estejam à venda para o ditador
ou demagogo que ofereça mais.

00:22:23.160 --> 00:22:25.790
(Aplausos)

00:22:30.480 --> 00:22:33.736
Então, parafraseando Hollywood novamente,

00:22:33.760 --> 00:22:37.496
queremos que o imenso potencial

00:22:37.520 --> 00:22:40.720
da inteligência artificial
e da tecnologia digital floresçam,

00:22:41.400 --> 00:22:46.336
mas para isso precisamos enfrentar
essa imensa ameaça,

00:22:46.360 --> 00:22:48.296
de olhos abertos e agora.

00:22:48.320 --> 00:22:49.536
Obrigada.

00:22:49.560 --> 00:22:51.762
(Aplausos)

