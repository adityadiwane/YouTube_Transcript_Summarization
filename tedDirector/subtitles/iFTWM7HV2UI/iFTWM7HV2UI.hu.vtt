WEBVTT
Kind: captions
Language: hu

00:00:00.000 --> 00:00:07.000
Fordító: Bálint Laza
Lektor: Andi Vida

00:00:12.760 --> 00:00:16.296
Amikor az emberek aggódnak
a mesterséges intelligencia miatt,

00:00:16.320 --> 00:00:20.296
nagyon gyakran ámokfutó
robotokra gondolnak.

00:00:20.320 --> 00:00:21.560
Ugye? A Terminátor?

00:00:22.400 --> 00:00:24.736
Ez tényleg megfontolandó lehet,

00:00:24.760 --> 00:00:26.616
de távoli fenyegetés.

00:00:26.640 --> 00:00:30.096
Vagy múltból szedett metaforákkal
idegeskedünk

00:00:30.120 --> 00:00:31.896
a digitális megfigyelés miatt.

00:00:31.920 --> 00:00:34.576
1984. George Orwell 1984-e

00:00:34.600 --> 00:00:36.880
újra bestseller.

00:00:37.960 --> 00:00:39.376
Remek könyv,

00:00:39.400 --> 00:00:43.280
de nem a 21. századról szóló disztópia.

00:00:43.560 --> 00:00:45.496
Amitől sokkal jobban kell félnünk,

00:00:45.520 --> 00:00:50.296
nem az, hogy a mesterséges intelligencia
mit fog tenni magától,

00:00:50.320 --> 00:00:55.056
hanem hogy a hatalomban lévők
mire fogják használni ezt,

00:00:55.080 --> 00:00:57.896
hogy szokatlan, néha rejtett,

00:00:57.920 --> 00:01:01.056
kifinomult és váratlan módokon

00:01:01.080 --> 00:01:04.096
ellenőrizzenek és manipuláljanak minket.

00:01:04.120 --> 00:01:05.976
A technológia nagy részét,

00:01:06.000 --> 00:01:10.336
ami szabadságunkat és méltóságunkat 
veszélyezteti a közeljövőben,

00:01:10.360 --> 00:01:12.216
olyan cégek fejlesztik,

00:01:12.240 --> 00:01:17.176
akiknek üzleti érdekében áll megszerezni
és eladni adatainkat, érdeklődési körünket

00:01:17.200 --> 00:01:19.456
hirdetőknek és másoknak:

00:01:19.480 --> 00:01:22.896
Facebook, Google, Amazon

00:01:22.920 --> 00:01:24.800
Alibaba, Tencent.

00:01:26.040 --> 00:01:31.536
A mesterséges intelligencia
már az ő üzleteiket is erősíti.

00:01:31.560 --> 00:01:33.656
És úgy tűnhet, a mesterséges intelligencia

00:01:33.680 --> 00:01:36.536
csak egy újabb lépés
az online reklámok után.

00:01:36.560 --> 00:01:37.776
De nem az.

00:01:37.800 --> 00:01:40.256
Kategóriaugrás.

00:01:40.280 --> 00:01:42.856
Teljesen más világ,

00:01:42.880 --> 00:01:45.496
és nagy lehetőségek vannak benne.

00:01:45.520 --> 00:01:52.440
Segíthet megérteni sok terület
tanulmányait és kutatásait.

00:01:53.120 --> 00:01:56.616
Egy híres hollywoodi filozófust idézve:

00:01:56.640 --> 00:02:00.280
„A hatalmas lehetőségek
hatalmas kockázattal járnak”

00:02:01.120 --> 00:02:05.056
Most nézzük meg digitális életünk
alapvetését, az online hirdetéseket.

00:02:05.080 --> 00:02:07.976
Észre sem vesszük őket, nem?

00:02:08.000 --> 00:02:09.976
Erőszakosnak, hatástalannak tűnnek.

00:02:10.000 --> 00:02:14.256
Mindegyikünk megtapasztalta már,
hogy olyan reklám követ minket a weben,

00:02:14.280 --> 00:02:17.056
ami a kereséseink, olvasmányaink
témájához kötődik.

00:02:17.080 --> 00:02:18.936
Rákeresünk például egy pár csizmára,

00:02:18.960 --> 00:02:22.336
ami aztán hetekig a nyomunkban jár,
bármerre járunk.

00:02:22.360 --> 00:02:26.016
Még azután is, hogy megadjuk magunkat,
és megvesszük.

00:02:26.040 --> 00:02:29.056
Hozzászokhattunk már ehhez az
egyszerű, olcsó manipulációhoz.

00:02:29.080 --> 00:02:32.480
Forgatjuk a szemünk, és azt gondoljuk,
„Eh, ez nem működik.”

00:02:33.720 --> 00:02:35.816
A digitális technológiák nemcsak reklámok,

00:02:35.840 --> 00:02:39.440
eltekintve online felületüktől.

00:02:40.240 --> 00:02:43.360
Ennek megértéséhez nézzünk
egy példát a fizikai világból.

00:02:43.840 --> 00:02:48.496
Tudják, hogy a szupermarketekben 
a pénztár közelében

00:02:48.520 --> 00:02:52.000
nyalókák és rágógumik vannak
a gyerekek szemmagasságában?

00:02:52.800 --> 00:02:56.296
Ezt úgy tervezték, hogy 
sírjanak a szüleiknek,

00:02:56.320 --> 00:02:59.400
amikor épp sorban állnak a fizetéshez.

00:03:00.040 --> 00:03:02.680
Ez a meggyőzés architektúrája.

00:03:03.160 --> 00:03:06.256
Nem szép, de láthatóan működik.

00:03:06.280 --> 00:03:08.320
Ezért látjuk ezt minden szupermarketben.

00:03:08.720 --> 00:03:10.416
Na most, a fizikai világban

00:03:10.440 --> 00:03:12.936
ez az architektúra
viszonylag korlátozott,

00:03:12.960 --> 00:03:17.776
mert nem lehet mindent
a pénztárhoz rakni, ugye?

00:03:17.800 --> 00:03:22.096
A nyalóka meg a rágógumi
mindenkinek ugyanolyan,

00:03:22.120 --> 00:03:23.576
bár általában jobban működik

00:03:23.600 --> 00:03:27.640
azoknál az embereknél,
akik mellett kicsi síró alakok állnak.

00:03:29.160 --> 00:03:33.080
A fizikai világban
együtt élünk ezekkel a korlátokkal.

00:03:34.280 --> 00:03:36.216
A digitális világban viszont

00:03:36.240 --> 00:03:40.560
a meggyőzési architektúrák
milliárdos nagyságrendűek lehetnek,

00:03:41.840 --> 00:03:45.696
és megcélozhatnak, felbecsülhetnek,
kiismerhetnek egyéneket,

00:03:45.720 --> 00:03:48.616
majd egyesével

00:03:48.640 --> 00:03:49.856
bevethetők ellenük,

00:03:49.880 --> 00:03:52.016
rátapintva gyengeségeikre,

00:03:52.040 --> 00:03:57.656
és bárki privát telefon-képernyőjére
elküldhetők,

00:03:57.680 --> 00:03:59.936
így a többiek nem látják.

00:03:59.960 --> 00:04:01.216
És ettől más.

00:04:01.240 --> 00:04:04.816
Ez csak egyetlen egyszerű dolog,
amire a mesterséges intelligencia képes.

00:04:04.840 --> 00:04:06.176
Nézzünk egy példát.

00:04:06.200 --> 00:04:08.896
Mondjuk, repülőjegyeket akarunk eladni
Las Vegasba. Jó?

00:04:08.920 --> 00:04:12.416
A régi világban tapasztalat
és feltételezések alapján

00:04:12.440 --> 00:04:14.960
némi demográfiai célt is tippelhetnénk.

00:04:15.560 --> 00:04:18.376
Tegyük fel, hogy mondjuk,

00:04:18.400 --> 00:04:20.896
25 és 35 közötti férfiaknak hirdetjük meg,

00:04:20.920 --> 00:04:24.440
vagy olyanoknak, akiknek
magas hitelkeret van a kártyájukon,

00:04:24.440 --> 00:04:26.256
vagy nyugdíjas pároknak. Nem?

00:04:26.280 --> 00:04:28.096
Ezt tennénk a múltban.

00:04:28.120 --> 00:04:31.016
Nagy adatmennyiséggel és gépi tanulással

00:04:31.040 --> 00:04:32.564
ez többé már nem így működik.

00:04:33.320 --> 00:04:35.496
Úgyhogy képzeljék el,

00:04:35.520 --> 00:04:39.376
gondoljanak az összes adatra,
amit a Facebook őriz önökről:

00:04:39.400 --> 00:04:41.936
minden állapotfrissítés, amit valaha 
begépeltek,

00:04:41.960 --> 00:04:43.976
minden Messenger-beszélgetés,

00:04:44.000 --> 00:04:45.880
minden hely, ahonnan bejelentkeztek,

00:04:48.400 --> 00:04:51.576
minden fotó, amit feltöltöttek oda.

00:04:51.600 --> 00:04:55.376
Ha elkezdenek begépelni valamit,
aztán meggondolják magukat, és törlik,

00:04:55.400 --> 00:04:58.600
a Facebook azt is megtartja, és elemzi.

00:04:59.160 --> 00:05:03.096
Sőt, megpróbálja összekötni önöket
offline adataikkal.

00:05:03.120 --> 00:05:06.296
Sok adatot vásárol
adatkereskedőktől is.

00:05:06.320 --> 00:05:09.736
Ez bármi lehet:
a pénzügyi nyilvántartásuktól kezdve

00:05:09.760 --> 00:05:11.880
a böngészési előzményeik jó részéig.

00:05:12.360 --> 00:05:17.776
Értik? Az Államokban ezeket az adatokat
rutinszerűen gyűjtik,

00:05:17.800 --> 00:05:19.760
elemzik és eladják.

00:05:20.320 --> 00:05:22.760
Európában ezt szigorúbban szabályozzák.

00:05:23.680 --> 00:05:25.880
Így az történik, hogy amint

00:05:26.920 --> 00:05:30.936
ezek a gépi tanulásos algoritmusok
végigfutnak az adatokon –

00:05:30.960 --> 00:05:33.856
ezért hívjuk tanuló algoritmusoknak –,

00:05:33.880 --> 00:05:37.976
megtanulják megérteni azok jellemzőit,

00:05:38.000 --> 00:05:40.520
akik korábban jegyet vettek Las Vegasba.

00:05:41.760 --> 00:05:45.296
Amikor meglévő adatból tanulnak,

00:05:45.320 --> 00:05:49.136
megtanulják azt is,
hogyan alkalmazzák új emberekre.

00:05:49.160 --> 00:05:52.216
Így ha új személlyel találkoznak,

00:05:52.240 --> 00:05:56.880
be tudják sorolni hogy ő valószínűleg
vesz-e jegyet Vegasba, vagy nem.

00:05:57.720 --> 00:06:03.176
Rendben. Önök azt hiszik,
ez egy ajánlat Vegasba.

00:06:03.200 --> 00:06:04.656
Ezt figyelmen kívül hagyhatom.

00:06:04.680 --> 00:06:06.896
A probléma azonban nem ez.

00:06:06.920 --> 00:06:08.496
Hanem az, hogy többé

00:06:08.520 --> 00:06:12.656
nem igazán értjük, hogyan működnek
ezek az összetett algoritmusok.

00:06:12.680 --> 00:06:16.136
Nem értjük, hogyan csoportosítanak.

00:06:16.160 --> 00:06:20.576
Hatalmas mátrixok,
több ezernyi sor és oszlop,

00:06:20.600 --> 00:06:22.560
talán milliónyi sor vagy oszlop,

00:06:23.320 --> 00:06:25.960
és sem a programozók,

00:06:26.760 --> 00:06:28.440
sem senki, aki ránéz,

00:06:29.440 --> 00:06:30.936
nem érti már
a pontos működését,

00:06:30.960 --> 00:06:35.576
még akkor sem, ha megvan az összes adat,

00:06:35.600 --> 00:06:39.376
mint ahogy azt sem tudnák,
mit gondolok éppen,

00:06:39.400 --> 00:06:43.360
még akkor sem, ha megmutatnám
az agyam keresztmetszetét.

00:06:44.360 --> 00:06:46.936
Mintha többé nem programoznánk,

00:06:46.960 --> 00:06:51.360
olyan intelligenciát fejlesztünk,
amit nem értünk teljesen.

00:06:52.520 --> 00:06:56.496
És mindezek csak akkor működnek,
ha rengeteg adatunk van,

00:06:56.520 --> 00:07:01.616
így arra is ösztönöznek, hogy alaposan
megfigyeljenek mindannyiunkat,

00:07:01.640 --> 00:07:03.976
hogy működhessenek
a gépi tanulásos algoritmusok.

00:07:04.000 --> 00:07:07.176
Ezért gyűjt össze a Facebook
minden adatot rólunk, amit csak tud.

00:07:07.200 --> 00:07:08.776
Az algoritmusok jobban működnek.

00:07:08.800 --> 00:07:11.496
Nézzük még kicsit a vegasi példát.

00:07:11.520 --> 00:07:15.200
Mi van, ha a rendszer, amit nem értünk,

00:07:16.200 --> 00:07:21.336
rájött, hogy egyszerűbb vegasi jegyeket
eladni azoknak,

00:07:21.360 --> 00:07:25.120
akiknek bipoláris zavara
éppen a mániákus szakaszba lép?

00:07:25.640 --> 00:07:30.560
Ők gyakran lesznek túlköltekezők,
kényszeres szerencsejátékosok.

00:07:31.280 --> 00:07:35.736
Kiválaszthatja őket ennek alapján,
és mi még csak nem is sejtjük.

00:07:35.760 --> 00:07:39.376
Egyszer informatikusok körében
mondtam el ezt a példát,

00:07:39.400 --> 00:07:41.456
utána egyikük odajött hozzám.

00:07:41.480 --> 00:07:45.000
Zavart volt, és azt mondta:
„Ezért nem tudom kiadni.”

00:07:45.600 --> 00:07:47.315
Mondom: „Mit nem tud kiadni?”

00:07:47.800 --> 00:07:53.270
Olyan szoftvert írt, ami azt vizsgálja:
megállapítható-e a mánia megjelenése

00:07:53.270 --> 00:07:56.896
közösségi médiás bejegyzésekből
a klinikai tünetek megjelenése előtt,

00:07:56.920 --> 00:07:58.696
és kiderült, hogy igen,

00:07:58.720 --> 00:08:00.776
a programja nagyon is jól működött,

00:08:00.800 --> 00:08:05.680
ő pedig nem tudta, hogyan működik,
vagy hogy milyen fogásra akadt.

00:08:06.840 --> 00:08:11.256
A probléma nem oldódik meg,
ha nem adja ki a szoftvert,

00:08:11.280 --> 00:08:13.176
mert már vannak cégek,

00:08:13.200 --> 00:08:15.736
akik fejlesztenek ilyen technológiát,

00:08:15.760 --> 00:08:18.560
és már sok minden elérhető.

00:08:19.240 --> 00:08:21.816
Ez ma már nem túl bonyolult.

00:08:21.840 --> 00:08:25.296
Mentek-e már úgy a Youtube-ra,
hogy csak egy videót néznek meg,

00:08:25.320 --> 00:08:27.680
és egy óra múlva
már a huszonhetediknél tartottak?

00:08:28.760 --> 00:08:31.256
Ismerik-e a Youtube-on
a jobb oldali oszlopot,

00:08:31.280 --> 00:08:33.496
ahol ez áll: „Következő”,

00:08:33.520 --> 00:08:35.336
és automatikusan lejátszik valamit?

00:08:35.360 --> 00:08:36.576
Az egy algoritmus,

00:08:36.600 --> 00:08:39.760
ami kiválasztja, hogy feltehetően
mi érdekli a látogatót,

00:08:39.760 --> 00:08:41.776
és nem biztos, hogy magunktól rátalálnánk.

00:08:41.800 --> 00:08:43.056
Ezt nem ember szerkeszti.

00:08:43.080 --> 00:08:44.496
Így működnek az algoritmusok.

00:08:44.520 --> 00:08:49.256
Annak alapján, amit mi
és a hozzánk hasonló látogatók megnézünk,

00:08:49.280 --> 00:08:53.496
kiválasztja és kikövetkezteti,
mi az, ami még érdekelhet minket,

00:08:53.520 --> 00:08:54.775
miből szeretnénk még,

00:08:54.799 --> 00:08:56.135
és ezekből többet mutat.

00:08:56.159 --> 00:08:58.360
Jószándékú és hasznos
funkciónak hangzik,

00:08:59.280 --> 00:09:00.480
de nem mindig az.

00:09:01.640 --> 00:09:08.600
2016-ban részt vettem 
az akkor jelölt Donald Trump gyűlésein,

00:09:09.840 --> 00:09:13.176
hogy kutatóként tanulmányozzam
az őt támogató mozgalmat.

00:09:13.200 --> 00:09:16.656
Társadalmi mozgalmakat kutatok,
ezért ezt is tanulmányoztam.

00:09:16.680 --> 00:09:20.016
Aztán írni akartam valamit
az egyik gyűléséről,

00:09:20.040 --> 00:09:22.000
ezért megnéztem párszor a Youtube-on.

00:09:22.720 --> 00:09:26.336
A Youtube fehér felsőbbrendűségről szóló
videókat kezdett ajánlgatni

00:09:26.360 --> 00:09:30.616
és automatikusan lejátszani nekem,

00:09:30.640 --> 00:09:33.296
egyre szélsőségesebbeket.

00:09:33.320 --> 00:09:35.136
Ha megnéztem egyet,

00:09:35.160 --> 00:09:38.136
még szélsőségesebbet kaptam,

00:09:38.160 --> 00:09:40.114
és azt is magától lejátszotta.

00:09:40.320 --> 00:09:44.856
Ha Hillary Clintonról vagy
Bernie Sandersről nézünk tartalmakat,

00:09:44.880 --> 00:09:49.576
a Youtube automatikusan 
összeesküvés-elméleteket kezd lejátszani,

00:09:49.600 --> 00:09:51.360
innentől már csak lejjebb van.

00:09:52.480 --> 00:09:55.536
Önök azt gondolhatják: ez politika,
de nem az.

00:09:55.560 --> 00:09:56.816
Ez nem a politikáról szól.

00:09:56.840 --> 00:09:59.936
Ez csak egy algoritmus,
ami kitalálja az emberi viselkedést.

00:09:59.960 --> 00:10:04.736
Egyszer megnéztem egy videót
a vegetarianizmusról a Youtube-on,

00:10:04.760 --> 00:10:09.696
a Youtube ajánlott és el is indított
egy videót a vegán életmódról.

00:10:09.720 --> 00:10:12.736
Olyan, mintha sosem lennénk
elég kemények a Youtube-nak.

00:10:12.760 --> 00:10:14.336
(Nevetés)

00:10:14.360 --> 00:10:15.920
Szóval, mi folyik itt?

00:10:16.510 --> 00:10:20.056
A Youtube algoritmusa
szabadalommal védett,

00:10:20.080 --> 00:10:22.440
de elmondom, mit gondolok.

00:10:23.360 --> 00:10:25.456
Az algoritmus rájött,

00:10:25.480 --> 00:10:29.176
ha rá tudjuk venni az embereket,
hogy azt gondolják:

00:10:29.200 --> 00:10:32.936
valami keményebbet tudunk nekik mutatni,

00:10:32.960 --> 00:10:35.376
akkor nagyobb valószínűséggel 
maradnak az oldalon,

00:10:35.400 --> 00:10:39.816
és nézik egyik videót a másik után,
egyre mélyebbre merülve,

00:10:39.840 --> 00:10:41.930
míg a Google hirdetéseket mutat nekik.

00:10:43.760 --> 00:10:46.880
Most, hogy senkit nem érdekel
a boltok etikája,

00:10:47.720 --> 00:10:51.960
ezek az oldalak pontosan
meg tudják találni

00:10:53.680 --> 00:10:55.600
a zsidógyűlölőket,

00:10:56.360 --> 00:10:58.840
azokat, akik szerint a zsidók paraziták,

00:11:00.320 --> 00:11:05.240
azokat, akik kifejezetten
antiszemita tartalmakat tesznek fel,

00:11:06.080 --> 00:11:08.080
és célzott reklámokat küld nekik.

00:11:09.200 --> 00:11:12.736
Arra is használhatják az algoritmusokat,

00:11:12.760 --> 00:11:15.896
hogy megtalálják a hasonló közönséget,

00:11:15.920 --> 00:11:21.496
azokat, akiknek nincs ilyen határozott
antiszemita tartalom a profilján,

00:11:21.520 --> 00:11:27.696
de az algoritmus felismeri,
hogy fogékonyak lehetnek ilyen üzenetekre,

00:11:27.720 --> 00:11:30.000
és nekik is küldi a célzott hirdetéseket.

00:11:30.680 --> 00:11:33.416
Ez valószínűtlen példának tűnhet,

00:11:33.440 --> 00:11:34.760
pedig valódi.

00:11:35.480 --> 00:11:37.616
A ProPublica nyomozott az ügyben,

00:11:37.640 --> 00:11:41.256
és rájöttek, hogy ez tényleg
megtehető a Facebookon,

00:11:41.280 --> 00:11:43.696
a Facebook még segítőkészen
javaslatokat is tesz,

00:11:43.720 --> 00:11:45.320
hogyan bővítsük ezt a közönséget.

00:11:46.720 --> 00:11:49.736
A BuzzFeed tesztelte a Google-t,
és nagyon gyorsan rájöttek,

00:11:49.760 --> 00:11:51.496
hogy bizony rá is
ugyanez vonatkozik.

00:11:51.520 --> 00:11:53.216
És még csak drága sem volt.

00:11:53.240 --> 00:11:57.656
A ProPublica riportere 30 dollárt költött

00:11:57.680 --> 00:11:59.920
ennek a kategóriának a bemérésére.

00:12:02.600 --> 00:12:07.896
Tavaly tehát Donald Trump 
közösségimédia-menedzsere elmondta,

00:12:07.920 --> 00:12:13.256
hogy úgynevezett "dark postokkal"
igyekeznek otthon tartani az embereket,

00:12:13.280 --> 00:12:14.896
nem azért, hogy meggyőzzék őket,

00:12:14.896 --> 00:12:17.480
hanem hogy egyáltalán ne szavazzanak.

00:12:18.524 --> 00:12:22.120
És ehhez kifinomult módon
állítottak be célcsoportokat,

00:12:22.120 --> 00:12:26.016
például kulcsvárosokban, például
Philadelphiában élő afro-amerikaiakat,

00:12:26.040 --> 00:12:28.496
és mindjárt felolvasom,
mit mondott pontosan.

00:12:28.520 --> 00:12:29.736
Idézem.

00:12:29.760 --> 00:12:32.776
"Nem nyilvános posztokat" használtak,

00:12:32.800 --> 00:12:34.976
"ezek olvasóit a kampány szabályozta,

00:12:35.000 --> 00:12:38.776
hogy csak azok lássák, akiket
erre célzottan kiválasztunk.

00:12:38.800 --> 00:12:40.016
Modelleztük ezt.

00:12:40.040 --> 00:12:44.760
Drasztikusan befolyásolja az embereket
a szavazásból való kimaradásra."

00:12:45.720 --> 00:12:48.000
Mi volt ezekben a "sötét bejegyzésekben"?

00:12:48.480 --> 00:12:50.136
Ötletünk sincs.

00:12:50.160 --> 00:12:51.360
A Facebook nem árulja el.

00:12:52.480 --> 00:12:56.856
A Facebook tehát algoritmus szerint 
rendezi a bejegyzéseket,

00:12:56.880 --> 00:13:00.616
amiket a barátaink kitesznek,
vagy az oldalakat, amiket követünk.

00:13:00.640 --> 00:13:02.856
Nem időrend szerint jeleníti meg ezeket.

00:13:02.880 --> 00:13:07.696
Az algoritmus elgondolása szerint
olyan sorrendbe rendezi,

00:13:07.720 --> 00:13:09.560
hogy minél több időt töltsünk ott.

00:13:10.250 --> 00:13:14.416
Ez pedig rengeteg következményt
von maga után.

00:13:14.440 --> 00:13:18.240
Azt gondolhatjuk, hogy valaki
mellőz minket a Facebookon.

00:13:18.800 --> 00:13:22.056
Pedig lehet, hogy az algoritmus
elrejti előle a bejegyzéseinket.

00:13:22.080 --> 00:13:28.040
Az algoritmus egy részüket kiemeli,
a többit eltemeti.

00:13:29.320 --> 00:13:30.616
A kísérletek szerint

00:13:30.640 --> 00:13:35.160
az algoritmus választásai
befolyásolhatják az érzelmeinket.

00:13:36.600 --> 00:13:37.800
De ez még nem minden.

00:13:38.280 --> 00:13:40.640
A politikai viselkedést is befolyásolja.

00:13:41.360 --> 00:13:46.016
2010-ben a félidős választásokon

00:13:46.040 --> 00:13:51.936
a Facebook 61 millió amerikat tesztelt,

00:13:51.960 --> 00:13:53.856
és ezt csak utólag mondták el.

00:13:53.880 --> 00:13:57.296
Néhány embernek azt mutatták:
"Ma vannak a választások",

00:13:57.320 --> 00:13:58.696
ez volt az egyszerűbb,

00:13:58.720 --> 00:14:02.616
másoknak bonyolultabbat,

00:14:02.640 --> 00:14:04.736
azokkal a kis bélyegképekkel

00:14:04.760 --> 00:14:07.600
a barátainkról, akik rákattintottak arra, 
hogy "Szavaztam".

00:14:09.000 --> 00:14:10.400
Ez egyszerű különbség.

00:14:11.520 --> 00:14:15.816
Értik? Csak a képek jelentették
az egyetlen eltérést,

00:14:15.840 --> 00:14:19.096
és a bejegyzést csak egyszer mutatták,

00:14:19.120 --> 00:14:25.176
ez a kutatás szerint

00:14:25.200 --> 00:14:26.896
340 ezer plusz szavazót jelentett

00:14:26.920 --> 00:14:28.616
azon a választáson,

00:14:28.640 --> 00:14:31.160
ezt a szavazólisták megerősítették.

00:14:32.920 --> 00:14:34.576
Mázli? Nem.

00:14:34.600 --> 00:14:39.960
Mert 2012-ben megismételték
ugyanezt a kísérletet.

00:14:40.840 --> 00:14:42.576
Ekkor

00:14:42.600 --> 00:14:45.896
ez az egyszer megjelenített
állampolgári üzenet

00:14:45.920 --> 00:14:50.360
270 ezer további szavazót hozott.

00:14:51.160 --> 00:14:56.376
Referenciaként, a 2016-os
amerikai elnökválasztást

00:14:56.400 --> 00:14:59.920
százezer szavazó döntötte el.

00:15:01.360 --> 00:15:06.096
A Facebook könnyedén kikövetkezteti
politikai vonzódásunkat,

00:15:06.120 --> 00:15:08.686
akkor is, ha ezt soha nem tárjuk
nyilvánosság elé.

00:15:08.686 --> 00:15:11.530
Ezeknek az algoritmusoknak ez nem kihívás.

00:15:11.960 --> 00:15:15.856
Mi van, ha egy platform
ezzel a típusú hatalommal

00:15:15.880 --> 00:15:20.920
úgy dönt, hogy egy jelöltet támogat
egy másikkal szemben?

00:15:21.680 --> 00:15:24.120
Honnan fogunk egyáltalán tudni róla?

00:15:25.560 --> 00:15:29.696
Valami látszólag ártalmatlan 
dologgal kezdtünk –

00:15:29.720 --> 00:15:31.936
minket követő online hirdetések –,

00:15:31.960 --> 00:15:33.800
és máshová lyukadtunk ki.

00:15:35.480 --> 00:15:37.936
Közösségként és állampolgárokként

00:15:37.960 --> 00:15:41.376
többé nem tudjuk,
hogy ugyanazt az információt látjuk,

00:15:41.400 --> 00:15:42.880
vagy azt, amit bárki más lát,

00:15:43.680 --> 00:15:46.256
és közös információs alapok nélkül,

00:15:46.280 --> 00:15:47.896
lépésről lépésre

00:15:47.920 --> 00:15:51.136
lehetetlenné válnak a nyilvános viták,

00:15:51.160 --> 00:15:54.136
és ennek még csak az elején tartunk.

00:15:54.160 --> 00:15:57.616
Ezek az algoritmusok
csak Facebook like-okból

00:15:57.640 --> 00:16:00.896
képesek kikövetkeztetni olyan jellemzőket,

00:16:00.920 --> 00:16:03.616
mint az emberek etnikuma,
vallási és politikai nézetei,

00:16:03.616 --> 00:16:06.656
személyiségjegyei, intelligenciája,
boldogsága,

00:16:06.680 --> 00:16:09.816
függőséget okozó szerek használata,

00:16:09.840 --> 00:16:11.800
válása, életkora, neme.

00:16:12.130 --> 00:16:17.496
Ezek az algoritmusok képesek 
tüntetőket azonosítani

00:16:17.520 --> 00:16:20.280
akkor is, ha az arcuk
részben el van takarva.

00:16:21.720 --> 00:16:28.336
Ezek az algoritmusok képesek emberek
szexuális orientációját behatárolni,

00:16:28.360 --> 00:16:31.560
csak randiprofiljuk fotójából.

00:16:33.560 --> 00:16:36.176
Persze, ezek valószínűségi találgatások,

00:16:36.200 --> 00:16:39.096
ezért nem lesznek
100 százalékosan pontosak,

00:16:39.120 --> 00:16:44.016
de túl erősnek látom a technológia
felhasználására irányuló kísértést,

00:16:44.040 --> 00:16:46.216
és nem győz meg néhány hamis eredmény,

00:16:46.240 --> 00:16:49.496
ami persze teljesen új
problémahegyeket hoz létre.

00:16:49.520 --> 00:16:52.456
Képzeljük el, mit kezdhet egy állam

00:16:52.480 --> 00:16:56.040
ezzel a rengeteg adattal 
saját állampolgárairól.

00:16:56.680 --> 00:17:01.456
Kína már használja 
az arcfelismerés technológiáját,

00:17:01.480 --> 00:17:04.360
hogy azonosítson és lecsukjon embereket.

00:17:05.280 --> 00:17:07.416
És itt a tragédia:

00:17:07.440 --> 00:17:12.976
az önkényuralmi megfigyelés
infrastruktúráját építjük

00:17:13.000 --> 00:17:15.960
csak azért, hogy az emberek 
hirdetésekre kattintsanak.

00:17:17.240 --> 00:17:19.816
És ez nem Orwell önkényuralma lesz.

00:17:19.839 --> 00:17:21.736
Ez nem az "1984".

00:17:21.760 --> 00:17:26.336
Ha ez az önkényuralom nyílt fenyegetéssel
félemlít meg minket,

00:17:26.359 --> 00:17:29.256
mind rettegni fogunk,
de tisztában leszünk vele,

00:17:29.280 --> 00:17:31.480
gyűlölni fogjuk, és ellenállunk.

00:17:32.880 --> 00:17:37.296
De ha a hatalomban lévők
ezeket az algoritmusokat arra használják,

00:17:37.319 --> 00:17:40.696
hogy csendben megfigyeljenek,

00:17:40.720 --> 00:17:42.800
hogy ítélkezzenek,

00:17:43.720 --> 00:17:47.896
hogy előre lássák és azonosítsák
a bajkeverőket és a lázadókat,

00:17:47.920 --> 00:17:51.816
hogy a meggyőzés szerkezetét
széles körben alkalmazzák,

00:17:51.840 --> 00:17:55.976
hogy személyes, egyéni gyengeségeik
és sebezhetőségeik felhasználásával

00:17:56.000 --> 00:18:01.440
egyenként manipuláljanak egyéneket,

00:18:02.720 --> 00:18:04.920
és ha ezt nagy léptékben végzik

00:18:06.080 --> 00:18:07.816
privát képernyőinken

00:18:07.840 --> 00:18:09.496
úgy, hogy még azt sem tudjuk,

00:18:09.520 --> 00:18:12.280
mit lát a többi állampolgár 
és a szomszédaink,

00:18:13.560 --> 00:18:18.376
ez az önkényuralom behálóz minket,
mint egy pók hálója,

00:18:18.400 --> 00:18:20.880
és talán észre sem vesszük majd,
hogy benne vagyunk.

00:18:22.440 --> 00:18:25.376
A Facebook piaci értéke

00:18:25.400 --> 00:18:28.696
megközelíti a fél billió dollárt.

00:18:28.720 --> 00:18:31.840
Mert jól működik mint
meggyőzési technika.

00:18:33.760 --> 00:18:36.576
De ennek az architektúrának a szerkezete

00:18:36.600 --> 00:18:39.816
ugyanaz akkor is, ha cipőket adunk el,

00:18:39.840 --> 00:18:42.336
és akkor is, ha politikát adunk el.

00:18:42.360 --> 00:18:45.480
Az algoritmusok nem tesznek különbséget.

00:18:46.240 --> 00:18:49.536
Ugyanazok az algoritmusok, amelyek

00:18:49.560 --> 00:18:52.736
befolyásolhatóbbá tesznek minket 
a hirdetésekkel szemben,

00:18:52.760 --> 00:18:59.496
ugyanazok szervezik politikai személyes
és társadalmi információnk áramlását,

00:18:59.520 --> 00:19:01.360
és ezen változtatnunk kell.

00:19:02.240 --> 00:19:04.536
Kérem, ne értsenek félre,

00:19:04.560 --> 00:19:08.240
azért használjuk a digitális platformokat,
mert értékesek számunkra.

00:19:09.120 --> 00:19:12.680
Facebookozom, hogy tartsam a kapcsolatot
barátaimmal és családommal.

00:19:14.000 --> 00:19:19.776
Írtam arról, milyen fontos a közösségi
média társadalmi mozgalmainkhoz.

00:19:19.800 --> 00:19:22.816
Azt kutattam, hogyan használhatjuk
ezeket a technológiákat arra,

00:19:22.840 --> 00:19:25.320
hogy világszerte megkerüljük a cenzúrát.

00:19:27.280 --> 00:19:33.696
De nem azért, hogy azok, akik
a Facebookot vagy a Google-t irányítják,

00:19:33.720 --> 00:19:36.416
rosszindulatúan és szándékosan
megpróbálják

00:19:36.440 --> 00:19:40.896
egyre jobban megosztani
az országot vagy a világot,

00:19:40.920 --> 00:19:42.600
és ösztönözni a szélsőségeket.

00:19:43.440 --> 00:19:47.416
Olvastam a sok jó szándékú kijelentést

00:19:47.440 --> 00:19:50.760
ezektől az emberektől.

00:19:51.600 --> 00:19:57.656
De nem a szándék, és nem a technológián
dolgozók kijelentései számítanak,

00:19:57.680 --> 00:20:01.240
hanem azok a szerkezetek, üzleti modellek,
amiket építenek.

00:20:02.360 --> 00:20:04.456
És ez a probléma lényege.

00:20:04.480 --> 00:20:09.200
A Facebook vagy egy hatalmas, 
félbillió dolláros cég,

00:20:10.200 --> 00:20:12.096
és nem működnek hirdetések a felületén,

00:20:12.120 --> 00:20:14.816
nem meggyőzési architektúraként működik,

00:20:14.840 --> 00:20:18.960
vagy pedig befolyásoló hatalma
aggodalomra ad okot.

00:20:20.560 --> 00:20:22.336
Vagy az egyik, vagy a másik.

00:20:22.360 --> 00:20:23.960
Ugyanez igaz a Google-ra.

00:20:24.880 --> 00:20:27.336
Mit tehetünk tehát?

00:20:27.360 --> 00:20:29.296
Ez nem maradhat így.

00:20:29.320 --> 00:20:31.896
Sajnos, nem tudok
egyszerű receptet ajánlani,

00:20:31.920 --> 00:20:34.176
mert át kell alakítanunk

00:20:34.200 --> 00:20:37.216
a digitális technológia
egész működését.

00:20:37.240 --> 00:20:41.336
A technológia fejlesztésétől kezdve addig,

00:20:41.360 --> 00:20:45.216
ahol a gazdasági és egyéb ösztönzők

00:20:45.240 --> 00:20:47.520
beépülnek a rendszerbe.

00:20:48.480 --> 00:20:51.936
Szembe kell néznünk és foglalkoznunk kell

00:20:51.960 --> 00:20:56.616
a védett algoritmusok
átláthatóságának hiányával,

00:20:56.640 --> 00:21:00.456
a gépi tanulás homályosságának
szerkezeti kihívásaival,

00:21:00.480 --> 00:21:03.880
a rólunk válogatás nélkül 
gyűjtött összes adattal.

00:21:05.000 --> 00:21:07.520
Nagy feladat áll előttünk.

00:21:08.160 --> 00:21:10.840
Mozgósítanunk kell a technológiánkat,

00:21:11.760 --> 00:21:13.336
kreativitásunkat,

00:21:13.360 --> 00:21:15.240
és igen, a politikánkat,

00:21:16.240 --> 00:21:18.896
hogy olyan mesterséges 
intelligenciát építsünk,

00:21:18.920 --> 00:21:22.040
ami támogatja emberi céljainkat,

00:21:22.800 --> 00:21:26.720
de amit emberi értékeink korlátoznak is.

00:21:27.600 --> 00:21:29.760
Értem én, hogy ez nem lesz egyszerű.

00:21:30.360 --> 00:21:33.960
Talán abban sem értünk teljesen egyet,
hogy mit jelentenek ezek a feltételek.

00:21:34.920 --> 00:21:37.320
De ha komolyan vesszük,

00:21:38.240 --> 00:21:44.216
hogy ezek a rendszerek, amelyek
működésétől annyira függünk,

00:21:44.240 --> 00:21:48.360
nem látom be, hogy elhalaszthatnánk
ezt a megbeszélést.

00:21:49.200 --> 00:21:51.736
Ezek a szerkezetek

00:21:51.760 --> 00:21:55.856
megszervezik, hogyan működünk,

00:21:55.880 --> 00:21:58.176
és irányítják,

00:21:58.200 --> 00:22:00.816
mit tehetünk, és mit nem.

00:22:00.840 --> 00:22:03.296
Sok, reklámokkal finanszírozott platform

00:22:03.320 --> 00:22:04.896
büszkén hirdeti, hogy ingyenes.

00:22:04.920 --> 00:22:09.480
Ami ebben a környezetben azt jelenti,
hogy mi vagyunk az eladott termék.

00:22:10.840 --> 00:22:13.576
Olyan digitális gazdaságra van szükségünk,

00:22:13.600 --> 00:22:17.096
ahol az adataink és az érdeklődési körünk

00:22:17.120 --> 00:22:22.200
nem eladó a legnagyobb ajánlatot tevő
önkényúrnak vagy demagógnak.

00:22:23.160 --> 00:22:26.960
(Taps)

00:22:30.480 --> 00:22:33.736
Szóval, vissza a hollywoodi idézethez:

00:22:33.760 --> 00:22:37.496
a mesterséges intelligencia
és a digitális technológia virágzásának

00:22:37.520 --> 00:22:40.720
hatalmas lehetőségei tényleg kellenek,

00:22:41.400 --> 00:22:46.336
de ehhez szembe kell néznünk
az óriási fenyegetéssel,

00:22:46.360 --> 00:22:48.296
nyitott szemmel, és most azonnal.

00:22:48.320 --> 00:22:49.536
Köszönöm.

00:22:49.560 --> 00:22:54.200
(Taps)

