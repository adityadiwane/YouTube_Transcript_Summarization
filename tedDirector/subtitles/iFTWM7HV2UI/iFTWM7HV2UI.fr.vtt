WEBVTT
Kind: captions
Language: fr

00:00:00.000 --> 00:00:07.000
Traducteur: Morgane Quilfen
Relecteur: Claire Ghyselen

00:00:12.760 --> 00:00:16.296
Quand les gens expriment de la peur
envers l'intelligence artificielle,

00:00:16.320 --> 00:00:20.296
très souvent, ils évoquent des images
de robots humanoïdes qui se déchaînent.

00:00:20.320 --> 00:00:21.560
Vous voyez ? Terminator ?

00:00:22.400 --> 00:00:24.736
Ce pourrait être à considérer

00:00:24.760 --> 00:00:26.616
mais c'est une menace très lointaine.

00:00:26.640 --> 00:00:30.096
Ou nous nous tracassons
au sujet de la surveillance numérique

00:00:30.120 --> 00:00:31.896
avec des métaphores du passé.

00:00:31.920 --> 00:00:34.576
« 1984 » de George Orwell

00:00:34.600 --> 00:00:36.880
est à nouveau parmi les livres
les plus vendus.

00:00:37.960 --> 00:00:39.376
C'est un super livre,

00:00:39.400 --> 00:00:43.280
mais ce n'est pas la bonne dystopie
pour le 21e siècle.

00:00:44.080 --> 00:00:45.496
Ce que nous devons craindre

00:00:45.520 --> 00:00:50.296
n'est pas ce que
l'intelligence artificielle fera seule,

00:00:50.320 --> 00:00:55.056
mais comment les gens au pouvoir
utiliseront l'intelligence artificielle

00:00:55.080 --> 00:00:57.896
pour nous contrôler et nous manipuler

00:00:57.920 --> 00:01:01.056
de façons nouvelles, parfois cachées,

00:01:01.080 --> 00:01:04.096
subtiles et inattendues.

00:01:04.120 --> 00:01:05.976
La plupart des technologies

00:01:06.000 --> 00:01:10.336
qui menacent notre liberté
et notre dignité dans un avenir proche

00:01:10.360 --> 00:01:12.216
sont développées par des entreprises

00:01:12.240 --> 00:01:15.470
dans le domaine
de l'enregistrement et de la vente

00:01:15.470 --> 00:01:17.200
de nos données et notre attention

00:01:17.200 --> 00:01:19.456
à des publicitaires et autres :

00:01:19.480 --> 00:01:22.896
Facebook, Google, Amazon,

00:01:22.920 --> 00:01:24.800
Alibaba, Tencent.

00:01:26.040 --> 00:01:31.536
L'intelligence artificielle a également
commencé à stimuler leur marché.

00:01:31.560 --> 00:01:33.936
On pourrait croire
que l'intelligence artificielle

00:01:33.936 --> 00:01:36.536
sera la suite des publicités en ligne.

00:01:36.560 --> 00:01:37.776
Ce n'est pas le cas.

00:01:37.800 --> 00:01:40.256
C'est un saut dans la catégorie.

00:01:40.280 --> 00:01:42.856
C'est un monde complètement différent

00:01:42.880 --> 00:01:45.496
qui a un énorme potentiel.

00:01:45.520 --> 00:01:50.120
Ça pourrait accélérer
notre compréhension

00:01:50.120 --> 00:01:53.120
de nombreux domaines
d'étude et de recherche.

00:01:53.120 --> 00:01:56.616
Pour paraphraser un célèbre
philosophe hollywoodien :

00:01:56.640 --> 00:02:00.280
« Un grand pouvoir implique
de grandes responsabilités. »

00:02:01.120 --> 00:02:05.056
Considérons un fait fondamental
de nos vies numériques : les publicités.

00:02:05.080 --> 00:02:07.976
Nous les ignorons.

00:02:08.000 --> 00:02:09.976
Elles semblent grossières, inefficaces.

00:02:10.000 --> 00:02:14.240
Nous avons tous été suivis sur internet

00:02:14.240 --> 00:02:17.246
par une pub basée sur une chose
que nous avions cherchée ou lue.

00:02:17.246 --> 00:02:18.936
Vous cherchez un paire de bottes

00:02:18.960 --> 00:02:22.336
et pendant une semaine, ces bottes
vous suivent partout où vous allez.

00:02:22.360 --> 00:02:24.600
Même après avoir succombé
et les avoir achetées,

00:02:24.600 --> 00:02:26.040
elles vous suivent encore.

00:02:26.040 --> 00:02:29.096
Nous sommes un peu protégés
contre cette manipulation bon marché.

00:02:29.096 --> 00:02:32.480
Nous levons les yeux au ciel et pensons :
« Ça ne marche pas. »

00:02:33.720 --> 00:02:35.816
Mis à part qu'en ligne,

00:02:35.840 --> 00:02:39.440
les technologies numériques
ne sont pas que des publicités.

00:02:40.240 --> 00:02:43.400
Pour le comprendre, réfléchissons
à un exemple du monde physique.

00:02:43.840 --> 00:02:48.496
Vous savez comment à la caisse
au supermarché, près du caissier,

00:02:48.520 --> 00:02:52.000
il y a des bonbons et chewing-gums
à hauteur des yeux des enfants ?

00:02:52.800 --> 00:02:56.296
C'est conçu pour les faire pleurnicher
auprès de leurs parents

00:02:56.320 --> 00:02:59.400
alors que les parents
sont sur le point de payer.

00:03:00.040 --> 00:03:02.680
C'est une architecture de persuasion.

00:03:03.160 --> 00:03:06.256
Ce n'est pas sympa, mais ça marche.

00:03:06.280 --> 00:03:08.320
C'est pourquoi vous le voyez partout.

00:03:08.720 --> 00:03:10.416
Dans le monde physique,

00:03:10.440 --> 00:03:12.936
de telles architectures
de persuasion sont limitées

00:03:12.960 --> 00:03:17.776
car on ne peut mettre qu'un certain nombre
de choses près de la caisse.

00:03:17.800 --> 00:03:22.096
Les bonbons et chewing-gums
sont les mêmes pour tout le monde,

00:03:22.120 --> 00:03:23.596
même si ça fonctionne

00:03:23.600 --> 00:03:27.640
surtout pour les gens ayant des petits
êtres pleurnichant à leurs côtés.

00:03:29.160 --> 00:03:33.080
Dans le monde physique,
nous vivons avec ces limites.

00:03:34.280 --> 00:03:36.216
Dans le monde numérique cependant,

00:03:36.240 --> 00:03:38.840
les architectures de persuasion
peuvent être adaptées

00:03:38.840 --> 00:03:41.840
pour des milliards de personnes,

00:03:41.840 --> 00:03:45.696
elles peuvent cibler,
déduire, comprendre

00:03:45.720 --> 00:03:48.616
et être déployées pour des individus,

00:03:48.640 --> 00:03:49.856
un par un,

00:03:49.880 --> 00:03:52.016
en déterminant nos faiblesses

00:03:52.040 --> 00:03:57.656
et elles peuvent être envoyées
sur l'écran privé des téléphones

00:03:57.680 --> 00:03:59.936
afin que ce ne soit pas
visible à nos yeux.

00:03:59.960 --> 00:04:01.216
C'est différent.

00:04:01.240 --> 00:04:04.816
Ce n'est qu'une chose basique dont
est capable l'intelligence artificielle.

00:04:04.840 --> 00:04:06.176
Prenons un exemple.

00:04:06.200 --> 00:04:08.896
Vous voulez vendre
des billets d'avion pour Las Vegas.

00:04:08.920 --> 00:04:12.416
Dans l'ancien monde, vous pensiez
à des segments démographiques à cibler

00:04:12.440 --> 00:04:15.080
d'après votre expérience
et ce que vous pouvez imaginer.

00:04:15.560 --> 00:04:18.376
Vous pourriez faire de la publicité

00:04:18.400 --> 00:04:20.896
aux hommes ayant entre 25 et 35 ans

00:04:20.920 --> 00:04:24.856
ou aux gens qui ont une limite élevée
sur leur carte de crédit

00:04:24.880 --> 00:04:26.296
ou aux couples à la retraite.

00:04:26.296 --> 00:04:28.096
C'est ce que vous auriez fait.

00:04:28.120 --> 00:04:31.016
Avec le big data
et l'apprentissage des machines,

00:04:31.040 --> 00:04:32.564
ça ne marche plus comme ça.

00:04:33.320 --> 00:04:35.496
Pour l'imaginer,

00:04:35.520 --> 00:04:39.376
pensez à toutes les données
que Facebook a sur vous :

00:04:39.400 --> 00:04:41.936
tous les status tapés,

00:04:41.960 --> 00:04:43.976
toutes les conversations Messenger,

00:04:44.000 --> 00:04:45.880
tous vos lieux de connexion,

00:04:48.400 --> 00:04:51.576
toutes les photos
que vous avez téléchargées.

00:04:51.600 --> 00:04:55.376
Si vous tapez quelque chose,
changez d'avis et le supprimez,

00:04:55.400 --> 00:04:58.600
Facebook le garde et l'analyse également.

00:04:59.160 --> 00:05:03.096
De plus en plus, il essaye
de vous relier à vos données hors ligne.

00:05:03.120 --> 00:05:06.296
Il achète également des données
auprès de courtiers de données.

00:05:06.320 --> 00:05:09.736
Ce pourrait être n'importe quoi,
de vos documents financiers

00:05:09.760 --> 00:05:11.880
à votre historique de recherche.

00:05:12.360 --> 00:05:17.776
Aux Etats-Unis, de telles données
sont systématiquement collectées,

00:05:17.800 --> 00:05:19.760
comparées et vendues.

00:05:20.320 --> 00:05:22.760
En Europe, les règles sont plus strictes.

00:05:23.680 --> 00:05:25.880
Ce qu'il se passe alors c'est que,

00:05:26.920 --> 00:05:28.760
en fouillant toutes ces données,

00:05:28.760 --> 00:05:30.960
ces algorithmes d'apprentissage
des machines --

00:05:30.960 --> 00:05:34.026
ils sont appelés algorithmes
d'apprentissage pour cette raison --

00:05:34.026 --> 00:05:37.976
ils apprennent à comprendre
les caractéristiques des gens

00:05:38.000 --> 00:05:40.520
ayant déjà acheté
des billets pour Las Vegas.

00:05:41.760 --> 00:05:45.296
Quand ils l'apprennent
de données existantes,

00:05:45.320 --> 00:05:49.136
ils apprennent aussi comment
l'appliquer à de nouvelles personnes.

00:05:49.160 --> 00:05:52.216
Face à une nouvelle personne,

00:05:52.240 --> 00:05:53.550
ils peuvent classifier

00:05:53.550 --> 00:05:57.760
si cette personne a des chances
d'acheter un billet pour Las Vegas ou pas.

00:05:57.760 --> 00:06:03.176
Bien. Vous vous dites qu'une offre
pour acheter des billets pour Las Vegas,

00:06:03.200 --> 00:06:04.656
vous pouvez l'ignorer.

00:06:04.680 --> 00:06:06.896
Mais le problème n'est pas là.

00:06:06.920 --> 00:06:08.240
Le problème,

00:06:08.240 --> 00:06:10.490
c'est que nous ne comprenons plus vraiment

00:06:10.490 --> 00:06:12.680
comment fonctionnent
ces algorithmes complexes.

00:06:12.680 --> 00:06:16.136
Nous ne comprenons pas
comment ils font cette catégorisation.

00:06:16.160 --> 00:06:20.576
Ce sont d'énormes matrices,
des milliers de lignes et colonnes,

00:06:20.600 --> 00:06:22.560
peut-être même des millions,

00:06:23.320 --> 00:06:25.960
et ni les programmeurs,

00:06:26.760 --> 00:06:28.440
ni quiconque les regardant,

00:06:29.440 --> 00:06:30.936
même avec toutes les données,

00:06:30.960 --> 00:06:35.576
ne comprend plus
comment ça opère exactement,

00:06:35.600 --> 00:06:39.376
pas plus que vous ne sauriez
ce que je pense en ce moment

00:06:39.400 --> 00:06:43.360
si l'on vous montrait
une coupe transversale de mon cerveau.

00:06:44.360 --> 00:06:46.936
C'est comme si nous ne programmions plus,

00:06:46.960 --> 00:06:51.360
nous élevons une intelligence
que nous ne comprenons pas vraiment.

00:06:52.520 --> 00:06:56.496
Ces choses fonctionnent seulement
s'il y a un énorme volume de données,

00:06:56.520 --> 00:07:01.616
elles pourraient donc aussi encourager
une surveillance intensifiée à notre égard

00:07:01.640 --> 00:07:03.970
afin que les algorithmes
d'apprentissage marchent.

00:07:03.970 --> 00:07:07.296
C'est pour ça que Facebook veut
collecter toutes les données sur vous.

00:07:07.296 --> 00:07:08.776
Les algorithmes marchent mieux.

00:07:08.800 --> 00:07:11.496
Allons un peu plus loin
avec cet exemple de Las Vegas.

00:07:11.520 --> 00:07:15.200
Et si le système
que nous ne comprenons pas

00:07:16.200 --> 00:07:21.336
déterminait qu'il est plus simple
de vendre des billets pour Las Vegas

00:07:21.360 --> 00:07:25.120
aux gens bipolaires qui sont sur le point
d'entrer dans la phase maniaque.

00:07:25.640 --> 00:07:30.560
De telles personnes ont tendance
à dépenser et parier de façon compulsive.

00:07:31.280 --> 00:07:35.736
Ils pourraient le faire, vous ignoreriez
que c'était là leur conclusion.

00:07:35.760 --> 00:07:39.376
Une fois, j'ai donné cet exemple
à quelques informaticiens

00:07:39.400 --> 00:07:41.456
et après, l'un d'eux est venu me voir.

00:07:41.480 --> 00:07:45.000
Il était préoccupé : « C'est pour ça
que je n'ai pas pu le publier ».

00:07:45.600 --> 00:07:47.315
J'ai dit : « Publier quoi ? »

00:07:47.800 --> 00:07:53.656
Il avait essayé de voir s'il l'on pouvait
détecter le début d'une manie

00:07:53.680 --> 00:07:57.016
d'après les posts sur les réseaux sociaux
avant les symptômes cliniques

00:07:57.016 --> 00:07:58.696
et ça avait fonctionné,

00:07:58.720 --> 00:08:00.776
ça avait très bien fonctionné,

00:08:00.800 --> 00:08:05.680
et il n'avait aucune idée de comment
ça marchait ou ce que ça détectait.

00:08:06.840 --> 00:08:11.256
Le problème n'est pas résolu
s'il ne le publie pas

00:08:11.280 --> 00:08:13.176
car il y a déjà des entreprises

00:08:13.200 --> 00:08:15.736
qui développent ce genre de technologie

00:08:15.760 --> 00:08:18.560
et beaucoup de choses existent déjà.

00:08:19.240 --> 00:08:21.816
Ce n'est plus très compliqué.

00:08:21.840 --> 00:08:25.296
Vous arrive-t-il d'aller sur YouTube
pour regarder une vidéo

00:08:25.320 --> 00:08:27.680
et, une heure plus tard,
d'en avoir regardées 27 ?

00:08:28.760 --> 00:08:31.256
Vous voyez cette colonne
que YouTube a sur la droite,

00:08:31.280 --> 00:08:33.496
qui dit « A suivre »

00:08:33.520 --> 00:08:35.336
et qui se lance automatiquement ?

00:08:35.360 --> 00:08:36.576
C'est un algorithme

00:08:36.600 --> 00:08:40.216
qui choisit ce qu'il pense
qui pourrait vous intéresser

00:08:40.240 --> 00:08:41.906
et que vous ne trouveriez pas seul.

00:08:41.906 --> 00:08:43.056
Ce n'est pas un humain,

00:08:43.080 --> 00:08:44.496
ce sont des algorithmes.

00:08:44.520 --> 00:08:49.256
Il considère ce que vous avez regardé
et ce que d'autres comme vous ont regardé

00:08:49.280 --> 00:08:53.496
et il déduit que ce doit être
ce qui vous intéresse,

00:08:53.520 --> 00:08:54.935
ce dont vous voulez voir plus

00:08:54.935 --> 00:08:56.135
et vous en montre plus.

00:08:56.159 --> 00:08:58.360
Ça semble être une fonction
bénigne et utile,

00:08:59.280 --> 00:09:00.480
mais ça ne l'est pas.

00:09:01.640 --> 00:09:08.600
En 2016, j'ai été à des rassemblements
du candidat Donald Trump

00:09:09.840 --> 00:09:13.176
pour étudier en tant que chercheuse
le mouvement le soutenant.

00:09:13.200 --> 00:09:16.656
J'étudie les mouvements sociaux,
alors j'étudiais ça aussi.

00:09:16.680 --> 00:09:20.056
Puis j'ai voulu écrire quelque chose
au sujet d'un de ses rassemblements

00:09:20.056 --> 00:09:22.000
alors je l'ai regardé sur YouTube.

00:09:23.240 --> 00:09:26.336
YouTube a commencé à me recommander

00:09:26.360 --> 00:09:30.616
et à lancer automatiquement
des vidéos de suprématistes blancs

00:09:30.640 --> 00:09:33.296
étant de plus en plus extrémistes.

00:09:33.320 --> 00:09:35.136
Si j'en regardais une,

00:09:35.160 --> 00:09:38.136
il m'en recommandait une
encore plus extrême

00:09:38.160 --> 00:09:39.704
et la lançais automatiquement.

00:09:40.320 --> 00:09:44.856
Si vous regardez du contenu
d'Hillary Clinton ou Bernie Sanders,

00:09:44.880 --> 00:09:49.576
YouTube recommande et lance
des vidéos gauchistes de conspiration

00:09:49.600 --> 00:09:51.360
et plus ça va, plus ça empire.

00:09:52.480 --> 00:09:55.536
Vous pensez peut-être
que c'est de la politique, mais non.

00:09:55.560 --> 00:09:57.006
Il ne s'agit pas de politique.

00:09:57.006 --> 00:09:59.936
Ce n'est qu'un algorithme
déterminant le comportement humain.

00:09:59.960 --> 00:10:04.736
Une fois, j'ai regardé une vidéo
sur le végétarisme sur YouTube

00:10:04.760 --> 00:10:09.696
et YouTube a recommandé et lancé
une vidéo sur le fait d'être végétalien.

00:10:09.720 --> 00:10:12.736
Vous n'êtes jamais
assez extrême pour YouTube.

00:10:12.760 --> 00:10:14.336
(Rires)

00:10:14.360 --> 00:10:15.920
Que se passe-t-il ?

00:10:16.520 --> 00:10:20.056
L'algorithme de YouTube est propriétaire,

00:10:20.080 --> 00:10:22.440
mais voici ce qui, à mon avis, se passe.

00:10:23.360 --> 00:10:25.456
L'algorithme a déterminé

00:10:25.480 --> 00:10:29.176
que si vous pouvez pousser les gens

00:10:29.200 --> 00:10:32.936
à penser que vous pouvez leur montrer
quelque chose de plus extrême,

00:10:32.960 --> 00:10:35.376
ils ont plus de chances
de rester sur le site

00:10:35.400 --> 00:10:39.816
à regarder vidéo sur vidéo,
descendant dans le terrier du lapin

00:10:39.840 --> 00:10:41.750
pendant que Google leur sert des pubs.

00:10:43.760 --> 00:10:46.880
Puisque personne ne fait attention
à l'éthique du magasin,

00:10:47.720 --> 00:10:51.960
ces sites peuvent profiler des gens

00:10:53.680 --> 00:10:55.600
comme haïssant les juifs,

00:10:56.360 --> 00:10:58.840
pensant que les juifs sont des parasites,

00:11:00.320 --> 00:11:05.240
qui ont du contenu
anti-sémite très explicite,

00:11:06.080 --> 00:11:08.080
et vous laisser les cibler avec des pubs.

00:11:09.200 --> 00:11:12.736
Ils peuvent mobiliser les algorithmes

00:11:12.760 --> 00:11:15.896
pour trouver des publics similaires,

00:11:15.920 --> 00:11:21.496
des gens n'ayant pas de contenu
anti-sémite si explicite sur leur profil

00:11:21.520 --> 00:11:27.696
mais que les algorithmes détectent
comme étant sensibles à de tels messages,

00:11:27.720 --> 00:11:29.810
et vous laisser les cibler avec des pubs.

00:11:30.680 --> 00:11:33.416
Ça peut sembler être
un exemple peu plausible,

00:11:33.440 --> 00:11:34.760
mais c'est vrai.

00:11:35.480 --> 00:11:37.616
ProPublica a enquêté sur ça

00:11:37.640 --> 00:11:41.256
et a découvert que vous pouviez
vraiment le faire sur Facebook

00:11:41.280 --> 00:11:43.696
et Facebook, serviable,
offrait des suggestions

00:11:43.720 --> 00:11:45.320
sur comment étendre ce public.

00:11:46.720 --> 00:11:49.736
BuzzFeed l'a essayé pour Google
et a rapidement découvert

00:11:49.760 --> 00:11:51.496
que vous pouvez le faire sur Google.

00:11:51.520 --> 00:11:53.216
Ce n'était même pas cher.

00:11:53.240 --> 00:11:57.656
Le reporter de ProPublica
a dépensé environ 30 dollars

00:11:57.680 --> 00:11:59.920
pour cibler cette catégorie.

00:12:02.600 --> 00:12:06.960
L'année dernière, le responsable
des réseaux sociaux de Donald Trump

00:12:06.960 --> 00:12:13.256
a révélé utiliser les dark posts
de Facebook pour démobiliser les gens,

00:12:13.280 --> 00:12:14.656
pas pour les persuader,

00:12:14.680 --> 00:12:17.480
mais pour les convaincre
de ne pas voter du tout.

00:12:18.520 --> 00:12:22.096
Pour ce faire,
ils ont spécifiquement ciblé,

00:12:22.120 --> 00:12:26.016
par exemple, les hommes afro-américains
dans des villes clés comme Philadelphie

00:12:26.040 --> 00:12:28.496
et je vais lire exactement ce qu'il a dit.

00:12:28.520 --> 00:12:29.736
Je cite.

00:12:29.760 --> 00:12:32.776
Ils utilisaient « des posts non publics

00:12:32.800 --> 00:12:34.976
dont la campagne contrôle l'audience

00:12:35.000 --> 00:12:38.776
afin que seuls les voient les gens
dont nous voulons qu'ils les voient.

00:12:38.800 --> 00:12:40.016
Nous l'avons façonné.

00:12:40.040 --> 00:12:44.760
Ça affectera considérablement
sa capacité à retourner ces gens. »

00:12:45.720 --> 00:12:48.000
Qu'y a-t-il dans ces dark posts ?

00:12:48.480 --> 00:12:50.136
Nous l'ignorons.

00:12:50.160 --> 00:12:51.460
Facebook refuse de le dire.

00:12:52.480 --> 00:12:56.856
Facebook arrange également
algorithmiquement les posts

00:12:56.880 --> 00:13:00.616
que vos amis mettent sur Facebook
ou des pages que vous suivez.

00:13:00.640 --> 00:13:02.856
Il ne vous montre pas tout
chronologiquement.

00:13:02.880 --> 00:13:07.696
Il le met dans l'ordre qui,
selon l'algorithme, devrait vous pousser

00:13:07.720 --> 00:13:09.560
à rester plus longtemps sur le site.

00:13:11.040 --> 00:13:14.416
Ça a de nombreuses conséquences.

00:13:14.440 --> 00:13:18.240
Vous pensez peut-être que quelqu'un
vous ignore sur Facebook.

00:13:18.800 --> 00:13:22.056
L'algorithme ne lui montre
peut-être jamais vos posts.

00:13:22.080 --> 00:13:28.040
L'algorithme en priorise certains
et en enterre d'autres.

00:13:29.320 --> 00:13:30.616
Des expériences montrent

00:13:30.640 --> 00:13:33.600
que ce que l'algorithme
choisit de vous montrer

00:13:33.600 --> 00:13:36.600
peut influencer vos émotions.

00:13:36.600 --> 00:13:37.800
Mais ce n'est pas tout.

00:13:38.280 --> 00:13:40.640
Ça influence aussi
votre comportement politique.

00:13:41.360 --> 00:13:46.016
En 2010, aux élections de mi-mandat,

00:13:46.040 --> 00:13:51.936
Facebook a conduit une expérience
sur 61 millions de personnes américaines

00:13:51.960 --> 00:13:53.856
qui a été révélée après les faits.

00:13:53.880 --> 00:13:57.296
Certaines personnes ont vu
« Aujourd'hui, c'est jour d'élection »,

00:13:57.320 --> 00:13:58.696
le plus simple,

00:13:58.720 --> 00:14:02.616
et certaines personnes ont vu
celui avec ce petit ajustement

00:14:02.640 --> 00:14:04.736
avec les petites photos

00:14:04.760 --> 00:14:07.600
de vos amis ayant cliqué
sur « j'ai voté ».

00:14:09.000 --> 00:14:10.400
Ce simple ajustement.

00:14:11.520 --> 00:14:15.816
D'accord ? Les photos étaient
la seule modification

00:14:15.840 --> 00:14:19.096
et ce post montré qu'une seule fois

00:14:19.120 --> 00:14:25.176
a attiré 340 000 électeurs supplémentaires

00:14:25.200 --> 00:14:26.896
dans cette élection,

00:14:26.920 --> 00:14:28.616
d'après cette recherche

00:14:28.640 --> 00:14:31.160
et la liste des électeurs l'a confirmé.

00:14:32.920 --> 00:14:34.576
Un hasard ? Non.

00:14:34.600 --> 00:14:39.960
Parce qu'en 2012, ils ont reconduit
cette même expérience.

00:14:40.840 --> 00:14:42.576
A cette époque-là,

00:14:42.600 --> 00:14:45.896
ce message civique montré une seule fois

00:14:45.920 --> 00:14:50.360
a attiré 270 000 électeurs
supplémentaires.

00:14:51.160 --> 00:14:56.376
En guise de référence,
l'élection présidentielle de 2016

00:14:56.400 --> 00:14:59.920
s'est jouée à environ 100 000 votes.

00:15:01.360 --> 00:15:06.096
Facebook peut aussi très facilement
déduire vos opinions politiques,

00:15:06.120 --> 00:15:08.456
même si vous ne les avez pas
révélées sur le site.

00:15:08.456 --> 00:15:10.920
Ces algorithmes peuvent
le faire assez facilement.

00:15:11.960 --> 00:15:15.856
Et si une plateforme avec un tel pouvoir

00:15:15.880 --> 00:15:20.920
décidait de retourner les partisans
d'un candidat pour l'autre ?

00:15:21.680 --> 00:15:24.120
Comment le saurions-nous ?

00:15:25.560 --> 00:15:29.696
Nous sommes partis de quelque chose
semble-t-il inoffensif --

00:15:29.720 --> 00:15:31.936
les pubs nous suivant partout en ligne --

00:15:31.960 --> 00:15:33.800
et sommes arrivés ailleurs.

00:15:35.480 --> 00:15:37.936
En tant que public et citoyens,

00:15:37.960 --> 00:15:41.376
nous ne savons plus
si nous voyons les mêmes informations

00:15:41.400 --> 00:15:42.880
ou ce que voient les autres

00:15:43.680 --> 00:15:46.256
et sans base d'informations commune,

00:15:46.280 --> 00:15:47.896
peu à peu,

00:15:47.920 --> 00:15:51.136
le débat public devient impossible

00:15:51.160 --> 00:15:54.136
et nous n'en sommes
qu'aux premières phases.

00:15:54.160 --> 00:15:57.616
Ces algorithmes peuvent facilement déduire

00:15:57.640 --> 00:16:01.936
des choses comme votre ethnie,
vos opinions religieuses, politiques,

00:16:01.936 --> 00:16:03.256
vos traits de personnalité,

00:16:03.280 --> 00:16:06.656
votre intelligence, votre bonheur,
votre usage de substances addictives,

00:16:06.680 --> 00:16:09.816
la situation maritale de vos parents,
votre âge et votre sexe,

00:16:09.840 --> 00:16:11.920
uniquement grâce
aux « J'aime » de Facebook.

00:16:13.440 --> 00:16:17.496
Ces algorithmes peuvent
identifier des manifestants

00:16:17.520 --> 00:16:20.280
même si leur visage
est partiellement caché.

00:16:21.720 --> 00:16:28.336
Ces algorithmes peuvent peut-être
détecter l'orientation sexuelle des gens

00:16:28.360 --> 00:16:31.560
simplement grâce à leur photo de profil
sur un site de rencontres.

00:16:33.560 --> 00:16:36.176
Ce sont des conjectures statistiques,

00:16:36.200 --> 00:16:39.096
ils n'auront pas raison à 100%,

00:16:39.120 --> 00:16:44.016
mais je ne vois pas les puissants résister
à la tentation d'utiliser ces technologies

00:16:44.040 --> 00:16:46.216
simplement parce qu'il y a
des faux positifs,

00:16:46.240 --> 00:16:49.496
ce qui, bien sûr, créera
un couche supplémentaire de problèmes.

00:16:49.520 --> 00:16:52.456
Imaginez ce qu'un Etat peut faire

00:16:52.480 --> 00:16:56.040
avec l'énorme volume de données
qu'il a sur ces citoyens.

00:16:56.680 --> 00:17:01.456
La Chine utilise déjà
une technologie de détection du visage

00:17:01.480 --> 00:17:04.360
pour identifier et arrêter des gens.

00:17:05.280 --> 00:17:07.416
Voici la tragédie :

00:17:07.440 --> 00:17:12.976
nous instaurons cette infrastructure
de surveillance d’autoritarisme

00:17:13.000 --> 00:17:15.960
simplement pour que les gens
cliquent sur des pubs.

00:17:17.240 --> 00:17:19.816
Ce ne sera pas l’autoritarisme d'Orwell.

00:17:19.839 --> 00:17:21.736
Ce n'est pas « 1984 ».

00:17:21.760 --> 00:17:26.336
Si l'autoritarisme une peur déclarée
pour nous terroriser,

00:17:26.359 --> 00:17:29.256
nous aurons tous peur
mais nous le saurons,

00:17:29.280 --> 00:17:31.480
nous détesterons ça et résisterons.

00:17:32.880 --> 00:17:37.296
Mais si les gens au pouvoir
utilisent ces algorithmes

00:17:37.319 --> 00:17:40.696
pour nous surveiller discrètement,

00:17:40.720 --> 00:17:42.800
pour nous juger et nous inciter,

00:17:43.720 --> 00:17:47.896
pour prédire et identifier
les fauteurs de trouble et les rebelles,

00:17:47.920 --> 00:17:51.816
pour déployer une architecture
de persuasion à grande échelle

00:17:51.840 --> 00:17:55.976
et pour manipuler les individus un par un

00:17:56.000 --> 00:18:01.440
grâce à leurs faiblesses et vulnérabilités
personnelles et individuelles

00:18:02.720 --> 00:18:04.920
et s'ils le font à grande échelle

00:18:06.080 --> 00:18:07.816
via nos écrans privés

00:18:07.840 --> 00:18:09.496
afin que nous ne sachions même pas

00:18:09.520 --> 00:18:12.280
ce que les autres citoyens
et nos voisins voient,

00:18:13.560 --> 00:18:18.376
l'autoritarisme nous enveloppera
tel une toile d'araignée

00:18:18.400 --> 00:18:20.880
et nous ignorons même
que nous sommes dans la toile.

00:18:22.440 --> 00:18:25.376
La capitalisation du marché de Facebook

00:18:25.400 --> 00:18:28.696
approche le demi milliard de dollars.

00:18:28.720 --> 00:18:31.840
C'est parce que c'est une très bonne
architecture de persuasion.

00:18:33.760 --> 00:18:36.576
Mais la structure de cette architecture

00:18:36.600 --> 00:18:39.816
est la même que vous vendiez
des chaussures

00:18:39.840 --> 00:18:42.336
ou de la politique.

00:18:42.360 --> 00:18:45.480
Les algorithmes
ne connaissent pas la différence.

00:18:46.240 --> 00:18:49.536
Les mêmes algorithmes utilisés sur nous

00:18:49.560 --> 00:18:52.736
pour nous rendre plus malléables
face aux publicités

00:18:52.760 --> 00:18:59.496
organisent aussi nos flux d'informations
politiques, personnelles et sociales

00:18:59.520 --> 00:19:01.360
et ça doit changer.

00:19:02.240 --> 00:19:04.536
Ne vous méprenez pas,

00:19:04.560 --> 00:19:08.240
nous utilisons les plateformes numériques
car elles ont beaucoup de valeur.

00:19:09.120 --> 00:19:11.000
J'utilise Facebook pour garder contact

00:19:11.000 --> 00:19:14.000
avec des amis et de la famille
à travers le monde.

00:19:14.000 --> 00:19:19.776
J'ai écrit sur l'importance des réseaux
sociaux pour les mouvements sociaux.

00:19:19.800 --> 00:19:22.816
J'ai étudié comment ces technologies
peuvent être utilisées

00:19:22.840 --> 00:19:25.320
pour contourner la censure
à travers le monde.

00:19:27.280 --> 00:19:33.696
Ce n'est pas que les dirigeants
de Facebook ou Google

00:19:33.720 --> 00:19:36.416
essayent perfidement et délibérément

00:19:36.440 --> 00:19:40.896
de polariser plus le pays ou le monde

00:19:40.920 --> 00:19:42.600
et d'encourager l'extrémisme.

00:19:43.440 --> 00:19:47.416
J'ai lu les nombreuses déclarations
bien intentionnées

00:19:47.440 --> 00:19:50.760
que ces gens ont publiées.

00:19:51.600 --> 00:19:54.680
Mais ce n'est pas l'intention
ou les déclarations

00:19:54.680 --> 00:19:57.680
des gens dans les technologies
qui comptent,

00:19:57.680 --> 00:20:01.240
ce sont les structures
et les modèles commerciaux qu'ils créent.

00:20:02.360 --> 00:20:04.456
C'est le cœur du problème.

00:20:04.480 --> 00:20:09.200
Soit Facebook est un escroc
d'un demi milliard de dollars

00:20:10.200 --> 00:20:12.106
et les pubs ne marchent pas sur le site,

00:20:12.120 --> 00:20:14.816
ça ne marche pas
comme une architecture de persuasion

00:20:14.840 --> 00:20:18.960
ou son pouvoir d'influence
est très préoccupant.

00:20:20.560 --> 00:20:22.336
C'est l'un ou l'autre.

00:20:22.360 --> 00:20:23.960
Il en va de même pour Google.

00:20:24.880 --> 00:20:27.336
Que pouvons-nous faire ?

00:20:27.360 --> 00:20:29.296
Ça doit changer.

00:20:29.320 --> 00:20:31.896
Je ne peux pas offrir de recette simple

00:20:31.920 --> 00:20:34.176
car nous devons restructurer

00:20:34.200 --> 00:20:37.216
tout le fonctionnement
de notre technologie numérique.

00:20:37.240 --> 00:20:41.336
Tout, de la façon dont
la technologie est développée

00:20:41.360 --> 00:20:45.216
à la façon dont les incitations,
économiques et autres,

00:20:45.240 --> 00:20:47.520
sont intégrées au système.

00:20:48.480 --> 00:20:51.936
Nous devons faire face et gérer

00:20:51.960 --> 00:20:56.616
le manque de transparence créé
par les algorithmes propriétaires,

00:20:56.640 --> 00:21:00.456
le défi structurel de l'opacité
de l'apprentissage des machines,

00:21:00.480 --> 00:21:03.880
toutes ces données
aveuglément collectées sur nous.

00:21:05.000 --> 00:21:07.520
Nous avons devant nous
une tâche importante.

00:21:08.160 --> 00:21:10.840
Nous devons mobiliser notre technologie,

00:21:11.760 --> 00:21:13.336
notre créativité

00:21:13.360 --> 00:21:15.240
et oui, notre science politique

00:21:16.240 --> 00:21:18.896
pour créer une intelligence artificielle

00:21:18.920 --> 00:21:22.040
qui soutient nos objectifs humains

00:21:22.800 --> 00:21:26.720
mais qui est aussi contrainte
par nos valeurs humaines.

00:21:27.600 --> 00:21:29.760
Je comprends que ce ne sera pas simple.

00:21:30.360 --> 00:21:33.960
Nous ne serons pas facilement d'accord
sur le sens de ces termes.

00:21:34.920 --> 00:21:37.320
Mais si nous prenons au sérieux

00:21:38.240 --> 00:21:44.216
le fonctionnement de ces systèmes
dont nous sommes si dépendants,

00:21:44.240 --> 00:21:48.360
je ne vois pas comment nous pouvons
encore retarder cette conversation.

00:21:49.200 --> 00:21:54.726
Ces structures organisent
notre fonctionnement

00:21:55.880 --> 00:22:00.266
et elles contrôlent
ce que nous pouvons faire ou non.

00:22:00.840 --> 00:22:03.296
Beaucoup de ces plateformes
financées par les pubs

00:22:03.320 --> 00:22:04.896
se proclament gratuites.

00:22:04.920 --> 00:22:09.480
Dans ce contexte, ça signifie
que nous sommes le produit qui est vendu.

00:22:10.840 --> 00:22:13.576
Nous avons besoin
d'une économie numérique

00:22:13.600 --> 00:22:17.096
où nos données et notre attention

00:22:17.120 --> 00:22:22.200
ne sont pas à vendre à l'autoritariste
ou au démagogue le plus offrant.

00:22:23.160 --> 00:22:26.960
(Applaudissements)

00:22:30.480 --> 00:22:33.736
Pour revenir à cette paraphrase
hollywoodienne,

00:22:33.760 --> 00:22:37.496
nous voulons que le grand pouvoir

00:22:37.520 --> 00:22:40.960
de l'intelligence artificielle
et de la technologie numérique fleurisse

00:22:41.400 --> 00:22:46.336
mais pour ça , nous devons
faire face à une grande menace,

00:22:46.360 --> 00:22:48.296
les yeux ouverts et maintenant.

00:22:48.320 --> 00:22:49.536
Merci.

00:22:49.560 --> 00:22:54.200
(Applaudissements)

