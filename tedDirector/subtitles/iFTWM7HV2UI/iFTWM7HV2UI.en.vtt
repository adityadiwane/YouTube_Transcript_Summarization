WEBVTT
Kind: captions
Language: en

00:00:12.760 --> 00:00:16.296
So when people voice fears
of artificial intelligence,

00:00:16.320 --> 00:00:20.296
very often, they invoke images
of humanoid robots run amok.

00:00:20.320 --> 00:00:21.560
You know? Terminator?

00:00:22.400 --> 00:00:24.736
You know, that might be
something to consider,

00:00:24.760 --> 00:00:26.616
but that's a distant threat.

00:00:26.640 --> 00:00:30.096
Or, we fret about digital surveillance

00:00:30.120 --> 00:00:31.896
with metaphors from the past.

00:00:31.920 --> 00:00:34.576
"1984," George Orwell's "1984,"

00:00:34.600 --> 00:00:36.880
it's hitting the bestseller lists again.

00:00:37.960 --> 00:00:39.376
It's a great book,

00:00:39.400 --> 00:00:43.280
but it's not the correct dystopia
for the 21st century.

00:00:44.080 --> 00:00:45.496
What we need to fear most

00:00:45.520 --> 00:00:50.296
is not what artificial intelligence
will do to us on its own,

00:00:50.320 --> 00:00:55.056
but how the people in power
will use artificial intelligence

00:00:55.080 --> 00:00:57.896
to control us and to manipulate us

00:00:57.920 --> 00:01:01.056
in novel, sometimes hidden,

00:01:01.080 --> 00:01:04.096
subtle and unexpected ways.

00:01:04.120 --> 00:01:05.976
Much of the technology

00:01:06.000 --> 00:01:10.336
that threatens our freedom
and our dignity in the near-term future

00:01:10.360 --> 00:01:12.216
is being developed by companies

00:01:12.240 --> 00:01:17.176
in the business of capturing
and selling our data and our attention

00:01:17.200 --> 00:01:19.456
to advertisers and others:

00:01:19.480 --> 00:01:22.896
Facebook, Google, Amazon,

00:01:22.920 --> 00:01:24.800
Alibaba, Tencent.

00:01:26.040 --> 00:01:31.536
Now, artificial intelligence has started
bolstering their business as well.

00:01:31.560 --> 00:01:33.656
And it may seem
like artificial intelligence

00:01:33.680 --> 00:01:36.536
is just the next thing after online ads.

00:01:36.560 --> 00:01:37.776
It's not.

00:01:37.800 --> 00:01:40.256
It's a jump in category.

00:01:40.280 --> 00:01:42.856
It's a whole different world,

00:01:42.880 --> 00:01:45.496
and it has great potential.

00:01:45.520 --> 00:01:52.440
It could accelerate our understanding
of many areas of study and research.

00:01:53.120 --> 00:01:56.616
But to paraphrase
a famous Hollywood philosopher,

00:01:56.640 --> 00:02:00.280
"With prodigious potential
comes prodigious risk."

00:02:01.120 --> 00:02:05.056
Now let's look at a basic fact
of our digital lives, online ads.

00:02:05.080 --> 00:02:07.976
Right? We kind of dismiss them.

00:02:08.000 --> 00:02:09.976
They seem crude, ineffective.

00:02:10.000 --> 00:02:14.256
We've all had the experience
of being followed on the web

00:02:14.280 --> 00:02:17.056
by an ad based on something
we searched or read.

00:02:17.080 --> 00:02:18.936
You know, you look up a pair of boots

00:02:18.960 --> 00:02:22.336
and for a week, those boots are following
you around everywhere you go.

00:02:22.360 --> 00:02:26.016
Even after you succumb and buy them,
they're still following you around.

00:02:26.040 --> 00:02:29.056
We're kind of inured to that kind
of basic, cheap manipulation.

00:02:29.080 --> 00:02:32.480
We roll our eyes and we think,
"You know what? These things don't work."

00:02:33.720 --> 00:02:35.816
Except, online,

00:02:35.840 --> 00:02:39.440
the digital technologies are not just ads.

00:02:40.240 --> 00:02:43.360
Now, to understand that,
let's think of a physical world example.

00:02:43.840 --> 00:02:48.496
You know how, at the checkout counters
at supermarkets, near the cashier,

00:02:48.520 --> 00:02:52.000
there's candy and gum
at the eye level of kids?

00:02:52.800 --> 00:02:56.296
That's designed to make them
whine at their parents

00:02:56.320 --> 00:02:59.400
just as the parents
are about to sort of check out.

00:03:00.040 --> 00:03:02.680
Now, that's a persuasion architecture.

00:03:03.160 --> 00:03:06.256
It's not nice, but it kind of works.

00:03:06.280 --> 00:03:08.320
That's why you see it
in every supermarket.

00:03:08.720 --> 00:03:10.416
Now, in the physical world,

00:03:10.440 --> 00:03:12.936
such persuasion architectures
are kind of limited,

00:03:12.960 --> 00:03:17.776
because you can only put
so many things by the cashier. Right?

00:03:17.800 --> 00:03:22.096
And the candy and gum,
it's the same for everyone,

00:03:22.120 --> 00:03:23.576
even though it mostly works

00:03:23.600 --> 00:03:27.640
only for people who have
whiny little humans beside them.

00:03:29.160 --> 00:03:33.080
In the physical world,
we live with those limitations.

00:03:34.280 --> 00:03:36.216
In the digital world, though,

00:03:36.240 --> 00:03:40.560
persuasion architectures
can be built at the scale of billions

00:03:41.840 --> 00:03:45.696
and they can target, infer, understand

00:03:45.720 --> 00:03:48.616
and be deployed at individuals

00:03:48.640 --> 00:03:49.856
one by one

00:03:49.880 --> 00:03:52.016
by figuring out your weaknesses,

00:03:52.040 --> 00:03:57.656
and they can be sent
to everyone's phone private screen,

00:03:57.680 --> 00:03:59.936
so it's not visible to us.

00:03:59.960 --> 00:04:01.216
And that's different.

00:04:01.240 --> 00:04:04.816
And that's just one of the basic things
that artificial intelligence can do.

00:04:04.840 --> 00:04:06.176
Now, let's take an example.

00:04:06.200 --> 00:04:08.896
Let's say you want to sell
plane tickets to Vegas. Right?

00:04:08.920 --> 00:04:12.416
So in the old world, you could think
of some demographics to target

00:04:12.440 --> 00:04:14.960
based on experience
and what you can guess.

00:04:15.560 --> 00:04:18.376
You might try to advertise to, oh,

00:04:18.400 --> 00:04:20.896
men between the ages of 25 and 35,

00:04:20.920 --> 00:04:24.856
or people who have
a high limit on their credit card,

00:04:24.880 --> 00:04:26.256
or retired couples. Right?

00:04:26.280 --> 00:04:28.096
That's what you would do in the past.

00:04:28.120 --> 00:04:31.016
With big data and machine learning,

00:04:31.040 --> 00:04:32.564
that's not how it works anymore.

00:04:33.320 --> 00:04:35.496
So to imagine that,

00:04:35.520 --> 00:04:39.376
think of all the data
that Facebook has on you:

00:04:39.400 --> 00:04:41.936
every status update you ever typed,

00:04:41.960 --> 00:04:43.976
every Messenger conversation,

00:04:44.000 --> 00:04:45.880
every place you logged in from,

00:04:48.400 --> 00:04:51.576
all your photographs
that you uploaded there.

00:04:51.600 --> 00:04:55.376
If you start typing something
and change your mind and delete it,

00:04:55.400 --> 00:04:58.600
Facebook keeps those
and analyzes them, too.

00:04:59.160 --> 00:05:03.096
Increasingly, it tries
to match you with your offline data.

00:05:03.120 --> 00:05:06.296
It also purchases
a lot of data from data brokers.

00:05:06.320 --> 00:05:09.736
It could be everything
from your financial records

00:05:09.760 --> 00:05:11.880
to a good chunk of your browsing history.

00:05:12.360 --> 00:05:17.776
Right? In the US,
such data is routinely collected,

00:05:17.800 --> 00:05:19.760
collated and sold.

00:05:20.320 --> 00:05:22.760
In Europe, they have tougher rules.

00:05:23.680 --> 00:05:25.880
So what happens then is,

00:05:26.920 --> 00:05:30.936
by churning through all that data,
these machine-learning algorithms --

00:05:30.960 --> 00:05:33.856
that's why they're called
learning algorithms --

00:05:33.880 --> 00:05:37.976
they learn to understand
the characteristics of people

00:05:38.000 --> 00:05:40.520
who purchased tickets to Vegas before.

00:05:41.760 --> 00:05:45.296
When they learn this from existing data,

00:05:45.320 --> 00:05:49.136
they also learn
how to apply this to new people.

00:05:49.160 --> 00:05:52.216
So if they're presented with a new person,

00:05:52.240 --> 00:05:56.880
they can classify whether that person
is likely to buy a ticket to Vegas or not.

00:05:57.720 --> 00:06:03.176
Fine. You're thinking,
an offer to buy tickets to Vegas.

00:06:03.200 --> 00:06:04.656
I can ignore that.

00:06:04.680 --> 00:06:06.896
But the problem isn't that.

00:06:06.920 --> 00:06:08.496
The problem is,

00:06:08.520 --> 00:06:12.656
we no longer really understand
how these complex algorithms work.

00:06:12.680 --> 00:06:16.136
We don't understand
how they're doing this categorization.

00:06:16.160 --> 00:06:20.576
It's giant matrices,
thousands of rows and columns,

00:06:20.600 --> 00:06:22.560
maybe millions of rows and columns,

00:06:23.320 --> 00:06:25.960
and not the programmers

00:06:26.760 --> 00:06:28.440
and not anybody who looks at it,

00:06:29.440 --> 00:06:30.936
even if you have all the data,

00:06:30.960 --> 00:06:35.576
understands anymore
how exactly it's operating

00:06:35.600 --> 00:06:39.376
any more than you'd know
what I was thinking right now

00:06:39.400 --> 00:06:43.360
if you were shown
a cross section of my brain.

00:06:44.360 --> 00:06:46.936
It's like we're not programming anymore,

00:06:46.960 --> 00:06:51.360
we're growing intelligence
that we don't truly understand.

00:06:52.520 --> 00:06:56.496
And these things only work
if there's an enormous amount of data,

00:06:56.520 --> 00:07:01.616
so they also encourage
deep surveillance on all of us

00:07:01.640 --> 00:07:03.976
so that the machine learning
algorithms can work.

00:07:04.000 --> 00:07:07.176
That's why Facebook wants
to collect all the data it can about you.

00:07:07.200 --> 00:07:08.776
The algorithms work better.

00:07:08.800 --> 00:07:11.496
So let's push that Vegas example a bit.

00:07:11.520 --> 00:07:15.200
What if the system
that we do not understand

00:07:16.200 --> 00:07:21.336
was picking up that it's easier
to sell Vegas tickets

00:07:21.360 --> 00:07:25.120
to people who are bipolar
and about to enter the manic phase.

00:07:25.640 --> 00:07:30.560
Such people tend to become
overspenders, compulsive gamblers.

00:07:31.280 --> 00:07:35.736
They could do this, and you'd have no clue
that's what they were picking up on.

00:07:35.760 --> 00:07:39.376
I gave this example
to a bunch of computer scientists once

00:07:39.400 --> 00:07:41.456
and afterwards, one of them came up to me.

00:07:41.480 --> 00:07:45.000
He was troubled and he said,
"That's why I couldn't publish it."

00:07:45.600 --> 00:07:47.315
I was like, "Couldn't publish what?"

00:07:47.800 --> 00:07:53.656
He had tried to see whether you can indeed
figure out the onset of mania

00:07:53.680 --> 00:07:56.896
from social media posts
before clinical symptoms,

00:07:56.920 --> 00:07:58.696
and it had worked,

00:07:58.720 --> 00:08:00.776
and it had worked very well,

00:08:00.800 --> 00:08:05.680
and he had no idea how it worked
or what it was picking up on.

00:08:06.840 --> 00:08:11.256
Now, the problem isn't solved
if he doesn't publish it,

00:08:11.280 --> 00:08:13.176
because there are already companies

00:08:13.200 --> 00:08:15.736
that are developing
this kind of technology,

00:08:15.760 --> 00:08:18.560
and a lot of the stuff
is just off the shelf.

00:08:19.240 --> 00:08:21.816
This is not very difficult anymore.

00:08:21.840 --> 00:08:25.296
Do you ever go on YouTube
meaning to watch one video

00:08:25.320 --> 00:08:27.680
and an hour later you've watched 27?

00:08:28.760 --> 00:08:31.256
You know how YouTube
has this column on the right

00:08:31.280 --> 00:08:33.496
that says, "Up next"

00:08:33.520 --> 00:08:35.336
and it autoplays something?

00:08:35.360 --> 00:08:36.576
It's an algorithm

00:08:36.600 --> 00:08:40.216
picking what it thinks
that you might be interested in

00:08:40.240 --> 00:08:41.776
and maybe not find on your own.

00:08:41.800 --> 00:08:43.056
It's not a human editor.

00:08:43.080 --> 00:08:44.496
It's what algorithms do.

00:08:44.520 --> 00:08:49.256
It picks up on what you have watched
and what people like you have watched,

00:08:49.280 --> 00:08:53.496
and infers that that must be
what you're interested in,

00:08:53.520 --> 00:08:54.775
what you want more of,

00:08:54.799 --> 00:08:56.135
and just shows you more.

00:08:56.159 --> 00:08:58.360
It sounds like a benign
and useful feature,

00:08:59.280 --> 00:09:00.480
except when it isn't.

00:09:01.640 --> 00:09:08.600
So in 2016, I attended rallies
of then-candidate Donald Trump

00:09:09.840 --> 00:09:13.176
to study as a scholar
the movement supporting him.

00:09:13.200 --> 00:09:16.656
I study social movements,
so I was studying it, too.

00:09:16.680 --> 00:09:20.016
And then I wanted to write something
about one of his rallies,

00:09:20.040 --> 00:09:22.000
so I watched it a few times on YouTube.

00:09:23.240 --> 00:09:26.336
YouTube started recommending to me

00:09:26.360 --> 00:09:30.616
and autoplaying to me
white supremacist videos

00:09:30.640 --> 00:09:33.296
in increasing order of extremism.

00:09:33.320 --> 00:09:35.136
If I watched one,

00:09:35.160 --> 00:09:38.136
it served up one even more extreme

00:09:38.160 --> 00:09:39.584
and autoplayed that one, too.

00:09:40.320 --> 00:09:44.856
If you watch Hillary Clinton
or Bernie Sanders content,

00:09:44.880 --> 00:09:49.576
YouTube recommends
and autoplays conspiracy left,

00:09:49.600 --> 00:09:51.360
and it goes downhill from there.

00:09:52.480 --> 00:09:55.536
Well, you might be thinking,
this is politics, but it's not.

00:09:55.560 --> 00:09:56.816
This isn't about politics.

00:09:56.840 --> 00:09:59.936
This is just the algorithm
figuring out human behavior.

00:09:59.960 --> 00:10:04.736
I once watched a video
about vegetarianism on YouTube

00:10:04.760 --> 00:10:09.696
and YouTube recommended
and autoplayed a video about being vegan.

00:10:09.720 --> 00:10:12.736
It's like you're never
hardcore enough for YouTube.

00:10:12.760 --> 00:10:14.336
(Laughter)

00:10:14.360 --> 00:10:15.920
So what's going on?

00:10:16.520 --> 00:10:20.056
Now, YouTube's algorithm is proprietary,

00:10:20.080 --> 00:10:22.440
but here's what I think is going on.

00:10:23.360 --> 00:10:25.456
The algorithm has figured out

00:10:25.480 --> 00:10:29.176
that if you can entice people

00:10:29.200 --> 00:10:32.936
into thinking that you can
show them something more hardcore,

00:10:32.960 --> 00:10:35.376
they're more likely to stay on the site

00:10:35.400 --> 00:10:39.816
watching video after video
going down that rabbit hole

00:10:39.840 --> 00:10:41.520
while Google serves them ads.

00:10:43.760 --> 00:10:46.880
Now, with nobody minding
the ethics of the store,

00:10:47.720 --> 00:10:51.960
these sites can profile people

00:10:53.680 --> 00:10:55.600
who are Jew haters,

00:10:56.360 --> 00:10:58.840
who think that Jews are parasites

00:11:00.320 --> 00:11:05.240
and who have such explicit
anti-Semitic content,

00:11:06.080 --> 00:11:08.080
and let you target them with ads.

00:11:09.200 --> 00:11:12.736
They can also mobilize algorithms

00:11:12.760 --> 00:11:15.896
to find for you look-alike audiences,

00:11:15.920 --> 00:11:21.496
people who do not have such explicit
anti-Semitic content on their profile

00:11:21.520 --> 00:11:27.696
but who the algorithm detects
may be susceptible to such messages,

00:11:27.720 --> 00:11:29.640
and lets you target them with ads, too.

00:11:30.680 --> 00:11:33.416
Now, this may sound
like an implausible example,

00:11:33.440 --> 00:11:34.760
but this is real.

00:11:35.480 --> 00:11:37.616
ProPublica investigated this

00:11:37.640 --> 00:11:41.256
and found that you can indeed
do this on Facebook,

00:11:41.280 --> 00:11:43.696
and Facebook helpfully
offered up suggestions

00:11:43.720 --> 00:11:45.320
on how to broaden that audience.

00:11:46.720 --> 00:11:49.736
BuzzFeed tried it for Google,
and very quickly they found,

00:11:49.760 --> 00:11:51.496
yep, you can do it on Google, too.

00:11:51.520 --> 00:11:53.216
And it wasn't even expensive.

00:11:53.240 --> 00:11:57.656
The ProPublica reporter
spent about 30 dollars

00:11:57.680 --> 00:11:59.920
to target this category.

00:12:02.600 --> 00:12:07.896
So last year, Donald Trump's
social media manager disclosed

00:12:07.920 --> 00:12:13.256
that they were using Facebook dark posts
to demobilize people,

00:12:13.280 --> 00:12:14.656
not to persuade them,

00:12:14.680 --> 00:12:17.480
but to convince them not to vote at all.

00:12:18.520 --> 00:12:22.096
And to do that,
they targeted specifically,

00:12:22.120 --> 00:12:26.016
for example, African-American men
in key cities like Philadelphia,

00:12:26.040 --> 00:12:28.496
and I'm going to read
exactly what he said.

00:12:28.520 --> 00:12:29.736
I'm quoting.

00:12:29.760 --> 00:12:32.776
They were using "nonpublic posts

00:12:32.800 --> 00:12:34.976
whose viewership the campaign controls

00:12:35.000 --> 00:12:38.776
so that only the people
we want to see it see it.

00:12:38.800 --> 00:12:40.016
We modeled this.

00:12:40.040 --> 00:12:44.760
It will dramatically affect her ability
to turn these people out."

00:12:45.720 --> 00:12:48.000
What's in those dark posts?

00:12:48.480 --> 00:12:50.136
We have no idea.

00:12:50.160 --> 00:12:51.360
Facebook won't tell us.

00:12:52.480 --> 00:12:56.856
So Facebook also algorithmically
arranges the posts

00:12:56.880 --> 00:13:00.616
that your friends put on Facebook,
or the pages you follow.

00:13:00.640 --> 00:13:02.856
It doesn't show you
everything chronologically.

00:13:02.880 --> 00:13:07.696
It puts the order in the way
that the algorithm thinks will entice you

00:13:07.720 --> 00:13:09.560
to stay on the site longer.

00:13:11.040 --> 00:13:14.416
Now, so this has a lot of consequences.

00:13:14.440 --> 00:13:18.240
You may be thinking
somebody is snubbing you on Facebook.

00:13:18.800 --> 00:13:22.056
The algorithm may never
be showing your post to them.

00:13:22.080 --> 00:13:28.040
The algorithm is prioritizing
some of them and burying the others.

00:13:29.320 --> 00:13:30.616
Experiments show

00:13:30.640 --> 00:13:35.160
that what the algorithm picks to show you
can affect your emotions.

00:13:36.600 --> 00:13:37.800
But that's not all.

00:13:38.280 --> 00:13:40.640
It also affects political behavior.

00:13:41.360 --> 00:13:46.016
So in 2010, in the midterm elections,

00:13:46.040 --> 00:13:51.936
Facebook did an experiment
on 61 million people in the US

00:13:51.960 --> 00:13:53.856
that was disclosed after the fact.

00:13:53.880 --> 00:13:57.296
So some people were shown,
"Today is election day,"

00:13:57.320 --> 00:13:58.696
the simpler one,

00:13:58.720 --> 00:14:02.616
and some people were shown
the one with that tiny tweak

00:14:02.640 --> 00:14:04.736
with those little thumbnails

00:14:04.760 --> 00:14:07.600
of your friends who clicked on "I voted."

00:14:09.000 --> 00:14:10.400
This simple tweak.

00:14:11.520 --> 00:14:15.816
OK? So the pictures were the only change,

00:14:15.840 --> 00:14:19.096
and that post shown just once

00:14:19.120 --> 00:14:25.176
turned out an additional 340,000 voters

00:14:25.200 --> 00:14:26.896
in that election,

00:14:26.920 --> 00:14:28.616
according to this research

00:14:28.640 --> 00:14:31.160
as confirmed by the voter rolls.

00:14:32.920 --> 00:14:34.576
A fluke? No.

00:14:34.600 --> 00:14:39.960
Because in 2012,
they repeated the same experiment.

00:14:40.840 --> 00:14:42.576
And that time,

00:14:42.600 --> 00:14:45.896
that civic message shown just once

00:14:45.920 --> 00:14:50.360
turned out an additional 270,000 voters.

00:14:51.160 --> 00:14:56.376
For reference, the 2016
US presidential election

00:14:56.400 --> 00:14:59.920
was decided by about 100,000 votes.

00:15:01.360 --> 00:15:06.096
Now, Facebook can also
very easily infer what your politics are,

00:15:06.120 --> 00:15:08.376
even if you've never
disclosed them on the site.

00:15:08.400 --> 00:15:10.920
Right? These algorithms
can do that quite easily.

00:15:11.960 --> 00:15:15.856
What if a platform with that kind of power

00:15:15.880 --> 00:15:20.920
decides to turn out supporters
of one candidate over the other?

00:15:21.680 --> 00:15:24.120
How would we even know about it?

00:15:25.560 --> 00:15:29.696
Now, we started from someplace
seemingly innocuous --

00:15:29.720 --> 00:15:31.936
online adds following us around --

00:15:31.960 --> 00:15:33.800
and we've landed someplace else.

00:15:35.480 --> 00:15:37.936
As a public and as citizens,

00:15:37.960 --> 00:15:41.376
we no longer know
if we're seeing the same information

00:15:41.400 --> 00:15:42.880
or what anybody else is seeing,

00:15:43.680 --> 00:15:46.256
and without a common basis of information,

00:15:46.280 --> 00:15:47.896
little by little,

00:15:47.920 --> 00:15:51.136
public debate is becoming impossible,

00:15:51.160 --> 00:15:54.136
and we're just at
the beginning stages of this.

00:15:54.160 --> 00:15:57.616
These algorithms can quite easily infer

00:15:57.640 --> 00:16:00.896
things like your people's ethnicity,

00:16:00.920 --> 00:16:03.256
religious and political views,
personality traits,

00:16:03.280 --> 00:16:06.656
intelligence, happiness,
use of addictive substances,

00:16:06.680 --> 00:16:09.816
parental separation, age and genders,

00:16:09.840 --> 00:16:11.800
just from Facebook likes.

00:16:13.440 --> 00:16:17.496
These algorithms can identify protesters

00:16:17.520 --> 00:16:20.280
even if their faces
are partially concealed.

00:16:21.720 --> 00:16:28.336
These algorithms may be able
to detect people's sexual orientation

00:16:28.360 --> 00:16:31.560
just from their dating profile pictures.

00:16:33.560 --> 00:16:36.176
Now, these are probabilistic guesses,

00:16:36.200 --> 00:16:39.096
so they're not going
to be 100 percent right,

00:16:39.120 --> 00:16:44.016
but I don't see the powerful resisting
the temptation to use these technologies

00:16:44.040 --> 00:16:46.216
just because there are
some false positives,

00:16:46.240 --> 00:16:49.496
which will of course create
a whole other layer of problems.

00:16:49.520 --> 00:16:52.456
Imagine what a state can do

00:16:52.480 --> 00:16:56.040
with the immense amount of data
it has on its citizens.

00:16:56.680 --> 00:17:01.456
China is already using
face detection technology

00:17:01.480 --> 00:17:04.360
to identify and arrest people.

00:17:05.280 --> 00:17:07.416
And here's the tragedy:

00:17:07.440 --> 00:17:12.976
we're building this infrastructure
of surveillance authoritarianism

00:17:13.000 --> 00:17:15.960
merely to get people to click on ads.

00:17:17.240 --> 00:17:19.816
And this won't be
Orwell's authoritarianism.

00:17:19.839 --> 00:17:21.736
This isn't "1984."

00:17:21.760 --> 00:17:26.336
Now, if authoritarianism
is using overt fear to terrorize us,

00:17:26.359 --> 00:17:29.256
we'll all be scared, but we'll know it,

00:17:29.280 --> 00:17:31.480
we'll hate it and we'll resist it.

00:17:32.880 --> 00:17:37.296
But if the people in power
are using these algorithms

00:17:37.319 --> 00:17:40.696
to quietly watch us,

00:17:40.720 --> 00:17:42.800
to judge us and to nudge us,

00:17:43.720 --> 00:17:47.896
to predict and identify
the troublemakers and the rebels,

00:17:47.920 --> 00:17:51.816
to deploy persuasion
architectures at scale

00:17:51.840 --> 00:17:55.976
and to manipulate individuals one by one

00:17:56.000 --> 00:18:01.440
using their personal, individual
weaknesses and vulnerabilities,

00:18:02.720 --> 00:18:04.920
and if they're doing it at scale

00:18:06.080 --> 00:18:07.816
through our private screens

00:18:07.840 --> 00:18:09.496
so that we don't even know

00:18:09.520 --> 00:18:12.280
what our fellow citizens
and neighbors are seeing,

00:18:13.560 --> 00:18:18.376
that authoritarianism
will envelop us like a spider's web

00:18:18.400 --> 00:18:20.880
and we may not even know we're in it.

00:18:22.440 --> 00:18:25.376
So Facebook's market capitalization

00:18:25.400 --> 00:18:28.696
is approaching half a trillion dollars.

00:18:28.720 --> 00:18:31.840
It's because it works great
as a persuasion architecture.

00:18:33.760 --> 00:18:36.576
But the structure of that architecture

00:18:36.600 --> 00:18:39.816
is the same whether you're selling shoes

00:18:39.840 --> 00:18:42.336
or whether you're selling politics.

00:18:42.360 --> 00:18:45.480
The algorithms do not know the difference.

00:18:46.240 --> 00:18:49.536
The same algorithms set loose upon us

00:18:49.560 --> 00:18:52.736
to make us more pliable for ads

00:18:52.760 --> 00:18:59.496
are also organizing our political,
personal and social information flows,

00:18:59.520 --> 00:19:01.360
and that's what's got to change.

00:19:02.240 --> 00:19:04.536
Now, don't get me wrong,

00:19:04.560 --> 00:19:08.240
we use digital platforms
because they provide us with great value.

00:19:09.120 --> 00:19:12.680
I use Facebook to keep in touch
with friends and family around the world.

00:19:14.000 --> 00:19:19.776
I've written about how crucial
social media is for social movements.

00:19:19.800 --> 00:19:22.816
I have studied how
these technologies can be used

00:19:22.840 --> 00:19:25.320
to circumvent censorship around the world.

00:19:27.280 --> 00:19:33.696
But it's not that the people who run,
you know, Facebook or Google

00:19:33.720 --> 00:19:36.416
are maliciously and deliberately trying

00:19:36.440 --> 00:19:40.896
to make the country
or the world more polarized

00:19:40.920 --> 00:19:42.600
and encourage extremism.

00:19:43.440 --> 00:19:47.416
I read the many
well-intentioned statements

00:19:47.440 --> 00:19:50.760
that these people put out.

00:19:51.600 --> 00:19:57.656
But it's not the intent or the statements
people in technology make that matter,

00:19:57.680 --> 00:20:01.240
it's the structures
and business models they're building.

00:20:02.360 --> 00:20:04.456
And that's the core of the problem.

00:20:04.480 --> 00:20:09.200
Either Facebook is a giant con
of half a trillion dollars

00:20:10.200 --> 00:20:12.096
and ads don't work on the site,

00:20:12.120 --> 00:20:14.816
it doesn't work
as a persuasion architecture,

00:20:14.840 --> 00:20:18.960
or its power of influence
is of great concern.

00:20:20.560 --> 00:20:22.336
It's either one or the other.

00:20:22.360 --> 00:20:23.960
It's similar for Google, too.

00:20:24.880 --> 00:20:27.336
So what can we do?

00:20:27.360 --> 00:20:29.296
This needs to change.

00:20:29.320 --> 00:20:31.896
Now, I can't offer a simple recipe,

00:20:31.920 --> 00:20:34.176
because we need to restructure

00:20:34.200 --> 00:20:37.216
the whole way our
digital technology operates.

00:20:37.240 --> 00:20:41.336
Everything from the way
technology is developed

00:20:41.360 --> 00:20:45.216
to the way the incentives,
economic and otherwise,

00:20:45.240 --> 00:20:47.520
are built into the system.

00:20:48.480 --> 00:20:51.936
We have to face and try to deal with

00:20:51.960 --> 00:20:56.616
the lack of transparency
created by the proprietary algorithms,

00:20:56.640 --> 00:21:00.456
the structural challenge
of machine learning's opacity,

00:21:00.480 --> 00:21:03.880
all this indiscriminate data
that's being collected about us.

00:21:05.000 --> 00:21:07.520
We have a big task in front of us.

00:21:08.160 --> 00:21:10.840
We have to mobilize our technology,

00:21:11.760 --> 00:21:13.336
our creativity

00:21:13.360 --> 00:21:15.240
and yes, our politics

00:21:16.240 --> 00:21:18.896
so that we can build
artificial intelligence

00:21:18.920 --> 00:21:22.040
that supports us in our human goals

00:21:22.800 --> 00:21:26.720
but that is also constrained
by our human values.

00:21:27.600 --> 00:21:29.760
And I understand this won't be easy.

00:21:30.360 --> 00:21:33.960
We might not even easily agree
on what those terms mean.

00:21:34.920 --> 00:21:37.320
But if we take seriously

00:21:38.240 --> 00:21:44.216
how these systems that we
depend on for so much operate,

00:21:44.240 --> 00:21:48.360
I don't see how we can postpone
this conversation anymore.

00:21:49.200 --> 00:21:51.736
These structures

00:21:51.760 --> 00:21:55.856
are organizing how we function

00:21:55.880 --> 00:21:58.176
and they're controlling

00:21:58.200 --> 00:22:00.816
what we can and we cannot do.

00:22:00.840 --> 00:22:03.296
And many of these ad-financed platforms,

00:22:03.320 --> 00:22:04.896
they boast that they're free.

00:22:04.920 --> 00:22:09.480
In this context, it means
that we are the product that's being sold.

00:22:10.840 --> 00:22:13.576
We need a digital economy

00:22:13.600 --> 00:22:17.096
where our data and our attention

00:22:17.120 --> 00:22:22.200
is not for sale to the highest-bidding
authoritarian or demagogue.

00:22:23.160 --> 00:22:26.960
(Applause)

00:22:30.480 --> 00:22:33.736
So to go back to
that Hollywood paraphrase,

00:22:33.760 --> 00:22:37.496
we do want the prodigious potential

00:22:37.520 --> 00:22:40.720
of artificial intelligence
and digital technology to blossom,

00:22:41.400 --> 00:22:46.336
but for that, we must face
this prodigious menace,

00:22:46.360 --> 00:22:48.296
open-eyed and now.

00:22:48.320 --> 00:22:49.536
Thank you.

00:22:49.560 --> 00:22:54.200
(Applause)

