WEBVTT
Kind: captions
Language: it

00:00:00.000 --> 00:00:07.000
Traduttore: antonio parlato
Revisore: Anna Cristiana Minoli

00:00:12.760 --> 00:00:16.296
Quando la gente dà voce alle paure
legate all'intelligenza artificiale,

00:00:16.320 --> 00:00:20.296
molto spesso evoca immagini
di robot umanoidi usciti fuori controllo.

00:00:20.320 --> 00:00:21.560
Avete presente Terminator?

00:00:22.400 --> 00:00:24.736
Potrebbe essere una possibilità
da considerare,

00:00:24.760 --> 00:00:26.616
ma si tratta di una minaccia remota.

00:00:26.640 --> 00:00:30.096
Oppure, ci agitiamo 
per il controllo digitale

00:00:30.120 --> 00:00:31.896
usando metafore del passato.

00:00:31.920 --> 00:00:34.576
"1984" di George Orwell

00:00:34.600 --> 00:00:36.880
è di nuovo tra i bestseller.

00:00:37.960 --> 00:00:39.376
È un gran libro,

00:00:39.400 --> 00:00:43.280
ma non è la corretta distopia
da applicare al 21esimo secolo.

00:00:44.080 --> 00:00:45.496
Ciò che dobbiamo temere

00:00:45.520 --> 00:00:50.296
non è cosa l'intelligenza artificiale
possa fare di per sé,

00:00:50.320 --> 00:00:55.056
ma come le persone di potere
potranno utilizzarla

00:00:55.080 --> 00:00:57.896
per controllarci e manipolarci

00:00:57.920 --> 00:01:01.056
in modi nuovi, talvolta nascosti,

00:01:01.080 --> 00:01:04.096
sottili e inaspettati.

00:01:04.120 --> 00:01:05.976
Gran parte della tecnologia

00:01:06.000 --> 00:01:10.336
che mette a rischio la nostra libertà
e dignità nel futuro imminente

00:01:10.360 --> 00:01:12.216
viene sviluppato da aziende

00:01:12.240 --> 00:01:17.176
che guadagnano catturando
e rivendendo i nostri dati e attenzione

00:01:17.200 --> 00:01:19.456
a inserzionisti e altre entità:

00:01:19.480 --> 00:01:22.896
Facebook, Google, Amazon,

00:01:22.920 --> 00:01:24.800
Alibaba, Tencent.

00:01:26.040 --> 00:01:31.536
Ora l'intelligenza artificiale comincia
a rafforzare il loro business.

00:01:31.560 --> 00:01:33.656
E sembrerebbe 
che l'intelligenza artificiale

00:01:33.680 --> 00:01:36.536
sia il passo successivo
alla pubblicità online.

00:01:36.560 --> 00:01:37.776
Non lo è.

00:01:37.800 --> 00:01:40.256
È un salto di categoria.

00:01:40.280 --> 00:01:42.856
È un mondo totalmente differente,

00:01:42.880 --> 00:01:45.496
con un grosso potenziale.

00:01:45.520 --> 00:01:52.440
Potrebbe accelerare la nostra comprensione
in molte aree di studio e di ricerca.

00:01:53.120 --> 00:01:56.616
Ma per parafrasare
un famoso filosofo di Hollywood,

00:01:56.640 --> 00:02:00.280
"Da un grande potenziale
derivano grandi rischi"

00:02:01.120 --> 00:02:05.056
Guardiamo a un fatto semplice
delle nostre vite digitali, la pubblicità.

00:02:05.080 --> 00:02:07.976
Giusto? Tendiamo a ignorarle.

00:02:08.000 --> 00:02:09.976
Sembrano rozze, di scarso effetto.

00:02:10.000 --> 00:02:14.256
Tutti abbiamo avuto la sensazione
di essere inseguiti sul web

00:02:14.280 --> 00:02:17.056
da pubblicità basate su cose 
che abbiamo cercato o letto.

00:02:17.080 --> 00:02:18.936
Lo sapete, cercate un paio di stivali

00:02:18.960 --> 00:02:22.336
e per giorni, questi stivali vi seguiranno
dovunque andiate.

00:02:22.360 --> 00:02:26.016
Persino quando cedete e li acquistate,
continueranno a seguirvi ancora.

00:02:26.040 --> 00:02:29.056
Siamo quasi abituati a questo tipo
di manipolazione grossolana.

00:02:29.080 --> 00:02:32.480
Alziamo gli occhi e pensiamo,
"Sai? Queste cose non funzionano."

00:02:33.720 --> 00:02:35.816
Tranne che, online,

00:02:35.840 --> 00:02:39.440
le tecnologie digitali
non sono solo pubblicità.

00:02:40.240 --> 00:02:43.360
Per capire meglio,
prendiamo un esempio del mondo fisico.

00:02:43.840 --> 00:02:48.496
Sapete che nei supermercati,
proprio vicino alle casse

00:02:48.520 --> 00:02:52.000
trovate dolci e gomme
ad altezza occhi di bambino?

00:02:52.800 --> 00:02:56.296
È fatto apposta 
per farli piagnucolare

00:02:56.320 --> 00:02:59.400
proprio mentre i genitori
stanno per pagare alla cassa.

00:03:00.040 --> 00:03:02.680
Questa è architettura persuasiva.

00:03:03.160 --> 00:03:06.256
Non è bella, ma funziona.

00:03:06.280 --> 00:03:08.320
E infatti la vedete
in tutti i supermarket.

00:03:08.720 --> 00:03:10.416
Nel mondo reale,

00:03:10.440 --> 00:03:12.936
queste architetture persuasive
sono un po' limitate,

00:03:12.960 --> 00:03:17.776
perché si può caricare 
fino a un certo punto l'angolo della cassa.

00:03:17.800 --> 00:03:22.096
Inoltre dolci e gomme,
sono uguali per tutti,

00:03:22.120 --> 00:03:23.576
anche se funziona di più

00:03:23.600 --> 00:03:27.640
per le persone che hanno
piccoli bimbi frignanti accanto.

00:03:29.160 --> 00:03:33.080
Nel mondo fisico,
viviamo con queste limitazioni.

00:03:34.280 --> 00:03:36.216
Nel mondo digitale, invece,

00:03:36.240 --> 00:03:38.840
le architetture persuasive
possono essere costruite

00:03:38.840 --> 00:03:41.840
su larghissima scala

00:03:41.840 --> 00:03:45.696
e possono colpire, dedurre, capire

00:03:45.720 --> 00:03:48.616
essere su misura per singoli individui

00:03:48.640 --> 00:03:49.856
uno per uno

00:03:49.880 --> 00:03:52.016
scoprendo le debolezze,

00:03:52.040 --> 00:03:57.656
e possono essere inviate
sugli schermi privati dei nostri telefoni,

00:03:57.680 --> 00:03:59.936
così da non essere visibili.

00:03:59.960 --> 00:04:01.216
E questo è differente.

00:04:01.240 --> 00:04:04.816
Questa è una delle cose basilari
che l'intelligenza artificiale può fare.

00:04:04.840 --> 00:04:06.176
Facciamo un esempio.

00:04:06.200 --> 00:04:08.896
Poniamo caso vogliate vendere
voli verso Las Vegas, ok?

00:04:08.920 --> 00:04:12.416
Nel vecchio mondo, potreste pensare
a un target demografico da colpire

00:04:12.440 --> 00:04:14.960
basato sull'esperienza
e su alcune supposizioni.

00:04:15.560 --> 00:04:18.376
Avreste diretto la pubblicità

00:04:18.400 --> 00:04:20.896
a uomini tra i 25 e i 35,

00:04:20.920 --> 00:04:24.856
o persone con un plafond alto
nelle carte di credito

00:04:24.880 --> 00:04:26.256
o coppie in pensione.Giusto?

00:04:26.280 --> 00:04:28.096
Questo è ciò che avreste fatto ieri.

00:04:28.120 --> 00:04:31.016
Grazie a Big Data e Machine Learning,

00:04:31.040 --> 00:04:32.564
non funziona più così.

00:04:33.320 --> 00:04:35.496
Per aiutarvi a capire,

00:04:35.520 --> 00:04:39.376
pensate a tutti i dati
che Facebook ha su di voi:

00:04:39.400 --> 00:04:41.936
tutti gli aggiornamenti che avete scritto,

00:04:41.960 --> 00:04:43.976
tutte le chat su Messenger,

00:04:44.000 --> 00:04:45.880
i posti da cui vi siete collegati,

00:04:48.400 --> 00:04:51.576
tutte le foto che avete aggiunto.

00:04:51.600 --> 00:04:55.376
Se cominciate a scrivere qualcosa
e poi ci ripensate e cancellate,

00:04:55.400 --> 00:04:58.600
Facebook conserva
e analizza anche quello.

00:04:59.160 --> 00:05:03.096
Prova sempre di più a collegarvi
con i vostri dati offline.

00:05:03.120 --> 00:05:06.296
E compra pure dei dati
da altri fornitori esterni.

00:05:06.320 --> 00:05:09.736
Dati di varia natura
come movimenti finanziari

00:05:09.760 --> 00:05:11.880
o la storia delle pagine
che avete visitato.

00:05:12.360 --> 00:05:17.776
Negli Stati Uniti questi dati
vengono raccolti con regolarità,

00:05:17.800 --> 00:05:19.760
esaminati e rivenduti.

00:05:20.320 --> 00:05:22.760
In Europa le leggi sono più restrittive.

00:05:23.680 --> 00:05:25.880
Quindi, ciò che accade

00:05:26.920 --> 00:05:30.936
macinando tutti questi dati,
gli algoritmi "machine learning"

00:05:30.960 --> 00:05:33.856
proprio per questo sono chiamati
algoritmi di "apprendimento"

00:05:33.880 --> 00:05:37.976
apprendono come capire 
le caratteristiche della gente

00:05:38.000 --> 00:05:40.520
che ha comprato in precedenza 
biglietti per Las Vegas

00:05:41.760 --> 00:05:45.296
Così come imparano dai dati esistenti,

00:05:45.320 --> 00:05:49.136
imparano anche come applicarlo
a nuove persone.

00:05:49.160 --> 00:05:52.216
Quindi quando si trovano davanti
un nuovo soggetto

00:05:52.240 --> 00:05:56.880
possono classificarlo e dire se vorrebbe
o meno un biglietto per Las Vegas

00:05:57.720 --> 00:06:03.176
Bene. Starete pensando,
un'offerta su un volo per Las Vegas.

00:06:03.200 --> 00:06:04.656
Posso ignorarla.

00:06:04.680 --> 00:06:06.896
Ma non è quello il problema.

00:06:06.920 --> 00:06:08.496
Il problema è

00:06:08.520 --> 00:06:12.656
che non riusciamo più a capire
come funzionano questi algoritmi complessi

00:06:12.680 --> 00:06:16.136
non capiamo
come facciano questa categorizzazione.

00:06:16.160 --> 00:06:20.576
Sono matrici giganti,
con migliaia di righe e colonne,

00:06:20.600 --> 00:06:22.560
forse milioni di righe e colonne,

00:06:23.320 --> 00:06:25.960
e né i programmatori

00:06:26.760 --> 00:06:28.440
né nessuno di quelli che ha accesso,

00:06:29.440 --> 00:06:30.936
anche se possiede tutti i dati,

00:06:30.960 --> 00:06:35.576
capisce più 
come stia funzionando esattamente

00:06:35.600 --> 00:06:39.376
non più di quanto voi potreste capire
a cosa sto pensando ora

00:06:39.400 --> 00:06:43.360
se vi fosse mostrata
una sezione del mio cervello.

00:06:44.360 --> 00:06:46.936
È come se non programmassimo più,

00:06:46.960 --> 00:06:51.360
ma accresciamo un'intelligenza
che non riusciamo a capire davvero.

00:06:52.520 --> 00:06:56.496
Tutto questo funziona
solo attraverso un'enorme quantità di dati,

00:06:56.520 --> 00:07:01.616
cosicché viene favorito 
un controllo approfondito su tutti noi

00:07:01.640 --> 00:07:03.976
per far funzionare gli algoritmi 
di apprendimento

00:07:04.000 --> 00:07:07.176
Per questo Facebook vuole raccogliere
tutti i vostri dati.

00:07:07.200 --> 00:07:08.776
L'algoritmo funziona meglio.

00:07:08.800 --> 00:07:11.496
Spingiamo ancora di più
l'esempio di Las Vegas.

00:07:11.520 --> 00:07:15.200
E se questo sistema
che non riusciamo a comprendere

00:07:16.200 --> 00:07:21.336
avesse imparato che è più semplice
vendere biglietti per Las Vegas

00:07:21.360 --> 00:07:25.120
a persone bipolari e sul punto
di entrare nella fase maniacale.

00:07:25.640 --> 00:07:30.560
Queste persone tendono a spendere di più
e a diventare scommettitori compulsivi.

00:07:31.280 --> 00:07:35.736
Potrebbero pescare da quel bacino 
e nessuno potrebbe mai saperlo.

00:07:35.760 --> 00:07:39.376
Ho fatto questo esempio
a un gruppo di ingegneri, una volta

00:07:39.400 --> 00:07:41.456
e subito dopo, uno di loro è venuto da me.

00:07:41.480 --> 00:07:45.000
Era turbato e mi disse,
"ecco perché non posso pubblicarlo"

00:07:45.600 --> 00:07:47.315
E io, "Non pubblicare cosa?"

00:07:47.800 --> 00:07:53.656
Aveva provato a vedere se fosse possibile
comprendere uno stato maniacale

00:07:53.680 --> 00:07:56.896
dai post sui social media
prima dei sintomi clinici,

00:07:56.920 --> 00:07:58.696
e aveva funzionato,

00:07:58.720 --> 00:08:00.776
aveva funzionato benissimo,

00:08:00.800 --> 00:08:05.680
e lui non aveva idea di come funzionasse
e cosa stesse pescando.

00:08:06.840 --> 00:08:11.256
Il problema non si risolve
se lui non pubblica il software,

00:08:11.280 --> 00:08:13.176
perché ci sono già aziende

00:08:13.200 --> 00:08:15.736
che stanno sviluppando
questo tipo di tecnologia,

00:08:15.760 --> 00:08:18.560
e una buona parte è già disponibile.

00:08:19.240 --> 00:08:21.816
Non è più una cosa complicata.

00:08:21.840 --> 00:08:25.296
Andate mai su YouTube
con l'idea di guardare un video

00:08:25.320 --> 00:08:27.680
e un'ora dopo ne avete guardati 27?

00:08:28.760 --> 00:08:31.256
Sapete che YouTube ha questa colonna
sulla destra

00:08:31.280 --> 00:08:33.496
che dice "Prossimi video"

00:08:33.520 --> 00:08:35.336
e poi ne parte in automatico un altro?

00:08:35.360 --> 00:08:36.576
È un algoritmo

00:08:36.600 --> 00:08:40.216
che sceglie qualcosa che pensa
possa interessarvi

00:08:40.240 --> 00:08:41.776
che magari non trovate da soli.

00:08:41.800 --> 00:08:43.056
Non è un editor umano.

00:08:43.080 --> 00:08:44.496
Lo fa un algoritmo.

00:08:44.520 --> 00:08:49.256
Studia quello che avete guardato voi
e le persone simili a voi

00:08:49.280 --> 00:08:53.496
e desume quello
che potrebbe interessarvi,

00:08:53.520 --> 00:08:54.775
cosa vorreste di più,

00:08:54.799 --> 00:08:56.135
e ve ne mostra di più.

00:08:56.159 --> 00:08:58.360
Sembra una funzionalità
innocua e utile

00:08:59.280 --> 00:09:00.480
tranne quando non lo è.

00:09:01.640 --> 00:09:08.600
Nel 2016, ho partecipato a raduni
dell'allora candidato Donald Trump

00:09:09.840 --> 00:09:13.176
per studiare accademicamente
il movimento che lo supportava.

00:09:13.200 --> 00:09:16.656
Studio movimenti sociali,
così studiavo anche quello.

00:09:16.680 --> 00:09:20.016
Volevo scrivere qualcosa
su uno di quei raduni,

00:09:20.040 --> 00:09:22.000
e l'ho guardato qualche volta su YouTube.

00:09:23.240 --> 00:09:26.336
YouTube ha cominciato a raccomandarmi

00:09:26.360 --> 00:09:30.616
e a far partire video
sui suprematisti bianchi

00:09:30.640 --> 00:09:33.296
in crescente ordine di estremismo.

00:09:33.320 --> 00:09:35.136
Ne guardavo uno,

00:09:35.160 --> 00:09:38.136
me ne proponeva uno ancora più estremo

00:09:38.160 --> 00:09:39.584
che partiva in automatico.

00:09:40.320 --> 00:09:44.856
Se guardi contenuti di Hillary Clinton
o di Bernie Sanders

00:09:44.880 --> 00:09:49.576
YouTube raccomanda e fa partire 
video cospirazionisti di sinistra,

00:09:49.600 --> 00:09:51.360
e da lì sempre più giù.

00:09:52.480 --> 00:09:55.536
Bene, potreste pensare,
questa è politica. Ma non lo è.

00:09:55.560 --> 00:09:56.816
Non si tratta di politica.

00:09:56.840 --> 00:09:59.936
Si tratta di un algoritmo
che ha compreso la natura umana.

00:09:59.960 --> 00:10:04.736
Una volta ho guardato un video
sul vegetarianismo su YouTube

00:10:04.760 --> 00:10:09.696
e YouTube mi ha consigliato un video
sull'essere vegani.

00:10:09.720 --> 00:10:12.736
È come se non ci credete mai abbastanza
per YouTube

00:10:12.760 --> 00:10:14.336
(Risate)

00:10:14.360 --> 00:10:15.920
Allora cosa succede?

00:10:16.520 --> 00:10:20.056
Anche se l'algoritmo di YouTube 
è proprietario

00:10:20.080 --> 00:10:22.440
ecco cosa penso stia accadendo.

00:10:23.360 --> 00:10:25.456
L'algoritmo ha capito

00:10:25.480 --> 00:10:29.176
che se puoi attrarre la gente

00:10:29.200 --> 00:10:32.936
a pensare che puoi mostrargli
qualcosa di ancora più spinto,

00:10:32.960 --> 00:10:35.376
la gente rimarrà sul sito
con maggiore probabilità

00:10:35.400 --> 00:10:39.816
guardando video dopo video
scendendo giù nella tana del coniglio

00:10:39.840 --> 00:10:41.520
mentre Google mostra le pubblicità.

00:10:43.760 --> 00:10:46.880
Mentre nessuno si accorge
dei risvolti etici,

00:10:47.720 --> 00:10:51.960
questi siti possono profilare persone

00:10:53.680 --> 00:10:55.600
che odiano gli ebrei,

00:10:56.360 --> 00:10:58.840
che pensano che gli ebrei siano parassiti

00:11:00.320 --> 00:11:05.240
e chi ha contenuti antisemiti
così espliciti,

00:11:06.080 --> 00:11:07.080
e vi consente

00:11:07.080 --> 00:11:09.200
di usarli come bersaglio 
per le pubblicità.

00:11:09.200 --> 00:11:12.736
Possono anche sguinzagliare gli algoritmi

00:11:12.760 --> 00:11:15.896
per trovare per voi 
gruppi di utenti simili,

00:11:15.920 --> 00:11:21.496
gente che sebbene non mostri
contenuti antisemiti così espliciti

00:11:21.520 --> 00:11:27.696
viene riconosciuta dall'algoritmo
come sensibile a questo tipo di messaggi,

00:11:27.720 --> 00:11:29.640
e vi fa bersagliare anche loro.

00:11:30.680 --> 00:11:33.416
Questo potrebbe sembrare 
un esempio poco plausibile,

00:11:33.440 --> 00:11:34.760
però è davvero reale.

00:11:35.480 --> 00:11:37.616
ProPublica ha investigato

00:11:37.640 --> 00:11:41.256
e scoperto che è possibile farlo davvero
su Facebook

00:11:41.280 --> 00:11:43.696
e che Facebook amichevolmente
ha dato suggerimenti

00:11:43.720 --> 00:11:45.320
su come espandere quest'audience.

00:11:46.720 --> 00:11:49.736
BuzzFeed ha provato a farlo con Google,
e ha subito scoperto

00:11:49.760 --> 00:11:51.496
che sì, si può fare anche su Google.

00:11:51.520 --> 00:11:53.216
E che non era nemmeno caro.

00:11:53.240 --> 00:11:57.656
Il giornalista di ProPublica
ha speso circa 30 dollari

00:11:57.680 --> 00:11:59.920
per colpire questa categoria.

00:12:02.600 --> 00:12:07.896
Il Social Media Manager di Trump
ha rivelato che l'anno scorso

00:12:07.920 --> 00:12:13.256
hanno utilizzato i post nascosti 
di Facebook per smobilitare la gente,

00:12:13.280 --> 00:12:14.656
non per persuaderla,

00:12:14.680 --> 00:12:17.480
ma per convincerla di non votare affatto.

00:12:18.520 --> 00:12:22.096
E per farlo,
hanno preso come target specifico

00:12:22.120 --> 00:12:26.016
per esempio, uomini afroamericani
in città chiave come Philadelphia

00:12:26.040 --> 00:12:28.496
e ora vi leggo
esattamente quello che ha detto.

00:12:28.520 --> 00:12:29.736
Sto citando.

00:12:29.760 --> 00:12:32.776
Stavano utilizzando, "Post non pubblici

00:12:32.800 --> 00:12:34.976
la cui audience è controllata 
nella campagna

00:12:35.000 --> 00:12:38.776
in modo tale che solo gente
che noi vogliamo che veda, vedrà.

00:12:38.800 --> 00:12:40.016
L'abbiamo costruita così.

00:12:40.040 --> 00:12:44.760
Questo aumenterà enormemente
la capacità di spegnere queste persone."

00:12:45.720 --> 00:12:48.000
Cosa c'era in questi post nascosti?

00:12:48.480 --> 00:12:50.136
Non ne abbiamo la minima idea.

00:12:50.160 --> 00:12:51.360
Facebook non ce lo dirà.

00:12:52.480 --> 00:12:56.856
L'algoritmo di Facebook
dispone i post

00:12:56.880 --> 00:13:00.616
che i vostri amici o le pagine che seguite
pubblicano.

00:13:00.640 --> 00:13:02.856
Non ve li mostra
in ordine cronologico.

00:13:02.880 --> 00:13:07.696
L'algoritmo li riordina in modo tale
che vi inducano

00:13:07.720 --> 00:13:09.560
a stare più a lungo sul sito.

00:13:11.040 --> 00:13:14.416
E questo ha molte conseguenze.

00:13:14.440 --> 00:13:18.240
Potreste pensare che qualcuno
vi stia ignorando su Facebook.

00:13:18.800 --> 00:13:22.056
L'algoritmo potrebbe non mostrare mai
i vostri post.

00:13:22.080 --> 00:13:28.040
L'algoritmo dà priorità ad alcuni di loro
e seppellisce gli altri.

00:13:29.320 --> 00:13:30.616
Esperimenti mostrano

00:13:30.640 --> 00:13:35.160
che ciò che l'algoritmo sceglie di mostrarvi
può influenzare le vostre emozioni.

00:13:36.600 --> 00:13:37.800
E non è tutto.

00:13:38.280 --> 00:13:40.640
Può influenzare 
il comportamento politico.

00:13:41.360 --> 00:13:46.016
Così nel 2010, 
durante le elezioni di metà mandato,

00:13:46.040 --> 00:13:51.936
Facebook ha fatto un esperimento
su 61 milioni di americani

00:13:51.960 --> 00:13:53.856
che è stato reso pubblico subito dopo.

00:13:53.880 --> 00:13:57.296
Ad alcuni veniva mostrata
"Oggi è il giorno delle elezioni",

00:13:57.320 --> 00:13:58.696
la più semplice,

00:13:58.720 --> 00:14:02.616
mentre ad altri veniva mostrata
quella con una piccola variazione

00:14:02.640 --> 00:14:04.736
con le piccole foto

00:14:04.760 --> 00:14:07.600
degli amici che hanno cliccato 
su "Ho votato".

00:14:09.000 --> 00:14:10.400
Questa semplice modifica.

00:14:11.520 --> 00:14:15.816
Ok? Le foto erano l'unica modifica,

00:14:15.840 --> 00:14:19.096
e quei post mostrati una volta sola

00:14:19.120 --> 00:14:25.176
hanno portato altri 340 000 votanti

00:14:25.200 --> 00:14:26.896
in quella elezione,

00:14:26.920 --> 00:14:28.616
secondo questa ricerca,

00:14:28.640 --> 00:14:31.160
come confermato
dalle liste degli elettori.

00:14:32.920 --> 00:14:34.576
Una coincidenza? No.

00:14:34.600 --> 00:14:39.960
Perché nel 2012,
hanno ripetuto lo stesso esperimento.

00:14:40.840 --> 00:14:42.576
E quella volta,

00:14:42.600 --> 00:14:45.896
quel messaggio civico mostrato 
una sola volta

00:14:45.920 --> 00:14:50.360
ha portato altri 270 000 votanti.

00:14:51.160 --> 00:14:56.376
Per la cronaca, nel 2016
le elezioni presidenziali negli USA

00:14:56.400 --> 00:14:59.920
sono state decise da circa 100 000 voti.

00:15:01.360 --> 00:15:06.096
Facebook può anche dedurre 
le vostre inclinazioni politiche

00:15:06.120 --> 00:15:08.376
anche se non le avete mai
esposte esplicitamente.

00:15:08.400 --> 00:15:10.920
Gli algoritmi
possono farlo facilmente.

00:15:11.960 --> 00:15:15.856
Cosa accade se una piattaforma
con questo tipo di potere

00:15:15.880 --> 00:15:20.920
decide di "spegnere" i sostenitori
di un candidato in favore dell'altro?

00:15:21.680 --> 00:15:24.120
Come potremmo mai accorgercene?

00:15:25.560 --> 00:15:29.696
Siamo partiti da qualcosa
che sembrava innocuo -

00:15:29.720 --> 00:15:31.936
le pubblicità che ci seguono dappertutto -

00:15:31.960 --> 00:15:33.800
e siamo arrivati a qualcos'altro.

00:15:35.480 --> 00:15:37.936
Come pubblico e come cittadini,

00:15:37.960 --> 00:15:41.376
non sappiamo più
se stiamo guardando le stesse informazioni

00:15:41.400 --> 00:15:42.880
o cosa stiano vedendo gli altri

00:15:43.680 --> 00:15:46.256
e senza una base comune d'informazione

00:15:46.280 --> 00:15:47.896
a poco a poco,

00:15:47.920 --> 00:15:51.136
il dibattito pubblico 
sta diventando impossibile,

00:15:51.160 --> 00:15:54.136
e siamo appena agli stadi iniziali
di tutto questo.

00:15:54.160 --> 00:15:57.616
Questi algoritmi 
possono facilmente dedurre

00:15:57.640 --> 00:16:00.896
cose come l'etnia,

00:16:00.920 --> 00:16:03.256
il pensiero politico e religioso,
la personalità

00:16:03.280 --> 00:16:06.656
l'intelligenza, la felicità,
l'uso di sostanze che danno dipendenza

00:16:06.680 --> 00:16:09.816
le separazioni dei genitori,
l'età e i sessi,

00:16:09.840 --> 00:16:11.800
solo dai like messi su Facebook.

00:16:13.440 --> 00:16:17.496
Questi algoritmi possono identificare
i manifestanti

00:16:17.520 --> 00:16:20.280
anche se le loro facce
sono parzialmente nascoste.

00:16:21.720 --> 00:16:28.336
Questi algoritmi possono essere capaci
di capire l'orientamento sessuale

00:16:28.360 --> 00:16:31.560
solo dalla foto di profilo 
nei siti d'incontri.

00:16:33.560 --> 00:16:36.176
Certo, queste sono sempre
deduzioni probabilistiche,

00:16:36.200 --> 00:16:39.096
quindi non arrivano a una precisione
del 100 per cento

00:16:39.120 --> 00:16:44.016
ma non credo che chi ha potere resista
alla tentazione di usare queste tecnologie

00:16:44.040 --> 00:16:46.216
solo perché esiste
qualche falso positivo,

00:16:46.240 --> 00:16:49.496
cosa che produrrà un'altra 
serie di problemi.

00:16:49.520 --> 00:16:52.456
Immaginate cosa possa fare uno stato

00:16:52.480 --> 00:16:56.040
con l'immensa quantità di dati
che possiede sui propri cittadini.

00:16:56.680 --> 00:17:01.456
La Cina sta già utilizzando
tecnologie di riconoscimento facciale

00:17:01.480 --> 00:17:04.360
per identificare e arrestare persone.

00:17:05.280 --> 00:17:07.416
Ed ecco la tragedia:

00:17:07.440 --> 00:17:12.976
costruiamo questa infrastruttura
di controllo autoritario

00:17:13.000 --> 00:17:15.960
solo per ottenere clic sulle pubblicità.

00:17:17.240 --> 00:17:19.816
E questo non sarà un autoritarismo
alla Orwell.

00:17:19.839 --> 00:17:21.736
Questo non è "1984".

00:17:21.760 --> 00:17:26.336
Se l'autoritarismo utilizza evidenti
meccanismi del terrore per spaventarci,

00:17:26.359 --> 00:17:29.256
saremo spaventati, ma sapremo perché,

00:17:29.280 --> 00:17:31.480
odieremo e resisteremo.

00:17:32.880 --> 00:17:37.296
Ma se le persone con potere
usano questo algoritmi

00:17:37.319 --> 00:17:40.696
per osservarci placidamente,

00:17:40.720 --> 00:17:42.800
per giudicarci e per spingerci,

00:17:43.720 --> 00:17:47.896
per predire e identificare
i piantagrane e i ribelli,

00:17:47.920 --> 00:17:51.816
per costruire un'architettura 
della persuasione su larga scala

00:17:51.840 --> 00:17:55.976
e per manipolare gli individui uno ad uno

00:17:56.000 --> 00:18:01.440
utilizzando le loro debolezze
e vulnerabilità individuali,

00:18:02.720 --> 00:18:04.920
e se lo fanno su larga scala

00:18:06.080 --> 00:18:07.816
attraverso i nostri schermi privati

00:18:07.840 --> 00:18:09.496
cosicché non sappiamo neppure

00:18:09.520 --> 00:18:12.280
quello che i nostri concittadini
e vicini stanno vedendo,

00:18:13.560 --> 00:18:18.376
questo autoritarismo 
ci avvolgerà come tela di ragno

00:18:18.400 --> 00:18:20.880
senza che ce ne rendiamo conto.

00:18:22.440 --> 00:18:25.376
La capitalizzazione di borsa di Facebook

00:18:25.400 --> 00:18:28.696
sta raggiungendo 
il mezzo trilione di dollari.

00:18:28.720 --> 00:18:31.840
È perché funziona benissimo
come architettura persuasiva.

00:18:33.760 --> 00:18:36.576
Ma la struttura di questa architettura

00:18:36.600 --> 00:18:39.816
è la stessa sia che voi vendiate scarpe

00:18:39.840 --> 00:18:42.336
o che stiate vendendo politica.

00:18:42.360 --> 00:18:45.480
L'algoritmo non ne conosce la differenza.

00:18:46.240 --> 00:18:49.536
Questi stessi algoritmi 
che vengono scatenati su di noi

00:18:49.560 --> 00:18:52.736
per renderci più malleabili 
alle pubblicità

00:18:52.760 --> 00:18:59.496
organizzano i nostri flussi d'informazione
politica, personale e sociale

00:18:59.520 --> 00:19:01.360
e questo è ciò che dovrebbe cambiare.

00:19:02.240 --> 00:19:04.536
Non mi fraintendete,

00:19:04.560 --> 00:19:08.240
usiamo le piattaforme digitali
perché ci forniscono grande valore.

00:19:09.120 --> 00:19:12.680
Uso Facebook per tenere contatti con
amici e familiari in tutto il mondo.

00:19:14.000 --> 00:19:19.776
Ho scritto di come siano importanti
i social media per i movimenti sociali.

00:19:19.800 --> 00:19:22.816
Ho studiato come queste tecnologie
possono essere utilizzate

00:19:22.840 --> 00:19:25.320
per aggirare la censura nel mondo.

00:19:27.280 --> 00:19:33.696
Non credo che le persone che guidano
Facebook e Google

00:19:33.720 --> 00:19:36.416
stiano maliziosamente 
e deliberatamente tentando

00:19:36.440 --> 00:19:40.896
di rendere il paese o il mondo 
più polarizzati

00:19:40.920 --> 00:19:42.600
incoraggiando gli estremismi.

00:19:43.440 --> 00:19:47.416
Ho letto molte frasi piene
di buone intenzioni

00:19:47.440 --> 00:19:50.760
pronunciate da queste persone.

00:19:51.600 --> 00:19:57.656
Ma non sono le intenzioni o le frasi
di costoro a fare differenza,

00:19:57.680 --> 00:20:01.240
ma le strutture e i modelli di business 
che costoro stanno costruendo.

00:20:02.360 --> 00:20:04.456
E questo è il nocciolo del problema.

00:20:04.480 --> 00:20:09.200
O Facebook è una truffa gigante
da mezzo trilione di dollari

00:20:10.200 --> 00:20:12.096
dove le pubblicità non funzionano

00:20:12.120 --> 00:20:14.816
e non funziona 
come un'architettura persuasiva

00:20:14.840 --> 00:20:18.960
o il suo potere d'influenza
è fonte di grande preoccupazione.

00:20:20.560 --> 00:20:22.336
O l'uno, o l'altro.

00:20:22.360 --> 00:20:23.960
Per Google la storia è simile.

00:20:24.880 --> 00:20:27.336
Quindi, cosa possiamo fare?

00:20:27.360 --> 00:20:29.296
Serve un cambiamento.

00:20:29.320 --> 00:20:31.896
Quindi.
Non posso offrire una ricetta semplice,

00:20:31.920 --> 00:20:34.176
perché dobbiamo ristrutturare
per intero

00:20:34.200 --> 00:20:37.216
il modo in cui opera
la tecnologia digitale.

00:20:37.240 --> 00:20:41.336
Tutto: dal modo in cui la tecnologia
viene sviluppata

00:20:41.360 --> 00:20:45.216
alla maniera in cui gli incentivi,
economici o di altro tipo,

00:20:45.240 --> 00:20:47.520
sono costruiti dentro il sistema.

00:20:48.480 --> 00:20:51.936
Dobbiamo confrontarci e occuparci

00:20:51.960 --> 00:20:56.616
della mancanza di trasparenza
creata dagli algoritmi proprietari,

00:20:56.640 --> 00:21:00.456
la sfida strutturale
dell'opacità del "machine learning",

00:21:00.480 --> 00:21:03.880
tutti questi dati su di noi 
che vengono raccolti indiscriminatamente.

00:21:05.000 --> 00:21:07.520
Abbiamo un grande compito di fronte a noi.

00:21:08.160 --> 00:21:10.840
Dobbiamo mobilitare 
la nostra tecnologia,

00:21:11.760 --> 00:21:13.336
la nostra creatività

00:21:13.360 --> 00:21:15.240
e sì, la nostra politica

00:21:16.240 --> 00:21:18.896
così da costruire
un'intelligenza artificiale

00:21:18.920 --> 00:21:22.040
che ci supporti 
negli obiettivi umani

00:21:22.800 --> 00:21:26.720
ma che sia anche vincolata
dai nostri valori umani.

00:21:27.600 --> 00:21:29.760
Capisco che non sarà semplice.

00:21:30.360 --> 00:21:33.960
Potremmo non accordarci facilmente 
sul significato di questi termini.

00:21:34.920 --> 00:21:37.320
Ma se prendiamo seriamente

00:21:38.240 --> 00:21:44.216
come operano questi sistemi 
da cui dipendiamo così tanto,

00:21:44.240 --> 00:21:48.360
non vedo come possiamo rimandare ancora
questa discussione.

00:21:49.200 --> 00:21:51.736
Queste strutture

00:21:51.760 --> 00:21:55.856
stanno organizzando 
il modo in cui funzioniamo

00:21:55.880 --> 00:21:58.176
e stanno controllando

00:21:58.200 --> 00:22:00.816
ciò che possiamo e non possiamo fare.

00:22:00.840 --> 00:22:02.030
Molte piattaforme

00:22:02.030 --> 00:22:03.320
che si finanziano con pubblicità.

00:22:03.320 --> 00:22:04.896
si vantano di essere gratuite.

00:22:04.920 --> 00:22:09.480
In questo contesto, significa
che noi siamo i prodotti da vendere.

00:22:10.840 --> 00:22:13.576
Serve un'economia digitale

00:22:13.600 --> 00:22:17.096
dove i nostri dati e la nostra attenzione

00:22:17.120 --> 00:22:22.200
non sono venduti al miglior offerente
autoritario o demagogo.

00:22:23.160 --> 00:22:26.960
(Applausi)

00:22:30.480 --> 00:22:33.736
Così, tornando indietro 
a quella parafrasi di Hollywood,

00:22:33.760 --> 00:22:37.496
noi vogliamo che il potenziale prodigioso

00:22:37.520 --> 00:22:40.720
dell'intelligenza artificiale 
e della tecnologia digitale fiorisca,

00:22:41.400 --> 00:22:46.336
ma per farlo, dobbiamo affrontare
questa grande minaccia,

00:22:46.360 --> 00:22:48.296
con occhi aperti e ora.

00:22:48.320 --> 00:22:49.536
Grazie.

00:22:49.560 --> 00:22:54.200
(Applausi)

