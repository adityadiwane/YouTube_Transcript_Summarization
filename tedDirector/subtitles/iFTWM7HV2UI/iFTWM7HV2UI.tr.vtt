WEBVTT
Kind: captions
Language: tr

00:00:00.000 --> 00:00:07.000
Çeviri: Cihan Ekmekçi
Gözden geçirme: Gizem Dumlu

00:00:12.760 --> 00:00:16.296
İnsanlar yapay zekayla ilgili
korkularını dile getirdiğinde,

00:00:16.320 --> 00:00:20.296
genellikle kontrolden çıkmış
insansı robotları hayal ederler.

00:00:20.320 --> 00:00:21.560
Terminatör gibi.

00:00:22.400 --> 00:00:24.736
Düşünmeye değer olsa da

00:00:24.760 --> 00:00:26.616
uzak bir tehdit bu.

00:00:26.640 --> 00:00:30.096
Bazen de geçmişe özgü benzetmelerle

00:00:30.120 --> 00:00:31.896
dijital gözetlenme kaygısı taşıyoruz.

00:00:31.920 --> 00:00:34.576
George Orwell'ın ''1984'' adlı eseri

00:00:34.600 --> 00:00:36.880
şu an yine en çok satanlar listesinde.

00:00:37.960 --> 00:00:39.376
Harika bir kitap

00:00:39.400 --> 00:00:43.280
ama 21. yüzyıl için doğru distopya değil.

00:00:44.080 --> 00:00:45.496
En çok korkmamız gereken şey

00:00:45.520 --> 00:00:50.296
yapay zekanın kendi başına
bize ne yapacağı değil,

00:00:50.320 --> 00:00:55.056
güç sahibi insanların
bizi kontrol ve manipüle etmek adına

00:00:55.080 --> 00:00:57.896
yeni, bazen saklı

00:00:57.920 --> 00:01:01.056
bazen belirsiz ve beklenmeyen şekilde

00:01:01.080 --> 00:01:04.096
bunu nasıl kullanacakları.

00:01:04.120 --> 00:01:05.976
Yakın gelecekteki bağımsızlığımızı

00:01:06.000 --> 00:01:10.336
ve itibarımızı tehdit eden 
teknolojinin büyük kısmı

00:01:10.360 --> 00:01:12.216
veri ve dikkatimizi toplayıp

00:01:12.240 --> 00:01:17.176
reklamcı ve benzerlerine satan

00:01:17.200 --> 00:01:19.456
şirketler tarafından geliştiriliyor:

00:01:19.480 --> 00:01:22.896
Facebook, Google, Amazon,

00:01:22.920 --> 00:01:24.800
Alibaba, Tencent.

00:01:26.040 --> 00:01:31.536
Şimdi yapay zeka da onların işlerine
katkıda bulunmaya başladı.

00:01:31.560 --> 00:01:33.656
Yapay zeka, internet 
reklamcılığından sonra gelen

00:01:33.680 --> 00:01:36.536
yeni bir teknoloji gibi görünse de

00:01:36.560 --> 00:01:37.776
durum farklı.

00:01:37.800 --> 00:01:40.256
Söz konusu olan, 
ilgili alanda yepyeni bir açılım.

00:01:40.280 --> 00:01:42.856
Tamamen farklı bir dünya

00:01:42.880 --> 00:01:45.496
ve büyük potansiyeli var.

00:01:45.520 --> 00:01:52.540
Araştırma ve inceleme alanlarındaki
kavrayışımızı hızlandırabilir.

00:01:53.030 --> 00:01:56.616
Ancak ünlü bir Hollywood filozofundan
alıntı yapacak olursam,

00:01:56.640 --> 00:02:00.600
''Muhteşem potansiyel
muhteşem riskler barındırır.''

00:02:01.277 --> 00:02:03.880
Gelin, dijital hayatımızdaki 
temel bir gerçeğe bakalım...

00:02:03.880 --> 00:02:07.660
İnternet reklamları. Öyle değil mi? 
Onları yok sayıyoruz.

00:02:08.156 --> 00:02:10.086
Basit ve dikkat dağıtıcı görünüyorlar.

00:02:10.086 --> 00:02:14.060
Okuduğumuz veya arattığımız
bir konuyla ilgili reklamlar tarafından

00:02:14.060 --> 00:02:17.056
internette takip edilme tecrübesini
hepimiz yaşadık.

00:02:17.080 --> 00:02:18.936
Hani bir çift botun fiyatına bakarsınız

00:02:18.960 --> 00:02:22.336
ve sonra bütün hafta girdiğiniz
her sayfada botlar sizi takip eder.

00:02:22.360 --> 00:02:26.016
Karşı koyamayıp satın aldıktan sonra bile
sizi takip ederler.

00:02:26.040 --> 00:02:29.056
Bu basit ve ucuz manipülasyonu
adeta kanıksamış durumdayız.

00:02:29.080 --> 00:02:32.480
Göz devirip kendi kendimize
''İşe yaramıyor bunlar.'' diyoruz.

00:02:33.720 --> 00:02:35.816
Ne var ki internet ortamında,

00:02:35.840 --> 00:02:39.440
dijital teknolojiler
reklamlardan ibaret değil.

00:02:40.240 --> 00:02:43.360
Bunu anlamak için
fiziksel bir dünya örneği ele alalım.

00:02:43.840 --> 00:02:48.496
Süpermarketlerde
kasaların hemen yanında

00:02:48.520 --> 00:02:52.000
çocukların göz hizasında
şekerleme ve sakız olur.

00:02:52.800 --> 00:02:56.296
İlgili düzenek, aileler tam
marketten çıkmak üzereyken

00:02:56.320 --> 00:02:59.400
çocuklarının bunları ısrarla istemeleri
için tasarlanmıştır.

00:03:00.040 --> 00:03:02.680
Bu bir ikna mimarisi.

00:03:03.160 --> 00:03:06.256
Pek hoş değil ama işe yarıyor.

00:03:06.280 --> 00:03:08.320
Bu yüzden de her süpermarkette görüyoruz.

00:03:08.720 --> 00:03:10.416
Fiziksel dünyada,

00:03:10.440 --> 00:03:12.936
bu ikna mimarileri sınırlıdır,

00:03:12.960 --> 00:03:17.776
çünkü kasiyerin yanına koyabileceğiniz
şeylerin bir sınırı var, değil mi?

00:03:17.800 --> 00:03:22.096
Şeker ve sakız herkes için aynı,

00:03:22.120 --> 00:03:23.576
her ne kadar yanında

00:03:23.600 --> 00:03:27.640
sızlanan çocuklar olan 
aileler için işe yarasa da.

00:03:29.160 --> 00:03:33.080
Fiziksel dünyada bu sınırlarla yaşıyoruz.

00:03:34.280 --> 00:03:36.216
Ancak dijital dünyada,

00:03:36.240 --> 00:03:40.560
ikna mimarisi, milyarlara erişecek 
şekilde inşa edilebilir

00:03:41.726 --> 00:03:45.696
ve aynı zamanda bireyleri

00:03:45.720 --> 00:03:48.120
teker teker hedef alarak anlayabilir

00:03:48.120 --> 00:03:49.856
onların zayıf noktalarını tespit ederek

00:03:49.880 --> 00:03:52.016
kişisel seviyede nüfuz edebilir.

00:03:52.040 --> 00:03:57.656
hatta herkesin kişisel telefon ekranına
bile gönderilebilir,

00:03:57.680 --> 00:03:59.936
böylelikle bizler görmeyiz.

00:03:59.960 --> 00:04:01.216
Ve bu oldukça farklı.

00:04:01.240 --> 00:04:04.816
Bu, yapay zekanın yapabileceği
temel şeylerden yalnızca biri.

00:04:04.840 --> 00:04:06.176
Bir örnek verelim.

00:04:06.200 --> 00:04:08.896
Diyelim ki Las Vegas'a uçak bileti 
satmak istiyorsunuz.

00:04:08.920 --> 00:04:12.416
Eski düzende,
deneyim ve öngörülerinize dayanarak

00:04:12.440 --> 00:04:14.960
hedef bir demografik kesim belirlersiniz.

00:04:15.560 --> 00:04:18.376
Reklam yapmayı da deneyebilirsiniz,

00:04:18.400 --> 00:04:20.896
25 - 35 yaş aralığındaki erkekler

00:04:20.920 --> 00:04:24.856
veya kredi kartı limiti 
yüksek olan insanlar

00:04:24.880 --> 00:04:26.256
veya emekli çiftler, değil mi?

00:04:26.280 --> 00:04:28.096
Geçmişte böyle yapardınız.

00:04:28.120 --> 00:04:31.016
Şimdi büyük veri ve makine öğrenimi ile

00:04:31.040 --> 00:04:32.564
işler artık böyle yürümüyor.

00:04:33.320 --> 00:04:35.496
Bunu anlamak için,

00:04:35.520 --> 00:04:39.376
Facebook'un sizinle ilgili
sahip olduğu tüm verileri düşünün:

00:04:39.400 --> 00:04:41.936
Yazdığınız her durum bildirisi,

00:04:41.960 --> 00:04:43.976
her bir Messenger sohbeti,

00:04:44.000 --> 00:04:45.880
oturum açtığınız her konum,

00:04:48.400 --> 00:04:51.576
yüklediğiniz tüm fotoğraflar.

00:04:51.600 --> 00:04:55.376
Bir şey yazmaya başlayıp
sonra vazgeçip silerseniz,

00:04:55.400 --> 00:04:58.600
Facebook bu silinenleri de
saklayıp analiz ediyor.

00:04:59.160 --> 00:05:03.096
Çevrimdışı verilerinizle sizi
gitgide eşleştirmeye çalışıyor.

00:05:03.120 --> 00:05:06.296
Ayrıca veri acentalarından da
çok fazla veri satın alıyor.

00:05:06.320 --> 00:05:09.736
Finansal kayıtlarınızdan
tarama geçmişinize kadar

00:05:09.760 --> 00:05:11.880
her şey bu veri setinde olabilir.

00:05:12.360 --> 00:05:17.776
ABD'de bu tür veriler
rutin olarak toplanıyor,

00:05:17.800 --> 00:05:19.760
karşılaştırılıyor ve satılıyor.

00:05:20.320 --> 00:05:22.760
Avrupa'da daha sıkı kurallar var.

00:05:23.680 --> 00:05:25.880
Yani aslında olan şey,

00:05:26.920 --> 00:05:30.936
tüm bu veriler harmanlanarak,
bu makine öğrenimli algoritmalar -

00:05:30.960 --> 00:05:33.856
onlara bu yüzden öğrenen 
algoritmalar deniyor -

00:05:33.880 --> 00:05:37.976
Daha önce Las Vegas'a gitmek için 
uçak bileti alan insanların

00:05:38.000 --> 00:05:40.520
özelliklerini nasıl ayrıştıracaklarını
öğreniyorlar.

00:05:41.760 --> 00:05:45.296
Var olan verilerden bunu öğrendiklerinde,

00:05:45.320 --> 00:05:49.136
bunu yeni insanlara 
uygulamayı da öğreniyorlar.

00:05:49.160 --> 00:05:52.216
Böylece, yeni bir bireyle 
karşılaştıklarında

00:05:52.240 --> 00:05:56.880
onun Vegas'a bilet alıp almayacağını
sınıflandırabiliyorlar.

00:05:57.720 --> 00:06:03.176
Olsun, diye düşünüyorsunuz,
alt tarafı Vegas'a uçak bileti teklifi.

00:06:03.200 --> 00:06:04.656
Görmezden gelebilirim.

00:06:04.680 --> 00:06:06.896
Ancak asıl sorun bu değil.

00:06:06.920 --> 00:06:08.496
Asıl sorun şu ki

00:06:08.520 --> 00:06:12.656
biz bu karmaşık algoritmaların
nasıl çalıştığını artık anlamıyoruz.

00:06:12.680 --> 00:06:16.136
Bu sınıflandırmayı nasıl yaptıklarını
artık anlamıyoruz.

00:06:16.160 --> 00:06:20.576
Dev matematik matrisleri,
binlerce sıra ve sütun,

00:06:20.600 --> 00:06:22.560
belki de milyonlarcası...

00:06:23.320 --> 00:06:25.960
Ve tüm verilere sahip olsalar bile,

00:06:26.760 --> 00:06:28.440
ne programcılar,

00:06:29.440 --> 00:06:30.936
ne bunları inceleyen herhangi biri

00:06:30.960 --> 00:06:35.576
bunun tam olarak nasıl 
işlediğini anlayabiliyor.

00:06:35.600 --> 00:06:39.376
Tıpkı size beynimden bir kesit göstersem

00:06:39.400 --> 00:06:43.360
ne düşündüğümü anlayamayacağınız gibi.

00:06:44.360 --> 00:06:46.936
Sanki artık programlama yapmıyoruz,

00:06:46.960 --> 00:06:51.360
tam olarak anlayamadığımız 
bir bilinç geliştiriyoruz.

00:06:52.520 --> 00:06:56.496
Ve bu mekanizmalar yalnızca
müthiş miktarda veri varsa çalışıyor,

00:06:56.520 --> 00:07:01.616
dolayısı ile hepimizin üzerinde kapsamlı 
bir gözetleme de teşvik ediliyor ki

00:07:01.640 --> 00:07:03.976
makine öğrenimli algoritmalar
işini yapabilsin.

00:07:04.000 --> 00:07:07.176
Bu yüzden Facebook, hakkınızda 
toplayabildiği tüm veriyi istiyor.

00:07:07.200 --> 00:07:08.776
Algoritmalar daha iyi çalışıyor.

00:07:08.800 --> 00:07:11.496
Şu Vegas örneğinin biraz üstüne gidelim.

00:07:11.520 --> 00:07:15.200
Ya anlamadığımız bu sistem

00:07:16.200 --> 00:07:21.336
mani döneme geçmek üzere olan
bipolar insanlara

00:07:21.360 --> 00:07:25.120
Vegas bileti satmanın 
daha kolay olduğunu anlarsa?

00:07:25.640 --> 00:07:30.560
Bu insanlar çok para harcamaya
ve dürtüsel kumarbazlığa meyilli oluyor.

00:07:31.280 --> 00:07:35.736
Bunu yapabilirler ve söz konusu kriteri 
seçtiklerinden haberiniz bile olmaz.

00:07:35.760 --> 00:07:39.376
Bu örneği,
bir grup bilgisayar bilimcisine verdim

00:07:39.400 --> 00:07:41.456
sonra içlerinden biri yanıma geldi.

00:07:41.480 --> 00:07:45.000
Rahatsız olmuştu ve şöyle dedi:
''İşte bu yüzden yayınlayamadım.''

00:07:45.600 --> 00:07:47.315
''Neyi yayınlayamadın?'' dedim.

00:07:47.800 --> 00:07:53.656
Mani halinin ön belirtilerinin
klinik semptomlardan önce

00:07:53.680 --> 00:07:56.896
sosyal medya paylaşımlarından
anlaşılabilirliğini incelemişti

00:07:56.920 --> 00:07:58.696
ve işe yaramıştı,

00:07:58.720 --> 00:08:00.776
gerçekten işe yaramıştı

00:08:00.800 --> 00:08:05.680
ama nasıl işe yaradığı veya ne tür 
bilgi topladığını o da bilmiyordu.

00:08:06.840 --> 00:08:11.256
Yayınlamadığı zaman problem çözülmüyor,

00:08:11.280 --> 00:08:13.176
çünkü zaten bu teknolojiyi geliştiren

00:08:13.200 --> 00:08:15.736
şirketler var.

00:08:15.760 --> 00:08:18.560
Bunun pek çoğu satışa hazır.

00:08:19.240 --> 00:08:21.816
Artık bunu yapmak çok zor değil.

00:08:21.840 --> 00:08:25.296
Tek bir video izlemek için YouTube'a girip

00:08:25.320 --> 00:08:27.680
bir saat sonra 27 video izlediğiniz
oluyor mu hiç?

00:08:28.760 --> 00:08:31.256
YouTube'ta sağ tarafta

00:08:31.280 --> 00:08:33.496
''Sıradaki'' diye bir sütun var

00:08:33.520 --> 00:08:35.336
ve otomatik yeni video başlatıyor.

00:08:35.360 --> 00:08:36.576
Bu bir algoritma,

00:08:36.600 --> 00:08:40.216
ilgilendiğinizi ve kendi başınıza
bulamayacağınızı düşündüğü

00:08:40.240 --> 00:08:41.776
videoları seçiyor.

00:08:41.800 --> 00:08:43.056
Editör bir insan değil.

00:08:43.080 --> 00:08:44.496
Algoritmaların işi bu.

00:08:44.520 --> 00:08:49.256
Sizin ve sizin gibi insanların
izlediklerini derliyor

00:08:49.280 --> 00:08:53.496
ve ilgi alanlarınızın bunlar olduğu
ve daha fazlasını görmek istediğiniz

00:08:53.520 --> 00:08:54.775
çıkarımını yapıyor,

00:08:54.799 --> 00:08:56.135
daha fazlasını gösteriyor.

00:08:56.159 --> 00:08:58.360
İyi, faydalı bir özelllik gibi görünüyor

00:08:59.280 --> 00:09:00.480
ama öyle değil.

00:09:01.640 --> 00:09:08.600
2016'da o zaman aday olan
Trump'ın toplantılarına

00:09:09.840 --> 00:09:13.176
destekçilerini araştırmak üzere
akademisyen olarak katıldım.

00:09:13.200 --> 00:09:16.656
İşim gereği sosyal akımları inceliyorum,
yani araştırıyordum da.

00:09:16.680 --> 00:09:20.016
Sonra toplantılarından biri hakkında
yazmak istedim,

00:09:20.040 --> 00:09:22.590
o yüzden de toplantıyı 
YouTube'da birkaç kez izledim.

00:09:23.240 --> 00:09:26.336
YouTube, beyaz ırk üstünlüğü ile ilgili

00:09:26.360 --> 00:09:30.616
radikallik seviyesi giderek artan 
videolar önermeye

00:09:30.640 --> 00:09:33.296
ve onları otomatik oynatmaya başladı.

00:09:33.320 --> 00:09:35.136
Eğer bir tane izlediysem,

00:09:35.160 --> 00:09:38.136
YouTube daha marjinal bir tanesini buldu

00:09:38.160 --> 00:09:39.584
ve onu da otomatik yürüttü.

00:09:40.320 --> 00:09:44.856
Hillary Clinton veya Bernie Sanders
ile ilgili içerikler izlerseniz,

00:09:44.880 --> 00:09:49.576
YouTube komplocu solcuları
öneriyor ve oynatıyor,

00:09:49.600 --> 00:09:51.360
ondan sonra da gittikçe kötüleşiyor.

00:09:52.480 --> 00:09:55.536
Bunun yalnızca siyaset olduğunu
düşünebilirsiniz ama değil.

00:09:55.560 --> 00:09:56.816
Bu siyasetle ilgili değil.

00:09:56.840 --> 00:09:59.936
Bu sadece insan davranışını
anlayan algoritma.

00:09:59.960 --> 00:10:04.736
Bir kez YouTube'ta vejeteryanlıkla ilgili
bir video izledim

00:10:04.760 --> 00:10:09.696
ve YouTube vegan olmak hakkında
bir video önerip oynattı.

00:10:09.720 --> 00:10:12.736
YouTube için hiçbir zaman
yeteri kadar cüretkar olamıyoruz.

00:10:12.760 --> 00:10:14.336
(Gülüşmeler)

00:10:14.360 --> 00:10:15.920
Peki aslında ne oluyor?,

00:10:16.520 --> 00:10:20.056
YouTube algoritması patentli,

00:10:20.080 --> 00:10:22.440
yine de şöyle olduğunu düşünüyorum.

00:10:23.360 --> 00:10:25.456
Algoritma şunu fark etti ki

00:10:25.480 --> 00:10:29.176
insanları etkilemek için

00:10:29.200 --> 00:10:32.936
onlara daha cüretkar videolar sunarsan,

00:10:32.960 --> 00:10:35.376
muhtemelen sitede daha fazla kalacak,

00:10:35.400 --> 00:10:39.816
o anlaşılmaz yola girerek
ardı ardına video izleyecek,

00:10:39.840 --> 00:10:41.890
bu esnada Google da 
reklam sunacak.

00:10:43.760 --> 00:10:47.710
Hazır, işin etik kısmını 
önemseyen kimse de yokken,

00:10:47.720 --> 00:10:50.930
bu siteler,

00:10:50.930 --> 00:10:55.600
Yahudiler aleyhine paylaşım yapan

00:10:56.360 --> 00:10:58.840
ve onların parazit olduğunu düşünen

00:11:00.320 --> 00:11:05.240
radikal Yahudi düşmanları özelinde
profilleme yapabiliyor

00:11:06.080 --> 00:11:08.270
ve reklamlarla onları 
hedeflemenizi sağlıyor.

00:11:09.200 --> 00:11:12.736
Ayrıca algoritmaları genişleterek,

00:11:12.760 --> 00:11:15.896
sizin için benzer kitleler bulup

00:11:15.920 --> 00:11:21.496
profillerinde bu tip, Yahudi karşıtı, 
aykırı içerik bulunmayan

00:11:21.520 --> 00:11:24.720
fakat algoritmanın bu tür mesajlara karşı

00:11:24.720 --> 00:11:27.720
duyarlı olabileceğini belirlediği 
kişileri yakalıyor

00:11:27.720 --> 00:11:30.200
ve onları da reklamlarla 
hedeflemenize izin veriyor.

00:11:30.680 --> 00:11:33.416
İnanılmaz bir örnek gibi gelebilir

00:11:33.440 --> 00:11:34.760
ama bu gerçek.

00:11:35.480 --> 00:11:37.616
ProPublica bunu soruşturdu

00:11:37.640 --> 00:11:41.256
ve Facebook'ta bunu gerçekten
yapabileceğinizi ortaya koydu,

00:11:41.280 --> 00:11:43.696
Facebook ilgili kitleyi genişletmede

00:11:43.720 --> 00:11:45.320
öneriler sunarak yardımcı oldu.

00:11:46.720 --> 00:11:49.736
BuzzFeed bunu Google için denedi
ve hızla anladılar ki

00:11:49.760 --> 00:11:51.496
bunu Google'da da yapabiliyoruz.

00:11:51.520 --> 00:11:53.216
Pahalı bile değildi.

00:11:53.240 --> 00:11:57.656
ProPublica habercisi
bu kategoriyi hedeflemek için

00:11:57.680 --> 00:11:59.920
30 dolar kadar harcadı.

00:12:02.600 --> 00:12:07.896
Geçen sene Trump'ın sosyal medya yetkilisi
kargaşayı sona erdirmek için

00:12:07.920 --> 00:12:13.256
gizli Facebook paylaşımları 
kullandıklarını açıkladı,

00:12:13.280 --> 00:12:14.656
insanları ikna için değil,

00:12:14.680 --> 00:12:17.480
hiç oy vermemelerini sağlamak için.

00:12:18.520 --> 00:12:22.096
Bunu yapmak için 
özel olarak hedef belirlediler,

00:12:22.120 --> 00:12:26.016
mesela önemli Philadelphia kentlerindeki 
Afro Amerikalı erkekler,

00:12:26.040 --> 00:12:28.496
hatta tam olarak ne dediğini okuyacağım.

00:12:28.520 --> 00:12:30.156
Alıntı yapıyorum.

00:12:30.526 --> 00:12:34.426
''Görülebilirliğini siyasi kampanyanın
kontrol ettiği

00:12:34.426 --> 00:12:37.266
böylelikle yalnızca görmesini istediğimiz
insanların görebileceği

00:12:37.266 --> 00:12:38.846
herkese açık olmayan paylaşımlar.

00:12:38.846 --> 00:12:40.016
Bunu biz tasarladık.

00:12:40.040 --> 00:12:42.720
Bu, onun söz konusu insanları 

00:12:42.720 --> 00:12:45.720
kazanma yetisini önemli 
ölçüde etkileyecektir.''

00:12:45.720 --> 00:12:48.000
Bu gizli paylaşımlarda ne var peki?

00:12:48.480 --> 00:12:50.136
Hiçbir fikrimiz yok.

00:12:50.160 --> 00:12:51.730
Facebook bize açıklamıyor.

00:12:52.480 --> 00:12:56.856
Facebook ayrıca algoritmik bir şekilde
arkadaşlarınızın paylaşımlarını

00:12:56.880 --> 00:13:00.616
ve takip ettiğiniz sayfaları düzenliyor.

00:13:00.640 --> 00:13:02.856
Size her şeyi 
kronolojik olarak göstermiyor.

00:13:02.880 --> 00:13:07.696
Algoritmanın, sitede daha fazla 
kalmanızı sağlayacak şekilde

00:13:07.720 --> 00:13:09.560
kurduğu düzeni uyguluyor.

00:13:11.040 --> 00:13:14.416
Bunun pek çok sonucu var.

00:13:14.440 --> 00:13:18.240
Facebook'ta birinin takipçiniz olduğunu
düşünüyor olabiliirsiniz.

00:13:18.800 --> 00:13:22.056
Oysa algoritma sizin paylaşımınızı
asla onlara göstermiyor olabilir.

00:13:22.080 --> 00:13:28.040
Algoritma kimini öne çıkarırken
kimini ortadan kaldırıyor.

00:13:29.320 --> 00:13:30.616
Deneyler gösteriyor ki

00:13:30.640 --> 00:13:35.160
algoritmanın sizin için seçtikleri
duygularınızı etkileyebilir.

00:13:36.600 --> 00:13:37.800
Bununla da bitmiyor.

00:13:38.280 --> 00:13:40.640
Siyasi davranışınızı da etkiliyor.

00:13:41.360 --> 00:13:46.016
2010 yılı orta dönem seçimlerinde,

00:13:46.040 --> 00:13:51.936
Facebook, 
ABD'deki 61 milyon insan üstünde

00:13:51.960 --> 00:13:53.856
daha sonra açıklanan bir deney yaptı.

00:13:53.880 --> 00:13:57.296
Bir grup insana
''Bugün seçim günü'' yazısı gösterildi,

00:13:57.320 --> 00:13:58.696
bu daha basit olandı,

00:13:58.720 --> 00:14:02.616
diğer bir gruba ise aynı şey, 
küçük bir farkla gösterildi:

00:14:02.640 --> 00:14:05.336
''Oy verdim'' butonuna 
tıklayan arkadaşlarının

00:14:05.336 --> 00:14:08.160
küçük fotoğraflarının bulunduğu versiyon.

00:14:09.110 --> 00:14:10.400
Bu kadar basit bir nüans.

00:14:11.520 --> 00:14:15.474
Değişen tek şey fotoğraflardı

00:14:15.919 --> 00:14:18.529
ve seçmen kütüğünce de onaylandığı üzere,

00:14:18.893 --> 00:14:22.760
bu araştırmaya istinaden

00:14:22.760 --> 00:14:24.640
yalnızca bir kez gösterilen bu paylaşım

00:14:24.853 --> 00:14:26.184
o seçimde

00:14:26.507 --> 00:14:31.160
340.000 ek seçmen olarak sonuçlandı.

00:14:32.920 --> 00:14:34.576
Şans eseri mi? Hayır.

00:14:34.600 --> 00:14:39.960
Çünkü 2012'de aynı deneyi tekrarladılar.

00:14:40.840 --> 00:14:42.576
O zaman,

00:14:42.600 --> 00:14:45.896
yalnızca bir kez gösterilen sivil mesaj

00:14:45.920 --> 00:14:50.360
270.000 ek seçmen olarak geri döndü.

00:14:51.160 --> 00:14:56.376
Hatırlatayım, 2016 ABD
başkanlık seçimleri

00:14:56.400 --> 00:14:59.920
yaklaşık 100.000 oy farkıyla belirlendi.

00:15:01.360 --> 00:15:06.096
Yani Facebook kolaylıkla
politikanız hakkında çıkarım yapabiliyor,

00:15:06.120 --> 00:15:08.376
siz bunu sitede 
hiç açıklamamış olsanız bile.

00:15:08.400 --> 00:15:10.920
Bu algoritmalar bunu
oldukça kolay başarabiliyorlar.

00:15:11.960 --> 00:15:15.856
Peki ya bu güce sahip bir platform

00:15:15.880 --> 00:15:20.920
bunu adaylardan birinin
destekçilerini arttırmak için kullanırsa?

00:15:21.680 --> 00:15:24.120
Bundan haberimiz olur mu?

00:15:25.560 --> 00:15:29.696
Masum gibi görünen bir yerden başladık:

00:15:29.720 --> 00:15:31.936
Bizi takip eden reklamlardan...

00:15:31.960 --> 00:15:33.800
şimdiyse çok farklı bir yerdeyiz.

00:15:35.480 --> 00:15:37.936
Hem halk hem de vatandaş olarak,

00:15:37.960 --> 00:15:41.376
artık aynı bilgileri görüp görmediğimizi

00:15:41.400 --> 00:15:43.270
ve başkalarının ne gördüğünü bilmiyoruz

00:15:43.680 --> 00:15:46.256
ve ortak bir bilgi tabanı olmadan,

00:15:46.280 --> 00:15:47.896
adım adım,

00:15:47.920 --> 00:15:51.136
toplumsal tartışma imkansız hale geliyor,

00:15:51.160 --> 00:15:54.136
biz bunun sadece başlangıç aşamasındayız.

00:15:54.160 --> 00:15:57.616
Bu algoritmalar kolaylıkla

00:15:57.640 --> 00:16:00.896
insanların etnik özelliklerini,

00:16:00.920 --> 00:16:03.256
dini ve siyasi görüşlerini,
kişilik özelliklerini,

00:16:03.280 --> 00:16:06.656
zekasını, mutluluğunu,
madde kullanıp kullanmadığını,

00:16:06.680 --> 00:16:09.816
ailesinin durumunu, yaş ve cinsiyetini

00:16:09.840 --> 00:16:11.800
sadece Facebook beğenilerinden 
tahmin edebilir.

00:16:13.440 --> 00:16:17.496
Bu algoritmalar, yüzleri 
kısmen gizlenmiş olsa da

00:16:17.520 --> 00:16:20.280
protestocuların kimliğini belirleyebilir.

00:16:21.720 --> 00:16:28.336
Bu algoritmalar 
insanların cinsel yönelimini

00:16:28.360 --> 00:16:31.560
flört uygulamalarında kullandığı 
profil fotoğraflarından anlayabilir.

00:16:33.560 --> 00:16:36.176
Tabii bunlar olasılıksal tahminler,

00:16:36.200 --> 00:16:39.096
%100 doğru olamazlar

00:16:39.120 --> 00:16:44.016
ama insanlar sadece bazı 
sonuçlar yanlış olduğu için

00:16:44.040 --> 00:16:46.216
bu teknolojileri kullanma 
arzularına direnmeyecekler

00:16:46.240 --> 00:16:49.496
bu da beraberinde 
bir yığın farklı sorun getirecek.

00:16:49.520 --> 00:16:52.456
Devletlerin vatandaşları 
hakkında sahip oldukları

00:16:52.480 --> 00:16:56.040
müthiş miktarda veriyle
neler yapabileceklerini düşünün.

00:16:56.680 --> 00:17:01.456
Çin, insanları tespit etmek 
ve tutuklamak için

00:17:01.480 --> 00:17:04.360
yüz tanıma teknolojisini kullanıyor bile.

00:17:05.280 --> 00:17:07.416
İşin acı kısmı şu ki biz,

00:17:07.440 --> 00:17:12.976
gözetlemeye dayalı bu otoriter altyapıyı

00:17:13.000 --> 00:17:15.960
yalnızca insanların 
reklamlara tıklaması için geliştiriyoruz.

00:17:17.240 --> 00:17:19.816
Bu Orwell'in otoriter rejimi olmayacak.

00:17:19.839 --> 00:17:21.736
Bu ''1984'' değil.

00:17:21.760 --> 00:17:26.336
Eğer otoriterlik bizi paniğe sürüklemek 
için aleni korku kullanacaksa

00:17:26.359 --> 00:17:29.256
hepimiz korkacağız
ama bundan haberimiz olacak,

00:17:29.280 --> 00:17:31.480
nefret duyacağız ve karşı koyacağız.

00:17:32.880 --> 00:17:37.296
Ancak mevki sahibi insanlar 
bu algoritmaları

00:17:37.319 --> 00:17:40.696
bizi sessizce izlemek,

00:17:40.720 --> 00:17:42.800
yargılamak ve dürtmek,

00:17:43.720 --> 00:17:47.896
sorun çıkaranlar ve asileri önceden 
tahmin etmek ve kimliğini belirlemek,

00:17:47.920 --> 00:17:51.816
üzerimizde ikna mimarisi oluşturmak

00:17:51.840 --> 00:17:55.976
ve tek tek bireyleri manipüle etmek için

00:17:56.000 --> 00:18:01.440
kişisel zayıf ve hassas noktalarımızdan
yararlanarak kullanırlarsa,

00:18:02.720 --> 00:18:04.920
dahası bunu ölçeklendirip

00:18:06.080 --> 00:18:07.816
özel ekranlarımızdan

00:18:07.840 --> 00:18:09.496
çevremizdeki insanların

00:18:09.520 --> 00:18:12.280
ne gördüklerini bilemeyeceğimiz
bir şekilde yaparlarsa,

00:18:13.560 --> 00:18:18.376
bu otoriter rejim
bizi bir örümcek ağı gibi kıstırır

00:18:18.400 --> 00:18:20.880
ve biz yakalandığımızı bile anlamayız.

00:18:22.440 --> 00:18:25.376
Facebook'un piyasa değeri

00:18:25.400 --> 00:18:28.696
yarım trilyon dolara yaklaşıyor.

00:18:28.720 --> 00:18:31.840
Bunun sebebi ikna mimarisi olarak
harika çalışıyor olması.

00:18:33.760 --> 00:18:36.576
Ancak bu mimari yapı

00:18:36.600 --> 00:18:39.816
ayakkabı satıyor olsanız da aynı

00:18:39.840 --> 00:18:42.336
siyaset satıyor olsanız da.

00:18:42.360 --> 00:18:45.480
Algoritmalar farkı anlamıyor.

00:18:46.240 --> 00:18:49.536
Reklamlara karşı bizi sabırlı kılmak için

00:18:49.560 --> 00:18:52.736
üzerimize salınan bu algoritmalar,

00:18:52.760 --> 00:18:59.496
aynı zamanda siyasi, kişisel ve sosyal
bilgi akışımızı da düzenliyor

00:18:59.520 --> 00:19:01.360
ve bu değişmek zorunda.

00:19:02.240 --> 00:19:04.536
Beni yanlış anlamayın,

00:19:04.560 --> 00:19:08.240
bize büyük fayda sağladıkları için
dijital platformları kullanıyoruz.

00:19:09.120 --> 00:19:13.280
Facebook ile dünyanın her yerinden
aile ve arkadaşlarımla görüşebiliyorum,

00:19:14.000 --> 00:19:19.776
Sosyal medyanın, sosyal hareketler için
ne kadar önemli olduğu hakkında yazdım.

00:19:19.800 --> 00:19:22.816
Bu teknolojilerin dünyadaki sansür 
uygulamalarının üstesinden gelmek için

00:19:22.840 --> 00:19:25.320
nasıl kullanılabileceği üzerine çalıştım.

00:19:27.280 --> 00:19:33.696
Facebook ve Google yöneticilerinin

00:19:33.720 --> 00:19:36.416
kasten ve kötü niyetli bir şekilde

00:19:36.440 --> 00:19:40.140
ülkeyi ve dünyayı kutuplaştırmaya

00:19:40.140 --> 00:19:42.600
veya radikalliği teşvik etmeye 
çalıştığını söylemiyorum.

00:19:43.440 --> 00:19:47.416
Bu insanların yayınladığı

00:19:47.440 --> 00:19:50.760
pek çok iyi niyetli yazı okudum.

00:19:51.600 --> 00:19:57.656
Ancak bu konuda 
niyet veya ifadelerin bir önemi yok.

00:19:57.680 --> 00:20:01.240
Sorun, inşa ettikleri bu yapı 
ve iş modelleri.

00:20:02.360 --> 00:20:04.456
Sorunun kökeninde bu var.

00:20:04.480 --> 00:20:09.200
Ya Facebook yarım trilyon değerinde
dev bir yapı

00:20:10.200 --> 00:20:12.096
ve reklamlar bu sitede çalışmıyor,

00:20:12.120 --> 00:20:14.816
ikna mimarisi olarak faaliyet göstermiyor

00:20:14.840 --> 00:20:18.960
ya da etki gücü dehşet verici.

00:20:20.560 --> 00:20:22.336
İkisinden biri.

00:20:22.360 --> 00:20:23.960
Google için de aynısı söz konusu.

00:20:24.880 --> 00:20:27.336
Peki ne yapabiliriz?

00:20:27.360 --> 00:20:29.296
Bunun değişmesi gerekiyor.

00:20:29.320 --> 00:20:31.896
Basit bir formül öneremem

00:20:31.920 --> 00:20:34.176
çünkü dijital teknolojimizin
çalışma şeklini

00:20:34.200 --> 00:20:37.216
yeniden inşa etmemiz gerekiyor.

00:20:37.240 --> 00:20:41.336
Teknolojinin geliştirilme biçiminden, 

00:20:41.360 --> 00:20:45.216
Ekonomik veya diğer 
alanlardaki teşviklerin

00:20:45.240 --> 00:20:47.520
sisteme taşınmasına kadar her şey...

00:20:48.480 --> 00:20:51.936
Tescilli algoritmalar tarafından yaratılan
şeffaflık noksanlığı,

00:20:51.960 --> 00:20:56.616
makine öğrenimi anlaşılmazlığının
yapısal zorluğu,

00:20:56.640 --> 00:21:00.456
ve hakkımızda toplanmakta olan
tüm bu gelişigüzel veri

00:21:00.480 --> 00:21:03.880
gibi meselelerle yüzleşmek ve bunların 
üstesinden gelmeye çalışmak zorundayız.

00:21:05.000 --> 00:21:07.520
Bize büyük bir görev düşüyor.

00:21:08.160 --> 00:21:10.840
Teknolojimizi, yaratıcılığımızı,

00:21:11.760 --> 00:21:13.336
ve evet, siyasetimizi

00:21:13.360 --> 00:21:15.240
harekete geçirmemiz lazım,

00:21:16.240 --> 00:21:22.256
böylece kişisel amaçlarımızda 
bizi destekleyen

00:21:22.256 --> 00:21:24.780
fakat insani değerlere de bağlı

00:21:24.780 --> 00:21:26.720
yapay zekayı inşa edebiliriz.

00:21:27.600 --> 00:21:29.760
Bunun kolay olmayacağını biliyorum.

00:21:30.360 --> 00:21:33.960
Bu terimlerin ne anlama geldiği
konusunda bile kolayca anlaşamayabiliriz.

00:21:34.920 --> 00:21:38.000
Ancak sürekli ihtiyaç duyduğumuz

00:21:38.240 --> 00:21:44.216
bu sistemlerin nasıl çalıştığını
ciddiye alırsak,

00:21:44.240 --> 00:21:48.360
Bu konuşmayı ertelemek için
hiçbir sebep göremiyorum.

00:21:49.200 --> 00:21:51.736
Bu yapılar,

00:21:51.760 --> 00:21:55.856
bizim işleyişimizi düzenliyor

00:21:55.880 --> 00:21:58.526
ve ne yapıp ne yapamayacağımızı

00:21:58.526 --> 00:21:59.946
kontrol ediyor.

00:22:00.710 --> 00:22:03.296
Reklamla finanse edilen 
bu platformların çoğu

00:22:03.320 --> 00:22:04.896
ücretsiz olmakla övünüyorlar.

00:22:04.920 --> 00:22:09.480
Bu bağlamda, bunun anlamı şu: 
Satılmakta olan ürün biziz.

00:22:10.840 --> 00:22:13.826
Veri ve dikkatimizin

00:22:13.826 --> 00:22:17.706
en yüksek ücreti veren 
otoriter veya demagoga satılmadığı

00:22:17.706 --> 00:22:22.200
bir dijital ekonomiye ihtiyacımız var.

00:22:23.160 --> 00:22:26.960
(Alkış)

00:22:30.480 --> 00:22:33.736
Şu Hollywood sözüne
geri dönmek gerekirse,

00:22:33.760 --> 00:22:38.136
yapay zeka ve dijital teknolojinin müthiş potansiyelinin

00:22:38.136 --> 00:22:40.720
çiçek açmasını elbette istiyoruz,

00:22:41.400 --> 00:22:46.336
fakat bunun olması için
bu müthiş tehditle yüzleşmemiz lazım,

00:22:46.360 --> 00:22:48.296
gözlerimiz tamamen açık ve şimdi.

00:22:48.320 --> 00:22:49.536
Teşekkür ederim.

00:22:49.560 --> 00:22:54.200
(Alkış)

