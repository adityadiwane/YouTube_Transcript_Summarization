WEBVTT
Kind: captions
Language: tr

00:00:00.000 --> 00:00:07.000
Çeviri: gizem köseoğlu
Gözden geçirme: Eren Gokce

00:00:12.780 --> 00:00:15.756
Hikâye 1985 yılında,

00:00:15.780 --> 00:00:17.756
ben daha 22 yaşımdayken

00:00:17.780 --> 00:00:20.156
Anatoly Karpov'u yenip

00:00:20.180 --> 00:00:23.380
Dünya Satranç Şampiyonu olmamla başlıyor.

00:00:24.300 --> 00:00:25.556
O yılın başlarında,

00:00:25.580 --> 00:00:29.196
Almanya Hamburg'da,
simultane satranç denen bir oyunda

00:00:29.220 --> 00:00:33.516
dünyanın en iyi 32 satranç makinesiyle

00:00:33.540 --> 00:00:34.900
aynı anda oynadım.

00:00:35.980 --> 00:00:37.180
Tüm oyunları ben kazandım.

00:00:38.380 --> 00:00:41.556
32 bilgisayarı aynı anda yenmeme rağmen

00:00:41.580 --> 00:00:45.620
o zamanlar buna pek
şaşıran olmamıştı, tabii.

00:00:46.300 --> 00:00:48.876
O benim altın çağımdı.

00:00:48.900 --> 00:00:50.996
(Gülüşmeler)

00:00:51.020 --> 00:00:52.540
Makineler zayıftı,

00:00:53.500 --> 00:00:54.836
saçlarım da gürdü.

00:00:54.860 --> 00:00:57.060
(Gülüşmeler)

00:00:58.540 --> 00:01:00.596
Bundan yalnızca 12 yıl sonra,

00:01:00.620 --> 00:01:05.236
canımı zor kurtardığım bir maçta
tek bir bilgisayara karşı savaşıyordum.

00:01:05.260 --> 00:01:06.460
Bu maça,

00:01:07.180 --> 00:01:09.236
Newsweek dergisinin kapağında

00:01:09.260 --> 00:01:11.036
"Beynin Son Çırpınışı" demişlerdi.

00:01:11.060 --> 00:01:12.276
Baskı yok canım.

00:01:12.300 --> 00:01:13.820
(Gülüşmeler)

00:01:14.860 --> 00:01:17.436
Mitolojiden bilim kurguya pek çok yerde

00:01:17.460 --> 00:01:20.196
insanın makineyle mücadelesi

00:01:20.220 --> 00:01:22.980
ölüm kalım savaşı olarak resmedilir.

00:01:23.780 --> 00:01:25.356
John Henry,

00:01:25.380 --> 00:01:27.076
yani 19. yüzyıldaki Afro-Amerikan

00:01:27.100 --> 00:01:30.876
halk efsanesinde adı geçen bir taş delici

00:01:30.900 --> 00:01:32.276
dağdaki kayaları delerken

00:01:32.300 --> 00:01:35.036
buharlı deliciye karşı girdiği yarışta

00:01:35.060 --> 00:01:37.420
delik deşik edilmişti.

00:01:38.620 --> 00:01:42.820
John Henry'nin efsanesi, insanın
makine karşısındaki yenilgisini konu alan

00:01:43.500 --> 00:01:46.580
uzun bir hikâyenin bir parçası yalnızca.

00:01:48.020 --> 00:01:50.900
Bu rekabetçi söylem bugün artık her yerde.

00:01:51.380 --> 00:01:53.340
Makinelere karşı yarıştayız.

00:01:54.180 --> 00:01:56.260
Kavga hatta savaş bile
diyebilirsiniz buna.

00:01:57.700 --> 00:01:59.316
İşler ortadan kalkıyor.

00:01:59.340 --> 00:02:02.900
İnsanların yerini makineler alıyor,
sanki insan kalmamış gibi.

00:02:04.060 --> 00:02:07.556
"Terminatör" ve "Matrix"
gibi filmlerin artık gerçek olması

00:02:07.580 --> 00:02:08.780
yeter de artar bile.

00:02:11.460 --> 00:02:15.780
İnsan beyninin veya bedeninin

00:02:17.180 --> 00:02:21.436
bir bilgisayarla veya robotla
eşit şartlarda kapışacağı

00:02:21.459 --> 00:02:23.300
çok az alan kaldı.

00:02:24.100 --> 00:02:25.958
Aslında, keşke bu kadar az olmasaydı.

00:02:27.580 --> 00:02:28.780
Ama aksine,

00:02:29.660 --> 00:02:34.316
bugün hâlâ konuşulan

00:02:34.340 --> 00:02:37.036
makineyle insanın mücadelesinde

00:02:37.060 --> 00:02:40.156
kelimenin tam anlamıyla efsaneleşmek

00:02:40.180 --> 00:02:42.060
benim hem şansım, hem de lanetimdi.

00:02:44.940 --> 00:02:49.956
John Henry'den sonraki en ünlü
insan-makine kapışmasında

00:02:49.980 --> 00:02:52.556
iki maç yaptım.

00:02:52.580 --> 00:02:56.020
İkisi de IBM süper bilgisayarı
Deep Blue'ya karşıydı.

00:02:58.860 --> 00:03:01.036
Hiç kimse ilk maçı kazandığımı hatırlamaz.

00:03:01.060 --> 00:03:03.396
(Gülüşmeler)

00:03:03.420 --> 00:03:06.820
(Alkış)

00:03:07.740 --> 00:03:12.716
Bu Philadelphia'daydı; sonraki sene
New York'ta ikinci maçı kaybetmeden önce.

00:03:12.740 --> 00:03:14.500
Bu gayet adil, tabii.

00:03:16.140 --> 00:03:21.236
Everest Dağı'na tırmanmayı
deneyip de başaramayanların adı

00:03:21.260 --> 00:03:24.756
tarihe yazılmadı, ta ki Sir Edmund Hillary

00:03:24.780 --> 00:03:27.516
ve Tenzing Norgay tepeye ulaşıp

00:03:27.540 --> 00:03:28.740
tarih yazana dek.

00:03:29.780 --> 00:03:33.540
1977 yılında ben hâlâ dünya şampiyonuydum,

00:03:36.340 --> 00:03:40.540
bilgisayarlarsa rüştünü yeni ispatlamıştı.

00:03:41.340 --> 00:03:43.316
Everest Dağı bendim

00:03:43.340 --> 00:03:44.940
ve Deep Blue zirveye ulaşmıştı.

00:03:46.420 --> 00:03:50.476
Aslına bakarsanız, bunu yapan
Deep Blue değildi tabii ki,

00:03:50.500 --> 00:03:52.636
onun insan yaratıcılarıydı bunu başaran,

00:03:52.660 --> 00:03:55.996
Anantharaman, Campbell, Hoane ve Hsu.

00:03:56.020 --> 00:03:57.220
Onlara şapka çıkarıyorum.

00:03:58.660 --> 00:04:03.076
Her zaman olduğu gibi, makinanın
zaferi aslında insanın zaferiydi.

00:04:03.100 --> 00:04:07.860
İnsanlar kendi yaratımları tarafından
geçildiğinde bunu unutuyoruz genellikle.

00:04:10.180 --> 00:04:11.620
Deep Blue galip gelmişti.

00:04:13.220 --> 00:04:14.420
Zeki miydi peki?

00:04:15.180 --> 00:04:16.940
Hayır, değildi.

00:04:18.020 --> 00:04:23.076
Alan Turing ve bilgisayar biliminin
diğer kurucularının umduğu şekilde

00:04:23.100 --> 00:04:24.300
değildi en azından.

00:04:25.060 --> 00:04:29.836
Görüldü ki donanım
yeterli hıza ulaştığında

00:04:29.860 --> 00:04:34.116
ve algoritmaların zekâsı keskinleştiğinde

00:04:34.140 --> 00:04:37.100
kaba kuvvet satrancı ezip geçebiliyordu.

00:04:38.580 --> 00:04:42.276
Oysa elde edilen sonuca bakıldığında

00:04:42.300 --> 00:04:45.516
büyük usta seviyesindeki
satranç oyunlarında

00:04:45.540 --> 00:04:46.820
Deep Blue zekiydi.

00:04:49.140 --> 00:04:51.540
Ama dakikada 200 milyon
hamle hesaplayabilen

00:04:52.380 --> 00:04:55.580
inanılmaz hızına rağmen

00:04:57.180 --> 00:04:58.380
Deep Blue'nun yöntemi

00:04:59.180 --> 00:05:05.780
insan zekâsının merak edilen gizemlerini
kavramamıza pek yardımcı olamadı.

00:05:08.780 --> 00:05:10.596
Pek yakında,

00:05:10.620 --> 00:05:13.196
makineler taksi şoförü,

00:05:13.220 --> 00:05:15.636
doktor ve profesör olacaklar.

00:05:15.660 --> 00:05:18.260
"Zeki" olacaklar mı peki?

00:05:19.660 --> 00:05:22.156
Bu tanımlamaları filozoflara

00:05:22.180 --> 00:05:25.740
veya sözlüklere bırakmayı tercih ederim.

00:05:27.260 --> 00:05:31.140
Asıl önemli olan soru şu,

00:05:32.140 --> 00:05:35.740
biz insanlar bu makinelerle yaşama
ve çalışma konusunda ne hissediyoruz?

00:05:37.980 --> 00:05:43.236
Deep Blue ile 1996 yılının
Şubat ayında tanıştım.

00:05:43.260 --> 00:05:45.860
O sırada 10 yıldan fazladır
dünya şampiyonuydum.

00:05:47.900 --> 00:05:51.916
182 dünya şampiyonluğu maçı oynamıştım.

00:05:51.940 --> 00:05:57.036
Farklı müsabakalarda dünyanın en iyi
oyuncularıyla yüzlerce maç yapmıştım.

00:05:57.060 --> 00:06:02.116
Hem rakiplerimden, hem de kendimden

00:06:02.140 --> 00:06:03.820
ne bekleyeceğimi biliyordum.

00:06:04.500 --> 00:06:09.676
Beden dillerini gözlemleyerek
ve gözlerine bakarak

00:06:09.700 --> 00:06:13.316
hamlelerini kafamda ölçmeye

00:06:13.340 --> 00:06:17.180
ya da ruh hallerini
hesaba katmaya alışkındım.

00:06:17.700 --> 00:06:21.700
Sonra Deep Blue'nun karşısında
satranç tahtasının başına geçtim.

00:06:24.780 --> 00:06:27.636
Daha o anda yeni bir şey sezdim,

00:06:27.660 --> 00:06:28.980
beni huzursuz eden bir şey.

00:06:31.260 --> 00:06:34.060
Buna benzer bir şeyi

00:06:35.140 --> 00:06:37.676
ilk kez şoförü olmayan
bir arabaya bindiğinizde

00:06:37.700 --> 00:06:42.540
veya yeni bilgisayar müdürünüz
size görev verdiğinde hissedebilirsiniz.

00:06:45.620 --> 00:06:48.740
Ama o ilk oyuna başladığımda

00:06:49.900 --> 00:06:52.036
o şeyin kapasitesinden

00:06:52.060 --> 00:06:55.740
pek emin olamadım.

00:06:56.740 --> 00:06:59.900
Teknoloji çok çabuk gelişebilir,
IBM ise dev bir yatırım yapmıştı.

00:07:00.500 --> 00:07:01.700
O seti kaybettim.

00:07:04.140 --> 00:07:05.916
Sonra elimde olmadan merak ettim,

00:07:05.940 --> 00:07:07.500
bu şey yenilmez olabilir miydi?

00:07:08.420 --> 00:07:10.780
Çok sevdiğim satrancın sonu muydu bu?

00:07:12.620 --> 00:07:16.756
Bunlar insani endişeler, insani korkulardı

00:07:16.780 --> 00:07:18.460
ve emin olduğum bir şey vardı.

00:07:19.220 --> 00:07:22.116
O da rakibim Deep Blue'nun
böyle kaygıları olmadığıydı.

00:07:22.140 --> 00:07:23.900
(Gülüşmeler)

00:07:25.740 --> 00:07:27.140
Bu yıkıcı darbenin ardından

00:07:28.220 --> 00:07:29.900
ilk maçı kazanmak için

00:07:30.820 --> 00:07:32.020
savaştım.

00:07:32.780 --> 00:07:34.420
Ama alnımıza yazılmıştı bir kere.

00:07:36.220 --> 00:07:38.356
Eninde sonunda makineye yenildim.

00:07:38.380 --> 00:07:41.436
Ama kazandıktan sonra 
elinde çekiciyle ölen

00:07:41.460 --> 00:07:44.500
John Henry ile aynı kadere sahip değildim.

00:07:49.540 --> 00:07:52.076
O gün gösterdi ki

00:07:52.100 --> 00:07:55.340
satranç dünyasının gönlü hâlâ
insan şampiyondan yanaydı.

00:07:56.740 --> 00:07:58.420
Bugün bile,

00:07:59.900 --> 00:08:03.356
son model cep telefonunun içindeki
ücretsiz satranç uygulaması dahi

00:08:03.380 --> 00:08:05.396
Deep Blue'dan daha güçlüyken

00:08:05.420 --> 00:08:06.900
insanlar hâlâ satranç oynuyor,

00:08:08.500 --> 00:08:10.740
hem de eskisinden de fazla.

00:08:11.620 --> 00:08:14.836
Felaket tellalları, makinenin
egemenliğine giren bir oyuna

00:08:14.860 --> 00:08:17.116
bir daha kimsenin elini
sürmeyeceğini söyledi.

00:08:17.140 --> 00:08:19.356
Ama yanılıyorlardı, haksız çıktılar.

00:08:19.380 --> 00:08:22.836
Konu teknoloji olduğunda,
felaket tellallığı

00:08:22.860 --> 00:08:24.220
hep çok popüler olmuştur.

00:08:26.180 --> 00:08:28.916
Tecrübelerim bana şunu öğretti,

00:08:28.940 --> 00:08:33.596
teknolojiden olabildiğince
faydalanmak istiyorsak

00:08:33.620 --> 00:08:37.340
korkularımızla yüzleşmeliyiz

00:08:38.180 --> 00:08:40.556
ve insanlığımızı ortaya koymak istiyorsak

00:08:40.580 --> 00:08:45.820
bu korkulara galip gelmeliyiz.

00:08:47.940 --> 00:08:49.715
Yaralarımı yalarken,

00:08:49.739 --> 00:08:51.700
Deep Blue'ya karşı giriştiğim mücadeleden

00:08:52.900 --> 00:08:55.595
ilham alarak çıkmıştım.

00:08:55.619 --> 00:08:58.740
Eski bir Rus atasözünün dediği gibi:
Bükemediğin eli öpeceksin.

00:09:00.700 --> 00:09:02.076
Sonra düşündüm.

00:09:02.100 --> 00:09:04.436
Bilgisayara karşı oynarken,

00:09:04.460 --> 00:09:07.620
yanımda başka bir bilgisayarla
güçlerimi birleştirsem nasıl olurdu?

00:09:08.980 --> 00:09:12.756
İnsan sezgileriyle
makinenin hesaplama gücü,

00:09:12.780 --> 00:09:15.476
insan stratejisiyle makinenin taktikleri,

00:09:15.500 --> 00:09:17.916
insan tecrübesiyle
makinenin hafızası birleşse

00:09:17.940 --> 00:09:20.140
oynanmış en mükemmel oyun olmaz mıydı?

00:09:21.820 --> 00:09:23.500
Bu fikrim 1998 yılında,

00:09:24.740 --> 00:09:28.116
Advanced Chess 
(İleri Satranç) adı altında,

00:09:28.140 --> 00:09:33.820
elit bir oyuncuya karşı bilgisayar
eşliğinde maç yaptığımda hayata geçti.

00:09:35.100 --> 00:09:36.996
Ama bu ilk denemede,

00:09:37.020 --> 00:09:43.380
ikimiz de insan zekâsıyla makine zekâsını
etkili bir şekilde bir arada kullanamadık.

00:09:46.740 --> 00:09:48.980
Advanced Chess internette yer buldu

00:09:49.980 --> 00:09:54.836
ve 2005'te serbest stil
satranç turnuvası adıyla

00:09:54.860 --> 00:09:56.220
büyük yankı uyandırdı.

00:09:59.060 --> 00:10:02.596
Büyük ustalardan ve en iyi makinelerden
oluşan bir takım da katıldı,

00:10:02.620 --> 00:10:05.356
ama oyunun sonunda zafer
büyük ustaların olmadı,

00:10:05.380 --> 00:10:06.740
süper bilgisayarın da.

00:10:07.500 --> 00:10:11.836
Turnuvanın galibi bir çift
amatör Amerikan satranç oyuncusuydu

00:10:11.860 --> 00:10:15.020
ve aynı anda üç sıradan
bilgisayar kullanıyorlardı.

00:10:17.380 --> 00:10:20.396
Makinelerini yönetmedeki yetenekleri

00:10:20.420 --> 00:10:26.196
büyük usta rakiplerinin
üstün satranç bilgisini

00:10:26.220 --> 00:10:27.796
ve diğerlerinin

00:10:27.820 --> 00:10:31.980
bilgisayar üstünlüğünü alt etti.

00:10:33.420 --> 00:10:35.380
Ben de şu sonuca ulaşmış oldum.

00:10:36.380 --> 00:10:39.756
Zayıf bir insan oyuncu, bir makine

00:10:39.780 --> 00:10:43.036
ve süreci iyi yönetme bir aradayken,

00:10:43.060 --> 00:10:45.476
çok güçlü tek bir makineden üstündü.

00:10:45.500 --> 00:10:49.396
Ama daha önemlisi;
güçlü bir insan oyuncu, bir makine

00:10:49.420 --> 00:10:51.380
ve kötü süreç yönetimi bir aradayken

00:10:52.940 --> 00:10:55.340
onlardan da üstündü.

00:10:58.180 --> 00:11:00.300
Bu da beni, makinelerimizin zekâsından

00:11:01.820 --> 00:11:05.500
daha etkili faydalanmak için
daha iyi arayüzlere

00:11:06.340 --> 00:11:08.060
ihtiyaç duyacağımıza ikna etti.

00:11:10.140 --> 00:11:13.436
Makineyle iş birliği içinde
çalışan insan hayal değil.

00:11:13.460 --> 00:11:14.676
Bu günümüzün bir gerçeği.

00:11:14.700 --> 00:11:18.836
Yabancı dilde bir gazetede yer alan
bir makalenin içeriğini öğrenmek için

00:11:18.860 --> 00:11:22.900
çevrimiçi çeviri araçlarını kullananlar

00:11:22.900 --> 00:11:24.820
bunların mükemmel olmadığını bilir.

00:11:25.500 --> 00:11:27.596
Sonra, bize verdiği çeviriyi anlamak için

00:11:27.620 --> 00:11:29.716
insan tecrübemizi kullanırız.

00:11:29.740 --> 00:11:32.516
Makine de bizim düzeltmelerimizden
doğrusunu öğrenir.

00:11:32.540 --> 00:11:37.500
Bu yöntem hastalık teşhisinde ve güvenlik
analizinde gitgide yaygınlaşıyor.

00:11:38.260 --> 00:11:40.380
Makine verilerin üzerinden geçiyor,

00:11:41.140 --> 00:11:42.876
olasılık hesaplaması yapıyor

00:11:42.900 --> 00:11:46.556
ve işin yüzde 80 veya 90'lık
kısmını hallederek

00:11:46.580 --> 00:11:50.956
insanların analiz yapmasını

00:11:50.980 --> 00:11:53.580
ve karar verme sürecini kolaylaştırıyor.

00:11:54.100 --> 00:11:58.940
Ama çocuklarınızı yüzde 90 oranında
hatasız olarak kendi kendine giden

00:11:59.820 --> 00:12:03.380
bir arabayla okula
gönderecek hâliniz yok, tabii.

00:12:04.420 --> 00:12:06.020
Yüzde 99'la bile olmaz.

00:12:07.380 --> 00:12:10.236
İşte bu yüzden bu yöntemin
yeni yeni alanlarda

00:12:10.260 --> 00:12:16.420
uygulanması için atılım
yapmamız gerekiyor.

00:12:18.980 --> 00:12:23.020
Deep Blue ile karşılaşmamdan 20 yıl sonra,

00:12:24.020 --> 00:12:25.636
ikinci karşılaşmada,

00:12:25.660 --> 00:12:31.956
"Beynin Son Çırpınışı"
adlı sansasyonel başlık

00:12:31.980 --> 00:12:33.556
olağanlaştı.

00:12:33.580 --> 00:12:36.116
Çünkü zeki makineler

00:12:36.140 --> 00:12:37.340
her gün

00:12:38.380 --> 00:12:40.580
yeni yeni sektörlere adım atıyor.

00:12:41.980 --> 00:12:45.076
Makinelerin, çiftlik hayvanlarının
ve ağır işlerin yerini aldığı

00:12:45.100 --> 00:12:46.740
eski günlerin aksine,

00:12:48.300 --> 00:12:50.676
bugün makineler, üniversite
diplomalı insanların ve

00:12:50.700 --> 00:12:53.196
ve siyasetin ardından

00:12:53.220 --> 00:12:54.500
ikinci planda geliyor.

00:12:55.940 --> 00:12:58.036
Makinelerle savaşmış ve
kaybetmiş biri olarak

00:12:58.060 --> 00:13:00.700
şunu söylemek istiyorum,
bu harika bir haber.

00:13:02.820 --> 00:13:05.036
Tüm meslekler er ya da geç

00:13:05.060 --> 00:13:07.156
bu baskıyı üzerinde hissedecek.

00:13:07.180 --> 00:13:12.780
Bunun aksi olursa, insanlık
ilerlemeyi durdurmuş demektir.

00:13:14.580 --> 00:13:15.780
Teknolojik gelişmenin

00:13:17.260 --> 00:13:18.980
nerede ve ne zaman

00:13:20.300 --> 00:13:23.020
duracağına biz karar veremeyiz.

00:13:24.980 --> 00:13:26.340
Yavaşlamak ise

00:13:27.780 --> 00:13:29.276
söz konusu olamaz.

00:13:29.300 --> 00:13:31.116
Aslına bakarsanız,

00:13:31.140 --> 00:13:33.060
hızlanmamız gerek.

00:13:36.420 --> 00:13:39.060
Hayatımızdaki zorluklukları
ve belirsizlikleri

00:13:41.020 --> 00:13:44.380
yok etmek konusunda teknolojinin
eline kimse su dökemez.

00:13:46.820 --> 00:13:49.636
İşte bu yüzden

00:13:49.660 --> 00:13:51.516
daha da zorlu ve belirsiz görevlerin

00:13:51.540 --> 00:13:55.620
peşine düşmemiz gerek.

00:14:00.020 --> 00:14:01.220
Makineler

00:14:03.700 --> 00:14:05.516
hesap yapabilir.

00:14:05.540 --> 00:14:07.116
Biz ise anlama gücüne sahibiz.

00:14:07.140 --> 00:14:09.180
Makineler talimata göre çalışır.

00:14:10.660 --> 00:14:12.516
Bizim ise bir amacımız var.

00:14:12.540 --> 00:14:14.820
Makineler

00:14:16.900 --> 00:14:18.116
nesneldir.

00:14:18.140 --> 00:14:19.340
Biz ise tutkuluyuz.

00:14:20.420 --> 00:14:26.396
Makinelerimizin yapabildiklerinden
endişe duymamıza gerek yok.

00:14:26.420 --> 00:14:30.996
Aksine, bugün yapamadıkları
bizi endişelendirmeli.

00:14:31.020 --> 00:14:36.516
Çünkü en büyük hayallerimizi
gerçekleştirmek için

00:14:36.540 --> 00:14:40.620
yeni ve zeki makinelerin
yardımına ihtiyacımız olacak.

00:14:41.820 --> 00:14:43.140
Başarısız olursak,

00:14:44.060 --> 00:14:48.716
başarısızlığımız makinelerimizin
zekâsının çok veya az

00:14:48.740 --> 00:14:50.140
olmasından kaynaklanmayacak.

00:14:51.020 --> 00:14:54.100
Elimizdekilerle yetindiğimiz
ve hırslarımıza ket vurduğumuz için

00:14:55.500 --> 00:14:57.060
başarısız olacağız.

00:14:58.340 --> 00:15:01.380
İnsanlığımızı gösteren şey, çekiç sallamak

00:15:03.100 --> 00:15:05.780
veya satranç oynamak
gibi yeteneklerimiz değildir.

00:15:06.380 --> 00:15:09.396
Yalnızca insanların
yapabildiği bir şey var.

00:15:09.420 --> 00:15:10.620
Hayal kurmak.

00:15:11.940 --> 00:15:14.476
O yüzden hayallerimiz hep büyük olsun.

00:15:14.500 --> 00:15:15.716
Teşekkür ederim.

00:15:15.740 --> 00:15:19.627
(Alkış)

