WEBVTT
Kind: captions
Language: ru

00:00:00.000 --> 00:00:07.000
Переводчик: Yulia Kallistratova
Редактор: Anna Kotova

00:00:11.820 --> 00:00:15.756
Эта история началась в 1985 году,

00:00:15.780 --> 00:00:17.756
когда в 22-летнем возрасте

00:00:17.780 --> 00:00:20.156
я стал чемпионом мира по шахматам,

00:00:20.180 --> 00:00:23.380
победив Анатолия Карпова.

00:00:24.300 --> 00:00:25.556
В начале того же года

00:00:25.580 --> 00:00:29.196
я проводил так называемый
сеанс одновременной игры

00:00:29.220 --> 00:00:33.516
против 32 лучших в мире
шахматных вычислительных машин

00:00:33.540 --> 00:00:34.900
в Гамбурге, в Германии.

00:00:35.980 --> 00:00:37.180
Я выиграл все партии,

00:00:38.380 --> 00:00:41.556
и в то время никому не казалось
чем-то удивительным,

00:00:41.580 --> 00:00:45.620
что я смог выиграть
у 32 компьютеров одновременно.

00:00:46.300 --> 00:00:48.876
Для меня это было золотое время.

00:00:48.900 --> 00:00:50.996
(Смех)

00:00:51.020 --> 00:00:52.540
Машины играли слабо,

00:00:53.500 --> 00:00:54.836
а волосы держались крепко.

00:00:54.860 --> 00:00:57.060
(Смех)

00:00:58.540 --> 00:01:00.596
Но уже 12 годами позже

00:01:00.620 --> 00:01:05.236
я сражался не на жизнь, а на смерть
против одного единственного компьютера

00:01:05.260 --> 00:01:06.460
в матче,

00:01:07.180 --> 00:01:09.236
названном на обложке «Newsweek»

00:01:09.260 --> 00:01:11.036
«Последним шансом разума».

00:01:11.060 --> 00:01:12.276
Без напряга.

00:01:12.300 --> 00:01:13.820
(Смех)

00:01:14.860 --> 00:01:17.436
И в мифологии, и в научной фантастике

00:01:17.460 --> 00:01:20.196
поединок человека против машины

00:01:20.220 --> 00:01:22.980
зачастую представлялся
как вопрос жизни и смерти.

00:01:23.780 --> 00:01:25.356
Джон Генри,

00:01:25.380 --> 00:01:27.076
«стальной молоток»,

00:01:27.100 --> 00:01:30.876
легендарный афроамериканский
супергерой XIX века,

00:01:30.900 --> 00:01:35.026
соревновался с паровым молотом,

00:01:35.060 --> 00:01:37.420
пробивая тоннель в скале.

00:01:38.620 --> 00:01:42.820
Легенда о Джоне Генри —
часть давно сложившейся парадигмы

00:01:43.500 --> 00:01:46.580
противоборства человека с технологиями.

00:01:48.020 --> 00:01:50.900
Эта риторика соперничества
доминирует и по сей день.

00:01:51.380 --> 00:01:53.340
Мы участвуем в гонке против машин,

00:01:54.180 --> 00:01:56.260
это сражение, если не война.

00:01:57.700 --> 00:01:59.316
Нас лишают работы.

00:01:59.340 --> 00:02:02.900
Людей заменяют, как будто
они уже исчезли с лица Земли.

00:02:04.060 --> 00:02:07.556
Достаточно представить, что такие фильмы,
как «Терминатор» или «Матрица»,

00:02:07.590 --> 00:02:09.600
стали реальностью.

00:02:11.460 --> 00:02:15.780
Есть лишь немного областей,

00:02:17.180 --> 00:02:21.436
где тело и разум человека
могут на равных соперничать

00:02:21.459 --> 00:02:23.300
с компьютером или роботом.

00:02:24.100 --> 00:02:26.328
Мне хотелось бы, чтобы их было больше.

00:02:27.580 --> 00:02:28.780
Вместо этого

00:02:29.660 --> 00:02:34.316
я оказался одновременно
благословлён и проклят,

00:02:34.340 --> 00:02:37.036
став олицетворением поражения

00:02:37.060 --> 00:02:40.156
в противоборстве человека и машины,

00:02:40.180 --> 00:02:42.060
о котором говорят до сих пор.

00:02:44.940 --> 00:02:49.956
В самом знаменитом после Джона Генри
поединке человека с машиной

00:02:49.980 --> 00:02:52.556
я сыграл два матча

00:02:52.580 --> 00:02:56.020
против Deep Blue — суперкомпьютера IBM.

00:02:58.850 --> 00:03:01.036
Никто уже не помнит,
что первый матч я выиграл.

00:03:01.060 --> 00:03:03.396
(Смех)

00:03:03.420 --> 00:03:06.820
(Аплодисменты)

00:03:07.740 --> 00:03:12.716
Это было в Филадельфии, до проигрыша
на следующий год в Нью-Йорке.

00:03:12.740 --> 00:03:15.000
Но, по-моему, это справедливо.

00:03:16.140 --> 00:03:21.236
Ведь не отмечены же какими-то
особыми датами в истории

00:03:21.260 --> 00:03:24.756
все провалившиеся попытки
покорить Эверест

00:03:24.780 --> 00:03:27.516
до того, как сэр Эдмунд Хиллари
вместе с Тэнцингом Норгеем

00:03:27.540 --> 00:03:28.740
достигли его вершины.

00:03:29.780 --> 00:03:33.540
Я всё ещё был чемпионом мира в 1997 году,

00:03:36.340 --> 00:03:40.540
когда компьютеры наконец
«доросли» до шахмат.

00:03:41.340 --> 00:03:43.316
Я был тогда вершиной Эвереста,

00:03:43.340 --> 00:03:44.940
которую покорил Deep Blue.

00:03:46.420 --> 00:03:50.476
Должен сказать, что, разумеется,
это сделал не Deep Blue,

00:03:50.500 --> 00:03:52.636
а его создатели:

00:03:52.660 --> 00:03:55.996
Анансараман, Кэмпбелл, Хоэн, Сю.

00:03:56.020 --> 00:03:57.220
Снимаю перед ними шляпу.

00:03:58.660 --> 00:04:03.076
Как всегда, победа машины
была человеческим триумфом,

00:04:03.100 --> 00:04:07.860
о чём мы склонны забывать,
когда наше же творение нас превосходит.

00:04:10.180 --> 00:04:11.620
Deep Blue победил,

00:04:13.220 --> 00:04:14.630
но был ли он при этом умён?

00:04:15.180 --> 00:04:16.940
Нет, не был.

00:04:18.020 --> 00:04:21.316
Во всяком случае, не в том смысле,
о каком мечтали Алан Тьюринг

00:04:21.340 --> 00:04:24.350
и другие основоположники информатики.

00:04:25.060 --> 00:04:29.836
Оказалось, что шахматы можно постичь,
применив грубую силу,

00:04:29.860 --> 00:04:34.116
то есть при достаточно быстрых процессорах

00:04:34.140 --> 00:04:37.100
и достаточно изощрённых алгоритмах.

00:04:38.580 --> 00:04:42.276
Хотя по факту достижения результата —

00:04:42.300 --> 00:04:45.516
умению играть в шахматы
на гроссмейстерском уровне —

00:04:45.540 --> 00:04:47.060
Deep Blue был умён.

00:04:49.140 --> 00:04:51.540
Но даже при невероятной скорости

00:04:52.380 --> 00:04:55.580
в 200 миллионов комбинаций в секунду

00:04:57.180 --> 00:04:58.380
метод Deep Blue

00:04:59.180 --> 00:05:05.780
не приблизил нас, как того хотелось,
к разгадкам тайн человеческого разума.

00:05:08.780 --> 00:05:10.596
Очень скоро

00:05:10.620 --> 00:05:13.196
роботы станут водителями такси,

00:05:13.220 --> 00:05:15.636
врачами и профессорами,

00:05:15.660 --> 00:05:18.260
но будут ли они «разумны»?

00:05:19.660 --> 00:05:22.156
Я бы предпочёл оставить этот вопрос

00:05:22.180 --> 00:05:25.740
философам и составителям словарей.

00:05:27.260 --> 00:05:31.140
Важно то, какие чувства мы, люди,

00:05:32.140 --> 00:05:35.740
испытываем, живя и работая
бок о бок с этими машинами.

00:05:37.980 --> 00:05:43.236
Когда мне впервые представили
Deep Blue в феврале 1996 года,

00:05:43.260 --> 00:05:45.860
я уже был чемпионом мира более десяти лет,

00:05:47.900 --> 00:05:51.916
и я сыграл 182 игры на мировых чемпионатах

00:05:51.940 --> 00:05:57.036
и сотни игр против других мастеров
в различных состязаниях.

00:05:57.060 --> 00:06:02.116
Я знал, чего ожидать от своих соперников

00:06:02.140 --> 00:06:03.820
и чего ожидать от самого себя.

00:06:04.500 --> 00:06:09.676
Я привык предугадывать их ходы

00:06:09.700 --> 00:06:13.316
и оценивать их эмоциональное состояние,

00:06:13.340 --> 00:06:17.180
наблюдая за их жестами
и поведением, глядя им в глаза.

00:06:17.700 --> 00:06:21.700
И тут я оказался напротив Deep Blue.

00:06:24.780 --> 00:06:27.636
Я немедленно ощутил некую новизну,

00:06:27.660 --> 00:06:28.980
некий дискомфорт.

00:06:31.260 --> 00:06:34.060
Нечто вроде того, что вы почувствовали бы

00:06:35.140 --> 00:06:37.676
во время первой поездки
на автомобиле без водителя

00:06:37.700 --> 00:06:42.540
или когда ваш новый компьютерный босс
отдал бы первое распоряжение.

00:06:45.620 --> 00:06:48.740
Во время той первой игры

00:06:49.900 --> 00:06:52.036
я не был уверен,

00:06:52.060 --> 00:06:55.740
чего ожидать от этой штуковины.

00:06:56.740 --> 00:07:00.320
Технологии развиваются стремительно,
и IBM вложила в это солидные средства.

00:07:00.500 --> 00:07:01.820
Ту игру я проиграл.

00:07:04.140 --> 00:07:05.916
И меня не покидала мысль:

00:07:05.940 --> 00:07:07.830
а можно ли его вообще победить?

00:07:08.420 --> 00:07:11.330
Пришёл ли конец моей
излюбленной игре в шахматы?

00:07:12.620 --> 00:07:16.756
То были естественные человеческие
сомнения и страхи,

00:07:16.780 --> 00:07:18.580
и я был совершенно уверен в том,

00:07:19.200 --> 00:07:22.116
что у моего оппонента Deep Blue
не было никаких беспокойств.

00:07:22.140 --> 00:07:23.670
(Смех)

00:07:25.740 --> 00:07:27.550
После того сокрушительного удара

00:07:28.220 --> 00:07:29.900
я сражался снова

00:07:30.820 --> 00:07:32.430
и отыграл первую партию,

00:07:32.780 --> 00:07:34.420
но написанного не сотрёшь.

00:07:36.220 --> 00:07:38.356
Пускай я проиграл машине,

00:07:38.380 --> 00:07:41.436
но я, по крайней мере, не удостоился
судьбы Джона Генри,

00:07:41.460 --> 00:07:44.820
который, победив, погиб
от истощения с молотом в руке.

00:07:49.540 --> 00:07:52.076
Оказалось, что миру шахмат

00:07:52.100 --> 00:07:55.340
всё ещё нужен был чемпион мира
в человеческом обличии.

00:07:56.740 --> 00:07:58.420
И даже сегодня,

00:07:59.900 --> 00:08:03.356
когда бесплатные шахматные приложения
на последних моделях телефонов

00:08:03.380 --> 00:08:05.396
мощнее Deep Blue,

00:08:05.420 --> 00:08:07.300
люди всё ещё играют в шахматы

00:08:08.500 --> 00:08:10.740
и даже больше, чем когда-либо.

00:08:11.620 --> 00:08:14.836
Пессимисты предрекали,
что никто не захочет играть в игру,

00:08:14.860 --> 00:08:17.116
в которой победила машина,

00:08:17.140 --> 00:08:19.356
и оказались неправы, и это доказано,

00:08:19.380 --> 00:08:22.836
но мрачные предсказания
в отношении технологий всегда были

00:08:22.860 --> 00:08:24.780
популярным времяпрепровождением.

00:08:26.180 --> 00:08:28.916
Я на собственном опыте убедился,

00:08:28.940 --> 00:08:33.596
что страхам надо смотреть в глаза,

00:08:33.620 --> 00:08:37.340
если мы хотим извлечь максимум
из того, на что способны технологии,

00:08:38.180 --> 00:08:40.556
и что надо побеждать эти страхи,

00:08:40.580 --> 00:08:45.820
если мы хотим сохранить всё самое
лучшее, на что способно человечество.

00:08:47.940 --> 00:08:49.715
Пока я зализывал раны,

00:08:49.739 --> 00:08:51.910
меня посетило вдохновение

00:08:52.900 --> 00:08:55.595
по поводу поединков против Deep Blue.

00:08:55.619 --> 00:08:59.000
Как говорится в русской пословице:
не можешь победить — присоединяйся!

00:09:00.700 --> 00:09:02.076
Тогда я подумал:

00:09:02.100 --> 00:09:04.436
а что, если сыграть вместе с компьютером,

00:09:04.460 --> 00:09:07.620
имея компьютер на своей стороне,
объединив наши силы —

00:09:08.980 --> 00:09:12.756
человеческую интуицию
с машинными расчётами,

00:09:12.780 --> 00:09:15.476
человеческую стратегию
с машинной тактикой,

00:09:15.500 --> 00:09:17.916
человеческий опыт с машинной памятью?

00:09:17.940 --> 00:09:20.590
Получилась бы тогда
идеальная игра всех времён?

00:09:21.820 --> 00:09:23.820
Моя идея воплотилась в жизнь

00:09:24.740 --> 00:09:28.116
в 1998 году под названием «адванс»,
или продвинутые шахматы,

00:09:28.140 --> 00:09:33.820
когда я играл в состязании человека
и машины против другого элитного игрока.

00:09:35.100 --> 00:09:36.996
Но в том первом эксперименте

00:09:37.020 --> 00:09:43.380
у нас не получилось эффективно
сочетать способности человека и машины.

00:09:46.740 --> 00:09:49.320
Адванс занял свою нишу в интернете,

00:09:49.980 --> 00:09:54.836
а в 2005-м соревнования
по так называемым фристайл-шахматам

00:09:54.860 --> 00:09:56.220
стали настоящим открытием.

00:09:59.060 --> 00:10:02.596
В нём участвовали гроссмейстеры
и ведущие шахматные компьютеры,

00:10:02.620 --> 00:10:05.356
однако победителями стали не гроссмейстеры

00:10:05.380 --> 00:10:06.740
и не суперкомпьютеры.

00:10:07.500 --> 00:10:11.836
Победителями стали двое
американских игроков-любителей,

00:10:11.860 --> 00:10:15.020
одновременно управлявших тремя
обычными домашними компьютерами.

00:10:17.380 --> 00:10:20.396
Их умелое управление компьютерами

00:10:20.420 --> 00:10:26.196
превзошло глубочайшие знания шахмат

00:10:26.220 --> 00:10:27.846
игравших против них гроссмейстеров

00:10:27.876 --> 00:10:31.980
и намного бóльшие вычислительные
мощности других машин.

00:10:33.420 --> 00:10:35.380
Тогда-то я и вывел эту формулу.

00:10:36.380 --> 00:10:39.756
Слабый игрок плюс компьютер

00:10:39.780 --> 00:10:43.036
плюс мастерское управление превосходят

00:10:43.060 --> 00:10:45.476
не только более мощную машину,

00:10:45.500 --> 00:10:49.396
но, что ещё поразительнее,
превосходят сильного игрока

00:10:49.420 --> 00:10:51.380
плюс компьютер

00:10:52.940 --> 00:10:55.340
при неэффективном управлении.

00:10:58.180 --> 00:11:00.590
Это убедило меня в том,
что нам понадобятся

00:11:01.820 --> 00:11:05.500
более совершенные интерфейсы,
чтобы помогать компьютерным процессам

00:11:06.340 --> 00:11:08.060
становиться более разумными.

00:11:10.140 --> 00:11:13.436
Человек плюс машина — это не будущее,

00:11:13.460 --> 00:11:14.676
это настоящее.

00:11:14.700 --> 00:11:18.836
Любой, кто пользовался
автоматическим переводом,

00:11:18.860 --> 00:11:23.156
чтобы понять суть новостной статьи
из иностранной онлайн-газеты,

00:11:23.180 --> 00:11:24.980
знает, что тот далёк от совершенства.

00:11:25.500 --> 00:11:27.596
Но когда мы добавляем собственный опыт,

00:11:27.620 --> 00:11:29.716
чтобы придать смысл прочитанному,

00:11:29.740 --> 00:11:32.516
машина учится на наших исправлениях.

00:11:32.540 --> 00:11:37.500
Эта модель распространяется и применяется
в медицинской диагностике, в безопасности.

00:11:38.260 --> 00:11:40.380
Машины перемалывают данные,

00:11:41.140 --> 00:11:42.876
вычисляют вероятности,

00:11:42.900 --> 00:11:46.556
доводят точность до 80%, до 90%,

00:11:46.580 --> 00:11:50.956
упрощая данные для анализа

00:11:50.980 --> 00:11:53.580
и процесса принятия решений человеком.

00:11:54.100 --> 00:11:58.940
Только не думаю, что вы отправите детей

00:11:59.820 --> 00:12:03.380
в школу на самоуправляемой машине
с 90-процентной точностью

00:12:04.420 --> 00:12:06.020
или даже с точностью в 99%.

00:12:07.380 --> 00:12:10.236
Поэтому нам необходим скачок,

00:12:10.260 --> 00:12:16.420
который добавит эти жизненно важные
недостающие знаки после запятой.

00:12:18.980 --> 00:12:23.020
Спустя 20 лет после
моего матча против Deep Blue,

00:12:24.020 --> 00:12:25.636
моего второго матча,

00:12:25.660 --> 00:12:31.956
тот сенсационный заголовок:
«Последний шанс разума»

00:12:31.980 --> 00:12:33.556
стал избитой истиной

00:12:33.580 --> 00:12:36.116
в отношении искусственного интеллекта,

00:12:36.140 --> 00:12:37.340
проникающего

00:12:38.380 --> 00:12:40.990
казалось бы, с каждым днём
всё глубже в каждый сектор.

00:12:41.980 --> 00:12:45.076
Однако вопреки прошлому,

00:12:45.100 --> 00:12:46.740
когда машины заменяли

00:12:48.300 --> 00:12:50.676
сельскохозяйственных животных,
ручной труд,

00:12:50.700 --> 00:12:53.196
сейчас они угрожают людям с дипломами

00:12:53.220 --> 00:12:54.500
и политическим влиянием.

00:12:55.940 --> 00:12:58.036
И как тот, кто сражался
с машиной и проиграл,

00:12:58.060 --> 00:13:01.080
я хочу сказать вам, что это
замечательная, отличная новость.

00:13:02.820 --> 00:13:05.036
В конце концов любой профессионал

00:13:05.060 --> 00:13:07.156
должен испытать на себе это давление,

00:13:07.180 --> 00:13:12.780
иначе человечество перестанет развиваться.

00:13:14.580 --> 00:13:15.780
Не нам

00:13:17.260 --> 00:13:18.980
выбирать,

00:13:20.300 --> 00:13:23.020
где и когда остановится
технологический прогресс.

00:13:24.980 --> 00:13:26.340
Мы не можем

00:13:27.780 --> 00:13:29.276
замедлиться.

00:13:29.300 --> 00:13:31.116
Говоря по правде,

00:13:31.140 --> 00:13:33.310
нам впору ускоряться.

00:13:36.420 --> 00:13:39.060
Технологии отлично
справляются с устранением

00:13:41.020 --> 00:13:44.380
сложностей и неопределённостей
из нашей жизни,

00:13:46.820 --> 00:13:49.636
так что мы должны искать

00:13:49.660 --> 00:13:51.516
ещё более сложные,

00:13:51.540 --> 00:13:55.620
ещё более неопределённые задачи.

00:14:00.020 --> 00:14:01.220
Машины способны

00:14:03.700 --> 00:14:05.516
на вычисления.

00:14:05.540 --> 00:14:07.116
Мы способны на понимание.

00:14:07.140 --> 00:14:09.180
У машин есть инструкции.

00:14:10.660 --> 00:14:12.516
У нас есть цели.

00:14:12.540 --> 00:14:14.820
Машинам присуща

00:14:16.900 --> 00:14:18.116
объективность.

00:14:18.140 --> 00:14:19.590
Нам присуща увлечённость.

00:14:20.420 --> 00:14:26.396
Нам не следует беспокоиться о том,
на что машины способны сегодня.

00:14:26.420 --> 00:14:30.996
Нам следует беспокоиться о том,
на что они пока ещё не способны,

00:14:31.020 --> 00:14:36.516
потому что нам понадобится
помощь новых, умных машин,

00:14:36.540 --> 00:14:40.620
чтобы воплотить наши
величайшие мечты в реальность.

00:14:41.820 --> 00:14:43.140
И если нам это не удастся,

00:14:44.060 --> 00:14:48.716
если не удастся, то это не потому,
что машины окажутся слишком умны

00:14:48.740 --> 00:14:50.140
или недостаточно умны.

00:14:51.020 --> 00:14:54.100
Если нам не удастся, то это оттого,
что мы избаловались

00:14:55.500 --> 00:14:57.310
и умалили собственные амбиции.

00:14:58.340 --> 00:15:01.380
Наше человеческое начало
не определяется набором навыков

00:15:03.100 --> 00:15:05.780
вроде махания молотом
или даже игры в шахматы.

00:15:06.380 --> 00:15:09.396
Единственная вещь,
на которую способны только люди, —

00:15:09.420 --> 00:15:10.620
это мечтать.

00:15:11.940 --> 00:15:14.476
Так давайте мечтать о большем.

00:15:14.500 --> 00:15:15.716
Спасибо.

00:15:15.740 --> 00:15:19.627
(Аплодисменты)

