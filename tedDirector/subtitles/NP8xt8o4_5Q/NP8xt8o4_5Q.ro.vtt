WEBVTT
Kind: captions
Language: ro

00:00:00.000 --> 00:00:07.000
Traducător: Ovidiu Panaite
Corector: Mirel-Gabriel Alexa

00:00:12.780 --> 00:00:15.756
Acestă poveste începe în 1985,

00:00:15.780 --> 00:00:17.756
când la vârsta de 22 de ani

00:00:17.780 --> 00:00:20.156
am devenit campion mondial la șah

00:00:20.180 --> 00:00:23.380
după ce l-am învins pe Anatoly Karpov.

00:00:24.300 --> 00:00:25.556
Mai devreme în acel an

00:00:25.580 --> 00:00:29.196
am jucat ceea ce se numește 
un simultan de șah

00:00:29.220 --> 00:00:33.516
împotriva celor mai performante
32 computere capabile să joace șah,

00:00:33.540 --> 00:00:34.900
în Hamburg, Germania.

00:00:35.980 --> 00:00:37.480
Am câștigat toate partidele,

00:00:38.380 --> 00:00:41.556
dar atunci nu era considerată
o surpriză prea mare

00:00:41.580 --> 00:00:45.620
faptul că am putut să înving 
32 de computere în același timp.

00:00:46.300 --> 00:00:48.876
Pentru mine, aceea era epoca de aur.

00:00:48.900 --> 00:00:50.996
(Râsete)

00:00:51.020 --> 00:00:52.540
Computerele erau slabe,

00:00:53.500 --> 00:00:54.836
iar părul meu era puternic.

00:00:54.860 --> 00:00:57.060
(Râsete)

00:00:58.540 --> 00:01:00.596
Doar 12 ani mai târziu

00:01:00.620 --> 00:01:05.236
luptam să supraviețuiesc
împotriva unui singur computer

00:01:05.260 --> 00:01:06.460
într-un meci

00:01:07.180 --> 00:01:09.236
numit de coperta revistei „Newsweek”

00:01:09.260 --> 00:01:11.036
„Ultima redută a creierului.”

00:01:11.060 --> 00:01:12.276
Nicio presiune.

00:01:12.300 --> 00:01:13.820
(Râsete)

00:01:14.860 --> 00:01:17.436
De la mitologie,
la domeniul științifico-fantastic,

00:01:17.460 --> 00:01:20.196
omul împotriva mașinăriilor

00:01:20.220 --> 00:01:22.980
a fost mereu descris 
ca o luptă pe viață și pe moarte.

00:01:23.780 --> 00:01:25.356
John Henry,

00:01:25.380 --> 00:01:27.076
numit „omul ce conduce oțelul”

00:01:27.100 --> 00:01:30.876
în folclorul afro-american 
din secolul al XIX-lea,

00:01:30.900 --> 00:01:32.276
a fost pus să concureze

00:01:32.300 --> 00:01:35.036
împotriva unui ciocan mecanic
alimentat cu aburi,

00:01:35.060 --> 00:01:37.420
să sape un tunel prin rocă montană.

00:01:38.620 --> 00:01:42.820
Legenda lui John Henry face parte 
dintr-o narațiune istorică lungă

00:01:43.500 --> 00:01:46.580
în care oamenii înfruntă tehnologia.

00:01:48.020 --> 00:01:50.900
Iar această retorică competitivă 
e o normă astăzi.

00:01:51.380 --> 00:01:53.540
Suntem într-o cursă 
împotriva mașinăriilor,

00:01:54.180 --> 00:01:56.260
într-o luptă sau chiar într-un război.

00:01:57.700 --> 00:01:59.316
Slujbele dispar.

00:01:59.340 --> 00:02:02.900
Oamenii sunt înlocuiți de parcă 
au dispărut de pe fața pământului.

00:02:04.060 --> 00:02:07.556
E de-ajuns ca să crezi că filmele
precum „Terminatorul” sau „Matrix”

00:02:07.580 --> 00:02:08.780
sunt non-ficțiune.

00:02:11.460 --> 00:02:15.780
Sunt foarte puține cazurile

00:02:17.180 --> 00:02:21.436
în care corpul și mintea umană 
pot concura de la egal la egal

00:02:21.459 --> 00:02:23.300
cu un computer sau un robot.

00:02:24.100 --> 00:02:25.958
De fapt, mi-aș dori să fie mai multe.

00:02:27.580 --> 00:02:28.780
În schimb,

00:02:29.660 --> 00:02:34.316
a fost binecuvântarea,
dar și blestemul meu

00:02:34.340 --> 00:02:37.036
să devin proverbialul om

00:02:37.060 --> 00:02:40.156
din întrecerea omului
împotriva mașinăriilor

00:02:40.180 --> 00:02:42.350
despre care încă vorbește toată lumea.

00:02:44.940 --> 00:02:49.956
În cea mai faimoasă întrecere 
om-mașină de la John Henry încoace,

00:02:49.980 --> 00:02:52.556
am jucat două meciuri

00:02:52.580 --> 00:02:56.020
împotriva supercomputerului IBM,
Deep Blue.

00:02:58.710 --> 00:03:01.156
Nimeni nu-și amintește 
că am câștigat primul meci.

00:03:01.156 --> 00:03:03.396
(Râsete)

00:03:03.420 --> 00:03:06.820
(Aplauze)

00:03:07.740 --> 00:03:12.716
În Philadelphia, înainte să pierd 
meciul din New York în anul următor.

00:03:12.740 --> 00:03:14.500
Dar cred că e just.

00:03:16.140 --> 00:03:21.236
Nu există nicio zi în istorie 
sau registru în calendar

00:03:21.260 --> 00:03:24.756
pentru toți cei care n-au reușit 
să urce pe Everest

00:03:24.780 --> 00:03:27.516
înainte ca Sir Edmund Hillary 
și Tenzin Norgay

00:03:27.540 --> 00:03:28.740
să ajungă în vârf.

00:03:29.780 --> 00:03:33.540
Iar în 1997 încă eram campion mondial,

00:03:36.340 --> 00:03:40.540
iar computerele care joacă șah 
s-au maturizat.

00:03:41.340 --> 00:03:43.316
Eu eram muntele Everest,

00:03:43.340 --> 00:03:44.940
iar Deep Blue a ajuns în vârf.

00:03:46.420 --> 00:03:50.476
Ar trebui să spun bineînțeles 
că nu Deep Blue a reușit,

00:03:50.500 --> 00:03:52.636
ci creatorii săi umani,

00:03:52.660 --> 00:03:55.996
Anantharaman, Campbell, Hoane, Hsu.

00:03:56.020 --> 00:03:57.220
Jos pălăria pentru ei.

00:03:58.660 --> 00:04:03.076
Ca de obicei, triumful mașinii 
a fost un triumf uman,

00:04:03.100 --> 00:04:07.860
lucru pe care îl cam uităm când oamenii
sunt depășiți de propriile creații.

00:04:10.180 --> 00:04:11.620
Deep Blue a câștigat,

00:04:13.220 --> 00:04:14.470
dar era el inteligent?

00:04:15.180 --> 00:04:16.940
Nu, nu era,

00:04:18.020 --> 00:04:23.076
nu în sensul în care Alan Turing
și alți fondatori ai informaticii

00:04:23.100 --> 00:04:24.300
au sperat.

00:04:25.060 --> 00:04:29.836
Se pare că șahul 
a putut fi descifrat de forța brută,

00:04:29.860 --> 00:04:34.116
odată ce procesoarele
au devenit destul de rapide,

00:04:34.140 --> 00:04:37.100
iar algoritmii au devenit 
destul de deștepți.

00:04:38.580 --> 00:04:42.276
Deși prin definiția titlului

00:04:42.300 --> 00:04:45.516
de mare maestru al șahului,

00:04:45.540 --> 00:04:46.820
Deep Blue era inteligent.

00:04:49.140 --> 00:04:51.540
Dar chiar și la acea viteză incredibilă

00:04:52.380 --> 00:04:55.580
de 200 de milioane de mutări pe secundă,

00:04:57.180 --> 00:04:58.380
metoda lui Deep Blue

00:04:59.180 --> 00:05:05.780
nu a oferit înțelegerea mult-visată 
a misterelor inteligenței umane.

00:05:08.780 --> 00:05:10.596
Curând,

00:05:10.620 --> 00:05:13.196
mașinăriile vor fi șoferi de taxi,

00:05:13.220 --> 00:05:15.636
doctori și profesori,

00:05:15.660 --> 00:05:18.260
dar vor fi ele „inteligente”?

00:05:19.660 --> 00:05:22.156
Aș lăsa mai degrabă aceste definiții

00:05:22.180 --> 00:05:25.740
în seama filozofilor și a dicționarelor.

00:05:27.260 --> 00:05:31.140
Ce contează de fapt e cum ne simțim

00:05:32.140 --> 00:05:35.740
noi oamenii, trăind și lucrând 
cu aceste mașinării.

00:05:37.980 --> 00:05:43.236
Când l-am cunoscut prima dată 
pe Deep Blue în februarie 1996,

00:05:43.260 --> 00:05:45.860
eram campion mondial
de mai bine de zece ani

00:05:47.900 --> 00:05:51.916
și jucasem 182 de partide 
pentru titlul mondial

00:05:51.940 --> 00:05:57.036
și sute de partide împotriva altor 
jucători de top în diverse competiții.

00:05:57.060 --> 00:06:02.116
Știam la ce să mă aștept de la adversari

00:06:02.140 --> 00:06:03.820
și la ce să mă aștept de la mine.

00:06:04.500 --> 00:06:09.676
Eram obișnuit să le măsor mutările

00:06:09.700 --> 00:06:13.316
și să le apreciez starea emoțională

00:06:13.340 --> 00:06:17.180
observându-le limbajul corpului 
și privindu-i în ochi.

00:06:17.700 --> 00:06:21.700
Apoi am stat în fața lui Deep Blue.

00:06:24.780 --> 00:06:27.636
Am simțit imediat ceva nou,

00:06:27.660 --> 00:06:28.980
ceva neliniștitor.

00:06:31.260 --> 00:06:34.060
Ați putea experimenta 
un sentiment asemănător

00:06:35.140 --> 00:06:37.676
când vă urcați prima oară 
într-o mașină fără șofer

00:06:37.700 --> 00:06:42.540
sau prima dată când noul vostru 
manager-computer vă dă ordine la lucru.

00:06:45.620 --> 00:06:48.740
Dar când m-am așezat 
la acea primă partidă,

00:06:49.900 --> 00:06:52.036
nu puteam fi sigur

00:06:52.060 --> 00:06:55.740
de ce e capabilă chestia aia.

00:06:56.740 --> 00:06:59.900
Tehnologia poate avansa în salturi, 
iar IBM a investit masiv.

00:07:00.500 --> 00:07:01.700
Am pierdut acea partidă.

00:07:04.140 --> 00:07:05.916
Și m-am întrebat:

00:07:05.940 --> 00:07:07.500
ar putea fi invincibil?

00:07:08.420 --> 00:07:10.780
S-a terminat cu mult-iubitul
meu joc de șah?

00:07:12.620 --> 00:07:16.756
Acestea erau îndoieli umane, frici umane,

00:07:16.780 --> 00:07:18.460
iar lucrul de care eram sigur

00:07:19.220 --> 00:07:22.116
era că adversarul meu Deep Blue 
nu are astfel de temeri.

00:07:22.140 --> 00:07:23.900
(Râsete)

00:07:25.740 --> 00:07:27.140
Am contraatacat

00:07:28.220 --> 00:07:29.900
după acea lovitură devastatoare

00:07:30.820 --> 00:07:32.200
și am câștigat primul meci,

00:07:32.780 --> 00:07:34.420
dar știam ce va urma.

00:07:36.220 --> 00:07:38.356
În final am pierdut în fața computerului,

00:07:38.380 --> 00:07:41.436
dar nu am avut soarta lui John Henry

00:07:41.460 --> 00:07:44.500
care a câștigat, dar a murit 
cu ciocanul în mână.

00:07:49.540 --> 00:07:52.076
S-a dovedit că lumea șahului

00:07:52.100 --> 00:07:55.340
încă își dorea să aibă un campion uman.

00:07:56.740 --> 00:07:58.420
Chiar și azi,

00:07:59.900 --> 00:08:03.356
când o aplicație gratuită pentru mobil

00:08:03.380 --> 00:08:05.396
e mai puternică decât Deep Blue,

00:08:05.420 --> 00:08:06.900
oamenii încă joacă șah,

00:08:08.500 --> 00:08:10.740
chiar mai mult decât înainte.

00:08:11.620 --> 00:08:14.836
Pesimiștii spuneau că nimeni 
nu se va mai atinge de joc

00:08:14.860 --> 00:08:17.116
odată ce putea fi câștigat de o mașinărie,

00:08:17.140 --> 00:08:19.356
dar s-a dovedit că n-au avut dreptate.

00:08:19.380 --> 00:08:22.836
Dar pesimismul a fost mereu
atitudinea preferată

00:08:22.860 --> 00:08:24.220
când e vorba de tehnologie.

00:08:26.180 --> 00:08:28.916
Ce am învățat din propria experiență

00:08:28.940 --> 00:08:33.596
e că trebuie să ne înfruntăm fricile

00:08:33.620 --> 00:08:37.340
dacă vrem să obținem 
cât mai multe din tehnologie,

00:08:38.180 --> 00:08:40.556
și trebuie să ne învingem fricile

00:08:40.580 --> 00:08:45.820
dacă vrem să obținem ce-i mai bun 
de la umanitate.

00:08:47.940 --> 00:08:49.715
Pe când îmi lingeam rănile,

00:08:49.739 --> 00:08:51.700
am fost inspirat foarte mult

00:08:52.900 --> 00:08:55.595
de bătăliile mele cu Deep Blue.

00:08:55.619 --> 00:08:59.150
O zicală veche rusească spune: 
„Dacă nu-i poți învinge, alătură-te lor.”

00:09:00.700 --> 00:09:02.076
Apoi m-am gândit,

00:09:02.100 --> 00:09:04.436
dacă aș putea juca cu un computer,

00:09:04.460 --> 00:09:07.620
cu un computer de partea mea, 
combinându-ne abilitățile,

00:09:08.980 --> 00:09:12.756
intuiția umană și calculele mașinăriei,

00:09:12.780 --> 00:09:15.476
strategia umană cu tactica mașinăriei,

00:09:15.500 --> 00:09:17.916
experiența umană cu memoria mașinăriei.

00:09:17.940 --> 00:09:20.380
Ar putea fi cea mai bună partidă
jucată vreodată?

00:09:21.820 --> 00:09:23.500
Ideea mea a luat ființă

00:09:24.740 --> 00:09:28.116
în 1998 sub numele de „Advanced Chess”,

00:09:28.140 --> 00:09:33.820
când am jucat împotriva altui jucător
de elită în formula om plus mașină.

00:09:35.100 --> 00:09:36.996
Dar în primul experiment

00:09:37.020 --> 00:09:43.380
nu am reușit să îmbinăm eficient 
abilitățile umane și tehnologice.

00:09:46.740 --> 00:09:48.980
„Advanced Chess” și-a găsit casa 
pe internet,

00:09:49.980 --> 00:09:54.836
iar în 2005, un așa-numit 
turneu de șah „freestyle”,

00:09:54.860 --> 00:09:56.220
a produs o revelație.

00:09:59.060 --> 00:10:02.596
Au participat o echipă de mari maeștri 
și computere de top,

00:10:02.620 --> 00:10:05.356
dar câștigătorii nu au fost maeștrii,

00:10:05.380 --> 00:10:06.740
nici supercomputerul.

00:10:07.500 --> 00:10:11.836
Câștigătorii au fost o pereche de jucători
amatori de șah din America

00:10:11.860 --> 00:10:15.020
operând trei computere normale 
în același timp.

00:10:17.380 --> 00:10:20.396
Abilitatea lor de a-și antrena computerele

00:10:20.420 --> 00:10:26.196
a contracarat cunoștințele 
superioare de șah

00:10:26.220 --> 00:10:27.796
ale marilor maeștri

00:10:27.820 --> 00:10:31.980
și puterea de calcul mult superioară 
a calculatoarelor.

00:10:33.420 --> 00:10:35.380
Astfel am ajuns la această formulare:

00:10:36.380 --> 00:10:39.756
un jucător de șah slab și un computer

00:10:39.780 --> 00:10:43.036
plus un proces mai bun sunt superiori

00:10:43.060 --> 00:10:45.476
unui computer de unul singur,

00:10:45.500 --> 00:10:49.396
dar și mai remarcabil, 
e superior unui jucător bun

00:10:49.420 --> 00:10:51.380
plus un computer,

00:10:52.940 --> 00:10:55.340
dar cu un proces mai slab.

00:10:58.180 --> 00:11:00.300
Asta m-a convins că vom avea nevoie

00:11:01.820 --> 00:11:05.500
de interfețe mai bune 
pentru a ne antrena computerele

00:11:06.340 --> 00:11:08.060
spre o inteligență mai folositoare.

00:11:10.140 --> 00:11:13.436
Oamenii plus computerele nu sunt viitorul,

00:11:13.460 --> 00:11:14.676
sunt prezentul.

00:11:14.700 --> 00:11:18.836
Oricine a folosit traducerile online

00:11:18.860 --> 00:11:23.156
pentru a înțelege un articol 
dintr-un ziar străin.

00:11:23.180 --> 00:11:24.820
Știind că nu sunt perfecte,

00:11:25.500 --> 00:11:27.596
dar ne folosim de experiența umană

00:11:27.620 --> 00:11:29.716
pentru a deduce sensul,

00:11:29.740 --> 00:11:32.516
apoi computerul învață
din corectările noastre.

00:11:32.540 --> 00:11:37.500
Modelul se propagă și apare în diagnostice
medicale, analize de securitate,

00:11:38.260 --> 00:11:40.380
computerele adună datele,

00:11:41.140 --> 00:11:42.876
calculează probabilitățile,

00:11:42.900 --> 00:11:46.556
face 80-90% din treabă,

00:11:46.580 --> 00:11:50.956
făcându-le mai ușor de analizat

00:11:50.980 --> 00:11:53.580
și facilitând procesul decizional uman.

00:11:54.100 --> 00:11:59.880
Dar nu-ți vei trimite copiii la școală

00:11:59.900 --> 00:12:03.380
într-o mașină automată fără șofer,
care are o acuratețe de 90%,

00:12:04.420 --> 00:12:06.020
și chiar 99%.

00:12:07.380 --> 00:12:10.236
Așa că avem nevoie de un salt înainte

00:12:10.260 --> 00:12:16.420
ca să mai adăugăm ceva după virgulă.

00:12:18.980 --> 00:12:23.020
Douăzeci de ani 
după meciul meu cu Deep Blue,

00:12:24.020 --> 00:12:25.636
al doilea meci,

00:12:25.660 --> 00:12:31.956
acest titlu „Ultima redută a creierului”

00:12:31.980 --> 00:12:33.556
a devenit ceva normal,

00:12:33.580 --> 00:12:36.116
când mașinăriile inteligente

00:12:36.140 --> 00:12:37.340
avansează

00:12:38.380 --> 00:12:40.580
în fiecare sector, aproape în fiecare zi.

00:12:41.980 --> 00:12:45.106
Dar spre deosebire de trecut,

00:12:45.106 --> 00:12:46.740
când mașinăriile înlocuiau

00:12:48.300 --> 00:12:50.676
animalele, munca manuală,

00:12:50.700 --> 00:12:53.196
acum vin peste oamenii 
cu diplome universitare

00:12:53.220 --> 00:12:54.500
și cu influență politică.

00:12:55.940 --> 00:12:58.036
Pentru cineva 
care s-a luptat și a pierdut,

00:12:58.060 --> 00:13:00.700
vă spun că acestea
sunt vești foarte, foarte bune.

00:13:02.820 --> 00:13:05.036
În final toate profesiile

00:13:05.060 --> 00:13:07.156
vor simți aceste presiuni,

00:13:07.180 --> 00:13:12.780
altfel ar însemna că umanitatea 
a încetat să progreseze.

00:13:14.580 --> 00:13:15.780
Nu putem

00:13:17.260 --> 00:13:18.980
să alegem

00:13:20.300 --> 00:13:23.020
când și unde se oprește 
progresul tehnologic.

00:13:24.980 --> 00:13:26.340
Nu putem

00:13:27.780 --> 00:13:29.276
încetini.

00:13:29.300 --> 00:13:31.116
De fapt,

00:13:31.140 --> 00:13:33.060
trebuie să accelerăm.

00:13:36.420 --> 00:13:39.060
Tehnologia excelează în eliminarea

00:13:41.020 --> 00:13:44.380
dificultăților și incertitudinilor 
din viețile noastre,

00:13:46.820 --> 00:13:49.666
astfel că trebuie să căutăm

00:13:49.666 --> 00:13:51.546
provocări tot mai dificile,

00:13:51.546 --> 00:13:55.620
tot mai nesigure.

00:14:00.020 --> 00:14:01.220
Mașinăriile

00:14:03.700 --> 00:14:05.516
au calcule.

00:14:05.540 --> 00:14:07.116
Noi avem înțelegere.

00:14:07.140 --> 00:14:09.180
Mașinăriile au instrucțiuni.

00:14:10.660 --> 00:14:12.516
Noi avem scop.

00:14:12.540 --> 00:14:14.820
Mașinăriile

00:14:16.900 --> 00:14:18.116
au obiectivitate.

00:14:18.140 --> 00:14:19.340
Noi avem pasiune.

00:14:20.420 --> 00:14:26.396
N-ar trebuie să ne îngrijoreze
ce pot face mașinăriile azi.

00:14:26.420 --> 00:14:30.996
De fapt, ar trebui să ne îngrijoreze 
ceea ce nu pot face încă,

00:14:31.020 --> 00:14:36.516
pentru că ne trebuie ajutorul 
mașinăriilor noi, inteligente,

00:14:36.540 --> 00:14:40.620
pentru a ne transpune 
cele mai mari vise în realitate.

00:14:41.820 --> 00:14:43.140
Iar dacă dăm greș,

00:14:44.060 --> 00:14:48.716
dacă dăm greș, nu e pentru că 
mașinăriile noastre sunt prea inteligente,

00:14:48.740 --> 00:14:50.370
sau nu îndeajuns de inteligente.

00:14:51.020 --> 00:14:54.100
Dacă dăm greș, 
e pentru că am devenit leneși

00:14:55.500 --> 00:14:57.060
și ne limităm ambițiile.

00:14:58.340 --> 00:15:01.380
Umanitatea nu e definită 
de nicio abilitate,

00:15:03.100 --> 00:15:05.780
precum lovirea cu ciocanul 
sau jocul de șah.

00:15:06.380 --> 00:15:09.396
Există un singur lucru 
pe care-l poate face numai un om:

00:15:09.420 --> 00:15:10.620
să viseze.

00:15:11.940 --> 00:15:14.476
Așa că haideți să avem vise mărețe.

00:15:14.500 --> 00:15:15.716
Mulțumesc.

00:15:15.740 --> 00:15:19.627
(Aplauze)

