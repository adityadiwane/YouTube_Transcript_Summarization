WEBVTT
Kind: captions
Language: nl

00:00:00.000 --> 00:00:07.000
Vertaald door: Rik Delaet
Nagekeken door: Axel Saffran

00:00:12.780 --> 00:00:15.756
Dit verhaal begint in 1985

00:00:15.780 --> 00:00:17.756
toen ik als 22-jarige

00:00:17.780 --> 00:00:20.156
wereldkampioen schaken werd,

00:00:20.180 --> 00:00:23.380
na Anatoly Karpov te hebben verslagen.

00:00:24.300 --> 00:00:25.556
Eerder dat jaar

00:00:25.580 --> 00:00:28.906
speelde ik simultaan

00:00:28.920 --> 00:00:33.236
tegen 32 van 's werelds 
beste schaakmachines

00:00:33.250 --> 00:00:35.020
in Hamburg, Duitsland.

00:00:35.980 --> 00:00:37.420
Ik won alle partijen,

00:00:38.380 --> 00:00:41.556
en destijds verbaasde het niemand

00:00:41.580 --> 00:00:45.620
dat ik 32 computers tegelijk kon verslaan.

00:00:46.300 --> 00:00:48.876
Voor mij was dat de gouden eeuw.

00:00:48.900 --> 00:00:50.996
(Gelach)

00:00:51.020 --> 00:00:52.540
Machines waren zwak,

00:00:53.260 --> 00:00:54.836
en mijn haar was sterk.

00:00:54.860 --> 00:00:57.060
(Gelach)

00:00:58.540 --> 00:01:00.596
Maar slechts 12 jaar later

00:01:00.620 --> 00:01:04.960
vocht ik voor mijn leven 
tegen slechts één computer

00:01:04.960 --> 00:01:06.440
in een wedstrijd

00:01:06.440 --> 00:01:08.970
die Newsweek op zijn cover betitelde als:

00:01:08.970 --> 00:01:11.186
‘De Ultieme Strijd van het Brein’.

00:01:11.186 --> 00:01:12.276
Geen druk.

00:01:12.300 --> 00:01:13.820
(Gelach)

00:01:14.860 --> 00:01:17.436
In de mythologie en in science fiction

00:01:17.460 --> 00:01:20.196
werd mens-tegen-machine

00:01:20.220 --> 00:01:22.980
vaak afgeschilderd 
als een zaak van leven of dood.

00:01:23.780 --> 00:01:25.356
John Henry,

00:01:25.380 --> 00:01:27.076
de 'stalen man' genoemd

00:01:27.100 --> 00:01:30.716
in de 19e eeuwse 
Afro-Amerikaanse volkslegende,

00:01:30.730 --> 00:01:32.276
moest een wedstrijd aangaan

00:01:32.300 --> 00:01:35.036
tegen een stoomaangedreven hamer

00:01:35.060 --> 00:01:37.420
om een tunnel door een berg te hakken.

00:01:38.620 --> 00:01:42.820
John Henry's legende is een deel 
van een lang historisch verhaal

00:01:43.500 --> 00:01:46.580
van strijd tussen mens en technologie.

00:01:48.020 --> 00:01:50.900
Deze competitieve retoriek 
is nu standaard.

00:01:51.380 --> 00:01:53.340
We zitten in een race 
tegen de machines

00:01:54.180 --> 00:01:56.260
in een gevecht of zelfs een oorlog.

00:01:57.700 --> 00:01:59.316
Jobs vallen weg.

00:01:59.340 --> 00:02:02.900
Mensen worden vervangen 
alsof ze van de aarde zijn verdwenen.

00:02:03.810 --> 00:02:07.556
Je zou bijna gaan denken dat films 
als ‘The Terminator’ of ‘The Matrix’

00:02:07.580 --> 00:02:08.930
non-fictie zijn.

00:02:11.460 --> 00:02:15.780
Er zijn zeer weinig gevallen van een arena

00:02:17.180 --> 00:02:21.436
waarbij de mens lichamelijk en geestelijk 
op gelijke voorwaarden kan concurreren

00:02:21.459 --> 00:02:23.300
met een computer of een robot.

00:02:24.100 --> 00:02:26.428
Eigenlijk wou ik dat er wat meer waren.

00:02:27.580 --> 00:02:28.780
In plaats daarvan

00:02:29.660 --> 00:02:34.316
was het mijn zegen en mijn vloek

00:02:34.340 --> 00:02:37.286
om letterlijk uit te groeien 
tot de spreekwoordelijke man

00:02:37.286 --> 00:02:40.156
in de mens-versus-machinecompetitie

00:02:40.180 --> 00:02:42.060
waar iedereen nog steeds over praat.

00:02:44.940 --> 00:02:49.956
In de meest bekende 
mens-machinecompetitie sinds John Henry

00:02:49.980 --> 00:02:52.556
speelde ik twee wedstrijden

00:02:52.580 --> 00:02:56.020
tegen de IBM-supercomputer Deep Blue.

00:02:58.860 --> 00:03:01.526
Niemand herinnert zich 
dat ik de eerste wedstrijd won --

00:03:01.526 --> 00:03:03.396
(Gelach)

00:03:03.420 --> 00:03:06.820
(Applaus)

00:03:07.740 --> 00:03:12.716
in Philadelphia voordat ik de terugmatch 
het jaar daarop in New York verloor.

00:03:12.740 --> 00:03:14.500
Maar ik denk dat het eerlijk is.

00:03:16.140 --> 00:03:21.236
Er is geen bijzondere herdenkingsdag

00:03:21.260 --> 00:03:24.756
voor iedereen die de top 
van de Everest niet haalde

00:03:24.780 --> 00:03:27.516
voordat Sir Edmund Hillary 
en Tenzing Norgay

00:03:27.540 --> 00:03:28.740
de top bereikten.

00:03:29.780 --> 00:03:33.540
En in 1997 was ik 
nog steeds de wereldkampioen

00:03:36.340 --> 00:03:40.540
toen de schaakcomputers 
eindelijk volwassen werden.

00:03:41.340 --> 00:03:43.316
Ik was de Everest

00:03:43.340 --> 00:03:44.940
en Deep Blue bereikte de top.

00:03:46.420 --> 00:03:50.476
Ik moet natuurlijk zeggen 
dat niet Deep Blue het deed,

00:03:50.500 --> 00:03:52.636
maar wel zijn menselijke makers --

00:03:52.660 --> 00:03:55.996
Anantharaman, Campbell, Hoane, Hsu.

00:03:56.190 --> 00:03:57.370
Petje af voor hen.

00:03:58.660 --> 00:04:03.076
Zoals altijd was de triomf 
van de machine een menselijke triomf,

00:04:03.100 --> 00:04:07.860
iets wat we wel eens vergeten wanneer 
onze eigen creaties ons overtreffen.

00:04:10.180 --> 00:04:11.810
Deep Blue heeft gezegevierd,

00:04:13.060 --> 00:04:14.420
maar was hij intelligent?

00:04:15.180 --> 00:04:16.940
Nee, nee, hij was het niet,

00:04:18.020 --> 00:04:21.080
althans niet 
op de manier waarop Alan Turing

00:04:21.080 --> 00:04:24.300
en andere grondleggers 
van de informatica het hadden gehoopt.

00:04:25.060 --> 00:04:29.836
Het bleek dat schaken kon worden 
gekraakt door brute kracht,

00:04:29.860 --> 00:04:34.116
zodra de hardware snel genoeg werd

00:04:34.140 --> 00:04:37.100
en de algoritmen slim genoeg.

00:04:38.580 --> 00:04:42.276
Maar naar de definitie van het resultaat,

00:04:42.300 --> 00:04:45.516
schaken op grootmeesterniveau,

00:04:45.540 --> 00:04:47.040
was Deep Blue intelligent.

00:04:49.140 --> 00:04:51.540
Maar ondanks de ongelooflijke snelheid

00:04:52.380 --> 00:04:55.580
van 200 miljoen posities per seconde,

00:04:57.180 --> 00:04:59.280
verschafte Deep Blue's methode

00:04:59.300 --> 00:05:02.320
weinig van het gedroomde inzicht

00:05:02.810 --> 00:05:07.000
in de mysteries 
van de menselijke intelligentie.

00:05:08.780 --> 00:05:10.596
Spoedig

00:05:10.620 --> 00:05:13.196
zullen machines taxichauffeurs,

00:05:13.220 --> 00:05:15.636
artsen en professoren zijn,

00:05:15.660 --> 00:05:18.260
maar zullen ze ‘intelligent’ zijn?

00:05:19.660 --> 00:05:22.156
Ik zou deze definities liever overlaten

00:05:22.180 --> 00:05:25.740
aan filosofen en woordenboeken.

00:05:27.260 --> 00:05:31.140
Wat echt belangrijk is, 
is hoe wij mensen

00:05:32.140 --> 00:05:35.740
ons voelen over leven 
en werken met deze machines.

00:05:37.980 --> 00:05:42.966
Toen ik voor het eerst Deep Blue 
in februari 1996 ontmoette,

00:05:42.990 --> 00:05:45.860
was ik al meer 
dan 10 jaar wereldkampioen,

00:05:47.900 --> 00:05:51.916
en had ik 182 WK-partijen gespeeld

00:05:51.940 --> 00:05:53.220
en honderden partijen

00:05:53.220 --> 00:05:56.600
tegen andere topspelers 
in andere wedstrijden.

00:05:57.060 --> 00:06:02.116
Ik wist wat te verwachten 
van mijn tegenstanders

00:06:02.140 --> 00:06:03.820
en wat te verwachten van mezelf.

00:06:04.500 --> 00:06:09.676
Ik was gewend 
om hun zetten in te schatten

00:06:09.700 --> 00:06:13.316
en hun emotionele toestand te peilen

00:06:13.340 --> 00:06:17.180
door te kijken 
naar hun lichaamstaal en hun ogen.

00:06:17.700 --> 00:06:21.700
Maar toen zat ik 
aan het schaakbord tegenover Deep Blue.

00:06:24.780 --> 00:06:27.636
Ik voelde meteen iets nieuws,

00:06:27.660 --> 00:06:28.980
iets verontrustends.

00:06:31.260 --> 00:06:34.060
Een soortgelijk gevoel 
zal je misschien ervaren

00:06:35.140 --> 00:06:37.676
de eerste keer dat je rijdt 
in een zelfrijdende auto

00:06:37.700 --> 00:06:41.090
of de eerste keer 
dat je nieuwe computer-manager

00:06:41.090 --> 00:06:43.400
je iets beveelt op het werk.

00:06:45.620 --> 00:06:48.740
Maar bij dat eerste spel

00:06:49.900 --> 00:06:52.036
wist ik niet zeker

00:06:52.060 --> 00:06:55.740
waartoe dit ding in staat was.

00:06:56.740 --> 00:07:00.320
Technologie kan sprongsgewijs vooruitgaan 
en IBM had er zwaar in geïnvesteerd.

00:07:00.350 --> 00:07:01.910
Ik verloor die wedstrijd.

00:07:04.020 --> 00:07:06.036
En ik kon niet anders dan me afvragen

00:07:06.060 --> 00:07:07.810
of hij onklopbaar kon zijn.

00:07:08.420 --> 00:07:10.780
Was mijn geliefde schaakspel om zeep?

00:07:12.620 --> 00:07:16.596
Dit waren menselijke twijfels, 
menselijke angsten,

00:07:16.620 --> 00:07:18.380
en het enige wat ik zeker wist,

00:07:18.380 --> 00:07:20.220
was dat mijn tegenstander Deep Blue

00:07:20.220 --> 00:07:22.440
zich daar helemaal 
geen zorgen over maakte.

00:07:22.440 --> 00:07:23.900
(Gelach)

00:07:25.740 --> 00:07:27.140
Ik vocht terug

00:07:28.220 --> 00:07:30.080
na deze verwoestende klap

00:07:30.820 --> 00:07:32.650
om de eerste wedstrijd te winnen,

00:07:32.780 --> 00:07:34.700
maar het was een teken aan de wand.

00:07:36.220 --> 00:07:38.356
Uiteindelijk verloor ik van de machine

00:07:38.380 --> 00:07:41.436
maar ik onderging niet 
het lot van John Henry

00:07:41.460 --> 00:07:44.500
die won, maar stierf 
met zijn hamer in de hand.

00:07:49.540 --> 00:07:52.076
Het bleek dat de schaakwereld

00:07:52.100 --> 00:07:55.340
nog steeds een menselijke 
schaakkampioen wilde hebben.

00:07:56.740 --> 00:07:58.420
En zelfs vandaag, nu

00:07:59.900 --> 00:08:03.356
een gratis schaak-app 
op de nieuwste mobiele telefoon

00:08:03.380 --> 00:08:05.396
sterker is dan Deep Blue,

00:08:05.420 --> 00:08:07.420
zijn mensen nog steeds aan het schaken,

00:08:08.500 --> 00:08:10.740
zelfs meer dan ooit tevoren.

00:08:11.470 --> 00:08:14.836
Doemdenkers voorspelden dat 
niemand het spel zou willen spelen

00:08:14.860 --> 00:08:17.116
dat door een machine bedwongen kon worden

00:08:17.140 --> 00:08:19.356
maar ze bleken het fout te hebben,

00:08:19.380 --> 00:08:22.836
maar doemdenken was altijd al 
een populair tijdverdrijf

00:08:22.860 --> 00:08:24.420
als het over technologie gaat.

00:08:26.180 --> 00:08:28.916
Wat ik van mijn eigen ervaring leerde,

00:08:28.940 --> 00:08:33.596
is dat we onze angsten 
onder ogen moeten zien

00:08:33.620 --> 00:08:37.340
als we het maximale 
uit onze technologie willen halen,

00:08:38.180 --> 00:08:40.556
en we moeten die angsten overwinnen

00:08:40.580 --> 00:08:45.820
als we het beste 
uit onze menselijkheid willen halen.

00:08:47.940 --> 00:08:49.715
Terwijl ik mijn wonden likte,

00:08:49.739 --> 00:08:51.700
kreeg ik heel wat inspiratie

00:08:52.900 --> 00:08:55.595
door mijn gevechten tegen Deep Blue.

00:08:55.619 --> 00:08:57.530
Zoals het oude Russische gezegde gaat:

00:08:57.530 --> 00:09:00.150
als je ze niet kunt verslaan, 
sluit je dan bij hen aan.

00:09:00.700 --> 00:09:02.076
Toen dacht ik:

00:09:02.100 --> 00:09:04.436
wat als ik samen 
met een computer kon spelen --

00:09:04.460 --> 00:09:07.620
met een computer aan mijn zijde 
onze krachten kon bundelen:

00:09:08.980 --> 00:09:12.756
menselijke intuïtie 
plus machineberekening,

00:09:12.780 --> 00:09:15.476
menselijk strategie, machinetactiek,

00:09:15.500 --> 00:09:17.916
menselijke ervaring, machinegeheugen.

00:09:17.940 --> 00:09:20.340
Zou dat het meest 
perfecte spel ooit kunnen zijn?

00:09:21.820 --> 00:09:23.610
Mijn idee kwam tot leven

00:09:24.710 --> 00:09:25.870
in 1998

00:09:26.360 --> 00:09:28.116
onder de naam Geavanceerd Schaken

00:09:28.140 --> 00:09:33.820
toen ik een mens-plus-machinewedstrijd 
speelde tegen een andere topspeler.

00:09:35.100 --> 00:09:36.996
Maar in dit eerste experiment,

00:09:37.020 --> 00:09:39.130
slaagden we er allebei niet in

00:09:39.200 --> 00:09:43.580
om mens- en machinevaardigheden 
effectief te combineren.

00:09:46.740 --> 00:09:49.770
Geavanceerd Schaken 
vond zijn thuis op het internet

00:09:49.980 --> 00:09:54.580
en in 2005 zorgde een zogenaamd 
freestyle schaaktoernooi

00:09:54.610 --> 00:09:56.220
voor een openbaring.

00:09:59.060 --> 00:10:02.596
Een team van grootmeesters 
en topmachines nam deel,

00:10:02.620 --> 00:10:05.356
maar de winnaars 
waren noch de grootmeesters

00:10:05.380 --> 00:10:06.740
noch de supercomputer.

00:10:07.500 --> 00:10:11.836
De winnaars waren 
een paar Amerikaanse schaakamateurs

00:10:11.860 --> 00:10:15.020
die drie gewone pc's 
tegelijk bedienden.

00:10:17.380 --> 00:10:20.396
Hun vaardigheid 
in het coachen van hun machines

00:10:20.420 --> 00:10:25.880
counterde effectief 
de superieure schaakkennis

00:10:25.900 --> 00:10:27.996
van hun grootmeester-tegenstanders

00:10:27.996 --> 00:10:31.980
en de grotere rekenkracht van anderen.

00:10:33.420 --> 00:10:35.380
Ik kwam tot deze formulering:

00:10:36.380 --> 00:10:39.756
een zwakke menselijke speler 
met een computer

00:10:39.780 --> 00:10:43.036
en een betere procesbeheersing 
is superieur

00:10:43.060 --> 00:10:45.476
aan een ​​zeer krachtige machine alleen,

00:10:45.500 --> 00:10:49.396
maar, nog opmerkelijker, is superieur 
aan een sterke menselijke speler

00:10:49.420 --> 00:10:51.380
met machine en...

00:10:52.940 --> 00:10:55.340
een inferieure procesbeheersing.

00:10:58.000 --> 00:10:59.450
Dit overtuigde me ervan

00:10:59.450 --> 00:11:02.940
dat we betere interfaces
nodig zouden hebben

00:11:02.940 --> 00:11:05.730
om ons te helpen bij
het coachen van onze machines

00:11:05.740 --> 00:11:08.060
richting een nuttiger intelligentie.

00:11:10.140 --> 00:11:13.436
Mens plus machine is niet de toekomst,

00:11:13.460 --> 00:11:14.676
het is er al.

00:11:14.700 --> 00:11:18.836
Iedereen die ooit 
onlinevertaling gebruikte

00:11:18.860 --> 00:11:23.156
om iets van een nieuwsartikel 
van een buitenlandse krant te snappen,

00:11:23.180 --> 00:11:25.090
weet dat ze verre van perfect is.

00:11:25.500 --> 00:11:27.596
Dan gebruiken we onze menselijke ervaring

00:11:27.620 --> 00:11:29.716
om er iets zinvols uit te distilleren,

00:11:29.740 --> 00:11:32.516
waarna de machine 
leert van onze correcties.

00:11:32.540 --> 00:11:35.260
Dit model verspreidt zich 
en wordt toegepast

00:11:35.260 --> 00:11:38.260
bij medische diagnose 
en veiligheidsanalyse.

00:11:38.260 --> 00:11:40.380
De machine verwerkt data,

00:11:41.140 --> 00:11:42.876
berekent waarschijnlijkheden,

00:11:42.900 --> 00:11:46.556
legt 80-90% van het traject af,

00:11:46.580 --> 00:11:50.956
waardoor analyse en besluitvorming

00:11:50.980 --> 00:11:53.580
makkelijker wordt 
voor de menselijke partij.

00:11:54.100 --> 00:11:58.940
Maar je stuurt je kinderen niet

00:11:59.820 --> 00:12:03.380
naar school in een zelfrijdende 
auto met 90% nauwkeurigheid,

00:12:04.420 --> 00:12:06.020
zelfs niet met 99%.

00:12:07.380 --> 00:12:10.236
Voor de sprong voorwaarts moeten er

00:12:10.260 --> 00:12:16.420
nog wat cruciale decimalen bijkomen.

00:12:18.980 --> 00:12:23.020
Twintig jaar na mijn match met Deep Blue,

00:12:24.020 --> 00:12:25.636
na de tweede wedstrijd,

00:12:25.660 --> 00:12:31.710
is de sensationele kop:
‘De Ultieme Strijd van het Brein’

00:12:31.740 --> 00:12:33.556
gemeengoed geworden,

00:12:33.580 --> 00:12:36.116
nu intelligente machines

00:12:36.140 --> 00:12:37.880
hun intrede doen

00:12:37.900 --> 00:12:40.750
in elke sector, bijna dagelijks.

00:12:41.980 --> 00:12:45.076
Maar in tegenstelling tot het verleden,

00:12:45.100 --> 00:12:46.740
toen machines

00:12:48.300 --> 00:12:50.676
boerderijdieren en handenarbeid vervingen,

00:12:50.700 --> 00:12:53.306
gaan ze dat nu doen 
met mensen met academische graden

00:12:53.306 --> 00:12:54.700
en politieke invloed.

00:12:55.940 --> 00:12:58.246
Als iemand 
die met machines vocht en verloor,

00:12:58.246 --> 00:13:00.700
vertel ik jullie 
dat dit geweldig nieuws is.

00:13:02.820 --> 00:13:05.036
Uiteindelijk zal elk beroep

00:13:05.060 --> 00:13:07.156
deze druk moeten ondergaan

00:13:07.180 --> 00:13:11.580
of anders zou het betekenen

00:13:11.580 --> 00:13:14.250
dat de mensheid 
ophoudt met vooruitgang boeken.

00:13:14.580 --> 00:13:15.780
We zullen niet

00:13:17.260 --> 00:13:18.980
kunnen kiezen

00:13:20.300 --> 00:13:23.020
wanneer en waar 
de technologische vooruitgang stopt.

00:13:24.980 --> 00:13:26.340
We kunnen niet

00:13:27.780 --> 00:13:29.276
vertragen.

00:13:29.300 --> 00:13:31.116
Eigenlijk

00:13:31.140 --> 00:13:33.060
moeten we versnellen.

00:13:36.420 --> 00:13:39.060
Onze technologie 
blinkt uit in het wegnemen

00:13:41.020 --> 00:13:44.380
van moeilijkheden 
en onzekerheden uit ons leven,

00:13:46.820 --> 00:13:49.636
en dus moeten we op zoek gaan naar

00:13:49.660 --> 00:13:51.516
steeds moeilijker,

00:13:51.540 --> 00:13:55.620
steeds onzekerder uitdagingen.

00:14:00.020 --> 00:14:01.220
Machines hebben

00:14:03.700 --> 00:14:05.516
berekeningen.

00:14:05.540 --> 00:14:07.116
Wij hebben begrip.

00:14:07.140 --> 00:14:09.180
Machines hebben instructies.

00:14:10.660 --> 00:14:12.516
Wij hebben doel.

00:14:12.540 --> 00:14:14.820
Machines hebben

00:14:16.770 --> 00:14:18.116
objectiviteit.

00:14:18.140 --> 00:14:19.580
Wij hebben passie.

00:14:20.420 --> 00:14:23.420
We moeten ons geen zorgen maken

00:14:23.420 --> 00:14:26.420
over wat onze machines 
vandaag kunnen.

00:14:26.420 --> 00:14:28.740
In plaats daarvan 
moeten we ons zorgen maken

00:14:28.740 --> 00:14:31.020
over wat ze vandaag 
nog steeds niet kunnen,

00:14:31.020 --> 00:14:36.516
omdat we de hulp van de nieuwe, 
intelligente machines nodig hebben

00:14:36.540 --> 00:14:40.620
om onze grootste dromen 
om te zetten in realiteit.

00:14:41.820 --> 00:14:43.140
En als we falen,

00:14:44.060 --> 00:14:48.716
als we falen, is het niet 
omdat onze machines te intelligent

00:14:48.740 --> 00:14:50.390
of niet intelligent genoeg zijn.

00:14:51.020 --> 00:14:54.100
Als we falen, dan is dat 
omdat we zelfgenoegzaam werden

00:14:55.500 --> 00:14:57.060
en onze ambities inperkten.

00:14:58.340 --> 00:15:01.380
Onze menselijkheid wordt 
niet bepaald door vaardigheden

00:15:03.100 --> 00:15:05.780
als met een hamer zwiepen
of zelfs als schaken.

00:15:06.380 --> 00:15:09.396
Er is één ding dat 
alleen maar een mens kan doen.

00:15:09.420 --> 00:15:10.620
Dat is dromen.

00:15:11.940 --> 00:15:14.476
Laten we dus groots dromen.

00:15:14.500 --> 00:15:15.716
Dank je.

00:15:15.740 --> 00:15:19.167
(Applaus)

