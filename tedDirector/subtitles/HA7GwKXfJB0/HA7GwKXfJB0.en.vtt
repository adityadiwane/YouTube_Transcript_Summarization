WEBVTT
Kind: captions
Language: en

00:00:17.260 --> 00:00:20.260
We live in in a remarkable time,

00:00:20.260 --> 00:00:23.260
the age of genomics.

00:00:23.260 --> 00:00:26.260
Your genome is the entire sequence of your DNA.

00:00:26.260 --> 00:00:29.260
Your sequence and mine are slightly different.

00:00:29.260 --> 00:00:31.260
That's why we look different.

00:00:31.260 --> 00:00:33.260
I've got brown eyes;

00:00:33.260 --> 00:00:36.260
you might have blue or gray.

00:00:36.260 --> 00:00:38.260
But it's not just skin-deep.

00:00:38.260 --> 00:00:40.260
The headlines tell us

00:00:40.260 --> 00:00:43.260
that genes can give us scary diseases,

00:00:43.260 --> 00:00:46.260
maybe even shape our personality,

00:00:46.260 --> 00:00:49.260
or give us mental disorders.

00:00:49.260 --> 00:00:52.260
Our genes seem to have

00:00:52.260 --> 00:00:55.260
awesome power over our destinies.

00:00:56.260 --> 00:00:59.260
And yet, I would like to think

00:00:59.260 --> 00:01:02.260
that I am more than my genes.

00:01:04.260 --> 00:01:06.260
What do you guys think?

00:01:06.260 --> 00:01:09.260
Are you more than your genes?

00:01:09.260 --> 00:01:11.260
(Audience: Yes.) Yes?

00:01:13.260 --> 00:01:15.260
I think some people agree with me.

00:01:15.260 --> 00:01:17.260
I think we should make a statement.

00:01:17.260 --> 00:01:19.260
I think we should say it all together.

00:01:20.260 --> 00:01:23.260
All right: "I'm more than my genes" -- all together.

00:01:23.260 --> 00:01:27.260
Everybody: I am more than my genes.

00:01:27.260 --> 00:01:29.260
(Cheering)

00:01:30.260 --> 00:01:32.260
Sebastian Seung: What am I?

00:01:32.260 --> 00:01:35.260
(Laughter)

00:01:35.260 --> 00:01:38.260
I am my connectome.

00:01:40.260 --> 00:01:42.260
Now, since you guys are really great,

00:01:42.260 --> 00:01:44.260
maybe you can humor me and say this all together too.

00:01:44.260 --> 00:01:46.260
(Laughter)

00:01:46.260 --> 00:01:48.260
Right. All together now.

00:01:48.260 --> 00:01:51.260
Everybody: I am my connectome.

00:01:53.260 --> 00:01:55.260
SS: That sounded great.

00:01:55.260 --> 00:01:57.260
You know, you guys are so great, you don't even know what a connectome is,

00:01:57.260 --> 00:01:59.260
and you're willing to play along with me.

00:01:59.260 --> 00:02:02.260
I could just go home now.

00:02:02.260 --> 00:02:05.260
Well, so far only one connectome is known,

00:02:05.260 --> 00:02:08.260
that of this tiny worm.

00:02:08.260 --> 00:02:10.260
Its modest nervous system

00:02:10.260 --> 00:02:12.260
consists of just 300 neurons.

00:02:12.260 --> 00:02:14.260
And in the 1970s and '80s,

00:02:14.260 --> 00:02:16.260
a team of scientists

00:02:16.260 --> 00:02:18.260
mapped all 7,000 connections

00:02:18.260 --> 00:02:20.260
between the neurons.

00:02:21.260 --> 00:02:23.260
In this diagram, every node is a neuron,

00:02:23.260 --> 00:02:25.260
and every line is a connection.

00:02:25.260 --> 00:02:27.260
This is the connectome

00:02:27.260 --> 00:02:31.260
of the worm C. elegans.

00:02:31.260 --> 00:02:34.260
Your connectome is far more complex than this

00:02:34.260 --> 00:02:36.260
because your brain

00:02:36.260 --> 00:02:38.260
contains 100 billion neurons

00:02:38.260 --> 00:02:41.260
and 10,000 times as many connections.

00:02:41.260 --> 00:02:43.260
There's a diagram like this for your brain,

00:02:43.260 --> 00:02:46.260
but there's no way it would fit on this slide.

00:02:47.260 --> 00:02:50.260
Your connectome contains one million times more connections

00:02:50.260 --> 00:02:53.260
than your genome has letters.

00:02:53.260 --> 00:02:55.260
That's a lot of information.

00:02:55.260 --> 00:02:58.260
What's in that information?

00:02:59.260 --> 00:03:02.260
We don't know for sure, but there are theories.

00:03:02.260 --> 00:03:05.260
Since the 19th century, neuroscientists have speculated

00:03:05.260 --> 00:03:07.260
that maybe your memories --

00:03:07.260 --> 00:03:09.260
the information that makes you, you --

00:03:09.260 --> 00:03:11.260
maybe your memories are stored

00:03:11.260 --> 00:03:13.260
in the connections between your brain's neurons.

00:03:15.260 --> 00:03:17.260
And perhaps other aspects of your personal identity --

00:03:17.260 --> 00:03:20.260
maybe your personality and your intellect --

00:03:20.260 --> 00:03:22.260
maybe they're also encoded

00:03:22.260 --> 00:03:25.260
in the connections between your neurons.

00:03:26.260 --> 00:03:29.260
And so now you can see why I proposed this hypothesis:

00:03:29.260 --> 00:03:32.260
I am my connectome.

00:03:32.260 --> 00:03:35.260
I didn't ask you to chant it because it's true;

00:03:35.260 --> 00:03:37.260
I just want you to remember it.

00:03:37.260 --> 00:03:39.260
And in fact, we don't know if this hypothesis is correct,

00:03:39.260 --> 00:03:41.260
because we have never had technologies

00:03:41.260 --> 00:03:43.260
powerful enough to test it.

00:03:44.260 --> 00:03:47.260
Finding that worm connectome

00:03:47.260 --> 00:03:50.260
took over a dozen years of tedious labor.

00:03:50.260 --> 00:03:53.260
And to find the connectomes of brains more like our own,

00:03:53.260 --> 00:03:56.260
we need more sophisticated technologies, that are automated,

00:03:56.260 --> 00:03:59.260
that will speed up the process of finding connectomes.

00:03:59.260 --> 00:04:02.260
And in the next few minutes, I'll tell you about some of these technologies,

00:04:02.260 --> 00:04:04.260
which are currently under development

00:04:04.260 --> 00:04:07.260
in my lab and the labs of my collaborators.

00:04:08.260 --> 00:04:11.260
Now you've probably seen pictures of neurons before.

00:04:11.260 --> 00:04:13.260
You can recognize them instantly

00:04:13.260 --> 00:04:16.260
by their fantastic shapes.

00:04:16.260 --> 00:04:19.260
They extend long and delicate branches,

00:04:19.260 --> 00:04:22.260
and in short, they look like trees.

00:04:22.260 --> 00:04:25.260
But this is just a single neuron.

00:04:25.260 --> 00:04:27.260
In order to find connectomes,

00:04:27.260 --> 00:04:30.260
we have to see all the neurons at the same time.

00:04:30.260 --> 00:04:32.260
So let's meet Bobby Kasthuri,

00:04:32.260 --> 00:04:34.260
who works in the laboratory of Jeff Lichtman

00:04:34.260 --> 00:04:36.260
at Harvard University.

00:04:36.260 --> 00:04:38.260
Bobby is holding fantastically thin slices

00:04:38.260 --> 00:04:40.260
of a mouse brain.

00:04:40.260 --> 00:04:43.260
And we're zooming in by a factor of 100,000 times

00:04:44.260 --> 00:04:46.260
to obtain the resolution,

00:04:46.260 --> 00:04:49.260
so that we can see the branches of neurons all at the same time.

00:04:50.260 --> 00:04:53.260
Except, you still may not really recognize them,

00:04:53.260 --> 00:04:56.260
and that's because we have to work in three dimensions.

00:04:56.260 --> 00:04:58.260
If we take many images of many slices of the brain

00:04:58.260 --> 00:05:00.260
and stack them up,

00:05:00.260 --> 00:05:02.260
we get a three-dimensional image.

00:05:02.260 --> 00:05:04.260
And still, you may not see the branches.

00:05:04.260 --> 00:05:06.260
So we start at the top,

00:05:06.260 --> 00:05:09.260
and we color in the cross-section of one branch in red,

00:05:09.260 --> 00:05:11.260
and we do that for the next slice

00:05:11.260 --> 00:05:13.260
and for the next slice.

00:05:13.260 --> 00:05:15.260
And we keep on doing that,

00:05:15.260 --> 00:05:18.260
slice after slice.

00:05:18.260 --> 00:05:20.260
If we continue through the entire stack,

00:05:20.260 --> 00:05:23.260
we can reconstruct the three-dimensional shape

00:05:23.260 --> 00:05:26.260
of a small fragment of a branch of a neuron.

00:05:26.260 --> 00:05:28.260
And we can do that for another neuron in green.

00:05:28.260 --> 00:05:30.260
And you can see that the green neuron touches the red neuron

00:05:30.260 --> 00:05:32.260
at two locations,

00:05:32.260 --> 00:05:34.260
and these are what are called synapses.

00:05:34.260 --> 00:05:36.260
Let's zoom in on one synapse,

00:05:36.260 --> 00:05:39.260
and keep your eyes on the interior of the green neuron.

00:05:39.260 --> 00:05:41.260
You should see small circles --

00:05:41.260 --> 00:05:44.260
these are called vesicles.

00:05:44.260 --> 00:05:47.260
They contain a molecule know as a neurotransmitter.

00:05:47.260 --> 00:05:49.260
And so when the green neuron wants to communicate,

00:05:49.260 --> 00:05:51.260
it wants to send a message to the red neuron,

00:05:51.260 --> 00:05:54.260
it spits out neurotransmitter.

00:05:54.260 --> 00:05:56.260
At the synapse, the two neurons

00:05:56.260 --> 00:05:58.260
are said to be connected

00:05:58.260 --> 00:06:01.260
like two friends talking on the telephone.

00:06:02.260 --> 00:06:04.260
So you see how to find a synapse.

00:06:04.260 --> 00:06:07.260
How can we find an entire connectome?

00:06:07.260 --> 00:06:10.260
Well, we take this three-dimensional stack of images

00:06:10.260 --> 00:06:13.260
and treat it as a gigantic three-dimensional coloring book.

00:06:13.260 --> 00:06:16.260
We color every neuron in, in a different color,

00:06:16.260 --> 00:06:18.260
and then we look through all of the images,

00:06:18.260 --> 00:06:20.260
find the synapses

00:06:20.260 --> 00:06:23.260
and note the colors of the two neurons involved in each synapse.

00:06:23.260 --> 00:06:26.260
If we can do that throughout all the images,

00:06:26.260 --> 00:06:28.260
we could find a connectome.

00:06:29.260 --> 00:06:31.260
Now, at this point,

00:06:31.260 --> 00:06:33.260
you've learned the basics of neurons and synapses.

00:06:33.260 --> 00:06:35.260
And so I think we're ready to tackle

00:06:35.260 --> 00:06:38.260
one of the most important questions in neuroscience:

00:06:39.260 --> 00:06:42.260
how are the brains of men and women different?

00:06:42.260 --> 00:06:44.260
(Laughter)

00:06:44.260 --> 00:06:46.260
According to this self-help book,

00:06:46.260 --> 00:06:48.260
guys brains are like waffles;

00:06:48.260 --> 00:06:51.260
they keep their lives compartmentalized in boxes.

00:06:51.260 --> 00:06:54.260
Girls' brains are like spaghetti;

00:06:54.260 --> 00:06:57.260
everything in their life is connected to everything else.

00:06:57.260 --> 00:06:59.260
(Laughter)

00:06:59.260 --> 00:07:01.260
You guys are laughing,

00:07:01.260 --> 00:07:03.260
but you know, this book changed my life.

00:07:03.260 --> 00:07:05.260
(Laughter)

00:07:07.260 --> 00:07:10.260
But seriously, what's wrong with this?

00:07:10.260 --> 00:07:13.260
You already know enough to tell me -- what's wrong with this statement?

00:07:20.260 --> 00:07:23.260
It doesn't matter whether you're a guy or girl,

00:07:23.260 --> 00:07:26.260
everyone's brains are like spaghetti.

00:07:26.260 --> 00:07:29.260
Or maybe really, really fine capellini with branches.

00:07:30.260 --> 00:07:32.260
Just as one strand of spaghetti

00:07:32.260 --> 00:07:35.260
contacts many other strands on your plate,

00:07:35.260 --> 00:07:37.260
one neuron touches many other neurons

00:07:37.260 --> 00:07:39.260
through their entangled branches.

00:07:39.260 --> 00:07:42.260
One neuron can be connected to so many other neurons,

00:07:42.260 --> 00:07:44.260
because there can be synapses

00:07:44.260 --> 00:07:47.260
at these points of contact.

00:07:49.260 --> 00:07:52.260
By now, you might have sort of lost perspective

00:07:52.260 --> 00:07:55.260
on how large this cube of brain tissue actually is.

00:07:55.260 --> 00:07:58.260
And so let's do a series of comparisons to show you.

00:07:58.260 --> 00:08:01.260
I assure you, this is very tiny. It's just six microns on a side.

00:08:03.260 --> 00:08:06.260
So, here's how it stacks up against an entire neuron.

00:08:06.260 --> 00:08:09.260
And you can tell that, really, only the smallest fragments of branches

00:08:09.260 --> 00:08:12.260
are contained inside this cube.

00:08:12.260 --> 00:08:15.260
And a neuron, well, that's smaller than brain.

00:08:17.260 --> 00:08:19.260
And that's just a mouse brain --

00:08:21.260 --> 00:08:24.260
it's a lot smaller than a human brain.

00:08:25.260 --> 00:08:27.260
So when show my friends this,

00:08:27.260 --> 00:08:29.260
sometimes they've told me,

00:08:29.260 --> 00:08:32.260
"You know, Sebastian, you should just give up.

00:08:32.260 --> 00:08:34.260
Neuroscience is hopeless."

00:08:34.260 --> 00:08:36.260
Because if you look at a brain with your naked eye,

00:08:36.260 --> 00:08:38.260
you don't really see how complex it is,

00:08:38.260 --> 00:08:40.260
but when you use a microscope,

00:08:40.260 --> 00:08:43.260
finally the hidden complexity is revealed.

00:08:45.260 --> 00:08:47.260
In the 17th century,

00:08:47.260 --> 00:08:49.260
the mathematician and philosopher, Blaise Pascal,

00:08:49.260 --> 00:08:52.260
wrote of his dread of the infinite,

00:08:52.260 --> 00:08:54.260
his feeling of insignificance

00:08:54.260 --> 00:08:57.260
at contemplating the vast reaches of outer space.

00:08:59.260 --> 00:09:01.260
And, as a scientist,

00:09:01.260 --> 00:09:04.260
I'm not supposed to talk about my feelings --

00:09:04.260 --> 00:09:06.260
too much information, professor.

00:09:06.260 --> 00:09:08.260
(Laughter)

00:09:08.260 --> 00:09:10.260
But may I?

00:09:10.260 --> 00:09:12.260
(Laughter)

00:09:12.260 --> 00:09:14.260
(Applause)

00:09:14.260 --> 00:09:16.260
I feel curiosity,

00:09:16.260 --> 00:09:18.260
and I feel wonder,

00:09:18.260 --> 00:09:21.260
but at times I have also felt despair.

00:09:22.260 --> 00:09:24.260
Why did I choose to study

00:09:24.260 --> 00:09:27.260
this organ that is so awesome in its complexity

00:09:27.260 --> 00:09:29.260
that it might well be infinite?

00:09:29.260 --> 00:09:31.260
It's absurd.

00:09:31.260 --> 00:09:33.260
How could we even dare to think

00:09:33.260 --> 00:09:36.260
that we might ever understand this?

00:09:38.260 --> 00:09:41.260
And yet, I persist in this quixotic endeavor.

00:09:41.260 --> 00:09:44.260
And indeed, these days I harbor new hopes.

00:09:45.260 --> 00:09:47.260
Someday,

00:09:47.260 --> 00:09:49.260
a fleet of microscopes will capture

00:09:49.260 --> 00:09:51.260
every neuron and every synapse

00:09:51.260 --> 00:09:54.260
in a vast database of images.

00:09:54.260 --> 00:09:57.260
And some day, artificially intelligent supercomputers

00:09:57.260 --> 00:10:00.260
will analyze the images without human assistance

00:10:00.260 --> 00:10:03.260
to summarize them in a connectome.

00:10:04.260 --> 00:10:07.260
I do not know, but I hope that I will live to see that day,

00:10:08.260 --> 00:10:10.260
because finding an entire human connectome

00:10:10.260 --> 00:10:13.260
is one of the greatest technological challenges of all time.

00:10:13.260 --> 00:10:16.260
It will take the work of generations to succeed.

00:10:17.260 --> 00:10:20.260
At the present time, my collaborators and I,

00:10:20.260 --> 00:10:22.260
what we're aiming for is much more modest --

00:10:22.260 --> 00:10:24.260
just to find partial connectomes

00:10:24.260 --> 00:10:27.260
of tiny chunks of mouse and human brain.

00:10:27.260 --> 00:10:30.260
But even that will be enough for the first tests of this hypothesis

00:10:30.260 --> 00:10:33.260
that I am my connectome.

00:10:35.260 --> 00:10:38.260
For now, let me try to convince you of the plausibility of this hypothesis,

00:10:38.260 --> 00:10:41.260
that it's actually worth taking seriously.

00:10:42.260 --> 00:10:44.260
As you grow during childhood

00:10:44.260 --> 00:10:47.260
and age during adulthood,

00:10:47.260 --> 00:10:50.260
your personal identity changes slowly.

00:10:50.260 --> 00:10:52.260
Likewise, every connectome

00:10:52.260 --> 00:10:54.260
changes over time.

00:10:55.260 --> 00:10:57.260
What kinds of changes happen?

00:10:57.260 --> 00:10:59.260
Well, neurons, like trees,

00:10:59.260 --> 00:11:01.260
can grow new branches,

00:11:01.260 --> 00:11:04.260
and they can lose old ones.

00:11:04.260 --> 00:11:07.260
Synapses can be created,

00:11:07.260 --> 00:11:10.260
and they can be eliminated.

00:11:10.260 --> 00:11:12.260
And synapses can grow larger,

00:11:12.260 --> 00:11:15.260
and they can grow smaller.

00:11:15.260 --> 00:11:17.260
Second question:

00:11:17.260 --> 00:11:20.260
what causes these changes?

00:11:20.260 --> 00:11:22.260
Well, it's true.

00:11:22.260 --> 00:11:25.260
To some extent, they are programmed by your genes.

00:11:25.260 --> 00:11:27.260
But that's not the whole story,

00:11:27.260 --> 00:11:29.260
because there are signals, electrical signals,

00:11:29.260 --> 00:11:31.260
that travel along the branches of neurons

00:11:31.260 --> 00:11:33.260
and chemical signals

00:11:33.260 --> 00:11:35.260
that jump across from branch to branch.

00:11:35.260 --> 00:11:38.260
These signals are called neural activity.

00:11:38.260 --> 00:11:40.260
And there's a lot of evidence

00:11:40.260 --> 00:11:43.260
that neural activity

00:11:43.260 --> 00:11:46.260
is encoding our thoughts, feelings and perceptions,

00:11:46.260 --> 00:11:48.260
our mental experiences.

00:11:48.260 --> 00:11:51.260
And there's a lot of evidence that neural activity

00:11:51.260 --> 00:11:54.260
can cause your connections to change.

00:11:54.260 --> 00:11:57.260
And if you put those two facts together,

00:11:57.260 --> 00:11:59.260
it means that your experiences

00:11:59.260 --> 00:12:02.260
can change your connectome.

00:12:02.260 --> 00:12:04.260
And that's why every connectome is unique,

00:12:04.260 --> 00:12:07.260
even those of genetically identical twins.

00:12:08.260 --> 00:12:11.260
The connectome is where nature meets nurture.

00:12:12.260 --> 00:12:14.260
And it might true

00:12:14.260 --> 00:12:16.260
that just the mere act of thinking

00:12:16.260 --> 00:12:18.260
can change your connectome --

00:12:18.260 --> 00:12:21.260
an idea that you may find empowering.

00:12:24.260 --> 00:12:26.260
What's in this picture?

00:12:28.260 --> 00:12:31.260
A cool and refreshing stream of water, you say.

00:12:32.260 --> 00:12:34.260
What else is in this picture?

00:12:37.260 --> 00:12:39.260
Do not forget that groove in the Earth

00:12:39.260 --> 00:12:42.260
called the stream bed.

00:12:42.260 --> 00:12:45.260
Without it, the water would not know in which direction to flow.

00:12:45.260 --> 00:12:47.260
And with the stream,

00:12:47.260 --> 00:12:49.260
I would like to propose a metaphor

00:12:49.260 --> 00:12:51.260
for the relationship between neural activity

00:12:51.260 --> 00:12:53.260
and connectivity.

00:12:54.260 --> 00:12:57.260
Neural activity is constantly changing.

00:12:57.260 --> 00:13:00.260
It's like the water of the stream; it never sits still.

00:13:00.260 --> 00:13:02.260
The connections

00:13:02.260 --> 00:13:04.260
of the brain's neural network

00:13:04.260 --> 00:13:06.260
determines the pathways

00:13:06.260 --> 00:13:08.260
along which neural activity flows.

00:13:08.260 --> 00:13:11.260
And so the connectome is like bed of the stream;

00:13:13.260 --> 00:13:16.260
but the metaphor is richer than that,

00:13:16.260 --> 00:13:19.260
because it's true that the stream bed

00:13:19.260 --> 00:13:21.260
guides the flow of the water,

00:13:21.260 --> 00:13:23.260
but over long timescales,

00:13:23.260 --> 00:13:26.260
the water also reshapes the bed of the stream.

00:13:26.260 --> 00:13:28.260
And as I told you just now,

00:13:28.260 --> 00:13:31.260
neural activity can change the connectome.

00:13:33.260 --> 00:13:35.260
And if you'll allow me to ascend

00:13:35.260 --> 00:13:38.260
to metaphorical heights,

00:13:38.260 --> 00:13:41.260
I will remind you that neural activity

00:13:41.260 --> 00:13:43.260
is the physical basis -- or so neuroscientists think --

00:13:43.260 --> 00:13:46.260
of thoughts, feelings and perceptions.

00:13:46.260 --> 00:13:48.260
And so we might even speak of

00:13:48.260 --> 00:13:50.260
the stream of consciousness.

00:13:50.260 --> 00:13:53.260
Neural activity is its water,

00:13:53.260 --> 00:13:56.260
and the connectome is its bed.

00:13:57.260 --> 00:13:59.260
So let's return from the heights of metaphor

00:13:59.260 --> 00:14:01.260
and return to science.

00:14:01.260 --> 00:14:03.260
Suppose our technologies for finding connectomes

00:14:03.260 --> 00:14:05.260
actually work.

00:14:05.260 --> 00:14:07.260
How will we go about testing the hypothesis

00:14:07.260 --> 00:14:10.260
"I am my connectome?"

00:14:10.260 --> 00:14:13.260
Well, I propose a direct test.

00:14:13.260 --> 00:14:15.260
Let us attempt

00:14:15.260 --> 00:14:18.260
to read out memories from connectomes.

00:14:18.260 --> 00:14:20.260
Consider the memory

00:14:20.260 --> 00:14:23.260
of long temporal sequences of movements,

00:14:23.260 --> 00:14:26.260
like a pianist playing a Beethoven sonata.

00:14:26.260 --> 00:14:29.260
According to a theory that dates back to the 19th century,

00:14:29.260 --> 00:14:31.260
such memories are stored

00:14:31.260 --> 00:14:34.260
as chains of synaptic connections inside your brain.

00:14:35.260 --> 00:14:38.260
Because, if the first neurons in the chain are activated,

00:14:38.260 --> 00:14:41.260
through their synapses they send messages to the second neurons, which are activated,

00:14:41.260 --> 00:14:43.260
and so on down the line,

00:14:43.260 --> 00:14:45.260
like a chain of falling dominoes.

00:14:45.260 --> 00:14:47.260
And this sequence of neural activation

00:14:47.260 --> 00:14:50.260
is hypothesized to be the neural basis

00:14:50.260 --> 00:14:52.260
of those sequence of movements.

00:14:52.260 --> 00:14:54.260
So one way of trying to test the theory

00:14:54.260 --> 00:14:56.260
is to look for such chains

00:14:56.260 --> 00:14:58.260
inside connectomes.

00:14:58.260 --> 00:15:01.260
But it won't be easy, because they're not going to look like this.

00:15:01.260 --> 00:15:03.260
They're going to be scrambled up.

00:15:03.260 --> 00:15:05.260
So we'll have to use our computers

00:15:05.260 --> 00:15:08.260
to try to unscramble the chain.

00:15:08.260 --> 00:15:10.260
And if we can do that,

00:15:10.260 --> 00:15:13.260
the sequence of the neurons we recover from that unscrambling

00:15:13.260 --> 00:15:16.260
will be a prediction of the pattern of neural activity

00:15:16.260 --> 00:15:19.260
that is replayed in the brain during memory recall.

00:15:19.260 --> 00:15:21.260
And if that were successful,

00:15:21.260 --> 00:15:24.260
that would be the first example of reading a memory from a connectome.

00:15:28.260 --> 00:15:30.260
(Laughter)

00:15:30.260 --> 00:15:32.260
What a mess --

00:15:33.260 --> 00:15:35.260
have you ever tried to wire up a system

00:15:35.260 --> 00:15:37.260
as complex as this?

00:15:37.260 --> 00:15:39.260
I hope not.

00:15:39.260 --> 00:15:42.260
But if you have, you know it's very easy to make a mistake.

00:15:45.260 --> 00:15:47.260
The branches of neurons are like the wires of the brain.

00:15:47.260 --> 00:15:51.260
Can anyone guess: what's the total length of wires in your brain?

00:15:54.260 --> 00:15:56.260
I'll give you a hint. It's a big number.

00:15:56.260 --> 00:15:58.260
(Laughter)

00:15:59.260 --> 00:16:02.260
I estimate, millions of miles,

00:16:02.260 --> 00:16:05.260
all packed in your skull.

00:16:05.260 --> 00:16:07.260
And if you appreciate that number,

00:16:07.260 --> 00:16:09.260
you can easily see

00:16:09.260 --> 00:16:11.260
there is huge potential for mis-wiring of the brain.

00:16:11.260 --> 00:16:14.260
And indeed, the popular press loves headlines like,

00:16:14.260 --> 00:16:16.260
"Anorexic brains are wired differently,"

00:16:16.260 --> 00:16:18.260
or "Autistic brains are wired differently."

00:16:18.260 --> 00:16:20.260
These are plausible claims,

00:16:20.260 --> 00:16:22.260
but in truth,

00:16:22.260 --> 00:16:24.260
we can't see the brain's wiring clearly enough

00:16:24.260 --> 00:16:26.260
to tell if these are really true.

00:16:26.260 --> 00:16:29.260
And so the technologies for seeing connectomes

00:16:29.260 --> 00:16:31.260
will allow us to finally

00:16:31.260 --> 00:16:33.260
read mis-wiring of the brain,

00:16:33.260 --> 00:16:36.260
to see mental disorders in connectomes.

00:16:40.260 --> 00:16:43.260
Sometimes the best way to test a hypothesis

00:16:43.260 --> 00:16:46.260
is to consider its most extreme implication.

00:16:46.260 --> 00:16:49.260
Philosophers know this game very well.

00:16:50.260 --> 00:16:53.260
If you believe that I am my connectome,

00:16:53.260 --> 00:16:56.260
I think you must also accept the idea

00:16:56.260 --> 00:16:58.260
that death is the destruction

00:16:58.260 --> 00:17:01.260
of your connectome.

00:17:02.260 --> 00:17:05.260
I mention this because there are prophets today

00:17:05.260 --> 00:17:08.260
who claim that technology

00:17:08.260 --> 00:17:11.260
will fundamentally alter the human condition

00:17:11.260 --> 00:17:14.260
and perhaps even transform the human species.

00:17:14.260 --> 00:17:17.260
One of their most cherished dreams

00:17:17.260 --> 00:17:19.260
is to cheat death

00:17:19.260 --> 00:17:21.260
by that practice known as cryonics.

00:17:21.260 --> 00:17:23.260
If you pay 100,000 dollars,

00:17:23.260 --> 00:17:26.260
you can arrange to have your body frozen after death

00:17:26.260 --> 00:17:28.260
and stored in liquid nitrogen

00:17:28.260 --> 00:17:30.260
in one of these tanks in an Arizona warehouse,

00:17:30.260 --> 00:17:32.260
awaiting a future civilization

00:17:32.260 --> 00:17:35.260
that is advanced to resurrect you.

00:17:36.260 --> 00:17:38.260
Should we ridicule the modern seekers of immortality,

00:17:38.260 --> 00:17:40.260
calling them fools?

00:17:40.260 --> 00:17:42.260
Or will they someday chuckle

00:17:42.260 --> 00:17:44.260
over our graves?

00:17:45.260 --> 00:17:47.260
I don't know --

00:17:47.260 --> 00:17:50.260
I prefer to test their beliefs, scientifically.

00:17:50.260 --> 00:17:52.260
I propose that we attempt to find a connectome

00:17:52.260 --> 00:17:54.260
of a frozen brain.

00:17:54.260 --> 00:17:56.260
We know that damage to the brain

00:17:56.260 --> 00:17:58.260
occurs after death and during freezing.

00:17:58.260 --> 00:18:01.260
The question is: has that damage erased the connectome?

00:18:01.260 --> 00:18:04.260
If it has, there is no way that any future civilization

00:18:04.260 --> 00:18:07.260
will be able to recover the memories of these frozen brains.

00:18:07.260 --> 00:18:09.260
Resurrection might succeed for the body,

00:18:09.260 --> 00:18:11.260
but not for the mind.

00:18:11.260 --> 00:18:14.260
On the other hand, if the connectome is still intact,

00:18:14.260 --> 00:18:17.260
we cannot ridicule the claims of cryonics so easily.

00:18:20.260 --> 00:18:22.260
I've described a quest

00:18:22.260 --> 00:18:25.260
that begins in the world of the very small,

00:18:25.260 --> 00:18:28.260
and propels us to the world of the far future.

00:18:28.260 --> 00:18:31.260
Connectomes will mark a turning point in human history.

00:18:32.260 --> 00:18:34.260
As we evolved from our ape-like ancestors

00:18:34.260 --> 00:18:36.260
on the African savanna,

00:18:36.260 --> 00:18:39.260
what distinguished us was our larger brains.

00:18:40.260 --> 00:18:42.260
We have used our brains to fashion

00:18:42.260 --> 00:18:45.260
ever more amazing technologies.

00:18:45.260 --> 00:18:48.260
Eventually, these technologies will become so powerful

00:18:48.260 --> 00:18:51.260
that we will use them to know ourselves

00:18:51.260 --> 00:18:54.260
by deconstructing and reconstructing

00:18:54.260 --> 00:18:57.260
our own brains.

00:18:57.260 --> 00:19:00.260
I believe that this voyage of self-discovery

00:19:00.260 --> 00:19:03.260
is not just for scientists,

00:19:03.260 --> 00:19:05.260
but for all of us.

00:19:05.260 --> 00:19:08.260
And I'm grateful for the opportunity to share this voyage with you today.

00:19:08.260 --> 00:19:10.260
Thank you.

00:19:10.260 --> 00:19:18.260
(Applause)

