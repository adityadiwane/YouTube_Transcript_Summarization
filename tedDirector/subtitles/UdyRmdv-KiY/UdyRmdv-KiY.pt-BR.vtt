WEBVTT
Kind: captions
Language: pt-BR

00:00:00.000 --> 00:00:18.330
Tradutor: Hitomi Iwamoto
Revisor: Tulio Leao

00:00:18.330 --> 00:00:23.330
O que quero lhes dizer hoje é como vejo os robôs invadindo nossas vidas

00:00:23.330 --> 00:00:26.330
em vários aspectos, em múltiplos momentos.

00:00:26.330 --> 00:00:30.330
E quando olho para o futuro, não consigo imaginar um mundo, daqui a 500 anos,

00:00:30.330 --> 00:00:32.330
em que nós não tenhamos robôs em todos os lugares,

00:00:32.330 --> 00:00:37.330
presumindo, apesar de todas as más previsões que muitos fazem sobre o futuro,

00:00:37.330 --> 00:00:41.330
que estaremos por aqui, não consigo imaginar o mundo não-populado por robôs.

00:00:41.330 --> 00:00:44.330
A questão é, se eles estiverem por aqui em 500 anos,

00:00:44.330 --> 00:00:46.330
será que estarão em todo lugar mais cedo que isso?

00:00:46.330 --> 00:00:48.330
Eles estarão por aí em 50 anos?

00:00:48.330 --> 00:00:51.330
Sim, acho que isso é bem provável. Haverá muitos robôs em todos lugares.

00:00:51.330 --> 00:00:54.330
De fato, acho que será muito mais cedo do que isso.

00:00:54.330 --> 00:00:58.330
Acho que está bem próximo dos robôs se tornarem comuns,

00:00:58.330 --> 00:01:04.330
e acho que estamos próximos de 1978 ou 1980 em termos de computadores pessoais,

00:01:04.330 --> 00:01:07.330
em que os primeiros robôs estão começando a aparecer.

00:01:07.330 --> 00:01:11.330
Os computadores meio que vieram através de jogos e brinquedos.

00:01:11.330 --> 00:01:14.330
O primeiro computador que as pessoas tiveram em casa

00:01:14.330 --> 00:01:16.330
pode ter sido um computador para jogar Pong,

00:01:16.330 --> 00:01:18.330
com um pequeno microprocessador embutido,

00:01:18.330 --> 00:01:21.330
e alguns jogos que vieram depois disso.

00:01:21.330 --> 00:01:24.330
E estamos começando a ver o mesmo tipo de coisa com os robôs:

00:01:24.330 --> 00:01:28.330
Lego Mindstorms, Furbies... Quem aqui teve um Furby?

00:01:28.330 --> 00:01:31.330
Sim, foram vendidos 38 milhões deles pelo mundo.

00:01:31.330 --> 00:01:33.330
Eles são bem comuns, e eles são pequenos robozinhos,

00:01:33.330 --> 00:01:35.330
um simples robô com alguns sensores.

00:01:35.330 --> 00:01:37.330
É uma atuação de pequeno processamento.

00:01:37.330 --> 00:01:40.330
Na direita há uma boneca-robô que você podia comprar há uns anos.

00:01:40.330 --> 00:01:42.330
E da mesma maneira que, nos primórdios,

00:01:42.330 --> 00:01:47.330
havia muita interação amadora acerca de computadores,

00:01:47.330 --> 00:01:51.330
hoje é possível conseguir vários kits e livros sobre hacking.

00:01:51.330 --> 00:01:55.330
À esquerda há uma plataforma da Evolution Robotics,

00:01:55.330 --> 00:01:58.330
você a conecta num computador para programá-la com uma GUI (Interface Gráfica Unificada)

00:01:58.330 --> 00:02:01.330
para que ela ande pela sua casa e faça várias coisas.

00:02:01.330 --> 00:02:04.330
E há também alguns tipos de brinquedos-robôs mais caros,

00:02:04.330 --> 00:02:08.330
o Aibo da Sony. E à direita, o robô desenvolvido pela NEC,

00:02:08.330 --> 00:02:11.330
o PaPeRo, que acho que não lançarão.

00:02:11.330 --> 00:02:14.330
Mas apesar disso, esse tipo de coisa está por aí.

00:02:14.330 --> 00:02:18.330
E vimos, nos últimos dois ou três anos, robôs que cortam a grama,

00:02:18.330 --> 00:02:24.330
o Husqvarna embaixo, e o Friendly Robotics em cima, de uma companhia Israelense.

00:02:24.330 --> 00:02:26.330
E nos últimos 12 meses

00:02:26.330 --> 00:02:30.330
começaram a surgir um monte de robôs que limpam a casa.

00:02:30.330 --> 00:02:33.330
O de cima, à esquerda, é um robô bem legal que limpa a casa

00:02:33.330 --> 00:02:37.330
feito pela Dyson, no Reino Unido. Mas era tão caro,

00:02:37.330 --> 00:02:39.330
3.500 dólares, que nem o lançaram.

00:02:39.330 --> 00:02:42.330
Porém, logo abaixo, à esquerda, há o Electrolux, que está à venda.

00:02:42.330 --> 00:02:44.330
O outro é da Karcher.

00:02:44.330 --> 00:02:46.330
Abaixo, à direita, há um que construí em meu laboratório

00:02:46.330 --> 00:02:49.330
há cerca de 10 anos, e finalmente o transformamos em um produto.

00:02:49.330 --> 00:02:51.330
Vou mostrar como ficou.

00:02:51.330 --> 00:02:55.330
Daremos um desse para alguém depois da palestra.

00:02:55.330 --> 00:03:01.330
Este é um robô que já está nas lojas, ele limpará seu chão.

00:03:05.330 --> 00:03:10.330
Ele começa zanzando por aí e vai percorrendo círculos cada vez maiores.

00:03:10.330 --> 00:03:14.330
Se ele tocar alguma coisa -- vocês viram isso?

00:03:14.330 --> 00:03:17.330
Agora está seguindo paredes, está andando em volta do meu pé

00:03:17.330 --> 00:03:21.330
para limpar ao meu redor. Vamos ver...

00:03:21.330 --> 00:03:26.330
Oh, quem roubou meu Rice Krispies? Roubaram meu Rice Krispies.

00:03:26.330 --> 00:03:32.330
(Risos)

00:03:32.330 --> 00:03:35.330
Não se preocupem, relaxem, é um robô, é esperto.

00:03:35.330 --> 00:03:38.330
(Risos)

00:03:38.330 --> 00:03:42.330
Crianças de três anos de idade não se preocupam com isso.

00:03:42.330 --> 00:03:44.330
São os adultos que ficam irritados.

00:03:44.330 --> 00:03:45.330
(Risos)

00:03:45.330 --> 00:03:47.330
Vamos colocar algumas porcarias aqui.

00:03:47.330 --> 00:03:51.330
(Risos)

00:03:51.330 --> 00:03:53.330
OK.

00:03:53.330 --> 00:03:57.330
(Risos)

00:03:57.330 --> 00:04:00.330
Não sei se conseguem ver, coloquei alguns Rice Krispies ali,

00:04:00.330 --> 00:04:07.330
algumas moedas, vamos colocá-lo lá e ver se ele limpa tudo.

00:04:10.330 --> 00:04:12.330
Sim, OK.

00:04:12.330 --> 00:04:16.330
Vamos deixar isso para depois.

00:04:16.330 --> 00:04:21.330
(Aplausos)

00:04:22.330 --> 00:04:26.330
Parte do truque foi construir um mecanismo de limpeza melhor, na verdade;

00:04:26.330 --> 00:04:30.330
a inteligência interna é razoavelmente simples.

00:04:30.330 --> 00:04:32.330
E ocorre o mesmo com um monte de robôs.

00:04:32.330 --> 00:04:36.330
Todos nós, imagino, nos tornamos um pouco chauvinistas computacionais,

00:04:36.330 --> 00:04:38.330
e pensamos que computação é tudo,

00:04:38.330 --> 00:04:40.330
mas a mecânica ainda importa.

00:04:40.330 --> 00:04:43.330
Aqui está outro robô, o PackBot,

00:04:43.330 --> 00:04:45.330
que estamos construindo já há alguns anos.

00:04:45.330 --> 00:04:51.330
É um robô militar de vigilância, que vai à frente das tropas,

00:04:51.330 --> 00:04:54.330
procurando cavernas, por exemplo.

00:04:54.330 --> 00:04:56.330
Porém, tivemos que fazê-lo mais robusto,

00:04:56.330 --> 00:05:03.330
muito mais que os robôs que construímos em nossos laboratórios.

00:05:03.330 --> 00:05:06.330
(Risos)

00:05:12.330 --> 00:05:16.330
Esse robô é um PC rodando Linux.

00:05:16.330 --> 00:05:22.330
Ele aguenta um choque de 400G. O robô tem uma inteligência local:

00:05:22.330 --> 00:05:28.330
ele se vira sozinho, e coloca a si mesmo no alcance da comunicação,

00:05:28.330 --> 00:05:31.330
consegue subir escadas por si só, etc.

00:05:38.330 --> 00:05:42.330
Ali ele está fazendo navegação local.

00:05:42.330 --> 00:05:48.330
Um soldado dá um comando para subir escadas, e ele o faz.

00:05:49.330 --> 00:05:52.330
Não foi uma caída planejada...

00:05:52.330 --> 00:05:54.330
(Risos)

00:05:54.330 --> 00:05:56.330
Agora ele irá partir.

00:05:56.330 --> 00:06:01.330
Esses robôs realmente se sobressaíram no 11 de Setembro.

00:06:01.330 --> 00:06:05.330
Nós tínhamos robôs no World Trade Center tarde da noite.

00:06:06.330 --> 00:06:08.330
Não dava para fazer muita coisa nas principais pilhas de escombros,

00:06:08.330 --> 00:06:11.330
as coisas estavam muito -- não havia nada que pudesse ser feito.

00:06:11.330 --> 00:06:16.330
Mas fomos a todos os edifícios que haviam sido evacuados ao redor,

00:06:16.330 --> 00:06:19.330
e procuramos por possíveis sobreviventes nos prédios

00:06:19.330 --> 00:06:21.330
que eram muito perigosos de se entrar.

00:06:21.330 --> 00:06:23.330
Vamos ao vídeo.

00:06:23.330 --> 00:06:26.330
Repórter: ...companheiros de guerra estão ajudando a reduzir os riscos de combate.

00:06:26.330 --> 00:06:29.330
Nick Robertson cobriu a história.

00:06:31.330 --> 00:06:33.330
Rodney Brooks: Podemos mostrar mais outro?

00:06:38.330 --> 00:06:40.330
Tudo certo.

00:06:43.330 --> 00:06:46.330
Então, este é um cabo que havia visto um robô duas semanas antes.

00:06:48.330 --> 00:06:52.330
Ele está enviando robôs a cavernas, vendo o que se passa.

00:06:52.330 --> 00:06:54.330
O robô é totalmente autônomo.

00:06:54.330 --> 00:06:58.330
A pior coisa que aconteceu na caverna até agora

00:06:58.330 --> 00:07:01.330
foi que um dos robôs caiu dez metros.

00:07:08.330 --> 00:07:11.330
Há um ano, o exército dos EUA não tinha esses robôs.

00:07:11.330 --> 00:07:13.330
Agora eles estão cumprindo seu dever no Afeganistão todos os dias.

00:07:13.330 --> 00:07:16.330
E é por isso que dizem que está ocorrendo uma invasão de robôs.

00:07:16.330 --> 00:07:20.330
A maré está mudando quanto aos rumos que a tecnologia está tomando.

00:07:20.330 --> 00:07:22.330
Obrigado.

00:07:23.330 --> 00:07:25.330
E nos próximos meses,

00:07:25.330 --> 00:07:28.330
vamos enviar robôs que estão na linha de produção

00:07:28.330 --> 00:07:32.330
a poços de petróleo para extrair os últimos anos de petróleo da terra.

00:07:32.330 --> 00:07:36.330
Ambientes muito hostis, 150 graus centígrados, pressão de 690 atmosferas.

00:07:36.330 --> 00:07:40.330
Robôs autônomos indo e fazendo esse tipo de trabalho.

00:07:40.330 --> 00:07:43.330
Mas robôs desse tipo são um pouco difíceis de programar.

00:07:43.330 --> 00:07:45.330
Como iremos programar nossos robôs no futuro

00:07:45.330 --> 00:07:47.330
e torná-los mais fáceis de usar?

00:07:47.330 --> 00:07:50.330
Vou usar um robô de verdade aqui,

00:07:50.330 --> 00:07:55.330
um robô chamado Chris, levante-se. Sim. Certo.

00:07:57.330 --> 00:08:01.330
Venha aqui. Percebam, ele pensa que robôs devem ser meio duros.

00:08:01.330 --> 00:08:04.330
Ele faz assim. Eu irei --

00:08:04.330 --> 00:08:06.330
Chris Anderson: Eu sou apenas inglês. RB: Oh.

00:08:06.330 --> 00:08:08.330
(Risos)

00:08:08.330 --> 00:08:10.330
(Aplausos)

00:08:10.330 --> 00:08:13.330
Eu mostrarei a esse robô uma tarefa. Uma tarefa muito complexa.

00:08:13.330 --> 00:08:16.330
Percebam, ele assentiu com a cabeça, me dando uma indicação

00:08:16.330 --> 00:08:19.330
de que estava entendendo o fluxo da comunicação.

00:08:19.330 --> 00:08:21.330
E se eu tivesse dito algo completamente bizarro

00:08:21.330 --> 00:08:24.330
ele me olharia de rabo de olho e regularia a conversa.

00:08:24.330 --> 00:08:27.330
Agora coloco isso na frente dele.

00:08:27.330 --> 00:08:31.330
Olho nos olhos dele, e noto que ele observou a tampa da garrafa.

00:08:31.330 --> 00:08:33.330
Estou realizando esta tarefa, e ele está observando.

00:08:33.330 --> 00:08:36.330
Os olhos dele passam por ali e por mim, para ver o que estou olhando,

00:08:36.330 --> 00:08:38.330
logo temos uma atenção compartilhada.

00:08:38.330 --> 00:08:41.330
Realizo essa tarefa, ele vê, e olha para mim

00:08:41.330 --> 00:08:45.330
para ver o que acontecerá em seguida. E agora lhe darei a garrafa,

00:08:45.330 --> 00:08:47.330
e veremos se ele consegue realizar a tarefa. Consegue?

00:08:47.330 --> 00:08:50.330
(Risos)

00:08:50.330 --> 00:08:54.330
Ok. Ele é muito bom. Sim. Bom, bom, bom.

00:08:54.330 --> 00:08:56.330
Eu não te mostrei como fazer isso.

00:08:56.330 --> 00:08:58.330
Vejamos se consegue colocar de volta no lugar.

00:08:58.330 --> 00:09:00.330
(Risos)

00:09:00.330 --> 00:09:01.330
Ele acha que robôs precisam ser bem lentos.

00:09:01.330 --> 00:09:03.330
Bom robô, isso é bom.

00:09:03.330 --> 00:09:05.330
Acabamos de ver um monte de coisas.

00:09:06.330 --> 00:09:09.330
Quando nós interagimos,

00:09:09.330 --> 00:09:13.330
tentamos mostrar como fazer algo, direcionamos a atenção visual do outro.

00:09:13.330 --> 00:09:17.330
Este outro nos comunica seu estado interno,

00:09:17.330 --> 00:09:20.330
se está entendendo ou não, regula uma interação social.

00:09:20.330 --> 00:09:22.330
Houve uma atenção compartilhada quando olhamos para a mesma coisa,

00:09:22.330 --> 00:09:26.330
e reconhecemos reforço na comunicação social no final.

00:09:26.330 --> 00:09:29.330
Estamos tentando colocar isso em nossos robôs, no laboratório,

00:09:29.330 --> 00:09:33.330
pois achamos que é assim que vamos interagir com robôs no futuro.

00:09:33.330 --> 00:09:35.330
Gostaria de mostrar a vocês apenas um diagrama técnico.

00:09:35.330 --> 00:09:39.330
A coisa mais importante ao construir um robô capaz de interagir socialmente

00:09:39.330 --> 00:09:41.330
é seu sistema de atenção visual.

00:09:41.330 --> 00:09:44.330
Porque ele presta atenção naquilo que estiver vendo

00:09:44.330 --> 00:09:47.330
e interagindo, e no seu entendimento das ações que ele realiza.

00:09:47.330 --> 00:09:50.330
Então nos videos que mostrarei a vocês,

00:09:50.330 --> 00:09:54.330
verão o sistema de atenção visual em um robô

00:09:54.330 --> 00:09:58.330
que observa o tom de pele quanto a matiz, saturação e luminosidade,

00:09:58.330 --> 00:10:02.330
e que funciona, portanto, com todas as colorações de pele.

00:10:02.330 --> 00:10:04.330
Ele procura cores altamente saturadas, de brinquedos.

00:10:04.330 --> 00:10:06.330
E procura objetos que se movem.

00:10:06.330 --> 00:10:09.330
Ele pesa todo esse conjunto em uma janela de atenção,

00:10:09.330 --> 00:10:11.330
e procura o elemento com a maior pontuação,

00:10:11.330 --> 00:10:13.330
aquele no qual os eventos mais interessantes estão ocorrendo.

00:10:13.330 --> 00:10:17.330
E é naquela direção que ele irá focar seu olhar.

00:10:17.330 --> 00:10:19.330
Ele olha pra lá em cheio.

00:10:19.330 --> 00:10:22.330
E ao mesmo tempo, uma abordagem "de cima para baixo"

00:10:22.330 --> 00:10:25.330
talvez decida que ele se sente solitário e procure por tons de pele,

00:10:25.330 --> 00:10:28.330
ou talvez fique entediado e vá procurar algum brinquedo.

00:10:28.330 --> 00:10:30.330
Então esses pesos mudam.

00:10:30.330 --> 00:10:32.330
E aqui, à direita,

00:10:32.330 --> 00:10:35.330
está o que chamamos de módulo de memória Steven Spielberg.

00:10:35.330 --> 00:10:37.330
Já viram o filme "Inteligência Artificial"? Plateia: Sim.

00:10:37.330 --> 00:10:39.330
RB: É, foi muito ruim, mas

00:10:39.330 --> 00:10:43.330
se lembram de quando Haley Joel Osment, o pequeno robô,

00:10:43.330 --> 00:10:47.330
ficou olhando a fada azul por 2.000 anos sem descolar os olhos dali?

00:10:47.330 --> 00:10:49.330
Isto aqui se livra daquele problema,

00:10:49.330 --> 00:10:53.330
pois é uma curvatura gaussiana que se torna negativa

00:10:53.330 --> 00:10:56.330
e cada vez mais intensa à medida que olha para uma única coisa.

00:10:56.330 --> 00:10:59.330
E ela fica entendiada, então ela passa a olhar pra outra coisa.

00:10:59.330 --> 00:11:03.330
Quando se tem isso -- eis aqui um robô, o Kismet,

00:11:03.330 --> 00:11:07.330
procurando um brinquedo. Percebe-se para onde ele olha.

00:11:07.330 --> 00:11:12.330
Pode-se estimar a direção de seu olhar pelos globos oculares que cobrem a câmera

00:11:12.330 --> 00:11:15.330
e percebe-se quando ele está olhando para o brinquedo.

00:11:15.330 --> 00:11:17.330
E ele tem uma pequena resposta emotiva.

00:11:17.330 --> 00:11:18.330
(Risos)

00:11:18.330 --> 00:11:20.330
Mas ele ainda presta atenção

00:11:20.330 --> 00:11:24.330
se algo mais significante adentrar seu campo de visão,

00:11:24.330 --> 00:11:28.330
como Cynthia Breazeal, a construtora deste robô, pela direita.

00:11:28.330 --> 00:11:33.330
Ele a vê e presta atenção nela.

00:11:33.330 --> 00:11:37.330
Kismet tem um espaço emocional tridimensional subjacente,

00:11:37.330 --> 00:11:40.330
um espaço vetorial de onde ele se encontra emocionalmente.

00:11:40.330 --> 00:11:45.330
E nos diferentes lugares deste espaço ele expressa --

00:11:46.330 --> 00:11:48.330
podemos aumentar o volume?

00:11:48.330 --> 00:11:50.330
Conseguem ouvir de onde estão? Plateia: Sim.

00:11:50.330 --> 00:11:55.330
Kismet: Acha mesmo? Acha mesmo?

00:11:57.330 --> 00:11:59.330
Acha mesmo?

00:12:00.330 --> 00:12:03.330
RB: Ele está expressando suas emoções em seu rosto

00:12:03.330 --> 00:12:05.330
e na prosódia de sua voz.

00:12:05.330 --> 00:12:09.330
Quando eu estava lidando com o meu robô,

00:12:09.330 --> 00:12:12.330
Chris, o robô, estava medindo a prosódia em minha voz,

00:12:12.330 --> 00:12:17.330
e nós fizemos o robô medir a prosódia de quatro mensagens básicas

00:12:17.330 --> 00:12:21.330
que as mães passam aos filhos pré-linguisticamente.

00:12:21.330 --> 00:12:24.330
Aqui temos sujeitos de teste ingênuos elogiando o robô,

00:12:26.330 --> 00:12:28.330
Voz: Robô bonito.

00:12:29.330 --> 00:12:31.330
Você é um robô tão bonitinho.

00:12:31.330 --> 00:12:33.330
(Risos)

00:12:33.330 --> 00:12:35.330
E o robô reage de acordo.

00:12:35.330 --> 00:12:39.330
Voz: ...muito bom, Kismet.

00:12:40.330 --> 00:12:42.330
(Risos)

00:12:42.330 --> 00:12:44.330
Voz: Olhe o meu sorriso.

00:12:46.330 --> 00:12:49.330
RB: Ele sorri. Ela imita o sorriso. Isso ocorre bastante.

00:12:49.330 --> 00:12:51.330
Estes são sujeitos de teste ingênuos.

00:12:51.330 --> 00:12:54.330
Aqui pedimos que atraíssem a atenção do robô

00:12:54.330 --> 00:12:57.330
e indicassem quando a tivessem.

00:12:57.330 --> 00:13:01.330
Voz: Ei, Kismet, ah, aí está.

00:13:01.330 --> 00:13:05.330
RB: Ela percebe que tem a atenção do robô.

00:13:08.330 --> 00:13:12.330
Voz: Kismet, você gosta do brinquedo? Oh.

00:13:13.330 --> 00:13:15.330
RB: Agora pedimos que proíbam o robô de fazer algo,

00:13:15.330 --> 00:13:19.330
e a primeira mulher realmente o deixa bem sensibilizado.

00:13:19.330 --> 00:13:24.330
Voz: Não. Não pode fazer isso. Não.

00:13:24.330 --> 00:13:27.330
(Risos)

00:13:27.330 --> 00:13:33.330
Voz: Não é certo. Não.

00:13:33.330 --> 00:13:36.330
(Risos)

00:13:36.330 --> 00:13:38.330
RB: Vou parar por aqui.

00:13:38.330 --> 00:13:40.330
Fizemos assim. E depois com alternações na fala.

00:13:40.330 --> 00:13:43.330
Quando falamos com alguém, nós falamos

00:13:43.330 --> 00:13:47.330
e depois levantamos as sobrancelhas, movimentamos os olhos,

00:13:47.330 --> 00:13:50.330
e damos a entender que é a vez da outra pessoa falar.

00:13:50.330 --> 00:13:54.330
E aí ela fala, e ficamos nos alternando sempre de um para o outro.

00:13:54.330 --> 00:13:56.330
Então colocamos isso no robô.

00:13:56.330 --> 00:13:58.330
Chamamos alguns sujeitos de teste ingênuos,

00:13:58.330 --> 00:14:00.330
não dissemos a eles nada acerca do robô,

00:14:00.330 --> 00:14:02.330
colocamo-los de frente com o robô e pedimos que conversassem com ele.

00:14:02.330 --> 00:14:04.330
O que eles não sabiam era que

00:14:04.330 --> 00:14:06.330
o robô não entendia uma palavra do que diziam,

00:14:06.330 --> 00:14:09.330
e que o robô não falava inglês.

00:14:09.330 --> 00:14:11.330
Ele apenas dizia fonemas do inglês ao acaso.

00:14:11.330 --> 00:14:13.330
E quero que prestem bastante atenção no início,

00:14:13.330 --> 00:14:17.330
esta pessoa, o Ritchie, que acabou conversando por 25 minutos com o robô --

00:14:17.330 --> 00:14:19.330
(Risos)

00:14:19.330 --> 00:14:21.330
-- diz: "Quero te mostrar uma coisa.

00:14:21.330 --> 00:14:23.330
Quero te mostrar meu relógio."

00:14:23.330 --> 00:14:28.330
E ele leva o relógio ao centro do campo de visão do robô,

00:14:28.330 --> 00:14:30.330
aponta para o relógio, espera alguma resposta emocional,

00:14:30.330 --> 00:14:32.330
e o robô olha para o relógio com bastante êxito.

00:14:32.330 --> 00:14:35.330
Não sabemos se ele entendeu ou não que o robô --

00:14:36.330 --> 00:14:38.330
Reparem nas alternações.

00:14:38.330 --> 00:14:41.330
Ritchie: Quero te mostrar uma coisa. Este é um relógio

00:14:41.330 --> 00:14:44.330
que minha namorada me deu.

00:14:44.330 --> 00:14:46.330
Robô: Oh, que legal.

00:14:46.330 --> 00:14:50.330
Ritchie: Ele tem uma luz azul dentro dele. Eu quase o perdi essa semana.

00:14:51.330 --> 00:14:55.330
(Risos)

00:14:55.330 --> 00:14:58.330
RB: Ele está fazendo contato visual, seguindo seus olhos.

00:14:58.330 --> 00:15:00.330
Ritchie: Você consegue fazer o mesmo? Robô: Sim, claro.

00:15:00.330 --> 00:15:02.330
RB: E eles têm esse tipo de comunicação com êxito.

00:15:02.330 --> 00:15:06.330
Há ainda outro aspecto do tipo de coisa que Chris e eu estávamos fazendo.

00:15:06.330 --> 00:15:08.330
Este é outro robô, Cog.

00:15:08.330 --> 00:15:14.330
Primeiro, eles fazem contato visual, e então Christie olha para o brinquedo,

00:15:14.330 --> 00:15:16.330
o robô estima a direção para onde ela olha

00:15:16.330 --> 00:15:18.330
e olha a mesma coisa que ela está olhando.

00:15:18.330 --> 00:15:19.330
(Risos)

00:15:19.330 --> 00:15:22.330
Então veremos cada vez mais desse tipo de robô

00:15:22.330 --> 00:15:24.330
nos próximos anos, em laboratórios.

00:15:24.330 --> 00:15:29.330
Porém, as grandes perguntas, duas grandes perguntas que me fazem são:

00:15:29.330 --> 00:15:31.330
se fizermos os robôs cada vez mais parecidos com humanos,

00:15:31.330 --> 00:15:36.330
será que os aceitaremos, será que eles precisarão de direitos, eventualmente?

00:15:36.330 --> 00:15:39.330
E a outra pergunta que me fazem é: "Eles irão querer dominar o mundo?"

00:15:39.330 --> 00:15:40.330
(Risos)

00:15:40.330 --> 00:15:43.330
Quanto à primeira, isso tem sido um tema hollywoodiano

00:15:43.330 --> 00:15:46.330
em muitos filmes. Vocês provavelmente reconhecem esses personagens,

00:15:46.330 --> 00:15:50.330
em cada um desses casos, os robôs querem mais respeito.

00:15:50.330 --> 00:15:53.330
É necessário mesmo respeitar os robôs?

00:15:54.330 --> 00:15:56.330
Afinal de contas, são apenas máquinas.

00:15:56.330 --> 00:16:00.330
Mas penso que também temos de aceitar que nós somos apenas máquinas.

00:16:00.330 --> 00:16:05.330
Afinal, é exatamente isso que a biologia molecular moderna diz a nosso respeito.

00:16:05.330 --> 00:16:08.330
Não se vê uma descrição de como

00:16:08.330 --> 00:16:12.330
a molécula A se junta a uma outra molécula.

00:16:12.330 --> 00:16:15.330
E ela passa adiante, impulsionada por vários impulsos,

00:16:15.330 --> 00:16:19.330
e então a alma chega e mexe nas moléculas para que elas se interliguem.

00:16:19.330 --> 00:16:22.330
É tudo mecanizado, nós somos um mecanismo.

00:16:22.330 --> 00:16:25.330
Se nós somos máquinas, então pelo menos em princípio,

00:16:25.330 --> 00:16:29.330
deveríamos ser capazes de construir máquinas com outros materiais,

00:16:29.330 --> 00:16:33.330
que seriam tão vivas como somos.

00:16:33.330 --> 00:16:35.330
Mas acho que para admitirmos isso,

00:16:35.330 --> 00:16:38.330
temos de renunciar ao fato de que somos, de certa forma, "especiais".

00:16:38.330 --> 00:16:40.330
E nos distanciamos dessa condição de "especiais"

00:16:40.330 --> 00:16:43.330
por conta da ciência e da tecnologia por muitas vezes

00:16:43.330 --> 00:16:45.330
nesses últimos séculos, pelo menos.

00:16:45.330 --> 00:16:47.330
Há 500 anos tivemos de desistir da ideia

00:16:47.330 --> 00:16:50.330
de que éramos o centro do universo

00:16:50.330 --> 00:16:52.330
quando a terra passou a girar em torno do sol;

00:16:52.330 --> 00:16:57.330
Há 150 anos, com Darwin, tivemos de desistir da ideia de que éramos diferentes dos animais.

00:16:57.330 --> 00:17:00.330
E imaginar que -- é sempre algo difícil para nós.

00:17:00.330 --> 00:17:03.330
Recentemente fomos atingidos pela ideia de que, talvez,

00:17:03.330 --> 00:17:05.330
nem tenhamos tido nosso evento de criação aqui na Terra,

00:17:05.330 --> 00:17:08.330
e as pessoas não gostaram muito. E então o genoma humano disse

00:17:08.330 --> 00:17:11.330
que talvez tivéssemos apenas 35.000 genes. E foi algo bastante --

00:17:11.330 --> 00:17:14.330
as pessoas não gostaram daquilo, nós temos mais genes do que isso.

00:17:14.330 --> 00:17:17.330
Não gostamos de renunciar à nossa condição de "especiais",

00:17:17.330 --> 00:17:19.330
então a ideia de que robôs poderiam, de fato, ter emoções,

00:17:19.330 --> 00:17:21.330
ou de que robôs poderiam ser serer vivos --

00:17:21.330 --> 00:17:23.330
acho que será difícil para nós aceitarmos.

00:17:23.330 --> 00:17:27.330
Mas é algo que iremos aceitar nos próximos 50 anos, é a estimativa.

00:17:27.330 --> 00:17:30.330
A segunda pergunta é: "As máquinas irão querer dominar o mundo?"

00:17:30.330 --> 00:17:35.330
E aí o cenário geralmente é o de que nós criamos estas coisas,

00:17:35.330 --> 00:17:38.330
elas crescem, nós as nutrimos, elas aprendem bastante conosco,

00:17:38.330 --> 00:17:42.330
e então decidem que somos muito entediantes e lentos.

00:17:42.330 --> 00:17:44.330
E querem tomar o mundo da gente.

00:17:44.330 --> 00:17:47.330
E aqueles que têm filhos adolescentes sabem como é.

00:17:47.330 --> 00:17:48.330
(Risos)

00:17:48.330 --> 00:17:51.330
Mas Hollywood extende isso aos robôs.

00:17:51.330 --> 00:17:54.330
E a pegunta é:

00:17:54.330 --> 00:17:58.330
"Será que alguém construirá, por acidente, um robô que tomará o mundo?"

00:17:58.330 --> 00:18:01.330
É o tipo de coisa parecida com o cara solitário no quintal

00:18:01.330 --> 00:18:04.330
que diz: "Eu construí um 747 por acidente."

00:18:04.330 --> 00:18:06.330
Sabem, acho que não vai acontecer.

00:18:06.330 --> 00:18:08.330
E acho que não --

00:18:08.330 --> 00:18:09.330
(Risos)

00:18:09.330 --> 00:18:12.330
-- Acho que não iremos construir, deliberadamente, robôs

00:18:12.330 --> 00:18:14.330
com os quais não nos sentimos confortáveis.

00:18:14.330 --> 00:18:16.330
Eles não vão fazer um robô super mau de uma vez.

00:18:16.330 --> 00:18:19.330
Antes disso farão um robô só meio mau,

00:18:19.330 --> 00:18:21.330
e antes disso um robô que nem é tão mau assim.

00:18:21.330 --> 00:18:22.330
(Risos)

00:18:22.330 --> 00:18:24.330
Não deixaremos que as coisas tomem esse rumo.

00:18:24.330 --> 00:18:25.330
(Risos)

00:18:25.330 --> 00:18:31.330
Acho que vou parar por aqui, então: os robôs estão a caminho,

00:18:31.330 --> 00:18:34.330
não temos muito com o que nos preocupar, será bem divertido,

00:18:34.330 --> 00:18:38.330
e espero que todos vocês aproveitem a jornada pelos próximos 50 anos.

00:18:38.330 --> 00:18:40.330
(Aplausos)

