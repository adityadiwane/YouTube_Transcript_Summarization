WEBVTT
Kind: captions
Language: es

00:00:00.000 --> 00:00:07.000
Traductor: Reynaldo Valerio
Revisor: William Martinez

00:00:18.330 --> 00:00:23.330
De lo que quiero hablarles hoy es de cómo veo a los robots invadiendo nuestras vidas

00:00:23.330 --> 00:00:26.330
en múltiples niveles y líneas de tiempo.

00:00:26.330 --> 00:00:30.330
Y cuando miro hacia el futuro, no puedo imaginarme un mundo, 500 años a partir de ahora,

00:00:30.330 --> 00:00:32.330
en el cual no tengamos robots por todas partes,

00:00:32.330 --> 00:00:37.330
suponiendo que -- a pesar de todas las espantosas predicciones que mucha gente hace sobre nuestro futuro --

00:00:37.330 --> 00:00:41.330
suponiendo que todavía estemos por aquí, no puedo imaginar el mundo no estando poblado por robots.

00:00:41.330 --> 00:00:44.330
Y luego la pregunta es, bueno, si ellos van a estar por aquí en 500 años,

00:00:44.330 --> 00:00:46.330
¿estarán por todas partes antes de ese tiempo?

00:00:46.330 --> 00:00:48.330
¿Estarán por aquí dentro de 50 años?

00:00:48.330 --> 00:00:51.330
Sí, pienso que es muy probable -- habrá muchos robots por todas partes.

00:00:51.330 --> 00:00:54.330
Y, de hecho, pienso que eso ocurrirá mucho antes.

00:00:54.330 --> 00:00:58.330
Pienso que estamos a punto de que los robots se vuelvan cosa común

00:00:58.330 --> 00:01:04.330
y pienso que estamos como en 1978 ó 1980 en años de las computadoras personales

00:01:04.330 --> 00:01:07.330
cuando los primeros robots están comenzando a aparecer.

00:01:07.330 --> 00:01:11.330
Las computadoras comenzaron a llegar a través de juegos de video y de juguetes.

00:01:11.330 --> 00:01:14.330
Y, saben, la primera computadora que muchas personas tuvieron en sus casas

00:01:14.330 --> 00:01:16.330
puede haber sido una para jugar Pong,

00:01:16.330 --> 00:01:18.330
con un pequeño microprocesador integrado,

00:01:18.330 --> 00:01:21.330
y luego algunos otros juegos de video que vinieron después de ese.

00:01:21.330 --> 00:01:24.330
Y estamos comenzado a ver el mismo tipo de cosas con los robots:

00:01:24.330 --> 00:01:28.330
LEGO Mindstorms, Furbies -- los cuales -- ¿alguien aquí tuvo un Furby?

00:01:28.330 --> 00:01:31.330
Claro, hay 38 millones vendidos a nivel mundial.

00:01:31.330 --> 00:01:33.330
Son muy comunes, y son un pequeño robot,

00:01:33.330 --> 00:01:35.330
un robot simple con algunos sensores.

00:01:35.330 --> 00:01:37.330
Es un poco de procesamiento causal.

00:01:37.330 --> 00:01:40.330
A la derecha vemos otra muñeca robot, la que podían obtener hace un par de años.

00:01:40.330 --> 00:01:42.330
Y tal como en los primeros días,

00:01:42.330 --> 00:01:47.330
cuando había mucha interacción de aficionados a las computadoras,

00:01:47.330 --> 00:01:51.330
pueden obtener ahora muchos juegos para armar robots, y libros de cómo armar robots.

00:01:51.330 --> 00:01:55.330
Y a la izquierda hay una plataforma de Evolution Robotics,

00:01:55.330 --> 00:01:58.330
a la cual conectas una PC, y programas esto con una Interfaz Gráfica

00:01:58.330 --> 00:02:01.330
para moverse por tu casa y hacer diversas cosas.

00:02:01.330 --> 00:02:04.330
Y luego hay un punto de más alto precio, una especie de robots de juguete --

00:02:04.330 --> 00:02:08.330
el Aibo de Sony. Y a la derecha ahí, hay uno desarrollado por NEC,

00:02:08.330 --> 00:02:11.330
el PaPeRo, el cual no creo que vayan a lanzar.

00:02:11.330 --> 00:02:14.330
Pero, sin embargo, ese tipo de cosas están ahí afuera.

00:02:14.330 --> 00:02:18.330
Y hemos visto, durante los últimos dos o tres años, robots para cortar el césped,

00:02:18.330 --> 00:02:24.330
Husqvarna en la parte de abajo, Friendly Robotics aquí arriba, una empresa israrelí.

00:02:24.330 --> 00:02:26.330
Y, luego, en los últimos 12 meses, aproximadamente,

00:02:26.330 --> 00:02:30.330
hemos comenzado a ver aparecer muchos robots para limpiar la casa.

00:02:30.330 --> 00:02:33.330
Arriba y a la derecha hay un robot muy bueno para limpiar la casa

00:02:33.330 --> 00:02:37.330
de una empresa llamada Dyson, de Reino Unido. Excepto que era tan caro --

00:02:37.330 --> 00:02:39.330
US$3,500 dólares -- que no lo lanzaron al mercado.

00:02:39.330 --> 00:02:42.330
Pero abajo a la izquierda ven el Electrolux, que sí está de venta.

00:02:42.330 --> 00:02:44.330
Otro de Karcher.

00:02:44.330 --> 00:02:46.330
Y abajo a la derecha está uno que yo construí en mi laboratorio

00:02:46.330 --> 00:02:49.330
hace unos 10 años, y que finalmente convertimos en un producto.

00:02:49.330 --> 00:02:51.330
Y permítanme mostrarles eso.

00:02:51.330 --> 00:02:55.330
Vamos a regalar éste, creo que Chris dijo, al final de la charla.

00:02:55.330 --> 00:03:01.330
Este es un robot que puedes salir y comprar, y que limpiará tu piso.

00:03:05.330 --> 00:03:10.330
Y arranca moviéndose en círculos cada vez más amplios.

00:03:10.330 --> 00:03:14.330
Si golpea con algo -- ¿vieron eso?

00:03:14.330 --> 00:03:17.330
Ahora está comenzando a seguir la pared, está siguiendo el contorno de mis pies

00:03:17.330 --> 00:03:21.330
para limpiar alrededor de mí. Veamos, vamos a --

00:03:21.330 --> 00:03:26.330
oh, ¿quién se llevó mis Rice Krispies? ¡Se robaron mis Rice Krispies!

00:03:26.330 --> 00:03:32.330
(Risas)

00:03:32.330 --> 00:03:35.330
¡No se preocupen, relájense, no, relájense, es un robot , es inteligente!

00:03:35.330 --> 00:03:38.330
(Risas)

00:03:38.330 --> 00:03:42.330
Ven, los niños de tres años no se preocupan por él.

00:03:42.330 --> 00:03:44.330
Son los adultos los que de verdad se sienten incómodos.

00:03:44.330 --> 00:03:45.330
(Risas)

00:03:45.330 --> 00:03:47.330
Vamos a poner algo de basura aquí.

00:03:47.330 --> 00:03:51.330
(Risas)

00:03:51.330 --> 00:03:53.330
Bien.

00:03:53.330 --> 00:03:57.330
(Risas)

00:03:57.330 --> 00:04:00.330
No sé si lo pueden ver -- bueno, puse un poco de Rice Krispies allí,

00:04:00.330 --> 00:04:07.330
puse algunas monedas, movámos el robot hacia allá, a ver si lo limpia.

00:04:10.330 --> 00:04:12.330
Sí, bien. Así que --

00:04:12.330 --> 00:04:16.330
dejaremos esto para luego.

00:04:16.330 --> 00:04:21.330
(Aplausos)

00:04:22.330 --> 00:04:26.330
De hecho, parte del truco fue construir un mejor mecanismo de limpieza;

00:04:26.330 --> 00:04:30.330
la inteligencia a bordo era bastante simple.

00:04:30.330 --> 00:04:32.330
Y eso es igual con muchos robots.

00:04:32.330 --> 00:04:36.330
Nos hemos convertido todos, pienso, en una especie de chauvinistas computacionales,

00:04:36.330 --> 00:04:38.330
y piensan que la computación lo es todo,

00:04:38.330 --> 00:04:40.330
pero, la mecánica todavía es importante.

00:04:40.330 --> 00:04:43.330
Aquí hay otro robot, el PackBot,

00:04:43.330 --> 00:04:45.330
que hemos estado construyendo durante algunos años.

00:04:45.330 --> 00:04:51.330
Es un robot militar de vigilancia, para ir delante de las tropas,

00:04:51.330 --> 00:04:54.330
y examinar cavernas, por ejemplo.

00:04:54.330 --> 00:04:56.330
Pero, tuvimos que hacerlo muy robusto,

00:04:56.330 --> 00:05:03.330
mucho más robusto que los robots que construimos en nuestros laboratorios.

00:05:03.330 --> 00:05:06.330
(Risas)

00:05:12.330 --> 00:05:16.330
A bordo de ese robot hay una PC que corre Linux.

00:05:16.330 --> 00:05:22.330
Esto puede soportar un golpe de 400G. El robot tienen inteligencia local:

00:05:22.330 --> 00:05:28.330
puede voltearse boca arriba él solo, y puede ubicarse dentro del rango de alcance de comunicaciones,

00:05:28.330 --> 00:05:31.330
puede subir escaleras por sí solo, etcétera.

00:05:38.330 --> 00:05:42.330
Bien, aquí está haciendo navegación local.

00:05:42.330 --> 00:05:48.330
Un soldado le da un comando de subir las escaleras y lo hace.

00:05:49.330 --> 00:05:52.330
¡Ese no fue un descenso controlado!

00:05:52.330 --> 00:05:54.330
(Risas)

00:05:54.330 --> 00:05:56.330
Ahora se va.

00:05:56.330 --> 00:06:01.330
Y la gran prueba para estos robots, realmente, fue el 11 de Septiembre.

00:06:01.330 --> 00:06:05.330
Llevamos los robots hasta el World Trade Center tarde esa noche.

00:06:06.330 --> 00:06:08.330
No pudieron hacer mucho en la montaña de escombros,

00:06:08.330 --> 00:06:11.330
todo estaba muy -- no había nada que hacer.

00:06:11.330 --> 00:06:16.330
Pero, sí penetramos en todos lo edificios circundantes que habían sido evacuados,

00:06:16.330 --> 00:06:19.330
y buscamos posibles sobrevivientes en esos edificios

00:06:19.330 --> 00:06:21.330
en los cuales era peligroso entrar.

00:06:21.330 --> 00:06:23.330
Veamos este video.

00:06:23.330 --> 00:06:26.330
Reportera: ... unos compañeros del campo de batalla están ayudando a reducir los riesgos del combate.

00:06:26.330 --> 00:06:29.330
Nick Robertson tiene esa historia.

00:06:31.330 --> 00:06:33.330
Rodney Brooks: ¿Podemos tener otro de estos?

00:06:38.330 --> 00:06:40.330
OK, bien.

00:06:43.330 --> 00:06:46.330
Bien, este es un soldado que ha visto un robot hace dos semanas.

00:06:48.330 --> 00:06:52.330
Está enviando robots dentro de cuevas, viendo lo que ocurre.

00:06:52.330 --> 00:06:54.330
El robot es totalmente autónomo.

00:06:54.330 --> 00:06:58.330
Lo peor que ha ocurrido dentro de la cueva, hasta ahora,

00:06:58.330 --> 00:07:01.330
es que uno de los robots cayó diez metros.

00:07:08.330 --> 00:07:11.330
Así que, hace un año, el ejército de los Estados Unidos no tenía estos robots.

00:07:11.330 --> 00:07:13.330
Ahora están en servicio activo en Afganistán todos los días.

00:07:13.330 --> 00:07:16.330
Y esa es una de las razones por las que dicen que se está dando una invasión de robots.

00:07:16.330 --> 00:07:20.330
Hay un gran cambio ahora en cómo -- hacia dónde la tecnología está caminando.

00:07:20.330 --> 00:07:22.330
Gracias

00:07:23.330 --> 00:07:25.330
Y en los próximos meses,

00:07:25.330 --> 00:07:28.330
vamos a estar enviando robots que están en producción

00:07:28.330 --> 00:07:32.330
a perforar pozos de petróleo para sacar de la tierra esos pocos años de petróleo que quedan.

00:07:32.330 --> 00:07:36.330
Son ambientes muy hostiles, 150 grados centígrados, 10,000 PSI.

00:07:36.330 --> 00:07:40.330
Robots autónomos descendiendo y haciendo este tipo de trabajo.

00:07:40.330 --> 00:07:43.330
Pero, robots como éstos, son un poco difíciles de programar.

00:07:43.330 --> 00:07:45.330
¿Cómo, en el futuro, vamos a programar nuestros robots

00:07:45.330 --> 00:07:47.330
y a hacerlos más fáciles de usar?

00:07:47.330 --> 00:07:50.330
Y, de hecho, quiero usar un robot en este momento --

00:07:50.330 --> 00:07:55.330
un robot llamado Chris -- ponte de pie. Sí. ¡Bien!

00:07:57.330 --> 00:08:01.330
Ven aquí. Ahora fíjense, él cree que los robots tienen que ser un poco tiesos.

00:08:01.330 --> 00:08:04.330
Él hace un poco eso. Pero voy a --

00:08:04.330 --> 00:08:06.330
Chris Anderson: Es sólo que soy inglés. RB: Oh.

00:08:06.330 --> 00:08:08.330
(Risas)

00:08:08.330 --> 00:08:10.330
(Aplauso)

00:08:10.330 --> 00:08:13.330
Voy a enseñarle a este robot una tarea. Es una tarea muy compleja.

00:08:13.330 --> 00:08:16.330
Ahora, fíjense, él asintió con la cabeza, dándome una indicación de que

00:08:16.330 --> 00:08:19.330
estaba comprendiendo el flujo de comunicación.

00:08:19.330 --> 00:08:21.330
Y, si hubiese dicho algo totalmente extraño

00:08:21.330 --> 00:08:24.330
él me habría mirado de lado, con dudas, y habría regulado la conversación.

00:08:24.330 --> 00:08:27.330
Bueno, ahora traje esto frente a él.

00:08:27.330 --> 00:08:31.330
Miré sus ojos y ví que sus ojos miraron la tapa de la botella.

00:08:31.330 --> 00:08:33.330
Y estoy realizando esta tarea aquí, y él está observando.

00:08:33.330 --> 00:08:36.330
Sus ojos van y vienen hacia mi para ver qué estoy mirando,

00:08:36.330 --> 00:08:38.330
así que tenemos atención compartida.

00:08:38.330 --> 00:08:41.330
Y entonces, realizo esta tarea, y él observa, y me mira

00:08:41.330 --> 00:08:45.330
para ver qué ocurrirá luego. Y ahora le pasaré la botella,

00:08:45.330 --> 00:08:47.330
y veremos si puede realizar la tarea. ¿Puedes hacer eso?

00:08:47.330 --> 00:08:50.330
(Risas)

00:08:50.330 --> 00:08:54.330
Bien. Lo hace muy bien. Sí. Bien, bien, bien.

00:08:54.330 --> 00:08:56.330
No te enseñé a hacer eso.

00:08:56.330 --> 00:08:58.330
Ahora veamos si lo puedes armar todo de nuevo.

00:08:58.330 --> 00:09:00.330
(Risas)

00:09:00.330 --> 00:09:01.330
Y piensa que un robot tiene que ser muy lento.

00:09:01.330 --> 00:09:03.330
¡Buen robot, bien hecho!

00:09:03.330 --> 00:09:05.330
Bien, aquí vimos unas cuantas cosas.

00:09:06.330 --> 00:09:09.330
Vimos que cuando interactuamos,

00:09:09.330 --> 00:09:13.330
tratamos de enseñarle a alguien cómo hacer algo, dirigimos su atención visual.

00:09:13.330 --> 00:09:17.330
Lo otro que nos comunica es su estado interno,

00:09:17.330 --> 00:09:20.330
si nos está comprendiendo o no, regula nuestra interacción social.

00:09:20.330 --> 00:09:22.330
Hubo atención compartida al mirar el mismo tipo de cosas,

00:09:22.330 --> 00:09:26.330
y reconocimiento del refuerzo social al final.

00:09:26.330 --> 00:09:29.330
Y hemos estado tratando de incluir eso en los robots de nuestro laboratorio

00:09:29.330 --> 00:09:33.330
porque pensamos que es así como ustedes querrán interactuar con los robots en el futuro.

00:09:33.330 --> 00:09:35.330
Sólo quiero mostrarles un diagrama técnico aquí.

00:09:35.330 --> 00:09:39.330
Lo más importante para construir un robot con el cual se pueda interactuar socialmente

00:09:39.330 --> 00:09:41.330
es su sistema de atención visual.

00:09:41.330 --> 00:09:44.330
Porque a lo que le presta atención es a lo que está viendo

00:09:44.330 --> 00:09:47.330
y con lo que está interactuando, y es así que comprendes lo que está haciendo.

00:09:47.330 --> 00:09:50.330
Así que, en los videos que voy a mostrarles,

00:09:50.330 --> 00:09:54.330
van a ver un sistema de atención visual de un robot

00:09:54.330 --> 00:09:58.330
el cual tiene -- busca tonos de piel en el espacio HSV,

00:09:58.330 --> 00:10:02.330
así que trabaja a través de todos los, bueno, los colores humanos.

00:10:02.330 --> 00:10:04.330
Busca colores muy saturados, en los juguetes.

00:10:04.330 --> 00:10:06.330
Y busca cosas que se mueven.

00:10:06.330 --> 00:10:09.330
Y compara todo eso en una ventana de atención,

00:10:09.330 --> 00:10:11.330
y busca el lugar de mayor puntuación --

00:10:11.330 --> 00:10:13.330
aquello en donde lo más interesante está ocurriendo.

00:10:13.330 --> 00:10:17.330
Y es a eso a lo que sus ojos siguen.

00:10:17.330 --> 00:10:19.330
Y mira directo hacia eso.

00:10:19.330 --> 00:10:22.330
Al mismo tiempo, toma decisiones comunes: :

00:10:22.330 --> 00:10:25.330
puede decidir que se siente solo y buscar tono de piel,

00:10:25.330 --> 00:10:28.330
o podría decidir que está aburrido y buscar un juguete para jugar.

00:10:28.330 --> 00:10:30.330
Así que estos pesos cambian.

00:10:30.330 --> 00:10:32.330
Y aquí arriba, a la derecha,

00:10:32.330 --> 00:10:35.330
esto es lo que llamamos el módulo en memoria de Steven Spielberg.

00:10:35.330 --> 00:10:37.330
¿Vieron la película IA? Audiencia: Sí.

00:10:37.330 --> 00:10:39.330
RB: Sí, era bastante mala, pero --

00:10:39.330 --> 00:10:43.330
¿recuerdan, especialmente cuando Haley Joel Osment, el pequeño robot,

00:10:43.330 --> 00:10:47.330
miró al hada azul por 2,000 años sin quitar sus ojos de ella?

00:10:47.330 --> 00:10:49.330
Bien, esto elimina ese asunto,

00:10:49.330 --> 00:10:53.330
porque esto es una habituación Gaussiana que se vuelve negativa,

00:10:53.330 --> 00:10:56.330
y más y más intensa cuando mira algo.

00:10:56.330 --> 00:10:59.330
Y se aburre, así que entonces mira hacia otro lado.

00:10:59.330 --> 00:11:03.330
Así que, una vez que tienes eso -- y aquí está un robot, aquí está Kismet,

00:11:03.330 --> 00:11:07.330
mirando alrededor buscando un juguete. Puedes darte cuenta de lo que está mirando.

00:11:07.330 --> 00:11:12.330
Puedes estimar la dirección de su mirada por esos globos oculares que cubren la cámara,

00:11:12.330 --> 00:11:15.330
y puedes darte cuenta de cuándo está viendo directamente al juguete.

00:11:15.330 --> 00:11:17.330
Y tiene algo de respuesta emocional aquí.

00:11:17.330 --> 00:11:18.330
(Risas)

00:11:18.330 --> 00:11:20.330
Pero, aún así va a poner atención

00:11:20.330 --> 00:11:24.330
si algo más significativo penetra en su campo visual --

00:11:24.330 --> 00:11:28.330
como, por ejemplo Cynthia Breazeal, quien construyó este robot -- desde la derecha.

00:11:28.330 --> 00:11:33.330
La mira, le presta atención a ella.

00:11:33.330 --> 00:11:37.330
Kismet tiene, subyacente, un espacio emocional tridimensional,

00:11:37.330 --> 00:11:40.330
un espacio vectorial, de dónde se encuentra emocionalmente.

00:11:40.330 --> 00:11:45.330
Y en diferentes lugares de ese espacio él expresa --

00:11:46.330 --> 00:11:48.330
¿podemos tener volumen aquí?

00:11:48.330 --> 00:11:50.330
¿Pueden escuchar eso ahora, ahí? Audiencia: Sí.

00:11:50.330 --> 00:11:55.330
Kismet: ¿Realmente piensas eso? ¿Realmente piensas eso?

00:11:57.330 --> 00:11:59.330
¿Realmente lo piensas?

00:12:00.330 --> 00:12:03.330
RB: Así que está expresando su emoción mediante su cara

00:12:03.330 --> 00:12:05.330
y la prosodia en su voz.

00:12:05.330 --> 00:12:09.330
Y, cuando yo estaba trabajando con mi robot aquí,

00:12:09.330 --> 00:12:12.330
Chris, el robot, estaba midiendo la prosodia en mi voz,

00:12:12.330 --> 00:12:17.330
así que hicimos que el robot midiera la prosodia a partir de cuatro mensajes básicos

00:12:17.330 --> 00:12:21.330
que las madres dan a sus hijos de forma pre-lingüística.

00:12:21.330 --> 00:12:24.330
Aquí tenemos sujetos ingenuos elogiando al robot,

00:12:26.330 --> 00:12:28.330
Voz: Lindo robot.

00:12:29.330 --> 00:12:31.330
Eres un robot tan lindo.

00:12:31.330 --> 00:12:33.330
(Risas)

00:12:33.330 --> 00:12:35.330
Y el robot está reaccionando adecuadamente.

00:12:35.330 --> 00:12:39.330
Voz: ... muy bien, Kismet.

00:12:40.330 --> 00:12:42.330
(Risas)

00:12:42.330 --> 00:12:44.330
Voz: Mira mi sonrisa.

00:12:46.330 --> 00:12:49.330
RB: Se sonríe. Imita la sonrisa. Esto pasa muchas veces.

00:12:49.330 --> 00:12:51.330
Esos son sujetos ingenuos.

00:12:51.330 --> 00:12:54.330
Aquí les pedimos que captaran la atención del robot

00:12:54.330 --> 00:12:57.330
y que indicaran cuándo tenían la atención del robot.

00:12:57.330 --> 00:13:01.330
Voz: Hey, Kismet, ah, ahí está.

00:13:01.330 --> 00:13:05.330
RB: Así que se da cuenta de que tiene la atención del robot.

00:13:08.330 --> 00:13:12.330
Voz: Kismet, ¿te gusta el juguete? Oh.

00:13:13.330 --> 00:13:15.330
RB: Ahora aquí les pedimos que prohibieran algo al robot,

00:13:15.330 --> 00:13:19.330
y la primera mujer realmente empuja al robot hacia un rincón emocional.

00:13:19.330 --> 00:13:24.330
Voz: No. No. No debes hacer eso. ¡No!

00:13:24.330 --> 00:13:27.330
(Risas)

00:13:27.330 --> 00:13:33.330
Voz: No es apropiado. No. No.

00:13:33.330 --> 00:13:36.330
(Risas)

00:13:36.330 --> 00:13:38.330
RB: Lo voy a dejar ahí.

00:13:38.330 --> 00:13:40.330
Integramos todo eso. Luego agregamos el tomar turnos.

00:13:40.330 --> 00:13:43.330
Cuando conversamos con alguien, hablamos.

00:13:43.330 --> 00:13:47.330
Luego levantamos nuestras cejas, movemos nuestros ojos,

00:13:47.330 --> 00:13:50.330
para darle a la otra persona la idea de que es su turno de hablar.

00:13:50.330 --> 00:13:54.330
Y luego ellos hablan, y nos pasamos la batuta el uno al otro.

00:13:54.330 --> 00:13:56.330
Así que ponemos esto en el robot.

00:13:56.330 --> 00:13:58.330
Buscamos un grupo de sujetos ingenuos,

00:13:58.330 --> 00:14:00.330
no les dijimos nada acerca del robot,

00:14:00.330 --> 00:14:02.330
los sentamos frente al robot y les dijimos "habla con el robot".

00:14:02.330 --> 00:14:04.330
Ahora, lo que no sabían era que,

00:14:04.330 --> 00:14:06.330
el robot no comprendía ni una palabra de lo que ellos le decían,

00:14:06.330 --> 00:14:09.330
y que el robot no estaba hablando Inglés.

00:14:09.330 --> 00:14:11.330
Sólo pronunciaba fonemas de Inglés que eran aleatorios.

00:14:11.330 --> 00:14:13.330
Y quiero que vean con atención, al inicio de esto,

00:14:13.330 --> 00:14:17.330
cuando esta persona, Ritchie, quien habló con el robot por 25 minutos --

00:14:17.330 --> 00:14:19.330
(Risas)

00:14:19.330 --> 00:14:21.330
-- dice "Quiero mostrarte algo.

00:14:21.330 --> 00:14:23.330
Quiero mostrarte mi reloj."

00:14:23.330 --> 00:14:28.330
Y coloca el centro del reloj dentro del campo visual del robot,

00:14:28.330 --> 00:14:30.330
lo señala, le da una pista emocional,

00:14:30.330 --> 00:14:32.330
y el robot mira al reloj exitosamente.

00:14:32.330 --> 00:14:35.330
No sabemos si él entendió o no que el robot --

00:14:36.330 --> 00:14:38.330
Fíjense en la toma de turnos.

00:14:38.330 --> 00:14:41.330
Ritchie: Bien, quiero mostrarte algo. Bueno, esto es un reloj

00:14:41.330 --> 00:14:44.330
que mi novia me regaló.

00:14:44.330 --> 00:14:46.330
Robot: Oh, bien.

00:14:46.330 --> 00:14:50.330
Ritchie: Sí, mira, tiene incluso una pequeña luz azul aquí. casi lo pierdo esta semana.

00:14:51.330 --> 00:14:55.330
(Risas)

00:14:55.330 --> 00:14:58.330
RB: Así que hace contacto visual con él, siguiendo sus ojos.

00:14:58.330 --> 00:15:00.330
Ritchie: ¿Puedes hacer lo mismo? Robot: Sí, claro.

00:15:00.330 --> 00:15:02.330
RB: Y ellos tienen este tipo de comunicación con éxito.

00:15:02.330 --> 00:15:06.330
Y aquí hay otro aspecto del tipo de cosas que Chris y yo estuvimos haciendo.

00:15:06.330 --> 00:15:08.330
Este es otro robot, Cog.

00:15:08.330 --> 00:15:14.330
Ellos primero hicieron contacto visual, y luego, cuando Christie mira hacia este juguete,

00:15:14.330 --> 00:15:16.330
el robot estima la dirección de la mirada de ella

00:15:16.330 --> 00:15:18.330
y mira el mismo objeto que ella está mirando.

00:15:18.330 --> 00:15:19.330
(Risas)

00:15:19.330 --> 00:15:22.330
Vamos a ver más y más de este tipo de robots

00:15:22.330 --> 00:15:24.330
en los próximos años en los laboratorios.

00:15:24.330 --> 00:15:29.330
Pero, entonces las grandes preguntas, dos grandes preguntas que la gente me hace son:

00:15:29.330 --> 00:15:31.330
si hacemos a estos robots cada vez más parecidos a los humanos,

00:15:31.330 --> 00:15:36.330
¿los aceptaremos? -- ¿tendrán derechos eventualmente?

00:15:36.330 --> 00:15:39.330
Y la otra pregunta que la gente me hace es, ¿querrán ellos tomar el control?

00:15:39.330 --> 00:15:40.330
(Risas)

00:15:40.330 --> 00:15:43.330
En cuanto a la primera -- saben, esto ha sido mucho un tema de Hollywood

00:15:43.330 --> 00:15:46.330
en muchas películas. Probablemente reconozcan a estos personajes aquí --

00:15:46.330 --> 00:15:50.330
en cada uno de estos casos, los robots querían más respeto.

00:15:50.330 --> 00:15:53.330
Bueno pero, realmente necesitamos respetar a los robots?

00:15:54.330 --> 00:15:56.330
Son sólo máquinas, después de todo.

00:15:56.330 --> 00:16:00.330
Pero creo que, bueno, tenemos que aceptar que nosotros somos sólo máquinas.

00:16:00.330 --> 00:16:05.330
Después de todo, eso es en verdad lo que la biología molecular moderna dice sobre nosotros.

00:16:05.330 --> 00:16:08.330
Tú no ves una descripción de cómo, bueno,

00:16:08.330 --> 00:16:12.330
la Molécula A, bueno, llega y se une a esta otra molecula.

00:16:12.330 --> 00:16:15.330
Y entonces se mueve hacia adelante, bueno, impulsada por varias cargas,

00:16:15.330 --> 00:16:19.330
y luego el alma entra y pellizca esas moleculas para que se conecten.

00:16:19.330 --> 00:16:22.330
Es todo mecánico, somos mecanismos.

00:16:22.330 --> 00:16:25.330
Si somos máquinas, entonces en principio al menos,

00:16:25.330 --> 00:16:29.330
deberíamos ser capaces de contruir máquinas con otros materiales,

00:16:29.330 --> 00:16:33.330
que estén tan vivas como estamos nosotros.

00:16:33.330 --> 00:16:35.330
Pero creo que para que podamos admitir esto,

00:16:35.330 --> 00:16:38.330
debemos renunciar a ser especiales, de alguna manera.

00:16:38.330 --> 00:16:40.330
Y hemos tenido una retirada de esto de ser especiales

00:16:40.330 --> 00:16:43.330
bajo la descarga de artillería de la ciencia y de la tecnología muchas veces

00:16:43.330 --> 00:16:45.330
en los últimos cientos de años, al menos.

00:16:45.330 --> 00:16:47.330
Hace 500 años tuvimos que abandonar la idea

00:16:47.330 --> 00:16:50.330
de que éramos el centro del universo

00:16:50.330 --> 00:16:52.330
cuando la Tierra comenzó a girar alrededor del Sol;

00:16:52.330 --> 00:16:57.330
hace 150 años, con Darwin, tuvimos que abandonar la idea de que éramos diferentes de los animales.

00:16:57.330 --> 00:17:00.330
Y que, bueno, imaginar -- saben, siempre es difícil para nosotros.

00:17:00.330 --> 00:17:03.330
Incluso, recientemente nos han maltratado con la idea de que tal vez

00:17:03.330 --> 00:17:05.330
ni siquiera tuvimos nuestro evento de creación, aquí en la Tierra,

00:17:05.330 --> 00:17:08.330
lo cual a la gente no le gustó mucho. Y luego el genoma humano dijo que

00:17:08.330 --> 00:17:11.330
tal vez sólo tenemos unos 35,000 genes. Y eso fue realmente --

00:17:11.330 --> 00:17:14.330
a la gente no le gustó, tenemos más genes que eso.

00:17:14.330 --> 00:17:17.330
No nos gusta abandonar nuestra especialidad, así que, saben,

00:17:17.330 --> 00:17:19.330
la idea de que los robots puedan en verdad tener emociones,

00:17:19.330 --> 00:17:21.330
o que los robots puedan ser criaturas vivientes --

00:17:21.330 --> 00:17:23.330
creo que eso va a ser difícil de aceptar para nosotros.

00:17:23.330 --> 00:17:27.330
Pero, lo aceptaremos de aquí a los próximos 50 años, más o menos.

00:17:27.330 --> 00:17:30.330
Y la segunda pregunta es, ¿querrán las máquinas tomar el control?

00:17:30.330 --> 00:17:35.330
Y aquí el escenario es que nosotros creamos estas cosas,

00:17:35.330 --> 00:17:38.330
ellas crecen, las alimentamos, ellas aprenden mucho de nosotros,

00:17:38.330 --> 00:17:42.330
y ellas comienzar a pensar que nosotros somos muy aburridos, lentos.

00:17:42.330 --> 00:17:44.330
Ellas quieren controlarnos.

00:17:44.330 --> 00:17:47.330
Y para aquellos de ustedes que tienen adolescentes, saben lo que se siente.

00:17:47.330 --> 00:17:48.330
(Risas)

00:17:48.330 --> 00:17:51.330
Pero, Hollywood lo extiende hasta los robots.

00:17:51.330 --> 00:17:54.330
Y la pregunta es, bueno,

00:17:54.330 --> 00:17:58.330
¿construirá alguien, accidentalmente, un robot que quiera tomar el control?

00:17:58.330 --> 00:18:01.330
Y es un poco este tipo solitario, en el patio trasero de su casa,

00:18:01.330 --> 00:18:04.330
y, bueno, "accidentalmente construí un 747."

00:18:04.330 --> 00:18:06.330
No creo que eso vaya a pasar.

00:18:06.330 --> 00:18:08.330
Y no creo que --

00:18:08.330 --> 00:18:09.330
(Risas)

00:18:09.330 --> 00:18:12.330
-- no creo que vamos deliberadamente a construir robots

00:18:12.330 --> 00:18:14.330
con los cuales nos sintamos incómodos.

00:18:14.330 --> 00:18:16.330
Bueno, pues, no van a construir un robot súper malvado.

00:18:16.330 --> 00:18:19.330
Antes que eso tendrá que haber, bueno, un robot medianamente malvado,

00:18:19.330 --> 00:18:21.330
y antes que ese uno que casi no sea malvado.

00:18:21.330 --> 00:18:22.330
(Risas)

00:18:22.330 --> 00:18:24.330
Y no vamos a dejar que eso ocurra.

00:18:24.330 --> 00:18:25.330
(Risas)

00:18:25.330 --> 00:18:31.330
Así que lo voy a dejar ahí: los robots ya vienen,

00:18:31.330 --> 00:18:34.330
no tenemos mucho de qué preocuparnos, será muy divertido,

00:18:34.330 --> 00:18:38.330
y espero que todos ustedes disfruten el viaje de los próximos 50 años.

00:18:38.330 --> 00:18:40.330
(Aplauso)

