WEBVTT
Kind: captions
Language: nl

00:00:00.000 --> 00:00:07.000
Vertaald door: Rik Delaet
Nagekeken door: Christel Foncke

00:00:18.330 --> 00:00:23.330
Ik wil het vandaag hebben over hoe robots ons leven

00:00:23.330 --> 00:00:26.330
op meerdere niveaus, over meerdere tijdschalen binnendringen.

00:00:26.330 --> 00:00:30.330
Ik kan me de toekomstige wereld, 500 jaar na nu,

00:00:30.330 --> 00:00:32.330
zonder robots niet voorstellen.

00:00:32.330 --> 00:00:37.330
Ervan uitgaande dat - ondanks alle sombere voorspellingen van veel mensen over onze toekomst -

00:00:37.330 --> 00:00:41.330
we er nog steeds zullen zijn, kan ik me niet voorstellen dat de wereld niet zal worden bevolkt door robots.

00:00:41.330 --> 00:00:44.330
De vraag is of het niet

00:00:44.330 --> 00:00:46.330
sneller zal gaan gebeuren?

00:00:46.330 --> 00:00:48.330
Bijvoorbeeld binnen 50 jaar?

00:00:48.330 --> 00:00:51.330
Ja, ik denk dat er overal heel veel robots gaan zijn.

00:00:51.330 --> 00:00:54.330
In feite zal het nog een stuk sneller gaan.

00:00:54.330 --> 00:00:58.330
We staan op de vooravond van hun komst.

00:00:58.330 --> 00:01:04.330
Je kan het vergelijken met 1978 of 1980 voor de pc.

00:01:04.330 --> 00:01:07.330
De eerste robots beginnen te verschijnen.

00:01:07.330 --> 00:01:11.330
De eerste computers verschenen onder de vorm van spellen en speelgoed.

00:01:11.330 --> 00:01:14.330
De eerste computer die de meeste mensen in huis haalden,

00:01:14.330 --> 00:01:16.330
was misschien wel een computer om Pong te spelen,

00:01:16.330 --> 00:01:18.330
met een kleine ingebouwde microprocessor.

00:01:18.330 --> 00:01:21.330
Daarna kwamen andere spellen.

00:01:21.330 --> 00:01:24.330
Met robots gaat het net zo:

00:01:24.330 --> 00:01:28.330
LEGO Mindstorms, Furbies - die hier - heeft iemand hier een Furby?

00:01:28.330 --> 00:01:31.330
Ja, er zijn er wereldwijd 38 miljoen van verkocht.

00:01:31.330 --> 00:01:33.330
Je ziet ze overal. Het zijn klein eenvoudige robotjes

00:01:33.330 --> 00:01:35.330
met wat sensoren en

00:01:35.330 --> 00:01:37.330
wat mogelijkheid tot activiteit.

00:01:37.330 --> 00:01:40.330
Rechts is een andere robotpop, die je een paar jaar geleden kon kopen.

00:01:40.330 --> 00:01:42.330
En net zoals vroeger met de computer

00:01:42.330 --> 00:01:47.330
toen er een heleboel amateurs actief waren,

00:01:47.330 --> 00:01:51.330
kan je nu allerlei doe-het-zelfboeken en kits kopen.

00:01:51.330 --> 00:01:55.330
Links is er een platform van Evolution Robotics,

00:01:55.330 --> 00:01:58.330
waar je een pc op zet en het ding programmeert met een GUI

00:01:58.330 --> 00:02:01.330
om door je huis te zwerven en allerlei dingen te doen.

00:02:01.330 --> 00:02:04.330
Dan is er ook nog robotspeelgoed in een hogere prijsklasse -

00:02:04.330 --> 00:02:08.330
de Sony Aibo. Rechts is er een van NEC, de PaPeRo,

00:02:08.330 --> 00:02:11.330
waarvan ik niet denk dat ze hem op de markt gaan brengen.

00:02:11.330 --> 00:02:14.330
Maar toch is dat soort dingen er al.

00:02:14.330 --> 00:02:18.330
De afgelopen twee of drie jaar hebben we grasmaairobots gezien,

00:02:18.330 --> 00:02:24.330
Onderaan Husqvarna, bovenaan Friendly Robotics van een Israëlisch bedrijf.

00:02:24.330 --> 00:02:26.330
Het laatste jaar

00:02:26.330 --> 00:02:30.330
zagen we een hoop stofzuigerrobots verschijnen.

00:02:30.330 --> 00:02:33.330
Linksboven zie je een zeer mooie stofzuigerrobot

00:02:33.330 --> 00:02:37.330
van het bedrijf Dyson, in het Verenigd Koninkrijk. Maar hij was zo duur -

00:02:37.330 --> 00:02:39.330
3500 dollar – dat ze hem niet op de markt brachten.

00:02:39.330 --> 00:02:42.330
Linksonder zie je de Electrolux, die is te koop.

00:02:42.330 --> 00:02:44.330
Een andere van Kärcher.

00:02:44.330 --> 00:02:46.330
Rechts onderaan is er een die ik bouwde in mijn lab

00:02:46.330 --> 00:02:49.330
ongeveer 10 jaar geleden en die nu marktklaar is.

00:02:49.330 --> 00:02:51.330
Ik toon hem even.

00:02:51.330 --> 00:02:55.330
We gaan deze na de talk weggeven zei Chris, denk ik.

00:02:55.330 --> 00:03:01.330
Dit is een robot die je kunt kopen en die je vloer zal schoonmaken.

00:03:05.330 --> 00:03:10.330
Hij begint rond te gaan in steeds grotere cirkels.

00:03:10.330 --> 00:03:14.330
Als hij iets raakt - zien jullie dat?

00:03:14.330 --> 00:03:17.330
Nu gaat hij de hindernis volgen, rond mijn voeten rijden,

00:03:17.330 --> 00:03:21.330
om me heen schoonmaken. Laten eens kijken, laten we -

00:03:21.330 --> 00:03:26.330
oh, wie heeft mijn Rice Krispies gepikt? Ze hebben mijn Rice Krispies gestolen!

00:03:26.330 --> 00:03:32.330
(Gelach)

00:03:32.330 --> 00:03:35.330
Maak je geen zorgen, kalm, het is een robot, hij is slim!

00:03:35.330 --> 00:03:38.330
(Gelach)

00:03:38.330 --> 00:03:42.330
Driejarige kinderen malen daar niet om.

00:03:42.330 --> 00:03:44.330
Alleen de volwassenen.

00:03:44.330 --> 00:03:45.330
(Gelach)

00:03:45.330 --> 00:03:47.330
We gaan hier gewoon wat rommel rondstrooien.

00:03:47.330 --> 00:03:51.330
(Gelach)

00:03:51.330 --> 00:03:53.330
Oké.

00:03:53.330 --> 00:03:57.330
(Gelach)

00:03:57.330 --> 00:04:00.330
Ik weet niet of je het ziet – ik liet daar wat Rice Krispies vallen,

00:04:00.330 --> 00:04:07.330
Ook wat centen, laten we het eens proberen, kijken of hij opruimt.

00:04:10.330 --> 00:04:12.330
Ja, Oké. -

00:04:12.330 --> 00:04:16.330
Dat laten we voor later.

00:04:16.330 --> 00:04:21.330
(Applaus)

00:04:22.330 --> 00:04:26.330
Een deel van de truc was eigenlijk het bouwen van een beter reinigingsmechanisme;

00:04:26.330 --> 00:04:30.330
de intelligentie aan boord was vrij simpel.

00:04:30.330 --> 00:04:32.330
Dat is waar voor veel robots.

00:04:32.330 --> 00:04:36.330
We zijn allemaal, denk ik, een beetje ‘computerchauvinisten’ geworden

00:04:36.330 --> 00:04:38.330
en denken dat berekenen alles is,

00:04:38.330 --> 00:04:40.330
maar de mechaniek is nog steeds belangrijk.

00:04:40.330 --> 00:04:43.330
Hier is nog een robot, de PackBot,

00:04:43.330 --> 00:04:45.330
die we al een tijdje maken.

00:04:45.330 --> 00:04:51.330
Het is een militaire surveillancerobot om de troepen vooraf te gaan -

00:04:51.330 --> 00:04:54.330
rondkijken in grotten, bijvoorbeeld.

00:04:54.330 --> 00:04:56.330
Maar we moesten hem vrij robuust maken,

00:04:56.330 --> 00:05:03.330
veel robuuster dan de robots die we in onze labs bouwen.

00:05:03.330 --> 00:05:06.330
(Gelach)

00:05:12.330 --> 00:05:16.330
Aan boord van die robot zit een pc met Linux.

00:05:16.330 --> 00:05:22.330
Hij is bestand tegen schokken van 400 g. De robot heeft ingebouwde intelligentie:

00:05:22.330 --> 00:05:28.330
hij kan kopje over doen, zich binnen communicatiebereik brengen,

00:05:28.330 --> 00:05:31.330
zelf een trap oplopen, enzovoort.

00:05:38.330 --> 00:05:42.330
Daar is hij bezig met lokale navigatie.

00:05:42.330 --> 00:05:48.330
Een soldaat geeft hem de opdracht om naar boven te gaan en hij doet het.

00:05:49.330 --> 00:05:52.330
Dat was geen gecontroleerde afdaling.

00:05:52.330 --> 00:05:54.330
(Gelach)

00:05:54.330 --> 00:05:56.330
Daar gaat ie.

00:05:56.330 --> 00:06:01.330
9/11 betekende de grote doorbraak voor deze robots.

00:06:01.330 --> 00:06:05.330
We brachten die avond de robots naar het World Trade Center.

00:06:06.330 --> 00:06:08.330
Maar ze konden niet veel doen in die grote stapel puin

00:06:08.330 --> 00:06:11.330
- er was niets meer te doen.

00:06:11.330 --> 00:06:16.330
Maar we zijn in alle geëvacueerde omliggende gebouwen

00:06:16.330 --> 00:06:19.330
gaan zoeken naar mogelijke overlevenden.

00:06:19.330 --> 00:06:21.330
Het was te gevaarlijk om erin te gaan.

00:06:21.330 --> 00:06:23.330
Laten we deze video eens bekijken.

00:06:23.330 --> 00:06:26.330
Reporter: ... deze slagveldmetgezellen helpen de risico's te verminderen.

00:06:26.330 --> 00:06:29.330
Een reportage van Nick Robertson.

00:06:31.330 --> 00:06:33.330
Rodney Brooks: Kunnen we er nog eentje hebben?

00:06:38.330 --> 00:06:40.330
Oké, goed.

00:06:43.330 --> 00:06:46.330
Dit is een korporaal die pas twee weken eerder een robot had gezien.

00:06:48.330 --> 00:06:52.330
Hij stuurt robots de grotten in om te kijken naar wat er gebeurt.

00:06:52.330 --> 00:06:54.330
De robot is volledig autonoom.

00:06:54.330 --> 00:06:58.330
Het ergste wat er tot nu toe in de grot is gebeurd,

00:06:58.330 --> 00:07:01.330
was dat een van de robots een val van tien meter maakte.

00:07:08.330 --> 00:07:11.330
Vorig jaar beschikte het Amerikaanse leger nog niet over deze robots.

00:07:11.330 --> 00:07:13.330
Nu worden ze in Afghanistan dagelijks gebruikt.

00:07:13.330 --> 00:07:16.330
Daarom zeggen ze dat er een robotinvasie aan de gang is.

00:07:16.330 --> 00:07:20.330
De technologie is in volle ontwikkeling.

00:07:20.330 --> 00:07:22.330
Bedankt.

00:07:23.330 --> 00:07:25.330
Binnen enkele maanden

00:07:25.330 --> 00:07:28.330
gaan we robots naar beneden sturen

00:07:28.330 --> 00:07:32.330
in de olieputten om die laatste paar jaar olie uit de grond te krijgen.

00:07:32.330 --> 00:07:36.330
Zeer vijandige omgevingen, 150 ˚C, 700 bar.

00:07:36.330 --> 00:07:40.330
Autonome robots doen daar beneden dit soort werk.

00:07:40.330 --> 00:07:43.330
Maar robots als deze zijn een beetje moeilijk om te programmeren.

00:07:43.330 --> 00:07:45.330
Hoe gaan we in de toekomst onze robots programmeren

00:07:45.330 --> 00:07:47.330
om ze gemakkelijker te kunnen gebruiken?

00:07:47.330 --> 00:07:50.330
Ik wil hier eigenlijk gebruik maken van een robot -

00:07:50.330 --> 00:07:55.330
een robot met de naam Chris – sta op. Yeah. Oké.

00:07:57.330 --> 00:08:01.330
Kom eens hier. Merk op dat hij denkt dat robots een beetje stijf moeten zijn.

00:08:01.330 --> 00:08:04.330
Hij doet dat een beetje. Maar ik ga -

00:08:04.330 --> 00:08:06.330
Chris Anderson: Ik ben alleen maar Brits. RB: Oh.

00:08:06.330 --> 00:08:08.330
(Gelach)

00:08:08.330 --> 00:08:10.330
(Applaus)

00:08:10.330 --> 00:08:13.330
Ik ga deze robot een taak laten zien. Een erg complexe taak.

00:08:13.330 --> 00:08:16.330
Let nu op, hij knikte daar, hij gaf me een indicatie

00:08:16.330 --> 00:08:19.330
dat hij de communicatiestroom begreep.

00:08:19.330 --> 00:08:21.330
Als ik iets heel bizar had gezegd

00:08:21.330 --> 00:08:24.330
zou hij me scheef hebben aangekeken en het gesprek in goede banen geleid.

00:08:24.330 --> 00:08:27.330
Ik plaatste dit voor hem.

00:08:27.330 --> 00:08:31.330
Ik keek naar zijn ogen en ik zag zijn ogen naar de flessenhals gaan.

00:08:31.330 --> 00:08:33.330
Ik voer deze taak uit en hij volgt dat.

00:08:33.330 --> 00:08:36.330
Zijn ogen volgen af en toe de mijne om te zien waar ik naar kijk -

00:08:36.330 --> 00:08:38.330
we hebben dus gedeelde aandacht.

00:08:38.330 --> 00:08:41.330
Ik voer deze taak uit, hij kijkt en kijkt ook naar mij

00:08:41.330 --> 00:08:45.330
om te zien wat er gaat gebeuren. Nu geef ik hem de fles

00:08:45.330 --> 00:08:47.330
en we zullen zien of hij de taak aankan. Kan je dat?

00:08:47.330 --> 00:08:50.330
(Gelach)

00:08:50.330 --> 00:08:54.330
Oké. Hij is best goed. Ja. Goed, goed, goed.

00:08:54.330 --> 00:08:56.330
Ik heb je niet laten zien hoe je dat moet doen.

00:08:56.330 --> 00:08:58.330
Nu eens zien of je het weer in elkaar kan zetten.

00:08:58.330 --> 00:09:00.330
(Gelach)

00:09:00.330 --> 00:09:01.330
Hij denkt dat een robot echt traag moet zijn.

00:09:01.330 --> 00:09:03.330
Goede robot, dat is goed.

00:09:03.330 --> 00:09:05.330
We zagen daar een hoop dingen.

00:09:06.330 --> 00:09:09.330
We zagen dat als we interageren,

00:09:09.330 --> 00:09:13.330
als we proberen iemand te tonen hoe iets gedaan wordt, we hun visuele aandacht geleiden.

00:09:13.330 --> 00:09:17.330
Het andere ding communiceert hun interne toestand naar ons en

00:09:17.330 --> 00:09:20.330
zorgt voor sociale interactie of hij het begrijpt of niet.

00:09:20.330 --> 00:09:22.330
Er was gedeelde aandacht voor hetzelfde soort dingen

00:09:22.330 --> 00:09:26.330
en aan het eind herkennen van sociaal gecommuniceerde bekrachtiging.

00:09:26.330 --> 00:09:29.330
Dat hebben we proberen te verwezenlijken in ons labrobots

00:09:29.330 --> 00:09:33.330
omdat we denken dat dit is hoe je in de toekomst met robots gaat willen communiceren.

00:09:33.330 --> 00:09:35.330
Ik toon jullie hier een technisch diagram.

00:09:35.330 --> 00:09:39.330
Het allerbelangrijkste voor het bouwen van een robot waarmee je sociaal kunt communiceren,

00:09:39.330 --> 00:09:41.330
is zijn visuele-aandachtsysteem.

00:09:41.330 --> 00:09:44.330
Want hij besteedt aandacht aan wat hij ziet

00:09:44.330 --> 00:09:47.330
en waarmee hij interageert, en of jij begrijpt wat hij doet.

00:09:47.330 --> 00:09:50.330
In de video’s die ik jullie ga tonen,

00:09:50.330 --> 00:09:54.330
ga je een visuele-aandachtsysteem zien bij een robot.

00:09:54.330 --> 00:09:58.330
Hij let op huidskleur in de HSV-kleurruimte,

00:09:58.330 --> 00:10:02.330
hij werkt met alle menselijke huidskleuren.

00:10:02.330 --> 00:10:04.330
Het zoekt naar zeer verzadigde kleuren, van speelgoed.

00:10:04.330 --> 00:10:06.330
En hij let op dingen die bewegen.

00:10:06.330 --> 00:10:09.330
Hij weegt die af in een aandachtsvenster,

00:10:09.330 --> 00:10:11.330
en gaat naar de plaats met de hoogste score kijken -

00:10:11.330 --> 00:10:13.330
waar de interessantste dingen gebeuren -

00:10:13.330 --> 00:10:17.330
en dat is wat zijn ogen dan gaan ‘segue-en’ (volgen).

00:10:17.330 --> 00:10:19.330
Daar kijkt hij naar.

00:10:19.330 --> 00:10:22.330
Op hetzelfde moment gebeuren er een aantal top-down dingen:

00:10:22.330 --> 00:10:25.330
hij zou kunnen besluiten dat hij eenzaam is en zoeken naar huidskleur,

00:10:25.330 --> 00:10:28.330
of hij kan beslissen dat hij zich verveelt en op zoek gaan naar een stuk speelgoed om mee te spelen.

00:10:28.330 --> 00:10:30.330
Dan krijgen andere zaken meer gewicht.

00:10:30.330 --> 00:10:32.330
Hier aan de rechterkant

00:10:32.330 --> 00:10:35.330
is wat wij de Steven-Spielberg-gedenktekenmodule noemen.

00:10:35.330 --> 00:10:37.330
Heeft iemand de film ‘AI’ gezien? (Publiek: Ja.)

00:10:37.330 --> 00:10:39.330
RB: Ja, hij was echt slecht, maar -

00:10:39.330 --> 00:10:43.330
herinner je je dat Haley Joel Osment, de kleine robot,

00:10:43.330 --> 00:10:47.330
2000 jaren naar de blauwe fee keek zonder zijn ogen af te wenden?

00:10:47.330 --> 00:10:49.330
Met dit ontdoen we ons ervan

00:10:49.330 --> 00:10:53.330
want dit is een Gaussiaanse voor gewenning die negatief wordt

00:10:53.330 --> 00:10:56.330
en wel des te meer als hij naar één ding blijft kijken.

00:10:56.330 --> 00:10:59.330
Hij raakt verveeld en zal dan wegkijken naar iets anders.

00:10:59.330 --> 00:11:03.330
Nu je dat hebt gesnapt: hier is een robot, hier is Kismet

00:11:03.330 --> 00:11:07.330
op zoek naar een stuk speelgoed. Je kan zien waar hij naar kijkt.

00:11:07.330 --> 00:11:12.330
Je kan zijn blikrichting raden door die oogballen op zijn camera

00:11:12.330 --> 00:11:15.330
en je kan zien wanneer hij het speelgoed in de gaten heeft.

00:11:15.330 --> 00:11:17.330
Hij toont hier wat emotie.

00:11:17.330 --> 00:11:18.330
(Gelach)

00:11:18.330 --> 00:11:20.330
Maar hij blijft aandachtig

00:11:20.330 --> 00:11:24.330
als er iets meer betekenisvol in zijn gezichtsveld komt -

00:11:24.330 --> 00:11:28.330
zoals Cynthia Breazeal, de bouwer van deze robot, van rechts.

00:11:28.330 --> 00:11:33.330
Hij ziet haar, besteedt er aandacht aan.

00:11:33.330 --> 00:11:37.330
Kismet heeft een onderliggende, driedimensionale emotionele ruimte,

00:11:37.330 --> 00:11:40.330
een vectorruimte, van waar hij zich emotioneel bevindt.

00:11:40.330 --> 00:11:45.330
Op verschillende plekken in die ruimte, drukt hij zich uit -

00:11:46.330 --> 00:11:48.330
kan het volume even wat hoger?

00:11:48.330 --> 00:11:50.330
Horen jullie het nu? (Publiek: Ja)

00:11:50.330 --> 00:11:55.330
Kismet: Denk je dat echt? Denk je dat echt?

00:11:57.330 --> 00:11:59.330
Denk je dat echt?

00:12:00.330 --> 00:12:03.330
RB: Hij uit zijn emoties via zijn gezicht

00:12:03.330 --> 00:12:05.330
en de intonatie in zijn stem.

00:12:05.330 --> 00:12:09.330
Toen ik bezig was met mijn robot hier, Chris,

00:12:09.330 --> 00:12:12.330
mat hij de intonatie van mijn stem.

00:12:12.330 --> 00:12:17.330
Daarom laten we de robot de intonatie meten voor vier elementaire boodschappen

00:12:17.330 --> 00:12:21.330
die moeders hun kinderen pre-linguïstisch meedelen.

00:12:21.330 --> 00:12:24.330
Hier laten we onvoorbereide proefpersonen de robot prijzen:

00:12:26.330 --> 00:12:28.330
Stem: Leuke robot.

00:12:29.330 --> 00:12:31.330
Je bent zo'n schattige kleine robot.

00:12:31.330 --> 00:12:33.330
(Gelach)

00:12:33.330 --> 00:12:35.330
RB: En de robot reageert adequaat.

00:12:35.330 --> 00:12:39.330
Stem: ... heel goed, Kismet.

00:12:40.330 --> 00:12:42.330
(Gelach)

00:12:42.330 --> 00:12:44.330
Stem: Kijk eens naar mijn glimlach.

00:12:46.330 --> 00:12:49.330
RB: Hij glimlacht. Hij imiteert de glimlach. Dit gebeurt vaak.

00:12:49.330 --> 00:12:51.330
Dit zijn onvoorbereide proefpersonen.

00:12:51.330 --> 00:12:54.330
Hier hebben we hun gevraagd om de aandacht van de robot te trekken

00:12:54.330 --> 00:12:57.330
en aan te geven wanneer ze zijn aandacht hebben.

00:12:57.330 --> 00:13:01.330
Stem: Hey, Kismet, ah, daar is ie.

00:13:01.330 --> 00:13:05.330
RB: Ze beseft dat ze de aandacht van de robot heeft.

00:13:08.330 --> 00:13:12.330
Stem: Kismet, vind je het speeltje leuk? Oh.

00:13:13.330 --> 00:13:15.330
RB: Hier zijn ze gevraagd om de robot iets te verbieden,

00:13:15.330 --> 00:13:19.330
en de eerste vrouw duwt de robot echt in een emotionele hoek.

00:13:19.330 --> 00:13:24.330
Stem: Nee, nee, dat mag niet. Nee.

00:13:24.330 --> 00:13:27.330
(Gelach)

00:13:27.330 --> 00:13:33.330
Mag niet. Nee, nee

00:13:33.330 --> 00:13:36.330
(Gelach)

00:13:36.330 --> 00:13:38.330
RB: Hier ga ik het bij laten.

00:13:38.330 --> 00:13:40.330
Dat zetten we in elkaar. Dan voegen we beurtelings werken toe.

00:13:40.330 --> 00:13:43.330
Wanneer we met iemand praten, praten we,

00:13:43.330 --> 00:13:47.330
dan trekken we onze wenkbrauwen op, bewegen onze ogen

00:13:47.330 --> 00:13:50.330
om de ander aan te geven dat het zijn beurt is om te praten.

00:13:50.330 --> 00:13:54.330
Dan gaan zij praten en dan geven we het stokje heen en weer aan elkaar door.

00:13:54.330 --> 00:13:56.330
Daarom hebben we dit in de robot ondergebracht.

00:13:56.330 --> 00:13:58.330
We brachten er een stel onvoorbereide proefpersonen bij,

00:13:58.330 --> 00:14:00.330
vertelden hen niets over de robot,

00:14:00.330 --> 00:14:02.330
zetten ze voor de robot en zeiden: “Praat met de robot.”

00:14:02.330 --> 00:14:04.330
Wat zij niet wisten was dat

00:14:04.330 --> 00:14:06.330
de robot geen woord snapte van wat ze zeiden

00:14:06.330 --> 00:14:09.330
en dat de robot geen Engels sprak.

00:14:09.330 --> 00:14:11.330
Hij produceerde alleen maar wat willekeurige Engelse fonemen.

00:14:11.330 --> 00:14:13.330
Let vooral in het begin goed op

00:14:13.330 --> 00:14:17.330
als deze persoon, Ritchie, die 25 minuten lang met de robot sprak -

00:14:17.330 --> 00:14:19.330
(Gelach)

00:14:19.330 --> 00:14:21.330
- zegt: "Ik wil je iets laten zien.

00:14:21.330 --> 00:14:23.330
Ik wil je mijn horloge tonen.”

00:14:23.330 --> 00:14:28.330
Hij brengt zijn horloge midden in het gezichtsveld van de robot,

00:14:28.330 --> 00:14:30.330
wijst ernaar, geeft hem een bewegingsaanwijzing

00:14:30.330 --> 00:14:32.330
en de robot slaagt erin naar het horloge te kijken.

00:14:32.330 --> 00:14:35.330
We weten niet of hij wel of niet begreep, dat de robot -

00:14:36.330 --> 00:14:38.330
Let op het beurtelings reageren.

00:14:38.330 --> 00:14:41.330
Ritchie: Ik wil je iets laten zien. Dit is een horloge

00:14:41.330 --> 00:14:44.330
dat ik kreeg van mijn vriendin.

00:14:44.330 --> 00:14:46.330
Robot: Oh, cool.

00:14:46.330 --> 00:14:50.330
Ritchie: Ja, kijk, er zit ook een blauw lichtje in. Ik verloor het deze week bijna.

00:14:51.330 --> 00:14:55.330
(Gelach)

00:14:55.330 --> 00:14:58.330
RB: Hij maakt oogcontact met hem, volgt zijn ogen.

00:14:58.330 --> 00:15:00.330
Ritchie: Kan je dat ook? Robot: Ja, zeker.

00:15:00.330 --> 00:15:02.330
RB: Ze slagen er dus in een bepaald soort communicatie tot stand te brengen.

00:15:02.330 --> 00:15:06.330
Hier is een ander aspect van de dingen die Chris en ik deden.

00:15:06.330 --> 00:15:08.330
Dit is een andere robot, Cog.

00:15:08.330 --> 00:15:14.330
Ze maken eerst oogcontact en dan, als Christie naar dit speeltje kijkt,

00:15:14.330 --> 00:15:16.330
raadt de robot haar blikrichting

00:15:16.330 --> 00:15:18.330
en kijkt naar hetzelfde ding waar zij naar kijkt.

00:15:18.330 --> 00:15:19.330
(Gelach)

00:15:19.330 --> 00:15:22.330
We gaan dit soort robot de komende jaren

00:15:22.330 --> 00:15:24.330
in laboratoria meer en meer te zien krijgen.

00:15:24.330 --> 00:15:29.330
Maar dan de grote vragen, de twee grote vragen die mensen mij stellen, zijn:

00:15:29.330 --> 00:15:31.330
“Als we deze robots meer en meer mensachtig gaan maken,

00:15:31.330 --> 00:15:36.330
zullen we ze dan accepteren, zullen ze dan uiteindelijk rechten moeten hebben?”

00:15:36.330 --> 00:15:39.330
De andere vraag die mensen mij stellen is: “Zullen ze de zaak willen overnemen?”

00:15:39.330 --> 00:15:40.330
(Gelach)

00:15:40.330 --> 00:15:43.330
Op de eerste – ja dat is een echt Hollywoodthema

00:15:43.330 --> 00:15:46.330
met een hoop films. Je herkent waarschijnlijk deze personages -

00:15:46.330 --> 00:15:50.330
waar in elk van deze gevallen, de robots meer respect willen.

00:15:50.330 --> 00:15:53.330
Zal je robots ooit moeten respecteren?

00:15:54.330 --> 00:15:56.330
Het zijn per slot van rekening toch alleen maar machines.

00:15:56.330 --> 00:16:00.330
Maar ik denk, dat we moeten accepteren dat wij ook slechts machines zijn.

00:16:00.330 --> 00:16:05.330
Dat is immers wat de moderne moleculaire biologie over ons zegt.

00:16:05.330 --> 00:16:08.330
Je zal geen beschrijving vinden van hoe

00:16:08.330 --> 00:16:12.330
molecule A zich, aangedreven door tegengestelde ladingen,

00:16:12.330 --> 00:16:15.330
beweegt naar molecule B om zich dan,

00:16:15.330 --> 00:16:19.330
door de bemoeienis van een ‘ziel’, eraan te binden.

00:16:19.330 --> 00:16:22.330
Het is allemaal mechanistisch. We zijn een mechanisme.

00:16:22.330 --> 00:16:25.330
Als we machines zijn, dan zijn we, in principe althans,

00:16:25.330 --> 00:16:29.330
in staat om machines te bouwen met behulp van andere dingen.

00:16:29.330 --> 00:16:33.330
Machines die net zo levend zijn als wij.

00:16:33.330 --> 00:16:35.330
Maar ik denk dat om dat toe te geven,

00:16:35.330 --> 00:16:38.330
we onze uniciteit moeten opgeven.

00:16:38.330 --> 00:16:40.330
We hebben onze uniciteit, op zijn minst

00:16:40.330 --> 00:16:43.330
in de afgelopen paar honderd jaar, al vaak moeten opgeven

00:16:43.330 --> 00:16:45.330
onder het spervuur van wetenschap en technologie.

00:16:45.330 --> 00:16:47.330
500 jaar geleden moesten we het idee opgeven

00:16:47.330 --> 00:16:50.330
dat wij het middelpunt van het universum waren

00:16:50.330 --> 00:16:52.330
toen de aarde rond de zon begon te draaien.

00:16:52.330 --> 00:16:57.330
150 jaar geleden moesten we met Darwin het idee opgeven dat we iets anders waren dan dieren.

00:16:57.330 --> 00:17:00.330
Voor ons moeilijk verteerbaar.

00:17:00.330 --> 00:17:03.330
Onlangs werden we om de oren geslagen met het idee dat we misschien

00:17:03.330 --> 00:17:05.330
op aarde niet eens onze speciale schepping hadden.

00:17:05.330 --> 00:17:08.330
Mensen houden daar niet van. Dan vertelde het menselijk genoom ons

00:17:08.330 --> 00:17:11.330
dat we misschien maar 35 duizend genen hadden. Daar

00:17:11.330 --> 00:17:14.330
houden mensen ook niet van, we willen meer genen dan dat.

00:17:14.330 --> 00:17:17.330
We houden er niet van onze uniciteit op te geven.

00:17:17.330 --> 00:17:19.330
Het idee dat robots echt emoties zouden kunnen hebben

00:17:19.330 --> 00:17:21.330
of dat robots levende wezens kunnen worden -

00:17:21.330 --> 00:17:23.330
ik denk dat we dat moeilijk zullen kunnen slikken.

00:17:23.330 --> 00:17:27.330
Maar we gaan het in de komende 50 jaar of zo moeten accepteren.

00:17:27.330 --> 00:17:30.330
En de tweede vraag: “Zullen de machines de zaak willen overnemen?”

00:17:30.330 --> 00:17:35.330
Het standaard scenario is dat we deze dingen gaan maken,

00:17:35.330 --> 00:17:38.330
ze groeien, we voeden ze, ze leren heel veel van ons

00:17:38.330 --> 00:17:42.330
en dan gaan ze vinden dat we maar erg saai en traag zijn.

00:17:42.330 --> 00:17:44.330
Ze zullen het willen overnemen van ons.

00:17:44.330 --> 00:17:47.330
Ouders met tieners kennen dat.

00:17:47.330 --> 00:17:48.330
(Gelach)

00:17:48.330 --> 00:17:51.330
Maar Hollywood breidt dat uit naar robots.

00:17:51.330 --> 00:17:54.330
De vraag is of

00:17:54.330 --> 00:17:58.330
iemand per ongeluk een robot bouwt die het overneemt van ons?

00:17:58.330 --> 00:18:01.330
Dat is zoiets als die eenling in zijn achtertuin

00:18:01.330 --> 00:18:04.330
die zegt: "Ik heb per ongeluk een 747 gebouwd."

00:18:04.330 --> 00:18:06.330
Ik denk niet dat dat gaat gebeuren.

00:18:06.330 --> 00:18:08.330
Ik denk niet dat -

00:18:08.330 --> 00:18:09.330
(Gelach)

00:18:09.330 --> 00:18:12.330
- ik denk niet dat we bewust robots gaan bouwen

00:18:12.330 --> 00:18:14.330
waar we ons ongemakkelijk bij gaan voelen.

00:18:14.330 --> 00:18:16.330
We gaan geen superslechte robot maken.

00:18:16.330 --> 00:18:19.330
Daarvoor moet je eerst een robot maken die een beetje slecht is

00:18:19.330 --> 00:18:21.330
en daarvóór een niet zo slechte robot.

00:18:21.330 --> 00:18:22.330
(Gelach)

00:18:22.330 --> 00:18:24.330
Zoiets gaan we gewoon niet laten gebeuren.

00:18:24.330 --> 00:18:25.330
(Gelach)

00:18:25.330 --> 00:18:31.330
Ik denk dat ik het hierbij ga laten: de robots komen.

00:18:31.330 --> 00:18:34.330
We hoeven ons daar niet te veel zorgen over te maken, het gaat heel leuk zijn

00:18:34.330 --> 00:18:38.330
en ik hoop dat jullie er de volgende 50 jaar nog veel plezier aan zullen beleven.

00:18:38.330 --> 00:18:40.330
(Applaus)

