WEBVTT
Kind: captions
Language: en

00:00:18.330 --> 00:00:23.330
What I want to tell you about today is how I see robots invading our lives

00:00:23.330 --> 00:00:26.330
at multiple levels, over multiple timescales.

00:00:26.330 --> 00:00:30.330
And when I look out in the future, I can't imagine a world, 500 years from now,

00:00:30.330 --> 00:00:32.330
where we don't have robots everywhere.

00:00:32.330 --> 00:00:37.330
Assuming -- despite all the dire predictions from many people about our future --

00:00:37.330 --> 00:00:41.330
assuming we're still around, I can't imagine the world not being populated with robots.

00:00:41.330 --> 00:00:44.330
And then the question is, well, if they're going to be here in 500 years,

00:00:44.330 --> 00:00:46.330
are they going to be everywhere sooner than that?

00:00:46.330 --> 00:00:48.330
Are they going to be around in 50 years?

00:00:48.330 --> 00:00:51.330
Yeah, I think that's pretty likely -- there's going to be lots of robots everywhere.

00:00:51.330 --> 00:00:54.330
And in fact I think that's going to be a lot sooner than that.

00:00:54.330 --> 00:00:58.330
I think we're sort of on the cusp of robots becoming common,

00:00:58.330 --> 00:01:04.330
and I think we're sort of around 1978 or 1980 in personal computer years,

00:01:04.330 --> 00:01:07.330
where the first few robots are starting to appear.

00:01:07.330 --> 00:01:11.330
Computers sort of came around through games and toys.

00:01:11.330 --> 00:01:14.330
And you know, the first computer most people had in the house

00:01:14.330 --> 00:01:16.330
may have been a computer to play Pong,

00:01:16.330 --> 00:01:18.330
a little microprocessor embedded,

00:01:18.330 --> 00:01:21.330
and then other games that came after that.

00:01:21.330 --> 00:01:24.330
And we're starting to see that same sort of thing with robots:

00:01:24.330 --> 00:01:28.330
LEGO Mindstorms, Furbies -- who here -- did anyone here have a Furby?

00:01:28.330 --> 00:01:31.330
Yeah, there's 38 million of them sold worldwide.

00:01:31.330 --> 00:01:33.330
They are pretty common. And they're a little tiny robot,

00:01:33.330 --> 00:01:35.330
a simple robot with some sensors,

00:01:35.330 --> 00:01:37.330
a little bit of processing actuation.

00:01:37.330 --> 00:01:40.330
On the right there is another robot doll, who you could get a couple of years ago.

00:01:40.330 --> 00:01:42.330
And just as in the early days,

00:01:42.330 --> 00:01:47.330
when there was a lot of sort of amateur interaction over computers,

00:01:47.330 --> 00:01:51.330
you can now get various hacking kits, how-to-hack books.

00:01:51.330 --> 00:01:55.330
And on the left there is a platform from Evolution Robotics,

00:01:55.330 --> 00:01:58.330
where you put a PC on, and you program this thing with a GUI

00:01:58.330 --> 00:02:01.330
to wander around your house and do various stuff.

00:02:01.330 --> 00:02:04.330
And then there's a higher price point sort of robot toys --

00:02:04.330 --> 00:02:08.330
the Sony Aibo. And on the right there, is one that the NEC developed,

00:02:08.330 --> 00:02:11.330
the PaPeRo, which I don't think they're going to release.

00:02:11.330 --> 00:02:14.330
But nevertheless, those sorts of things are out there.

00:02:14.330 --> 00:02:18.330
And we've seen, over the last two or three years, lawn-mowing robots,

00:02:18.330 --> 00:02:24.330
Husqvarna on the bottom, Friendly Robotics on top there, an Israeli company.

00:02:24.330 --> 00:02:26.330
And then in the last 12 months or so

00:02:26.330 --> 00:02:30.330
we've started to see a bunch of home-cleaning robots appear.

00:02:30.330 --> 00:02:33.330
The top left one is a very nice home-cleaning robot

00:02:33.330 --> 00:02:37.330
from a company called Dyson, in the U.K. Except it was so expensive --

00:02:37.330 --> 00:02:39.330
3,500 dollars -- they didn't release it.

00:02:39.330 --> 00:02:42.330
But at the bottom left, you see Electrolux, which is on sale.

00:02:42.330 --> 00:02:44.330
Another one from Karcher.

00:02:44.330 --> 00:02:46.330
At the bottom right is one that I built in my lab

00:02:46.330 --> 00:02:49.330
about 10 years ago, and we finally turned that into a product.

00:02:49.330 --> 00:02:51.330
And let me just show you that.

00:02:51.330 --> 00:02:55.330
We're going to give this away I think, Chris said, after the talk.

00:02:55.330 --> 00:03:01.330
This is a robot that you can go out and buy, and that will clean up your floor.

00:03:05.330 --> 00:03:10.330
And it starts off sort of just going around in ever-increasing circles.

00:03:10.330 --> 00:03:14.330
If it hits something -- you people see that?

00:03:14.330 --> 00:03:17.330
Now it's doing wall-following, it's following around my feet

00:03:17.330 --> 00:03:21.330
to clean up around me. Let's see, let's --

00:03:21.330 --> 00:03:26.330
oh, who stole my Rice Krispies? They stole my Rice Krispies!

00:03:26.330 --> 00:03:32.330
(Laughter)

00:03:32.330 --> 00:03:35.330
Don't worry, relax, no, relax, it's a robot, it's smart!

00:03:35.330 --> 00:03:38.330
(Laughter)

00:03:38.330 --> 00:03:42.330
See, the three-year-old kids, they don't worry about it.

00:03:42.330 --> 00:03:44.330
It's grown-ups that get really upset.

00:03:44.330 --> 00:03:45.330
(Laughter)

00:03:45.330 --> 00:03:47.330
We'll just put some crap here.

00:03:47.330 --> 00:03:51.330
(Laughter)

00:03:51.330 --> 00:03:53.330
Okay.

00:03:53.330 --> 00:03:57.330
(Laughter)

00:03:57.330 --> 00:04:00.330
I don't know if you see -- so, I put a bunch of Rice Krispies there,

00:04:00.330 --> 00:04:07.330
I put some pennies, let's just shoot it at that, see if it cleans up.

00:04:10.330 --> 00:04:12.330
Yeah, OK. So --

00:04:12.330 --> 00:04:16.330
we'll leave that for later.

00:04:16.330 --> 00:04:21.330
(Applause)

00:04:22.330 --> 00:04:26.330
Part of the trick was building a better cleaning mechanism, actually;

00:04:26.330 --> 00:04:30.330
the intelligence on board was fairly simple.

00:04:30.330 --> 00:04:32.330
And that's true with a lot of robots.

00:04:32.330 --> 00:04:36.330
We've all, I think, become, sort of computational chauvinists,

00:04:36.330 --> 00:04:38.330
and think that computation is everything,

00:04:38.330 --> 00:04:40.330
but the mechanics still matter.

00:04:40.330 --> 00:04:43.330
Here's another robot, the PackBot,

00:04:43.330 --> 00:04:45.330
that we've been building for a bunch of years.

00:04:45.330 --> 00:04:51.330
It's a military surveillance robot, to go in ahead of troops --

00:04:51.330 --> 00:04:54.330
looking at caves, for instance.

00:04:54.330 --> 00:04:56.330
But we had to make it fairly robust,

00:04:56.330 --> 00:05:03.330
much more robust than the robots we build in our labs.

00:05:03.330 --> 00:05:06.330
(Laughter)

00:05:12.330 --> 00:05:16.330
On board that robot is a PC running Linux.

00:05:16.330 --> 00:05:22.330
It can withstand a 400G shock. The robot has local intelligence:

00:05:22.330 --> 00:05:28.330
it can flip itself over, can get itself into communication range,

00:05:28.330 --> 00:05:31.330
can go upstairs by itself, et cetera.

00:05:38.330 --> 00:05:42.330
Okay, so it's doing local navigation there.

00:05:42.330 --> 00:05:48.330
A soldier gives it a command to go upstairs, and it does.

00:05:49.330 --> 00:05:52.330
That was not a controlled descent.

00:05:52.330 --> 00:05:54.330
(Laughter)

00:05:54.330 --> 00:05:56.330
Now it's going to head off.

00:05:56.330 --> 00:06:01.330
And the big breakthrough for these robots, really, was September 11th.

00:06:01.330 --> 00:06:05.330
We had the robots down at the World Trade Center late that evening.

00:06:06.330 --> 00:06:08.330
Couldn't do a lot in the main rubble pile,

00:06:08.330 --> 00:06:11.330
things were just too -- there was nothing left to do.

00:06:11.330 --> 00:06:16.330
But we did go into all the surrounding buildings that had been evacuated,

00:06:16.330 --> 00:06:19.330
and searched for possible survivors in the buildings

00:06:19.330 --> 00:06:21.330
that were too dangerous to go into.

00:06:21.330 --> 00:06:23.330
Let's run this video.

00:06:23.330 --> 00:06:26.330
Reporter: ...battlefield companions are helping to reduce the combat risks.

00:06:26.330 --> 00:06:29.330
Nick Robertson has that story.

00:06:31.330 --> 00:06:33.330
Rodney Brooks: Can we have another one of these?

00:06:38.330 --> 00:06:40.330
Okay, good.

00:06:43.330 --> 00:06:46.330
So, this is a corporal who had seen a robot two weeks previously.

00:06:48.330 --> 00:06:52.330
He's sending robots into caves, looking at what's going on.

00:06:52.330 --> 00:06:54.330
The robot's being totally autonomous.

00:06:54.330 --> 00:06:58.330
The worst thing that's happened in the cave so far

00:06:58.330 --> 00:07:01.330
was one of the robots fell down ten meters.

00:07:08.330 --> 00:07:11.330
So one year ago, the US military didn't have these robots.

00:07:11.330 --> 00:07:13.330
Now they're on active duty in Afghanistan every day.

00:07:13.330 --> 00:07:16.330
And that's one of the reasons they say a robot invasion is happening.

00:07:16.330 --> 00:07:20.330
There's a sea change happening in how -- where technology's going.

00:07:20.330 --> 00:07:22.330
Thanks.

00:07:23.330 --> 00:07:25.330
And over the next couple of months,

00:07:25.330 --> 00:07:28.330
we're going to be sending robots in production

00:07:28.330 --> 00:07:32.330
down producing oil wells to get that last few years of oil out of the ground.

00:07:32.330 --> 00:07:36.330
Very hostile environments, 150Ëš C, 10,000 PSI.

00:07:36.330 --> 00:07:40.330
Autonomous robots going down, doing this sort of work.

00:07:40.330 --> 00:07:43.330
But robots like this, they're a little hard to program.

00:07:43.330 --> 00:07:45.330
How, in the future, are we going to program our robots

00:07:45.330 --> 00:07:47.330
and make them easier to use?

00:07:47.330 --> 00:07:50.330
And I want to actually use a robot here --

00:07:50.330 --> 00:07:55.330
a robot named Chris -- stand up. Yeah. Okay.

00:07:57.330 --> 00:08:01.330
Come over here. Now notice, he thinks robots have to be a bit stiff.

00:08:01.330 --> 00:08:04.330
He sort of does that. But I'm going to --

00:08:04.330 --> 00:08:06.330
Chris Anderson: I'm just British. RB: Oh.

00:08:06.330 --> 00:08:08.330
(Laughter)

00:08:08.330 --> 00:08:10.330
(Applause)

00:08:10.330 --> 00:08:13.330
I'm going to show this robot a task. It's a very complex task.

00:08:13.330 --> 00:08:16.330
Now notice, he nodded there, he was giving me some indication

00:08:16.330 --> 00:08:19.330
he was understanding the flow of communication.

00:08:19.330 --> 00:08:21.330
And if I'd said something completely bizarre

00:08:21.330 --> 00:08:24.330
he would have looked askance at me, and regulated the conversation.

00:08:24.330 --> 00:08:27.330
So now I brought this up in front of him.

00:08:27.330 --> 00:08:31.330
I'd looked at his eyes, and I saw his eyes looked at this bottle top.

00:08:31.330 --> 00:08:33.330
And I'm doing this task here, and he's checking up.

00:08:33.330 --> 00:08:36.330
His eyes are going back and forth up to me, to see what I'm looking at --

00:08:36.330 --> 00:08:38.330
so we've got shared attention.

00:08:38.330 --> 00:08:41.330
And so I do this task, and he looks, and he looks to me

00:08:41.330 --> 00:08:45.330
to see what's happening next. And now I'll give him the bottle,

00:08:45.330 --> 00:08:47.330
and we'll see if he can do the task. Can you do that?

00:08:47.330 --> 00:08:50.330
(Laughter)

00:08:50.330 --> 00:08:54.330
Okay. He's pretty good. Yeah. Good, good, good.

00:08:54.330 --> 00:08:56.330
I didn't show you how to do that.

00:08:56.330 --> 00:08:58.330
Now see if you can put it back together.

00:08:58.330 --> 00:09:00.330
(Laughter)

00:09:00.330 --> 00:09:01.330
And he thinks a robot has to be really slow.

00:09:01.330 --> 00:09:03.330
Good robot, that's good.

00:09:03.330 --> 00:09:05.330
So we saw a bunch of things there.

00:09:06.330 --> 00:09:09.330
We saw when we're interacting,

00:09:09.330 --> 00:09:13.330
we're trying to show someone how to do something, we direct their visual attention.

00:09:13.330 --> 00:09:17.330
The other thing communicates their internal state to us,

00:09:17.330 --> 00:09:20.330
whether he's understanding or not, regulates a social interaction.

00:09:20.330 --> 00:09:22.330
There was shared attention looking at the same sort of thing,

00:09:22.330 --> 00:09:26.330
and recognizing socially communicated reinforcement at the end.

00:09:26.330 --> 00:09:29.330
And we've been trying to put that into our lab robots

00:09:29.330 --> 00:09:33.330
because we think this is how you're going to want to interact with robots in the future.

00:09:33.330 --> 00:09:35.330
I just want to show you one technical diagram here.

00:09:35.330 --> 00:09:39.330
The most important thing for building a robot that you can interact with socially

00:09:39.330 --> 00:09:41.330
is its visual attention system.

00:09:41.330 --> 00:09:44.330
Because what it pays attention to is what it's seeing

00:09:44.330 --> 00:09:47.330
and interacting with, and what you're understanding what it's doing.

00:09:47.330 --> 00:09:50.330
So in the videos I'm about to show you,

00:09:50.330 --> 00:09:54.330
you're going to see a visual attention system on a robot

00:09:54.330 --> 00:09:58.330
which has -- it looks for skin tone in HSV space,

00:09:58.330 --> 00:10:02.330
so it works across all human colorings.

00:10:02.330 --> 00:10:04.330
It looks for highly saturated colors, from toys.

00:10:04.330 --> 00:10:06.330
And it looks for things that move around.

00:10:06.330 --> 00:10:09.330
And it weights those together into an attention window,

00:10:09.330 --> 00:10:11.330
and it looks for the highest-scoring place --

00:10:11.330 --> 00:10:13.330
the stuff where the most interesting stuff is happening --

00:10:13.330 --> 00:10:17.330
and that is what its eyes then segue to.

00:10:17.330 --> 00:10:19.330
And it looks right at that.

00:10:19.330 --> 00:10:22.330
At the same time, some top-down sort of stuff:

00:10:22.330 --> 00:10:25.330
might decide that it's lonely and look for skin tone,

00:10:25.330 --> 00:10:28.330
or might decide that it's bored and look for a toy to play with.

00:10:28.330 --> 00:10:30.330
And so these weights change.

00:10:30.330 --> 00:10:32.330
And over here on the right,

00:10:32.330 --> 00:10:35.330
this is what we call the Steven Spielberg memorial module.

00:10:35.330 --> 00:10:37.330
Did people see the movie "AI"? (Audience: Yes.)

00:10:37.330 --> 00:10:39.330
RB: Yeah, it was really bad, but --

00:10:39.330 --> 00:10:43.330
remember, especially when Haley Joel Osment, the little robot,

00:10:43.330 --> 00:10:47.330
looked at the blue fairy for 2,000 years without taking his eyes off it?

00:10:47.330 --> 00:10:49.330
Well, this gets rid of that,

00:10:49.330 --> 00:10:53.330
because this is a habituation Gaussian that gets negative,

00:10:53.330 --> 00:10:56.330
and more and more intense as it looks at one thing.

00:10:56.330 --> 00:10:59.330
And it gets bored, so it will then look away at something else.

00:10:59.330 --> 00:11:03.330
So, once you've got that -- and here's a robot, here's Kismet,

00:11:03.330 --> 00:11:07.330
looking around for a toy. You can tell what it's looking at.

00:11:07.330 --> 00:11:12.330
You can estimate its gaze direction from those eyeballs covering its camera,

00:11:12.330 --> 00:11:15.330
and you can tell when it's actually seeing the toy.

00:11:15.330 --> 00:11:17.330
And it's got a little bit of an emotional response here.

00:11:17.330 --> 00:11:18.330
(Laughter)

00:11:18.330 --> 00:11:20.330
But it's still going to pay attention

00:11:20.330 --> 00:11:24.330
if something more significant comes into its field of view --

00:11:24.330 --> 00:11:28.330
such as Cynthia Breazeal, the builder of this robot, from the right.

00:11:28.330 --> 00:11:33.330
It sees her, pays attention to her.

00:11:33.330 --> 00:11:37.330
Kismet has an underlying, three-dimensional emotional space,

00:11:37.330 --> 00:11:40.330
a vector space, of where it is emotionally.

00:11:40.330 --> 00:11:45.330
And at different places in that space, it expresses --

00:11:46.330 --> 00:11:48.330
can we have the volume on here?

00:11:48.330 --> 00:11:50.330
Can you hear that now, out there? (Audience: Yeah.)

00:11:50.330 --> 00:11:55.330
Kismet: Do you really think so? Do you really think so?

00:11:57.330 --> 00:11:59.330
Do you really think so?

00:12:00.330 --> 00:12:03.330
RB: So it's expressing its emotion through its face

00:12:03.330 --> 00:12:05.330
and the prosody in its voice.

00:12:05.330 --> 00:12:09.330
And when I was dealing with my robot over here,

00:12:09.330 --> 00:12:12.330
Chris, the robot, was measuring the prosody in my voice,

00:12:12.330 --> 00:12:17.330
and so we have the robot measure prosody for four basic messages

00:12:17.330 --> 00:12:21.330
that mothers give their children pre-linguistically.

00:12:21.330 --> 00:12:24.330
Here we've got naive subjects praising the robot:

00:12:26.330 --> 00:12:28.330
Voice: Nice robot.

00:12:29.330 --> 00:12:31.330
You're such a cute little robot.

00:12:31.330 --> 00:12:33.330
(Laughter)

00:12:33.330 --> 00:12:35.330
RB: And the robot's reacting appropriately.

00:12:35.330 --> 00:12:39.330
Voice: ...very good, Kismet.

00:12:40.330 --> 00:12:42.330
(Laughter)

00:12:42.330 --> 00:12:44.330
Voice: Look at my smile.

00:12:46.330 --> 00:12:49.330
RB: It smiles. She imitates the smile. This happens a lot.

00:12:49.330 --> 00:12:51.330
These are naive subjects.

00:12:51.330 --> 00:12:54.330
Here we asked them to get the robot's attention

00:12:54.330 --> 00:12:57.330
and indicate when they have the robot's attention.

00:12:57.330 --> 00:13:01.330
Voice: Hey, Kismet, ah, there it is.

00:13:01.330 --> 00:13:05.330
RB: So she realizes she has the robot's attention.

00:13:08.330 --> 00:13:12.330
Voice: Kismet, do you like the toy? Oh.

00:13:13.330 --> 00:13:15.330
RB: Now, here they're asked to prohibit the robot,

00:13:15.330 --> 00:13:19.330
and this first woman really pushes the robot into an emotional corner.

00:13:19.330 --> 00:13:24.330
Voice: No. No. You're not to do that. No.

00:13:24.330 --> 00:13:27.330
(Laughter)

00:13:27.330 --> 00:13:33.330
Not appropriate. No. No.

00:13:33.330 --> 00:13:36.330
(Laughter)

00:13:36.330 --> 00:13:38.330
RB: I'm going to leave it at that.

00:13:38.330 --> 00:13:40.330
We put that together. Then we put in turn taking.

00:13:40.330 --> 00:13:43.330
When we talk to someone, we talk.

00:13:43.330 --> 00:13:47.330
Then we sort of raise our eyebrows, move our eyes,

00:13:47.330 --> 00:13:50.330
give the other person the idea it's their turn to talk.

00:13:50.330 --> 00:13:54.330
And then they talk, and then we pass the baton back and forth between each other.

00:13:54.330 --> 00:13:56.330
So we put this in the robot.

00:13:56.330 --> 00:13:58.330
We got a bunch of naive subjects in,

00:13:58.330 --> 00:14:00.330
we didn't tell them anything about the robot,

00:14:00.330 --> 00:14:02.330
sat them down in front of the robot and said, talk to the robot.

00:14:02.330 --> 00:14:04.330
Now what they didn't know was,

00:14:04.330 --> 00:14:06.330
the robot wasn't understanding a word they said,

00:14:06.330 --> 00:14:09.330
and that the robot wasn't speaking English.

00:14:09.330 --> 00:14:11.330
It was just saying random English phonemes.

00:14:11.330 --> 00:14:13.330
And I want you to watch carefully, at the beginning of this,

00:14:13.330 --> 00:14:17.330
where this person, Ritchie, who happened to talk to the robot for 25 minutes --

00:14:17.330 --> 00:14:19.330
(Laughter)

00:14:19.330 --> 00:14:21.330
-- says, "I want to show you something.

00:14:21.330 --> 00:14:23.330
I want to show you my watch."

00:14:23.330 --> 00:14:28.330
And he brings the watch center, into the robot's field of vision,

00:14:28.330 --> 00:14:30.330
points to it, gives it a motion cue,

00:14:30.330 --> 00:14:32.330
and the robot looks at the watch quite successfully.

00:14:32.330 --> 00:14:35.330
We don't know whether he understood or not that the robot --

00:14:36.330 --> 00:14:38.330
Notice the turn-taking.

00:14:38.330 --> 00:14:41.330
Ritchie: OK, I want to show you something. OK, this is a watch

00:14:41.330 --> 00:14:44.330
that my girlfriend gave me.

00:14:44.330 --> 00:14:46.330
Robot: Oh, cool.

00:14:46.330 --> 00:14:50.330
Ritchie: Yeah, look, it's got a little blue light in it too. I almost lost it this week.

00:14:51.330 --> 00:14:55.330
(Laughter)

00:14:55.330 --> 00:14:58.330
RB: So it's making eye contact with him, following his eyes.

00:14:58.330 --> 00:15:00.330
Ritchie: Can you do the same thing? Robot: Yeah, sure.

00:15:00.330 --> 00:15:02.330
RB: And they successfully have that sort of communication.

00:15:02.330 --> 00:15:06.330
And here's another aspect of the sorts of things that Chris and I were doing.

00:15:06.330 --> 00:15:08.330
This is another robot, Cog.

00:15:08.330 --> 00:15:14.330
They first make eye contact, and then, when Christie looks over at this toy,

00:15:14.330 --> 00:15:16.330
the robot estimates her gaze direction

00:15:16.330 --> 00:15:18.330
and looks at the same thing that she's looking at.

00:15:18.330 --> 00:15:19.330
(Laughter)

00:15:19.330 --> 00:15:22.330
So we're going to see more and more of this sort of robot

00:15:22.330 --> 00:15:24.330
over the next few years in labs.

00:15:24.330 --> 00:15:29.330
But then the big questions, two big questions that people ask me are:

00:15:29.330 --> 00:15:31.330
if we make these robots more and more human-like,

00:15:31.330 --> 00:15:36.330
will we accept them, will we -- will they need rights eventually?

00:15:36.330 --> 00:15:39.330
And the other question people ask me is, will they want to take over?

00:15:39.330 --> 00:15:40.330
(Laughter)

00:15:40.330 --> 00:15:43.330
And on the first -- you know, this has been a very Hollywood theme

00:15:43.330 --> 00:15:46.330
with lots of movies. You probably recognize these characters here --

00:15:46.330 --> 00:15:50.330
where in each of these cases, the robots want more respect.

00:15:50.330 --> 00:15:53.330
Well, do you ever need to give robots respect?

00:15:54.330 --> 00:15:56.330
They're just machines, after all.

00:15:56.330 --> 00:16:00.330
But I think, you know, we have to accept that we are just machines.

00:16:00.330 --> 00:16:05.330
After all, that's certainly what modern molecular biology says about us.

00:16:05.330 --> 00:16:08.330
You don't see a description of how, you know,

00:16:08.330 --> 00:16:12.330
Molecule A, you know, comes up and docks with this other molecule.

00:16:12.330 --> 00:16:15.330
And it's moving forward, you know, propelled by various charges,

00:16:15.330 --> 00:16:19.330
and then the soul steps in and tweaks those molecules so that they connect.

00:16:19.330 --> 00:16:22.330
It's all mechanistic. We are mechanism.

00:16:22.330 --> 00:16:25.330
If we are machines, then in principle at least,

00:16:25.330 --> 00:16:29.330
we should be able to build machines out of other stuff,

00:16:29.330 --> 00:16:33.330
which are just as alive as we are.

00:16:33.330 --> 00:16:35.330
But I think for us to admit that,

00:16:35.330 --> 00:16:38.330
we have to give up on our special-ness, in a certain way.

00:16:38.330 --> 00:16:40.330
And we've had the retreat from special-ness

00:16:40.330 --> 00:16:43.330
under the barrage of science and technology many times

00:16:43.330 --> 00:16:45.330
over the last few hundred years, at least.

00:16:45.330 --> 00:16:47.330
500 years ago we had to give up the idea

00:16:47.330 --> 00:16:50.330
that we are the center of the universe

00:16:50.330 --> 00:16:52.330
when the earth started to go around the sun;

00:16:52.330 --> 00:16:57.330
150 years ago, with Darwin, we had to give up the idea we were different from animals.

00:16:57.330 --> 00:17:00.330
And to imagine -- you know, it's always hard for us.

00:17:00.330 --> 00:17:03.330
Recently we've been battered with the idea that maybe

00:17:03.330 --> 00:17:05.330
we didn't even have our own creation event, here on earth,

00:17:05.330 --> 00:17:08.330
which people didn't like much. And then the human genome said,

00:17:08.330 --> 00:17:11.330
maybe we only have 35,000 genes. And that was really --

00:17:11.330 --> 00:17:14.330
people didn't like that, we've got more genes than that.

00:17:14.330 --> 00:17:17.330
We don't like to give up our special-ness, so, you know,

00:17:17.330 --> 00:17:19.330
having the idea that robots could really have emotions,

00:17:19.330 --> 00:17:21.330
or that robots could be living creatures --

00:17:21.330 --> 00:17:23.330
I think is going to be hard for us to accept.

00:17:23.330 --> 00:17:27.330
But we're going to come to accept it over the next 50 years or so.

00:17:27.330 --> 00:17:30.330
And the second question is, will the machines want to take over?

00:17:30.330 --> 00:17:35.330
And here the standard scenario is that we create these things,

00:17:35.330 --> 00:17:38.330
they grow, we nurture them, they learn a lot from us,

00:17:38.330 --> 00:17:42.330
and then they start to decide that we're pretty boring, slow.

00:17:42.330 --> 00:17:44.330
They want to take over from us.

00:17:44.330 --> 00:17:47.330
And for those of you that have teenagers, you know what that's like.

00:17:47.330 --> 00:17:48.330
(Laughter)

00:17:48.330 --> 00:17:51.330
But Hollywood extends it to the robots.

00:17:51.330 --> 00:17:54.330
And the question is, you know,

00:17:54.330 --> 00:17:58.330
will someone accidentally build a robot that takes over from us?

00:17:58.330 --> 00:18:01.330
And that's sort of like this lone guy in the backyard,

00:18:01.330 --> 00:18:04.330
you know -- "I accidentally built a 747."

00:18:04.330 --> 00:18:06.330
I don't think that's going to happen.

00:18:06.330 --> 00:18:08.330
And I don't think --

00:18:08.330 --> 00:18:09.330
(Laughter)

00:18:09.330 --> 00:18:12.330
-- I don't think we're going to deliberately build robots

00:18:12.330 --> 00:18:14.330
that we're uncomfortable with.

00:18:14.330 --> 00:18:16.330
We'll -- you know, they're not going to have a super bad robot.

00:18:16.330 --> 00:18:19.330
Before that has to come to be a mildly bad robot,

00:18:19.330 --> 00:18:21.330
and before that a not so bad robot.

00:18:21.330 --> 00:18:22.330
(Laughter)

00:18:22.330 --> 00:18:24.330
And we're just not going to let it go that way.

00:18:24.330 --> 00:18:25.330
(Laughter)

00:18:25.330 --> 00:18:31.330
So, I think I'm going to leave it at that: the robots are coming,

00:18:31.330 --> 00:18:34.330
we don't have too much to worry about, it's going to be a lot of fun,

00:18:34.330 --> 00:18:38.330
and I hope you all enjoy the journey over the next 50 years.

00:18:38.330 --> 00:18:40.330
(Applause)

