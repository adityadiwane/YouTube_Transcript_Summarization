WEBVTT
Kind: captions
Language: ru

00:00:00.000 --> 00:00:07.000
Переводчик: Valeriya Salter
Редактор: Anna Kotova

00:00:12.820 --> 00:00:16.900
Сегодня я поговорю
о технологиях и обществе.

00:00:18.860 --> 00:00:22.556
Министерство транспорта
подсчитало, что в прошлом году

00:00:22.580 --> 00:00:26.660
35 000 человек погибло
в автомобильных авариях в США.

00:00:27.860 --> 00:00:32.660
В мире 1,2 миллиона человек
погибает в авариях ежегодно.

00:00:33.580 --> 00:00:37.676
Если бы была инициатива
сократить эту цифру на 90%,

00:00:37.700 --> 00:00:38.900
вы бы её поддержали?

00:00:39.540 --> 00:00:40.836
Конечно же да.

00:00:40.860 --> 00:00:44.515
Технология беспилотных
автомобилей обещает это

00:00:44.540 --> 00:00:47.356
за счёт исключения
основной причины аварий —

00:00:47.380 --> 00:00:48.580
человеческой ошибки.

00:00:49.740 --> 00:00:55.156
Представьте, что вы
в беспилотной машине в 2030 году —

00:00:55.180 --> 00:00:58.636
смотрите этот винтажный ролик
TEDxCambridge.

00:00:58.660 --> 00:01:00.660
(Смех)

00:01:01.340 --> 00:01:02.556
Как вдруг

00:01:02.580 --> 00:01:05.860
происходит поломка
и машина не может затормозить.

00:01:07.180 --> 00:01:08.700
Если машина продолжит движение,

00:01:09.540 --> 00:01:13.660
она врежется в пешеходов,
переходящих дорогу.

00:01:14.900 --> 00:01:17.035
Но машина может повернуть

00:01:17.059 --> 00:01:18.916
и сбить прохожего,

00:01:18.940 --> 00:01:21.020
убив его и сохранив жизнь
других пешеходов.

00:01:21.860 --> 00:01:24.460
Что должна сделать машина,
и кто это решает?

00:01:25.340 --> 00:01:28.876
А что, если машина врежется в стену,

00:01:28.900 --> 00:01:32.196
убив тебя, пассажира,

00:01:32.220 --> 00:01:34.540
чтобы спасти этих пешеходов?

00:01:35.060 --> 00:01:38.140
Этот сценарий был вдохновлён
«Проблемой вагонетки» —

00:01:38.780 --> 00:01:42.556
она была придумана философами
несколько десятилетий назад

00:01:42.580 --> 00:01:44.110
с целью поразмышлять об этике.

00:01:45.940 --> 00:01:48.436
Важно, как мы решаем
этот этический вопрос.

00:01:48.460 --> 00:01:51.076
Мы можем, например,
вообще об этом не задумываться.

00:01:51.100 --> 00:01:54.476
Можно сказать, что сценарий
не реалистичен,

00:01:54.500 --> 00:01:56.820
маловероятен и глуп.

00:01:57.580 --> 00:02:00.316
Я думаю, что такая критика
упускает главное

00:02:00.340 --> 00:02:03.010
из-за того, что смотрит
на проблему слишком буквально.

00:02:03.740 --> 00:02:06.476
Конечно, авария будет выглядеть иначе,

00:02:06.500 --> 00:02:09.836
в авариях не бывает,
чтобы два или три исхода

00:02:09.860 --> 00:02:11.860
вели к чьей-то неминуемой смерти.

00:02:13.300 --> 00:02:15.876
Вместо этого машина будет
рассчитывать вероятность

00:02:15.900 --> 00:02:20.796
вреда для группы людей

00:02:20.820 --> 00:02:24.156
при различной смене движения,

00:02:24.180 --> 00:02:27.636
возможно, повышая риск
для пассажиров или других водителей

00:02:27.660 --> 00:02:29.196
по сравнению с пешеходами.

00:02:29.220 --> 00:02:31.380
Такой расчёт будет довольно сложным,

00:02:32.300 --> 00:02:34.820
но так или иначе будет
включать компромисс,

00:02:35.660 --> 00:02:38.540
а компромисс — это часто вопрос этики.

00:02:39.660 --> 00:02:42.396
Кто-то скажет: «Давайте
не будем об этом думать,

00:02:42.420 --> 00:02:47.060
пока технология не разовьётся
и не станет безопасной на 100%».

00:02:48.340 --> 00:02:52.020
Представим, что мы сможем
сократить число аварий на 90%,

00:02:52.900 --> 00:02:55.740
или даже 99%, в ближайшие десять лет,

00:02:56.740 --> 00:02:59.916
а на устранение последнего процента аварий

00:02:59.940 --> 00:03:03.060
потребуется ещё 50 лет исследований,

00:03:04.220 --> 00:03:06.020
нужно ли будет внедрить технологию?

00:03:06.540 --> 00:03:11.316
60 миллионов человек погибнут
в автомобильных авариях,

00:03:11.340 --> 00:03:13.100
если мы продолжим ждать.

00:03:14.580 --> 00:03:15.796
Я хочу сказать,

00:03:15.820 --> 00:03:19.436
что продолжать ждать — это такой же выбор,

00:03:19.460 --> 00:03:21.620
требующий компромисса.

00:03:23.380 --> 00:03:27.716
Люди в социальных сетях
обмениваются идеями,

00:03:27.740 --> 00:03:29.756
как избежать этого конфликта.

00:03:29.780 --> 00:03:32.996
Один человек предложил,
что машина должна повернуть так,

00:03:33.020 --> 00:03:35.156
чтобы проехать между пешеходами...

00:03:35.180 --> 00:03:36.196
(Смех)

00:03:36.220 --> 00:03:37.476
и прохожим.

00:03:37.500 --> 00:03:40.860
Конечно, если такой вариант есть,
машина так и поступит.

00:03:41.740 --> 00:03:44.580
Нас интересуют сценарии,
где такой исход невозможен.

00:03:45.100 --> 00:03:50.516
Мой любимый вариант дал один блогер,

00:03:50.540 --> 00:03:53.556
который предложил сделать
кнопку эвакуации в машине...

00:03:53.580 --> 00:03:54.796
(Смех)

00:03:54.820 --> 00:03:56.487
на которую нажимаешь перед аварией.

00:03:56.511 --> 00:03:58.191
(Смех)

00:03:59.660 --> 00:04:04.860
В общем, если принять,
что компромисс на дорогах неизбежен,

00:04:06.020 --> 00:04:07.900
как мы подойдём к его решению

00:04:09.140 --> 00:04:10.716
и кто будет принимать решение?

00:04:10.740 --> 00:04:13.876
Мы можем провести опрос,
чтобы узнать общественное мнение,

00:04:13.900 --> 00:04:15.356
в конце концов

00:04:15.380 --> 00:04:19.340
законы и регламенты являются отражением
общественных ценностей.

00:04:19.860 --> 00:04:21.100
Мы так и поступили.

00:04:21.700 --> 00:04:23.316
Мои коллеги,

00:04:23.340 --> 00:04:25.676
Жан-Франсуа Боннефон и Азим Шариф,

00:04:25.700 --> 00:04:27.316
и я провели опрос,

00:04:27.340 --> 00:04:30.195
в котором предлагались
такого рода сценарии

00:04:30.219 --> 00:04:33.996
и два варианта решения,
навеянные двумя философами,

00:04:34.020 --> 00:04:36.660
Иеремией Бентамом и Иммануилом Кантом.

00:04:37.420 --> 00:04:40.516
По Бентаму, машина должна
следовать утилитарной этике:

00:04:40.540 --> 00:04:43.956
действовать так,
чтобы минимизировать вред,

00:04:43.980 --> 00:04:46.796
даже если такое действие
приведёт к гибели прохожего

00:04:46.820 --> 00:04:49.260
или пассажира.

00:04:49.940 --> 00:04:54.916
По Иммануилу Канту, машина должна 
действовать по принципу долга:

00:04:54.940 --> 00:04:56.500
«Не убий».

00:04:57.300 --> 00:05:01.756
То есть нельзя совершать
действие, которое навредит человеку,

00:05:01.780 --> 00:05:04.236
машина должна продолжать движение,

00:05:04.260 --> 00:05:06.540
даже если в результате
погибнет больше людей.

00:05:07.460 --> 00:05:08.660
Что вы думаете?

00:05:09.180 --> 00:05:10.700
Бентам или Кант?

00:05:11.580 --> 00:05:12.836
Мы обнаружили следующее.

00:05:12.860 --> 00:05:15.090
Бо́льшая часть людей выбирает Бентама.

00:05:15.980 --> 00:05:19.756
Кажется, что люди более утилитарны

00:05:19.780 --> 00:05:21.196
и хотят минимизировать вред,

00:05:21.220 --> 00:05:22.796
и надо этому следовать.

00:05:22.820 --> 00:05:24.020
Проблема решена.

00:05:25.060 --> 00:05:26.540
Но есть небольшая загвоздка.

00:05:27.740 --> 00:05:31.476
Когда мы спросили людей,
купят ли они такой автомобиль,

00:05:31.500 --> 00:05:33.116
они ответили: «Ни за что».

00:05:33.140 --> 00:05:35.436
(Смех)

00:05:35.460 --> 00:05:39.356
Они хотят купить машину,
которая их защитит, что бы ни случилось,

00:05:39.380 --> 00:05:42.996
при этом пусть другие покупают
машины, минимизирующие вред.

00:05:43.020 --> 00:05:45.540
(Смех)

00:05:46.540 --> 00:05:48.396
Эта проблема не нова,

00:05:48.420 --> 00:05:50.310
она называется социальной дилеммой.

00:05:50.980 --> 00:05:52.796
И чтобы понять социальную дилемму,

00:05:52.820 --> 00:05:54.860
вернёмся ранее в историю,

00:05:55.820 --> 00:05:58.396
в XIX век.

00:05:58.420 --> 00:06:02.156
Английский экономист Уильям Фостер Ллойд
опубликовал памфлет,

00:06:02.180 --> 00:06:04.396
описывающий сценарий,

00:06:04.420 --> 00:06:06.076
в котором группа фермеров,

00:06:06.100 --> 00:06:07.436
английских фермеров,

00:06:07.460 --> 00:06:10.140
имеет общее поле,
на котором пасутся их овцы.

00:06:11.340 --> 00:06:13.916
Если у каждого определённое
количество овец,

00:06:13.940 --> 00:06:15.436
скажем, три,

00:06:15.460 --> 00:06:17.556
то поле успевает восстановиться,

00:06:17.580 --> 00:06:18.796
фермеры довольны,

00:06:18.820 --> 00:06:20.436
овцы счастливы,

00:06:20.460 --> 00:06:21.660
всё прекрасно.

00:06:22.260 --> 00:06:24.780
Если один фермер приведёт
ещё одну овцу,

00:06:25.620 --> 00:06:30.340
он заживёт чуть лучше, никому не навредив.

00:06:30.980 --> 00:06:34.620
Но если каждый фермер
примет такое разумное решение,

00:06:35.660 --> 00:06:38.380
то поле будет переполнено, оно истощится

00:06:39.180 --> 00:06:41.356
в убыток всем фермерам

00:06:41.380 --> 00:06:43.500
и, конечно, к огорчению овец.

00:06:44.540 --> 00:06:48.220
Такого рода проблема встречается часто,

00:06:48.900 --> 00:06:52.076
например черезмерный отлов рыбы

00:06:52.100 --> 00:06:56.660
или сокращение выброса углерода
для предотвращения изменения климата.

00:06:58.980 --> 00:07:01.900
Когда речь идёт о регламентах
для беспилотных автомобилей,

00:07:02.900 --> 00:07:07.236
общее поле — это, по сути,
общественная безопасность,

00:07:07.260 --> 00:07:08.500
это общее благо,

00:07:09.220 --> 00:07:11.196
где фермеры — это пассажиры

00:07:11.220 --> 00:07:14.820
или владельцы автомобилей,
которые в них ездят.

00:07:16.780 --> 00:07:19.396
Если каждый сделает выбор,
рациональный для себя,

00:07:19.420 --> 00:07:22.236
приоритизируя собственную безопасность,

00:07:22.260 --> 00:07:25.396
то в целом пострадает общее благо,

00:07:25.420 --> 00:07:27.620
то есть минимизация общего вреда.

00:07:30.140 --> 00:07:32.276
Это традиционно называется

00:07:32.300 --> 00:07:33.596
«трагедией общин».

00:07:33.620 --> 00:07:36.716
Но в случае с беспилотными
автомобилями, я думаю,

00:07:36.740 --> 00:07:39.596
проблема более хитрая,

00:07:39.620 --> 00:07:43.116
так как необязательно кто-то конкретный

00:07:43.140 --> 00:07:44.836
принимает эти решения.

00:07:44.860 --> 00:07:48.156
Производители машин могут
запрограммировать их так,

00:07:48.180 --> 00:07:50.700
чтобы максимизировать
безопасность своих клиентов,

00:07:51.900 --> 00:07:54.876
и эти машины могут самостоятельно усвоить,

00:07:54.900 --> 00:07:58.420
что это будет несколько
более рискованно для пешеходов.

00:07:59.340 --> 00:08:00.756
Используя метафору с овцами,

00:08:00.780 --> 00:08:04.396
получается, у нас теперь есть 
электронная овца со своим разумом.

00:08:04.420 --> 00:08:05.876
(Смех)

00:08:05.900 --> 00:08:08.980
И они могут пастись самостоятельно,
не спрашивая фермера.

00:08:10.460 --> 00:08:14.436
Можно это назвать «трагедией
алгоритмических общин»,

00:08:14.460 --> 00:08:16.820
уже с другими задачами.

00:08:22.340 --> 00:08:24.236
Традиционно такого рода

00:08:24.260 --> 00:08:27.596
социальные дилеммы решаются
с помощью законов,

00:08:27.620 --> 00:08:30.356
правительства или сообщества
вместе решают,

00:08:30.380 --> 00:08:34.116
что им нужно и как для этого

00:08:34.140 --> 00:08:36.796
ограничить поведение

00:08:36.820 --> 00:08:38.020
отдельной личности.

00:08:39.420 --> 00:08:42.036
Далее они контролируют
соблюдение этих правил,

00:08:42.060 --> 00:08:44.619
чтобы обеспечить сохранность
общественного блага.

00:08:45.260 --> 00:08:46.835
Почему бы тогда

00:08:46.859 --> 00:08:48.355
ни создать закон,

00:08:48.379 --> 00:08:51.276
требующий минимизации вреда в машинах?

00:08:51.300 --> 00:08:53.540
В конце концов, это то, чего хотят люди.

00:08:55.020 --> 00:08:56.436
И что более важно,

00:08:56.460 --> 00:08:59.556
я могу быть уверенным, что как личность,

00:08:59.580 --> 00:09:03.436
если я покупаю машину
и в редком случае она мной жертвует,

00:09:03.460 --> 00:09:05.116
я буду не единственным лохом,

00:09:05.140 --> 00:09:07.820
в то время как другие получают
абсолютную безопасность.

00:09:08.940 --> 00:09:12.276
В нашем опросе мы спросили
людей, поддержат ли они такой закон,

00:09:12.300 --> 00:09:13.500
и вот что мы обнаружили.

00:09:14.180 --> 00:09:17.940
Прежде всего, люди были
против такого закона,

00:09:19.100 --> 00:09:20.356
и во-вторых, они сказали:

00:09:20.380 --> 00:09:24.316
«Если машины будут обязаны
минимизировать вред,

00:09:24.340 --> 00:09:25.820
то я такую машину не куплю».

00:09:27.220 --> 00:09:28.596
Как ни странно,

00:09:28.620 --> 00:09:32.116
обязывая машины минимизировать вред,

00:09:32.140 --> 00:09:33.980
можно спровоцировать больший вред,

00:09:34.860 --> 00:09:38.516
так как люди не обязательно выберут
более безопасную технологию,

00:09:38.540 --> 00:09:40.620
даже если она надёжнее, чем водитель.

00:09:42.180 --> 00:09:45.596
У меня нет конечного ответа
на эту загадку,

00:09:45.620 --> 00:09:47.196
но я думаю, что для начала

00:09:47.220 --> 00:09:50.516
нужно, чтобы общество

00:09:50.540 --> 00:09:53.300
согласилось с тем, что является
приемлемым компромиссом

00:09:54.180 --> 00:09:57.660
и как его можно обеспечить.

00:09:58.240 --> 00:10:00.876
В качестве отправного пункта
мои замечательные студенты

00:10:00.900 --> 00:10:03.356
Эдмонд Авад и Сохан Дзуза

00:10:03.380 --> 00:10:05.180
сделали сайт «Moral Machine»,

00:10:06.020 --> 00:10:08.700
который генерирует различные сценарии,

00:10:09.900 --> 00:10:12.356
по сути, различные дилеммы,

00:10:12.380 --> 00:10:16.300
где вы выбираете, что машина должна
сделать в каждом случае.

00:10:16.860 --> 00:10:21.460
Мы варьируем возраст
и даже биологический вид жертв.

00:10:22.860 --> 00:10:26.556
Мы собрали более пяти миллионов решений

00:10:26.580 --> 00:10:28.780
от более миллиона человек со всего мира

00:10:30.220 --> 00:10:31.420
на этом сайте.

00:10:32.180 --> 00:10:34.596
Это помогает нам иметь
раннее представление

00:10:34.620 --> 00:10:37.236
о приемлемых компромиссах

00:10:37.260 --> 00:10:39.156
и о том, что важно людям

00:10:39.180 --> 00:10:40.620
разных культур.

00:10:42.060 --> 00:10:43.556
И ещё важнее,

00:10:43.580 --> 00:10:46.956
такое упражнение помогает людям осознать

00:10:46.980 --> 00:10:49.796
тяжесть такого рода решений

00:10:49.820 --> 00:10:53.620
и что тот, кто принимает закон,
столкнётся с невыносимым выбором.

00:10:55.180 --> 00:10:58.756
И возможно, это даст нам,
обществу, понять, какого рода компромиссы

00:10:58.780 --> 00:11:01.836
будут в итоге приняты в законодательстве.

00:11:01.860 --> 00:11:03.596
Я действительно был очень рад

00:11:03.620 --> 00:11:05.636
увидеть, что первый набор регламентов

00:11:05.660 --> 00:11:07.796
от Министерства транспорта,

00:11:07.820 --> 00:11:09.326
объявленный на прошлой неделе,

00:11:09.326 --> 00:11:15.796
включает 15 вопросов для всех
автопроизводителей,

00:11:15.820 --> 00:11:19.076
где вопрос под номером 14 — этический,

00:11:19.100 --> 00:11:20.820
какое решение принять.

00:11:23.620 --> 00:11:26.276
Мы также позволяем людям
поразмышлять над выбором,

00:11:26.300 --> 00:11:29.300
увидев сумму своих решений.

00:11:30.260 --> 00:11:31.916
Дам вам один пример.

00:11:31.940 --> 00:11:35.476
Я предупреждаю, это не самый обычный

00:11:35.500 --> 00:11:36.876
и типичный пользователь.

00:11:36.900 --> 00:11:40.516
Котика он спасал чаще всего,
а ребёнком чаще всего жертвовал.

00:11:40.540 --> 00:11:45.740
(Смех)

00:11:46.500 --> 00:11:48.396
Кто-то может с ним

00:11:48.420 --> 00:11:50.060
или с ней согласиться.

00:11:52.300 --> 00:11:58.436
Он также предпочитает пассажиров пешеходам

00:11:58.460 --> 00:12:00.556
в своём выборе

00:12:00.580 --> 00:12:03.396
и рад наказать тех, кто перебегает дорогу.

00:12:03.420 --> 00:12:06.460
(Смех)

00:12:09.140 --> 00:12:10.356
Подводя итог,

00:12:10.379 --> 00:12:13.795
мы начали с вопроса,
назовём его этической дилеммой,

00:12:13.820 --> 00:12:16.876
о том, что должна сделать машина
в определённом сценарии,

00:12:16.900 --> 00:12:18.330
повернуть или ехать дальше?

00:12:19.060 --> 00:12:21.796
Но затем мы поняли,
что проблема не в этом.

00:12:21.820 --> 00:12:26.356
Проблема в том, как достичь
согласия в обществе и как контролировать

00:12:26.380 --> 00:12:28.316
установленные компромиссы.

00:12:28.340 --> 00:12:29.596
Это социальная дилемма.

00:12:29.620 --> 00:12:34.636
В 1940-е Айзек Азимов написал
свои знаменитые законы роботехники,

00:12:34.660 --> 00:12:35.980
три закона.

00:12:37.060 --> 00:12:39.516
Робот не может причинить вред человеку,

00:12:39.540 --> 00:12:42.076
робот должен повиноваться
приказам человека,

00:12:42.100 --> 00:12:45.356
Робот должен заботиться
о своей безопасности.

00:12:45.380 --> 00:12:47.340
В этом порядке.

00:12:48.180 --> 00:12:50.316
Около 40 лет спустя,

00:12:50.340 --> 00:12:54.076
после всех историй,
проверяющих эти законы,

00:12:54.100 --> 00:12:57.796
Азимов ввёл нулевой закон,

00:12:57.820 --> 00:13:00.076
идущий прежде всех остальных:

00:13:00.100 --> 00:13:03.380
робот не должен навредить
человечеству в целом.

00:13:04.300 --> 00:13:08.676
Я не знаю, какое это имеет влияние
в контексте беспилотных автомобилей

00:13:08.700 --> 00:13:11.436
или любой конкретной ситуации,

00:13:11.460 --> 00:13:13.676
и не знаю, как мы можем это обеспечить,

00:13:13.700 --> 00:13:15.236
но думаю, что важно осознать,

00:13:15.260 --> 00:13:21.396
что законы для беспилотных автомобилей
не только проблема технологии,

00:13:21.420 --> 00:13:24.700
но и проблема взаимного сотрудничества.

00:13:25.620 --> 00:13:28.500
Я надеюсь, что мы начнём
задавать правильные вопросы.

00:13:29.020 --> 00:13:30.236
Спасибо.

00:13:30.260 --> 00:13:33.180
(Аплодисменты)

