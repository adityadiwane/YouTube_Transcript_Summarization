WEBVTT
Kind: captions
Language: iw

00:00:00.000 --> 00:00:07.000
מתרגם: Zeeva Livshitz
מבקר: hila scherba

00:00:12.820 --> 00:00:16.900
היום אני רוצה לדבר על טכנולוגיה וחברה.

00:00:18.860 --> 00:00:22.556
משרד התחבורה העריך שבשנה שעברה

00:00:22.580 --> 00:00:26.660
35,000 אנשים נהרגו בתאונות דרכים,
בארה"ב בלבד.

00:00:27.860 --> 00:00:32.660
ברחבי העולם, 1.2 מיליון אנשים
נהרגים מדי שנה בתאונות דרכים.

00:00:33.580 --> 00:00:37.676
אם היתה דרך למנוע 90% מהתאונות,

00:00:37.700 --> 00:00:38.900
הייתם תומכים בזה?

00:00:39.540 --> 00:00:40.836
כמובן שכן.

00:00:40.860 --> 00:00:44.515
זה מה שהטכנולוגיה של המכונית האוטונומית
מבטיחה להשיג

00:00:44.540 --> 00:00:47.356
על ידי מניעת המקור העיקרי לתאונות --

00:00:47.380 --> 00:00:48.580
טעות אנוש.

00:00:49.740 --> 00:00:55.156
עכשיו דמיינו את עצמכם
במכונית אוטונומית בשנת 2030.

00:00:55.180 --> 00:00:58.636
נשענים אחורה וצופים בוידאו משובח זה
של TEDxCambridge .

00:00:58.660 --> 00:01:00.660
(צחוק)

00:01:01.340 --> 00:01:02.556
לפתע פתאום,

00:01:02.580 --> 00:01:05.860
המכונית חווה תקלה מכנית
ואינה מסוגלת לעצור.

00:01:07.180 --> 00:01:08.700
אם המכונית ממשיכה,

00:01:09.540 --> 00:01:13.660
היא תתנגש בחבורת הולכי רגל 
שחוצים את הכביש,

00:01:14.900 --> 00:01:17.035
אבל המכונית יכולה לסטות,

00:01:17.059 --> 00:01:18.916
ולפגוע בעובר אורח,

00:01:18.940 --> 00:01:21.020
להרוג אותו, כדי להציל את הולכי הרגל.

00:01:21.860 --> 00:01:24.460
מה על המכונית לעשות, ומי צריך להחליט?

00:01:25.340 --> 00:01:28.876
מה אם במקום זה המכונית תסטה ותיתקע בקיר.

00:01:28.900 --> 00:01:32.196
תתרסק ותהרוג אותך, הנוסע,

00:01:32.220 --> 00:01:34.540
כדי להציל את הולכי הרגל?

00:01:35.060 --> 00:01:38.140
תרחיש זה הינו בהשראה של בעיית העגלה,

00:01:38.780 --> 00:01:42.556
שהומצאה על ידי פילוסופים לפני כמה עשורים

00:01:42.580 --> 00:01:43.820
תוך חשיבה על אתיקה.

00:01:45.940 --> 00:01:48.436
כעת, הדרך בה אנחנו חושבים
על בעיה זו היא חשובה.

00:01:48.460 --> 00:01:51.076
אנו יכולים למשל לא לחשוב על זה בכלל.

00:01:51.100 --> 00:01:54.476
אנו יכולים לומר שתרחיש זה אינו מציאותי,

00:01:54.500 --> 00:01:56.820
מאוד לא סביר, או פשוט טיפשי.

00:01:57.580 --> 00:02:00.316
אבל אני חושב שהביקורת הזאת 
מחמיצה את הנקודה

00:02:00.340 --> 00:02:02.500
כי היא בוחנת את התרחיש באופן מילולי מדי.

00:02:03.740 --> 00:02:06.476
כמובן ששום תאונה לא תיראה כך;

00:02:06.500 --> 00:02:09.836
לשום תאונה אין שתיים או שלוש אפשרויות

00:02:09.860 --> 00:02:11.860
שבה כולם מתים איכשהו.

00:02:13.300 --> 00:02:15.876
במקום זה, המכונית הולכת לחשב משהו

00:02:15.900 --> 00:02:20.796
כמו ההסתברות לפגוע בקבוצה מסוימת של אנשים,

00:02:20.820 --> 00:02:24.156
אם אתם סוטים בכיוון אחד ולא לכיוון אחר,

00:02:24.180 --> 00:02:27.636
אתם עלולים להגדיל מעט 
את הסיכון לנוסעים או לנהגים אחרים

00:02:27.660 --> 00:02:29.196
לעומת הולכי רגל.

00:02:29.220 --> 00:02:31.380
זה הולך להיות חישוב מורכב יותר,

00:02:32.300 --> 00:02:34.820
אבל זה עדיין יהיה כרוך בבחירת חלופות,

00:02:35.660 --> 00:02:38.540
ובחירת חלופות נזקקת לעתים קרובות לאתיקה

00:02:39.660 --> 00:02:42.396
אפשר לומר אם כן, 
"טוב, בואו לא נדאג בקשר לזה.

00:02:42.420 --> 00:02:47.060
בואו נחכה עד שהטכנולוגיה תהיה 
מוכנה לחלוטין ו 100% בטוחה."

00:02:48.340 --> 00:02:52.020
נניח שאנחנו יכולים באמת 
למנוע 90% מהתאונות האלו,

00:02:52.900 --> 00:02:55.740
או אפילו 99 אחוזים ב -10 השנים הבאות.

00:02:56.740 --> 00:02:59.916
מה אם מניעת האחוז האחרון של התאונות

00:02:59.940 --> 00:03:03.060
תדרוש עוד 50 שנים של מחקר?

00:03:04.220 --> 00:03:06.020
האם עלינו לא לאמץ את הטכנולוגיה?

00:03:06.540 --> 00:03:11.316
זה 60 מיליון אנשים שנהרגים בתאונות דרכים

00:03:11.340 --> 00:03:13.100
אם משמרים את הקצב הנוכחי

00:03:14.580 --> 00:03:15.796
אז הנקודה היא,

00:03:15.820 --> 00:03:19.436
שהמתנה לבטיחות מלאה היא גם בחירה,

00:03:19.460 --> 00:03:21.620
והיא גם כרוכה בבחירת חלופות.

00:03:23.380 --> 00:03:27.716
אנשים במדיה החברתית ברשת
מצאו כל מיני דרכים

00:03:27.740 --> 00:03:29.756
כיצד לא לחשוב על בעיה זו.

00:03:29.780 --> 00:03:32.996
אדם אחד הציע שעל המכונית רק לסטות איכשהו 

00:03:33.020 --> 00:03:35.156
אל בין הנוסעים --

00:03:35.180 --> 00:03:36.196
(צחוק)

00:03:36.220 --> 00:03:37.476
ועובר האורח.

00:03:37.500 --> 00:03:40.860
כמובן, שאם זה מה שהמכונית
יכולה לעשות, זה מה שעליה לעשות.

00:03:41.740 --> 00:03:44.580
אנו מעוניינים בתרחישים שבהם זה אינו אפשרי.

00:03:45.100 --> 00:03:50.516
והמועדפת האישית שלי היתה של בלוגר שהציע

00:03:50.540 --> 00:03:53.556
שיהיה כפתור חילוץ במכונית, שלוחצים עליו --

00:03:53.580 --> 00:03:54.796
(צחוק)

00:03:54.820 --> 00:03:56.487
ממש לפני שהמכונית משמידה את עצמה.

00:03:56.511 --> 00:03:58.191
(צחוק)

00:03:59.660 --> 00:04:04.860
אז אם אנחנו מודים שהמכוניות האלו 
יצטרכו לבחור חלופות על הכביש,

00:04:06.020 --> 00:04:07.900
איך אנו חושבים על אותן חלופות.

00:04:09.140 --> 00:04:10.716
וכיצד אנו מחליטים?

00:04:10.740 --> 00:04:13.876
טוב, אולי כדאי שערוך סקר
כדי לברר מה החברה רוצה,

00:04:13.900 --> 00:04:15.356
כי בסופו של דבר,

00:04:15.380 --> 00:04:19.340
התקנות והחוק הם השתקפות של ערכים חברתיים.

00:04:19.860 --> 00:04:21.100
אז זה מה שעשינו.

00:04:21.700 --> 00:04:23.316
עם שותפיי לעבודה,

00:04:23.340 --> 00:04:25.676
ז'אן פרנסואה בונפון ועזים שריף,

00:04:25.700 --> 00:04:27.316
הרצנו סקר

00:04:27.340 --> 00:04:30.195
שבו הצגנו בפני אנשים סוגים אלה של תרחישים.

00:04:30.219 --> 00:04:33.996
נתנו להם שתי אפשרויות 
בהשראת שני פילוסופים:

00:04:34.020 --> 00:04:36.660
ג'רמי בנת'ם ועמנואל קאנט.

00:04:37.420 --> 00:04:40.516
בנת'ם אומר שהמכונית צריכה 
לעקוב אחר האתיקה התועלתנית:

00:04:40.540 --> 00:04:43.956
צריך שהיא תבחר בפעולה שתפחית
עד למינימום את הנזק הכולל -

00:04:43.980 --> 00:04:46.796
גם אם פעולה זו תהרוג עובר אורח

00:04:46.820 --> 00:04:49.260
וגם אם פעולה זו תהרוג את הנוסע.

00:04:49.940 --> 00:04:54.916
עמנואל קאנט אומר שהמכונית
צריכה לעקוב אחר עקרונות מחייבים,

00:04:54.940 --> 00:04:56.500
כמו "לא תרצח".

00:04:57.300 --> 00:05:01.756
אז לא צריך לנקוט פעולה
שפוגעת במפורש בבני אדם,

00:05:01.780 --> 00:05:04.236
וצריך לאפשר למכונית לקחת את המסלול שלה

00:05:04.260 --> 00:05:06.220
גם אם זה יפגע ביותר אנשים.

00:05:07.460 --> 00:05:08.660
מה אתם חושבים?

00:05:09.180 --> 00:05:10.700
בנת'ם או קאנט?

00:05:11.580 --> 00:05:12.836
הנה מה שמצאנו.

00:05:12.860 --> 00:05:14.660
רוב האנשים צידדו עם בנת'ם.

00:05:15.980 --> 00:05:19.756
אז נראה שאנשים רוצים 
שהמכוניות יהיו תועלתניות,

00:05:19.780 --> 00:05:21.196
מזעור של הנזק הכולל,

00:05:21.220 --> 00:05:22.796
וזה מה שכולנו צריכים לעשות.

00:05:22.820 --> 00:05:24.020
הבעיה נפתרה.

00:05:25.060 --> 00:05:26.540
אבל יש מלכוד קטן.

00:05:27.740 --> 00:05:31.476
כששאלנו אנשים אם הם ירכשו מכוניות כאלה,

00:05:31.500 --> 00:05:33.116
הם אמרו, "בהחלט לא".

00:05:33.140 --> 00:05:35.436
{צחוק}

00:05:35.460 --> 00:05:39.356
הם רוצים לקנות מכוניות
שיגנו עליהם בכל מחיר,

00:05:39.380 --> 00:05:42.996
אבל הם רוצים שכל האחרים
יקנו מכוניות שימזערו את הנזק.

00:05:43.020 --> 00:05:45.540
(צחוק)

00:05:46.540 --> 00:05:48.396
ראינו את הבעיה הזו בעבר.

00:05:48.420 --> 00:05:49.980
היא נקראת דילמה חברתית.

00:05:50.980 --> 00:05:52.568
וכדי להבין את הדילמה החברתית,

00:05:52.568 --> 00:05:54.860
אנחנו צריכים ללכת קצת חזרה להיסטוריה

00:05:55.820 --> 00:05:58.396
במאה ה 19,

00:05:58.420 --> 00:06:02.156
הכלכלן האנגלי ויליאם פורסטר לויד 
פרסם חוברת

00:06:02.180 --> 00:06:04.396
שמתארת את התרחיש הבא.

00:06:04.420 --> 00:06:06.076
יש לכם קבוצה של חקלאים -

00:06:06.100 --> 00:06:07.436
חקלאים אנגלים --

00:06:07.460 --> 00:06:10.140
שחולקים קרקע משותפת 
שמשמשת מרעה לכבשים שלהם.

00:06:11.340 --> 00:06:13.916
אם כל חקלאי מביא מספר מסוים של כבשים --

00:06:13.940 --> 00:06:15.436
שלוש כבשים למשל -

00:06:15.460 --> 00:06:17.556
הקרקע תתחדש,

00:06:17.580 --> 00:06:18.796
החקלאים יהיו מרוצים,

00:06:18.820 --> 00:06:20.436
הכבשים תהיינה מרוצות,

00:06:20.460 --> 00:06:21.660
הכל טוב.

00:06:22.260 --> 00:06:24.780
עכשיו, אם אחד החקלאים מביא כבשה נוספת,

00:06:25.620 --> 00:06:30.340
אותו איכר ישפר מעט את מצבו, 
ואף אחד אחר לא ייפגע.

00:06:30.980 --> 00:06:34.620
אבל אם כל חקלאי יקבל החלטה 
אינדיווידואלית רציונלית זו,

00:06:35.660 --> 00:06:38.380
האדמה תהיה עמוסה ותידלדל

00:06:39.180 --> 00:06:41.356
מה שיפגע בכל החקלאים,

00:06:41.380 --> 00:06:43.500
וכמובן, יפגע בכבשים.

00:06:44.540 --> 00:06:48.220
אנו רואים בעיה זו במקומות רבים:

00:06:48.900 --> 00:06:52.076
בקושי של ניהול דיג בהיקף מוגזם,

00:06:52.100 --> 00:06:56.660
או בהפחתת פליטת הפחמן
כדי להקל על שינויי האקלים.

00:06:58.980 --> 00:07:01.900
כשמדובר ברגולציה של מכוניות אוטונומיות,

00:07:02.900 --> 00:07:07.236
הקרקע המשותפת עכשיו
הוא ביסודו של דבר בטוחה לציבור -

00:07:07.260 --> 00:07:08.500
זה הטוב המשותף -

00:07:09.220 --> 00:07:11.196
והחקלאים הם הנוסעים

00:07:11.220 --> 00:07:14.820
או בעלי המכוניות שבוחרים
לנסוע במכוניות אלה.

00:07:16.780 --> 00:07:19.396
ועל ידי עשיית בחירה אישית הגיונית

00:07:19.420 --> 00:07:22.236
של מתן עדיפות לביטחונם,

00:07:22.260 --> 00:07:25.396
הם עשויים לגרום יחד להפחתת הטוב המשותף,

00:07:25.420 --> 00:07:27.620
שהוא מזעור הנזק המוחלט.

00:07:30.140 --> 00:07:32.276
זה נקרא הטרגדיה של נחלת הכלל,

00:07:32.300 --> 00:07:33.596
באופן מסורתי,

00:07:33.620 --> 00:07:36.716
אבל אני חושב שבמקרה של מכוניות אוטונומיות,

00:07:36.740 --> 00:07:39.596
הבעיה עשויה להיות קצת יותר ערמומית

00:07:39.620 --> 00:07:43.116
כי אין כאן בהכרח אדם יחיד

00:07:43.140 --> 00:07:44.836
שמקבל החלטות אלה.

00:07:44.860 --> 00:07:48.156
כך שיצרני הרכב יכולים פשוט לתכנת מכוניות

00:07:48.180 --> 00:07:50.700
שימקסמו את הבטיחות עבור הלקוחות שלהם,

00:07:51.900 --> 00:07:54.876
והמכוניות האלה עשויות ללמוד
באופן אוטומטי בעצמן

00:07:54.900 --> 00:07:58.420
שעשייה כזו דורשת מעט
הגדלת סיכון להולכי רגל.

00:07:59.340 --> 00:08:00.756
אז אם נשתמש במטפורת הכבשים,

00:08:00.780 --> 00:08:04.396
זה כאילו שיש לנו עכשיו כבשים חשמליות
שיש להן חשיבה משל עצמן.

00:08:04.420 --> 00:08:05.876
(צחוק)

00:08:05.900 --> 00:08:08.980
והן יכולות ללכת ולרעות
גם אם האיכר אינו יודע על כך.

00:08:10.460 --> 00:08:14.436
אז זה מה שאנחנו יכולים לקרוא לו
הטרגדיה של נחלת הכלל האלגוריתמית,

00:08:14.460 --> 00:08:17.000
והיא מציעה סוגים חדשים של אתגרים.

00:08:22.340 --> 00:08:24.236
בדרך כלל, באופן מסורתי,

00:08:24.260 --> 00:08:27.596
אנו פותרים סוגים אלה של דילמות חברתיות
באמצעות רגולציה,

00:08:27.620 --> 00:08:30.356
אז או שהממשלות או שהקהילות נפגשות,

00:08:30.380 --> 00:08:34.116
ומחליטות ביחד
באיזו סוג של תוצאה הם מעונינים

00:08:34.140 --> 00:08:36.796
ואיזה סוג של אילוצים על התנהגות אישית

00:08:36.820 --> 00:08:38.020
הם צריכים ליישם.

00:08:39.420 --> 00:08:42.036
ואז באמצעות ניטור ואכיפה,

00:08:42.060 --> 00:08:44.619
הם יכולים לוודא שטובת הכלל נשמרת.

00:08:45.260 --> 00:08:46.835
אז למה אנחנו איננו פשוט,

00:08:46.859 --> 00:08:48.355
כרגולטורים,

00:08:48.379 --> 00:08:51.276
דורשים מכל המכוניות למזער נזק?

00:08:51.300 --> 00:08:53.540
אחרי הכל, זה מה שאנשים אומרים שהם רוצים.

00:08:55.020 --> 00:08:56.436
וחשוב יותר,

00:08:56.460 --> 00:08:59.556
אני יכול להיות בטוח כי כאדם יחיד,

00:08:59.580 --> 00:09:03.436
אם אני קונה מכונית שאולי
תקריב אותי במקרה נדיר מאוד,

00:09:03.460 --> 00:09:05.116
אני לא הפראייר היחיד שעושה את זה

00:09:05.140 --> 00:09:07.820
בעוד כל האחרים נהנים מהגנה בלתי מותנית.

00:09:08.940 --> 00:09:12.276
בסקר שלנו, שאלנו אנשים אם הם 
יתמכו ברגולציה

00:09:12.300 --> 00:09:13.500
והנה מה שמצאנו.

00:09:14.180 --> 00:09:17.940
ראשית, אנשים אמרו לא לרגולציה;

00:09:19.100 --> 00:09:20.356
ושנית, הם אמרו,

00:09:20.380 --> 00:09:24.316
"אם תהיה תקנה שהמכוניות
יעשו זאת, וכדי למזער את הנזק הכולל,

00:09:24.340 --> 00:09:25.820
אני לא אקנה את המכוניות האלו".

00:09:27.220 --> 00:09:28.596
אז באופן אירוני,

00:09:28.620 --> 00:09:32.116
על ידי הסדרת המכוניות כדי למזער נזק,

00:09:32.140 --> 00:09:33.980
אנו עשויים למעשה לסיים עם יותר נזק

00:09:34.860 --> 00:09:38.516
כי אנשים עשויים לא לבחור 
בטכנולוגיה הבטוחה יותר

00:09:38.540 --> 00:09:40.620
גם אם זה הרבה יותר בטוח מנהגים אנושיים

00:09:42.180 --> 00:09:45.596
אין לי את התשובה הסופית לחידה זו,

00:09:45.620 --> 00:09:47.196
אבל לדעתי, כנקודת מוצא,

00:09:47.220 --> 00:09:50.516
אנחנו צריכים שהחברה תתאחד

00:09:50.540 --> 00:09:53.300
כדי להחליט מהן החלופות 
שאיתן אנו מרגישים בנוח

00:09:54.180 --> 00:09:57.660
ולמצוא דרכים שבעזרתן נוכל לאכוף אותן.

00:09:58.340 --> 00:10:00.876
כנקודת מוצא, הסטודנטים המבריקים שלי,

00:10:00.900 --> 00:10:03.356
אדמונד עוואד וסוהאן דסוזה,

00:10:03.380 --> 00:10:05.180
בנו את אתר האינטרנט "מכונת המוסר",

00:10:06.020 --> 00:10:08.700
שמייצר תרחישים אקראיים -

00:10:09.900 --> 00:10:12.356
בעצם מקבץ של דילמות אקראיות ברצף

00:10:12.380 --> 00:10:16.300
שבהן תצטרכו לבחור מה המכונית 
צריכה לעשות בתרחיש נתון.

00:10:16.860 --> 00:10:21.460
ואנחנו משנים את הגילאים ואפילו את המין
של הקורבנות השונים.

00:10:22.860 --> 00:10:26.556
עד כה אספנו מעל 5 מיליון החלטות

00:10:26.580 --> 00:10:28.780
של יותר ממיליון אנשים ברחבי העולם

00:10:30.220 --> 00:10:31.420
מהאתר.

00:10:32.180 --> 00:10:34.596
וזה עוזר לנו ליצור תמונה מוקדמת

00:10:34.620 --> 00:10:37.236
של החלופות שלאנשים נוח איתן

00:10:37.260 --> 00:10:39.156
ומה חשוב להם -

00:10:39.180 --> 00:10:40.620
אפילו באופן שחוצה תרבויות.

00:10:42.060 --> 00:10:43.556
אבל חשוב יותר,

00:10:43.580 --> 00:10:46.956
לעשות את התרגיל הזה
עוזר לאנשים לזהות

00:10:46.980 --> 00:10:49.796
את הקושי שבקבלת החלטות אלה

00:10:49.820 --> 00:10:53.620
וכי על הרגולטורים מוטלות 
בחירות בלתי אפשריות.

00:10:55.180 --> 00:10:58.756
ואולי זה יעזור לנו כחברה
להבין את סוגי החלופות

00:10:58.780 --> 00:11:01.836
שייושמו בסופו של דבר.

00:11:01.860 --> 00:11:03.596
ואכן, שמחתי מאוד לשמוע

00:11:03.620 --> 00:11:05.636
שמערכת התקנות הראשונה

00:11:05.660 --> 00:11:07.796
שהגיעה ממשרד התחבורה --

00:11:07.820 --> 00:11:09.196
הודיעה בשבוע שעבר --

00:11:09.220 --> 00:11:15.796
שהיא כללה רשימה של 15
נקודות שעל כל יצרן מכוניות לספק,

00:11:15.820 --> 00:11:19.076
ומספר 14 היה שיקול מוסרי -

00:11:19.100 --> 00:11:20.820
איך אתם מתכוונים להתמודד עם זה.

00:11:23.620 --> 00:11:26.276
אנחנו גם משקפים לאנשים את ההחלטות שלהם

00:11:26.300 --> 00:11:29.300
על ידי הצגת סיכומים של מה שהם בחרו.

00:11:30.260 --> 00:11:31.916
אתן לכם דוגמה אחת --

00:11:31.940 --> 00:11:35.476
אני רק עומד להזהיר אתכם
שזו לא דוגמה טיפוסית שלכם,

00:11:35.500 --> 00:11:36.876
המשתמש הטיפוסי שלכם.

00:11:36.900 --> 00:11:40.516
אלו הדמות שהכי נשמרה וזו שהכי הוקרבה
אצל אדם מסוים.

00:11:40.540 --> 00:11:45.740
(צחוק)

00:11:46.500 --> 00:11:48.396
אחדים מכם עשויים להסכים איתו,

00:11:48.420 --> 00:11:50.060
או איתה, איננו יודעים.

00:11:52.300 --> 00:11:58.436
אבל נראה שאדם זה מעדיף במקצת
את הנוסעים על פני הולכי רגל

00:11:58.460 --> 00:12:00.556
בבחירות שלהם.

00:12:00.580 --> 00:12:03.396
והוא שמח מאוד להעניש
את אלה שחוצים כביש ברשלנות.

00:12:03.420 --> 00:12:06.460
(צחוק)

00:12:09.083 --> 00:12:10.072
אז בואו ונסכם.

00:12:10.072 --> 00:12:13.795
התחלנו עם השאלה - שנקרא לה הדילמה האתית -

00:12:13.820 --> 00:12:16.876
של מה שהמכונית צריכה לעשות בתרחיש ספציפי:

00:12:16.900 --> 00:12:18.100
לסטות או להישאר?

00:12:19.060 --> 00:12:21.796
אבל אז הבנו שהבעיה היתה שונה.

00:12:21.820 --> 00:12:26.356
זו היתה הבעיה של איך לגרום לחברה
להסכים ולאכוף

00:12:26.380 --> 00:12:28.316
את החלופות שהם מרגישים איתן נוח.

00:12:28.340 --> 00:12:29.596
זו דילמה חברתית.

00:12:29.620 --> 00:12:34.636
בשנות הארבעים, איזק אסימוב כתב את 
החוקים המפורסמים של הרובוטיקה -

00:12:34.660 --> 00:12:35.980
את שלושת חוקי הרובוטיקה.

00:12:37.060 --> 00:12:39.516
רובוט לא רשאי לפגוע בבני אדם,

00:12:39.540 --> 00:12:42.076
אסור לרובוט לא להישמע לאדם,

00:12:42.100 --> 00:12:45.356
ורובוט לא יכול להרשות לעצמו 
לבוא כדי לפגוע -

00:12:45.380 --> 00:12:47.340
על פי סדר חשיבות זה.

00:12:48.180 --> 00:12:50.316
אבל אחרי 40 שנה בערך

00:12:50.340 --> 00:12:54.076
ואחרי כל כך הרבה סיפורים שדוחקים
חוקים אלה עד קצה גבול התחום המותר,

00:12:54.100 --> 00:12:57.796
אסימוב הציג את חוק האפס

00:12:57.820 --> 00:13:00.076
שנותן קדימות מעל לכל,

00:13:00.100 --> 00:13:03.380
לכך שהרובוט לא יפגע באנושות בכללותה.

00:13:04.300 --> 00:13:08.676
אינני יודע מה זה אומר בהקשר
למכוניות אוטונומיות

00:13:08.700 --> 00:13:11.436
או למצב ספציפי כלשהו,

00:13:11.460 --> 00:13:13.676
ואינני יודע איך אנחנו יכולים ליישם את זה,

00:13:13.700 --> 00:13:15.236
אבל אני חושב שעל ידי הכרה בכך

00:13:15.260 --> 00:13:21.396
שהרגולציה של מכוניות אוטונומיות
אינה רק בעיה טכנולוגית

00:13:21.420 --> 00:13:24.700
אלא גם בעיה של שיתוף פעולה חברתי,

00:13:25.620 --> 00:13:28.500
אני מקווה שנוכל לפחות להתחיל
לשאול את השאלות הנכונות.

00:13:29.020 --> 00:13:30.236
תודה לכם.

00:13:30.260 --> 00:13:33.180
(מחיאות כפיים)

