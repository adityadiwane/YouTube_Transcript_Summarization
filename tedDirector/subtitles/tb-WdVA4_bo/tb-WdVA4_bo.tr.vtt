WEBVTT
Kind: captions
Language: tr

00:00:00.000 --> 00:00:07.000
Çeviri: Esra Çakmak
Gözden geçirme: Ramazan Şen

00:00:12.820 --> 00:00:16.900
Bugün, teknoloji ve toplum
hakkında konuşacağım.

00:00:18.860 --> 00:00:22.556
Ulaştırma Bakanlığı verilere göre 
geçen sene yalnızca ABD'de

00:00:22.556 --> 00:00:26.636
35.000 insanın trafik kazalarında 
yaşamını yitirdiğini bildirdi.

00:00:27.860 --> 00:00:32.660
Dünya çapında, her yıl 1.2 milyon insan
trafik kazalarında hayatını kaybediyor.

00:00:33.580 --> 00:00:37.676
Bu kazaların yüzde 90'ını ortadan 
kaldırabileceğimiz bir yöntem olsaydı,

00:00:37.676 --> 00:00:38.876
bunu destekler miydiniz?

00:00:39.564 --> 00:00:40.860
Elbette desteklerdiniz.

00:00:40.860 --> 00:00:44.515
Sürücüsüz araba teknolojisi
işte bunu vadediyor;

00:00:44.515 --> 00:00:47.331
kazaların ana sebebini,
yani insan hatasını

00:00:47.331 --> 00:00:48.531
ortadan kaldırmayı.

00:00:49.740 --> 00:00:55.156
Şimdi kendinizi 2030 yılında
sürücüsüz bir araba içerisinde hayal edin,

00:00:55.156 --> 00:00:58.612
arkada oturuyor ve bu nostaljik
TEDxCambridge videosunu izliyorsunuz.

00:00:58.612 --> 00:01:00.612
(Gülüşmeler)

00:01:01.340 --> 00:01:02.556
Aniden,

00:01:02.556 --> 00:01:05.836
arabada mekanik bir arıza oluyor
ve duramıyor.

00:01:07.180 --> 00:01:08.700
Araba sürüşe devam ederse,

00:01:09.540 --> 00:01:13.660
yoldan geçen bir grup insana çarpacak,

00:01:14.900 --> 00:01:17.035
fakat araç direksiyon kırabilir,

00:01:17.035 --> 00:01:18.892
görgü tanıklarından birine çarpar

00:01:18.892 --> 00:01:20.972
ve böylece daha çok yaya kurtulmuş olur.

00:01:21.860 --> 00:01:24.460
Araba ne yapmalı
ve buna kim karar vermeli?

00:01:25.340 --> 00:01:28.876
Farz edelim ki,
araba direksiyonu bir duvara kırıyor,

00:01:28.876 --> 00:01:31.930
diğer yayaları kurtarmak adına,

00:01:31.930 --> 00:01:34.540
duvara çarparak arabadaki 
yolcuyu yani sizi öldürüyor.

00:01:35.060 --> 00:01:38.140
Bu senaryo,
bundan 30-40 yıl önce

00:01:38.780 --> 00:01:42.556
ahlak kuramını irdelemek adına
filozoflarca yaratılan tren ikileminden

00:01:42.556 --> 00:01:43.796
esinlenerek ortaya konmuş.

00:01:45.964 --> 00:01:48.460
Bu probleme bakış açımız önem taşıyor.

00:01:48.460 --> 00:01:51.076
Mesela, hiç üzerine kafa yormayabiliriz.

00:01:51.076 --> 00:01:54.452
Bu senaryonun gerçekçi olmadığını,
son derece olağan dışı olduğunu

00:01:54.452 --> 00:01:56.772
veya aptalca olduğunu düşünebiliriz.

00:01:57.580 --> 00:02:00.316
Fakat bu eleştirinin,
işin özünü kaçırdığını düşünüyorum,

00:02:00.316 --> 00:02:02.476
çünkü senaryoyu
fazla direkt yorumluyorlar.

00:02:03.740 --> 00:02:06.476
Elbette hiçbir kaza bu şekilde olmaz,

00:02:06.476 --> 00:02:09.812
herkesin o veya bu şekilde öldüğü

00:02:09.812 --> 00:02:11.812
iki üç olasılığa indirgenemez.

00:02:13.300 --> 00:02:15.876
Buna karşın araba,

00:02:15.876 --> 00:02:20.772
vuracağı kişi sayısı gibi
kendi içinde hesaplamalara gidecek,

00:02:20.772 --> 00:02:24.108
direksiyonu çevireceği
yönleri kıyaslayarak

00:02:24.108 --> 00:02:27.564
yolcu veya diğer sürücülere 
vereceği hasarı, yayalara oranla

00:02:27.564 --> 00:02:29.100
nispeten arttırabilir.

00:02:29.220 --> 00:02:31.380
Çok daha karmaşık bir hesaplama olacaktır,

00:02:32.300 --> 00:02:34.820
ama yine de risk oranını
hesaba katmış olacaktır

00:02:35.660 --> 00:02:38.540
ve risk oranları,
sıklıkla ahlak kuramı içindedir.

00:02:39.660 --> 00:02:42.736
Şu şekilde düşünebiliriz,
''Peki, o hâlde daha fazla uzatmayalım.

00:02:42.736 --> 00:02:47.060
Teknolojinin tamamen hazır olmasını,
%100 güvenli hâle gelmesini bekleyelim.''

00:02:48.340 --> 00:02:52.020
Gelecek 10 yılda, bu kazaların %90'ını

00:02:52.900 --> 00:02:55.740
ve hatta %99'unu
önleyebildiğimizi farz edelim.

00:02:56.740 --> 00:02:59.870
Peki ya diğer %10'luk
kaza dilimini önleyebilmek,

00:02:59.870 --> 00:03:03.060
50 yıllık bir çalışma daha
gerektiriyor olsaydı?

00:03:04.220 --> 00:03:06.020
Teknolojiyi uyarlamamız gerekmez mi?

00:03:06.540 --> 00:03:11.316
Mevcut oranı korursak,
bu 60 milyon insanın araba kazalarında

00:03:11.316 --> 00:03:13.076
canlarını yitirdiği anlamına geliyor.

00:03:14.580 --> 00:03:15.796
Demek istediğim şu ki,

00:03:15.796 --> 00:03:19.412
tamamen güvenliği sağlamak için
beklemek bir seçenek,

00:03:19.412 --> 00:03:21.572
fakat aynı zamanda,
kendi içinde risk taşıyor.

00:03:23.380 --> 00:03:27.716
Sosyal medyada insanlar,
bu problemi düşünmemek için

00:03:27.716 --> 00:03:29.732
türlü fikirler öne sürüyorlar.

00:03:29.780 --> 00:03:32.996
Bir kişi arabanın bir şekilde,
yaya ile tanık arasından

00:03:32.996 --> 00:03:35.132
geçmesi fikrini

00:03:35.132 --> 00:03:36.148
(Gülüşmeler)

00:03:36.148 --> 00:03:37.404
öne sürdü.

00:03:37.500 --> 00:03:40.860
Elbette araba eğer bunu yapabiliyorsa,
bunu yapması gerekir.

00:03:41.740 --> 00:03:44.670
Böyle bir çözümün mümkün
olmadığı senaryolardan bahsediyoruz.

00:03:45.100 --> 00:03:50.516
Bir blog yazarınca öne sürülen,
şahsen benim en sevdiğim öneride,

00:03:50.516 --> 00:03:53.532
arabada basılabilen fırlatma tuşu ile

00:03:53.532 --> 00:03:54.748
(Gülüşmeler)

00:03:54.748 --> 00:03:57.145
arabanın devamında
kendini imha etmesi çözümüydü.

00:03:57.145 --> 00:03:58.119
(Gülüşmeler)

00:03:59.660 --> 00:04:04.860
Arabaların trafikte risk oranı tahlili
yapacağını kabul edecek olursak,

00:04:06.020 --> 00:04:08.640
bu risk oranı tahlilleri hakkında
nasıl düşüneceğiz

00:04:09.164 --> 00:04:10.740
ve nasıl karar vereceğiz?

00:04:10.740 --> 00:04:14.046
Belki de toplumun ne istediğini
anlamak için bir anket düzenlemeliyiz,

00:04:14.046 --> 00:04:15.356
çünkü nihayetinde,

00:04:15.380 --> 00:04:19.340
düzenlemeler ve yasalar
toplumsal değerlerin bir yansımasıdır.

00:04:19.860 --> 00:04:21.100
Böyle yaptık.

00:04:21.724 --> 00:04:23.340
Ekip arkadaşlarım ile birlikte,

00:04:23.340 --> 00:04:25.676
Jean-François Bonnefon ve Azim Shariff,

00:04:25.676 --> 00:04:27.292
bir anket yaptık

00:04:27.340 --> 00:04:30.195
ve bu ankette insanlara
bu tarz senaryolar sunduk.

00:04:30.219 --> 00:04:33.996
Onlara, iki filozoftan ilham aldığımız
iki seçenek sunduk:

00:04:34.020 --> 00:04:36.660
Jeremy Bentham ve Immanuel Kant.

00:04:37.444 --> 00:04:40.540
Bentham'a göre, araba faydacı
bir etiğe göre hareket etmeli:

00:04:40.540 --> 00:04:43.956
büyük çaptaki hasarı en aza
indirgeyecek şekilde hareket etmeli,

00:04:43.956 --> 00:04:46.772
bu eylem bir tanığı öldürecek olsa da,

00:04:46.772 --> 00:04:49.212
bu eylem bir yolcuyu öldürecek olsa da.

00:04:49.940 --> 00:04:54.916
Immanuel Kant'a göre araba,
vazifesinin icabına göre hareket etmeli,

00:04:54.916 --> 00:04:56.476
''Öldürmeyeceksin'' gibi.

00:04:57.300 --> 00:05:01.756
Bariz bir şekilde, bir insana
zarar verecek şekilde hareket etmemeli

00:05:01.756 --> 00:05:04.212
ve araba kendi hâline bırakılmalı,

00:05:04.212 --> 00:05:06.242
daha fazla insana zarar verecek olsa bile.

00:05:07.460 --> 00:05:08.660
Siz ne düşünüyorsunuz?

00:05:09.180 --> 00:05:10.700
Bentham mı, Kant mı?

00:05:11.580 --> 00:05:12.896
Bizim bulgumuzu paylaşayım.

00:05:12.896 --> 00:05:14.660
Çoğu insan Bentham'a hak verdi.

00:05:15.980 --> 00:05:19.610
Öyle ki, insanlar arabaların faydacı
olmalarını istiyorlar gibi görünüyor,

00:05:19.610 --> 00:05:21.356
toplam zararı en aza indirgemesini,

00:05:21.356 --> 00:05:23.436
ki bu hepimizin
yapması gereken şey aslında.

00:05:23.436 --> 00:05:24.240
Sorun çözüldü.

00:05:25.060 --> 00:05:26.940
Dikkat etmemiz gereken bir nokta var.

00:05:27.740 --> 00:05:31.476
İnsanlara bu tarz arabaları satın alma
durumlarını sorduğumuzda,

00:05:31.500 --> 00:05:33.116
''Kesinlikle hayır.'' dediler.

00:05:33.116 --> 00:05:35.412
(Gülüşmeler)

00:05:35.412 --> 00:05:39.308
Kendilerini her an koruyan
arabalar satın almak isterken,

00:05:39.380 --> 00:05:43.806
diğer herkesin zararı en aza indirgeyen
arabalardan almalarını istiyorlar.

00:05:43.806 --> 00:05:45.540
(Gülüşmeler)

00:05:46.540 --> 00:05:48.396
Bu problemle daha önce karşılaşmıştık.

00:05:48.420 --> 00:05:50.050
Buna ''toplumsal ikilem'' deniyor.

00:05:50.980 --> 00:05:52.796
Toplumsal ikilemi anlayabilmek için,

00:05:52.796 --> 00:05:54.556
biraz eskilere gitmemiz gerekir.

00:05:56.850 --> 00:05:58.396
1800'lerde,

00:05:58.420 --> 00:06:01.880
İngiliz iktisatçı William Forster Lloyd,

00:06:01.880 --> 00:06:04.396
şu senaryoyu aktaran bir broşür yayınladı:

00:06:04.444 --> 00:06:06.100
Bir grup çiftçi var,

00:06:06.100 --> 00:06:07.436
İngiliz çiftçiler,

00:06:07.436 --> 00:06:10.176
koyunlarını otlatmak için
umumi bir araziyi paylaşıyorlar.

00:06:11.340 --> 00:06:13.890
Her bir çiftçi,
belirli sayıda koyun getirirse

00:06:13.890 --> 00:06:15.506
- 3 koyun diyelim -

00:06:15.506 --> 00:06:17.540
toprak yenilenmiş olacak,

00:06:17.540 --> 00:06:18.780
çiftçiler mutlu,

00:06:18.780 --> 00:06:20.390
koyunlar mutlu,

00:06:20.390 --> 00:06:21.660
her şey iyi olacak.

00:06:22.260 --> 00:06:24.780
Fakat bir çiftçi,
fazladan bir koyun daha getirirse,

00:06:25.620 --> 00:06:30.340
o çiftçinin işleri için daha iyi olacak
ve bu kimseye zarar vermeyecek.

00:06:30.980 --> 00:06:34.620
Ama her bir çiftçi, bireysel olarak
bu mantıklı kararı verecek olursa,

00:06:35.660 --> 00:06:38.380
alan istilaya uğramış olacak
ve yeşillik tükenecek,

00:06:39.180 --> 00:06:41.356
bu da her çiftçinin zararına olacak

00:06:41.356 --> 00:06:43.476
ve tabii ki koyunların da zararına olacak.

00:06:44.540 --> 00:06:48.220
Bu sorun ile birçok kez karşılaşıyoruz:

00:06:48.924 --> 00:06:52.100
aşırı balık avlanması hususunda,

00:06:52.100 --> 00:06:56.660
ya da iklim değişikliğini azaltmak adına
karbon salınımını azaltmada.

00:06:58.980 --> 00:07:01.900
Sürücüsüz araba hukukuna baktığımızda,

00:07:02.900 --> 00:07:07.236
umumi alan toplum güvenliği oluyor

00:07:07.236 --> 00:07:08.476
- yani kamu yararı -

00:07:09.220 --> 00:07:11.196
ve çiftçiler yolcular oluyor

00:07:11.196 --> 00:07:14.796
veya o arabaları kullanmayı
tercih eden araç sahipleri oluyor.

00:07:16.780 --> 00:07:19.396
Ve kendi güvenliklerine öncelik tanıyan,

00:07:19.396 --> 00:07:22.212
bireysel mantıklı seçimlerini yaparak

00:07:22.260 --> 00:07:25.396
hep birlikte genelin iyiliğini azaltarak

00:07:25.396 --> 00:07:28.276
toplam zarar riskini aza indirgemeyi
düşürüyor olabilirler.

00:07:30.140 --> 00:07:32.616
Buna bilindiği üzere
ortak malların trajedisi

00:07:32.616 --> 00:07:33.596
deniyor,

00:07:33.620 --> 00:07:36.716
fakat bana kalırsa,
sürücüsüz araba hususunda

00:07:36.716 --> 00:07:39.572
sorun biraz daha derin olabilir,

00:07:39.572 --> 00:07:43.068
çünkü temelde bu kararları veren

00:07:43.068 --> 00:07:44.764
bir insan söz konusu değil.

00:07:44.860 --> 00:07:48.706
Araba imalatçıları, müşterileri için
üst düzey güvenlik önlemleri alan

00:07:48.706 --> 00:07:50.700
araba programlayabilirler

00:07:51.900 --> 00:07:54.876
ve bu arabalar, otomatik olarak
bu seçimleri yapmanın

00:07:54.876 --> 00:07:58.396
yayalar için riskleri bir nebze de olsa
arttırdığını öğrenebilir.

00:07:59.084 --> 00:08:00.780
Koyun benzetmesine dönecek olursak,

00:08:00.780 --> 00:08:04.396
şu anda kendi fikirlerini verebilen
elektrikli koyunlara sahibiz diyebiliriz.

00:08:04.396 --> 00:08:05.852
(Gülüşmeler)

00:08:05.852 --> 00:08:08.932
Ve kendi başlarına kaçıp
çiftçinin haberi olmadan otlanabilirler.

00:08:10.460 --> 00:08:14.436
Bunu da şu şekilde ifade edebiliriz:
algoritmik ortak malların trajedisi.

00:08:14.436 --> 00:08:16.796
Beraberinde yeni zorluklar getiriyor.

00:08:22.364 --> 00:08:24.260
Genellikle, çoğunlukla,

00:08:24.260 --> 00:08:27.596
bu tür toplumsal ikilemleri
yasalar nezdinde çözüme kavuştururuz;

00:08:27.596 --> 00:08:30.332
ya hükümet ya da topluluklar
bir araya gelir

00:08:30.332 --> 00:08:34.068
ve hep birlikle ne tür bir sonuç
elde etmek istediklerine karar verirler

00:08:34.068 --> 00:08:37.484
ve bireysel davranışlarda,
ne tür kısıtlamalara gidilmesi gerektiğini

00:08:37.484 --> 00:08:38.648
kararlaştırırlar.

00:08:39.420 --> 00:08:42.036
Gözlem ve yürürlülüğü kullanarak

00:08:42.036 --> 00:08:44.595
kamu yararının korunduğuna emin olurlar.

00:08:45.308 --> 00:08:46.883
Düzenleyiciler olarak,

00:08:46.883 --> 00:08:48.379
neden bütün arabalara

00:08:48.379 --> 00:08:51.276
zararı en aza indirgeme statüsü
kazandırmıyoruz?

00:08:51.300 --> 00:08:53.790
Nihayetinde,
herkes bunu istediğini dile getiriyor.

00:08:55.044 --> 00:08:56.460
Daha da önemlisi,

00:08:56.460 --> 00:08:59.556
bir birey olarak eminim ki

00:08:59.556 --> 00:09:03.412
çok nadir bir durumda beni
gözden çıkarabilecek bir araba alırsam,

00:09:03.412 --> 00:09:04.442
diğer herkes

00:09:04.442 --> 00:09:07.772
koşulsuz korumanın keyfini çıkarırken
bunu yapan tek keriz ben olmam.

00:09:08.940 --> 00:09:12.476
Ankette insanlara, yasal düzenlemeye
destek verip vermeyeceklerini sorduk

00:09:12.476 --> 00:09:13.610
ve sonuç şu şekildeydi.

00:09:14.180 --> 00:09:17.940
İlk önce, insanlar yasaya hayır dediler

00:09:19.100 --> 00:09:20.356
ve sonra dediler ki,

00:09:20.356 --> 00:09:24.292
''Arabaları bunun için ve toplam zararı
en aza indirgemek için düzenlersek,

00:09:24.292 --> 00:09:25.772
bu arabaları satın almam.''

00:09:27.244 --> 00:09:28.620
İşe bakın ki,

00:09:28.620 --> 00:09:32.116
zararı en aza indirgemek için
arabaları düzenleyerek

00:09:32.116 --> 00:09:34.096
daha fazla zarara sebebiyet verebiliriz,

00:09:34.860 --> 00:09:38.516
çünkü insanlar daha güvenli olan bu
teknolojiye dahil olmak istemeyebilirler,

00:09:38.516 --> 00:09:40.596
insan sürücülerden daha güvenli olsa bile.

00:09:42.180 --> 00:09:45.596
Bu bilmeceye nihai bir cevabım yok,

00:09:45.596 --> 00:09:47.172
ama başlangıç noktası olarak,

00:09:47.172 --> 00:09:50.468
toplumun bir araya gelip

00:09:50.468 --> 00:09:53.488
risk oranı dengesinde ortak bir noktada
karara varması gerekir

00:09:54.180 --> 00:09:57.520
ve bu denge unsurunun uygulamasında
izleyeceğimiz yol belirlenebilir.

00:09:58.364 --> 00:10:00.900
Başlangıç noktası olarak,
benim parlak öğrencilerim

00:10:00.900 --> 00:10:03.356
Edmond Awad ve Sohan Dsouza,

00:10:03.356 --> 00:10:05.256
Ahlak Makinesi internet sitesini kurdu.

00:10:06.020 --> 00:10:08.700
Bu site size rastgele senaryolar sunuyor,

00:10:09.900 --> 00:10:12.356
bir dizi ikilemler yöneltiyor

00:10:12.356 --> 00:10:16.276
ve verilen bu senaryoda, arabanın ne
yapması gerektiğine karar veriyorsun.

00:10:16.860 --> 00:10:21.460
Yaşları ve hatta farklı kurbanların
türlerini bile çeşitlendirdik.

00:10:22.860 --> 00:10:25.866
Şu ana kadar, site üzerinden
dünya genelinde

00:10:25.866 --> 00:10:28.756
bir milyonu aşkın insan tarafından
beş milyonun üzerinde

00:10:30.220 --> 00:10:31.420
karar topladık.

00:10:32.020 --> 00:10:34.420
Bu bulgular, kültürler arası bile

00:10:34.420 --> 00:10:36.830
insanların neleri feda edebileceklerini

00:10:36.830 --> 00:10:38.836
ve onlar için
önem arz eden şeyleri anlayıp

00:10:38.836 --> 00:10:40.840
bir temel oluşturmamızda yardımcı oluyor.

00:10:42.084 --> 00:10:43.580
Ama daha önemlisi,

00:10:43.580 --> 00:10:46.956
bu araştırma insanlara,
bu seçimleri yapmanın

00:10:46.956 --> 00:10:49.772
ne kadar zor olduğunu

00:10:49.772 --> 00:10:53.572
ve yasal düzenlemelerin, imkânsız
seçimler ile çevrelendiğini hatırlatıyor.

00:10:55.204 --> 00:10:58.780
Bu belki de toplum olarak,
nihayetinde yasalara işlenecek türde

00:10:58.780 --> 00:11:01.760
risk oranı tahlilini
anlamamıza yardımcı olabilir.

00:11:01.760 --> 00:11:03.806
Geçtiğimiz hafta,

00:11:03.806 --> 00:11:05.876
Ulaştırma Bakanlığı tarafından yapılan

00:11:05.876 --> 00:11:07.600
ilk yasal düzenlemelerden

00:11:07.600 --> 00:11:09.196
haberdar olduğuma çok sevindim.

00:11:09.220 --> 00:11:15.796
Tüm araba üreticilerinin sağlaması gereken
15 maddelik bir kontrol listesine sahip

00:11:15.796 --> 00:11:18.976
ve 14. madde işin
ahlaki boyutu ile alakalıydı,

00:11:18.976 --> 00:11:21.536
bu sorumluluğu
nasıl taşıyabileceğimizle alakalıydı.

00:11:23.620 --> 00:11:26.806
İnsanlara, seçimlerinin sonuçlarını
özet şeklinde sunarak

00:11:26.806 --> 00:11:29.300
kendi kararlarının getirilerini gösterdik.

00:11:30.284 --> 00:11:31.940
Sizlere bir örnek vereceğim.

00:11:31.940 --> 00:11:35.476
Belirtmeliyim ki bu genel bir örnek,

00:11:35.476 --> 00:11:36.852
genel bir kullanıcı değil.

00:11:36.852 --> 00:11:40.468
Bu kişi için en çok kurtarılan (kedi)
ve feda edilen (bebek) görüldüğü gibi.

00:11:40.468 --> 00:11:42.068
(Gülüşmeler)

00:11:46.500 --> 00:11:48.536
Bazılarınız bu kişiye katılıyor olabilir,

00:11:48.536 --> 00:11:50.060
bilmiyoruz.

00:11:52.300 --> 00:11:58.436
Bu kişi aynı zamanda seçimlerinde,
yayalara karşın yolcuları daha çok tutuyor

00:11:58.436 --> 00:12:00.532
gibi gözüküyor

00:12:00.532 --> 00:12:03.988
ve kırmızı ışıkta geçenleri
cezalandırmaktan memnuniyet duyuyor.

00:12:03.988 --> 00:12:06.460
(Gülüşmeler)

00:12:09.163 --> 00:12:10.379
Konuyu toparlayalım.

00:12:10.379 --> 00:12:13.985
Arabanın belirli bir senaryoda ne
yapması gerektiğini irdeleyen bir soruyla,

00:12:13.985 --> 00:12:16.520
daha doğrusu ahlaki bir ikilemle başladık:

00:12:16.520 --> 00:12:18.430
dönecek mi, kalacak mı?

00:12:19.060 --> 00:12:21.796
Fakat sonra, sorunun farklı
bir şey olduğunu fark ettik.

00:12:21.820 --> 00:12:26.356
Toplum olarak, risk durumunda insanların
nelerden fedakarlık yapabileceklerine

00:12:26.356 --> 00:12:28.292
uzlaşma ve uygulama sorunu.

00:12:28.292 --> 00:12:30.118
Toplumsal bir ikilemden bahsediyoruz.

00:12:30.118 --> 00:12:34.636
1940'larda, Isaac Asimov
o meşhur Üç Robot Yasasını

00:12:34.636 --> 00:12:35.956
kaleme aldı.

00:12:37.060 --> 00:12:39.516
Robot, bir insana zarar veremez,

00:12:39.540 --> 00:12:42.076
robot bir insana itaatsizlik edemez

00:12:42.100 --> 00:12:45.356
ve robot kendisini gelecek zararlardan
korumakla yükümlüdür,

00:12:45.380 --> 00:12:47.340
bu önem sırasına göre.

00:12:48.180 --> 00:12:50.316
40 yıl kadar sonra,

00:12:50.340 --> 00:12:54.076
bu yasaların sınırlarını zorlayan
birçok hikâyenin ardından Asimov,

00:12:54.076 --> 00:12:57.772
diğer bütün yasalara üstünlük gösteren

00:12:57.772 --> 00:13:00.028
sıfırıncı yasasını yayınladı;

00:13:00.028 --> 00:13:03.688
bu yasa bir robotun genel olarak
insanlığa zarar veremeyeceğini belirtiyor.

00:13:04.300 --> 00:13:08.676
Bu yasanın, sürücüsüz arabalar konusunda

00:13:08.676 --> 00:13:11.412
veya belirli bir durumdaki
yerini bilmiyorum.

00:13:11.412 --> 00:13:13.728
Bunu nasıl yürürlüğe
koyacağımızı da bilmiyorum,

00:13:13.728 --> 00:13:16.156
fakat sürücüsüz araba düzenlemesinin,

00:13:16.156 --> 00:13:21.826
yalnızca bir teknolojik problem olduğunu
değil de, aynı zamanda toplumsal uzlaşma

00:13:21.826 --> 00:13:24.676
problemi olduğunu kabul edersek,

00:13:25.620 --> 00:13:29.000
umuyorum ki, en azından sonunda
doğru soruları sormaya başlayabileceğiz.

00:13:29.000 --> 00:13:30.216
Teşekkürler.

00:13:30.216 --> 00:13:33.136
(Alkışlar)

