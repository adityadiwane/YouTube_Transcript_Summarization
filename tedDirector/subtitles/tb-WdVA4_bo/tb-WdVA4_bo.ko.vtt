WEBVTT
Kind: captions
Language: ko

00:00:00.000 --> 00:00:07.000
번역: Moonjeong Kang
검토: Jihyeon J. Kim

00:00:12.820 --> 00:00:16.900
저는 오늘 기술과 사회에 대한 
이야기를 하려고 합니다.

00:00:18.860 --> 00:00:21.000
교통부 추산에 따르면

00:00:21.000 --> 00:00:26.890
작년 한 해 동안 미국에서만 
3만 5천 명이 교통사고로 죽었습니다.

00:00:27.860 --> 00:00:32.900
세계적으로는 매년 120만 명이 
교통사고로 죽죠.

00:00:33.580 --> 00:00:37.676
만약 이런 사고의 90%를
없앨 수 있는 방법이 있다면

00:00:37.700 --> 00:00:39.070
여러분은 지지하시겠습니까?

00:00:39.540 --> 00:00:40.836
물론 그러시겠죠.

00:00:40.860 --> 00:00:44.515
이것이 바로 자율주행차 
기술의 목표입니다.

00:00:44.540 --> 00:00:47.060
사고의 주요 원인이라 할 수 있는

00:00:47.060 --> 00:00:49.060
인간의 실수를 없앰으로써 말이죠.

00:00:49.740 --> 00:00:55.070
2030년에 자율주행차에 
타고 있다고 상상해 보세요.

00:00:55.070 --> 00:00:58.636
느긋이 앉아서 오래된 
TEDxCambridge 영상을 보고 있죠.

00:00:58.660 --> 00:01:00.660
(웃음)

00:01:01.340 --> 00:01:02.556
여기서 갑자기

00:01:02.580 --> 00:01:05.860
차량 오작동으로 인해 
차를 멈출 수 없게 됩니다.

00:01:07.180 --> 00:01:08.700
만약 계속 간다면

00:01:09.540 --> 00:01:13.660
길 건너편에 있는 
보행자들과 부딪칠 거예요.

00:01:14.900 --> 00:01:17.035
하지만 차는 방향을 바꾸어서

00:01:17.059 --> 00:01:18.916
한 사람만 치어 죽이고

00:01:18.940 --> 00:01:21.020
보행자 무리를 구할 수도 있어요.

00:01:21.860 --> 00:01:24.920
차는 무엇을 해야 하며, 
이런 결정은 누가 해야 할까요?

00:01:25.340 --> 00:01:28.876
만약에 보행자들을 구하기 위해서

00:01:28.900 --> 00:01:32.196
차가 벽으로 돌진해서

00:01:32.220 --> 00:01:34.540
탑승자인 당신을 죽게 한다면요?

00:01:35.060 --> 00:01:38.570
이 시나리오는 '트롤리의 문제'에서 
영감을 받았어요.

00:01:38.780 --> 00:01:44.046
수 십 년전에 철학자들이
윤리에 대해 생각하기 위해 고안한 문제죠.

00:01:45.940 --> 00:01:48.436
우리가 이 문제에 대해 
생각하는 방식은 중요합니다.

00:01:48.460 --> 00:01:51.076
예를 들어, 우리는 이 문제에 대해 
전혀 생각하지 않을 수 있고요.

00:01:51.100 --> 00:01:54.476
이 시나리오가 비현실적이라거나

00:01:54.500 --> 00:01:57.110
있을 수 없는 일이라거나,
그저 바보같다고도 할 수도 있죠.

00:01:57.580 --> 00:02:00.316
하지만 이런 비판은 논점을 벗어나요.

00:02:00.340 --> 00:02:02.850
시나리오를 문자 그대로 
받아들이기 때문이죠.

00:02:03.740 --> 00:02:06.476
물론 사고가 이런 식으로 
일어나긴 힘듭니다.

00:02:06.500 --> 00:02:09.836
사고가 일어날 수 있는 
두 세가지의 확률에서

00:02:09.860 --> 00:02:12.320
모든 경우에 사망자가 
생기는 경우는 잘 없죠.

00:02:13.300 --> 00:02:15.876
대신에 차는 어떤 
확률 같은 걸 계산 할 거예요.

00:02:15.900 --> 00:02:20.796
각 경우에 사람들을 
치게 되는 확률 말이죠.

00:02:20.820 --> 00:02:24.156
가령, 여러분이 저 쪽 대신 
이 쪽으로 방향을 바꾸면

00:02:24.180 --> 00:02:29.066
탑승자나 다른 운전자들의 위험도가
보행자에 비해 더 증가할겁니다.

00:02:29.220 --> 00:02:32.080
조금 더 복잡한 계산이 되겠지만

00:02:32.310 --> 00:02:35.343
이것은 트레이드 오프와 
관련이 있습니다.

00:02:35.663 --> 00:02:38.776
트레이드 오프는 
흔히 도덕성을 필요로 합니다.

00:02:39.660 --> 00:02:42.396
그럼 이렇게 말하겠죠
"음, 이건 신경쓰지 말자"

00:02:42.420 --> 00:02:47.060
"기술이 좀 더 발전해서 
100% 안전해 질 때까지 기다리자"

00:02:48.340 --> 00:02:52.020
우리가 앞으로 10년안에 

00:02:52.900 --> 00:02:56.690
사고율을 90%, 혹은 99% 까지 
감소시킬 수 있다고 가정해 봅시다.

00:02:56.740 --> 00:02:59.916
나머지 1% 의 사고확률을 없애기 위해

00:02:59.940 --> 00:03:03.060
50 여년의 연구가 더 필요하다면요?

00:03:04.220 --> 00:03:06.310
그 기술을 사용하지 말아야 할까요?

00:03:06.540 --> 00:03:11.316
이 말은 현재의 추세로 따져보았을 때,
6천만 명의 사람들이

00:03:11.340 --> 00:03:13.590
차 사고로 죽는 셈이 됩니다.

00:03:14.580 --> 00:03:15.796
요점은

00:03:15.820 --> 00:03:19.436
완벽한 안전을 위해 기다리는 것 
또한 하나의 선택이고

00:03:19.460 --> 00:03:22.100
트레이드 오프와 관련이 있다는 겁니다.

00:03:23.380 --> 00:03:25.710
사람들은 온라인 소셜미디어에서

00:03:25.710 --> 00:03:29.756
이 문제를 회피하기 위한
온갖 방법을 제시해 왔습니다.

00:03:29.780 --> 00:03:35.156
어떤 사람은 차가 어떻게 해서든지
탑승자들과 행인 사이로 가도록

00:03:35.180 --> 00:03:36.196
(웃음)

00:03:36.220 --> 00:03:37.476
방향을 틀어야 한다고 했죠.

00:03:37.500 --> 00:03:41.100
만약 차가 그렇게 할 수만 있다면, 
그렇게 해야 할 겁니다.

00:03:41.740 --> 00:03:44.580
우리는 이런 시나리오가 불가능한 경우에 
관심이 있습니다.

00:03:45.100 --> 00:03:50.516
제가 개인적으로 가장 좋아하는 의견은
어떤 블로거가 제안한 것인데요.

00:03:50.540 --> 00:03:53.556
차에 탈출 버튼이 있어서

00:03:53.580 --> 00:03:54.280
(웃음)

00:03:54.280 --> 00:03:56.947
차가 부서지기 직전에
그 버튼을 누르는 거예요.

00:03:56.947 --> 00:03:58.191
(웃음)

00:03:59.660 --> 00:04:05.310
이렇게 차들이 도로에서 
트레이드 오프 함을 인정한다면

00:04:06.020 --> 00:04:08.580
우리는 이 트레이드 오프를 
어떤 방식으로 대해야 하며

00:04:09.140 --> 00:04:10.716
어떻게 결정을 내려야 할까요?

00:04:10.740 --> 00:04:13.876
사회가 원하는 게 무엇인지 알기 위해 
우리는 아마 설문 조사를 해야 할 거예요.

00:04:13.900 --> 00:04:15.356
왜냐하면, 궁극적으로

00:04:15.380 --> 00:04:19.340
규제과 법은 사회적 가치를 
반영하고 있기 때문입니다.

00:04:19.860 --> 00:04:21.560
그래서 우리는 다음과 같은 일을 했어요

00:04:21.700 --> 00:04:23.316
저는 제 동료인

00:04:23.340 --> 00:04:25.676
장 프랑수아 보네퐁, 
아짐 샤리프와 함께

00:04:25.700 --> 00:04:27.316
설문조사를 실시했어요.

00:04:27.340 --> 00:04:30.195
이런 종류의 시나리오들을 
사람들에게 보여줍니다.

00:04:30.219 --> 00:04:33.996
두 철학자에게서 영감을 받은
두 가지 옵션을 줍니다.

00:04:34.020 --> 00:04:36.660
제레미 벤담과 엠마누엘 칸트

00:04:37.420 --> 00:04:40.516
벤담은 차가 공리주의적 윤리에 
따라야 한다고 말합니다.

00:04:40.540 --> 00:04:43.956
전체의 피해를 최소화할 수 있는 
행동을 취해야 한다는 입장이죠

00:04:43.980 --> 00:04:46.796
그 결정이 한 사람의 행인이나

00:04:46.820 --> 00:04:49.260
탑승자를 죽이는 결정이라 하더라도요.

00:04:49.940 --> 00:04:54.916
칸트는 차가 의무에 입각한 
원칙을 따라야 한다고 합니다.

00:04:54.940 --> 00:04:56.890
가령 "살인을 하지 말라" 와 
같은 원칙이죠.

00:04:57.300 --> 00:05:01.756
명백하게 인류를 해치게 만드는 
행위를 해서는 안되고

00:05:01.780 --> 00:05:04.236
차가 원래 가던길을 가게 해야 합니다.

00:05:04.260 --> 00:05:06.220
그 결정이 더 많은 사람들을 
죽이게 될지라도요.

00:05:07.460 --> 00:05:08.960
어떻게 생각하시나요?

00:05:09.180 --> 00:05:10.700
벤담입니까? 칸트입니까?

00:05:11.580 --> 00:05:12.836
설문조사의 결과입니다.

00:05:12.860 --> 00:05:14.960
대부분의 사람들이 
벤담의 손을 들었습니다.

00:05:15.980 --> 00:05:19.756
사람들은 차가 공리주의적으로

00:05:19.780 --> 00:05:21.196
전체 피해를 최소화하길 원하며

00:05:21.220 --> 00:05:22.796
우리 모두가 그렇게 하길 바랍니다.

00:05:22.820 --> 00:05:24.020
문제는 해결되었습니다.

00:05:25.060 --> 00:05:26.540
하지만 조금 걸리는 점이 있어요.

00:05:27.740 --> 00:05:31.476
우리가 사람들에게 
이런 차를 사겠냐고 물었더니

00:05:31.500 --> 00:05:33.116
절대로 사지 않겠다고 대답했어요.

00:05:33.140 --> 00:05:35.436
(웃음)

00:05:35.460 --> 00:05:39.356
사람들은 어떻게 해서든 
자기를 보호할 차를 사고싶어 하지만

00:05:39.380 --> 00:05:42.996
다른 사람들은 전체 피해를 
최소화하는 차를 사길 원하죠.

00:05:43.020 --> 00:05:45.540
(웃음)

00:05:46.540 --> 00:05:48.396
우리는 전에 
이 문제를 본 적이 있어요.

00:05:48.420 --> 00:05:49.980
이것을 사회적 딜레마라고 합니다.

00:05:50.980 --> 00:05:52.796
사회적 딜레마를 이해하기 위해서는

00:05:52.820 --> 00:05:54.860
역사를 잠시 살펴보아야 합니다.

00:05:55.820 --> 00:05:58.396
1800년대에

00:05:58.420 --> 00:06:02.156
영국 경제학자인 윌리엄 포스터 로이드가
글을 발표했는데

00:06:02.180 --> 00:06:04.396
다음과 같은 시나리오를 담고 있어요.

00:06:04.420 --> 00:06:06.076
한 무리의 농부들이 있어요.

00:06:06.100 --> 00:06:07.436
영국 농부들이구요

00:06:07.460 --> 00:06:10.140
이들은 양을 방목하기 위한 
땅을 공유하고 있어요.

00:06:11.340 --> 00:06:13.916
각 농부마다 
몇 마리의 양을 데리고 와요.

00:06:13.940 --> 00:06:15.436
농부당 세 마리 라고 합시다.

00:06:15.460 --> 00:06:17.556
땅은 활력을 찾을 것이고

00:06:17.580 --> 00:06:18.796
농부들은 행복하고

00:06:18.820 --> 00:06:20.436
양들도 행복하고

00:06:20.460 --> 00:06:21.660
모든 것이 좋을 거예요.

00:06:22.260 --> 00:06:24.780
이제 한 농부가 양을 
한 마리 더 데리고 와요.

00:06:25.620 --> 00:06:30.340
그 농부는 조금 더 이익을 보지만, 
여기서 손해를 보는 사람은 아무도 없어요.

00:06:30.980 --> 00:06:34.620
하지만 모든 농부가 이렇게 
개인에게 합리적인 선택을 한다면

00:06:35.660 --> 00:06:38.380
땅은 꽉 차게 되고, 고갈될 거예요.

00:06:39.180 --> 00:06:41.356
농부들에게 해를 입히고

00:06:41.380 --> 00:06:43.500
양들에게까지 해를 입힐 정도로 말이죠.

00:06:44.540 --> 00:06:48.220
우리는 많은 곳에서
이와 비슷한 문제를 봅니다.

00:06:48.900 --> 00:06:52.076
남획 관리의 어려움에서

00:06:52.100 --> 00:06:57.030
혹은 기후 변화를 완화하기 위해
탄소 배출량을 줄이는 것에서요.

00:06:58.980 --> 00:07:01.900
자율주행차에 대한 규제에 있어서

00:07:02.900 --> 00:07:07.236
공유지는 기본적으로 공공안전입니다.

00:07:07.260 --> 00:07:08.500
그것이 공익입니다.

00:07:09.220 --> 00:07:11.196
농부들은 탑승자들 혹은

00:07:11.220 --> 00:07:14.820
그 차에 타는 결정을 한 
차량소유주들 이겠죠.

00:07:16.780 --> 00:07:19.396
본인의 안전을 우선으로 하는

00:07:19.420 --> 00:07:22.236
개인에게 합리적인 선택을 함으로써

00:07:22.260 --> 00:07:23.400
총체적으로는

00:07:23.400 --> 00:07:27.620
전체적인 피해를 최소화하는, 이 공익을
약화시킬 겁니다.

00:07:30.140 --> 00:07:32.276
이것은 공유지의 비극으로 불립니다

00:07:32.300 --> 00:07:33.596
전통적으로요,

00:07:33.620 --> 00:07:36.716
하지만 자율주행차의 경우엔

00:07:36.740 --> 00:07:39.596
이 문제는 좀 미묘하게 됩니다.

00:07:39.620 --> 00:07:43.276
왜냐하면 개개인이 반드시 
그런 선택을 해야하는 것은

00:07:44.860 --> 00:07:48.156
따라서 자동차 제조사들은 아마
고객의 안전을 최대한 보장하도록

00:07:48.180 --> 00:07:50.700
차량 프로그램을 단순하게 할 겁니다.

00:07:51.900 --> 00:07:54.876
그러면 차들은 자동으로 
자율학습을 할 수 있습니다.

00:07:54.900 --> 00:07:58.420
그러한 선택이 보행자들의 위험을
살짝 증가시킨다는 점을 말이죠.

00:07:59.340 --> 00:08:00.756
양으로 비유하자면

00:08:00.780 --> 00:08:04.396
우린 이제 스스로 생각할 수 있는
전자 양으로 가진 셈이죠.

00:08:04.420 --> 00:08:05.876
(웃음)

00:08:05.900 --> 00:08:08.980
이 양들은 농부 몰래 가서
풀을 뜯어 먹을 수 있어요.

00:08:10.460 --> 00:08:14.436
우리는 이것을 알고리즘적 
공유지의 비극이라 부를 수 있겠고요.

00:08:14.460 --> 00:08:16.820
이는 새로운 종류의 도전을 야기합니다.

00:08:22.340 --> 00:08:24.236
일반적으로, 전통적으로

00:08:24.260 --> 00:08:27.596
이런 종류의 사회적 딜레마를 해결하기 위해서
규제가 사용됩니다.

00:08:27.620 --> 00:08:32.056
정부나 지역사회들이 함께 모여
집단적으로 결정합니다.

00:08:32.056 --> 00:08:34.066
어떤 종류의 결과를 원하는지

00:08:34.140 --> 00:08:35.900
그리고 개인들의 행동에 대해선

00:08:35.900 --> 00:08:38.300
어떤 종류의 제한을 
둘 것인지를 말이죠.

00:08:39.420 --> 00:08:42.036
그런 다음에, 감시과 강화를 통해

00:08:42.060 --> 00:08:44.619
공익이 보존됨을 확인할 수 있죠.

00:08:45.260 --> 00:08:46.835
그렇다면 우리는

00:08:46.859 --> 00:08:48.355
규제를 만드는 입장으로서

00:08:48.379 --> 00:08:51.276
모든 차량들에게 그저 피해를 
최소화하도록 요구하면 되는 것 아닐까요?

00:08:51.300 --> 00:08:53.540
결국엔 이것이
사람들이 원한다고 말하는 것이에요.

00:08:55.020 --> 00:08:56.436
그리고 더 중요한 것은

00:08:56.460 --> 00:08:59.556
한 개인으로서 저는 확신합니다.

00:08:59.580 --> 00:09:03.200
제가 만약 아주 드물 확율로 
저를 희생시킬 수도 있는 차를 산다면

00:09:03.200 --> 00:09:05.306
다른 모든 사람들이
무조건적인 보호를 즐기는 동안

00:09:05.320 --> 00:09:08.060
저만 홀로 그런 선택을 하는 사람은
아닐 겁니다.

00:09:08.940 --> 00:09:12.276
우리 설문 조사에서 사람들에게 
규제를 지지하는지를 물어보았고

00:09:12.300 --> 00:09:13.500
여기 그 결과가 있습니다.

00:09:14.180 --> 00:09:17.940
우선 사람들은
규제에는 반대한다고 했습니다.

00:09:19.100 --> 00:09:20.356
그리고 말하더군요.

00:09:20.380 --> 00:09:24.316
"만약 차량에 대해 이런식으로 규제를 하고,
전체 피해를 줄이고자 한다면"

00:09:24.340 --> 00:09:25.820
"그 차를 사지 않겠어요"

00:09:27.220 --> 00:09:28.596
아이러니하게도

00:09:28.620 --> 00:09:32.116
피해를 최소화하려고 규제를 하는 것은

00:09:32.140 --> 00:09:33.980
더 큰 피해를 불러올 수 있습니다

00:09:34.860 --> 00:09:38.516
왜냐하면 사람들이 더 안전한 기술에
동의하지 않을 수도 있기 때문이죠.

00:09:38.540 --> 00:09:40.620
비록 그것이 사람이 운전하는 것보다
훨씬 안전하다 할지라도요.

00:09:42.180 --> 00:09:45.596
이 난제에 대한 최종 정답은 없습니다.

00:09:45.620 --> 00:09:47.196
하지만 출발점으로서

00:09:47.220 --> 00:09:50.516
사회를 한 곳에 모을 필요는 있습니다.

00:09:50.540 --> 00:09:53.560
어떤 트레이드 오프가 
우리에게 편안한지를 결정하고

00:09:54.180 --> 00:09:57.660
그 트레이드 오프를 실행하기 위한 방법들을 
제시하기 위해서 말이죠

00:09:58.340 --> 00:09:59.900
출발점으로서

00:09:59.900 --> 00:10:03.290
저의 똑똑한 학생들인
에드몬드 어와드와 소한 드수자는

00:10:03.290 --> 00:10:05.370
도덕 기계 (Moral Machine) 
웹사이트를 만들었습니다.

00:10:06.020 --> 00:10:08.700
랜덤한 시나리오들을
여러분에게 보여줍니다.

00:10:09.900 --> 00:10:12.356
기본적으로 랜덤한 딜레마들이 
연속적으로 나오는 것이고요.

00:10:12.380 --> 00:10:16.300
여러분은 차가 무엇을 
해야 하는지를 골라야 합니다.

00:10:16.860 --> 00:10:21.460
여기서 희생자들의 
나이와 인종은 각각 다릅니다.

00:10:22.860 --> 00:10:26.556
지금까지 우리는
500만 건이 넘는 결정들을 수집했습니다.

00:10:26.580 --> 00:10:28.780
전 세계적으로 
100만명이 넘는 사람들이

00:10:30.220 --> 00:10:31.420
웹사이트를 통해 참여했습니다.

00:10:32.180 --> 00:10:35.086
이 결과는 저희가 초기 연구를 만드는 데에
도움이 되고 있습니다.

00:10:35.086 --> 00:10:37.236
사람들이 편안해 하는 
트레이드 오프가 무엇이고

00:10:37.260 --> 00:10:39.156
무엇이 그들에게 중요한지에 대해서

00:10:39.180 --> 00:10:40.620
문화를 넘어서까지 말이죠.

00:10:42.060 --> 00:10:43.556
하지만 더 중요한 것은

00:10:43.580 --> 00:10:46.690
이 훈련은 사람들로 하여금

00:10:46.690 --> 00:10:49.796
이런 선택을 하는 것이 힘들다는 것과

00:10:49.820 --> 00:10:54.320
결정권자들이 불가능한 선택을 가지고 일함을
인지시키는데에 도움이 되고 있습니다.

00:10:55.180 --> 00:10:59.536
이것은 사회가 궁극적으로 규제로 만들 
트레이드 오프의 종류를 이해하는 데

00:10:59.546 --> 00:11:01.836
도움이 될 것입니다.

00:11:01.860 --> 00:11:03.596
지난주에 교통부에서

00:11:03.620 --> 00:11:07.516
첫 번째 규제를 발표했다는 소식을 듣고

00:11:07.516 --> 00:11:08.866
저는 참으로 기뻤습니다.

00:11:09.220 --> 00:11:15.796
이 규제는 모든 자동차 회사들이 제공해야 할
15개의 체크리스트를 담고 있는데

00:11:15.820 --> 00:11:19.076
이 중 14번이 윤리적인 고려입니다.

00:11:19.100 --> 00:11:21.550
그걸 어떻게 처리할 것이냐
하는 것이죠.

00:11:23.620 --> 00:11:26.276
우리는 또 사람들에게
자신의 선택들을 돌이켜보게 합니다.

00:11:26.300 --> 00:11:29.300
그들이 선택한 것을
다시 요약해줌으로써 말이죠.

00:11:30.260 --> 00:11:31.916
한 가지 예를 들어볼게요.

00:11:31.940 --> 00:11:33.360
여러분에게 경고드립니다.

00:11:33.360 --> 00:11:36.876
이것은 일반적인 예나
일반적인 사용자가 아닙니다.

00:11:36.900 --> 00:11:40.516
이것은 이 사람에게
가장 희생되고 가장 구원받은 캐릭터입니다.

00:11:40.540 --> 00:11:45.740
(웃음)

00:11:46.500 --> 00:11:48.396
여러분 중 일부는

00:11:48.420 --> 00:11:50.710
이 사람에게 동의하실지도 모르겠어요.

00:11:52.300 --> 00:11:56.640
하지만 이 사람의 선택을 살펴보면

00:11:56.640 --> 00:12:00.556
보행자 보다는 
탑승자를 약간 더 선호하고

00:12:00.580 --> 00:12:03.396
무단횡단자를 벌주고 싶어해요.

00:12:03.420 --> 00:12:06.460
(웃음)

00:12:09.140 --> 00:12:10.356
정리하겠습니다.

00:12:10.379 --> 00:12:13.795
우리는 이 질문과 함께 시작했어요.
윤리적 딜레마라고 부를게요.

00:12:13.820 --> 00:12:16.876
시나리오 상에서 차가 
어떻게 해야하는지를 물어요.

00:12:16.900 --> 00:12:18.100
방향을 틀 것인가? 가만히 있을 것인가?

00:12:19.060 --> 00:12:21.796
그런 다음에 우린 문제는 
다른 것이라는 사실을 깨닫죠.

00:12:21.820 --> 00:12:26.356
그것은 어떻게 
사회적인 합의를 이끌어내고

00:12:26.380 --> 00:12:28.316
사람들이 편안해하는 트레이드 오프를
집행할 것인가의 문제였죠.

00:12:28.340 --> 00:12:29.596
이것은 사회적 딜레마입니다.

00:12:29.620 --> 00:12:32.250
1940년대에 아이작 아시모프는

00:12:32.250 --> 00:12:35.980
그의 유명한 
로봇의 3원칙을 저술합니다.

00:12:37.060 --> 00:12:39.516
로봇은 인간에게 해를 끼칠 수 없고

00:12:39.540 --> 00:12:42.076
로봇은 인간이 내리는 
명령에 복종해야 하며

00:12:42.100 --> 00:12:45.356
로봇은 자신의 존재를 보호해야 한다.

00:12:45.380 --> 00:12:47.340
순서대로 중요하죠.

00:12:48.180 --> 00:12:50.316
하지만 40여년의 세월이 흐르고

00:12:50.340 --> 00:12:54.076
많은 스토리들이 나타나며
이 법칙들이 한계에 부딪치자

00:12:54.100 --> 00:12:57.796
아시모프는 0번째 법칙을 소개했습니다.

00:12:57.820 --> 00:13:00.076
이는 3가지 법칙보다 우선하는 법칙으로

00:13:00.100 --> 00:13:03.380
바로, 로봇은 인류 전체에게 
해를 끼칠 수 없다는 것입니다.

00:13:04.300 --> 00:13:08.676
저는 이것이 자율주행차의 
경우나 다른 경우에 있어

00:13:08.700 --> 00:13:11.436
어떤 의미가 되는지 잘 모르겠습니다.

00:13:11.460 --> 00:13:13.676
우리가 어떻게 
구현할지도 잘 모르겠고요.

00:13:13.700 --> 00:13:15.426
하지만 자율주행차에 대한 규제가

00:13:15.426 --> 00:13:21.396
단지 기술적인 문제가 아니라

00:13:21.420 --> 00:13:25.060
사회적 협력 문제임을 인식함으로써

00:13:25.620 --> 00:13:28.980
우리가 적어도 올바른 질문을 
시작할 수 있게 되기를 희망해봅니다.

00:13:29.020 --> 00:13:30.236
감사합니다.

00:13:30.260 --> 00:13:33.180
(박수)

