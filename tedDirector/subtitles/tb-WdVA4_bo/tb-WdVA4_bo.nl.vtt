WEBVTT
Kind: captions
Language: nl

00:00:00.000 --> 00:00:07.000
Vertaald door: Eric Cornelissen
Nagekeken door: Peter van de Ven

00:00:12.820 --> 00:00:16.900
Vandaag ga ik het hebben
over technologie en maatschappij.

00:00:18.860 --> 00:00:22.530
Het ministerie van transport
schatte dat afgelopen jaar

00:00:22.530 --> 00:00:26.660
35.000 mensen zijn overleden
door verkeersongelukken in de VS.

00:00:27.860 --> 00:00:32.660
Wereldwijd overlijden er ieder jaar
1,2 miljoen mensen in verkeersongelukken.

00:00:33.580 --> 00:00:37.676
Als er een manier was
om 90% van die ongelukken te voorkomen,

00:00:37.700 --> 00:00:38.900
zou je er dan vóór zijn?

00:00:39.520 --> 00:00:40.856
Natuurlijk ben je daar voor.

00:00:40.860 --> 00:00:44.515
Dit is wat zelfrijdende auto's
beloven te doen

00:00:44.520 --> 00:00:48.350
door de hoofdoorzaak van ongevallen,
menselijke fouten, weg te nemen.

00:00:49.740 --> 00:00:55.026
Stel je voor dat je in
een zelfrijdende auto zit, het is 2030,

00:00:55.026 --> 00:00:58.556
en je kijkt naar deze
vintage TEDxCambridge video.

00:00:58.580 --> 00:01:00.660
(Gelach)

00:01:01.340 --> 00:01:05.566
Plotseling gaat er iets fout met de auto
en de auto kan niet meer stoppen.

00:01:07.180 --> 00:01:08.700
Als de auto door blijft rijden,

00:01:09.540 --> 00:01:13.660
rijdt deze in op een groep voetgangers
die de straat aan het oversteken zijn.

00:01:14.900 --> 00:01:18.645
Maar de auto kan uitwijken
en een omstander aanrijden,

00:01:18.645 --> 00:01:21.580
wiens dood de redding
van de voetgangers zou betekenen.

00:01:21.860 --> 00:01:24.460
Wat zou de auto moeten doen
en wie moet dat besluiten?

00:01:25.340 --> 00:01:28.876
Wat als de auto in plaats daarvan
kan uitwijken naar een muur,

00:01:28.900 --> 00:01:32.196
waarbij jij dan wel overlijdt,

00:01:32.220 --> 00:01:34.540
maar de voetgangers worden gered?

00:01:35.060 --> 00:01:38.140
Dit scenario is gebaseerd
op 'The trolley problem',

00:01:38.780 --> 00:01:42.556
bedacht door filosofen
een paar decennium geleden

00:01:42.560 --> 00:01:43.910
om te redeneren over ethiek.

00:01:45.940 --> 00:01:48.436
Hoe we over dit probleem
denken, is belangrijk.

00:01:48.460 --> 00:01:51.076
We kunnen het bijvoorbeeld negeren.

00:01:51.100 --> 00:01:54.476
We kunnen zeggen
dat dit scenario onrealistisch,

00:01:54.500 --> 00:01:56.820
onwaarschijnlijk of zelfs flauw is.

00:01:57.580 --> 00:02:00.316
Maar ik denk dat
dit soort kritiek het punt mist,

00:02:00.340 --> 00:02:02.500
omdat het het scenario
te letterlijk neemt.

00:02:03.730 --> 00:02:06.506
Natuurlijk is er geen ongeluk
wat er precies zo uit ziet.

00:02:06.506 --> 00:02:09.846
Er zijn nooit maar twee
of drie mogelijkheden,

00:02:09.850 --> 00:02:11.860
waarbij altijd iedereen overlijdt.

00:02:13.300 --> 00:02:15.876
In plaats daarvan
gaat de auto iets berekenen.

00:02:15.900 --> 00:02:20.796
Bijvoorbeeld, de kans dat hij een
bepaalde groep mensen gaat raken;

00:02:20.820 --> 00:02:24.156
als je naar de ene kant uitwijkt
in plaats van de andere;

00:02:24.180 --> 00:02:27.636
je kunt het risico van passagiers
of andere bestuurders verhogen

00:02:27.660 --> 00:02:29.196
tegenover dat van voetgangers.

00:02:29.220 --> 00:02:31.380
De berekening zal complexer zijn.

00:02:32.300 --> 00:02:34.820
Maar er zullen afwegingen
gemaakt moeten worden.

00:02:35.660 --> 00:02:38.540
En afwegingen vereisen vaak
ethische overwegingen.

00:02:39.570 --> 00:02:42.436
We kunnen ervoor kiezen
om ons er geen zorgen over te maken.

00:02:42.436 --> 00:02:47.060
Wacht gewoon tot de technologie
helemaal klaar en 100% veilig is.

00:02:48.340 --> 00:02:52.020
Stel dat we daadwerkelijk 90%
van de ongelukken kunnen voorkomen,

00:02:52.900 --> 00:02:55.740
of zelfs 99%, in de komende 10 jaar.

00:02:56.740 --> 00:02:59.916
Wat als het voorkomen van die laatste 1%

00:02:59.940 --> 00:03:03.060
ons 50 jaar extra onderzoek kost?

00:03:04.200 --> 00:03:06.170
Moeten we de technologie dan negeren?

00:03:06.540 --> 00:03:11.326
Dat betekent dat er 60 miljoen mensen
overlijden in auto-ongelukken,

00:03:11.326 --> 00:03:13.120
gegeven de huidige trend.

00:03:14.580 --> 00:03:15.796
Het punt is,

00:03:15.820 --> 00:03:19.436
wachten op veiligheid is ook een keuze

00:03:19.460 --> 00:03:21.620
en heeft ook ethische afwegingen.

00:03:23.380 --> 00:03:27.630
Mensen, online en op social media,
hebben allerlei manieren bedacht

00:03:27.630 --> 00:03:29.780
om niet stil te hoeven staan
bij dit probleem.

00:03:29.780 --> 00:03:31.430
Iemand stelde voor:

00:03:31.430 --> 00:03:35.186
"De auto moet op de een of andere manier
tussen de passagiers en de voetgangers

00:03:35.186 --> 00:03:36.196
(Gelach)

00:03:36.210 --> 00:03:37.496
door rijden."

00:03:37.500 --> 00:03:40.860
Natuurlijk moet de auto
dat doen als dat mogelijk is.

00:03:41.740 --> 00:03:44.580
Onze focus ligt op de scenarios
waar dit niet mogelijk is.

00:03:45.100 --> 00:03:50.516
Mijn persoonlijke favoriet was
een suggestie van een blogger,

00:03:50.540 --> 00:03:53.556
om een schietstoel in de auto
te hebben die afgaat --

00:03:53.580 --> 00:03:54.520
(Gelach)

00:03:54.520 --> 00:03:56.101
net voor de auto botst.

00:03:56.101 --> 00:03:58.191
(Gelach)

00:03:59.660 --> 00:04:04.860
Als we erkennen dat auto's
afwegingen moeten maken op de weg,

00:04:06.020 --> 00:04:07.900
hoe denken we over die overwegingen

00:04:09.140 --> 00:04:10.710
en hoe nemen we die beslissing.

00:04:10.710 --> 00:04:13.956
Misschien moeten we een enquête houden
om te bepalen wat men wil,

00:04:13.956 --> 00:04:16.870
want uiteindelijk zijn regels en wetten

00:04:16.870 --> 00:04:19.560
een afspiegeling
van maatschappelijke waarden.

00:04:19.860 --> 00:04:21.100
Dus deden we dat.

00:04:21.700 --> 00:04:23.316
Met mijn collega's,

00:04:23.340 --> 00:04:25.676
Jean-François Bonnefon en Azim Shariff,

00:04:25.700 --> 00:04:27.316
hielden we een enquête

00:04:27.340 --> 00:04:30.195
waarin we mensen
dit soort scenario's voorlegden.

00:04:30.219 --> 00:04:33.996
We gaven mensen twee opties
geïnspireerd door twee filosofen:

00:04:34.020 --> 00:04:36.660
Jeremy Bentham en Immanuel Kant.

00:04:37.420 --> 00:04:40.516
Bentham zegt dat de auto
utilitaristisch moet handelen:

00:04:40.540 --> 00:04:43.956
de auto moet dát doen
wat het minste letsel veroorzaakt.

00:04:43.980 --> 00:04:46.796
Zelfs als dat betekent
dat er een omstander overlijdt

00:04:46.800 --> 00:04:49.290
en zelfs als dat betekent
dat de passagier overlijdt.

00:04:49.940 --> 00:04:54.916
Immanuel Kant zegt dat de auto
plichtgebonden moet handelen,

00:04:54.940 --> 00:04:56.500
zoals in 'gij zult niet doden'.

00:04:57.300 --> 00:05:01.756
Hij mag dus niets doen
wat expliciet iemand letsel toebrengt

00:05:01.780 --> 00:05:04.236
en de auto moet niet
van richting veranderen,

00:05:04.260 --> 00:05:06.220
zelfs als dat meer letsel veroorzaakt.

00:05:07.460 --> 00:05:08.660
Wat denk jij?

00:05:09.180 --> 00:05:10.700
Bentham of Kant?

00:05:11.530 --> 00:05:12.856
Dit is wat wij vonden.

00:05:12.860 --> 00:05:14.660
De meeste mensen kozen voor Bentham.

00:05:15.980 --> 00:05:19.700
Het lijkt dus dat mensen
utilitaire auto's willen,

00:05:19.700 --> 00:05:21.196
letsel willen voorkomen,

00:05:21.220 --> 00:05:22.796
en dat iedereen dat zo moet doen.

00:05:22.820 --> 00:05:24.020
Probleem opgelost.

00:05:25.060 --> 00:05:26.540
Maar er is één klein probleem.

00:05:27.740 --> 00:05:31.476
Toen we vroegen of mensen
zo'n auto zouden kopen,

00:05:31.500 --> 00:05:33.116
zeiden ze: "Natuurlijk niet."

00:05:33.140 --> 00:05:35.436
(Gelach)

00:05:35.460 --> 00:05:39.356
Men koopt graag een auto
die hen te allen tijde beschermt,

00:05:39.380 --> 00:05:42.996
maar wil dat de rest een auto koopt
die de totale schade vermindert.

00:05:43.020 --> 00:05:45.540
(Gelach)

00:05:46.540 --> 00:05:48.396
Dit is een bekent probleem

00:05:48.420 --> 00:05:49.980
en het heet een sociaal dilemma.

00:05:50.960 --> 00:05:52.826
En om dit sociale dilemma te begrijpen,

00:05:52.826 --> 00:05:54.860
moeten we naar het verleden kijken.

00:05:55.820 --> 00:05:58.396
In de 18de eeuw

00:05:58.420 --> 00:06:02.156
publiceerde de Engelse econoom
William Forster Lloyd een pamflet

00:06:02.180 --> 00:06:04.396
waarin het volgende scenario
werd beschreven.

00:06:04.420 --> 00:06:06.076
Er is een groep boeren --

00:06:06.100 --> 00:06:07.436
Engelse boeren --

00:06:07.460 --> 00:06:10.140
die een stuk land delen
om hun schapen te laten grazen.

00:06:11.340 --> 00:06:13.916
Als iedere boer een bepaald
aantal schapen heeft --

00:06:13.940 --> 00:06:15.436
laten we zeggen drie --

00:06:15.460 --> 00:06:17.556
dan blijft het land goed

00:06:17.580 --> 00:06:20.426
en zijn de boeren en de schapen gelukkig.

00:06:20.460 --> 00:06:21.660
Alles is goed.

00:06:22.260 --> 00:06:24.780
Als een boer een extra
schaap introduceert,

00:06:25.620 --> 00:06:30.340
dan heeft die boer het iets beter
en daar heeft niemand last van.

00:06:30.980 --> 00:06:34.620
Maar als iedere boer
die individueel rationele keuze maakt,

00:06:35.660 --> 00:06:38.380
wordt het land overbegrazen
en is het niet meer bruikbaar.

00:06:39.180 --> 00:06:41.356
Ten nadele van alle boeren

00:06:41.380 --> 00:06:43.500
en natuurlijk de schapen.

00:06:44.540 --> 00:06:48.220
Dit is een probleem wat veel voorkomt:

00:06:48.900 --> 00:06:52.076
bij het beleid van overbevissing,

00:06:52.100 --> 00:06:56.660
of CO2-uitstoot verminderen
om klimaatverandering tegen te gaan.

00:06:58.980 --> 00:07:01.900
Als het gaat over het reguleren
van autonome voertuigen

00:07:02.900 --> 00:07:07.220
kan veiligheid worden gezien
als het gedeelde land van de boeren --

00:07:07.220 --> 00:07:08.510
dat is het algemeen goed --

00:07:09.220 --> 00:07:11.196
en de boeren zijn de passagiers

00:07:11.220 --> 00:07:14.820
of de eigenaren van die auto's
die ervoor kiezen er in te rijden.

00:07:16.780 --> 00:07:19.396
En door de individueel
rationele keuze te maken,

00:07:19.420 --> 00:07:22.236
het prioriteren van de eigen veiligheid,

00:07:22.260 --> 00:07:25.396
zorgt iedereen ervoor
dat het algemeen goed afneemt,

00:07:25.420 --> 00:07:27.620
wat het verminderen
van het totale letsel is.

00:07:30.140 --> 00:07:32.276
Dit heet de tragedie van de meent,

00:07:32.300 --> 00:07:33.596
van oudsher,

00:07:33.620 --> 00:07:36.716
maar in de context van zelfrijdende auto's

00:07:36.740 --> 00:07:39.596
is het probleem
misschien wat verraderlijk,

00:07:39.620 --> 00:07:43.116
want er is niet perse
een menselijk individu

00:07:43.140 --> 00:07:44.836
dat de beslissingen neemt.

00:07:44.860 --> 00:07:48.156
En dus kunnen autofabrikanten
auto's zo programmeren

00:07:48.180 --> 00:07:50.700
dat ze de veiligheid
van hun klanten maximaliseren,

00:07:51.900 --> 00:07:54.876
en dit soort auto's leren
automatisch en zelfstandig

00:07:54.900 --> 00:07:58.420
dat dit vereist dat voetgangers
meer risico lopen.

00:07:59.340 --> 00:08:00.676
Dus in het schaapmetafoor:

00:08:00.676 --> 00:08:04.396
we hebben nu elektrische schapen
die zelf kunnen denken.

00:08:04.420 --> 00:08:05.876
(Gelach)

00:08:05.900 --> 00:08:08.980
En die kunnen gaan grazen
zonder dat de boer het weet.

00:08:10.460 --> 00:08:14.436
Dit kunnen we 'de tragedie
van de algoritmische meent' noemen.

00:08:14.460 --> 00:08:16.820
En het brengt nieuwe
uitdagingen met zich mee.

00:08:22.340 --> 00:08:24.236
Normaal gesproken, traditioneel,

00:08:24.260 --> 00:08:27.596
lossen we dit soort
sociale dilemmas op met regulaties,

00:08:27.620 --> 00:08:30.356
dus overheden of
gemeenschappen komen samen

00:08:30.380 --> 00:08:34.116
en beslissen samen
wat voor uitkomst men wil

00:08:34.140 --> 00:08:38.036
en wat voor limitaties
op individueel vlak daarvoor nodig zijn.

00:08:39.420 --> 00:08:42.036
En dan, door middel van
monitoring en handhaving,

00:08:42.060 --> 00:08:44.679
zorgen ze ervoor dat het
publieke goed beschermd wordt.

00:08:45.260 --> 00:08:48.045
Dus waarom vragen we regelgevers niet

00:08:48.379 --> 00:08:51.276
om te vereisen dat alle auto's
letsel verminderen.

00:08:51.300 --> 00:08:53.540
Uiteindelijk zegt men dat te willen.

00:08:55.020 --> 00:08:56.436
En nog belangrijker,

00:08:56.460 --> 00:08:59.556
als individu ben ik zeker dat,

00:08:59.580 --> 00:09:03.436
indien mijn auto mij opoffert
in een buitengewoon scenario,

00:09:03.460 --> 00:09:07.816
ik niet de enige ben die dat doet en dat 
anderen ongelimiteerd beschermd worden.

00:09:08.940 --> 00:09:12.270
In onze enquête vroegen we mensen
of ze regulatie zouden steunen

00:09:12.270 --> 00:09:13.810
en dit is waar we achter kwamen.

00:09:14.180 --> 00:09:17.940
Ten eerste, mensen
zeiden nee tegen regulatie.

00:09:19.100 --> 00:09:20.356
En ten tweede zegt men:

00:09:20.380 --> 00:09:24.316
"Als je wel de auto's reguleert
om letsel te verminderen,

00:09:24.340 --> 00:09:25.820
dan koop ik die autos niet."

00:09:27.220 --> 00:09:28.596
Ironisch:

00:09:28.620 --> 00:09:32.116
door auto's te reguleren
om letsel te verminderen,

00:09:32.140 --> 00:09:33.980
eindigen we misschien met meer letsel.

00:09:34.860 --> 00:09:38.516
Want mensen kiezen dan niet
voor de veiligere technologie,

00:09:38.540 --> 00:09:40.700
zelfs als die veel veiliger is dan mensen.

00:09:42.180 --> 00:09:45.596
Ik heb niet de oplossing
voor dit probleem,

00:09:45.620 --> 00:09:47.196
maar ik denk dat om te beginnen

00:09:47.220 --> 00:09:50.516
de maatschappij samen moet komen

00:09:50.540 --> 00:09:53.300
om te besluiten met welke trade-offs
we comfortabel zijn,

00:09:54.180 --> 00:09:57.660
en om manieren te bedenken
waarop we die kunnen afdwingen.

00:09:58.340 --> 00:10:00.910
Dus als een startpunt
hebben mijn briljante studenten

00:10:00.960 --> 00:10:03.320
Edmond Awad en Sohan Dsouza

00:10:03.380 --> 00:10:05.180
de Morele Machine website gebouwd,

00:10:06.020 --> 00:10:08.700
die random scenarios voor je maakt --

00:10:09.900 --> 00:10:12.356
simpelweg een hoop
random dilemma's op een rij

00:10:12.380 --> 00:10:16.300
waarbij jij moet kiezen wat de auto
moet doen in dat scenario.

00:10:16.860 --> 00:10:21.460
Hierbij variëren de leeftijden
en zelfs de diersoort van de slachtoffers.

00:10:22.860 --> 00:10:26.556
Tot nu toe hebben we meer
dan vijf miljoen beslissingen verzameld,

00:10:26.580 --> 00:10:28.780
door meer dan een miljoen mensen,

00:10:30.220 --> 00:10:31.420
via de website.

00:10:32.180 --> 00:10:34.596
Dit helpt ons een beeld te schetsen

00:10:34.620 --> 00:10:37.236
van het soort trade-offs
waar men comfortabel mee is

00:10:37.260 --> 00:10:39.156
en wat voor hen belangrijk is --

00:10:39.180 --> 00:10:41.020
zelfs voor verschillende culturen.

00:10:42.060 --> 00:10:43.556
Maar nog belangrijker,

00:10:43.580 --> 00:10:46.956
door dit te doen herkennen mensen

00:10:46.980 --> 00:10:49.796
hoe lastig het is
om dit soort keuzes te maken

00:10:49.820 --> 00:10:53.620
en dat regelgevers de taak hebben
om onmogelijke keuzes te maken.

00:10:55.180 --> 00:10:58.756
En misschien helpt dit ons
om verschillende trade-offs te begrijpen

00:10:58.780 --> 00:11:01.836
die geïmplementeerd
zullen worden in regulaties.

00:11:01.860 --> 00:11:03.596
En ik was dan ook blij om te horen

00:11:03.620 --> 00:11:05.636
dat de eerste regulaties

00:11:05.660 --> 00:11:07.796
van het ministerie van transport --

00:11:07.820 --> 00:11:09.196
vorige week aangekondigd --

00:11:09.220 --> 00:11:15.790
een 15-punts checklist
voor alle autofabrikanten bevatten,

00:11:15.790 --> 00:11:19.060
en nummer 14 was ethische overwegingen --

00:11:19.060 --> 00:11:21.380
hoe ben je van plan dit te implementeren?

00:11:23.540 --> 00:11:26.316
We vragen ook te reflecteren
op de eigen beslissingen

00:11:26.316 --> 00:11:29.300
door men een samenvatting
te geven van hun keuzes.

00:11:30.260 --> 00:11:31.916
Ik zal een voorbeeld geven --

00:11:31.940 --> 00:11:35.476
en ik waarschuw dat dit niet
een typisch voorbeeld is

00:11:35.500 --> 00:11:36.850
van een typische gebruiker.

00:11:36.850 --> 00:11:40.556
Dit zijn de figuren die het meest
opgeofferd and gered zijn door de persoon.

00:11:41.086 --> 00:11:45.740
(Gelach)

00:11:46.500 --> 00:11:48.396
Misschien ben je het met hem eens,

00:11:48.420 --> 00:11:50.060
of haar, dat weten we niet.

00:11:52.300 --> 00:11:58.436
Maar deze persoon lijkt een voorkeur te
hebben voor passagiers boven voetgangers

00:11:58.460 --> 00:12:00.556
in de keuzes

00:12:00.580 --> 00:12:03.396
en straft gevaarlijk oversteken af.

00:12:03.420 --> 00:12:06.460
(Gelach)

00:12:09.140 --> 00:12:10.019
Om af te ronden.

00:12:10.019 --> 00:12:13.795
We begonnen met de vraag --
noem 't het ethisch dilemma --

00:12:13.820 --> 00:12:16.876
wat moet een auto doen
in een bepaald scenario:

00:12:16.900 --> 00:12:18.100
uitwijken of doorrijden?

00:12:18.940 --> 00:12:21.796
Toen realiseerden we ons
dat het om een ander probleem ging.

00:12:21.820 --> 00:12:26.350
Namelijk: de gemeenschap zal het eens
moeten worden over wat ze wil

00:12:26.350 --> 00:12:28.336
en dit ook gaan afdwingen.

00:12:28.336 --> 00:12:29.646
Het is een sociaal dilemma.

00:12:29.646 --> 00:12:34.630
In de jaren 40 schreef Isaac Asimov
zijn bekende wetten van robotica,

00:12:34.630 --> 00:12:36.110
de drie wetten van de robotica.

00:12:37.060 --> 00:12:39.516
Een robot mag niet
een mens letsel toebrengen;

00:12:39.540 --> 00:12:42.076
een robot moet mensen gehoorzamen;

00:12:42.100 --> 00:12:45.356
een robot mag zichzelf
niet laten beschadigen.

00:12:45.380 --> 00:12:47.340
In afnemende mate van belangrijkheid.

00:12:48.180 --> 00:12:50.316
Maar na ongeveer 40 jaar

00:12:50.340 --> 00:12:54.076
en na zoveel verhalen
die deze wetten hebben beproefd,

00:12:54.100 --> 00:12:57.796
introduceerde Asimov de nulde wet,

00:12:57.820 --> 00:13:00.070
die belangrijker is dan alle andere.

00:13:00.070 --> 00:13:03.540
Deze zegt dat een robot niet de mensheid
als geheel letsel mag toebrengen.

00:13:04.300 --> 00:13:08.676
Ik weet niet wat dit betekent
in de context van zelfrijdende auto's

00:13:08.700 --> 00:13:11.410
of welke specifieke situatie dan ook.

00:13:11.410 --> 00:13:13.676
En ik weet niet hoe we
het kunnen implementeren,

00:13:13.700 --> 00:13:15.876
maar ik hoop dat door te erkennen

00:13:15.876 --> 00:13:21.396
dat de regulatie van zelfrijdende auto's
niet alleen een technisch probleem is,

00:13:21.420 --> 00:13:24.700
maar ook een sociaal-economisch probleem,

00:13:25.620 --> 00:13:28.500
we op zijn minst de juiste vragen
kunnen beginnen te stellen.

00:13:29.020 --> 00:13:30.236
Dank u zeer.

00:13:30.260 --> 00:13:33.180
(Applaus)

