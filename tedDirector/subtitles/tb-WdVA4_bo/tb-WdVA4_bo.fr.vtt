WEBVTT
Kind: captions
Language: fr

00:00:00.000 --> 00:00:07.000
Traducteur: Mariana Ciric
Relecteur: eric vautier

00:00:12.820 --> 00:00:16.900
Aujourd'hui, je vais parler 
de technologie et de société.

00:00:18.860 --> 00:00:22.556
Le Département du Transport a estimé
que, l'année dernière,

00:00:22.580 --> 00:00:26.660
le nombre de morts dû aux accidents de la
circulation à 35 000, seulement aux USA.

00:00:27.860 --> 00:00:32.660
Au niveau mondial, c'est 1,2 million de
gens qui meurent chaque année.

00:00:33.580 --> 00:00:37.676
S'il y avait une possibilité d'éliminer
90% de ces accidents,

00:00:37.700 --> 00:00:38.900
la soutiendriez-vous ?

00:00:39.540 --> 00:00:40.836
Bien sûr que oui.

00:00:40.860 --> 00:00:44.515
La technologie des voitures sans 
conducteur promet d'y arriver

00:00:44.540 --> 00:00:47.356
en éliminant la principale source 
d'accidents--

00:00:47.380 --> 00:00:48.580
l'erreur humaine.

00:00:49.740 --> 00:00:55.156
Imaginez vous, en 2030, dans une voiture 
sans conducteur,

00:00:55.180 --> 00:00:58.636
regardant cette ancienne vidéo TED.

00:00:58.660 --> 00:01:00.660
(Rires)

00:01:01.340 --> 00:01:02.556
Quand tout à coup,

00:01:02.580 --> 00:01:05.860
une panne mécanique l'empêche
de s'arrêter.

00:01:07.180 --> 00:01:08.700
Si la voiture roule toujours,

00:01:09.540 --> 00:01:13.660
elle peut écraser les piétons qui sont
en train de traverser;

00:01:14.900 --> 00:01:17.035
mais elle peut aussi faire une embardée,

00:01:17.059 --> 00:01:18.916
touchant un passant en le tuant

00:01:18.940 --> 00:01:21.020
pour sauver les piétons.

00:01:21.860 --> 00:01:24.460
Que devrait faire la voiture
et qui devrait décider ?

00:01:25.340 --> 00:01:28.876
Et si la voiture fonçait dans un mur,

00:01:28.900 --> 00:01:32.196
vous tuant vous, le passager

00:01:32.220 --> 00:01:34.540
pour sauver ces piétons ?

00:01:35.060 --> 00:01:38.140
Ce scénario est inspiré du problème 
venant des trolleybus,

00:01:38.780 --> 00:01:42.556
inventé par des philosophes il y a 
quelques décennies

00:01:42.580 --> 00:01:43.820
pour penser à l'éthique.

00:01:45.940 --> 00:01:48.436
Il est important de savoir ce que nous
en pensons.

00:01:48.460 --> 00:01:51.076
Peut être que nous n'y pensons
pas du tout.

00:01:51.100 --> 00:01:54.476
On peut trouver ce scénario irréel,

00:01:54.500 --> 00:01:56.820
extrêmement peu probable ou juste stupide.

00:01:57.580 --> 00:02:00.316
Je pense que cette critique manque
d'un point important

00:02:00.340 --> 00:02:02.500
car elle suit ce scénario à la lettre.

00:02:03.740 --> 00:02:06.476
Il est évident qu'aucun accident
ne ressemblera à ça,

00:02:06.500 --> 00:02:09.836
aucun accident n'a deux ou trois options

00:02:09.860 --> 00:02:11.860
où tout le monde finit par mourir.

00:02:13.300 --> 00:02:15.876
Pour contrer tout cela, la voiture
pourrait calculer

00:02:15.900 --> 00:02:20.796
la probabilité d'heurter
un groupe de personnes,

00:02:20.820 --> 00:02:24.156
si on dévie d'un côté ou d'un autre,

00:02:24.180 --> 00:02:27.636
on augmenterait le risque des passagers
ou des conducteurs

00:02:27.660 --> 00:02:29.196
versus les piétons.

00:02:29.220 --> 00:02:31.380
Il s'agit d'un calcul un peu plus 
complexe,

00:02:32.300 --> 00:02:34.820
mais cela impliquerait quand même
des compromis,

00:02:35.660 --> 00:02:38.540
et les compromis demandent souvent 
de l'éthique.

00:02:39.660 --> 00:02:42.396
On pourrait donc dire :
« Ne nous en préoccupons pas.

00:02:42.420 --> 00:02:47.060
Attendons à ce que la technologie soit 
prête et sûre à 100%. »

00:02:48.340 --> 00:02:52.020
Imaginez qu'on puisse éliminer 90%
de ces accidents,

00:02:52.900 --> 00:02:55.740
ou même 99% dans les dix prochaines 
années.

00:02:56.740 --> 00:02:59.916
Et si l'élimination du dernier 1%
des accidents

00:02:59.940 --> 00:03:03.060
demandait plus de 50 ans de recherches ?

00:03:04.220 --> 00:03:06.556
Ne devrions-nous pas
adopter cette technologie ?

00:03:06.556 --> 00:03:11.316
Ce sont 60 millions de morts à cause 
des accidents de voiture

00:03:11.340 --> 00:03:13.100
si nous maintenons les taux actuels.

00:03:14.580 --> 00:03:15.796
Le fait est que,

00:03:15.820 --> 00:03:19.436
attendre pour une sécurité optimale 
est aussi un choix

00:03:19.460 --> 00:03:21.620
mais cela implique aussi des compromis.

00:03:23.380 --> 00:03:27.716
Les gens ont trouvé plusieurs manières,
sur les réseaux sociaux,

00:03:27.740 --> 00:03:29.756
pour éviter de penser à ce problème.

00:03:29.780 --> 00:03:32.996
Une personne a suggéré que la voiture 
devrait pouvoir dévier

00:03:33.020 --> 00:03:35.156
entre les piétons--

00:03:35.180 --> 00:03:36.196
(Rires)

00:03:36.220 --> 00:03:37.476
et les passants.

00:03:37.500 --> 00:03:40.900
Si la voiture avait les capacités 
de le faire, elle le ferait sûrement.

00:03:41.740 --> 00:03:44.580
Nous sommes intéressés
par d'autres scénarios.

00:03:45.100 --> 00:03:50.516
Mon préféré est une suggestion 
de la part d'un blogueur,

00:03:50.540 --> 00:03:53.556
qui est d'avoir un bouton pour
siège éjectable--

00:03:53.580 --> 00:03:54.796
(Rires)

00:03:54.820 --> 00:03:56.487
juste avant l'autodestruction.

00:03:56.511 --> 00:03:58.191
(Rires)

00:03:59.660 --> 00:04:04.860
Si nous considérons que les voitures
devront aussi faire des compromis,

00:04:06.020 --> 00:04:07.900
comment y penser,

00:04:09.140 --> 00:04:10.716
mais surtout comment se décider ?

00:04:10.740 --> 00:04:13.876
Peut être devrions-nous faire un sondage
auprès de la société,

00:04:13.900 --> 00:04:15.356
car finalement,

00:04:15.380 --> 00:04:19.340
les règlements et la loi sont les reflets
des valeurs sociétales.

00:04:19.860 --> 00:04:21.100
Nous avons donc fait ceci.

00:04:21.700 --> 00:04:23.363
En compagnie de mes collaborateurs,

00:04:23.363 --> 00:04:25.676
Jean-François Bonnefon et Azim Shariff,

00:04:25.700 --> 00:04:27.316
nous avons mené une enquête

00:04:27.340 --> 00:04:30.195
où nous avons présenté ces scénarios
aux gens.

00:04:30.219 --> 00:04:33.996
Nous leur avons donné deux options, 
inspirées par deux philosophes :

00:04:34.020 --> 00:04:36.660
Jeremy Bentham et Emmanuel Kant.

00:04:37.420 --> 00:04:40.516
Selon Bentham, la voiture devrait suivre
l'éthique utilitariste :

00:04:40.540 --> 00:04:43.956
son action devrait être celle qui réduit
tous les dommages --

00:04:43.980 --> 00:04:46.796
même si cette action finira par tuer 
un passant

00:04:46.820 --> 00:04:49.260
ou même si elle finira par tuer 
le passager.

00:04:49.940 --> 00:04:54.916
Selon Kant, la voiture devrait agir selon
certains commandements,

00:04:54.940 --> 00:04:56.500
comme  « Tu ne tueras point. »

00:04:57.300 --> 00:05:01.756
Ton action ne devrait donc pas nuire à
un être humain de manière explicite,

00:05:01.780 --> 00:05:04.236
et tu devrais laisser la voiture faire
sa route

00:05:04.260 --> 00:05:06.220
même si plus de gens finiront blessés.

00:05:07.460 --> 00:05:08.660
Qu'en pensez-vous ?

00:05:09.180 --> 00:05:10.700
Bentham ou bien Kant ?

00:05:11.580 --> 00:05:12.836
Voici notre solution.

00:05:12.860 --> 00:05:14.660
La plupart étaient pour Bentham.

00:05:15.980 --> 00:05:19.756
Les gens veulent donc que la voiture
soit utilitariste,

00:05:19.780 --> 00:05:21.196
réduire les dommages,

00:05:21.220 --> 00:05:22.796
ce que nous devrions tous faire.

00:05:22.820 --> 00:05:24.020
Problème résolu.

00:05:25.060 --> 00:05:26.540
Mais il y a un petit piège.

00:05:27.740 --> 00:05:31.476
Lorsque nous leur avons demandé
s'ils achèteraient ces voitures,

00:05:31.500 --> 00:05:33.348
ils ont répondu : « Absolument pas. »

00:05:33.348 --> 00:05:35.436
(Rires)

00:05:35.460 --> 00:05:39.356
Ils aimeraient acheter des voitures qui
les protègent à tout prix,

00:05:39.380 --> 00:05:42.996
mais ils veulent que tous les autres les
achètent pour réduire les dommages.

00:05:43.020 --> 00:05:45.540
(Rires)

00:05:46.540 --> 00:05:48.396
Nous connaissons ce type de problème.

00:05:48.420 --> 00:05:49.980
C'est un dilemme social.

00:05:50.980 --> 00:05:52.796
Pour comprendre ce dilemme social,

00:05:52.820 --> 00:05:54.860
nous devons remonter un peu dans le temps.

00:05:55.820 --> 00:05:58.396
Dans les années 1800,

00:05:58.420 --> 00:06:02.156
l'économiste anglais William Forster Lloyd
a publié un pamphlet

00:06:02.180 --> 00:06:04.396
décrivant le scénario suivant.

00:06:04.420 --> 00:06:06.076
Vous avez un groupe de fermiers--

00:06:06.100 --> 00:06:07.436
fermiers anglais--

00:06:07.460 --> 00:06:10.140
qui se partagent des terres
pour leurs moutons.

00:06:11.340 --> 00:06:13.916
Si chaque fermier ramène un certain nombre
de moutons--

00:06:13.940 --> 00:06:15.436
disons trois moutons--

00:06:15.460 --> 00:06:17.556
la terre sera régénérée,

00:06:17.580 --> 00:06:18.796
les fermiers heureux,

00:06:18.820 --> 00:06:20.436
les moutons heureux,

00:06:20.460 --> 00:06:21.660
tout est en ordre.

00:06:22.260 --> 00:06:24.780
Maintenant si un fermier en ramène un
de plus,

00:06:25.620 --> 00:06:30.340
il s'en sortira un peu mieux,
au détriment de personne.

00:06:30.980 --> 00:06:34.620
Mais si chaque fermier faisait de même,

00:06:35.660 --> 00:06:38.380
la terre serait surexploitée et appauvrie

00:06:39.180 --> 00:06:41.356
au détriment de tous les fermiers

00:06:41.380 --> 00:06:43.500
et bien sûr à celui des moutons.

00:06:44.540 --> 00:06:48.220
Nous rencontrons ce problème
à plusieurs endroits :

00:06:48.900 --> 00:06:52.076
dans la difficulté à gérer la surpêche,

00:06:52.100 --> 00:06:56.660
ou à réduire les émissions de carbone pour
atténuer le changement climatique.

00:06:58.980 --> 00:07:01.900
Quand il s'agit du règlement des voitures
sans conducteur,

00:07:02.900 --> 00:07:07.236
la terre commune représente
la sécurité publique --

00:07:07.260 --> 00:07:08.500
c'est le bien commun --

00:07:09.220 --> 00:07:11.196
et les fermiers sont les passagers

00:07:11.220 --> 00:07:14.820
ou les propriétaires de ces voitures qui 
décident de les conduire.

00:07:16.780 --> 00:07:19.396
En faisant ce choix rationnel

00:07:19.420 --> 00:07:22.236
de prioriser leur propre sécurité,

00:07:22.260 --> 00:07:25.396
ils sont peut être en train d'affaiblir
le bien commun,

00:07:25.420 --> 00:07:27.620
de réduire donc les dommages.

00:07:30.140 --> 00:07:32.276
C'est la Tragédie des biens communs,

00:07:32.300 --> 00:07:33.596
typiquement,

00:07:33.620 --> 00:07:36.716
mais je pense que dans le cas des voitures
sans conducteur,

00:07:36.740 --> 00:07:39.596
le problème est peut-être
un peu plus insidieux

00:07:39.620 --> 00:07:43.116
car ce n'est pas forcément
un seul être humain

00:07:43.140 --> 00:07:44.836
qui prend ces décisions.

00:07:44.860 --> 00:07:48.156
Les fabricants de voitures pourraient 
simplement les programmer

00:07:48.180 --> 00:07:50.700
pour maximiser la sécurité des clients,

00:07:51.900 --> 00:07:54.876
et elles pourraient apprendre
par elles-mêmes

00:07:54.900 --> 00:07:58.532
qu'en agissant de cette manière, elles
augmenteraient le risque des piétons.

00:07:59.340 --> 00:08:00.756
Revenons à notre métaphore,

00:08:00.780 --> 00:08:04.396
c'est comme si maintenant nous avions
des moutons électriques conscients.

00:08:04.420 --> 00:08:05.876
(Rires)

00:08:05.900 --> 00:08:08.980
Ils peuvent brouter sans que le fermier
le sache.

00:08:10.460 --> 00:08:14.436
C'est ce qu'on pourrait appeler la 
Tragédie des communs algorithmiques,

00:08:14.460 --> 00:08:16.820
qui offre de nouveaux types de défis.

00:08:22.340 --> 00:08:24.236
De manière générale, typiquement,

00:08:24.260 --> 00:08:27.596
nous résolvons ces types de dilemmes 
sociaux grâce aux lois :

00:08:27.620 --> 00:08:30.356
soit les gouvernements soit
les communautés se réunissent

00:08:30.380 --> 00:08:34.116
pour décider ensemble quel est
le résultat voulu

00:08:34.140 --> 00:08:36.796
et quelles sortes de contraintes liées
au comportement

00:08:36.820 --> 00:08:38.020
doivent être appliquées.

00:08:39.420 --> 00:08:42.036
Ensuite grâce à la surveillance
et au contrôle,

00:08:42.060 --> 00:08:44.619
ils peuvent s'assurer que le bien civil
soit préservé.

00:08:45.260 --> 00:08:46.835
Pourquoi ne pourrions-nous pas,

00:08:46.859 --> 00:08:48.355
en tant que régulateurs,

00:08:48.379 --> 00:08:51.276
exiger à ce que toutes les voitures
réduisent les dommages ?

00:08:51.300 --> 00:08:53.540
Après tout, c'est ce que veulent les gens.

00:08:55.020 --> 00:08:56.436
Plus important encore,

00:08:56.460 --> 00:08:59.556
je peux être sûr qu'en tant qu'individu,

00:08:59.580 --> 00:09:03.436
si j'achète une voiture pouvant
me sacrifier dans un cas très rare,

00:09:03.460 --> 00:09:05.116
je ne vais pas être le seul pigeon

00:09:05.140 --> 00:09:07.820
à le faire alors que tous les autres
sont protégés.

00:09:08.940 --> 00:09:12.276
Dans notre enquête, nous avons demandé
s'ils valideraient un telle loi

00:09:12.300 --> 00:09:13.500
et voici le résultat.

00:09:14.180 --> 00:09:17.940
Tout d'abord, les gens ont répondu
non à la loi ;

00:09:19.100 --> 00:09:20.356
ensuite, ils ont dit :

00:09:20.380 --> 00:09:24.316
« Si vous légiférer pour que les voitures
réduisent les dommages,

00:09:24.340 --> 00:09:25.820
je ne les achèterais pas. »

00:09:27.220 --> 00:09:28.596
Donc ironiquement,

00:09:28.620 --> 00:09:32.116
en réglant les voitures pour réduire
les dommages,

00:09:32.140 --> 00:09:33.980
ça pourrait être pire

00:09:34.860 --> 00:09:38.516
car les gens n'opteraient pas pour
la technologie plus sécurisée

00:09:38.540 --> 00:09:40.620
même si c'est plus sûr qu'un conducteur.

00:09:42.180 --> 00:09:45.596
Je n'ai pas la réponse finale de
cette énigme,

00:09:45.620 --> 00:09:47.196
mais pour commencer,

00:09:47.220 --> 00:09:50.516
notre société devrait s'unir

00:09:50.540 --> 00:09:53.300
pour décider quels compromis
nous correspondent le plus

00:09:54.180 --> 00:09:57.660
et trouver des moyens pour les appliquer.

00:09:58.340 --> 00:10:00.876
Pour commencer, mes brillants étudiants,

00:10:00.900 --> 00:10:03.356
Edmond Awad et Sohan De Souza

00:10:03.380 --> 00:10:05.180
ont créé le site Moral Machine;

00:10:06.020 --> 00:10:08.700
celui-ci génère des scénarios aléatoires--

00:10:09.900 --> 00:10:12.356
une série de dilemmes à la chaîne

00:10:12.380 --> 00:10:16.300
où vous devez choisir ce que ferait 
la voiture dans ces cas.

00:10:16.860 --> 00:10:21.460
Nous varions les âges et la nature 
des différentes victimes.

00:10:22.860 --> 00:10:26.556
Jusqu'à présent, nous avons récolté
plus de 5 millions de décisions

00:10:26.580 --> 00:10:28.780
prises par plus d'un million de gens

00:10:30.220 --> 00:10:31.420
depuis le site internet.

00:10:32.180 --> 00:10:34.596
Ceci nous aide à avoir une idée

00:10:34.620 --> 00:10:37.236
de quels compromis les gens sont prêts
à accepter

00:10:37.260 --> 00:10:39.156
et ce qui compte pour eux--

00:10:39.180 --> 00:10:40.620
même à travers les cultures.

00:10:42.060 --> 00:10:43.556
Mais surtout,

00:10:43.580 --> 00:10:46.956
ces exercices aident les gens
à reconnaître

00:10:46.980 --> 00:10:49.796
la difficulté que représente le fait
de faire un choix

00:10:49.820 --> 00:10:53.620
et que les lesgilateurs sont confrontés
à des choix impossibles.

00:10:55.180 --> 00:10:58.756
Peut-être que ceci nous aidera
à comprendre, en tant que société,

00:10:58.780 --> 00:11:01.836
quels compromis seront mis en place
dans la loi.

00:11:01.860 --> 00:11:03.596
En effet, j'ai été très heureux

00:11:03.620 --> 00:11:05.636
d'entendre que la première série

00:11:05.660 --> 00:11:07.796
venue du Département des Transports--

00:11:07.820 --> 00:11:09.196
annoncée la semaine dernière

00:11:09.220 --> 00:11:15.796
a inclus une liste de 15 points à remplir
par tous les fabricants de voitures.

00:11:15.820 --> 00:11:19.076
Le numéro 14 était la consideration 
éthique --

00:11:19.100 --> 00:11:20.820
comment allez-vous gérer ça ?

00:11:23.620 --> 00:11:26.276
Ces personnes peuvent aussi réfléchir
à leurs décisions

00:11:26.300 --> 00:11:29.300
en recevant des résumés à propos
de leurs choix.

00:11:30.260 --> 00:11:31.916
Je vais vous donner un exemple --

00:11:31.940 --> 00:11:35.476
je vous avertis juste que ceci n'est pas
un exemple typique,

00:11:35.500 --> 00:11:36.876
ni un utilisateur typique.

00:11:36.900 --> 00:11:40.516
Voici ce que la personne a sauvé
et a tué le plus.

00:11:40.540 --> 00:11:45.740
(Rires)

00:11:46.500 --> 00:11:48.396
Certains d'entre vous allez être

00:11:48.420 --> 00:11:50.060
d'accord avec lui ou avec elle.

00:11:52.300 --> 00:11:58.436
Cette personne a aussi l'air de préférer
les passagers aux piétons

00:11:58.460 --> 00:12:00.556
dans ses choix

00:12:00.580 --> 00:12:03.996
et est très heureuse de punir
ceux qui traversent en dehors des clous.

00:12:03.996 --> 00:12:06.460
(Rires)

00:12:09.140 --> 00:12:10.356
Résumons.

00:12:10.379 --> 00:12:13.795
Nous avons commencé avec une question --
un dilemme éthique --

00:12:13.820 --> 00:12:16.876
de ce que devrait faire la voiture dans
un scénario précis :

00:12:16.900 --> 00:12:18.100
dévier ou rester ?

00:12:19.060 --> 00:12:21.796
Mais nous avons réalisé que
le problème était autre.

00:12:21.820 --> 00:12:26.356
Le problème était de comment faire pour
que la société accepte et applique

00:12:26.380 --> 00:12:28.316
les compromis qui lui conviennent.

00:12:28.340 --> 00:12:29.596
C'est un dilemme social.

00:12:29.620 --> 00:12:34.636
Dans les années 1940, Isaac Asimov a écrit
sa célèbre loi de la robotique --

00:12:34.660 --> 00:12:35.980
les trois règles de la robotique.

00:12:37.060 --> 00:12:39.516
Un robot ne nuira pas à un être humain,

00:12:39.540 --> 00:12:42.076
un robot ne désobéira pas

00:12:42.100 --> 00:12:45.356
et il ne s'autorisera pas
à faire au mal --

00:12:45.380 --> 00:12:47.340
dans cet ordre d'importance.

00:12:48.180 --> 00:12:50.316
Mais environ 40 ans plus tard

00:12:50.340 --> 00:12:54.076
et après tellement de scénarios qui ont
testé ces règles,

00:12:54.100 --> 00:12:57.796
Asimov a présenté la loi Zéro

00:12:57.820 --> 00:13:00.076
qui a préséance avant tout.

00:13:00.100 --> 00:13:03.380
Un robot ne devrait pas nuire
à l'Humanité.

00:13:04.300 --> 00:13:08.676
Je ne sais pas ce que cela représente dans
le cas des voitures sans conducteur

00:13:08.700 --> 00:13:11.436
ou dans une situation spécifique,

00:13:11.460 --> 00:13:13.676
et je ne sais pas comment
la mettre en œuvre ;

00:13:13.700 --> 00:13:15.236
mais en admettant que

00:13:15.260 --> 00:13:21.396
la législation de ces voitures n'est pas 
uniquement un problème technique

00:13:21.420 --> 00:13:24.700
mais aussi un problème de coopération
sociétale, j'espère que

00:13:25.620 --> 00:13:28.500
nous puissions au moins nous poser
les bonnes questions.

00:13:29.020 --> 00:13:30.236
Merci.

00:13:30.260 --> 00:13:33.180
(Applaudissements)

