WEBVTT
Kind: captions
Language: ar

00:00:00.000 --> 00:00:07.000
المترجم: Lalla Khadija Tigha
المدقّق: Riyad Almubarak

00:00:12.820 --> 00:00:16.900
سأتحدث اليوم عن التكنولوجيا والمجتمع.

00:00:18.860 --> 00:00:22.556
قدّرت هيئة المواصلات أنه في السنة الماضية

00:00:22.580 --> 00:00:26.660
توفي 35000 شخص إثر حوادث سير
في الولايات المتحدة وحدها.

00:00:27.860 --> 00:00:32.660
على مستوى العالم، يموت 1.2 مليون شخص
سنويًا إثر حوادث سير.

00:00:33.580 --> 00:00:37.676
إن كانت هناك طريقة تستطيع تجنب 90 في المئة
من هذه الحوادث،

00:00:37.700 --> 00:00:38.900
هل كنت لتدعمها؟

00:00:39.540 --> 00:00:40.836
بالطبع ستفعل.

00:00:40.860 --> 00:00:44.515
هذا ما تطمح السيارات ذاتية القيادة
للوصول إليه

00:00:44.540 --> 00:00:47.356
عن طريق حذف السبب الرئيس للحوادث --

00:00:47.380 --> 00:00:48.580
وهو الخطأ البشري.

00:00:49.740 --> 00:00:55.156
تخيل نفسك الآن في سيارة ذاتية القيادة
سنة 2030،

00:00:55.180 --> 00:00:58.636
وأنت تجلس في الخلف 
وتشاهد هذا الشريط القديم ل TEDxCambridge.

00:00:58.660 --> 00:01:00.660
(ضحك)

00:01:01.340 --> 00:01:02.556
وفجأة،

00:01:02.580 --> 00:01:05.860
تتعرض السيارة لخلل ميكانيكي
ولا تستطيع التوقف.

00:01:07.180 --> 00:01:08.700
إن استمرت السيارة في التقدم،

00:01:09.540 --> 00:01:13.660
ستصطدم بمجموعة من الراجلين
وهم يقطعون الطريق،

00:01:14.900 --> 00:01:17.035
لكن السيارة تنحرف،

00:01:17.059 --> 00:01:18.916
لتصطدم بشخص واقف،

00:01:18.940 --> 00:01:21.020
لتقتله وتنقذ الراجلين.

00:01:21.860 --> 00:01:24.460
ما الذي يجب على السيارة فعله،
ومن عليه أن يقرر؟

00:01:25.340 --> 00:01:28.876
ماذا لو انحرفت السيارة في اتجاه الحائط

00:01:28.900 --> 00:01:32.196
لتسحقك وتقتلك، أنت الراكب فيها،

00:01:32.220 --> 00:01:34.540
لتنقذ هؤلاء الراجلين؟

00:01:35.060 --> 00:01:38.140
تم استلهام هذا السيناريو
من مشكلة الترولي،

00:01:38.780 --> 00:01:42.556
والتي تم اختراعها من طرف فلاسفة
قبل عقود قليلة

00:01:42.580 --> 00:01:43.820
للتفكير حول الأخلاقيات.

00:01:45.940 --> 00:01:48.436
الآن، الطريقة التي نفكر بها
في هذا المشكل مهمة.

00:01:48.460 --> 00:01:51.076
يمكننا مثلًا ألا نفكر فيه إطلاقًا.

00:01:51.100 --> 00:01:54.476
يمكننا أن نقول أن هذا السيناريو غير واقعي،

00:01:54.500 --> 00:01:56.820
غير ممكن بتاتًا، أو أنه تافه.

00:01:57.580 --> 00:02:00.316
لكن أعتقد أن هذا الانتقاد يغفل المغزى

00:02:00.340 --> 00:02:02.500
لأنه ينظر إلى السيناريو بطريقة بسيطة.

00:02:03.740 --> 00:02:06.476
بالطبع، لايمكن لحادثة أن تكون هكذا،

00:02:06.500 --> 00:02:09.836
ليس هناك احتمالان أو ثلاثة لحادثة

00:02:09.860 --> 00:02:11.860
حيث يموت الجميع بطريقة أو بأخرى.

00:02:13.300 --> 00:02:15.876
عوضا عن ذلك، ستقوم السيارة بحساب ما

00:02:15.900 --> 00:02:20.796
مثل احتمال الاصطدام
بمجموعة معينة من الناس،

00:02:20.820 --> 00:02:24.156
حين تنحرف إلى اتجاه دون آخر،

00:02:24.180 --> 00:02:27.636
يمكن أن ترفع الخطر بشكل ضئيل
على الركاب أو السائقين الآخرين

00:02:27.660 --> 00:02:29.196
في مقابل الراجلين.

00:02:29.220 --> 00:02:31.380
ستكون عبارة عن حسابات معقدة،

00:02:32.300 --> 00:02:34.820
لكنها ستشمل دائمًا نوعا من المقايضة،

00:02:35.660 --> 00:02:38.540
وغالبا مما تتطلب المقايضة حضور الأخلاقيات.

00:02:39.660 --> 00:02:42.396
يمكننا أن نقول حينها، "حسنا، 
لمَ لا نترك القلق جانيًا.

00:02:42.420 --> 00:02:47.060
فلننتظر حتى تصير التكنولوجيا جاهزة
وآمنة 100 بالمئة."

00:02:48.340 --> 00:02:52.020
لنفترض أننا فعلا نستطيع حذف 90 في المئة
من هذه الحوادث،

00:02:52.900 --> 00:02:55.740
أو حتى 99 في المئة منها
خلال 10 سنوات المقبلة.

00:02:56.740 --> 00:02:59.916
ماذا لو كان يتطلب حذف العشر في المئة
المتبقية من الحوادث

00:02:59.940 --> 00:03:03.060
50 سنة أخرى من البحث؟

00:03:04.220 --> 00:03:06.020
أليس علينا أن نتبنى التكنولوجيا؟

00:03:06.540 --> 00:03:11.316
سيكون لدينا 60 مليون حالة وفاة
من حوادث السير

00:03:11.340 --> 00:03:13.100
إذا حافظنا على المعدل الحالي.

00:03:14.580 --> 00:03:15.796
فالفكرة هنا،

00:03:15.820 --> 00:03:19.436
التريث حتى نصل
للسلامة الكاملة هو اختيار أيضا،

00:03:19.460 --> 00:03:21.620
ويشمل مقايضات أيضا.

00:03:23.380 --> 00:03:27.716
قام الناس على مواقع التواصل الاجتماعية
باختلاق كل الطرق الممكنة

00:03:27.740 --> 00:03:29.756
لعدم التفكير في هذا المشكل.

00:03:29.780 --> 00:03:32.996
اقترح شخص أن على السيارة
أن تنحرف في اتجاه ما فحسب

00:03:33.020 --> 00:03:35.156
بين الركاب --

00:03:35.180 --> 00:03:36.196
(ضحك)

00:03:36.220 --> 00:03:37.476
والشخص الواقف.

00:03:37.500 --> 00:03:40.860
بالطبع، إن كانت السيارة تستطبع فعل ذلك،
فعليها فعل ذلك.

00:03:41.740 --> 00:03:44.580
نحن مهتمون بإيجاد سيناريوهات
حيث لا يكون هذا ممكنا.

00:03:45.100 --> 00:03:50.516
والاقتراح المفضل لدي شخصيا كان من مدون

00:03:50.540 --> 00:03:53.556
بأن تضع زر إخراج في السيارة، 
حيث يمكنك الضغط عليه --

00:03:53.580 --> 00:03:54.796
(ضحك)

00:03:54.820 --> 00:03:56.487
قبل أن تدمر السيارة نفسها.

00:03:56.511 --> 00:03:58.191
(ضحك)

00:03:59.660 --> 00:04:04.860
إذن حين نعترف أنه من الممكن للسيارات
أن تقوم بمقايضات على الطريق،

00:04:06.020 --> 00:04:07.900
كيف يمكننا التفكير حيال هذه المقايضات،

00:04:09.140 --> 00:04:10.716
وكيف لنا أن نقرر؟

00:04:10.740 --> 00:04:13.876
حسنا، ربما علينا إجراء استقصاء
لنعرف ما الذي يريده المجتمع،

00:04:13.900 --> 00:04:15.356
لأنه في النهاية،

00:04:15.380 --> 00:04:19.340
القوانين هي انعكاس للقيم المجتمعية.

00:04:19.860 --> 00:04:21.100
إذن فهذا ما قمنا به.

00:04:21.700 --> 00:04:23.316
مع زملائي،

00:04:23.340 --> 00:04:25.676
جون فرونسوا بونفون وأزيم شريف،

00:04:25.700 --> 00:04:27.316
أجرينا استقصاءً

00:04:27.340 --> 00:04:30.195
حيث قدمنا للناس هذه الأنواع
من السيناريوهات.

00:04:30.219 --> 00:04:33.996
قدمنا لهم احتمالين مستلهمين
من فيلسوفين:

00:04:34.020 --> 00:04:36.660
جيرمي بينتام وإيمانويل كانت.

00:04:37.420 --> 00:04:40.516
يقول بينتام أن على السيارة 
أن تتبع الأخلاقيات النفعية:

00:04:40.540 --> 00:04:43.956
عليها أن تخطو الخطوة
التي ستقلل مجموع الضرر --

00:04:43.980 --> 00:04:46.796
حتى وإن كانت ستقتل الرجل الواقف

00:04:46.820 --> 00:04:49.260
حتى وإن كانت هذه الخطوة ستقتل الراكب.

00:04:49.940 --> 00:04:54.916
يقول إيمانويل كارت أن على السيارة
أن تتبع مبادئ الواجب الملزم،

00:04:54.940 --> 00:04:56.500
"عليك ألا تقتل."

00:04:57.300 --> 00:05:01.756
إذن فعليك ألا تتخذ خطوة ستضر بها
عنوة شخصا ما،

00:05:01.780 --> 00:05:04.236
وعليك أن تترك السيارة تأخذ مسارها

00:05:04.260 --> 00:05:06.220
حتى وإن كانت ستلحق الضرر بأشخاص أكثر.

00:05:07.460 --> 00:05:08.660
ماذا تعتقدون؟

00:05:09.180 --> 00:05:10.700
بيتم أو كانت؟

00:05:11.580 --> 00:05:12.836
إليكم ماوجدنا.

00:05:12.860 --> 00:05:14.660
معظم الناس وقفوا إلى جانب بينتام.

00:05:15.980 --> 00:05:19.756
لذا يبدو أن الناس يريدون
أن تكون السيارات نفعية،

00:05:19.780 --> 00:05:21.196
وتقلل مجموع الضرر،

00:05:21.220 --> 00:05:22.796
وهذا ماعلينا أن نقوم به جميعا.

00:05:22.820 --> 00:05:24.020
تم حل المشكلة.

00:05:25.060 --> 00:05:26.540
لكن هناك نقطة صغيرة.

00:05:27.740 --> 00:05:31.476
حين سألنا الناس إن كانوا سيشترون
مثل هذه السيارات،

00:05:31.500 --> 00:05:33.116
قالوا "بالطبع لا."

00:05:33.140 --> 00:05:35.436
(ضحك)

00:05:35.460 --> 00:05:39.356
هم يريدون أن يشتروا سيارات تحميهم بأي ثمن،

00:05:39.380 --> 00:05:42.996
لكنهم يريدون أن يشتري الآخرون سيارات
تقلل من الضرر.

00:05:43.020 --> 00:05:45.540
(ضحك)

00:05:46.540 --> 00:05:48.396
لقد رأينا هذا المشكل من قبل.

00:05:48.420 --> 00:05:49.980
وهو يدعى المعضلة الاجتماعية.

00:05:50.980 --> 00:05:52.796
ولفهم المعضلة الاجتماعية

00:05:52.820 --> 00:05:54.860
علينا أن نعود قليلا إلى التاريخ.

00:05:55.820 --> 00:05:58.396
في القرن 19،

00:05:58.420 --> 00:06:02.156
نشر خبير الاقتصاد البريطاني
ويليام فورستر لويد كتيبا

00:06:02.180 --> 00:06:04.396
يصف فيه السيناريو الآتي.

00:06:04.420 --> 00:06:06.076
لديك مجموعة من المزارعين --

00:06:06.100 --> 00:06:07.436
مزارعون بريطانيون --

00:06:07.460 --> 00:06:10.140
يتقاسمون أرضا مشتركة لترعى بها أغنامهم.

00:06:11.340 --> 00:06:13.916
الآن، إذا قام كل مزارع بجلب
عدد معين من الأغنام --

00:06:13.940 --> 00:06:15.436
لنقل ثلاثة أغنام --

00:06:15.460 --> 00:06:17.556
ستزدهر الأرض،

00:06:17.580 --> 00:06:18.796
وسيفرح المزارعون،

00:06:18.820 --> 00:06:20.436
وستفرح الأغنام،

00:06:20.460 --> 00:06:21.660
كل شيء على ما يرام.

00:06:22.260 --> 00:06:24.780
الآن، إذا قام مزارع بجلب خروف إضافي،

00:06:25.620 --> 00:06:30.340
سيكون ذلك المزارع ناجحا بقدر يسير،
ولن يتضرر أحد من هذا.

00:06:30.980 --> 00:06:34.620
لكن إن أخذ كل مزارع 
هذا القرار الفردي المنطقي،

00:06:35.660 --> 00:06:38.380
سيتم استغلال الأرض بشكل جائر
وستُستنزف

00:06:39.180 --> 00:06:41.356
ولن يستفيد المزارعون جميعهم.

00:06:41.380 --> 00:06:43.500
وبالتأكيد، لن تستفيد الأغنام.

00:06:44.540 --> 00:06:48.220
نرى هذا المشكل في كل مكان:

00:06:48.900 --> 00:06:52.076
في صعوبة إدلرة الصيد الجائر،

00:06:52.100 --> 00:06:56.660
أو تقليل انبعاث الكربون
للتعامل مع التغير المناخي.

00:06:58.980 --> 00:07:01.900
حين يتعلق الأمر بتقنين
السيارات ذاتية القيادة،

00:07:02.900 --> 00:07:07.236
فالأرض المشتركة الآن هي بالأساس
السلامة العامة --

00:07:07.260 --> 00:07:08.500
هذا هو الشيء المشترك --

00:07:09.220 --> 00:07:11.196
والمزارعون هم الركاب

00:07:11.220 --> 00:07:14.820
أو أصحاب السيارات الذيم يختارون
أن يتجولوا بهذه السيارات.

00:07:16.780 --> 00:07:19.396
وباتخاذ اختيار منطقي فردي

00:07:19.420 --> 00:07:22.236
أو إعطاء الأولوية للسلامة الشخصية،

00:07:22.260 --> 00:07:25.396
فقد يكونون بصدد إلحاق الضرر
بالشيء المشترك،

00:07:25.420 --> 00:07:27.620
والذي هو تقليل مجموع الضرر.

00:07:30.140 --> 00:07:32.276
وهذا مايسمى بتراجيديا المشاع،

00:07:32.300 --> 00:07:33.596
بشكل تقليدي،

00:07:33.620 --> 00:07:36.716
لكنني أعتقد أنه في حالة السيارات
ذاتية القيادة،

00:07:36.740 --> 00:07:39.596
فإن المشكلة قد تكون أكثر تعقيدًا

00:07:39.620 --> 00:07:43.116
لأنه ليس هناك بالضرورة شخص واحد

00:07:43.140 --> 00:07:44.836
يقوم بهذه القرارات.

00:07:44.860 --> 00:07:48.156
إذ يمكن لمصنعي السيارات ببساطة
أن يبرمجوا سيارات

00:07:48.180 --> 00:07:50.700
تعطي حماية قصوى لعملائهم،

00:07:51.900 --> 00:07:54.876
ويمكن لهذه السيارات
أن تتعلم تلقائيا بمفردها

00:07:54.900 --> 00:07:58.420
أن القيام بذلك يتطلب
زيادة الخطر بالنسبة للراجلين.

00:07:59.340 --> 00:08:00.756
وباستخدام مثال الأغنام،

00:08:00.780 --> 00:08:04.396
فهذا أشبه بخروف كهربائي
له عقله الخاص.

00:08:04.420 --> 00:08:05.876
(ضحك)

00:08:05.900 --> 00:08:08.980
ويمكنه أن يذهب ويرعى دون أن يدري المزارع.

00:08:10.460 --> 00:08:14.436
وهذا مايمكننا أن نسميه
تراجيديا المشاع الخوارزمية،

00:08:14.460 --> 00:08:16.820
وهي تقدم أنواعا جديدة من التحديات.

00:08:22.340 --> 00:08:24.236
عادة، وبشكل تقليدي،

00:08:24.260 --> 00:08:27.596
نحل هذا النوع من المعضلات الاجتماعية
عن طريق القوانين،

00:08:27.620 --> 00:08:30.356
حيث تجتمع الحكومات أو المجتمعات

00:08:30.380 --> 00:08:34.116
ليقرروا جماعيا ما نوع النتائج 
التي يريدونها

00:08:34.140 --> 00:08:36.796
وما نوع العوائق على مستوى السلوك الفردي

00:08:36.820 --> 00:08:38.020
التي يجب عليهم تنفيذها.

00:08:39.420 --> 00:08:42.036
وبعد ذلك، باستخدام المراقبة والفرض،

00:08:42.060 --> 00:08:44.619
يمكنهم ضمان الحفاظ على الصالح العام.

00:08:45.260 --> 00:08:46.835
لذا، لم لا نقوم،

00:08:46.859 --> 00:08:48.355
كواضعي قوانين،

00:08:48.379 --> 00:08:51.276
بإجبار السيارات على تقليل الضرر؟

00:08:51.300 --> 00:08:53.540
في جميع الأحوال، هذا ما يريده الناس.

00:08:55.020 --> 00:08:56.436
والأهم من هذا،

00:08:56.460 --> 00:08:59.556
يمكنني أن أكون متأكدا كفرد،

00:08:59.580 --> 00:09:03.436
إن اشتريت سيارة يمكنها أن تضحي بي
في حالة نادرة جدا،

00:09:03.460 --> 00:09:05.116
فلن أكون الأبله الوحيد
الذي يقوم بهذا

00:09:05.140 --> 00:09:07.820
بينما يتمتع الجميع بحماية لامحدودة.

00:09:08.940 --> 00:09:12.276
في استقصائنا، سألنا الناس فيما إن كانوا
سيدعمون القوانين

00:09:12.300 --> 00:09:13.500
وإليكم ماوجدنا.

00:09:14.180 --> 00:09:17.940
أولا، رفض الناس القوانين،

00:09:19.100 --> 00:09:20.356
ثانيا، قالوا،

00:09:20.380 --> 00:09:24.316
"حسنا، إن قننتم السيارات لتقوم بهذا
وتقلل مجموع الضرر،

00:09:24.340 --> 00:09:25.820
فلن أشتري هذه السيارة."

00:09:27.220 --> 00:09:28.596
لذا، من المضحك،

00:09:28.620 --> 00:09:32.116
أنه بتقنين السيارات لتقليل الضرر،

00:09:32.140 --> 00:09:33.980
ربما سينتهي بنا المطاف
بالمزيد من الضرر

00:09:34.860 --> 00:09:38.516
لأن الناس لن يختاروا التكنولوجيا
الأكثر أمنا

00:09:38.540 --> 00:09:40.620
حتى وإن كانت أكثر أمنا
من الأشخاص السائقين.

00:09:42.180 --> 00:09:45.596
ليست لدي إجابة نهائية لهذا اللغز،

00:09:45.620 --> 00:09:47.196
لكنني أعتقد كبداية،

00:09:47.220 --> 00:09:50.516
نريد أن يجتمع المجتمع

00:09:50.540 --> 00:09:53.300
لنقرر ما هي المقايضات التي نرتاح لها

00:09:54.180 --> 00:09:57.660
ونجد طرقا يمكننا بها فرض هذه المقايضات.

00:09:58.340 --> 00:10:00.876
وكنقطة بداية، بنى طلابي الأذكياء،

00:10:00.900 --> 00:10:03.356
إدموند عواد و سوهان دسوزا،

00:10:03.380 --> 00:10:05.180
موقع Moral Machine،

00:10:06.020 --> 00:10:08.700
الذي يخرج سيناريوهات عشوائية لكم --

00:10:09.900 --> 00:10:12.356
مجموعة من المعضلات العشوائية بتسلسل

00:10:12.380 --> 00:10:16.300
حيث يمكنك أن تختار ما الذي يجب
أن تقوم به السيارة في سيناريو معين.

00:10:16.860 --> 00:10:21.460
ونقوم بتغيير الأعمار
وأنواع الضحايا المختلفة.

00:10:22.860 --> 00:10:26.556
ولحد الآن فقد جمعنا 5 ملايين قرار

00:10:26.580 --> 00:10:28.780
من ما يفوق مليون شخص حول العالم

00:10:30.220 --> 00:10:31.420
من الموقع.

00:10:32.180 --> 00:10:34.596
وهذا يساعدنا على تكوين صورة مبكرة

00:10:34.620 --> 00:10:37.236
عن ماهية المقايضات التي يرتاح لها الناس

00:10:37.260 --> 00:10:39.156
وما هي الأشياء المهمة بالنسبة لهم --

00:10:39.180 --> 00:10:40.620
حتى وإن اختلفت الثقافات.

00:10:42.060 --> 00:10:43.556
لكن الأمر الأهم،

00:10:43.580 --> 00:10:46.956
يساعد هذا التمرين الناس على التعرف

00:10:46.980 --> 00:10:49.796
على صعوبة اتخاذ هذه الاختيارات

00:10:49.820 --> 00:10:53.620
وأن أمام واضعي القوانين مهمة صعبة 
ذات اختيارات مستحيلة.

00:10:55.180 --> 00:10:58.756
وهذا سيساعدنا كمجتمع على أن نفهم
أنواع المقايضات

00:10:58.780 --> 00:11:01.836
التس سيتم تطبيقها في النهاية في القوانين.

00:11:01.860 --> 00:11:03.596
وبالفعل، كنت سعيدا جدا حينما سمعت

00:11:03.620 --> 00:11:05.636
بأن الدفعة الأول من القوانين

00:11:05.660 --> 00:11:07.796
التي صدرت عن هيئة المواصلات --

00:11:07.820 --> 00:11:09.276
أُعلِن عنها الأسبوع الماضي --

00:11:09.276 --> 00:11:15.796
تتضمن لائحة من 15 نقطة
على كل مصنعي السيارات توفيرها،

00:11:15.820 --> 00:11:19.076
والنقطة رقم 14 كانت ذات اعتبار أخلاقي --

00:11:19.100 --> 00:11:20.820
كيف ستقوم بالتعامل مع هذا.

00:11:23.620 --> 00:11:26.276
لقد جعلنا الناس يفكرون في قراراتهم الخاصة

00:11:26.300 --> 00:11:29.300
من خلال إعطائهم ملخصات عما اختاروه.

00:11:30.260 --> 00:11:31.916
سأعطيكم مثالًا --

00:11:31.940 --> 00:11:35.476
وعلي أن أحذركم
أن هذا ليس مثالا اعتدتم عليه،

00:11:35.500 --> 00:11:36.876
كمستعمل اعتيادي

00:11:36.900 --> 00:11:40.516
هذه هي الشخصية التي يتم إنقاذها بكثرة
وهذه هي التي يتم التضحية بها

00:11:40.540 --> 00:11:45.740
(ضحك)

00:11:46.500 --> 00:11:48.396
قد يتفق بعضكم معه،

00:11:48.420 --> 00:11:50.060
أو معها، لا ندري.

00:11:52.300 --> 00:11:58.436
لكن يبدو أن هذا الشخص يفضل
قليلا الركاب على الراجلين

00:11:58.460 --> 00:12:00.556
في اختياره

00:12:00.580 --> 00:12:03.396
وهو سعيد بأن يعاقب
كل من لا يحترم إشارة المرور .

00:12:03.420 --> 00:12:06.460
(ضحك)

00:12:09.140 --> 00:12:10.356
إذن لنلخص ما سبق.

00:12:10.379 --> 00:12:13.795
فقد بدأنا بسؤال --
لنسمه المعضلة الأخلاقية --

00:12:13.820 --> 00:12:16.876
ما الذي يجب على السيارة أن تقوم به
في حالة معينة:

00:12:16.900 --> 00:12:18.100
أن تنحرف أو لا؟

00:12:19.060 --> 00:12:21.796
لكن بعدها أدركنا أن المشكل كان مختلفًا.

00:12:21.820 --> 00:12:26.356
كان المشكل عن كيفية جعل المجتمع
يوافق ويفرض

00:12:26.380 --> 00:12:28.316
المقايضات التي يرتاح لها.

00:12:28.340 --> 00:12:29.596
هذه معضلة اجتماعية.

00:12:29.620 --> 00:12:34.636
في الأربعينيات، كتب إسحاق عظيموف
قوانينه الشهيرة عن الروبوتات --

00:12:34.660 --> 00:12:35.980
قوانين الروبوتات الثلاث --

00:12:37.060 --> 00:12:39.516
الروبوت لن يضر الإنسان،

00:12:39.540 --> 00:12:42.076
الروبوت لن يعصي الإنسان،

00:12:42.100 --> 00:12:45.356
ولن يسمح الروبوت لنفسه بأن يأتي الضرر --

00:12:45.380 --> 00:12:47.340
بهذا التسلسل حسب الأهمية.

00:12:48.180 --> 00:12:50.316
لكن بعد 40 سنة ونيف

00:12:50.340 --> 00:12:54.076
وبعد العديد من القصص التي دفعت 
هذه القوانين لتظهر ضعفها،

00:12:54.100 --> 00:12:57.796
قدم عظيموف القانون الصفري للترموديناميك

00:12:57.820 --> 00:13:00.076
والذي كانت له الأولوية قبل كل شيء،

00:13:00.100 --> 00:13:03.380
وهي أن الروبوت لن يضر الإنسانية جمعاء.

00:13:04.300 --> 00:13:08.676
لست أدري ما الذي يعنيه هذا 
في سياق السيارات ذاتية القيادة

00:13:08.700 --> 00:13:11.436
أو أي حالة معينة،

00:13:11.460 --> 00:13:13.676
ولست أدري كيف يمكننا تطبيق هذا،

00:13:13.700 --> 00:13:15.236
لكنني أعتقد أنه من خلال الإدراك

00:13:15.260 --> 00:13:21.396
أن تقنين السيارات ذاتية القيادة 
ليس مشكلا تكنولوجيا فقط

00:13:21.420 --> 00:13:24.700
ولكنه مشكل تعاون مجتمعي،

00:13:25.620 --> 00:13:28.550
آمل أن يكون بمقدرونا أن نبدأ على الأقل
في طرح الأسئلة المناسبة

00:13:29.020 --> 00:13:30.236
شكرا لكم.

00:13:30.260 --> 00:13:33.180
(تصفيق)

