WEBVTT
Kind: captions
Language: hu

00:00:00.000 --> 00:00:07.000
Fordító: Péter Pallós
Lektor: Zsuzsanna Lőrincz

00:00:12.820 --> 00:00:16.900
Ma a technológiáról
és a társadalomról fogok beszélni.

00:00:18.860 --> 00:00:22.556
A Közlekedési Minisztérium
2015-ös adata szerint

00:00:22.580 --> 00:00:26.660
csupán az USA-ban 35 000 fő
halt meg közlekedési balesetben.

00:00:27.860 --> 00:00:32.660
A világon ugyanezen okból 1,2 milliónyian.

00:00:33.580 --> 00:00:37.676
Ha mód lenne a balesetek
90%-ának elkerülésére,

00:00:37.700 --> 00:00:38.900
támogatnák-e?

00:00:39.540 --> 00:00:40.836
Nyilvánvalóan igen.

00:00:40.860 --> 00:00:44.515
A vezető nélküli autó 
technológiája erre ad reményt,

00:00:44.540 --> 00:00:47.356
mert megszünteti a fő baleseti forrást,

00:00:47.380 --> 00:00:48.580
az emberi hibát.

00:00:49.740 --> 00:00:55.156
Képzeljük el magunkat a 2030-as évek
vezető nélküli autójában,

00:00:55.180 --> 00:00:58.636
amint hátuljában nézzük
a TEDxCambridge ősrégi videóját.

00:00:58.660 --> 00:01:00.660
(Nevetés)

00:01:01.340 --> 00:01:02.556
Az autó váratlanul

00:01:02.580 --> 00:01:05.860
műszaki hibát észlel,
de képtelen megállni.

00:01:07.180 --> 00:01:08.700
Ha továbbhalad,

00:01:09.540 --> 00:01:13.660
elüt egy csomó, zebrán átkelő gyalogost,

00:01:14.900 --> 00:01:17.035
de elkanyarodhat,

00:01:17.059 --> 00:01:18.916
s így egy bámészkodót gázol halálra,

00:01:18.940 --> 00:01:21.020
s ezzel megóvja a gyalogosokat.

00:01:21.860 --> 00:01:24.460
Mit tegyen a kocsi, és ki döntsön róla?

00:01:25.340 --> 00:01:28.876
De mi lenne, ha falba csapódna,

00:01:28.900 --> 00:01:32.196
s ezzel önt, az utast ölné meg,

00:01:32.220 --> 00:01:34.540
hogy a gyalogosokat mentse meg?

00:01:35.060 --> 00:01:38.140
E feladványt az ún.
villamoskocsi-dilemma ihlette,

00:01:38.780 --> 00:01:42.350
amelyet filozófusok pár évtizede

00:01:42.370 --> 00:01:43.930
erkölcsi döntésre gondoltak ki.

00:01:45.940 --> 00:01:48.436
Fontos, miként oldjuk meg a dilemmát.

00:01:48.460 --> 00:01:51.076
De azt is lehet, hogy nem törődünk vele.

00:01:51.100 --> 00:01:54.476
Mondhatjuk, hogy a feladvány nem reális,

00:01:54.500 --> 00:01:56.820
teljesen valószínűtlen és ostobaság.

00:01:57.580 --> 00:02:00.316
De a kritika szem elől
téveszti a lényeget,

00:02:00.340 --> 00:02:02.500
mert szó szerint veszi a feladványt.

00:02:03.740 --> 00:02:06.476
Igaz, hogy ilyen baleset nem történik,

00:02:06.500 --> 00:02:09.836
nem szokott két-három
választási lehetőség adódni,

00:02:09.860 --> 00:02:11.860
ahol valaki mindenképp meghal.

00:02:13.300 --> 00:02:15.876
Az autó valamit ki fog számolni,

00:02:15.900 --> 00:02:20.796
pl. mennyi valamely embercsoportba
való csapódás valószínűsége,

00:02:20.820 --> 00:02:24.156
ha egyik vagy másik irányba kanyarodik,

00:02:24.180 --> 00:02:27.636
kissé növelhető az utasok
vagy idegen vezetők kockázata

00:02:27.660 --> 00:02:29.196
a gyalogosokéhoz képest.

00:02:29.220 --> 00:02:31.380
A számítás elég bonyolult lesz,

00:02:32.300 --> 00:02:34.820
de a kompromisszum benne lesz,

00:02:35.660 --> 00:02:38.540
és a kompromisszum
gyakran erkölcsi kérdés.

00:02:39.660 --> 00:02:42.396
Mondhatják: "Ne törődjünk vele, várjuk ki,

00:02:42.420 --> 00:02:47.060
amíg a technológia 100%-osan
fejlett és biztonságos nem lesz."

00:02:48.340 --> 00:02:52.020
Tegyük föl, hogy 10 éven belül valóban
kiküszöbölhetjük a balesetek 90%-át,

00:02:52.900 --> 00:02:55.740
vagy akár 99%-ukat.

00:02:56.740 --> 00:02:59.916
Mi van, ha az utolsó
százalék kiküszöbölése

00:02:59.940 --> 00:03:03.060
további 50 kutatói évet igényel?

00:03:04.220 --> 00:03:06.020
Addig ne vezessük be a technológiát?

00:03:06.540 --> 00:03:11.316
A jelenlegi arányok mellett

00:03:11.340 --> 00:03:13.100
ez 60 millió halott.

00:03:14.580 --> 00:03:15.796
A lényeg:

00:03:15.820 --> 00:03:19.436
a teljes biztonság kivárása is lehetőség,

00:03:19.460 --> 00:03:21.620
s ennek ára is kompromisszum.

00:03:23.380 --> 00:03:27.716
A közösségi oldalakon rengeteg az ötlet,

00:03:27.740 --> 00:03:29.756
hogyan kerüljük el e konfliktust.

00:03:29.780 --> 00:03:32.996
Valaki azt javasolta, hogy az autó

00:03:33.020 --> 00:03:35.156
valahogy szlalomozzon a gyalogosok...

00:03:35.180 --> 00:03:36.196
(Nevetés)

00:03:36.220 --> 00:03:37.476
és a bámészkodó között.

00:03:37.500 --> 00:03:40.860
Persze, ha nincs más lehetőség,
az autó így fog tenni.

00:03:41.740 --> 00:03:44.780
De minket azok a változatok
érdekelnek, ahol ez nem lehetséges.

00:03:45.100 --> 00:03:50.516
Kedvencem az egyik blogger ötlete,

00:03:50.540 --> 00:03:53.556
hogy legyen katapultáló gomb
az autóban, amelyet megnyomunk...

00:03:53.580 --> 00:03:54.560
(Nevetés)

00:03:54.560 --> 00:03:56.597
mielőtt az autó
megsemmisítené magát.

00:03:56.597 --> 00:03:57.851
(Nevetés)

00:03:59.660 --> 00:04:04.860
Ha elfogadjuk, hogy az autóknak
kompromisszumokat kell kötniük

00:04:06.020 --> 00:04:07.900
mit tartunk róluk,

00:04:09.140 --> 00:04:10.716
és ki fog dönteni?

00:04:10.740 --> 00:04:13.876
Tán kutatást kellene végeznünk,
hogy mit akar a társadalom,

00:04:13.900 --> 00:04:15.356
mert végül is,

00:04:15.380 --> 00:04:19.340
a szabályozások és törvények
a társadalmi értékekhez igazodnak.

00:04:19.860 --> 00:04:21.100
Ezt tettük.

00:04:21.700 --> 00:04:23.316
Munkatársaimmal,

00:04:23.340 --> 00:04:25.676
Jean-François Bonnefonnal
és Azim Shariff-fal,

00:04:25.700 --> 00:04:27.316
kutatást végeztünk,

00:04:27.340 --> 00:04:30.195
amelyben az emberek elé
tártuk a lehetőségeket.

00:04:30.219 --> 00:04:33.480
Két filozófus, Jeremy Bentham

00:04:33.480 --> 00:04:36.660
és Immanuel Kant alapján
két lehetőséget adtunk meg.

00:04:37.420 --> 00:04:40.516
Bentham szerint az autónak
haszonelvű etikát kell követnie:

00:04:40.540 --> 00:04:43.956
úgy kell tennie, hogy csak 
minimális kár keletkezzék,

00:04:43.980 --> 00:04:46.796
még ha a bámészkodó meghal is,

00:04:46.820 --> 00:04:49.260
és még ha miatta meghal egy utas.

00:04:49.940 --> 00:04:54.916
Immanuel Kant szerint az autónak
kötelességelvű etikát kell követnie:

00:04:54.940 --> 00:04:56.500
pl, a "Ne ölj!" elvét.

00:04:57.300 --> 00:05:01.756
Nem szabad olyat tenni,
ami kárt okoz egy emberi lénynek,

00:05:01.780 --> 00:05:04.236
s hagyni kell, hogy a kocsi csak menjen,

00:05:04.260 --> 00:05:06.220
még ha több embert megsebesít is.

00:05:07.460 --> 00:05:08.660
Önök mit gondolnak?

00:05:09.180 --> 00:05:10.700
Bentham vagy Kant?

00:05:11.580 --> 00:05:12.836
Mi az eredmény?

00:05:12.860 --> 00:05:14.660
A legtöbben Benthamot támogatták.

00:05:15.980 --> 00:05:19.756
Úgy látszik, az emberek
inkább haszonelvűek,

00:05:19.780 --> 00:05:21.196
minimalizálják a teljes kárt,

00:05:21.220 --> 00:05:22.796
és ezt kell tennünk.

00:05:22.820 --> 00:05:24.020
A kérdés megoldva.

00:05:25.060 --> 00:05:26.540
De van egy kis bökkenő.

00:05:27.740 --> 00:05:31.476
Amikor azt kérdeztük:
"Vennének-e ilyen autót?",

00:05:31.500 --> 00:05:33.116
határozott nemmel válaszoltak.

00:05:33.140 --> 00:05:35.436
(Nevetés)

00:05:35.460 --> 00:05:39.356
Olyan kocsit vennének,
amely mindenáron megvédi őket,

00:05:39.380 --> 00:05:42.996
ám azt akarják, hogy mások csakis
a kárt minimalizálót vegyenek.

00:05:43.020 --> 00:05:45.540
(Nevetés)

00:05:46.540 --> 00:05:48.396
Már találkoztunk ezzel a problémával:

00:05:48.420 --> 00:05:49.980
ez a társadalmi dilemma.

00:05:50.980 --> 00:05:52.796
A társadalmi dilemma megértéséhez

00:05:52.820 --> 00:05:54.860
egy kissé menjünk vissza a történelemben.

00:05:55.820 --> 00:05:58.396
Az 1800-as években

00:05:58.420 --> 00:06:01.390
William Forster Lloyd angol közgazdász

00:06:01.420 --> 00:06:04.396
az alábbi tartalmú vitairatot
jelentette meg.

00:06:04.420 --> 00:06:06.076
Gazdák egy csoportja –

00:06:06.100 --> 00:06:07.126
angol gazdák –

00:06:07.430 --> 00:06:10.500
fölosztják maguk között a közlegelőt,
hogy birkáik legelhessenek.

00:06:11.340 --> 00:06:13.916
Na mármost, ha mindenki ugyanannyi juhot,

00:06:13.940 --> 00:06:15.436
mondjuk, hármat legeltet,

00:06:15.460 --> 00:06:17.556
a legelő megújul,

00:06:17.580 --> 00:06:18.796
a gazdák elégedettek,

00:06:18.820 --> 00:06:20.436
a birkák elégedettek,

00:06:20.460 --> 00:06:21.660
minden rendben.

00:06:22.260 --> 00:06:24.780
Ha valamelyik gazda
még egy birkát odavisz,

00:06:25.620 --> 00:06:30.340
az kissé jobban jár,
és senki sem károsodik.

00:06:30.980 --> 00:06:34.620
Ám ha mindenki ezt
az egyénileg észszerű döntést hozná,

00:06:35.660 --> 00:06:38.380
a legelőt túlhasználnák és fölélnék

00:06:39.180 --> 00:06:41.356
minden gazda hátrányára,

00:06:41.380 --> 00:06:43.500
és persze, a birkák hátrányára is.

00:06:44.540 --> 00:06:48.220
E problémával gyakran találkozunk:

00:06:48.900 --> 00:06:52.076
pl. a túlhalászat esetén,

00:06:52.100 --> 00:06:56.660
vagy a klímaváltozás mérséklésére
a CO₂-kibocsátás csökkentésénél.

00:06:58.980 --> 00:07:01.900
A vezető nélküli autó szabályozása esetén

00:07:02.900 --> 00:07:07.236
a közlegelő itt a közbiztonság,

00:07:07.260 --> 00:07:08.500
ez a közjó;

00:07:09.220 --> 00:07:11.196
a gazdák az utasok,

00:07:11.220 --> 00:07:14.820
vagy a kocsitulajdonos,
aki az autóban utazik.

00:07:16.780 --> 00:07:19.396
Egyénileg észszerű döntéssel

00:07:19.420 --> 00:07:22.236
előnyben részesítve saját biztonságunkat

00:07:22.260 --> 00:07:25.396
közösségileg csökkentenénk a közjót,

00:07:25.420 --> 00:07:27.620
amely minimalizálná a teljes kárt.

00:07:30.140 --> 00:07:32.276
Hagyományosan ezt hívjuk

00:07:32.300 --> 00:07:33.596
a közbirtok tragédiájának,

00:07:33.620 --> 00:07:36.716
de a vezető nélküli autó esetében

00:07:36.740 --> 00:07:39.596
a dilemma tán kissé még fogósabb,

00:07:39.620 --> 00:07:43.116
mert nem szükségképpen emberi lény

00:07:43.140 --> 00:07:44.836
hozza a döntéseket.

00:07:44.860 --> 00:07:48.156
Az autógyárak a kocsikat
egyszerűen beprogramozhatják,

00:07:48.180 --> 00:07:50.700
amellyel maximalizálhatják
ügyfeleik biztonságát,

00:07:51.900 --> 00:07:54.876
és a kocsik automatikusan
önállóan tanulhatnak,

00:07:54.900 --> 00:07:58.420
és ez a biztonság kissé fokozza
a gyalogosok kockázatát.

00:07:59.340 --> 00:08:00.756
A birka-hasonlattal élve:

00:08:00.780 --> 00:08:04.396
nekünk most saját tudatú
elektromos birkáink vannak.

00:08:04.420 --> 00:08:05.876
(Nevetés)

00:08:05.900 --> 00:08:08.980
A gazda tudtán kívül is
bemehetnek a legelőre.

00:08:10.460 --> 00:08:14.436
Hívhatjuk ezt az algoritmikus
közbirtok tragédiájának,

00:08:14.460 --> 00:08:16.820
s ez új feladatot ró ránk.

00:08:22.340 --> 00:08:24.236
E társadalmi jellegű dilemmákat

00:08:24.260 --> 00:08:27.596
általában szabályozással oldjuk meg.

00:08:27.620 --> 00:08:30.356
Kormányszervek vagy közösségek
megvitatják őket,

00:08:30.380 --> 00:08:34.116
és közösen döntik el,
milyen eredményt szeretnének,

00:08:34.140 --> 00:08:35.600
és az egyéni cselekvés

00:08:35.630 --> 00:08:38.020
milyen korlátozására van szükség.

00:08:39.420 --> 00:08:42.036
Aztán ellenőrzési 
és végrehajtási intézkedésekkel

00:08:42.060 --> 00:08:44.619
elérhető a közjó megmaradása.

00:08:45.260 --> 00:08:46.835
Akkor szabályozó szervként

00:08:46.859 --> 00:08:48.355
mondjuk ki,

00:08:48.379 --> 00:08:51.276
hogy minden autó köteles
minimalizálni a kárt!

00:08:51.300 --> 00:08:53.540
Végtére, mindenki ezt akarja.

00:08:55.020 --> 00:08:56.436
De ennél is fontosabb,

00:08:56.460 --> 00:08:59.556
hogy személy szerint biztos lehetek benne,

00:08:59.580 --> 00:09:03.010
hogy ha ilyen kocsit veszek,
és kivételes esetben engem föláldoz,

00:09:03.020 --> 00:09:05.116
nem én leszek az egyetlen palimadár,

00:09:05.140 --> 00:09:07.820
miközben a többiek élvezik
a feltétlen biztonságot.

00:09:08.940 --> 00:09:11.240
Kutatásunkban azt kérdeztük az emberektől,

00:09:11.260 --> 00:09:13.500
támogatnák-e a szabályozást;
íme, az eredmény.

00:09:14.180 --> 00:09:17.940
Nemet mondtak a szabályozásra,

00:09:19.100 --> 00:09:20.356
s hozzátették:

00:09:20.380 --> 00:09:24.316
"Ha szabályozzák az autókat,
hogy minimalizálják a kárt,

00:09:24.340 --> 00:09:25.820
nem veszek olyan kocsit."

00:09:27.220 --> 00:09:28.596
Elég vicces,

00:09:28.620 --> 00:09:32.116
hogy a kárminimalizációs szabályozás

00:09:32.140 --> 00:09:33.980
nagyobb kárral járhat,

00:09:34.860 --> 00:09:38.516
mert az emberek nem a biztonságosabb
technológiát választják,

00:09:38.540 --> 00:09:40.620
még ha a vezetőnél megbízhatóbb is.

00:09:42.180 --> 00:09:45.596
Nincs végső válaszom e rejtélyre,

00:09:45.620 --> 00:09:47.196
de mindenekelőtt

00:09:47.220 --> 00:09:50.516
a társadalomnak kell megvitatnia,

00:09:50.540 --> 00:09:53.300
mi az elfogadható kompromisszum,

00:09:54.180 --> 00:09:57.660
és azt hogyan lehet megvalósítani.

00:09:58.340 --> 00:10:00.876
Kiváló hallgatóim,

00:10:00.900 --> 00:10:03.356
Edmond Awad és Sohan Dsouza

00:10:03.380 --> 00:10:05.180
létrehozták a Moral Machine honlapot,

00:10:06.020 --> 00:10:08.940
amely véletlenszerűen
generál baleseti helyzeteket,

00:10:09.900 --> 00:10:12.356
dilemmákat,

00:10:12.380 --> 00:10:16.300
ahol ki kell választanunk,
mit tegyen az autó.

00:10:16.860 --> 00:10:21.460
Az áldozatok korát
és fajtáját változtatjuk.

00:10:22.860 --> 00:10:26.556
Idáig több mint ötmillió döntést
gyűjtöttünk össze világszerte,

00:10:26.580 --> 00:10:28.780
egymillió fő részvételével

00:10:30.220 --> 00:10:31.420
a honlapról.

00:10:32.180 --> 00:10:34.596
Ezzel fölvázolhatjuk,
milyen kompromisszummal

00:10:34.620 --> 00:10:37.236
lennének elégedettek az emberek,

00:10:37.260 --> 00:10:39.156
és mi fontos nekik

00:10:39.180 --> 00:10:40.620
a különböző kultúrákban.

00:10:42.060 --> 00:10:43.556
De még ennél is fontosabb,

00:10:43.580 --> 00:10:46.956
hogy e gyakorlatok
megértetik az emberekkel,

00:10:46.980 --> 00:10:49.796
milyen bonyolult dönteni,

00:10:49.820 --> 00:10:53.620
s hogy a szabályozó szervek
lehetetlen döntésekre kényszerülnek.

00:10:55.180 --> 00:10:58.756
Talán ez elősegíti, hogy a társadalom
megértse a kompromisszum szerepét

00:10:58.780 --> 00:11:01.836
a meghozandó szabályozásban.

00:11:01.860 --> 00:11:03.596
Nagyon örültem, amikor meghallottam,

00:11:03.620 --> 00:11:05.636
hogy a Közlekedési Minisztérium

00:11:05.660 --> 00:11:07.796
kezdeti szabályozása

00:11:07.820 --> 00:11:09.196
– múlt héten tették közzé –

00:11:09.220 --> 00:11:15.796
minden autógyártóra kötelező, 15 tételből
álló követelménylistát tartalmaz;

00:11:15.820 --> 00:11:19.076
és a 14. pont erkölcsi jellegű:

00:11:19.100 --> 00:11:20.820
milyen döntés szülessen.

00:11:23.280 --> 00:11:26.276
Arra késztetjük az embereket,
hogy gondolkozzanak el döntéseiken

00:11:26.300 --> 00:11:29.300
az összesített eredmények ismeretében.

00:11:30.260 --> 00:11:31.916
Mondok egy példát.

00:11:31.940 --> 00:11:35.476
A példa nem tipikus.

00:11:35.500 --> 00:11:36.876
A használó sem az.

00:11:36.900 --> 00:11:40.516
Az illető megkíméli a macskákat,
és föl szokta áldozni a gyerekeket.

00:11:40.540 --> 00:11:41.870
(Nevetés)

00:11:46.500 --> 00:11:48.396
Lehet, hogy van,

00:11:48.420 --> 00:11:50.060
aki egyetért vele.

00:11:52.300 --> 00:11:58.436
De az illető inkább az utasoknak
kedvez döntései során,

00:11:58.460 --> 00:12:00.556
semmint a gyalogosoknak, és boldog,

00:12:00.580 --> 00:12:03.396
ha megbüntetheti a szabálytalankodó
gyalogosokat.

00:12:03.420 --> 00:12:06.460
(Nevetés)

00:12:09.140 --> 00:12:10.356
Összegezve:

00:12:10.379 --> 00:12:13.795
azzal kezdtük,
hívjuk erkölcsi dilemmának,

00:12:13.820 --> 00:12:16.876
mit tegyen az autó bizonyos helyzetben,

00:12:16.900 --> 00:12:18.280
kanyarodjon vagy álljon meg.

00:12:19.060 --> 00:12:21.796
De rájöttünk, hogy más a probléma,

00:12:21.820 --> 00:12:25.960
az, hogyan vegyük rá a társadalmat

00:12:25.990 --> 00:12:28.316
egy általa elfogadott
kompromisszum betartására?

00:12:28.340 --> 00:12:29.596
Ez társadalmi dilemma.

00:12:29.620 --> 00:12:34.636
1940-ben Isaac Asimov leírta
a híres robotikai törvényeit,

00:12:34.660 --> 00:12:35.980
a három törvényt.

00:12:37.060 --> 00:12:39.516
A robot nem árthat az emberi lénynek,

00:12:39.540 --> 00:12:42.076
engedelmeskednie kell neki,

00:12:42.100 --> 00:12:45.356
és a robot magában sem tehet kárt:

00:12:45.380 --> 00:12:47.340
ez a fontossági sorrend.

00:12:48.180 --> 00:12:50.316
Ám kb. 40 év múltán,

00:12:50.340 --> 00:12:54.076
s annyi történet után,
amelyek kipróbálták a törvényeket,

00:12:54.100 --> 00:12:57.796
Asimov bevezette a nulladik törvényt,

00:12:57.820 --> 00:13:00.076
amely minden mást megelőz.

00:13:00.100 --> 00:13:03.380
Ez pedig: a robot nem árthat
az egész emberiségnek.

00:13:04.300 --> 00:13:08.676
Nem tudom, hogy értendő ez
a vezető nélküli autóra

00:13:08.700 --> 00:13:11.436
vagy valami különleges helyzetre,

00:13:11.460 --> 00:13:13.676
s azt sem, hogyan vezethetnénk be,

00:13:13.700 --> 00:13:15.236
de ha elismerjük,

00:13:15.260 --> 00:13:21.396
hogy a vezető nélküli autóra vonatkozó
szabályozás nem csak technológiai,

00:13:21.420 --> 00:13:24.700
hanem társadalmi együttműködési
probléma is, remélem,

00:13:25.620 --> 00:13:28.500
hogy legalább hozzáláthatunk
a helyes kérdések föltevéséhez.

00:13:29.020 --> 00:13:30.236
Köszönöm.

00:13:30.260 --> 00:13:33.180
(Taps)

