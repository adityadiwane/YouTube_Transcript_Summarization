WEBVTT
Kind: captions
Language: fa

00:00:00.000 --> 00:00:07.000
Translator: Iman Mirzadeh
Reviewer: Leila Ataei

00:00:12.820 --> 00:00:16.900
امروز قراره درباره
تکنولوژی و جامعه صحبت کنم.

00:00:18.860 --> 00:00:22.556
دپارتمان حمل و نقل،
در سال گذشته تخمین زد که

00:00:22.580 --> 00:00:26.660
فقط در آمریکا، ۳۵،۰۰۰ نفر
به علت تصادفات ترافیکی مرده‌اند.

00:00:27.860 --> 00:00:32.660
در سراسر جهان هر ساله ۱.۲ میلیون نفر
به خاطر سوانح ترافیکی میمیرند.

00:00:33.580 --> 00:00:37.676
اگه راهی بود که میتونسیتم باهاش
۹۰ درصد این تصادف‌ها رو حذف کنیم،

00:00:37.700 --> 00:00:38.900
آیا ازش حمایت میکردین؟

00:00:39.540 --> 00:00:40.836
البته که میکردین.

00:00:40.860 --> 00:00:44.515
این چیزیه که تکنولوژی ماشین خودران
( بدونِ راننده) وعده میده که بهش به

00:00:44.540 --> 00:00:47.356
به وسیله منبع اصلی تصادفات میرسه--

00:00:47.380 --> 00:00:48.580
خطای انسان.

00:00:49.740 --> 00:00:55.156
حالا خودتون رو تصور کنید
که در سال ۲۰۳۰ در یک ماشین خودران نشستید،

00:00:55.180 --> 00:00:58.636
و دارین این ویدیو قدیمی 
TEDxCambrdige رو میبینین.

00:00:58.660 --> 00:01:00.660
(خنده حاضرین)

00:01:01.340 --> 00:01:02.556
ناگهان،

00:01:02.580 --> 00:01:05.860
ماشین به مشکل مکانیکی میخوره
و نمیتونه متوقف شه.

00:01:07.180 --> 00:01:08.700
اگه ماشین ادامه بده،

00:01:09.540 --> 00:01:13.660
به عده‌ای از عابرین پیاده
که در حال عبور از خیابان هستند خواهد زد.

00:01:14.900 --> 00:01:17.035
ولی ممکنه ماشین منحرف بشه،

00:01:17.059 --> 00:01:18.916
و به یکی از افراد داخل پیاده رو بزنه،

00:01:18.940 --> 00:01:21.020
و اونا رو برای نجات جان عابران پیاده بکشه.

00:01:21.860 --> 00:01:24.460
ماشین باید چیکار کنه
و چه کسی باید تصمیم بگیره؟

00:01:25.340 --> 00:01:28.876
اگه به جای اینکار
ماشین بتونه به سمت دیوار منحرف شه،

00:01:28.900 --> 00:01:32.196
و تصادف کنه و شما که سرنشین هستین رو بکشه،

00:01:32.220 --> 00:01:34.540
برای نجات اون عابران پیاده، اونوقت چی؟

00:01:35.060 --> 00:01:38.140
این سناریو
از مشکل قطار برقی الهام گرفته است،

00:01:38.780 --> 00:01:42.556
که چندین دهه پیش
توسط فلاسفه اختراع شد

00:01:42.580 --> 00:01:43.820
تا درباره اخلاق فکر کنند.

00:01:45.940 --> 00:01:48.436
حالا، جوری که ما
به این مساله فکر میکنیم مهمه.

00:01:48.460 --> 00:01:51.076
مثلا ممکنه اصلاً بهش فکر نکنیم.

00:01:51.100 --> 00:01:54.476
ممکنه بگید این سناریو غیر واقعیه،

00:01:54.500 --> 00:01:56.820
یا غیر محتمل، یا احمقانه.

00:01:57.580 --> 00:02:00.316
ولی من فکر میکنم این انتقاد
از نکته اصلی غافل میشه

00:02:00.340 --> 00:02:02.500
چون سناریو رو خیلی سطحی بررسی میکنه.

00:02:03.740 --> 00:02:06.476
البته که هیچ تصادفی این شکلی نیست،

00:02:06.500 --> 00:02:09.836
هیچ تصادفی دو یا سه گزینه نداره

00:02:09.860 --> 00:02:11.860
که توش هرکی به طریقی میمیره.

00:02:13.300 --> 00:02:15.876
به جاش، ماشین میره سراغ حساب کردن

00:02:15.900 --> 00:02:20.796
چیزی مثل احتمال اصابت به گروهی از مردم،

00:02:20.820 --> 00:02:24.156
وقتی به جهتی منحرف بشین
نسبت به یک جهت دیگه،

00:02:24.180 --> 00:02:27.636
شاید اندکی ریسک رو برای
سرنشینان یا سایر رانندگان

00:02:27.660 --> 00:02:29.196
نسبت به عابران پیاده زیاد کنین.

00:02:29.220 --> 00:02:31.380
این محاسبات پیچیده‌تر هم خواهد شد،

00:02:32.300 --> 00:02:34.820
ولی همچنان شامل بده‌ بستان‌هایی خواهد بود،

00:02:35.660 --> 00:02:38.540
و بده بستان‌ها
غالباً نیازمند اخلاقیات هستند.

00:02:39.660 --> 00:02:42.396
ممکنه بگیم،
"خب، بیا نگران این مساله نباشیم.

00:02:42.420 --> 00:02:47.060
بیاین صبر کنیم تا تکنولوژی
کاملاً آماده و صد درصد امن بشه."

00:02:48.340 --> 00:02:52.020
فرض کنید ما بتونیم
واقعاً ۹۰ درصد این تصادفات رو حذف کنیم،

00:02:52.900 --> 00:02:55.740
یا حتی ۹۹ درصدشون رو طی ۱۰ سال آینده.

00:02:56.740 --> 00:02:59.916
اگه حذف کردن اون ۱ درصدِ
باقی مونده از تصادفات

00:02:59.940 --> 00:03:03.060
به ۵۰ سال تحقیق نیاز داشته باشه،
اونوقت تکلیف چیه؟

00:03:04.220 --> 00:03:06.020
آیا باید قید این تکنولوژی رو بزنیم؟

00:03:06.540 --> 00:03:11.316
که یعنی ۶۰ میلیون مرگ در تصادفات رانندگی

00:03:11.340 --> 00:03:13.100
اگر که با همین نرخ تصادفات پیش بریم.

00:03:14.580 --> 00:03:15.796
منظورم اینه که،

00:03:15.820 --> 00:03:19.436
صبر کردن برای رسیدن به ایمنی کامل
یک گزینه هست،

00:03:19.460 --> 00:03:21.620
ولی بده-بستانهای خودش رو داره.

00:03:23.380 --> 00:03:27.716
مردم در رسانه‌های اجتماعی 
به انواع مختلفی از روش‌ها رسیدن که

00:03:27.740 --> 00:03:29.756
به این مشکل فکر نکنیم.

00:03:29.780 --> 00:03:32.996
یه نفر پیشنهاد داده بود که
ماشین باید به سمتی منحرف بشه

00:03:33.020 --> 00:03:35.156
که ماشین بخوره به جایی بین سرنشین‌ها --

00:03:35.180 --> 00:03:36.196
(خنده حاضرین)

00:03:36.220 --> 00:03:37.476
و عابر پیاده.

00:03:37.500 --> 00:03:40.860
البته اگه این کاریه که ماشین میتونه بکنه،
باید همین کار رو بکنه.

00:03:41.740 --> 00:03:44.580
ولی ما درباره سناریو‌هایی کنجکاویم
که نمیشه این کار رو کرد.

00:03:45.100 --> 00:03:50.516
و راه مورد علاقه خودم
پیشنهاد یک بلاگ‌نویس هستش

00:03:50.540 --> 00:03:53.556
که داخل ماشین یک دکمه پرتاب به بیرون باشه
که فشارش بدین --

00:03:53.580 --> 00:03:54.796
(خنده حاضرین)

00:03:54.820 --> 00:03:56.487
درست قبل از خود نابودسازی ماشین.

00:03:56.511 --> 00:03:58.191
(خنده حاضرین)

00:03:59.660 --> 00:04:04.860
پس اگه ما قبول کنیم که ماشین‌ها
بده‌-بستان‌هایی رو در جاده خواهند داشت،

00:04:06.020 --> 00:04:07.900
تکلیف این بده بستانها چیه،

00:04:09.140 --> 00:04:10.716
و چطور تصمیم میگیریم؟

00:04:10.740 --> 00:04:13.876
خب، شاید باید یه نظرسنجی برگزار کنیم
تا ببینیم جامعه چی میخواد،

00:04:13.900 --> 00:04:15.356
چون در نهایت،

00:04:15.380 --> 00:04:19.340
قوانین و مقررات
بازتابی از ارزش‌های اجتماعی هستند.

00:04:19.860 --> 00:04:21.100
ما هم همین کارو کردیم.

00:04:21.700 --> 00:04:23.316
با همکارانم،

00:04:23.340 --> 00:04:25.676
عظیم شریف و‌ ژان-فرانیوا بونفون

00:04:25.700 --> 00:04:27.316
یک نظرسنجی برگزار کردیم

00:04:27.340 --> 00:04:30.195
که توش به مردم
این سناریوها رو نشون میدادیم.

00:04:30.219 --> 00:04:33.996
بهشون دوتا حق انتخاب میدادیم که
الهام گرفته از دو فیلسوف بودن:

00:04:34.020 --> 00:04:36.660
امانوئل کانت و جرمی بنتهام.

00:04:37.420 --> 00:04:40.516
بنتهام میگه که ماشین
باید از اخلاق سودگرایانه پیروی کنه:

00:04:40.540 --> 00:04:43.956
باید کاری کنه که مجموعِ ضرر کمینه بشه --

00:04:43.980 --> 00:04:46.796
حتی اگه این کار
باعث کشته شدن یک عابر پیاده بشه

00:04:46.820 --> 00:04:49.260
حتی اگه این کار یک سرنشین رو بکشه.

00:04:49.940 --> 00:04:54.916
امانوئل کانت میگه ماشین باید
از اصول تکلیفی پیروی کنه،

00:04:54.940 --> 00:04:56.500
مثل فرمان "نباید به قتل برسانی."

00:04:57.300 --> 00:05:01.756
پس شما نباید کاری کنین که به صراحت
به یک انسان آسیب بزنین،

00:05:01.780 --> 00:05:04.236
و باید بذارین که ماشین به مسیرش ادامه بده

00:05:04.260 --> 00:05:06.220
حتی اگه به مردم بیشتری آسیب بزنه.

00:05:07.460 --> 00:05:08.660
شما نظرتون چیه؟

00:05:09.180 --> 00:05:10.700
بنتهام درست میگه یا کانت؟

00:05:11.580 --> 00:05:12.836
ما به این نتیجه رسیدیم.

00:05:12.860 --> 00:05:14.660
بیشتر مردم میگن بنتهام.

00:05:15.980 --> 00:05:19.756
به نظر میرسه مردم میخوان که ماشین
سودمندگرا باشه،

00:05:19.780 --> 00:05:21.196
مجموع ضرر رو کمینه کنه،

00:05:21.220 --> 00:05:22.796
و این کاریه که همه باید بکنیم.

00:05:22.820 --> 00:05:24.020
مشکل حل شد.

00:05:25.060 --> 00:05:26.540
ولی یک گیر کوچیک اینجا هست.

00:05:27.740 --> 00:05:31.476
وقتی ما از مردم پرسیدیم که
آیا شما اینجور ماشین‌ها رو میخرین،

00:05:31.500 --> 00:05:33.116
گفتن: "قطعا نه."

00:05:33.140 --> 00:05:35.436
(خنده حاضرین)

00:05:35.460 --> 00:05:39.356
مردم میخوان ماشین‌ها رو بخرن که
تحت هر شرایطی ازشون محافظت کنه،

00:05:39.380 --> 00:05:42.996
ولی میخوان بقیه ماشینی رو بخرن که
ضرر رو کمینه کنه.

00:05:43.020 --> 00:05:45.540
(خنده حاضرین)

00:05:46.540 --> 00:05:48.396
ما این مشکل رو قبلاً هم دیده‌ایم.

00:05:48.420 --> 00:05:49.980
بهش میگن دوراهی اجتماعی.

00:05:50.530 --> 00:05:52.796
و برای اینکه دوراهی اجتماعی رو درک کنین،

00:05:52.820 --> 00:05:54.860
باید در تاریخ یه ذره به عقب برگردیم.

00:05:55.820 --> 00:05:58.396
حدود سال ۱۸۰۰ میلادی،

00:05:58.420 --> 00:06:02.156
اقتصاددان انگلیسی ویلیام فورستر للوید
رساله‌ای رو منتشر کرد

00:06:02.180 --> 00:06:04.396
که این سناریو‌ها رو توش توضیح میداد.

00:06:04.420 --> 00:06:06.086
فرض کنید گروهی از چوپان‌ها دارین --

00:06:06.100 --> 00:06:07.436
چوپان‌های انگلیسی --

00:06:07.460 --> 00:06:10.140
که زمین مشترکی
برای چروندن گوسفنداشون دارن.

00:06:11.340 --> 00:06:13.916
حالا، اگه هر چوپان
تعداد مشخصی گوسفند‌ بیاره --

00:06:13.940 --> 00:06:15.436
مثلاً هرکی ۳ تا گوسفند --

00:06:15.460 --> 00:06:17.556
زمین سالم میمونه و کاریش نمیشه،

00:06:17.580 --> 00:06:18.796
چوپان‌ها خوشحالن،

00:06:18.820 --> 00:06:20.436
گوسفند‌ها خوشحالن،

00:06:20.460 --> 00:06:21.660
همه چی خوبه.

00:06:22.260 --> 00:06:24.780
حالا اگه یکی از چوپان‌ها
یه گوسفند اضافی بیاره،

00:06:25.620 --> 00:06:30.340
اون چوپان‌ یکم وضعش بهتر میشه،
و هیچکس ضرر نمیکنه.

00:06:30.980 --> 00:06:34.620
ولی اگه همه چوپان‌ها این تصمیم فردی رو
که برای هرکدومشون منطقیه بگیرن،

00:06:35.660 --> 00:06:38.380
زمین بیش از اندازه بهش فشار میاد،
و خراب میشه.

00:06:39.180 --> 00:06:41.356
چوپان‌ها ضرر میکنن،

00:06:41.380 --> 00:06:43.500
و البته گوسفند‌ها هم ضرر میکنن.

00:06:44.540 --> 00:06:48.220
این مشکل رو خیلی جاها میبینیم:

00:06:48.900 --> 00:06:52.076
مشکل حل ماهیگیری بیش از اندازه،

00:06:52.100 --> 00:06:56.660
یا کاهش انتشار کربن،
برای کند کردن روند تغییرات اقلیمی.

00:06:58.980 --> 00:07:01.900
وقتی این مشکل به مقررات
ماشین‌های بدون راننده(خودران) میرسه،

00:07:02.900 --> 00:07:07.236
زمین مشترک برای چَرا
همون امنیت عمومی میشه --

00:07:07.260 --> 00:07:08.500
که ارزش مشترک هست --

00:07:09.220 --> 00:07:11.196
و چوپان‌ها همون سرنشین‌ها میشن

00:07:11.220 --> 00:07:14.820
یا صاحبان ماشین‌ها که دارن رانندگی میکنن.

00:07:16.780 --> 00:07:19.396
و تصمیم فردی عاقلانه

00:07:19.420 --> 00:07:22.236
که اولیت دادن به امنیت خود شخص هست،

00:07:22.260 --> 00:07:25.396
ممکنه به صورت جمعی در حال
تخریب مال مشترکشون(زمین چَرا) باشن،

00:07:25.420 --> 00:07:27.620
که کمینه کردن کل ضرر هست.

00:07:30.140 --> 00:07:32.276
بهش میگن تراژدی اشتراکها،

00:07:32.300 --> 00:07:33.596
به طور سنتی،

00:07:33.620 --> 00:07:36.716
ولی من فکر میکنم در مورد
ماشین‌های بدون راننده(خودران)،

00:07:36.740 --> 00:07:39.596
شاید مشکل یه ذره ظریف‌تر باشه

00:07:39.620 --> 00:07:43.116
چون حتماً اونی که تصمیم رو میگیره،

00:07:43.140 --> 00:07:44.836
یک انسان نیست.

00:07:44.860 --> 00:07:48.156
ممکنه سازندگان ماشین
خودرو‌ها رو طوری برنامه‌ریزی کنن،

00:07:48.180 --> 00:07:50.700
که امنیت مشتریانشون رو بیشینه کنه،

00:07:51.900 --> 00:07:54.876
و اون ماشین‌ها شاید خودشون یاد بگیرن که
به صورت اتوماتیک

00:07:54.900 --> 00:07:58.420
انجام این کار خطراتی رو
برای عابرین پیاده داره.

00:07:59.340 --> 00:08:00.756
مثلاً توی مثال گوسفند‌ها،

00:08:00.780 --> 00:08:04.396
یک گوسفند الکتریکی داشته باشیم
که ذهن خودش رو داره.

00:08:04.420 --> 00:08:05.876
(خنده حاضرین)

00:08:05.900 --> 00:08:08.980
و گوسفند‌ها شاید برن و بِچرند
در حالی که چوپان نمیدونه.

00:08:10.460 --> 00:08:14.436
این چیزیه که اسمش رو میذاریم
تراژدی اشتراکات الگورتیمی،

00:08:14.460 --> 00:08:16.820
که چالش‌های جدیدی رو در خودش داره.

00:08:22.340 --> 00:08:24.236
معمولاً و به طور سنتی،

00:08:24.260 --> 00:08:27.596
ما این نوع دوراهی‌های اجتماعی رو
با استفاده از مقررات حل میکنیم،

00:08:27.620 --> 00:08:30.356
پس حکومت‌ها یا جوامع گِرد هم میان،

00:08:30.380 --> 00:08:34.116
و تصمیم میگیرن که چه نتیجه‌ای رو میخوان،

00:08:34.140 --> 00:08:36.796
و چه محدودیت‌هایی روی
رفتارهای فردی

00:08:36.820 --> 00:08:38.020
باید اعمال کنند.

00:08:39.420 --> 00:08:42.036
و بعدش با استفاده از نظارت و الزام،

00:08:42.060 --> 00:08:44.619
میتونن مطمئن شن که منافع عمومی حفظ میشه.

00:08:45.260 --> 00:08:46.835
پس چرا نیایم،

00:08:46.859 --> 00:08:48.355
به عنوان قانون گذاران،

00:08:48.379 --> 00:08:51.276
بخوایم که همه ماشین‌ها ضرر رو کمینه کنن؟

00:08:51.300 --> 00:08:53.540
به هرحال، این چیزیه که همه
ادعا میکنن میخوانش.

00:08:55.020 --> 00:08:56.436
و مهمتر از اون،

00:08:56.460 --> 00:08:59.746
مطمئناً منِ نوعی،

00:08:59.746 --> 00:09:03.436
اگه یه ماشین بخرم که ممکن باشه
من رو در یه حالت خیلی خاص قربانی کنه،

00:09:03.460 --> 00:09:05.376
من تنها احمقی نیستم که این کار رو میکنه

00:09:05.376 --> 00:09:07.820
در حالی که بقیه از
محافظت بی‌قید و شرط لذت میبرن.

00:09:08.660 --> 00:09:12.276
در نظرسنجی ما، از مردم پرسیدیم که آیا
از قانون گذاری حمایت میکنن یا نه

00:09:12.300 --> 00:09:13.500
و به این نتایج رسیدیم.

00:09:14.180 --> 00:09:17.940
اول از همه، مردم به قانون گذاری نه گفتند.

00:09:19.100 --> 00:09:20.356
و دوم، اونا گفتن

00:09:20.380 --> 00:09:24.316
"خب اگه شما برای ماشین‌ها قانون بذارین
که این‌ کار رو بکنن و ضرر رو کمینه کنن،

00:09:24.340 --> 00:09:25.820
من اون ماشین‌ها رو نمیخرم."

00:09:27.220 --> 00:09:28.596
پس به طرز مسخره‌ای،

00:09:28.620 --> 00:09:32.116
با قانون‌گذاری برای ماشین‌ها
برای کمینه کردن ضرر،

00:09:32.140 --> 00:09:33.980
ما بیشتر ضرر میکنیم!

00:09:34.860 --> 00:09:38.516
چون مردم نمیخوان که
از تکنولوژی امن‌تر استفاده کنن

00:09:38.540 --> 00:09:40.920
حتی اگه خیلی خیلی امن‌تر از
رانندگان انسانی باشه.

00:09:42.180 --> 00:09:45.596
من جواب نهایی این معما رو نمیدونم،

00:09:45.620 --> 00:09:47.706
ولی فکر میکنم به عنوان یک نقطه شروع،

00:09:47.706 --> 00:09:50.516
ما لازم داریم که همه جمع شیم،

00:09:50.540 --> 00:09:53.300
تا تصمیم بگیریم که چه بده-بستون‌هایی
رو باهاش کنار میایم

00:09:54.180 --> 00:09:57.660
و به راه‌هایی برسیم که
بتونیم اون بده-بستون‌ها رو اجرایی کنیم.

00:09:58.340 --> 00:10:00.876
به عنوان یک نقطه شروع،
دانشجویان فوق‌العاده من،

00:10:00.900 --> 00:10:03.356
ادموند اواد و سوهن دئسوزا.

00:10:03.380 --> 00:10:05.180
سایت ماشین با اخلاق رو ساختند،

00:10:06.020 --> 00:10:08.700
-- که سناریوهای تصادفی
برای شما تولید میکنه

00:10:09.900 --> 00:10:12.356
اساساً دنباله‌ای از تعدادی مساله تصادفی

00:10:12.380 --> 00:10:16.300
که شما باید تصمیم بگیرین که ماشین
در سناریو گفته شده چه کار باید بکنه.

00:10:16.860 --> 00:10:21.460
و ما حتی سن و نوع قربانی‌ها رو عوض میکنیم.

00:10:22.860 --> 00:10:26.826
تا اینجا ما بیش از ۵ میلیون تصمیم
از این سایت برای این سناریو‌ها جمع کردیم

00:10:26.826 --> 00:10:30.520
توسط یک میلیون نفر در سرتاسر جهان

00:10:31.900 --> 00:10:34.596
و این کمک میکنه که
یک تصویر ابتدایی داشته باشیم

00:10:34.620 --> 00:10:37.236
از اینکه مردم
با چه بده-بستان‌هایی کنار میان

00:10:37.260 --> 00:10:39.156
و چه چیزهایی براشون مهمه --

00:10:39.180 --> 00:10:40.620
حتی در فرهنگ‌های مختلف.

00:10:42.060 --> 00:10:43.556
ولی مهمتر از این،

00:10:43.580 --> 00:10:46.956
انجام این کار،
به مردم کمک میکنه که

00:10:46.980 --> 00:10:49.796
دشوار این تصمیم گرفتن‌ها رو بشناسن

00:10:49.820 --> 00:10:53.620
و اینکه قانون‌گذاران
با تصمیم‌های غیرممکن روبرو هستند.

00:10:55.180 --> 00:10:58.756
و شاید این به ما به عنوان یک جامعه کمک کنه
انواع بده-بستانها رو درک کنیم

00:10:58.780 --> 00:11:01.836
که بالاخره در مقررات به کار خواهد رفت.

00:11:01.860 --> 00:11:03.596
و در واقع، من خیلی خوشحالم که میشنوم

00:11:03.620 --> 00:11:05.636
اولین مجموعه قوانین و مقرارت

00:11:05.660 --> 00:11:07.796
توسط وزارت حمل و نقل --

00:11:07.820 --> 00:11:09.196
هفته پیش اعلام شد --

00:11:09.220 --> 00:11:15.796
شامل یک چک لیست ۱۵ امتیازی
برای تمام خودروسازان،

00:11:15.820 --> 00:11:19.076
و شماره ۱۴ یک ملاحظه اخلاقی بود --

00:11:19.100 --> 00:11:20.820
با آن چطوری کنار خواهید آمد.

00:11:22.810 --> 00:11:26.276
همچنین ما افرادی رو داریم که
با دادن خلاصه‌‌ای از آنچه انتخاب کرده‌اند

00:11:26.300 --> 00:11:29.300
تصمیماتشون رو بازتاب میدن.

00:11:30.260 --> 00:11:31.916
بهتون یک مثال نشون میدم --

00:11:31.940 --> 00:11:35.476
فقط بهتون هشدار میدم که
این یک مثال عادی نیست،

00:11:35.500 --> 00:11:36.876
یا کاربر عادی.

00:11:36.900 --> 00:11:40.516
این مثال بیشترین قربانی و بیشترین نجات 
توسط این آدم رو نشون میده.

00:11:40.560 --> 00:11:45.740
(خنده حاضرین)

00:11:46.260 --> 00:11:48.396
بعضی از شما شاید
با نظر این مرد موافق باشین،

00:11:48.420 --> 00:11:50.060
یا شایدم نظر این زن، نمیدونم.

00:11:52.300 --> 00:11:58.436
ولی به نظر میرسه این فرد
یکم سرنشین‌ها رو نسبت به عابرین پیاده

00:11:58.460 --> 00:12:00.556
توی انتخاب‌هاش ترجیح میده

00:12:00.580 --> 00:12:03.396
و از تنبیه کردن عابرین داخل
خیابون خوشحال میشه.

00:12:03.420 --> 00:12:06.460
(خنده حاضرین)

00:12:09.140 --> 00:12:10.356
حالا بیاین جمع بندی کنیم.

00:12:10.379 --> 00:12:13.795
ما با یک سوال که اسمش رو
دوراهی اخلاقی میذاریم، شروع کردیم --

00:12:13.820 --> 00:12:16.876
که ماشین توی سناریو خاص چه کار باید بکنه:

00:12:16.900 --> 00:12:18.250
منحرف بشه یا ادامه بده؟

00:12:19.060 --> 00:12:21.796
ولی بعدش فهمیدیم که این مساله دیگه‌ای بود.

00:12:21.820 --> 00:12:26.356
مساله، هم‌نظر کردن جامعه
و اجرای بده-بستان‌هایی بود

00:12:26.380 --> 00:12:28.316
که جامعه باهاش کنار میاد.

00:12:28.340 --> 00:12:29.596
که یک معمای اجتماعی هست.

00:12:29.620 --> 00:12:34.636
در سال ۱۹۴۰، ایزاک آسیموف
قوانین مشهورش برای ربات‌ها را نوشت --

00:12:34.660 --> 00:12:35.980
سه قانون برای ربات‌ها.

00:12:37.060 --> 00:12:39.516
ربات نباید به انسان آسیب بزنه،

00:12:39.540 --> 00:12:42.076
ربات نباید از انسان سرپیچی کنه،

00:12:42.100 --> 00:12:45.356
و ربات نباید به خودش اجازه بده 
که آسیب بزنه --

00:12:45.380 --> 00:12:47.340
به ترتیب اهمیت از اول به سوم.

00:12:48.180 --> 00:12:50.316
ولی بعد از گذشت حدود ۴۰ سال

00:12:50.340 --> 00:12:54.076
و بعد داستان‌های زیادی که
به این قوانین فشار آوردن،

00:12:54.100 --> 00:12:57.796
آسیموف قانون صفرم رو مطرح کرد

00:12:57.820 --> 00:13:00.076
که بر هر سه قانون قبلی تقدم داره،

00:13:00.100 --> 00:13:03.380
و اون این بود که ربات نباید به
کلیت انسان‌ها آسیب بزنه.

00:13:04.300 --> 00:13:08.676
من نمیدونم مصداق این قانون در زمینه
ماشین‌های خودران چی میشه

00:13:08.700 --> 00:13:11.436
یا هر حالت خاصی،

00:13:11.460 --> 00:13:13.676
و نمیدونم که ما چطوری میخوایم اجراش کنیم،

00:13:13.700 --> 00:13:15.766
ولی فکر میکنم که با فهمیدنِ اینکه

00:13:15.766 --> 00:13:21.396
مقررات ماشین‌های خودران تنها 
یک مساله مرتبط با تکنولوژی نیست

00:13:21.420 --> 00:13:24.700
و یک مساله همکاری اجتماعی هم هست،

00:13:25.620 --> 00:13:28.500
میتونم امیدوار باشم که حداقل شروع به
پرسیدن سوالات درست کنیم.

00:13:29.020 --> 00:13:30.236
سپاس گزارم.

00:13:30.260 --> 00:13:33.180
(تشویق حاضرین)

