WEBVTT
Kind: captions
Language: pt-PT

00:00:00.000 --> 00:00:07.000
Tradutor: Diana Almeida
Revisora: Margarida Ferreira

00:00:12.820 --> 00:00:16.900
Hoje, vou falar
sobre tecnologia e sociedade.

00:00:18.860 --> 00:00:22.286
O Departamento de Transportes
estimou que, no ano passado,

00:00:22.580 --> 00:00:26.890
morreram 35 mil pessoas devido
a acidentes rodoviários, somente nos EUA.

00:00:27.860 --> 00:00:32.880
A nível mundial, 1,2 milhões de pessoas 
morrem por ano em acidentes rodoviários.

00:00:33.580 --> 00:00:37.530
Se existisse uma forma de eliminar 
90% desses acidentes,

00:00:37.620 --> 00:00:39.240
vocês iriam apoiá-la?

00:00:39.540 --> 00:00:40.800
Claro que sim.

00:00:40.970 --> 00:00:44.455
É isto que a tecnologia de
carros autónomos pretende alcançar

00:00:44.540 --> 00:00:47.356
ao eliminar a causa principal 
dos acidentes

00:00:47.380 --> 00:00:48.930
— o erro humano.

00:00:49.740 --> 00:00:54.966
Imaginem-se num carro autónomo
no ano de 2030,

00:00:55.180 --> 00:00:58.886
descontraídos e a ver este vídeo
"vintage" do TEDxCambridge.

00:00:58.930 --> 00:01:00.930
(Risos)

00:01:01.340 --> 00:01:02.556
De repente,

00:01:02.580 --> 00:01:06.000
o carro sofre uma falha mecânica
e é incapaz de parar.

00:01:07.180 --> 00:01:09.030
Se o carro continuar em frente,

00:01:09.540 --> 00:01:13.860
irá colidir com um monte de peões
que atravessam a rua,

00:01:14.900 --> 00:01:17.035
mas o carro poderá desviar-se,

00:01:17.059 --> 00:01:19.086
atingindo um peão no passeio,

00:01:19.140 --> 00:01:21.440
matando-o para salvar os restantes peões.

00:01:21.860 --> 00:01:24.690
O que deverá o carro fazer, 
e quem deverá decidir?

00:01:25.340 --> 00:01:28.876
E se o carro pudesse desviar-se 
contra uma parede,

00:01:28.900 --> 00:01:32.196
chocando e matando o passageiro,

00:01:32.220 --> 00:01:34.690
de forma a salvar aqueles peões?

00:01:35.060 --> 00:01:38.400
Este cenário é inspirado 
pelo dilema do elétrico,

00:01:38.780 --> 00:01:42.726
inventado por filósofos 
há algumas décadas,

00:01:42.800 --> 00:01:44.340
para pensar sobre a ética.

00:01:45.940 --> 00:01:48.570
A forma como pensamos
neste problema é importante.

00:01:48.630 --> 00:01:51.076
Por exemplo, nós podemos
nem sequer pensar nele.

00:01:51.100 --> 00:01:54.476
Podemos dizer que 
este cenário é irrealista,

00:01:54.500 --> 00:01:57.180
extremamente improvável
ou simplesmente absurdo.

00:01:57.580 --> 00:02:00.436
Mas eu acho que esta crítica
perde de vista o objetivo

00:02:00.470 --> 00:02:03.010
porque leva o cenário demasiado à letra.

00:02:03.740 --> 00:02:06.296
É óbvio que nenhum acidente 
se parecerá com este;

00:02:06.500 --> 00:02:09.696
nenhum acidente tem duas ou três opções

00:02:09.860 --> 00:02:12.640
em que alguém acaba morto inevitavelmente.

00:02:13.300 --> 00:02:16.016
Em vez disso, o carro irá calcular algo

00:02:16.070 --> 00:02:20.796
como a probabilidade de embater 
num certo grupo de pessoas

00:02:20.820 --> 00:02:24.156
e, caso se desvie para 
uma direção ou para outra,

00:02:24.180 --> 00:02:27.636
poderá aumentar o risco dos passageiros
ou de outros condutores

00:02:27.660 --> 00:02:29.196
em vez dos peões.

00:02:29.220 --> 00:02:31.790
Irá ser um cálculo mais complexo,

00:02:32.300 --> 00:02:34.930
mas irá envolver na mesma
tomar decisões

00:02:35.660 --> 00:02:38.930
e muitas das vezes, 
tomar essas decisões requer ética.

00:02:39.660 --> 00:02:42.396
Podemos dizer:
"Não nos vamos preocupar com isso.

00:02:42.420 --> 00:02:47.150
"Vamos esperar até que a tecnologia esteja
totalmente desenvolvida e 100% segura."

00:02:48.340 --> 00:02:52.270
Suponhamos que, de facto, se possam
eliminar 90% desses acidentes,

00:02:52.900 --> 00:02:56.040
ou mesmo 99% nos próximos 10 anos.

00:02:56.740 --> 00:02:59.916
E se eliminar esse 1% dos acidentes

00:02:59.940 --> 00:03:03.190
exigir mais 50 anos de pesquisa?

00:03:04.220 --> 00:03:06.190
Não deveremos adotar a tecnologia?

00:03:06.540 --> 00:03:11.316
Trata-se de 60 milhões de pessoas
mortas em acidentes,

00:03:11.340 --> 00:03:13.480
se mantivermos o ritmo atual.

00:03:14.580 --> 00:03:18.126
Isto significa que
esperar por segurança total

00:03:18.146 --> 00:03:19.576
também é uma escolha

00:03:19.656 --> 00:03:21.950
e também envolve tomar decisões.

00:03:23.380 --> 00:03:27.716
Nas redes sociais, as pessoas
têm arranjado inúmeras formas

00:03:27.740 --> 00:03:29.756
para não ter de pensar neste problema.

00:03:29.780 --> 00:03:32.996
Uma pessoa sugeriu que o carro 
devia, de alguma forma,

00:03:33.020 --> 00:03:35.156
passar entre os peões...

00:03:35.180 --> 00:03:36.196
(Risos)

00:03:36.220 --> 00:03:37.686
e o peão no passeio.

00:03:37.730 --> 00:03:40.970
Claro que, se isso for possível, 
é o que o carro deve fazer.

00:03:41.740 --> 00:03:44.860
Estamos interessados em cenários 
em que isso não é possível.

00:03:45.100 --> 00:03:50.516
O meu cenário favorito foi 
uma sugestão feita por um "blogger",

00:03:50.540 --> 00:03:53.556
de o carro ter um botão de ejeção 
que se pressiona...

00:03:53.580 --> 00:03:54.796
(Risos)

00:03:54.820 --> 00:03:56.647
antes de o carro se autodestruir.

00:03:56.741 --> 00:03:58.421
(Risos)

00:03:59.660 --> 00:04:04.960
Se nós reconhecermos que os carros 
terão que tomar decisões na estrada,

00:04:06.020 --> 00:04:08.260
como devemos ponderar essas decisões

00:04:09.140 --> 00:04:10.716
e como é que decidimos?

00:04:10.830 --> 00:04:14.206
Talvez devêssemos fazer um inquérito 
para saber o que a sociedade quer

00:04:14.266 --> 00:04:15.860
porque, em última análise,

00:04:15.880 --> 00:04:19.630
as normas e as leis são o espelho
dos valores da sociedade.

00:04:19.990 --> 00:04:21.480
Então foi o que fizemos.

00:04:21.700 --> 00:04:23.316
Com os meus colaboradores,

00:04:23.340 --> 00:04:25.676
Jean-François Bonnefon e Azim Shariff,

00:04:25.700 --> 00:04:27.316
realizámos um inquérito

00:04:27.340 --> 00:04:30.195
no qual apresentámos às pessoas 
estes tipos de cenários.

00:04:30.219 --> 00:04:33.996
Demos-lhes duas opções, 
inspiradas por dois filósofos:

00:04:34.020 --> 00:04:36.760
Jeremy Bentham e Immanuel Kant.

00:04:37.420 --> 00:04:40.516
Bentham diz que o carro 
deve seguir a ética utilitarista:

00:04:40.540 --> 00:04:43.956
fazer aquilo que irá 
minimizar os danos totais

00:04:43.980 --> 00:04:46.796
— mesmo que isso implique 
matar um peão no passeio

00:04:46.820 --> 00:04:49.660
e, mesmo que isso 
acabe por matar o passageiro.

00:04:49.940 --> 00:04:54.916
Immanul Kant diz que o carro 
deve seguir princípios morais,

00:04:54.940 --> 00:04:56.750
como "Não matarás."

00:04:57.300 --> 00:05:01.756
Então, não se deve agir, caso isso 
implique magoar um ser humano,

00:05:01.780 --> 00:05:04.426
e deverá deixar-se que o carro 
siga o seu percurso

00:05:04.500 --> 00:05:06.520
mesmo que se magoem mais pessoas.

00:05:07.460 --> 00:05:08.860
O que acham?

00:05:09.190 --> 00:05:10.900
Bentham ou Kant?

00:05:11.580 --> 00:05:13.016
Estes são os resultados.

00:05:13.090 --> 00:05:15.030
A maioria concordou com Bentham.

00:05:15.980 --> 00:05:19.756
Parece que as pessoas preferem 
que os carros sejam utilitários,

00:05:19.760 --> 00:05:22.756
minimizem os danos totais,
e é isso que todos nós devemos fazer.

00:05:22.820 --> 00:05:24.160
Problema resolvido.

00:05:25.060 --> 00:05:26.830
Mas há um pequeno senão.

00:05:27.740 --> 00:05:31.266
Quando perguntámos às pessoas 
se elas comprariam esses carros,

00:05:31.500 --> 00:05:33.366
elas responderam: "Claro que não."

00:05:33.430 --> 00:05:35.176
(Risos)

00:05:35.460 --> 00:05:39.276
Elas gostariam de comprar carros 
que as protegessem a todo o custo,

00:05:39.380 --> 00:05:43.126
mas querem que os restantes 
comprem carros que minimizem os danos.

00:05:43.236 --> 00:05:45.750
(Risos)

00:05:46.540 --> 00:05:48.396
Já vimos este problema.

00:05:48.420 --> 00:05:50.180
Chama-se dilema social.

00:05:50.980 --> 00:05:52.796
Para perceber o dilema social,

00:05:52.820 --> 00:05:54.860
temos que recuar um pouco na história.

00:05:55.820 --> 00:05:58.396
No início do século XIX,

00:05:58.420 --> 00:06:02.156
o economista inglês William Forster Lloyd 
publicou um folheto

00:06:02.180 --> 00:06:04.396
que descrevia o seguinte cenário:

00:06:04.420 --> 00:06:07.376
Temos um grupo de pastores
— pastores ingleses —

00:06:07.460 --> 00:06:10.320
que partilham um terreno
para as ovelhas pastarem.

00:06:11.340 --> 00:06:13.916
Se cada pastor levar
um certo número de ovelhas

00:06:13.940 --> 00:06:15.566
— por exemplo, três ovelhas —

00:06:15.596 --> 00:06:18.726
o terreno recuperará,
os pastores ficam contentes,

00:06:18.820 --> 00:06:20.436
as ovelhas ficam contentes,

00:06:20.460 --> 00:06:21.840
tudo fica bem.

00:06:22.260 --> 00:06:24.820
Se um pastor levar uma ovelha extra,

00:06:25.620 --> 00:06:30.440
terá uma pequena vantagem, 
mas ninguém será prejudicado.

00:06:30.980 --> 00:06:34.790
Mas, se cada pastor tomar
a mesma decisão,

00:06:35.660 --> 00:06:38.870
a terra será sobreutilizada
e ficará empobrecida

00:06:39.180 --> 00:06:41.356
prejudicando todos os pastores

00:06:41.380 --> 00:06:43.500
e, claro, as ovelhas também.

00:06:44.540 --> 00:06:48.220
Vemos este problema em muitos sítios:

00:06:48.900 --> 00:06:51.916
na dificuldade de controlar 
a pesca excessiva,

00:06:52.100 --> 00:06:56.890
ou em reduzir as emissões de carbono 
para atenuar as mudanças climáticas.

00:06:58.980 --> 00:07:02.220
Quando se trata da regulamentação 
de carros autónomos,

00:07:02.900 --> 00:07:07.236
os terrenos baldios são,
basicamente, a segurança pública

00:07:07.260 --> 00:07:09.090
— ou seja, o bem comum.

00:07:09.220 --> 00:07:11.316
Os pastores são os passageiros

00:07:11.370 --> 00:07:15.120
ou os donos de carros que escolham 
conduzir esse tipo de veículo.

00:07:16.780 --> 00:07:19.396
E ao fazer essa escolha
individual e racional

00:07:19.420 --> 00:07:22.236
de dar prioridade à sua segurança,

00:07:22.260 --> 00:07:25.396
eles podem estar, em conjunto,
a prejudicar o bem comum,

00:07:25.420 --> 00:07:28.010
que é o que está
a minimizar os danos totais.

00:07:30.140 --> 00:07:33.536
É a chamada "tragédia dos comuns",
tradicionalmente.

00:07:33.620 --> 00:07:36.716
Mas eu penso que, no caso 
de carros autónomos,

00:07:36.740 --> 00:07:39.596
o problema pode ser 
um pouco mais insidioso

00:07:39.620 --> 00:07:43.116
pois não é necessariamente um ser humano

00:07:43.140 --> 00:07:44.836
a tomar essas decisões.

00:07:44.860 --> 00:07:48.156
Os fabricantes de automóveis
podem programar carros

00:07:48.180 --> 00:07:50.930
que irão maximizar
a segurança dos seus clientes,

00:07:51.900 --> 00:07:54.876
e esses carros poderão aprender sozinhos

00:07:54.900 --> 00:07:58.660
que, ao fazer isso, aumentará
o risco para os peões.

00:07:59.340 --> 00:08:00.836
Na metáfora das ovelhas,

00:08:00.860 --> 00:08:04.536
é como se tivéssemos ovelhas eletrónicas 
que pensam por si mesmas.

00:08:04.610 --> 00:08:05.876
(Risos)

00:08:06.050 --> 00:08:09.380
Elas podem ir pastar
sem que o pastor saiba disso.

00:08:10.460 --> 00:08:14.436
É a isso que chamamos
a tragédia dos algoritmos comuns,

00:08:14.460 --> 00:08:17.160
e isso apresenta novos tipos de problemas.

00:08:22.340 --> 00:08:24.236
Tipicamente,

00:08:24.260 --> 00:08:27.686
nós resolvemos este tipo de 
dilemas sociais com regulamentações

00:08:27.720 --> 00:08:30.356
em que os governos
e as comunidades se juntam,

00:08:30.380 --> 00:08:34.116
e decidem coletivamente
que tipo de resultado querem

00:08:34.140 --> 00:08:38.216
e que tipo de restrições é necessário
aplicar no comportamento individual.

00:08:39.420 --> 00:08:42.086
E depois, com o acompanhamento 
e a sua imposição,

00:08:42.116 --> 00:08:44.899
podem certificar-se de que 
o bem público é preservado.

00:08:45.260 --> 00:08:48.275
Então porque é que nós,
enquanto entidades reguladoras,

00:08:48.379 --> 00:08:51.276
não exigimos que todos os carros 
minimizem os danos?

00:08:51.490 --> 00:08:54.080
Afinal, as pessoas dizem 
que é isso que querem.

00:08:55.020 --> 00:08:56.436
E mais importante ainda,

00:08:56.460 --> 00:09:00.526
enquanto indivíduo, tenho a certeza
de que, se comprasse um carro,

00:09:00.580 --> 00:09:03.436
numa situação improvável, 
decidisse sacrificar-me,

00:09:03.460 --> 00:09:05.216
eu não seria o único a fazer isso

00:09:05.270 --> 00:09:08.210
enquanto as demais pessoas
teriam uma proteção ilimitada.

00:09:08.940 --> 00:09:12.276
No inquérito, perguntámos às pessoas 
se apoiariam a regulamentação

00:09:12.300 --> 00:09:13.780
e obtivemos este resultado:

00:09:14.180 --> 00:09:18.450
Para começar, as pessoas disseram 
que não apoiariam a regulamentação.

00:09:19.100 --> 00:09:20.546
E em segundo, disseram:

00:09:20.590 --> 00:09:24.316
"Se regulamentarem os carros para 
fazer isso e minimizar os danos totais,

00:09:24.340 --> 00:09:26.450
"não irei comprar esses carros."

00:09:27.220 --> 00:09:28.596
Então, ironicamente,

00:09:28.620 --> 00:09:32.116
ao regulamentar os carros 
para minimizarem os danos,

00:09:32.140 --> 00:09:34.350
pode-se estar a causar mais danos

00:09:34.860 --> 00:09:38.516
pois as pessoas podem não optar 
pela tecnologia mais segura

00:09:38.516 --> 00:09:41.046
mesmo sendo mais segura
do que os condutores humanos.

00:09:42.180 --> 00:09:45.476
Eu não possuo a resposta final
para este enigma,

00:09:45.586 --> 00:09:47.116
mas acho que, para começar,

00:09:47.220 --> 00:09:49.186
é necessário que a sociedade

00:09:49.186 --> 00:09:53.536
concorde nas decisões mais adequadas

00:09:54.180 --> 00:09:57.990
e encontre formas de como 
se podem impor essas decisões.

00:09:58.340 --> 00:10:00.916
Como ponto de partida, 
os meus brilhantes alunos,

00:10:00.960 --> 00:10:03.356
Edmond Awad e Sohan Dsouza,

00:10:03.380 --> 00:10:05.610
criaram o site "Moral Machine",

00:10:06.020 --> 00:10:08.970
que gera cenários aleatórios

00:10:09.900 --> 00:10:12.196
— basicamente, um monte
de dilemas aleatórios

00:10:12.230 --> 00:10:14.030
em que vocês têm que escolher

00:10:14.080 --> 00:10:16.770
o que um carro deve fazer
num determinado cenário.

00:10:16.860 --> 00:10:21.460
E vamos mudando as idades
e as espécies das diversas vítimas.

00:10:22.860 --> 00:10:26.556
Até agora, recolhemos 
mais de cinco milhões de respostas

00:10:26.640 --> 00:10:29.440
de mais de 1 milhão de pessoas,
a nível mundial,

00:10:30.220 --> 00:10:31.780
através desse site.

00:10:32.180 --> 00:10:34.596
Isto está a ajudar-nos
a formar um quadro inicial

00:10:34.620 --> 00:10:37.236
do tipo de decisões que 
as pessoas acham adequadas

00:10:37.260 --> 00:10:39.156
e o que é importante para elas,

00:10:39.180 --> 00:10:40.950
mesmo noutras culturas.

00:10:42.060 --> 00:10:43.556
Mas mais importante,

00:10:43.580 --> 00:10:46.086
fazer este exercício ajuda as pessoas

00:10:46.110 --> 00:10:49.796
a reconhecer a dificuldade
de fazer aquelas escolhas

00:10:49.820 --> 00:10:53.840
e que a entidade reguladora está
incumbida de fazer escolhas impossíveis.

00:10:55.180 --> 00:10:58.926
Enquanto sociedade, talvez isto nos 
ajude a perceber que tipos de decisões

00:10:58.980 --> 00:11:01.656
deverão ser implementadas 
na regulamentação.

00:11:01.860 --> 00:11:03.596
Fiquei bastante feliz ao ouvir

00:11:03.620 --> 00:11:05.776
que o primeiro conjunto de regulamentações

00:11:05.820 --> 00:11:08.976
anunciado pelo Departamento de Transporte,
na semana passada,

00:11:09.220 --> 00:11:11.600
inclui uma lista de 15 pontos

00:11:11.670 --> 00:11:15.490
que todos os fabricantes
de automóveis devem fornecer.

00:11:15.820 --> 00:11:19.076
O número 14 foi uma reflexão ética

00:11:19.120 --> 00:11:21.300
sobre como a pessoa vai lidar com isso.

00:11:23.620 --> 00:11:26.316
Nós fazemos as pessoas 
refletir nas suas decisões

00:11:26.390 --> 00:11:29.590
ao dar-lhes uma síntese do que escolheram.

00:11:30.260 --> 00:11:31.916
Vou dar-vos um exemplo

00:11:31.940 --> 00:11:36.596
e desde já vos aviso que este não é 
o exemplo de um utilizador habitual.

00:11:36.900 --> 00:11:40.716
Estas são a personagem mais sacrificada 
e a mais salvo para esta pessoa.

00:11:40.910 --> 00:11:44.000
(Risos)

00:11:46.500 --> 00:11:50.126
Alguns de vocês podem concordar com ele,
ou com ela — não sabemos.

00:11:52.300 --> 00:11:58.286
Mas esta pessoa preferiu salvar
os passageiros ao invés dos peões

00:11:58.460 --> 00:12:00.346
nas suas escolhas,

00:12:00.580 --> 00:12:03.616
e parece muito feliz por punir
peões que não cumprem a lei.

00:12:03.700 --> 00:12:06.740
(Risos)

00:12:09.140 --> 00:12:10.416
Vamos então, concluir.

00:12:10.459 --> 00:12:13.795
Começámos com a questão
— chamemos-lhe dilema ético —

00:12:13.820 --> 00:12:16.876
sobre o que o carro deverá
fazer num cenário específico:

00:12:16.960 --> 00:12:18.720
desviar-se ou continuar?

00:12:19.060 --> 00:12:21.796
Mas depois apercebemo-nos 
que o problema era outro.

00:12:21.920 --> 00:12:26.356
O problema era sobre como levar 
a sociedade a concordar e a impor

00:12:26.380 --> 00:12:28.316
as decisões que consideram adequadas.

00:12:28.340 --> 00:12:29.726
É um dilema social.

00:12:29.790 --> 00:12:34.636
Nos anos 40, Isaac Asimov escreveu 
as suas famosas leis da robótica,

00:12:34.780 --> 00:12:36.520
as três leis da robótica.

00:12:37.060 --> 00:12:39.516
Um robô não pode magoar um ser humano,

00:12:39.540 --> 00:12:42.076
um robô não pode desobedecer 
a um ser humano,

00:12:42.100 --> 00:12:45.356
e um robô não se pode 
danificar-se a ele mesmo

00:12:45.400 --> 00:12:47.830
— seguindo esta ordem de importância.

00:12:48.180 --> 00:12:50.316
Mas cerca de 40 anos depois,

00:12:50.340 --> 00:12:53.866
e depois de tantas histórias 
que levavam estas leis ao limite,

00:12:54.100 --> 00:12:57.636
Asimov introduziu a lei zero,

00:12:57.836 --> 00:13:00.126
que as precede a todas,

00:13:00.250 --> 00:13:03.800
aquela em que um robô 
não pode prejudicar a humanidade.

00:13:04.300 --> 00:13:08.676
Eu não sei o que isto significa
no contexto de carros autónomos

00:13:08.700 --> 00:13:11.146
ou em qualquer situação específica,

00:13:11.460 --> 00:13:13.676
e não sei como é que 
podemos implementar isto,

00:13:13.700 --> 00:13:15.286
mas acho que, ao reconhecer

00:13:15.350 --> 00:13:21.226
que a regulamentação dos carros autónomos 
não é apenas um problema tecnológico,

00:13:21.420 --> 00:13:25.010
mas também um problema
de cooperação social,

00:13:25.620 --> 00:13:28.790
espero que possamos começar 
a fazer as questões certas.

00:13:29.020 --> 00:13:30.236
Obrigado.

00:13:30.430 --> 00:13:33.530
(Aplausos)

