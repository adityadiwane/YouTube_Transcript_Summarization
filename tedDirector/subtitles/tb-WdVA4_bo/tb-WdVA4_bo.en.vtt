WEBVTT
Kind: captions
Language: en

00:00:12.820 --> 00:00:16.900
Today I'm going to talk
about technology and society.

00:00:18.860 --> 00:00:22.556
The Department of Transport
estimated that last year

00:00:22.580 --> 00:00:26.660
35,000 people died
from traffic crashes in the US alone.

00:00:27.860 --> 00:00:32.660
Worldwide, 1.2 million people
die every year in traffic accidents.

00:00:33.580 --> 00:00:37.676
If there was a way we could eliminate
90 percent of those accidents,

00:00:37.700 --> 00:00:38.900
would you support it?

00:00:39.540 --> 00:00:40.836
Of course you would.

00:00:40.860 --> 00:00:44.515
This is what driverless car technology
promises to achieve

00:00:44.540 --> 00:00:47.356
by eliminating the main
source of accidents --

00:00:47.380 --> 00:00:48.580
human error.

00:00:49.740 --> 00:00:55.156
Now picture yourself
in a driverless car in the year 2030,

00:00:55.180 --> 00:00:58.636
sitting back and watching
this vintage TEDxCambridge video.

00:00:58.660 --> 00:01:00.660
(Laughter)

00:01:01.340 --> 00:01:02.556
All of a sudden,

00:01:02.580 --> 00:01:05.860
the car experiences mechanical failure
and is unable to stop.

00:01:07.180 --> 00:01:08.700
If the car continues,

00:01:09.540 --> 00:01:13.660
it will crash into a bunch
of pedestrians crossing the street,

00:01:14.900 --> 00:01:17.035
but the car may swerve,

00:01:17.059 --> 00:01:18.916
hitting one bystander,

00:01:18.940 --> 00:01:21.020
killing them to save the pedestrians.

00:01:21.860 --> 00:01:24.460
What should the car do,
and who should decide?

00:01:25.340 --> 00:01:28.876
What if instead the car
could swerve into a wall,

00:01:28.900 --> 00:01:32.196
crashing and killing you, the passenger,

00:01:32.220 --> 00:01:34.540
in order to save those pedestrians?

00:01:35.060 --> 00:01:38.140
This scenario is inspired
by the trolley problem,

00:01:38.780 --> 00:01:42.556
which was invented
by philosophers a few decades ago

00:01:42.580 --> 00:01:43.820
to think about ethics.

00:01:45.940 --> 00:01:48.436
Now, the way we think
about this problem matters.

00:01:48.460 --> 00:01:51.076
We may for example
not think about it at all.

00:01:51.100 --> 00:01:54.476
We may say this scenario is unrealistic,

00:01:54.500 --> 00:01:56.820
incredibly unlikely, or just silly.

00:01:57.580 --> 00:02:00.316
But I think this criticism
misses the point

00:02:00.340 --> 00:02:02.500
because it takes
the scenario too literally.

00:02:03.740 --> 00:02:06.476
Of course no accident
is going to look like this;

00:02:06.500 --> 00:02:09.836
no accident has two or three options

00:02:09.860 --> 00:02:11.860
where everybody dies somehow.

00:02:13.300 --> 00:02:15.876
Instead, the car is going
to calculate something

00:02:15.900 --> 00:02:20.796
like the probability of hitting
a certain group of people,

00:02:20.820 --> 00:02:24.156
if you swerve one direction
versus another direction,

00:02:24.180 --> 00:02:27.636
you might slightly increase the risk
to passengers or other drivers

00:02:27.660 --> 00:02:29.196
versus pedestrians.

00:02:29.220 --> 00:02:31.380
It's going to be
a more complex calculation,

00:02:32.300 --> 00:02:34.820
but it's still going
to involve trade-offs,

00:02:35.660 --> 00:02:38.540
and trade-offs often require ethics.

00:02:39.660 --> 00:02:42.396
We might say then,
"Well, let's not worry about this.

00:02:42.420 --> 00:02:47.060
Let's wait until technology
is fully ready and 100 percent safe."

00:02:48.340 --> 00:02:52.020
Suppose that we can indeed
eliminate 90 percent of those accidents,

00:02:52.900 --> 00:02:55.740
or even 99 percent in the next 10 years.

00:02:56.740 --> 00:02:59.916
What if eliminating
the last one percent of accidents

00:02:59.940 --> 00:03:03.060
requires 50 more years of research?

00:03:04.220 --> 00:03:06.020
Should we not adopt the technology?

00:03:06.540 --> 00:03:11.316
That's 60 million people
dead in car accidents

00:03:11.340 --> 00:03:13.100
if we maintain the current rate.

00:03:14.580 --> 00:03:15.796
So the point is,

00:03:15.820 --> 00:03:19.436
waiting for full safety is also a choice,

00:03:19.460 --> 00:03:21.620
and it also involves trade-offs.

00:03:23.380 --> 00:03:27.716
People online on social media
have been coming up with all sorts of ways

00:03:27.740 --> 00:03:29.756
to not think about this problem.

00:03:29.780 --> 00:03:32.996
One person suggested
the car should just swerve somehow

00:03:33.020 --> 00:03:35.156
in between the passengers --

00:03:35.180 --> 00:03:36.196
(Laughter)

00:03:36.220 --> 00:03:37.476
and the bystander.

00:03:37.500 --> 00:03:40.860
Of course if that's what the car can do,
that's what the car should do.

00:03:41.740 --> 00:03:44.580
We're interested in scenarios
in which this is not possible.

00:03:45.100 --> 00:03:50.516
And my personal favorite
was a suggestion by a blogger

00:03:50.540 --> 00:03:53.556
to have an eject button in the car
that you press --

00:03:53.580 --> 00:03:54.796
(Laughter)

00:03:54.820 --> 00:03:56.487
just before the car self-destructs.

00:03:56.511 --> 00:03:58.191
(Laughter)

00:03:59.660 --> 00:04:04.860
So if we acknowledge that cars
will have to make trade-offs on the road,

00:04:06.020 --> 00:04:07.900
how do we think about those trade-offs,

00:04:09.140 --> 00:04:10.716
and how do we decide?

00:04:10.740 --> 00:04:13.876
Well, maybe we should run a survey
to find out what society wants,

00:04:13.900 --> 00:04:15.356
because ultimately,

00:04:15.380 --> 00:04:19.340
regulations and the law
are a reflection of societal values.

00:04:19.860 --> 00:04:21.100
So this is what we did.

00:04:21.700 --> 00:04:23.316
With my collaborators,

00:04:23.340 --> 00:04:25.676
Jean-FranÃ§ois Bonnefon and Azim Shariff,

00:04:25.700 --> 00:04:27.316
we ran a survey

00:04:27.340 --> 00:04:30.195
in which we presented people
with these types of scenarios.

00:04:30.219 --> 00:04:33.996
We gave them two options
inspired by two philosophers:

00:04:34.020 --> 00:04:36.660
Jeremy Bentham and Immanuel Kant.

00:04:37.420 --> 00:04:40.516
Bentham says the car
should follow utilitarian ethics:

00:04:40.540 --> 00:04:43.956
it should take the action
that will minimize total harm --

00:04:43.980 --> 00:04:46.796
even if that action will kill a bystander

00:04:46.820 --> 00:04:49.260
and even if that action
will kill the passenger.

00:04:49.940 --> 00:04:54.916
Immanuel Kant says the car
should follow duty-bound principles,

00:04:54.940 --> 00:04:56.500
like "Thou shalt not kill."

00:04:57.300 --> 00:05:01.756
So you should not take an action
that explicitly harms a human being,

00:05:01.780 --> 00:05:04.236
and you should let the car take its course

00:05:04.260 --> 00:05:06.220
even if that's going to harm more people.

00:05:07.460 --> 00:05:08.660
What do you think?

00:05:09.180 --> 00:05:10.700
Bentham or Kant?

00:05:11.580 --> 00:05:12.836
Here's what we found.

00:05:12.860 --> 00:05:14.660
Most people sided with Bentham.

00:05:15.980 --> 00:05:19.756
So it seems that people
want cars to be utilitarian,

00:05:19.780 --> 00:05:21.196
minimize total harm,

00:05:21.220 --> 00:05:22.796
and that's what we should all do.

00:05:22.820 --> 00:05:24.020
Problem solved.

00:05:25.060 --> 00:05:26.540
But there is a little catch.

00:05:27.740 --> 00:05:31.476
When we asked people
whether they would purchase such cars,

00:05:31.500 --> 00:05:33.116
they said, "Absolutely not."

00:05:33.140 --> 00:05:35.436
(Laughter)

00:05:35.460 --> 00:05:39.356
They would like to buy cars
that protect them at all costs,

00:05:39.380 --> 00:05:42.996
but they want everybody else
to buy cars that minimize harm.

00:05:43.020 --> 00:05:45.540
(Laughter)

00:05:46.540 --> 00:05:48.396
We've seen this problem before.

00:05:48.420 --> 00:05:49.980
It's called a social dilemma.

00:05:50.980 --> 00:05:52.796
And to understand the social dilemma,

00:05:52.820 --> 00:05:54.860
we have to go a little bit
back in history.

00:05:55.820 --> 00:05:58.396
In the 1800s,

00:05:58.420 --> 00:06:02.156
English economist William Forster Lloyd
published a pamphlet

00:06:02.180 --> 00:06:04.396
which describes the following scenario.

00:06:04.420 --> 00:06:06.076
You have a group of farmers --

00:06:06.100 --> 00:06:07.436
English farmers --

00:06:07.460 --> 00:06:10.140
who are sharing a common land
for their sheep to graze.

00:06:11.340 --> 00:06:13.916
Now, if each farmer
brings a certain number of sheep --

00:06:13.940 --> 00:06:15.436
let's say three sheep --

00:06:15.460 --> 00:06:17.556
the land will be rejuvenated,

00:06:17.580 --> 00:06:18.796
the farmers are happy,

00:06:18.820 --> 00:06:20.436
the sheep are happy,

00:06:20.460 --> 00:06:21.660
everything is good.

00:06:22.260 --> 00:06:24.780
Now, if one farmer brings one extra sheep,

00:06:25.620 --> 00:06:30.340
that farmer will do slightly better,
and no one else will be harmed.

00:06:30.980 --> 00:06:34.620
But if every farmer made
that individually rational decision,

00:06:35.660 --> 00:06:38.380
the land will be overrun,
and it will be depleted

00:06:39.180 --> 00:06:41.356
to the detriment of all the farmers,

00:06:41.380 --> 00:06:43.500
and of course,
to the detriment of the sheep.

00:06:44.540 --> 00:06:48.220
We see this problem in many places:

00:06:48.900 --> 00:06:52.076
in the difficulty of managing overfishing,

00:06:52.100 --> 00:06:56.660
or in reducing carbon emissions
to mitigate climate change.

00:06:58.980 --> 00:07:01.900
When it comes to the regulation
of driverless cars,

00:07:02.900 --> 00:07:07.236
the common land now
is basically public safety --

00:07:07.260 --> 00:07:08.500
that's the common good --

00:07:09.220 --> 00:07:11.196
and the farmers are the passengers

00:07:11.220 --> 00:07:14.820
or the car owners who are choosing
to ride in those cars.

00:07:16.780 --> 00:07:19.396
And by making the individually
rational choice

00:07:19.420 --> 00:07:22.236
of prioritizing their own safety,

00:07:22.260 --> 00:07:25.396
they may collectively be
diminishing the common good,

00:07:25.420 --> 00:07:27.620
which is minimizing total harm.

00:07:30.140 --> 00:07:32.276
It's called the tragedy of the commons,

00:07:32.300 --> 00:07:33.596
traditionally,

00:07:33.620 --> 00:07:36.716
but I think in the case
of driverless cars,

00:07:36.740 --> 00:07:39.596
the problem may be
a little bit more insidious

00:07:39.620 --> 00:07:43.116
because there is not necessarily
an individual human being

00:07:43.140 --> 00:07:44.836
making those decisions.

00:07:44.860 --> 00:07:48.156
So car manufacturers
may simply program cars

00:07:48.180 --> 00:07:50.700
that will maximize safety
for their clients,

00:07:51.900 --> 00:07:54.876
and those cars may learn
automatically on their own

00:07:54.900 --> 00:07:58.420
that doing so requires slightly
increasing risk for pedestrians.

00:07:59.340 --> 00:08:00.756
So to use the sheep metaphor,

00:08:00.780 --> 00:08:04.396
it's like we now have electric sheep
that have a mind of their own.

00:08:04.420 --> 00:08:05.876
(Laughter)

00:08:05.900 --> 00:08:08.980
And they may go and graze
even if the farmer doesn't know it.

00:08:10.460 --> 00:08:14.436
So this is what we may call
the tragedy of the algorithmic commons,

00:08:14.460 --> 00:08:16.820
and if offers new types of challenges.

00:08:22.340 --> 00:08:24.236
Typically, traditionally,

00:08:24.260 --> 00:08:27.596
we solve these types
of social dilemmas using regulation,

00:08:27.620 --> 00:08:30.356
so either governments
or communities get together,

00:08:30.380 --> 00:08:34.116
and they decide collectively
what kind of outcome they want

00:08:34.140 --> 00:08:36.796
and what sort of constraints
on individual behavior

00:08:36.820 --> 00:08:38.020
they need to implement.

00:08:39.420 --> 00:08:42.036
And then using monitoring and enforcement,

00:08:42.060 --> 00:08:44.619
they can make sure
that the public good is preserved.

00:08:45.260 --> 00:08:46.835
So why don't we just,

00:08:46.859 --> 00:08:48.355
as regulators,

00:08:48.379 --> 00:08:51.276
require that all cars minimize harm?

00:08:51.300 --> 00:08:53.540
After all, this is
what people say they want.

00:08:55.020 --> 00:08:56.436
And more importantly,

00:08:56.460 --> 00:08:59.556
I can be sure that as an individual,

00:08:59.580 --> 00:09:03.436
if I buy a car that may
sacrifice me in a very rare case,

00:09:03.460 --> 00:09:05.116
I'm not the only sucker doing that

00:09:05.140 --> 00:09:07.820
while everybody else
enjoys unconditional protection.

00:09:08.940 --> 00:09:12.276
In our survey, we did ask people
whether they would support regulation

00:09:12.300 --> 00:09:13.500
and here's what we found.

00:09:14.180 --> 00:09:17.940
First of all, people
said no to regulation;

00:09:19.100 --> 00:09:20.356
and second, they said,

00:09:20.380 --> 00:09:24.316
"Well if you regulate cars to do this
and to minimize total harm,

00:09:24.340 --> 00:09:25.820
I will not buy those cars."

00:09:27.220 --> 00:09:28.596
So ironically,

00:09:28.620 --> 00:09:32.116
by regulating cars to minimize harm,

00:09:32.140 --> 00:09:33.980
we may actually end up with more harm

00:09:34.860 --> 00:09:38.516
because people may not
opt into the safer technology

00:09:38.540 --> 00:09:40.620
even if it's much safer
than human drivers.

00:09:42.180 --> 00:09:45.596
I don't have the final
answer to this riddle,

00:09:45.620 --> 00:09:47.196
but I think as a starting point,

00:09:47.220 --> 00:09:50.516
we need society to come together

00:09:50.540 --> 00:09:53.300
to decide what trade-offs
we are comfortable with

00:09:54.180 --> 00:09:57.660
and to come up with ways
in which we can enforce those trade-offs.

00:09:58.340 --> 00:10:00.876
As a starting point,
my brilliant students,

00:10:00.900 --> 00:10:03.356
Edmond Awad and Sohan Dsouza,

00:10:03.380 --> 00:10:05.180
built the Moral Machine website,

00:10:06.020 --> 00:10:08.700
which generates random scenarios at you --

00:10:09.900 --> 00:10:12.356
basically a bunch
of random dilemmas in a sequence

00:10:12.380 --> 00:10:16.300
where you have to choose what
the car should do in a given scenario.

00:10:16.860 --> 00:10:21.460
And we vary the ages and even
the species of the different victims.

00:10:22.860 --> 00:10:26.556
So far we've collected
over five million decisions

00:10:26.580 --> 00:10:28.780
by over one million people worldwide

00:10:30.220 --> 00:10:31.420
from the website.

00:10:32.180 --> 00:10:34.596
And this is helping us
form an early picture

00:10:34.620 --> 00:10:37.236
of what trade-offs
people are comfortable with

00:10:37.260 --> 00:10:39.156
and what matters to them --

00:10:39.180 --> 00:10:40.620
even across cultures.

00:10:42.060 --> 00:10:43.556
But more importantly,

00:10:43.580 --> 00:10:46.956
doing this exercise
is helping people recognize

00:10:46.980 --> 00:10:49.796
the difficulty of making those choices

00:10:49.820 --> 00:10:53.620
and that the regulators
are tasked with impossible choices.

00:10:55.180 --> 00:10:58.756
And maybe this will help us as a society
understand the kinds of trade-offs

00:10:58.780 --> 00:11:01.836
that will be implemented
ultimately in regulation.

00:11:01.860 --> 00:11:03.596
And indeed, I was very happy to hear

00:11:03.620 --> 00:11:05.636
that the first set of regulations

00:11:05.660 --> 00:11:07.796
that came from
the Department of Transport --

00:11:07.820 --> 00:11:09.196
announced last week --

00:11:09.220 --> 00:11:15.796
included a 15-point checklist
for all carmakers to provide,

00:11:15.820 --> 00:11:19.076
and number 14 was ethical consideration --

00:11:19.100 --> 00:11:20.820
how are you going to deal with that.

00:11:23.620 --> 00:11:26.276
We also have people
reflect on their own decisions

00:11:26.300 --> 00:11:29.300
by giving them summaries
of what they chose.

00:11:30.260 --> 00:11:31.916
I'll give you one example --

00:11:31.940 --> 00:11:35.476
I'm just going to warn you
that this is not your typical example,

00:11:35.500 --> 00:11:36.876
your typical user.

00:11:36.900 --> 00:11:40.516
This is the most sacrificed and the most
saved character for this person.

00:11:40.540 --> 00:11:45.740
(Laughter)

00:11:46.500 --> 00:11:48.396
Some of you may agree with him,

00:11:48.420 --> 00:11:50.060
or her, we don't know.

00:11:52.300 --> 00:11:58.436
But this person also seems to slightly
prefer passengers over pedestrians

00:11:58.460 --> 00:12:00.556
in their choices

00:12:00.580 --> 00:12:03.396
and is very happy to punish jaywalking.

00:12:03.420 --> 00:12:06.460
(Laughter)

00:12:09.140 --> 00:12:10.356
So let's wrap up.

00:12:10.379 --> 00:12:13.795
We started with the question --
let's call it the ethical dilemma --

00:12:13.820 --> 00:12:16.876
of what the car should do
in a specific scenario:

00:12:16.900 --> 00:12:18.100
swerve or stay?

00:12:19.060 --> 00:12:21.796
But then we realized
that the problem was a different one.

00:12:21.820 --> 00:12:26.356
It was the problem of how to get
society to agree on and enforce

00:12:26.380 --> 00:12:28.316
the trade-offs they're comfortable with.

00:12:28.340 --> 00:12:29.596
It's a social dilemma.

00:12:29.620 --> 00:12:34.636
In the 1940s, Isaac Asimov
wrote his famous laws of robotics --

00:12:34.660 --> 00:12:35.980
the three laws of robotics.

00:12:37.060 --> 00:12:39.516
A robot may not harm a human being,

00:12:39.540 --> 00:12:42.076
a robot may not disobey a human being,

00:12:42.100 --> 00:12:45.356
and a robot may not allow
itself to come to harm --

00:12:45.380 --> 00:12:47.340
in this order of importance.

00:12:48.180 --> 00:12:50.316
But after 40 years or so

00:12:50.340 --> 00:12:54.076
and after so many stories
pushing these laws to the limit,

00:12:54.100 --> 00:12:57.796
Asimov introduced the zeroth law

00:12:57.820 --> 00:13:00.076
which takes precedence above all,

00:13:00.100 --> 00:13:03.380
and it's that a robot
may not harm humanity as a whole.

00:13:04.300 --> 00:13:08.676
I don't know what this means
in the context of driverless cars

00:13:08.700 --> 00:13:11.436
or any specific situation,

00:13:11.460 --> 00:13:13.676
and I don't know how we can implement it,

00:13:13.700 --> 00:13:15.236
but I think that by recognizing

00:13:15.260 --> 00:13:21.396
that the regulation of driverless cars
is not only a technological problem

00:13:21.420 --> 00:13:24.700
but also a societal cooperation problem,

00:13:25.620 --> 00:13:28.500
I hope that we can at least begin
to ask the right questions.

00:13:29.020 --> 00:13:30.236
Thank you.

00:13:30.260 --> 00:13:33.180
(Applause)

