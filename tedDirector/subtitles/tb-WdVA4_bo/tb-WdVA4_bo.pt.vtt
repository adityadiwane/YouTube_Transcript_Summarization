WEBVTT
Kind: captions
Language: pt

00:00:00.000 --> 00:00:07.000
Tradutor: Carl Lenny Homer
Revisor: Maricene Crus

00:00:12.820 --> 00:00:16.900
Hoje, vou falar
sobre tecnologia e sociedade.

00:00:18.860 --> 00:00:22.556
O Departamento de Transportes
estimou que, no ano passado,

00:00:22.580 --> 00:00:26.660
35 mil pessoas morreram em acidentes
de trânsito, somente nos EUA.

00:00:27.860 --> 00:00:32.660
No mundo todo, 1,2 milhão de pessoas
morrem por ano em acidentes de trânsito.

00:00:33.580 --> 00:00:38.636
Se houvesse uma forma de eliminar
90% desses acidentes, você a apoiaria?

00:00:39.540 --> 00:00:40.836
Claro que sim.

00:00:40.860 --> 00:00:44.515
Isso é o que a tecnologia
de carro autônomo promete realizar,

00:00:44.540 --> 00:00:48.886
eliminando a principal fonte
de acidentes, o erro humano.

00:00:49.740 --> 00:00:54.696
Agora, imagine-se em um carro
sem motorista, no ano de 2030,

00:00:55.180 --> 00:00:58.636
relaxando e assistindo a este vídeo
vintage do TEDxCambridge.

00:00:58.660 --> 00:01:00.660
(Risos)

00:01:01.340 --> 00:01:06.196
De repente, o carro tem uma falha
mecânica e não consegue parar.

00:01:07.180 --> 00:01:08.700
Se o carro continuar,

00:01:09.540 --> 00:01:13.660
vai atropelar um monte de gente
que está atravessando a rua,

00:01:14.900 --> 00:01:18.840
mas o carro pode desviar,
atingindo um transeunte,

00:01:18.840 --> 00:01:21.020
matando-o, para salvar os pedestres.

00:01:21.860 --> 00:01:24.460
O que o carro deve fazer
e quem deve decidir?

00:01:25.340 --> 00:01:28.876
E se, em vez disso, o carro
pudesse desviar para uma parede,

00:01:28.900 --> 00:01:34.426
batendo e matando o passageiro,
que é você, para salvar os pedestres?

00:01:35.060 --> 00:01:38.140
Este cenário é inspirado
no problema do bonde,

00:01:38.780 --> 00:01:44.136
que foi concebido por filósofos há algumas
décadas, para se refletir sobre ética.

00:01:45.940 --> 00:01:48.436
A forma de pensar
nesse problema é importante.

00:01:48.460 --> 00:01:51.076
Podemos, por exemplo,
simplesmente não pensar nisso.

00:01:51.100 --> 00:01:54.476
Podemos dizer que esse cenário não é real,

00:01:54.500 --> 00:01:57.200
bastante improvável ou simplesmente tolo.

00:01:57.580 --> 00:02:00.316
Mas acho que essa crítica não procede,

00:02:00.340 --> 00:02:02.840
porque enfoca o problema
de forma muito literal.

00:02:03.740 --> 00:02:06.476
Naturalmente, nenhum acidente
vai se parecer com esse;

00:02:06.500 --> 00:02:09.836
nenhum acidente tem duas ou três opções,

00:02:09.860 --> 00:02:12.160
nas quais alguém acaba morto.

00:02:13.300 --> 00:02:15.876
Em vez disso, o carro
vai fazer alguns cálculos,

00:02:15.900 --> 00:02:20.356
como a probabilidade de atingir
um certo grupo de pessoas,

00:02:20.820 --> 00:02:24.156
se houver um desvio
numa ou noutra direção,

00:02:24.180 --> 00:02:27.636
pode-se aumentar o risco
aos passageiros ou a outros motoristas,

00:02:27.660 --> 00:02:29.196
versus atropelar os pedestres.

00:02:29.220 --> 00:02:31.380
Vai ser um cálculo mais complexo,

00:02:32.300 --> 00:02:34.820
mas ainda vai envolver compromissos,

00:02:35.660 --> 00:02:38.540
e compromissos
frequentemente requerem ética.

00:02:39.660 --> 00:02:42.396
Pode-se dizer: "Bem, não vamos
nos preocupar com isso.

00:02:42.420 --> 00:02:47.060
Vamos aguardar até que a tecnologia esteja
totalmente desenvolvida e 100% segura".

00:02:48.340 --> 00:02:52.020
Suponha que se possa
eliminar 90% desses acidentes,

00:02:52.900 --> 00:02:55.740
ou mesmo 99% nos próximos 10 anos.

00:02:56.740 --> 00:02:59.916
E se para eliminar
o último 1% dos acidentes

00:02:59.940 --> 00:03:03.060
precisássemos de mais 50 anos de pesquisa?

00:03:04.220 --> 00:03:06.020
Não deveríamos adotar a tecnologia?

00:03:06.540 --> 00:03:11.316
Serão 60 milhões de pessoas
mortas em acidentes de carro,

00:03:11.340 --> 00:03:13.400
se mantivermos a taxa atual.

00:03:14.580 --> 00:03:15.796
Então a questão é:

00:03:15.820 --> 00:03:19.436
esperar por uma segurança
total também é uma escolha,

00:03:19.460 --> 00:03:21.620
e, também, envolve compromissos.

00:03:23.380 --> 00:03:27.716
As pessoas nas mídias sociais
aparecem com todo tipo de desculpas

00:03:27.740 --> 00:03:29.756
para não pensar sobre isso.

00:03:29.780 --> 00:03:34.850
Uma pessoa sugeriu que o carro deveria
simplesmente passar entre os pedestres

00:03:34.850 --> 00:03:36.180
(Risos)

00:03:36.180 --> 00:03:37.476
e o transeunte.

00:03:37.500 --> 00:03:40.860
Naturalmente, se for possível,
é o que o carro deve fazer.

00:03:41.740 --> 00:03:44.580
Estamos interessados nos casos
em que isso não é possível.

00:03:45.100 --> 00:03:50.176
E a minha favorita foi
a sugestão de um blogueiro:

00:03:50.540 --> 00:03:53.556
que houvesse um botão de ejeção
no carro, que você pressiona,

00:03:53.580 --> 00:03:54.796
(Risos)

00:03:54.820 --> 00:03:56.487
pouco antes da batida.

00:03:56.511 --> 00:03:58.191
(Risos)

00:03:59.660 --> 00:04:04.860
Se admitirmos que os carros
terão que tomar decisões nas ruas,

00:04:06.260 --> 00:04:08.900
como resolveremos esses compromissos

00:04:09.140 --> 00:04:10.716
e como decidiremos?

00:04:10.740 --> 00:04:13.876
Talvez uma pesquisa para avaliar
a posição da sociedade,

00:04:13.900 --> 00:04:15.356
porque, em última instância,

00:04:15.380 --> 00:04:19.340
as regulamentações e as leis são
um reflexo dos valores da sociedade.

00:04:19.860 --> 00:04:21.100
Então, fizemos o seguinte.

00:04:21.700 --> 00:04:23.316
Com meus colaboradores,

00:04:23.340 --> 00:04:27.286
Jean-François Bonnefon e Azim
Shariff, fizemos uma pesquisa

00:04:27.340 --> 00:04:30.195
em que apresentamos
às pessoas esses tipos de cenários.

00:04:30.219 --> 00:04:33.996
Oferecemos duas opções
inspiradas por dois filósofos:

00:04:34.020 --> 00:04:36.660
Jeremy Bentham e Immanuel Kant.

00:04:37.420 --> 00:04:40.516
Bentham diz que o carro
deveria seguir a ética prática:

00:04:40.540 --> 00:04:43.956
fazer aquilo que minimiza os danos totais,

00:04:43.980 --> 00:04:49.436
mesmo que isso mate um transeunte
e mesmo que isso mate o passageiro.

00:04:49.940 --> 00:04:54.916
Immanuel Kant diz que o carro
deve seguir princípios morais,

00:04:54.940 --> 00:04:56.640
tais como: "Não matarás".

00:04:57.300 --> 00:05:01.756
Você não deve fazer nada que prejudique
especificamente um ser humano

00:05:01.780 --> 00:05:06.226
e deve deixar o carro seguir seu curso,
mesmo que isso prejudique mais pessoas.

00:05:07.460 --> 00:05:08.880
O que você acha?

00:05:09.180 --> 00:05:10.700
Bentham ou Kant?

00:05:11.580 --> 00:05:12.836
Veja os resultados.

00:05:12.860 --> 00:05:14.990
A maioria concordou com Bentham.

00:05:15.980 --> 00:05:19.756
Parece que as pessoas querem
que os carros sejam práticos,

00:05:19.780 --> 00:05:22.866
minimizando os danos totais
e é o que todos deveríamos fazer.

00:05:22.866 --> 00:05:24.360
Problema resolvido.

00:05:25.060 --> 00:05:26.740
Mas há um pequeno senão.

00:05:27.740 --> 00:05:31.476
Quando perguntamos às pessoas
se comprariam esses carros,

00:05:31.500 --> 00:05:33.116
disseram: "Absolutamente não".

00:05:33.140 --> 00:05:34.896
(Risos)

00:05:35.460 --> 00:05:39.356
Elas gostariam de comprar
carros que os protejam totalmente,

00:05:39.380 --> 00:05:42.996
mas querem que as demais comprem
os carros que minimizem os danos.

00:05:43.020 --> 00:05:45.540
(Risos)

00:05:46.540 --> 00:05:48.396
Já vimos esse problema antes.

00:05:48.420 --> 00:05:50.100
Chama-se de dilema social.

00:05:50.980 --> 00:05:52.796
E para entender esse dilema,

00:05:52.820 --> 00:05:54.860
temos que voltar um pouco na história.

00:05:55.820 --> 00:05:58.396
Nos anos 1800,

00:05:58.420 --> 00:06:02.156
o economista inglês William
Forster Lloyd publicou um folheto

00:06:02.180 --> 00:06:04.396
que descrevia o seguinte cenário:

00:06:04.420 --> 00:06:07.426
existia um grupo de agricultores ingleses,

00:06:07.460 --> 00:06:10.140
que compartilhavam um terreno
para suas ovelhas pastarem.

00:06:11.340 --> 00:06:13.916
Se cada fazendeiro levasse
um certo número de ovelhas,

00:06:13.940 --> 00:06:17.536
digamos três ovelhas,
a terra seria revitalizada,

00:06:17.580 --> 00:06:20.426
os agricultores e as ovelhas
ficariam felizes,

00:06:20.460 --> 00:06:21.880
e tudo ficaria bem.

00:06:22.260 --> 00:06:24.780
Agora, se um agricultor
trouxesse uma ovelha a mais,

00:06:25.620 --> 00:06:30.340
ele teria uma pequena vantagem,
e ninguém mais seria prejudicado.

00:06:30.980 --> 00:06:34.620
Mas, se cada fazendeiro
tomasse essa decisão racional,

00:06:35.660 --> 00:06:38.380
a terra seria exaurida e empobrecida

00:06:39.180 --> 00:06:43.506
em detrimento de todos os agricultores
e, naturalmente, das ovelhas.

00:06:44.540 --> 00:06:48.220
Vemos este problema em muitos lugares:

00:06:48.900 --> 00:06:52.076
na dificuldade de gerenciar
a pesca excessiva,

00:06:52.100 --> 00:06:56.890
ou na redução das emissões de carbono
para mitigar as mudanças climáticas.

00:06:58.980 --> 00:07:01.900
Quando se trata da regulamentação
de carros autônomos,

00:07:02.900 --> 00:07:08.486
a terra comum agora é basicamente
a segurança pública, que é o bem comum,

00:07:09.220 --> 00:07:11.196
os agricultores são os passageiros

00:07:11.220 --> 00:07:14.820
ou os donos de carros que decidiram
usar aquele tipo de veículo.

00:07:16.780 --> 00:07:22.276
Fazendo uma escolha racional
de priorizar sua própria segurança,

00:07:22.276 --> 00:07:25.446
pode-se estar, coletivamente,
depreciando o bem comum,

00:07:25.446 --> 00:07:27.620
que é reduzir os danos totais.

00:07:30.140 --> 00:07:33.606
Isso é tradicionalmente
chamado de tragédia compartilhada,

00:07:33.620 --> 00:07:36.716
mas acredito que, no caso
dos carros autônomos,

00:07:36.740 --> 00:07:39.596
o problema pode ser
um pouco mais capcioso,

00:07:39.620 --> 00:07:43.116
porque não existe
necessariamente um ser humano

00:07:43.140 --> 00:07:44.836
tomando essas decisões.

00:07:44.860 --> 00:07:48.156
Os fabricantes de automóveis podem
simplesmente programar os carros

00:07:48.180 --> 00:07:50.700
que irão maximizar
a segurança de seus clientes,

00:07:51.900 --> 00:07:54.876
e esses carros podem
aprender por conta própria

00:07:54.900 --> 00:07:58.420
e isso resulta num risco
ligeiramente maior para os pedestres.

00:07:59.340 --> 00:08:00.756
Na metáfora das ovelhas,

00:08:00.780 --> 00:08:04.396
é como se existisse uma ovelha
elétrica tomando decisões próprias.

00:08:04.420 --> 00:08:05.876
(Risos)

00:08:05.900 --> 00:08:08.980
Elas poderiam ir pastar,
sem o agricultor saber.

00:08:10.460 --> 00:08:14.436
Isso é o que se chama de tragédia
dos algorítmicos compartilhados,

00:08:14.460 --> 00:08:16.820
e oferece novos tipos de desafios.

00:08:22.340 --> 00:08:24.236
Normalmente, tradicionalmente,

00:08:24.260 --> 00:08:27.596
resolvemos esses tipos de dilemas
sociais com a regulamentação,

00:08:27.620 --> 00:08:30.356
em que os governos
ou as próprias comunidades,

00:08:30.380 --> 00:08:34.116
decidem coletivamente o que se deseja

00:08:34.140 --> 00:08:38.216
e quais restrições sobre comportamentos
individuais são necessárias.

00:08:39.420 --> 00:08:42.036
Então, com o acompanhamento e a aplicação,

00:08:42.060 --> 00:08:44.619
pode-se garantir
a preservação do bem público.

00:08:45.260 --> 00:08:48.375
Então, por que não nós,
como regulamentadores,

00:08:48.379 --> 00:08:51.276
exigimos que todos os carros
minimizem os prejuízos?

00:08:51.300 --> 00:08:53.740
Afinal, isso é o que as pessoas querem.

00:08:55.020 --> 00:08:59.556
E, o mais importante, posso
ter certeza que, como indivíduo,

00:08:59.580 --> 00:09:03.436
se eu comprar um carro que possa
me prejudicar muito raramente,

00:09:03.460 --> 00:09:05.116
não seria o único fazendo isso,

00:09:05.140 --> 00:09:07.820
enquanto as outras pessoas
teriam proteção incondicional.

00:09:08.940 --> 00:09:12.276
Na pesquisa, perguntamos se as pessoas
apoiariam a regulamentação

00:09:12.300 --> 00:09:13.630
e este é o resultado.

00:09:14.180 --> 00:09:17.940
Em primeiro lugar,
elas disseram não à regulamentação;

00:09:19.100 --> 00:09:20.356
e em segundo, disseram:

00:09:20.380 --> 00:09:24.316
"Se houver uma regulamentação dos carros
para fazer isso e minimizar danos totais,

00:09:24.340 --> 00:09:26.200
não comprarei esses carros".

00:09:27.220 --> 00:09:28.596
Ironicamente,

00:09:28.620 --> 00:09:32.116
regulamentando-se os carros
para minimizarem os danos,

00:09:32.140 --> 00:09:34.390
pode-se, de fato, causar mais danos

00:09:34.860 --> 00:09:38.516
porque as pessoas podem
não optar pela tecnologia mais segura,

00:09:38.540 --> 00:09:40.960
mesmo se for mais seguro
que motoristas humanos.

00:09:42.180 --> 00:09:45.596
Não tenho uma resposta
definitiva para este enigma,

00:09:45.620 --> 00:09:50.556
mas acho que, como início, precisamos
de uma resposta da sociedade,

00:09:50.556 --> 00:09:53.300
e decidir quais são
os compromissos mais adequados

00:09:54.180 --> 00:09:57.660
e para encontrar formas
de impor esses compromissos.

00:09:58.340 --> 00:10:00.876
Como ponto de partida,
meus alunos brilhantes,

00:10:00.900 --> 00:10:05.506
Edmond Awad e Sohan Dsouza,
criaram o site chamado Moral Machine,

00:10:06.056 --> 00:10:08.700
que produz situações aleatórias,

00:10:09.900 --> 00:10:12.356
ou seja, vários dilemas
aleatórios em sequência,

00:10:12.380 --> 00:10:16.300
nos quais se deve escolher que decisão
o carro deve tomar numa certa situação.

00:10:16.860 --> 00:10:21.460
Variamos as idades e até mesmo
os tipos das diferentes vítimas.

00:10:22.860 --> 00:10:26.556
Até agora, obtivemos
mais de 5 milhões de respostas,

00:10:26.580 --> 00:10:28.940
de mais de um milhão
de pessoas no mundo todo,

00:10:30.220 --> 00:10:31.420
a partir do site.

00:10:32.180 --> 00:10:34.596
Isso está nos ajudando
a formar um quadro inicial

00:10:34.620 --> 00:10:37.236
de como as pessoas encaram os compromissos

00:10:37.260 --> 00:10:40.516
e com o que elas se importam,
mesmo em outras culturas.

00:10:42.060 --> 00:10:43.556
O mais importante

00:10:43.580 --> 00:10:46.956
ao fazer este exercício,
é ajudar as pessoas a reconhecer

00:10:46.980 --> 00:10:49.796
a dificuldade de se fazerem as escolhas,

00:10:49.820 --> 00:10:53.620
que os legisladores terão de enfrentar
com essas alternativas impossíveis.

00:10:55.180 --> 00:10:58.756
Talvez isso ajude a sociedade
a entender os tipos de compromissos

00:10:58.780 --> 00:11:01.836
que existirão na regulamentação.

00:11:01.860 --> 00:11:05.656
Fiquei muito feliz ao ouvir
que o primeiro conjunto de regulamentos,

00:11:05.660 --> 00:11:09.216
anunciado pelo Departamento
de Transportes, na semana passada,

00:11:09.220 --> 00:11:12.820
tem uma lista de 15 pontos

00:11:12.820 --> 00:11:15.820
que todas as montadoras devem fornecer,

00:11:15.820 --> 00:11:21.016
e o número 14 é uma consideração
ética, de como se vai lidar com isso.

00:11:23.620 --> 00:11:26.276
Temos pessoas refletindo
sobre suas próprias decisões,

00:11:26.300 --> 00:11:29.300
porque se mostrou a elas um resumo
de suas próprias escolhas.

00:11:30.260 --> 00:11:31.916
Vamos ver um exemplo,

00:11:31.940 --> 00:11:36.906
e já aviso que este não é
um exemplo de um usuário típico.

00:11:36.906 --> 00:11:40.516
Este é o personagem mais sacrificado
e mais salvo para essa pessoa.

00:11:40.540 --> 00:11:41.540
(Risos)

00:11:41.540 --> 00:11:42.968
[Resultados]

00:11:42.968 --> 00:11:44.808
[Personagem mais salvo]

00:11:44.808 --> 00:11:46.500
[Personagem mais morto]

00:11:46.500 --> 00:11:48.396
Alguns podem concordar com ele,

00:11:48.420 --> 00:11:50.200
ou com ela, não sabemos.

00:11:52.300 --> 00:11:58.436
Essa pessoa também parece preferir
ligeiramente passageiros do que pedestres

00:11:58.460 --> 00:12:00.556
em suas escolhas

00:12:00.580 --> 00:12:03.396
e fica muito feliz em punir
pedestres imprudentes.

00:12:03.420 --> 00:12:05.420
(Risos)

00:12:09.140 --> 00:12:10.356
Vamos terminar aqui.

00:12:10.379 --> 00:12:13.795
Nós começamos com uma pergunta,
vamos chamá-la de dilema ético,

00:12:13.820 --> 00:12:16.876
sobre o que os carros deveriam
fazer num cenário específico:

00:12:16.900 --> 00:12:18.100
desviar ou não?

00:12:19.060 --> 00:12:21.796
Então, percebe-se
que o problema é diferente.

00:12:21.820 --> 00:12:26.356
É o problema de como conseguir
que a sociedade concorde e respeite

00:12:26.380 --> 00:12:29.636
os compromissos em que se sentem
confortáveis. É um dilema social.

00:12:29.636 --> 00:12:34.636
Na década de 1940, Isaac Asimov
escreveu suas famosas leis da robótica,

00:12:34.660 --> 00:12:36.160
as três leis da robótica:

00:12:37.060 --> 00:12:39.516
um robô não pode ferir um ser humano,

00:12:39.540 --> 00:12:42.076
um robô não pode
desobedecer um ser humano,

00:12:42.100 --> 00:12:45.356
e um robô não pode se prejudicar,

00:12:45.380 --> 00:12:47.340
nesta ordem de importância.

00:12:48.180 --> 00:12:50.316
Depois de 40 anos

00:12:50.340 --> 00:12:54.076
e depois de tantas histórias
levando essas leis ao limite,

00:12:54.100 --> 00:12:57.796
Asimov criou a lei zero,

00:12:57.820 --> 00:13:00.076
que tem prioridade acima de tudo,

00:13:00.100 --> 00:13:03.380
aquela em que um robô
não pode fazer mal à humanidade.

00:13:04.300 --> 00:13:08.676
Não sei o que isso significa
no contexto dos carros autônomos,

00:13:08.700 --> 00:13:11.436
ou em qualquer situação específica,

00:13:11.460 --> 00:13:15.266
e não sei como podemos implementar isso,
mas acredito que, ao reconhecer

00:13:15.266 --> 00:13:21.396
que a regulamentação
não é apenas um problema tecnológico,

00:13:21.420 --> 00:13:24.700
mas também um problema
de cooperação social,

00:13:25.620 --> 00:13:28.500
espero que possamos começar
a fazer as perguntas certas.

00:13:29.020 --> 00:13:30.236
Obrigado.

00:13:30.260 --> 00:13:32.080
(Aplausos)

