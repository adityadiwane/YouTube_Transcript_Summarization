WEBVTT
Kind: captions
Language: es

00:00:00.000 --> 00:00:07.000
Traductor: Analia Padin
Revisor: Sebastian Betti

00:00:12.820 --> 00:00:16.900
Hoy voy a hablar de tecnología 
y de la sociedad.

00:00:18.860 --> 00:00:22.556
El Departamento de Transporte
estimó que, el año pasado,

00:00:22.580 --> 00:00:26.660
hubo 35 000 muertos
en accidentes de auto, tan solo en EE.UU.

00:00:27.860 --> 00:00:32.660
A nivel mundial, mueren 1,2 millones
de personas por año en accidentes de auto.

00:00:33.580 --> 00:00:37.676
Si hubiera una manera de eliminar
el 90 % de esos accidentes,

00:00:37.700 --> 00:00:38.900
¿apoyarían la causa?

00:00:39.540 --> 00:00:40.836
Por supuesto que lo harían.

00:00:40.860 --> 00:00:44.515
Esto es lo que promete
la tecnología de vehículos autónomos,

00:00:44.540 --> 00:00:47.356
al eliminar la principal causa
de accidentes:

00:00:47.380 --> 00:00:48.580
el error humano.

00:00:49.740 --> 00:00:55.156
Imagínense en el año 2030,
viajando en un vehículo autónomo,

00:00:55.180 --> 00:00:58.636
sentados, mirando este video vintage
de un evento TEDxCambridge.

00:00:58.660 --> 00:01:00.660
(Risas)

00:01:01.340 --> 00:01:02.556
De pronto,

00:01:02.580 --> 00:01:05.860
el auto tiene una falla mecánica
y no puede detenerse.

00:01:07.180 --> 00:01:08.700
Si el auto continúa,

00:01:09.540 --> 00:01:13.660
va a atropellar a un grupo
de peatones que cruza la calle.

00:01:14.900 --> 00:01:17.035
Pero el auto puede cambiar de dirección,

00:01:17.059 --> 00:01:18.916
atropellar a un solo transeúnte,

00:01:18.940 --> 00:01:21.020
matarlo, y así salvar a los peatones.

00:01:21.860 --> 00:01:24.460
¿Qué debería hacer el auto,
y quién debería decidir?

00:01:25.340 --> 00:01:28.876
¿Y si en cambio el auto
pudiera irse contra una pared,

00:01:28.900 --> 00:01:32.196
chocar y matarte a ti, el pasajero,

00:01:32.220 --> 00:01:34.540
para salvar a los peatones?

00:01:35.060 --> 00:01:38.140
Este caso hipotético está inspirado
en el dilema del tranvía,

00:01:38.780 --> 00:01:42.556
que fue inventado por unos filósofos
hace unas décadas

00:01:42.580 --> 00:01:44.150
para reflexionar sobre la ética.

00:01:45.940 --> 00:01:48.436
Es importante saber
cómo pensamos en este problema.

00:01:48.460 --> 00:01:51.076
Podríamos, por ejemplo,
no pensar en esto en absoluto.

00:01:51.100 --> 00:01:54.476
Podríamos decir que la situación 
es poco realista,

00:01:54.500 --> 00:01:56.820
remotamente probable,
o simplemente absurda.

00:01:57.580 --> 00:02:00.316
Pero para mí esta crítica
pierde de vista el problema,

00:02:00.340 --> 00:02:02.810
porque se toma la situación
muy al pie de la letra.

00:02:03.740 --> 00:02:06.476
Por supuesto que ningún accidente
es exactamente así;

00:02:06.500 --> 00:02:09.836
ningún accidente tiene dos o tres opciones

00:02:09.860 --> 00:02:11.860
donde de una forma u otra muere alguien.

00:02:13.300 --> 00:02:15.876
Lo que va a ocurrir,
es que el auto va a calcular algo,

00:02:15.900 --> 00:02:20.796
como la probabilidad de atropellar
a un determinado grupo de personas

00:02:20.820 --> 00:02:24.156
si toma una dirección u otra.

00:02:24.180 --> 00:02:27.636
Se podría aumentar levemente el riesgo
de los pasajeros y conductores,

00:02:27.660 --> 00:02:29.196
en favor de los peatones.

00:02:29.220 --> 00:02:31.380
Va a ser un cálculo más complejo,

00:02:32.300 --> 00:02:34.820
pero aun así va a implicar
hacer concesiones,

00:02:35.660 --> 00:02:38.540
y las concesiones normalmente
requieren una ética.

00:02:39.660 --> 00:02:42.396
Podríamos decir: "Bueno,
no nos preocupemos por esto.

00:02:42.420 --> 00:02:47.060
Esperemos a que la tecnología esté
totalmente preparada y sea 100 % segura".

00:02:48.340 --> 00:02:52.020
Supongamos que efectivamente podemos 
eliminar el 90 % de esos accidentes,

00:02:52.900 --> 00:02:55.740
o incluso el 99 % en los próximos 10 años.

00:02:56.740 --> 00:02:59.916
¿Qué pasaría si eliminar el 1 % 
de los accidentes restante

00:02:59.940 --> 00:03:03.060
llevara 50 años más de investigación?

00:03:04.220 --> 00:03:06.020
¿No deberíamos adoptar la tecnología?

00:03:06.540 --> 00:03:11.316
Estaríamos hablando de 60 millones
de muertos en accidentes de auto

00:03:11.340 --> 00:03:13.100
si seguimos al ritmo al que vamos.

00:03:14.580 --> 00:03:15.796
Quiere decir que,

00:03:15.820 --> 00:03:19.436
esperar a tener seguridad total
también es una elección

00:03:19.460 --> 00:03:21.620
y también implica hacer concesiones.

00:03:23.380 --> 00:03:27.716
En línea, en las redes sociales, la gente
ha estado proponiendo todo tipo de ideas

00:03:27.740 --> 00:03:29.756
para no pensar en este problema.

00:03:29.780 --> 00:03:32.996
Una persona sugirió que el auto
debería zigzaguear de alguna forma

00:03:33.020 --> 00:03:35.156
entre los peatones...

00:03:35.180 --> 00:03:36.196
(Risas)

00:03:36.220 --> 00:03:37.476
y el transeúnte.

00:03:37.500 --> 00:03:40.860
Por supuesto que, si el auto fuera capaz
de hacer eso, debería hacerlo.

00:03:41.740 --> 00:03:44.580
Pero nos interesan las situaciones
donde esto no es posible.

00:03:45.100 --> 00:03:50.526
Mi favorita fue la idea de un bloguero

00:03:50.540 --> 00:03:53.560
que propuso un botón de "eyectarse"
que se presiona justo antes...

00:03:53.580 --> 00:03:54.800
(Risas)

00:03:54.820 --> 00:03:56.867
de que el auto se autodestruya.

00:03:56.897 --> 00:03:58.191
(Risas)

00:03:59.660 --> 00:04:04.860
Entonces, si aceptamos que los autos
van a tener que hacer concesiones,

00:04:06.020 --> 00:04:07.900
¿cómo pensamos en esas concesiones,

00:04:09.140 --> 00:04:10.716
y cómo decidimos cuáles son?

00:04:10.740 --> 00:04:13.876
Tal vez deberíamos hacer una encuesta
y ver qué quiere la sociedad,

00:04:13.900 --> 00:04:15.356
porque en última instancia,

00:04:15.380 --> 00:04:19.340
las regulaciones y las leyes
son el reflejo de los valores sociales.

00:04:19.860 --> 00:04:21.100
Y fue lo que hicimos.

00:04:21.700 --> 00:04:23.316
Con mis colaboradores,

00:04:23.340 --> 00:04:25.676
Jean-François Bonnefon y Azim Shariff,

00:04:25.700 --> 00:04:27.316
hicimos una encuesta

00:04:27.340 --> 00:04:30.195
en la que le propusimos a la gente
este tipo de situaciones.

00:04:30.219 --> 00:04:33.996
Les dimos dos opciones
inspiradas en dos filósofos:

00:04:34.020 --> 00:04:36.660
Jeremy Bentham e Immanuel Kant.

00:04:37.420 --> 00:04:40.516
Bentham dice que el auto
debería seguir la ética del utilitarismo:

00:04:40.540 --> 00:04:43.956
debería tomar la acción
que minimice el daño total,

00:04:43.980 --> 00:04:46.796
aun si esto implica matar a un transeúnte,

00:04:46.820 --> 00:04:49.260
y aun si esto implica matar al pasajero.

00:04:49.940 --> 00:04:54.916
Immanuel Kant dice que el auto
debería seguir la ética del deber,

00:04:54.940 --> 00:04:56.500
tal como "No matarás".

00:04:57.300 --> 00:05:01.756
O sea que no deberías tomar ninguna acción
que implique hacerle daño a un ser humano,

00:05:01.780 --> 00:05:04.236
y deberías dejar que el auto siga su curso

00:05:04.260 --> 00:05:06.220
aun si eso resulta en más heridos.

00:05:07.460 --> 00:05:08.660
¿Uds. qué piensan?

00:05:09.180 --> 00:05:10.700
¿Bentham o Kant?

00:05:11.580 --> 00:05:12.836
El resultado fue este:

00:05:12.860 --> 00:05:14.660
la mayoría optó por Bentham.

00:05:15.980 --> 00:05:19.756
Así que parece que la gente quiere 
que los autos sean utilitarios,

00:05:19.780 --> 00:05:21.196
minimizar el daño total,

00:05:21.220 --> 00:05:22.796
y eso es lo que deberíamos hacer.

00:05:22.820 --> 00:05:24.020
Problema solucionado.

00:05:25.060 --> 00:05:26.540
Pero hay una pequeña trampa.

00:05:27.740 --> 00:05:31.476
Cuando le preguntamos a la gente
si comprarían estos autos,

00:05:31.500 --> 00:05:33.116
la respuesta fue un "No" rotundo.

00:05:33.140 --> 00:05:35.436
(Risas)

00:05:35.460 --> 00:05:39.356
Les gustaría comprar un auto
que los proteja a ellos a toda costa,

00:05:39.380 --> 00:05:42.996
pero quieren que los demás compren autos
que minimicen el daño.

00:05:43.020 --> 00:05:45.540
(Risas)

00:05:46.540 --> 00:05:48.396
Ya conocemos este tipo de problema.

00:05:48.420 --> 00:05:49.980
Es un dilema social.

00:05:50.980 --> 00:05:52.796
Y para entender el dilema social,

00:05:52.820 --> 00:05:54.860
hay que retroceder
un poco en la historia.

00:05:55.820 --> 00:05:58.396
En el 1800,

00:05:58.420 --> 00:06:02.156
el economista inglés William Forster Lloyd
publicó un folleto

00:06:02.180 --> 00:06:04.396
que describe la siguiente situación.

00:06:04.420 --> 00:06:06.076
Hay un grupo de pastores,

00:06:06.100 --> 00:06:07.436
pastores ingleses,

00:06:07.460 --> 00:06:10.140
que comparten un prado común
donde pastan sus ovejas.

00:06:11.340 --> 00:06:13.916
Si cada pastor 
trae una cierta cantidad de ovejas,

00:06:13.940 --> 00:06:15.436
digamos tres ovejas,

00:06:15.460 --> 00:06:17.556
el suelo se recupera bien,

00:06:17.580 --> 00:06:18.796
los pastores contentos,

00:06:18.820 --> 00:06:20.436
las ovejas contentas,

00:06:20.460 --> 00:06:21.660
todo va bien.

00:06:22.260 --> 00:06:24.780
Ahora, si uno de los pastores
trae una oveja extra,

00:06:25.620 --> 00:06:30.340
ese pastor va a estar un poquito mejor,
y nadie más va a salir perjudicado.

00:06:30.980 --> 00:06:34.620
Pero si cada pastor tomara esa decisión
individualmente racional,

00:06:35.660 --> 00:06:38.380
el prado se vería sobreexplotado
y el pasto se agotaría,

00:06:39.180 --> 00:06:41.356
en detrimento de todos los pastores,

00:06:41.380 --> 00:06:43.500
y por supuesto,
en detrimento de las ovejas.

00:06:44.540 --> 00:06:48.220
Es un problema que se ve mucho:

00:06:48.900 --> 00:06:52.076
en la dificultad de
controlar la sobrepesca,

00:06:52.100 --> 00:06:56.660
o al reducir las emisiones de carbono
para contrarrestar el cambio climático.

00:06:58.980 --> 00:07:01.900
Volviendo al tema de la regulación
de vehículos autónomos,

00:07:02.900 --> 00:07:07.236
el prado común vendría a ser
básicamente la seguridad pública;

00:07:07.260 --> 00:07:08.500
ese es el bien común.

00:07:09.220 --> 00:07:11.196
Y los pastores serían los pasajeros

00:07:11.220 --> 00:07:14.820
o los dueños de los autos que 
eligen viajar en esos vehículos.

00:07:16.780 --> 00:07:19.396
Y al tomar la decisión
individualmente racional

00:07:19.420 --> 00:07:22.236
de priorizar su propia seguridad,

00:07:22.260 --> 00:07:25.396
podrían estar disminuyendo
colectivamente el bien común,

00:07:25.420 --> 00:07:27.620
que es minimizar el daño total.

00:07:30.140 --> 00:07:32.290
Esto se llama "tragedia de los comunes",

00:07:32.300 --> 00:07:33.620
tradicionalmente,

00:07:33.620 --> 00:07:36.716
pero creo que en el caso
de los vehículos autónomos,

00:07:36.740 --> 00:07:39.596
el problema es tal vez
un poquito más traicionero

00:07:39.620 --> 00:07:43.116
porque no es necesariamente
un ser humano

00:07:43.140 --> 00:07:44.836
el que toma las decisiones.

00:07:44.860 --> 00:07:48.156
Entonces, los fabricantes podrían
simplemente programar los autos

00:07:48.180 --> 00:07:50.700
para maximizar la seguridad
de sus clientes.

00:07:51.900 --> 00:07:54.876
Y esos autos podrían aprender,
automáticamente y por su cuenta,

00:07:54.900 --> 00:07:58.420
que hacer eso requiere aumentar
levemente el riesgo de los peatones.

00:07:59.340 --> 00:08:00.960
O sea que, volviendo a las ovejas:

00:08:00.990 --> 00:08:04.396
es como si ahora tuviéramos
ovejas eléctricas que piensan solas.

00:08:04.420 --> 00:08:05.876
(Risas)

00:08:05.900 --> 00:08:08.980
Y pueden irse a pastar solas
aunque el pastor no lo sepa.

00:08:10.460 --> 00:08:14.436
O sea que a esto podríamos llamarlo
"la tragedia de los comunes algorítmicos",

00:08:14.460 --> 00:08:16.820
y nos presenta nuevos desafíos.

00:08:22.340 --> 00:08:24.236
Típicamente, tradicionalmente,

00:08:24.260 --> 00:08:27.596
este tipo de dilemas sociales
se resuelven implementando regulación.

00:08:27.620 --> 00:08:30.356
Ya sea el gobierno
o la comunidad, se juntan

00:08:30.380 --> 00:08:34.116
y deciden colectivamente
qué resultado quieren obtener,

00:08:34.140 --> 00:08:36.796
y qué tipo de restricciones
a la conducta individual

00:08:36.820 --> 00:08:38.020
necesitan implementar.

00:08:39.420 --> 00:08:42.036
Y luego, a través del control
y la imposición de normas,

00:08:42.060 --> 00:08:44.619
pueden garantizar
la preservación del bien común.

00:08:45.260 --> 00:08:46.835
Entonces ¿por qué no exigimos,

00:08:46.859 --> 00:08:48.354
en nuestro rol de reguladores,

00:08:48.378 --> 00:08:51.276
que todos los autos
tienen que minimizar el daño?

00:08:51.300 --> 00:08:53.979
A fin de cuentas, eso es
lo que la gente dice que quiere.

00:08:55.020 --> 00:08:56.436
Y más importante aún,

00:08:56.460 --> 00:08:59.556
puedo estar seguro de que, como individuo,

00:08:59.580 --> 00:09:03.436
si compro un auto que en un caso
muy extremo podría llegar a sacrificarme,

00:09:03.460 --> 00:09:05.176
no soy el único papanatas haciéndolo

00:09:05.200 --> 00:09:07.820
mientras todos los demás
gozan de protección ilimitada.

00:09:08.940 --> 00:09:12.276
En nuestra encuesta indagamos
acerca de la idea de la regulación,

00:09:12.300 --> 00:09:13.500
y el resultado fue este:

00:09:14.180 --> 00:09:17.940
Primero, la gente dijo 
"no" a la regulación;

00:09:19.100 --> 00:09:20.356
y segundo:

00:09:20.380 --> 00:09:24.316
"Bueno, si van a regular los autos
para actuar así y minimizar el daño total,

00:09:24.340 --> 00:09:25.820
yo no los voy a comprar".

00:09:27.220 --> 00:09:28.596
Entonces, irónicamente,

00:09:28.620 --> 00:09:32.116
al regular los autos
para minimizar el daño,

00:09:32.140 --> 00:09:33.980
podríamos acabar con más daño

00:09:34.860 --> 00:09:38.516
porque la gente no adoptaría
esta nueva tecnología

00:09:38.540 --> 00:09:41.300
aun cuando es mucho más segura
que los conductores humanos.

00:09:42.180 --> 00:09:45.596
No tengo la respuesta final
a este acertijo,

00:09:45.620 --> 00:09:47.196
pero creo que, para empezar,

00:09:47.220 --> 00:09:50.516
necesitamos que la sociedad
se ponga de acuerdo

00:09:50.540 --> 00:09:53.600
sobre las concesiones
que está dispuesta a aceptar,

00:09:54.180 --> 00:09:57.660
y las posibles maneras
de imponer esas concesiones.

00:09:58.340 --> 00:10:00.876
Como punto de partida,
mis brillantes alumnos,

00:10:00.900 --> 00:10:03.356
Edmond Awad y Sohan Dsouza,

00:10:03.380 --> 00:10:05.330
construyeron el sitio web Moral Machine,

00:10:06.020 --> 00:10:08.700
que genera y presenta
situaciones hipotéticas al azar;

00:10:09.900 --> 00:10:12.356
es básicamente una secuencia
aleatoria de dilemas

00:10:12.380 --> 00:10:16.300
donde tienes que elegir
qué debería hacer el auto en cada caso.

00:10:16.860 --> 00:10:21.460
Y variamos las edades y hasta las especies
de las distintas víctimas.

00:10:22.860 --> 00:10:26.556
Hasta ahora hemos recolectado
más de 5 millones de decisiones,

00:10:26.580 --> 00:10:28.860
de más de un millón de personas
en todo el mundo

00:10:30.220 --> 00:10:31.420
a través del sitio web.

00:10:32.180 --> 00:10:34.596
Esto nos está ayudando
a pintar un panorama inicial

00:10:34.620 --> 00:10:37.236
de las concesiones
que la gente está dispuesta a hacer

00:10:37.260 --> 00:10:39.156
y de qué es lo que les importa,

00:10:39.180 --> 00:10:40.620
incluso en distintas culturas.

00:10:42.060 --> 00:10:43.556
Pero, lo que es más importante,

00:10:43.580 --> 00:10:46.956
este ejercicio está ayudando
a la gente a comprender

00:10:46.980 --> 00:10:49.796
lo difícil que es tomar esas decisiones,

00:10:49.820 --> 00:10:53.620
y que los organismos reguladores
se enfrentan con decisiones imposibles.

00:10:55.180 --> 00:10:58.756
Y tal vez esto nos ayude, como sociedad,
a entender el tipo de concesiones

00:10:58.780 --> 00:11:01.836
que se van a implementar,
en última instancia, como normativa.

00:11:01.860 --> 00:11:03.596
Me alegré mucho cuando me enteré

00:11:03.620 --> 00:11:05.636
de que el primer conjunto de regulaciones

00:11:05.660 --> 00:11:07.796
que publicó el Departamento de Transporte

00:11:07.820 --> 00:11:09.196
la semana pasada

00:11:09.220 --> 00:11:15.796
incluye una lista de 15 ítems
que deben presentar los fabricantes,

00:11:15.820 --> 00:11:19.076
y el número 14
es "consideraciones éticas";

00:11:19.100 --> 00:11:20.820
cómo van a manejar ese tema.

00:11:23.620 --> 00:11:26.836
En el sitio, la gente también
puede reflexionar sobre sus decisiones

00:11:26.856 --> 00:11:29.350
al recibir un resumen
de las opciones que eligieron.

00:11:30.260 --> 00:11:31.916
Les voy a dar un ejemplo.

00:11:31.940 --> 00:11:35.476
Les advierto que no es un ejemplo típico;

00:11:35.500 --> 00:11:36.876
no es un usuario típico.

00:11:36.900 --> 00:11:40.516
Estos son los individuos que esta persona
más salvó y más sacrificó.

00:11:40.540 --> 00:11:45.740
(Risas)

00:11:46.500 --> 00:11:48.506
Algunos de Uds. estarán de acuerdo con él,

00:11:48.530 --> 00:11:50.060
o ella, no sabemos.

00:11:52.300 --> 00:11:58.436
Esta persona también parece inclinarse
más a favor del pasajero que del peatón,

00:11:58.460 --> 00:12:00.556
según las opciones que escogió,

00:12:00.580 --> 00:12:03.396
y no tiene problema
en castigar al peatón imprudente.

00:12:03.420 --> 00:12:06.460
(Risas)

00:12:09.140 --> 00:12:10.356
Entonces, redondeando.

00:12:10.379 --> 00:12:13.795
Empezamos con la pregunta,
llamémosle dilema ético,

00:12:13.820 --> 00:12:16.876
de qué debería hacer el auto
en una situación específica:

00:12:16.900 --> 00:12:18.490
¿cambiar de dirección o seguir?

00:12:19.060 --> 00:12:21.796
Pero luego nos dimos cuenta
de que el problema es otro.

00:12:21.820 --> 00:12:24.726
El problema es que la sociedad
se ponga de acuerdo

00:12:24.750 --> 00:12:28.316
sobre qué concesiones le son aceptables
y cómo imponerlas.

00:12:28.340 --> 00:12:29.596
Es un dilema social.

00:12:29.620 --> 00:12:34.636
En 1940, Isaac Asimov escribió
sus famosas leyes de la robótica;

00:12:34.660 --> 00:12:36.190
las tres leyes de la robótica.

00:12:37.060 --> 00:12:39.516
Un robot no hará daño al ser humano,

00:12:39.540 --> 00:12:42.076
un robot debe obedecer al ser humano,

00:12:42.100 --> 00:12:45.356
y un robot debe preservarse a sí mismo.

00:12:45.380 --> 00:12:47.340
En ese orden de importancia.

00:12:48.180 --> 00:12:50.316
Pero después de 40 años más o menos,

00:12:50.340 --> 00:12:54.076
y después de tantas historias
que llevaron estas leyes al límite,

00:12:54.100 --> 00:12:57.796
Asimov introdujo la ley cero,

00:12:57.820 --> 00:13:00.076
que precede a las demás,

00:13:00.100 --> 00:13:03.380
y es que un robot
no hará daño a la Humanidad.

00:13:04.300 --> 00:13:08.676
No sé qué quiere decir esto
en el contexto de los vehículos autónomos,

00:13:08.700 --> 00:13:11.436
o en cualquier situación específica,

00:13:11.460 --> 00:13:13.676
y no sé cómo lo podemos implementar,

00:13:13.700 --> 00:13:15.236
pero creo que reconociendo

00:13:15.260 --> 00:13:21.396
que la regulación de vehículos autónomos
no es solo un problema tecnológico

00:13:21.420 --> 00:13:24.700
sino también un problema
de cooperación social,

00:13:25.620 --> 00:13:28.840
espero que podamos al menos empezar
a hacer las preguntas adecuadas.

00:13:29.020 --> 00:13:30.236
Gracias.

00:13:30.260 --> 00:13:33.180
(Aplausos)

