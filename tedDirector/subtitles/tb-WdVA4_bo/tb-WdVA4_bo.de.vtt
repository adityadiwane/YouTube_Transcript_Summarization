WEBVTT
Kind: captions
Language: de

00:00:00.000 --> 00:00:07.000
Übersetzung: Lisa Dietrich
Lektorat: seohyeon Yoon

00:00:12.820 --> 00:00:16.900
Ich werde heute über 
Technologie und Gesellschaft sprechen.

00:00:18.860 --> 00:00:22.556
Das Verkehrsministerium schätzt,
dass im letzten Jahr

00:00:22.580 --> 00:00:26.660
35.000 Menschen bei Verkehrsunfällen
allein in den USA gestorben sind.

00:00:27.860 --> 00:00:32.660
Weltweit sterben jährlich 1,2 Millionen
Menschen bei Verkehrsunfällen.

00:00:33.580 --> 00:00:37.676
Wenn es eine Möglichkeit gäbe,
90 % dieser Unfälle zu vermeiden,

00:00:37.700 --> 00:00:38.900
wären Sie dafür?

00:00:39.540 --> 00:00:40.836
Natürlich wären Sie das.

00:00:40.860 --> 00:00:44.515
Genau das ist es, was die Technologie
fahrerloser Autos erreichen möchte,

00:00:44.540 --> 00:00:47.356
indem sie die Hauptursache
für Autounfälle eliminiert –

00:00:47.380 --> 00:00:48.580
menschliches Versagen.

00:00:49.740 --> 00:00:55.156
Stellen Sie sich vor, Sie sitzen
im Jahr 2030 in einem fahrerlosen Auto.

00:00:55.180 --> 00:00:58.636
Sie sind entspannt und schauen
dieses alte TEDxCambridge-Video.

00:00:58.660 --> 00:01:00.660
(Lachen)

00:01:01.470 --> 00:01:05.860
Plötzlich tritt ein mechanischer Fehler
auf und das Auto kann nicht anhalten.

00:01:07.180 --> 00:01:08.700
Wenn das Auto weiterfährt,

00:01:09.540 --> 00:01:13.660
wird es in in ein paar Fußgänger fahren,
die gerade die Straße überqueren.

00:01:14.900 --> 00:01:17.035
Das Auto könnte jedoch auch ausweichen,

00:01:17.059 --> 00:01:19.046
einen Beobachter treffen und töten,

00:01:19.046 --> 00:01:21.020
um die Fußgänger zu schützen.

00:01:21.860 --> 00:01:24.460
Was sollte das Auto machen
und wer sollte entscheiden?

00:01:25.340 --> 00:01:28.876
Was wäre, wenn das Auto stattdessen
ausweichen und eine Wand treffen könnte

00:01:28.900 --> 00:01:32.196
und dabei Sie, den Insassen, töten würde,

00:01:32.220 --> 00:01:34.540
um die Fußgänger zu retten?

00:01:35.060 --> 00:01:38.140
Dieses Szenario ist vom
Trolley-Problem inspiriert,

00:01:38.780 --> 00:01:42.556
das vor einigen Jahrzehnten
von Philosophen erfunden wurde,

00:01:42.580 --> 00:01:43.820
um Ethik zu erforschen.

00:01:45.940 --> 00:01:48.436
Die Art, wie wir über 
das Problem denken, zählt.

00:01:48.460 --> 00:01:51.076
So könnten wir etwa
gar nicht darüber nachdenken.

00:01:51.100 --> 00:01:54.476
Wir könnten sagen, 
dass das Szenario unrealistisch,

00:01:54.500 --> 00:01:56.820
extrem unwahrscheinlich oder albern ist.

00:01:57.580 --> 00:02:00.316
Aber ich denke, dass diese Kritik
am Thema vorbeigeht,

00:02:00.340 --> 00:02:02.500
da sie das Szenario zu wörtlich nimmt.

00:02:03.740 --> 00:02:06.476
Natürlich wird kein Unfall
genauso aussehen.

00:02:06.500 --> 00:02:09.836
Bei keinem Unfall gibt es
zwei oder drei Optionen,

00:02:09.860 --> 00:02:11.860
bei denen immer jemand stirbt.

00:02:13.300 --> 00:02:15.876
Stattdessen wird das Auto
eine Zahl ausrechnen,

00:02:15.900 --> 00:02:20.796
wie die Wahrscheinlichkeit,
eine bestimmte Menschengruppe zu treffen.

00:02:20.820 --> 00:02:24.156
Wenn es in die eine oder
die andere Richtung ausweicht.

00:02:24.180 --> 00:02:27.636
könnte das Risiko für Passagiere
geringfügig höher sein,

00:02:27.660 --> 00:02:29.196
als das Risiko für Fußgänger.

00:02:29.220 --> 00:02:31.380
Die Berechnung wäre komplexer,

00:02:32.300 --> 00:02:34.820
würde aber immer noch
Abwägungen beinhalten

00:02:35.660 --> 00:02:38.540
und Abwägungen beinhalten oft Ethik.

00:02:39.660 --> 00:02:42.396
Jetzt könnten wir sagen:
„Lasst uns keine Sorgen machen.

00:02:42.420 --> 00:02:47.060
Lasst uns warten, bis die Technologie
bereit und zu 100 % sicher ist.“

00:02:48.340 --> 00:02:52.020
Nehmen wir an, wir können tatsächlich
90 % dieser Unfälle vermeiden

00:02:52.900 --> 00:02:55.740
oder sogar 99 % in den nächsten 10 Jahren.

00:02:56.740 --> 00:02:59.916
Was wäre, wenn die Eliminierung 
der letzen 1 % der Unfälle

00:02:59.940 --> 00:03:03.060
50 weitere Jahre der Forschung
bedeuten würde?

00:03:04.220 --> 00:03:06.020
Sollten wir die Technologie ablehnen?

00:03:06.540 --> 00:03:11.316
Dann würden 60 Millionen Menschen
bei Autounfällen sterben,

00:03:11.340 --> 00:03:13.100
wenn wir die Rate so beibehalten.

00:03:14.580 --> 00:03:15.796
Der Punkt ist also,

00:03:15.820 --> 00:03:19.436
dass das Warten auf vollkommene Sicherheit
eine Möglichkeit wäre,

00:03:19.460 --> 00:03:21.620
diese aber auch
Abwägungen beinhalten würde.

00:03:23.380 --> 00:03:27.716
In den sozialen Medien haben die Nutzer
unzählige Möglichkeiten entwickelt,

00:03:27.740 --> 00:03:29.756
um diesem Problem aus dem Weg zu gehen.

00:03:29.780 --> 00:03:32.996
Eine Person schlug vor, das sich das Auto

00:03:33.020 --> 00:03:35.156
zwischen den Fußgängern
durchschlängeln soll,

00:03:35.180 --> 00:03:36.196
(Lachen)

00:03:36.220 --> 00:03:37.476
und dem Beobachter.

00:03:37.500 --> 00:03:40.860
Natürlich könnte und sollte
das Auto dies machen,

00:03:41.740 --> 00:03:44.580
aber wir beleuchten Szenarien,
in denen das unmöglich ist.

00:03:45.100 --> 00:03:49.440
Mein Lieblingsvorschlag
kam von einem Blogger,

00:03:49.440 --> 00:03:53.556
der vorschlug eine Schleudersitz-Taste
im Auto zu haben, die gedrückt wird,

00:03:53.580 --> 00:03:54.796
(Lachen)

00:03:54.820 --> 00:03:56.487
bevor sich das Auto zerstört.

00:03:56.511 --> 00:03:58.191
(Lachen)

00:03:59.660 --> 00:04:04.860
Wenn wir also anerkennen, dass Autos
auf der Straße abwägen müssen,

00:04:06.020 --> 00:04:07.900
wie denken wir über diese Abwägungen

00:04:09.140 --> 00:04:10.716
und wie entscheiden wir uns?

00:04:10.740 --> 00:04:13.876
Vielleicht sollten wir eine Umfrage
in der Gesellschaft machen.

00:04:13.900 --> 00:04:15.356
Denn letztendlich

00:04:15.380 --> 00:04:19.340
sind Vorschriften und das Gesetz
ein Spiegel der gesellschaftlichen Werte.

00:04:19.860 --> 00:04:21.100
Genau das haben wir getan.

00:04:21.700 --> 00:04:23.316
Mit meinen Mitarbeitern,

00:04:23.340 --> 00:04:25.676
Jean-François Bonnefon und Azim Shariff,

00:04:25.700 --> 00:04:27.316
haben wir eine Umfrage gemacht,

00:04:27.340 --> 00:04:30.195
in der wir Menschen
diese Art von Szenarien zeigten.

00:04:30.219 --> 00:04:33.996
Wir haben ihnen zwei Optionen gegeben,
von zwei Philosophen inspiriert:

00:04:34.020 --> 00:04:36.660
Jeremy Bentham und Immanuel Kant.

00:04:37.420 --> 00:04:40.516
Bentham sagt, dass das Auto
utilitaristischer Ethik folgen sollte:

00:04:40.540 --> 00:04:43.956
Es soll die Handlung ausführen,
die den Gesamtschaden minimiert,

00:04:43.980 --> 00:04:46.796
sogar, wenn dies einen Beobachter tötet

00:04:46.820 --> 00:04:49.260
und sogar, wenn diese Handlung
den Passagier tötet.

00:04:49.940 --> 00:04:54.916
Immanuel Kant sagt, dass das Auto
pflichtgemäßen Prinzipien folgen sollte,

00:04:54.940 --> 00:04:56.500
wie „Du sollst nicht töten.“

00:04:57.300 --> 00:05:01.756
Man sollte also nicht so handeln,
dass es einen Menschen explizit verletzt,

00:05:01.780 --> 00:05:04.236
sondern das Auto seinen Weg nehmen lassen,

00:05:04.260 --> 00:05:06.220
sogar, wenn dies mehr Menschen verletzt.

00:05:07.460 --> 00:05:08.660
Was denken Sie?

00:05:09.180 --> 00:05:10.700
Bentham oder Kant?

00:05:11.580 --> 00:05:12.836
Hier ist unser Ergebnis.

00:05:12.860 --> 00:05:14.660
Die Meisten sind auf Benthams Seite.

00:05:15.980 --> 00:05:19.520
Die Menschen scheinen also zu wollen,
dass Autos utilitaristisch sind

00:05:19.520 --> 00:05:21.196
und den Gesamtschaden minimieren

00:05:21.220 --> 00:05:22.796
und das sollten wir alle tun.

00:05:22.820 --> 00:05:24.020
Problem gelöst.

00:05:25.060 --> 00:05:26.540
Da gibt es aber einen Haken.

00:05:27.740 --> 00:05:31.476
Als wir die Leute fragten,
ob sie solche Autos kaufen würden

00:05:31.500 --> 00:05:33.116
antworteten sie: „Niemals.“

00:05:33.140 --> 00:05:35.436
(Lachen)

00:05:35.460 --> 00:05:39.356
Sie möchten Autos kaufen,
die sie um jeden Preis schützen,

00:05:39.380 --> 00:05:42.996
aber alle anderen sollen Autos kaufen,
die Schaden minimieren.

00:05:43.020 --> 00:05:45.540
(Lachen)

00:05:46.540 --> 00:05:48.396
Das Problem kannten wir.

00:05:48.420 --> 00:05:49.980
Man nennt es soziales Dilemma.

00:05:50.980 --> 00:05:52.796
Um das soziale Dilemma zu verstehen,

00:05:52.820 --> 00:05:54.860
müssen wir in die Vergangenheit schauen.

00:05:55.820 --> 00:05:58.396
Im 19. Jahrhundert veröffentlichte

00:05:58.420 --> 00:06:02.156
der englische Ökonom
William Forster Lloyd ein Merkblatt,

00:06:02.180 --> 00:06:04.396
das das folgende Szenario beschreibt.

00:06:04.420 --> 00:06:06.076
Eine Gruppe von Bauern –

00:06:06.100 --> 00:06:07.436
englische Bauern –

00:06:07.460 --> 00:06:10.140
teilen sich ein Stück Land,
auf dem ihre Schafe grasen.

00:06:11.340 --> 00:06:13.916
Wenn jeder Bauer
eine bestimmte Anzahl Schafe hat,

00:06:13.940 --> 00:06:15.436
sagen wir 3 Schafe,

00:06:15.460 --> 00:06:17.556
wird das Land verjüngt.

00:06:17.580 --> 00:06:18.796
Die Bauern sind glücklich

00:06:18.820 --> 00:06:20.436
und die Schafe sind glücklich.

00:06:20.460 --> 00:06:21.660
Alles ist gut.

00:06:22.260 --> 00:06:24.780
Wenn jetzt aber ein Bauer
ein Schaf mehr hat,

00:06:25.620 --> 00:06:30.340
geht es ihm etwas besser
und niemand kommt zu Schaden.

00:06:30.980 --> 00:06:34.620
Trifft aber jeder Bauer diese
individuelle rationale Entscheidung,

00:06:35.660 --> 00:06:38.380
ist das Stück Land überlaufen
und wird erschöpft,

00:06:39.180 --> 00:06:41.356
zum Nachteil aller Bauern

00:06:41.380 --> 00:06:43.500
und natürlich zum Nachteil der Schafe.

00:06:44.540 --> 00:06:48.220
Wir sehen dieses Problem überall:

00:06:48.900 --> 00:06:52.076
beim Versuch Überfischung zu vermeiden

00:06:52.100 --> 00:06:56.660
oder beim Versuch Emission zu reduzieren,
um den Klimawandel einzudämmen.

00:06:58.980 --> 00:07:01.900
Wenn es um Vorschriften
für fahrerlose Autos geht,

00:07:02.900 --> 00:07:07.236
steht das gemeinsame Land
für die öffentliche Sicherheit –

00:07:07.260 --> 00:07:08.500
das Gemeinwohl –

00:07:09.220 --> 00:07:11.196
und die Bauern sind die Passagiere

00:07:11.220 --> 00:07:14.820
oder die Autobesitzer,
die in diesen Autos fahren.

00:07:16.780 --> 00:07:19.396
Beim Treffen der individuellen
rationalen Entscheidung,

00:07:19.420 --> 00:07:22.236
die eigene Sicherheit zu bevorzugen,

00:07:22.260 --> 00:07:25.396
könnten sie das Gemeinwohl
zusammen vermindern,

00:07:25.420 --> 00:07:27.620
was den Gesamtschaden verringert.

00:07:30.140 --> 00:07:32.276
Man nennt dies die Tragik der Allmende,

00:07:32.300 --> 00:07:33.596
herkömmlicherweise,

00:07:33.620 --> 00:07:36.716
aber im Fall fahrerloser Autos,

00:07:36.740 --> 00:07:39.596
könnte das Problem
etwas komplizierter sein,

00:07:39.620 --> 00:07:43.116
weil nicht unbedingt ein Mensch

00:07:43.140 --> 00:07:44.836
diese Entscheidungen trifft.

00:07:44.860 --> 00:07:48.156
Autohersteller könnten also einfach
Autos so programmieren,

00:07:48.180 --> 00:07:50.700
dass sie maximale Sicherheit
für ihre Kunden gewähren

00:07:51.900 --> 00:07:54.876
und diese Autos könnten
automatisch, alleine lernen,

00:07:54.900 --> 00:07:58.420
dass diese Sicherheit mit einem
erhöhten Risiko für Fußgänger einhergeht.

00:07:59.340 --> 00:08:00.756
In der Schaf-Metapher

00:08:00.780 --> 00:08:04.396
würde das elektronische Schafe
mit eigener Meinung bedeuten.

00:08:04.420 --> 00:08:05.876
(Lachen)

00:08:05.900 --> 00:08:08.980
Und diese könnten grasen,
ohne, dass der Bauer davon weiß.

00:08:10.460 --> 00:08:14.436
Der Name dafür könnte also
Tragik der algorithmischen Allmende sein

00:08:14.460 --> 00:08:16.820
und diese stellt uns vor
ganz neue Probleme.

00:08:22.340 --> 00:08:24.236
Normalerweise

00:08:24.260 --> 00:08:27.596
haben wir diese Art von sozialen Dilemmas
mit Vorschriften gelöst.

00:08:27.620 --> 00:08:30.356
Entweder Regierungen
oder Gemeinden kommen zusammen

00:08:30.380 --> 00:08:34.116
und entscheiden gemeinsam,
welches Ergebnis sie möchten

00:08:34.140 --> 00:08:36.796
und welche Einschränkungen 
des individuellen Verhaltens

00:08:36.820 --> 00:08:38.020
sie einführen müssen.

00:08:39.420 --> 00:08:42.446
Dann wird mittels Überwachung
und Vollstreckung sichergestellt,

00:08:42.446 --> 00:08:44.619
dass das Gemeinwohl gewahrt bleibt.

00:08:45.260 --> 00:08:46.835
Warum sagen wir dann nicht,

00:08:46.859 --> 00:08:48.355
als Regulatoren,

00:08:48.379 --> 00:08:51.276
dass alle Autos Schaden minimieren müssen?

00:08:51.300 --> 00:08:53.540
Im Grunde, ist es das,
was Menschen wollen.

00:08:55.020 --> 00:08:56.436
Und was noch wichtiger ist,

00:08:56.460 --> 00:08:59.556
ich, als Individuum, kann sicher sein,

00:08:59.580 --> 00:09:03.436
dass, wenn ich ein Auto kaufe,
dass mich im seltenen Fall opfern würde,

00:09:03.460 --> 00:09:05.116
ich nicht der einzige Dumme bin,

00:09:05.140 --> 00:09:07.820
während alle anderen
bedingungslose Sicherheit genießen.

00:09:08.940 --> 00:09:12.276
In unser Umfrage haben wir gefragt,
ob Vorschriften befürwortet werden

00:09:12.300 --> 00:09:13.500
und hier ist die Antwort.

00:09:14.180 --> 00:09:17.940
Zuerst haben sich die Teilnehmer
gegen Vorschriften ausgesprochen.

00:09:19.100 --> 00:09:20.356
Dann haben sie gesagt:

00:09:20.380 --> 00:09:24.316
„Wenn Autos so reguliert werden
und den Schaden minimieren sollen,

00:09:24.340 --> 00:09:25.820
kaufe ich das Auto nicht. “

00:09:27.220 --> 00:09:28.596
Ironischerweise

00:09:28.620 --> 00:09:32.116
indem wir Autos regulieren,
so dass sie Schaden minimieren

00:09:32.140 --> 00:09:33.980
könnten wir noch mehr Schaden nehmen,

00:09:34.860 --> 00:09:38.516
weil die Menschen sich nicht
für sichere Technologie entscheiden,

00:09:38.540 --> 00:09:40.620
obwohl sie viel sicherer
als der Mensch ist.

00:09:42.180 --> 00:09:45.596
Ich habe keine ultimative Antwort
für dieses Rätsel,

00:09:45.620 --> 00:09:47.020
aber ich denke,

00:09:47.020 --> 00:09:50.516
zuallererst sollte die Gesellschaft
zusammen kommen

00:09:50.540 --> 00:09:53.300
und die Kompromisse beschließen,
die wir verkraften können

00:09:54.180 --> 00:09:57.660
und Ideen entwickeln,
wie wir diese durchsetzen können.

00:09:58.340 --> 00:10:00.876
Als Grundlage haben
meine brillanten Studenten,

00:10:00.900 --> 00:10:03.356
Edmong Awad und Sohan Dsouza,

00:10:03.380 --> 00:10:05.180
die Moral Machine Website gebaut,

00:10:06.020 --> 00:10:08.700
die eine Reihe von Szenarien vorgibt,

00:10:09.900 --> 00:10:12.356
eine Reihe von zufälligen
Dilemmas in Folge,

00:10:12.380 --> 00:10:16.300
bei denen Sie bestimmen müssen,
wie das Auto reagiert.

00:10:16.860 --> 00:10:21.460
Wir verändern das Alter und sogar
die Spezies der verschiedenen Opfer.

00:10:22.860 --> 00:10:26.556
Bisher haben wir 
über fünf Millionen Entscheidungen

00:10:26.580 --> 00:10:28.780
von über einer Million Menschen weltweit

00:10:30.220 --> 00:10:31.420
mit der Webseite erhoben.

00:10:32.180 --> 00:10:34.686
Und das hilft uns dabei,
von Anfang an zu verstehen,

00:10:34.686 --> 00:10:37.236
mit welchen Kompromissen
die Menschen leben können

00:10:37.260 --> 00:10:39.156
und was ihnen wichtig ist –

00:10:39.180 --> 00:10:40.620
sogar kulturübergreifend.

00:10:42.060 --> 00:10:43.556
Das Wichtigste ist jedoch,

00:10:43.580 --> 00:10:46.956
dass diese Übung dabei hilft,
zu verstehen,

00:10:46.980 --> 00:10:49.796
wie schwierig es ist,
diese Entscheidungen zu treffen,

00:10:49.820 --> 00:10:53.620
und dass Regulatoren vor
unmöglichen Entscheidungen stehen.

00:10:55.180 --> 00:10:58.430
Und vielleicht hilft es
der Gesellschaft zu verstehen,

00:10:58.430 --> 00:11:01.836
welche Abwägungen letztendlich
in Vorschriften enthalten sind.

00:11:01.860 --> 00:11:03.596
Und ich war glücklich zu hören,

00:11:03.620 --> 00:11:05.636
dass die erste Reihe von Vorschriften

00:11:05.660 --> 00:11:07.796
vom Verkehrsministerium,

00:11:07.820 --> 00:11:09.306
wie letzte Woche angekündigt,

00:11:09.306 --> 00:11:15.796
eine 15-Punkte-Checkliste
für alle Autohersteller enthielten

00:11:15.820 --> 00:11:19.076
und Nummer 14 waren moralische Bedenken

00:11:19.100 --> 00:11:20.820
und wie man mit diesen umgeht.

00:11:23.620 --> 00:11:26.276
Wir haben Menschen
ihre Angaben reflektieren lassen,

00:11:26.300 --> 00:11:29.300
indem wir ihnen eine Zusammenfassung
ihrer Entscheidung gaben.

00:11:30.260 --> 00:11:31.916
Hier ist ein Beispiel.

00:11:31.940 --> 00:11:35.476
Ich muss Sie nur vorher warnen,
dies ist kein typisches Beispiel,

00:11:35.500 --> 00:11:36.876
kein typischer Nutzer.

00:11:36.900 --> 00:11:40.516
Dies ist der am meisten geopferte und
der am meisten geschützte Charakter.

00:11:40.540 --> 00:11:45.740
(Lachen)

00:11:46.500 --> 00:11:48.396
Manche mögen seine Meinung teilen

00:11:48.420 --> 00:11:50.060
oder ihre Meinung, wer weiß.

00:11:52.300 --> 00:11:58.436
Diese Person neigt jedoch mehr dazu,
Passagiere über Fußgänger zu stellen

00:11:58.460 --> 00:12:00.556
und bei den Entscheidungen

00:12:00.580 --> 00:12:03.396
verkehrswidriges Verhalten zu bestrafen.

00:12:03.420 --> 00:12:06.460
(Lachen)

00:12:09.140 --> 00:12:10.356
Fassen wir also zusammen.

00:12:10.379 --> 00:12:13.795
Wir starteten mit der Frage –
wir nennen sie moralisches Dilemma –

00:12:13.820 --> 00:12:16.876
wie sich das Auto in einem
spezifischen Szenario verhalten soll:

00:12:16.900 --> 00:12:18.100
ausweichen oder bleiben?

00:12:19.060 --> 00:12:21.796
Aber dann merkten wir,
dass das Problem ein anderes war.

00:12:21.820 --> 00:12:26.356
Die Frage war, wie man die Gesellschaft
dazu bringt, Kompromisse durchzusetzen,

00:12:26.380 --> 00:12:28.316
mit denen sie sich wohl fühlen.

00:12:28.340 --> 00:12:29.596
Ein soziales Dilemma.

00:12:29.620 --> 00:12:34.636
In den 1940ern schrieb Isaac Asimov
die berühmten Robotergesetze –

00:12:34.660 --> 00:12:35.980
die drei Robotergesetze.

00:12:37.060 --> 00:12:39.516
Ein Roboter darf kein
menschliches Wesen verletzen.

00:12:39.540 --> 00:12:42.076
Ein Roboter muss einem Menschen gehorchen.

00:12:42.100 --> 00:12:45.356
Ein Roboter muss 
seine Existenz beschützen.

00:12:45.380 --> 00:12:47.340
In dieser Reihenfolge.

00:12:48.180 --> 00:12:50.316
Aber nach ungefähr 40 Jahren

00:12:50.340 --> 00:12:54.076
und nach so viele Geschichten,
die diese Gesetze an ihre Grenzen bringen,

00:12:54.100 --> 00:12:57.796
entwickelte Asimov das nullte Gesetz,

00:12:57.820 --> 00:13:00.076
das über allen anderen steht.

00:13:00.100 --> 00:13:03.380
Es besagt, dass ein Roboter
die Menschheit nicht verletzen darf.

00:13:04.300 --> 00:13:08.676
Ich weiß nicht, was das in Verbindung
mit fahrerlosen Autos bedeutet

00:13:08.700 --> 00:13:11.436
oder in Verbindung mit anderen Situationen

00:13:11.460 --> 00:13:13.676
und ich weiß nicht,
wie wir es umsetzen können.

00:13:13.700 --> 00:13:15.236
Wenn wir jedoch anerkennen,

00:13:15.260 --> 00:13:21.396
dass die Vorschriften für fahrerlose Autos
nicht nur ein technologisches Problem,

00:13:21.420 --> 00:13:24.700
sondern auch ein gesellschaftliches
Kooperationsproblem sind,

00:13:25.620 --> 00:13:28.500
dann können wir beginnen,
die richtigen Fragen zu stellen.

00:13:29.020 --> 00:13:30.236
Danke.

00:13:30.260 --> 00:13:33.180
(Applaus)

