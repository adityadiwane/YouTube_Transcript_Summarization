WEBVTT
Kind: captions
Language: en

00:00:13.373 --> 00:00:16.722
Most of us think of motion
as a very visual thing.

00:00:17.889 --> 00:00:22.977
If I walk across this stage
or gesture with my hands while I speak,

00:00:22.977 --> 00:00:25.238
that motion is something that you can see.

00:00:26.255 --> 00:00:31.737
But there's a world of important motion
that's too subtle for the human eye,

00:00:31.737 --> 00:00:33.778
and over the past few years,

00:00:33.778 --> 00:00:35.775
we've started to find that cameras

00:00:35.775 --> 00:00:39.185
can often see this motion
even when humans can't.

00:00:40.305 --> 00:00:41.856
So let me show you what I mean.

00:00:42.717 --> 00:00:46.339
On the left here, you see video
of a person's wrist,

00:00:46.339 --> 00:00:49.486
and on the right, you see video
of a sleeping infant,

00:00:49.486 --> 00:00:52.632
but if I didn't tell you
that these were videos,

00:00:52.632 --> 00:00:56.393
you might assume that you were looking
at two regular images,

00:00:56.393 --> 00:00:58.065
because in both cases,

00:00:58.065 --> 00:01:01.112
these videos appear to be
almost completely still.

00:01:02.175 --> 00:01:06.060
But there's actually a lot
of subtle motion going on here,

00:01:06.060 --> 00:01:08.452
and if you were to touch
the wrist on the left,

00:01:08.452 --> 00:01:10.448
you would feel a pulse,

00:01:10.448 --> 00:01:12.933
and if you were to hold
the infant on the right,

00:01:12.933 --> 00:01:15.324
you would feel the rise
and fall of her chest

00:01:15.324 --> 00:01:16.714
as she took each breath.

00:01:17.762 --> 00:01:21.338
And these motions carry
a lot of significance,

00:01:21.338 --> 00:01:24.681
but they're usually
too subtle for us to see,

00:01:24.681 --> 00:01:26.957
so instead, we have to observe them

00:01:26.957 --> 00:01:29.857
through direct contact, through touch.

00:01:30.997 --> 00:01:32.262
But a few years ago,

00:01:32.262 --> 00:01:36.667
my colleagues at MIT developed
what they call a motion microscope,

00:01:36.667 --> 00:01:41.051
which is software that finds
these subtle motions in video

00:01:41.051 --> 00:01:44.613
and amplifies them so that they
become large enough for us to see.

00:01:45.416 --> 00:01:48.899
And so, if we use their software
on the left video,

00:01:48.899 --> 00:01:52.149
it lets us see the pulse in this wrist,

00:01:52.149 --> 00:01:53.844
and if we were to count that pulse,

00:01:53.844 --> 00:01:56.199
we could even figure out
this person's heart rate.

00:01:57.095 --> 00:02:00.160
And if we used the same software
on the right video,

00:02:00.160 --> 00:02:03.387
it lets us see each breath
that this infant takes,

00:02:03.387 --> 00:02:07.524
and we can use this as a contact-free way
to monitor her breathing.

00:02:08.884 --> 00:02:14.232
And so this technology is really powerful
because it takes these phenomena

00:02:14.232 --> 00:02:16.599
that we normally have
to experience through touch

00:02:16.599 --> 00:02:19.556
and it lets us capture them visually
and non-invasively.

00:02:21.104 --> 00:02:25.515
So a couple years ago, I started working
with the folks that created that software,

00:02:25.515 --> 00:02:28.882
and we decided to pursue a crazy idea.

00:02:28.882 --> 00:02:31.575
We thought, it's cool
that we can use software

00:02:31.575 --> 00:02:34.710
to visualize tiny motions like this,

00:02:34.710 --> 00:02:39.168
and you can almost think of it
as a way to extend our sense of touch.

00:02:39.168 --> 00:02:43.227
But what if we could do the same thing
with our ability to hear?

00:02:44.508 --> 00:02:49.173
What if we could use video
to capture the vibrations of sound,

00:02:49.173 --> 00:02:52.000
which are just another kind of motion,

00:02:52.000 --> 00:02:55.346
and turn everything that we see
into a microphone?

00:02:56.236 --> 00:02:58.207
Now, this is a bit of a strange idea,

00:02:58.207 --> 00:03:00.793
so let me try to put it
in perspective for you.

00:03:01.523 --> 00:03:05.011
Traditional microphones
work by converting the motion

00:03:05.011 --> 00:03:08.610
of an internal diaphragm
into an electrical signal,

00:03:08.610 --> 00:03:12.928
and that diaphragm is designed
to move readily with sound

00:03:12.928 --> 00:03:17.735
so that its motion can be recorded
and interpreted as audio.

00:03:17.735 --> 00:03:21.403
But sound causes all objects to vibrate.

00:03:21.403 --> 00:03:26.883
Those vibrations are just usually
too subtle and too fast for us to see.

00:03:26.883 --> 00:03:30.621
So what if we record them
with a high-speed camera

00:03:30.621 --> 00:03:34.197
and then use software
to extract tiny motions

00:03:34.197 --> 00:03:36.287
from our high-speed video,

00:03:36.287 --> 00:03:40.561
and analyze those motions to figure out
what sounds created them?

00:03:41.859 --> 00:03:47.308
This would let us turn visible objects
into visual microphones from a distance.

00:03:49.080 --> 00:03:51.263
And so we tried this out,

00:03:51.263 --> 00:03:53.190
and here's one of our experiments,

00:03:53.190 --> 00:03:56.139
where we took this potted plant
that you see on the right

00:03:56.139 --> 00:03:58.577
and we filmed it with a high-speed camera

00:03:58.577 --> 00:04:02.106
while a nearby loudspeaker
played this sound.

00:04:02.275 --> 00:04:10.465
(Music: "Mary Had a Little Lamb")

00:04:11.820 --> 00:04:14.644
And so here's the video that we recorded,

00:04:14.644 --> 00:04:18.568
and we recorded it at thousands
of frames per second,

00:04:18.568 --> 00:04:20.890
but even if you look very closely,

00:04:20.890 --> 00:04:22.841
all you'll see are some leaves

00:04:22.841 --> 00:04:25.906
that are pretty much
just sitting there doing nothing,

00:04:25.906 --> 00:04:30.712
because our sound only moved those leaves
by about a micrometer.

00:04:31.103 --> 00:04:35.379
That's one ten-thousandth of a centimeter,

00:04:35.379 --> 00:04:39.535
which spans somewhere between
a hundredth and a thousandth

00:04:39.535 --> 00:04:41.834
of a pixel in this image.

00:04:41.881 --> 00:04:44.768
So you can squint all you want,

00:04:44.768 --> 00:04:48.103
but motion that small is pretty much
perceptually invisible.

00:04:49.667 --> 00:04:53.824
But it turns out that something
can be perceptually invisible

00:04:53.824 --> 00:04:56.633
and still be numerically significant,

00:04:56.633 --> 00:04:58.635
because with the right algorithms,

00:04:58.635 --> 00:05:02.322
we can take this silent,
seemingly still video

00:05:02.322 --> 00:05:03.849
and we can recover this sound.

00:05:04.690 --> 00:05:12.074
(Music: "Mary Had a Little Lamb")

00:05:12.074 --> 00:05:17.902
(Applause)

00:05:22.058 --> 00:05:23.997
So how is this possible?

00:05:23.997 --> 00:05:28.341
How can we get so much information
out of so little motion?

00:05:28.341 --> 00:05:33.702
Well, let's say that those leaves
move by just a single micrometer,

00:05:33.702 --> 00:05:38.010
and let's say that that shifts our image
by just a thousandth of a pixel.

00:05:39.269 --> 00:05:41.841
That may not seem like much,

00:05:41.841 --> 00:05:43.837
but a single frame of video

00:05:43.837 --> 00:05:47.094
may have hundreds of thousands
of pixels in it,

00:05:47.094 --> 00:05:50.548
and so if we combine all
of the tiny motions that we see

00:05:50.548 --> 00:05:52.846
from across that entire image,

00:05:52.846 --> 00:05:55.469
then suddenly a thousandth of a pixel

00:05:55.469 --> 00:05:58.244
can start to add up
to something pretty significant.

00:05:58.870 --> 00:06:02.505
On a personal note, we were pretty psyched
when we figured this out.

00:06:02.505 --> 00:06:04.825
(Laughter)

00:06:04.825 --> 00:06:08.078
But even with the right algorithm,

00:06:08.078 --> 00:06:11.695
we were still missing
a pretty important piece of the puzzle.

00:06:11.695 --> 00:06:15.299
You see, there are a lot of factors
that affect when and how well

00:06:15.299 --> 00:06:17.296
this technique will work.

00:06:17.296 --> 00:06:20.500
There's the object and how far away it is;

00:06:20.500 --> 00:06:22.894
there's the camera
and the lens that you use;

00:06:22.894 --> 00:06:26.985
how much light is shining on the object
and how loud your sound is.

00:06:27.945 --> 00:06:31.320
And even with the right algorithm,

00:06:31.320 --> 00:06:34.710
we had to be very careful
with our early experiments,

00:06:34.710 --> 00:06:37.102
because if we got
any of these factors wrong,

00:06:37.102 --> 00:06:39.470
there was no way to tell
what the problem was.

00:06:39.470 --> 00:06:42.117
We would just get noise back.

00:06:42.117 --> 00:06:45.437
And so a lot of our early
experiments looked like this.

00:06:45.437 --> 00:06:47.643
And so here I am,

00:06:47.643 --> 00:06:51.683
and on the bottom left, you can kind of
see our high-speed camera,

00:06:51.683 --> 00:06:53.866
which is pointed at a bag of chips,

00:06:53.866 --> 00:06:56.815
and the whole thing is lit
by these bright lamps.

00:06:56.815 --> 00:07:01.180
And like I said, we had to be
very careful in these early experiments,

00:07:01.180 --> 00:07:03.688
so this is how it went down.

00:07:03.688 --> 00:07:07.449
(Video) Abe Davis: Three, two, one, go.

00:07:07.449 --> 00:07:12.836
Mary had a little lamb!
Little lamb! Little lamb!

00:07:12.836 --> 00:07:17.336
(Laughter)

00:07:17.336 --> 00:07:20.150
AD: So this experiment
looks completely ridiculous.

00:07:20.150 --> 00:07:21.938
(Laughter)

00:07:21.938 --> 00:07:24.283
I mean, I'm screaming at a bag of chips --

00:07:24.283 --> 00:07:25.834
(Laughter) --

00:07:25.834 --> 00:07:27.951
and we're blasting it with so much light,

00:07:27.951 --> 00:07:32.430
we literally melted the first bag
we tried this on. (Laughter)

00:07:32.525 --> 00:07:35.799
But ridiculous as this experiment looks,

00:07:35.799 --> 00:07:37.587
it was actually really important,

00:07:37.587 --> 00:07:40.513
because we were able
to recover this sound.

00:07:40.513 --> 00:07:45.225
(Audio) Mary had a little lamb!
Little lamb! Little lamb!

00:07:45.225 --> 00:07:49.313
(Applause)

00:07:49.313 --> 00:07:51.194
AD: And this was really significant,

00:07:51.194 --> 00:07:55.313
because it was the first time
we recovered intelligible human speech

00:07:55.424 --> 00:07:57.765
from silent video of an object.

00:07:57.765 --> 00:08:00.156
And so it gave us this point of reference,

00:08:00.156 --> 00:08:04.027
and gradually we could start
to modify the experiment,

00:08:04.106 --> 00:08:07.911
using different objects
or moving the object further away,

00:08:07.911 --> 00:08:10.681
using less light or quieter sounds.

00:08:11.887 --> 00:08:14.761
And we analyzed all of these experiments

00:08:14.761 --> 00:08:18.383
until we really understood
the limits of our technique,

00:08:18.383 --> 00:08:20.333
because once we understood those limits,

00:08:20.333 --> 00:08:22.679
we could figure out how to push them.

00:08:22.679 --> 00:08:25.860
And that led to experiments like this one,

00:08:25.860 --> 00:08:28.599
where again, I'm going to speak
to a bag of chips,

00:08:28.599 --> 00:08:33.429
but this time we've moved our camera
about 15 feet away,

00:08:33.429 --> 00:08:36.262
outside, behind a soundproof window,

00:08:36.262 --> 00:08:39.065
and the whole thing is lit
by only natural sunlight.

00:08:40.529 --> 00:08:42.684
And so here's the video that we captured.

00:08:44.450 --> 00:08:49.009
And this is what things sounded like
from inside, next to the bag of chips.

00:08:49.009 --> 00:08:54.047
(Audio) Mary had a little lamb
whose fleece was white as snow,

00:08:54.047 --> 00:08:59.666
and everywhere that Mary went,
that lamb was sure to go.

00:08:59.666 --> 00:09:03.683
AD: And here's what we were able
to recover from our silent video

00:09:03.683 --> 00:09:06.028
captured outside behind that window.

00:09:06.028 --> 00:09:10.463
(Audio) Mary had a little lamb
whose fleece was white as snow,

00:09:10.463 --> 00:09:15.920
and everywhere that Mary went,
that lamb was sure to go.

00:09:15.920 --> 00:09:22.421
(Applause)

00:09:22.421 --> 00:09:25.963
AD: And there are other ways
that we can push these limits as well.

00:09:25.963 --> 00:09:27.761
So here's a quieter experiment

00:09:27.761 --> 00:09:31.871
where we filmed some earphones
plugged into a laptop computer,

00:09:31.871 --> 00:09:35.981
and in this case, our goal was to recover
the music that was playing on that laptop

00:09:35.981 --> 00:09:38.280
from just silent video

00:09:38.280 --> 00:09:40.787
of these two little plastic earphones,

00:09:40.787 --> 00:09:42.970
and we were able to do this so well

00:09:42.970 --> 00:09:45.431
that I could even Shazam our results.

00:09:45.431 --> 00:09:47.842
(Laughter)

00:09:49.191 --> 00:09:59.225
(Music: "Under Pressure" by Queen)

00:10:01.615 --> 00:10:06.584
(Applause)

00:10:06.584 --> 00:10:11.135
And we can also push things
by changing the hardware that we use.

00:10:11.135 --> 00:10:13.596
Because the experiments
I've shown you so far

00:10:13.596 --> 00:10:15.918
were done with a camera,
a high-speed camera,

00:10:15.918 --> 00:10:18.797
that can record video
about a 100 times faster

00:10:18.797 --> 00:10:20.724
than most cell phones,

00:10:20.724 --> 00:10:23.533
but we've also found a way
to use this technique

00:10:23.533 --> 00:10:25.763
with more regular cameras,

00:10:25.763 --> 00:10:29.832
and we do that by taking advantage
of what's called a rolling shutter.

00:10:29.832 --> 00:10:34.630
You see, most cameras
record images one row at a time,

00:10:34.630 --> 00:10:40.332
and so if an object moves
during the recording of a single image,

00:10:40.344 --> 00:10:43.061
there's a slight time delay
between each row,

00:10:43.061 --> 00:10:46.218
and this causes slight artifacts

00:10:46.218 --> 00:10:49.701
that get coded into each frame of a video.

00:10:49.701 --> 00:10:53.507
And so what we found
is that by analyzing these artifacts,

00:10:53.507 --> 00:10:58.122
we can actually recover sound
using a modified version of our algorithm.

00:10:58.122 --> 00:11:00.034
So here's an experiment we did

00:11:00.034 --> 00:11:01.729
where we filmed a bag of candy

00:11:01.729 --> 00:11:03.470
while a nearby loudspeaker played

00:11:03.470 --> 00:11:06.442
the same "Mary Had a Little Lamb"
music from before,

00:11:06.442 --> 00:11:10.645
but this time, we used just a regular
store-bought camera,

00:11:10.645 --> 00:11:13.819
and so in a second, I'll play for you
the sound that we recovered,

00:11:13.819 --> 00:11:15.869
and it's going to sound
distorted this time,

00:11:15.869 --> 00:11:18.705
but listen and see if you can still
recognize the music.

00:11:19.723 --> 00:11:25.946
(Audio: "Mary Had a Little Lamb")

00:11:37.527 --> 00:11:40.992
And so, again, that sounds distorted,

00:11:40.992 --> 00:11:45.378
but what's really amazing here
is that we were able to do this

00:11:45.378 --> 00:11:48.004
with something
that you could literally run out

00:11:48.004 --> 00:11:49.448
and pick up at a Best Buy.

00:11:51.122 --> 00:11:52.485
So at this point,

00:11:52.485 --> 00:11:54.459
a lot of people see this work,

00:11:54.459 --> 00:11:57.872
and they immediately think
about surveillance.

00:11:57.872 --> 00:12:00.287
And to be fair,

00:12:00.287 --> 00:12:04.420
it's not hard to imagine how you might use
this technology to spy on someone.

00:12:04.420 --> 00:12:08.367
But keep in mind that there's already
a lot of very mature technology

00:12:08.367 --> 00:12:09.946
out there for surveillance.

00:12:09.946 --> 00:12:12.036
In fact, people have been using lasers

00:12:12.036 --> 00:12:14.835
to eavesdrop on objects
from a distance for decades.

00:12:15.978 --> 00:12:18.003
But what's really new here,

00:12:18.003 --> 00:12:19.443
what's really different,

00:12:19.443 --> 00:12:23.738
is that now we have a way
to picture the vibrations of an object,

00:12:23.738 --> 00:12:27.151
which gives us a new lens
through which to look at the world,

00:12:27.151 --> 00:12:28.661
and we can use that lens

00:12:28.661 --> 00:12:33.560
to learn not just about forces like sound
that cause an object to vibrate,

00:12:33.560 --> 00:12:35.848
but also about the object itself.

00:12:36.975 --> 00:12:38.668
And so I want to take a step back

00:12:38.668 --> 00:12:42.917
and think about how that might change
the ways that we use video,

00:12:42.917 --> 00:12:46.470
because we usually use video
to look at things,

00:12:46.470 --> 00:12:48.792
and I've just shown you how we can use it

00:12:48.792 --> 00:12:50.649
to listen to things.

00:12:50.649 --> 00:12:54.620
But there's another important way
that we learn about the world:

00:12:54.620 --> 00:12:56.895
that's by interacting with it.

00:12:56.895 --> 00:13:00.006
We push and pull and poke and prod things.

00:13:00.006 --> 00:13:03.187
We shake things and see what happens.

00:13:03.187 --> 00:13:07.460
And that's something that video
still won't let us do,

00:13:07.460 --> 00:13:09.596
at least not traditionally.

00:13:09.596 --> 00:13:11.546
So I want to show you some new work,

00:13:11.546 --> 00:13:14.213
and this is based on an idea I had
just a few months ago,

00:13:14.213 --> 00:13:17.514
so this is actually the first time
I've shown it to a public audience.

00:13:17.514 --> 00:13:22.877
And the basic idea is that we're going
to use the vibrations in a video

00:13:22.877 --> 00:13:27.358
to capture objects in a way
that will let us interact with them

00:13:27.358 --> 00:13:29.332
and see how they react to us.

00:13:31.120 --> 00:13:32.884
So here's an object,

00:13:32.884 --> 00:13:36.716
and in this case, it's a wire figure
in the shape of a human,

00:13:36.716 --> 00:13:39.804
and we're going to film that object
with just a regular camera.

00:13:39.804 --> 00:13:41.928
So there's nothing special
about this camera.

00:13:41.928 --> 00:13:44.889
In fact, I've actually done this
with my cell phone before.

00:13:44.889 --> 00:13:47.141
But we do want to see the object vibrate,

00:13:47.141 --> 00:13:48.274
so to make that happen,

00:13:48.274 --> 00:13:51.620
we're just going to bang a little bit
on the surface where it's resting

00:13:51.620 --> 00:13:53.758
while we record this video.

00:13:59.398 --> 00:14:03.069
So that's it: just five seconds
of regular video,

00:14:03.069 --> 00:14:05.205
while we bang on this surface,

00:14:05.205 --> 00:14:08.718
and we're going to use
the vibrations in that video

00:14:08.718 --> 00:14:13.262
to learn about the structural
and material properties of our object,

00:14:13.262 --> 00:14:18.096
and we're going to use that information
to create something new and interactive.

00:14:24.866 --> 00:14:27.519
And so here's what we've created.

00:14:27.519 --> 00:14:29.748
And it looks like a regular image,

00:14:29.748 --> 00:14:32.859
but this isn't an image,
and it's not a video,

00:14:32.859 --> 00:14:35.227
because now I can take my mouse

00:14:35.227 --> 00:14:38.086
and I can start interacting
with the object.

00:14:44.936 --> 00:14:47.293
And so what you see here

00:14:47.389 --> 00:14:49.615
is a simulation of how this object

00:14:49.615 --> 00:14:54.073
would respond to new forces
that we've never seen before,

00:14:54.073 --> 00:14:57.706
and we created it from just
five seconds of regular video.

00:14:59.249 --> 00:15:03.964
(Applause)

00:15:09.421 --> 00:15:12.648
And so this is a really powerful
way to look at the world,

00:15:12.648 --> 00:15:15.620
because it lets us predict
how objects will respond

00:15:15.620 --> 00:15:17.443
to new situations,

00:15:17.443 --> 00:15:20.916
and you could imagine, for instance,
looking at an old bridge

00:15:20.916 --> 00:15:24.443
and wondering what would happen,
how would that bridge hold up

00:15:24.443 --> 00:15:27.276
if I were to drive my car across it.

00:15:27.276 --> 00:15:30.050
And that's a question
that you probably want to answer

00:15:30.050 --> 00:15:32.610
before you start driving
across that bridge.

00:15:33.988 --> 00:15:37.260
And of course, there are going to be
limitations to this technique,

00:15:37.260 --> 00:15:39.722
just like there were
with the visual microphone,

00:15:39.722 --> 00:15:42.903
but we found that it works
in a lot of situations

00:15:42.903 --> 00:15:44.778
that you might not expect,

00:15:44.778 --> 00:15:47.546
especially if you give it longer videos.

00:15:47.546 --> 00:15:50.054
So for example,
here's a video that I captured

00:15:50.054 --> 00:15:52.353
of a bush outside of my apartment,

00:15:52.353 --> 00:15:55.441
and I didn't do anything to this bush,

00:15:55.441 --> 00:15:58.146
but by capturing a minute-long video,

00:15:58.146 --> 00:16:01.524
a gentle breeze caused enough vibrations

00:16:01.524 --> 00:16:05.111
that we could learn enough about this bush
to create this simulation.

00:16:07.270 --> 00:16:13.412
(Applause)

00:16:13.412 --> 00:16:16.384
And so you could imagine giving this
to a film director,

00:16:16.384 --> 00:16:18.103
and letting him control, say,

00:16:18.103 --> 00:16:23.025
the strength and direction of wind
in a shot after it's been recorded.

00:16:24.810 --> 00:16:29.345
Or, in this case, we pointed our camera
at a hanging curtain,

00:16:29.345 --> 00:16:33.474
and you can't even see
any motion in this video,

00:16:33.474 --> 00:16:36.399
but by recording a two-minute-long video,

00:16:36.399 --> 00:16:38.837
natural air currents in this room

00:16:38.837 --> 00:16:43.249
created enough subtle,
imperceptible motions and vibrations

00:16:43.249 --> 00:16:45.814
that we could learn enough
to create this simulation.

00:16:48.243 --> 00:16:50.609
And ironically,

00:16:50.609 --> 00:16:53.697
we're kind of used to having
this kind of interactivity

00:16:53.697 --> 00:16:56.344
when it comes to virtual objects,

00:16:56.344 --> 00:16:59.641
when it comes to video games
and 3D models,

00:16:59.641 --> 00:17:04.045
but to be able to capture this information
from real objects in the real world

00:17:04.045 --> 00:17:06.862
using just simple, regular video,

00:17:06.862 --> 00:17:09.045
is something new that has
a lot of potential.

00:17:10.410 --> 00:17:15.314
So here are the amazing people
who worked with me on these projects.

00:17:16.057 --> 00:17:21.653
(Applause)

00:17:24.819 --> 00:17:27.876
And what I've shown you today
is only the beginning.

00:17:27.876 --> 00:17:29.989
We've just started to scratch the surface

00:17:29.989 --> 00:17:32.961
of what you can do
with this kind of imaging,

00:17:32.961 --> 00:17:35.247
because it gives us a new way

00:17:35.342 --> 00:17:40.066
to capture our surroundings
with common, accessible technology.

00:17:40.066 --> 00:17:41.995
And so looking to the future,

00:17:41.995 --> 00:17:44.032
it's going to be
really exciting to explore

00:17:44.032 --> 00:17:45.888
what this can tell us about the world.

00:17:46.381 --> 00:17:47.585
Thank you.

00:17:47.610 --> 00:17:53.717
(Applause)

