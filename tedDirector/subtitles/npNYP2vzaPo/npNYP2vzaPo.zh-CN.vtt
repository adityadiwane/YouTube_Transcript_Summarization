WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:07.000
翻译人员: Hong Li
校对人员: Li Li

00:00:13.373 --> 00:00:16.722
大部分人认为
动作是明显可见的。

00:00:17.889 --> 00:00:22.977
比如我走过这个舞台，
或者边做手势边说话，

00:00:22.977 --> 00:00:25.238
这些动作都能被大家看到。

00:00:26.255 --> 00:00:31.737
但还有很多重要的动作
肉眼很难察觉到，

00:00:31.737 --> 00:00:33.778
在过去几年中，

00:00:33.778 --> 00:00:35.775
我们致力于寻找某种摄像机

00:00:35.775 --> 00:00:39.185
可以捕捉到人眼看不到的运动。

00:00:40.305 --> 00:00:41.856
请看大屏幕。

00:00:42.717 --> 00:00:46.339
左边是一个人的手腕，

00:00:46.339 --> 00:00:49.486
右边是一个熟睡的婴儿，

00:00:49.486 --> 00:00:52.532
但是如果我不告诉你们这是一段视频，

00:00:52.532 --> 00:00:56.283
你们可能会认为
这只是两张普通的图片，

00:00:56.283 --> 00:00:57.825
因为乍一看，

00:00:57.825 --> 00:01:01.112
这两段视频几乎是完全静止的。

00:01:02.175 --> 00:01:06.060
但实际上，画面中
有许多细微的运动变化，

00:01:06.060 --> 00:01:08.452
如果你能碰到左边的那个手腕，

00:01:08.452 --> 00:01:10.448
你会感受到脉搏的跳动，

00:01:10.448 --> 00:01:12.933
如果你抱起右边的婴儿，

00:01:12.933 --> 00:01:15.324
你能感受到她胸腔的起伏，

00:01:15.324 --> 00:01:16.954
感受到她的每一次呼吸。

00:01:17.762 --> 00:01:21.338
这些动作都很重要，

00:01:21.338 --> 00:01:24.681
但由于过于细微，
很难被我们察觉，

00:01:24.681 --> 00:01:26.957
要想感受到这些动作的存在

00:01:26.957 --> 00:01:29.857
只能通过直接接触。

00:01:30.997 --> 00:01:32.262
然而几年前，

00:01:32.262 --> 00:01:36.667
我在麻省理工学院的同事们
开发出了一种被称为“动作显微镜”的软件，

00:01:36.667 --> 00:01:41.051
能够发现视频中细微的运动，

00:01:41.051 --> 00:01:44.613
并将其放大到肉眼可见的级别。

00:01:45.416 --> 00:01:48.899
如果我们运用这一软件分析左边的视频，

00:01:48.899 --> 00:01:52.149
我们就能看到手腕上的脉搏跳动，

00:01:52.149 --> 00:01:53.844
通过计算脉搏数量，

00:01:53.844 --> 00:01:56.199
就能得知这个人的心率。

00:01:57.095 --> 00:02:00.160
而用这一软件分析右边的视频，

00:02:00.160 --> 00:02:03.387
我们就能看清婴儿的每一次呼吸，

00:02:03.387 --> 00:02:07.524
不需要触碰就能监控她的呼吸。

00:02:08.884 --> 00:02:14.232
这项技术非常强大，
因为它能帮助我们看到

00:02:14.232 --> 00:02:16.599
原本要靠触觉才能感受到的东西，

00:02:16.599 --> 00:02:19.556
并且这一过程是可见和无创的。

00:02:21.104 --> 00:02:25.515
因此在几年前，我开始
与这个软件的编写者们一起工作，

00:02:25.515 --> 00:02:28.882
我们产生了一个疯狂的想法。

00:02:28.882 --> 00:02:31.575
我们觉得，运用软件将细微的动作

00:02:31.575 --> 00:02:34.710
可视化的这个点子非常酷，

00:02:34.710 --> 00:02:39.168
你甚至可以把它当做拓展
人类触觉感官的好方法。

00:02:39.168 --> 00:02:43.227
那如果我们能用相同的方法
来增强我们的听觉呢？

00:02:44.508 --> 00:02:49.173
如果我们能通过视频捕捉到声音的振动，

00:02:49.173 --> 00:02:52.000
声音的振动实际上也是一种运动，

00:02:52.000 --> 00:02:55.346
将“看到”的东西录入麦克风呢？

00:02:56.236 --> 00:02:58.207
也许听起来有点不太好理解，

00:02:58.207 --> 00:03:00.793
我试着为大家解释一下。

00:03:01.523 --> 00:03:05.011
传统麦克风的工作原理

00:03:05.011 --> 00:03:08.610
是将其内部薄膜的振动转换成电信号，

00:03:08.610 --> 00:03:12.928
这个薄膜极易随声音振动，

00:03:12.928 --> 00:03:17.735
这个振动可以被记录下来
并还原成声音。

00:03:17.735 --> 00:03:21.403
而声音事实上可以
引起任何物体的振动。

00:03:21.403 --> 00:03:26.883
只不过这种振动对我们而言
通常很细微而且转瞬即逝。

00:03:26.883 --> 00:03:30.621
但如果我们用高速摄影机
将这种振动录下来，

00:03:30.621 --> 00:03:34.197
并通过软件从这些高速视频中

00:03:34.197 --> 00:03:36.287
提取出这些细小的振动，

00:03:36.287 --> 00:03:40.561
然后分析这些振动来
弄清声音的来源，会怎么样呢？

00:03:41.859 --> 00:03:47.308
这样一来我们可以将远处的
可见物体转化为可视化麦克风。

00:03:49.080 --> 00:03:51.263
我们进行了各种尝试，

00:03:51.263 --> 00:03:53.030
以下是我们的试验之一，

00:03:53.030 --> 00:03:56.139
右边是一株盆栽植物，

00:03:56.139 --> 00:03:58.577
我们用高速摄影机拍下它，

00:03:58.577 --> 00:04:02.106
同时旁边的音箱在播放这个声音。

00:04:02.275 --> 00:04:10.465
（音乐：玛丽有一只小羊羔）

00:04:11.820 --> 00:04:14.644
这是我们录下的视频，

00:04:14.644 --> 00:04:18.568
用的是每秒数千帧的速度，

00:04:18.568 --> 00:04:20.890
但即使你凑得非常近，

00:04:20.890 --> 00:04:22.841
也只能看到一些叶子

00:04:22.841 --> 00:04:25.906
静静地呆在那儿，一动不动，

00:04:25.906 --> 00:04:30.712
因为刚才的音乐
只能让叶子移动一微米，

00:04:31.103 --> 00:04:35.379
也就是一厘米的万分之一，

00:04:35.379 --> 00:04:39.535
只占这幅图像中一个像素的

00:04:39.535 --> 00:04:41.834
百分之一到千分之一。

00:04:41.881 --> 00:04:44.768
你大可以眯着眼使劲儿看，

00:04:44.768 --> 00:04:48.503
但如此细微的运动
从感官上来说是不可见的。

00:04:49.667 --> 00:04:53.824
但事实证明感官上不可见的东西

00:04:53.824 --> 00:04:56.633
在数值上可能很惊人,

00:04:56.633 --> 00:04:58.635
因为通过正确的算法，

00:04:58.635 --> 00:05:02.322
我们就可以从这段无声的
看似静止的视频中

00:05:02.322 --> 00:05:03.849
还原出这段声音。

00:05:04.690 --> 00:05:12.074
（音乐：玛丽有一只小羊羔）

00:05:12.074 --> 00:05:20.932
（掌声）

00:05:22.058 --> 00:05:23.997
这怎么可能呢？

00:05:23.997 --> 00:05:28.341
我们怎么能从如此细小的运动中
得到如此丰富的信息？

00:05:28.341 --> 00:05:33.702
我们必须承认这些叶子
只移动了一微米，

00:05:33.702 --> 00:05:38.010
只改变了图像中一个像素的千分之一。

00:05:39.269 --> 00:05:41.841
看起来很微不足道，

00:05:41.841 --> 00:05:43.837
但是视频中的每一帧

00:05:43.837 --> 00:05:47.094
都包含数以万计的像素，

00:05:47.094 --> 00:05:50.548
当我们将整幅画面中
所有细微的运动

00:05:50.548 --> 00:05:52.846
组合在一起来看的时候，

00:05:52.846 --> 00:05:55.469
无数个千分之一像素聚在一起

00:05:55.469 --> 00:05:58.244
就能组合出有十分意义的信息。

00:05:58.870 --> 00:06:02.505
老实说，当我们想通
这一点的时候真是乐疯了。

00:06:02.505 --> 00:06:04.825
（笑声）

00:06:04.825 --> 00:06:08.078
但是，即便运用正确的算法

00:06:08.078 --> 00:06:11.695
我们还是会丢失掉很多重要的信息。

00:06:11.695 --> 00:06:15.299
这项技术能否成功

00:06:15.299 --> 00:06:17.296
取决于很多因素。

00:06:17.296 --> 00:06:20.500
比如目标物体的距离；

00:06:20.500 --> 00:06:22.894
摄影机和镜头的选用；

00:06:22.894 --> 00:06:26.985
光线是否充足，
声音是否够大等等。

00:06:27.945 --> 00:06:31.320
因此，即便我们的算法正确，

00:06:31.320 --> 00:06:34.710
在早期试验中
我们还是得万分谨慎，

00:06:34.710 --> 00:06:37.102
因为一着不慎，满盘皆输，

00:06:37.102 --> 00:06:39.470
得不到有用的信息，
也查不出原因。

00:06:39.470 --> 00:06:42.117
还原出来的只有噪音。

00:06:42.117 --> 00:06:45.437
初期的试验场景是这样的。

00:06:45.437 --> 00:06:47.643
左边的是我，

00:06:47.643 --> 00:06:51.683
左下角是我们的高速摄影机，

00:06:51.683 --> 00:06:53.866
正对着一袋薯片，

00:06:53.866 --> 00:06:56.815
薯片被一盏明亮的灯照着。

00:06:56.815 --> 00:07:01.180
就像刚才我说的，
在初期试验中我们需要十分小心，

00:07:01.180 --> 00:07:03.688
得有多小心呢？请看。

00:07:03.688 --> 00:07:07.449
（视频：三、二、一，开始）

00:07:07.449 --> 00:07:12.836
（视频：玛丽有一只小羊羔！
小羊羔！小羊羔！）

00:07:12.836 --> 00:07:17.336
（笑声）

00:07:17.336 --> 00:07:20.150
这试验看起来真是弱爆了。

00:07:20.150 --> 00:07:21.938
（笑声）

00:07:21.938 --> 00:07:24.283
我可是对着一袋薯片在咆哮——

00:07:24.283 --> 00:07:25.834
（笑声）

00:07:25.834 --> 00:07:27.951
而且我们用的灯功率太大，

00:07:27.951 --> 00:07:32.485
差点把第一袋薯片点着了。
（笑声）

00:07:32.485 --> 00:07:35.799
虽然看起来很不靠谱，

00:07:35.799 --> 00:07:37.587
但结果还是不错的，

00:07:37.587 --> 00:07:40.513
因为我们最终还原出了这段声音。

00:07:40.513 --> 00:07:45.225
（音频：玛丽有一只小羊羔！
小羊羔！小羊羔！）

00:07:45.225 --> 00:07:49.313
（掌声）

00:07:49.313 --> 00:07:51.194
这绝对是一个里程碑，

00:07:51.194 --> 00:07:55.433
因为这是我们第一次
从一段无声录像中

00:07:55.433 --> 00:07:57.765
还原出具有意义的人声。

00:07:57.765 --> 00:08:00.156
因此我们以此为出发点

00:08:00.156 --> 00:08:04.097
不断修正我们的试验，

00:08:04.106 --> 00:08:07.911
更换试验对象，调整距离，

00:08:07.911 --> 00:08:10.681
减小光线强度，降低声音等等。

00:08:11.887 --> 00:08:14.761
我们不断分析试验结果，

00:08:14.761 --> 00:08:18.383
直到发现这一技术的局限性，

00:08:18.383 --> 00:08:20.333
因为只有搞清楚局限在哪儿

00:08:20.333 --> 00:08:22.679
我们才能不断取得突破。

00:08:22.679 --> 00:08:25.860
于是，就有了下面这个试验，

00:08:25.860 --> 00:08:28.599
这一次，我还是对着一袋薯片说话，

00:08:28.599 --> 00:08:33.429
但将摄影机后退到了15英尺
（4.572米）远的室外，

00:08:33.429 --> 00:08:36.262
隔着一层隔音玻璃，

00:08:36.262 --> 00:08:39.065
只借助自然光线。

00:08:40.529 --> 00:08:42.684
这是我们拍下的视频。

00:08:44.450 --> 00:08:49.009
这是在室内，
在薯片旁说话的原声。

00:08:49.009 --> 00:08:54.047
（音频：玛丽有一只小羊羔，
身上羊毛白又好，

00:08:54.047 --> 00:08:59.666
无论玛丽走到哪，
小羊都会跟着跑。）

00:08:59.666 --> 00:09:03.683
这是通过我们从室外
隔音玻璃后采集的无声影像

00:09:03.683 --> 00:09:06.028
还原出来的声音。

00:09:06.028 --> 00:09:10.463
（音频：玛丽有一只小羊羔，
身上羊毛白又好，

00:09:10.463 --> 00:09:15.920
无论玛丽走到哪，
小羊都会跟着跑。）

00:09:15.920 --> 00:09:22.421
（掌声）

00:09:22.421 --> 00:09:25.963
我们还调整了其它参数。

00:09:25.963 --> 00:09:27.761
比如说降低音量，

00:09:27.761 --> 00:09:31.871
这有一副耳机，插在笔记本电脑上，

00:09:31.871 --> 00:09:35.981
在这个实验中，我们想仅通过拍摄下
这对塑料耳机的

00:09:35.981 --> 00:09:38.280
无声视频来还原

00:09:38.280 --> 00:09:40.787
笔记本里播放的音乐，

00:09:40.787 --> 00:09:42.970
结果很理想，

00:09:42.970 --> 00:09:45.431
我甚至能用Shazam
来识别出这段音乐。

00:09:45.431 --> 00:09:47.842
（笑声）

00:09:49.191 --> 00:10:01.615
（音乐：“皇后乐队”的《重压之下》）

00:10:01.615 --> 00:10:06.584
（掌声）

00:10:06.584 --> 00:10:11.135
我们还尝试了更换试验设备
来完善我们的成果。

00:10:11.135 --> 00:10:13.596
因为前面我给大家展示的试验

00:10:13.596 --> 00:10:15.918
都是通过高速摄影机完成的，

00:10:15.918 --> 00:10:18.797
它的拍摄速度比大多数手机摄像头

00:10:18.797 --> 00:10:20.724
快100倍，

00:10:20.724 --> 00:10:23.533
但是我们也找到了用普通摄影机

00:10:23.533 --> 00:10:25.763
来完成试验的方法，

00:10:25.763 --> 00:10:29.832
我们利用了叫做“滚动快门”的技术。

00:10:29.832 --> 00:10:34.630
大部分摄像头是逐行拍摄影像的，

00:10:34.630 --> 00:10:40.332
因此如果在拍摄单张照片时
物体发生了移动，

00:10:40.344 --> 00:10:43.061
每一行影像间就会出现少许延迟，

00:10:43.061 --> 00:10:46.218
这种延迟使得视频的每一帧

00:10:46.218 --> 00:10:49.701
都会产生轻微的变形。

00:10:49.701 --> 00:10:53.507
通过分析这种变形，

00:10:53.507 --> 00:10:58.122
运用调整过的算法
我们还是可以还原声音。

00:10:58.122 --> 00:11:00.034
在接下来这个试验里，

00:11:00.034 --> 00:11:01.729
我们拍摄的是一袋糖果，

00:11:01.729 --> 00:11:03.470
旁边的喇叭里播放的

00:11:03.470 --> 00:11:06.442
还是之前那首“玛丽有一只小羊羔”，

00:11:06.442 --> 00:11:10.645
但这一次我们使用的是
能在店里买到的普通摄影机，

00:11:10.645 --> 00:11:13.629
下面请听我们还原出来的声音，

00:11:13.629 --> 00:11:15.869
这次的声音有些失真，

00:11:15.869 --> 00:11:19.555
但仔细听一下，
看你能否分辨出来这段音乐。

00:11:19.723 --> 00:11:36.186
（音频：玛丽有一只小羊羔）

00:11:37.527 --> 00:11:40.992
就是这样，听起来有点失真，

00:11:40.992 --> 00:11:45.378
但别忘了
我们这次用的是普通摄影机，

00:11:45.378 --> 00:11:48.004
你随便到一家百思买
这样的电器商店

00:11:48.004 --> 00:11:49.448
就可以买到。

00:11:51.122 --> 00:11:52.485
那么目前为止，

00:11:52.485 --> 00:11:54.459
相信许多人看到这儿

00:11:54.459 --> 00:11:57.872
立刻想到了监听。

00:11:57.872 --> 00:12:00.287
说实话，

00:12:00.287 --> 00:12:04.420
用这个技术去监听
还真不是什么难事。

00:12:04.420 --> 00:12:08.367
但请大家注意，
早就有很多成熟的技术

00:12:08.367 --> 00:12:09.946
被用于监听了。

00:12:09.946 --> 00:12:12.036
实际上，将激光投射在物体上

00:12:12.036 --> 00:12:14.835
进行远距离监听的技术
已经出现几十年了。

00:12:15.978 --> 00:12:18.003
但我们这项技术的创新之处，

00:12:18.003 --> 00:12:19.443
与众不同之处

00:12:19.443 --> 00:12:23.738
在于我们掌握了一种
描绘物体振动的方法，

00:12:23.738 --> 00:12:27.151
使我们能通过一种全新的镜头
去看这个世界。

00:12:27.151 --> 00:12:28.661
通过这个镜头，

00:12:28.661 --> 00:12:33.560
不仅能看清使物体产生振动的外力，
比如声音，

00:12:33.560 --> 00:12:35.848
还能了解物体本身的性质。

00:12:36.975 --> 00:12:38.668
因此我想换个角度

00:12:38.668 --> 00:12:42.917
思考这将如何改变
我们使用视频的方式，

00:12:42.917 --> 00:12:46.470
我们通常用视频来“看”东西，

00:12:46.470 --> 00:12:48.792
而我刚刚给大家展示的是如何用视频

00:12:48.792 --> 00:12:50.649
来“听”东西。

00:12:50.649 --> 00:12:54.620
但是还有一种认识世界的重要方式，

00:12:54.620 --> 00:12:56.895
就是与世界互动。

00:12:56.895 --> 00:13:00.006
我们可以移动或触碰某个物体。

00:13:00.006 --> 00:13:03.187
或者摇晃它，看它会发生什么变化。

00:13:03.187 --> 00:13:07.460
但这一变化（可能太过微小）
视频没法捕捉，

00:13:07.460 --> 00:13:09.596
至少用传统的方式实现不了。

00:13:09.596 --> 00:13:11.546
因此我想向大家展示一项新的成果，

00:13:11.546 --> 00:13:14.213
这项成果基于我几个月前的一个想法，

00:13:14.213 --> 00:13:17.514
今天其实是我第一次将它公之于众。

00:13:17.514 --> 00:13:22.877
简而言之就是，
我们会利用视频里的振动

00:13:22.877 --> 00:13:27.358
来与物体进行互动，

00:13:27.358 --> 00:13:30.052
然后看物体如何反应。

00:13:31.120 --> 00:13:32.884
这是我们的试验对象，

00:13:32.884 --> 00:13:36.716
一个用铁丝做成的小人，

00:13:36.716 --> 00:13:39.804
我们使用的是一台普通的摄影机。

00:13:39.804 --> 00:13:41.928
没有任何特别之处。

00:13:41.928 --> 00:13:44.889
实际上，我用手机也能做到。

00:13:44.889 --> 00:13:47.141
但如果我们想让这个小人振动，

00:13:47.141 --> 00:13:48.274
要怎么做呢，

00:13:48.274 --> 00:13:51.620
我们仅仅在放置小人的
台子上敲了几下，

00:13:51.620 --> 00:13:53.758
并把过程拍了下来。

00:13:59.398 --> 00:14:03.069
就这样，我们得到了一段
五秒钟的普通视频，

00:14:03.069 --> 00:14:05.205
敲了几下台子，

00:14:05.205 --> 00:14:08.718
我们将利用视频里的振动

00:14:08.718 --> 00:14:13.262
来研究这个小人的
结构特征和材料特征，

00:14:13.262 --> 00:14:18.096
并利用这些信息
创造出一种新的具有互动性的东西。

00:14:24.866 --> 00:14:27.519
这就是我们的成果

00:14:27.519 --> 00:14:29.748
看起来像一张普通的图片，

00:14:29.748 --> 00:14:32.859
但这不是图片，
也不是视频，

00:14:32.859 --> 00:14:35.227
因为我可以移动鼠标

00:14:35.227 --> 00:14:38.086
与这个小人进行互动。

00:14:44.936 --> 00:14:47.463
现在大家看到的

00:14:47.463 --> 00:14:49.615
是模拟小人在受到外力时

00:14:49.615 --> 00:14:54.073
会如何反应，
即使这种外力是初次施加的，

00:14:54.073 --> 00:14:57.706
而这都来源于那
短短五秒钟的普通视频。

00:14:59.249 --> 00:15:07.564
（掌声）

00:15:09.421 --> 00:15:12.648
这的确是一种审视世界的有效方法，

00:15:12.648 --> 00:15:15.620
让我们可以预测物体在新的条件下

00:15:15.620 --> 00:15:17.443
会作何反应，

00:15:17.443 --> 00:15:20.916
想象一下，前面有一座很旧的桥，

00:15:20.916 --> 00:15:24.443
我们不知道它是否足够结实，

00:15:24.443 --> 00:15:27.276
我们能不能把车开过去。

00:15:27.276 --> 00:15:30.050
而这种问题
最好在你开车上桥之前

00:15:30.050 --> 00:15:32.610
就搞清楚答案。

00:15:33.988 --> 00:15:37.260
当然，这项技术有它的局限，

00:15:37.260 --> 00:15:39.722
就像之前的视觉麦克风试验一样，

00:15:39.722 --> 00:15:42.903
但我们也发现
它能在许多场景下发挥作用，

00:15:42.903 --> 00:15:44.778
有时甚至出乎你的意料，

00:15:44.778 --> 00:15:47.546
特别是当视频时间足够长的时候。

00:15:47.546 --> 00:15:50.054
举个例子，这段视频

00:15:50.054 --> 00:15:52.353
拍的是我公寓外的灌木丛，

00:15:52.353 --> 00:15:55.441
我没有动过它，

00:15:55.441 --> 00:15:58.146
只是拍了一段1分钟长的视频，

00:15:58.146 --> 00:16:01.524
微风不断吹动灌木，

00:16:01.524 --> 00:16:05.111
让我能够收集到足够的信息
来完成这段模拟。

00:16:07.270 --> 00:16:13.412
（掌声）

00:16:13.412 --> 00:16:16.384
想象一下，
如果电影导演掌握了这项技术，

00:16:16.384 --> 00:16:18.103
他就可以在后期制作时

00:16:18.103 --> 00:16:23.025
随心所欲地控制风的大小和方向。

00:16:24.810 --> 00:16:29.345
来看另一个例子，
我们拍摄了一副挂起来的窗帘，

00:16:29.345 --> 00:16:33.474
在这段视频里
你甚至看不出来窗帘在动，

00:16:33.474 --> 00:16:36.399
但是利用2分钟长的一段视频，

00:16:36.399 --> 00:16:38.837
仅仅靠房间里的自然空气流动

00:16:38.837 --> 00:16:43.249
引发的无法察觉的动作和振动，

00:16:43.249 --> 00:16:48.244
就能使我们提取出足够多的
信息来完成这段模拟。

00:16:48.244 --> 00:16:50.609
神奇的是，

00:16:50.609 --> 00:16:53.697
以往我们都是针对虚拟物体，

00:16:53.697 --> 00:16:56.344
针对游戏和3D模型

00:16:56.344 --> 00:16:59.641
来实现这种互动，

00:16:59.641 --> 00:17:04.045
而这项技术仅仅是利用
普通的视频

00:17:04.045 --> 00:17:06.862
对现实世界中的
真实物体进行采样，

00:17:06.862 --> 00:17:10.415
它极富新意，
具有广阔的应用前景。

00:17:10.415 --> 00:17:16.064
这些是跟我共同研究
这项技术的优秀的同事。

00:17:16.064 --> 00:17:23.803
（掌声）

00:17:24.819 --> 00:17:27.876
今天向大家展示的
只是一个技术雏形。

00:17:27.876 --> 00:17:29.989
关于如何使用这种新型图像，

00:17:29.989 --> 00:17:32.961
我们才刚刚入门，

00:17:32.961 --> 00:17:35.337
它为我们提供了一种

00:17:35.342 --> 00:17:40.066
运用已有的普通技术
来记录周围事物的新方法。

00:17:40.066 --> 00:17:41.995
展望一下未来，

00:17:41.995 --> 00:17:44.032
我们迫不及待地想要看到如何

00:17:44.032 --> 00:17:46.438
利用这项技术去更好地了解世界。

00:17:46.438 --> 00:17:47.655
谢谢大家。

00:17:47.655 --> 00:17:53.717
（掌声）

