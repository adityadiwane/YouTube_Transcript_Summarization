WEBVTT
Kind: captions
Language: en

00:00:12.556 --> 00:00:16.573
Our emotions influence
every aspect of our lives,

00:00:16.573 --> 00:00:20.149
from our health and how we learn,
to how we do business and make decisions,

00:00:20.149 --> 00:00:21.922
big ones and small.

00:00:22.672 --> 00:00:26.162
Our emotions also influence
how we connect with one another.

00:00:27.132 --> 00:00:31.108
We've evolved to live
in a world like this,

00:00:31.108 --> 00:00:35.427
but instead, we're living
more and more of our lives like this --

00:00:35.427 --> 00:00:38.561
this is the text message
from my daughter last night --

00:00:38.561 --> 00:00:41.301
in a world that's devoid of emotion.

00:00:41.301 --> 00:00:43.252
So I'm on a mission to change that.

00:00:43.252 --> 00:00:47.343
I want to bring emotions
back into our digital experiences.

00:00:48.223 --> 00:00:51.300
I started on this path 15 years ago.

00:00:51.300 --> 00:00:53.366
I was a computer scientist in Egypt,

00:00:53.366 --> 00:00:57.871
and I had just gotten accepted to
a Ph.D. program at Cambridge University.

00:00:57.871 --> 00:00:59.984
So I did something quite unusual

00:00:59.984 --> 00:01:04.209
for a young newlywed Muslim Egyptian wife:

00:01:05.599 --> 00:01:08.598
With the support of my husband,
who had to stay in Egypt,

00:01:08.598 --> 00:01:11.616
I packed my bags and I moved to England.

00:01:11.616 --> 00:01:14.844
At Cambridge, thousands of miles
away from home,

00:01:14.844 --> 00:01:18.257
I realized I was spending
more hours with my laptop

00:01:18.257 --> 00:01:20.486
than I did with any other human.

00:01:20.486 --> 00:01:25.339
Yet despite this intimacy, my laptop
had absolutely no idea how I was feeling.

00:01:25.339 --> 00:01:28.550
It had no idea if I was happy,

00:01:28.550 --> 00:01:31.538
having a bad day, or stressed, confused,

00:01:31.538 --> 00:01:34.460
and so that got frustrating.

00:01:35.600 --> 00:01:40.831
Even worse, as I communicated
online with my family back home,

00:01:41.421 --> 00:01:44.703
I felt that all my emotions
disappeared in cyberspace.

00:01:44.703 --> 00:01:49.858
I was homesick, I was lonely,
and on some days I was actually crying,

00:01:49.858 --> 00:01:54.786
but all I had to communicate
these emotions was this.

00:01:54.786 --> 00:01:56.806
(Laughter)

00:01:56.806 --> 00:02:01.780
Today's technology
has lots of I.Q., but no E.Q.;

00:02:01.780 --> 00:02:04.956
lots of cognitive intelligence,
but no emotional intelligence.

00:02:04.956 --> 00:02:07.153
So that got me thinking,

00:02:07.153 --> 00:02:10.777
what if our technology
could sense our emotions?

00:02:10.777 --> 00:02:14.853
What if our devices could sense
how we felt and reacted accordingly,

00:02:14.853 --> 00:02:17.866
just the way an emotionally
intelligent friend would?

00:02:18.666 --> 00:02:22.230
Those questions led me and my team

00:02:22.230 --> 00:02:26.607
to create technologies that can read
and respond to our emotions,

00:02:26.607 --> 00:02:29.697
and our starting point was the human face.

00:02:30.577 --> 00:02:33.750
So our human face happens to be
one of the most powerful channels

00:02:33.750 --> 00:02:37.766
that we all use to communicate
social and emotional states,

00:02:37.766 --> 00:02:40.776
everything from enjoyment, surprise,

00:02:40.776 --> 00:02:44.979
empathy and curiosity.

00:02:44.979 --> 00:02:49.907
In emotion science, we call each
facial muscle movement an action unit.

00:02:49.907 --> 00:02:52.832
So for example, action unit 12,

00:02:52.832 --> 00:02:54.870
it's not a Hollywood blockbuster,

00:02:54.870 --> 00:02:58.312
it is actually a lip corner pull,
which is the main component of a smile.

00:02:58.312 --> 00:03:01.300
Try it everybody. Let's get
some smiles going on.

00:03:01.300 --> 00:03:03.954
Another example is action unit 4.
It's the brow furrow.

00:03:03.954 --> 00:03:06.192
It's when you draw your eyebrows together

00:03:06.192 --> 00:03:08.459
and you create all
these textures and wrinkles.

00:03:08.459 --> 00:03:12.754
We don't like them, but it's
a strong indicator of a negative emotion.

00:03:12.754 --> 00:03:14.960
So we have about 45 of these action units,

00:03:14.960 --> 00:03:18.350
and they combine to express
hundreds of emotions.

00:03:18.350 --> 00:03:22.251
Teaching a computer to read
these facial emotions is hard,

00:03:22.251 --> 00:03:25.223
because these action units,
they can be fast, they're subtle,

00:03:25.223 --> 00:03:27.777
and they combine in many different ways.

00:03:27.777 --> 00:03:31.515
So take, for example,
the smile and the smirk.

00:03:31.515 --> 00:03:35.268
They look somewhat similar,
but they mean very different things.

00:03:35.268 --> 00:03:36.986
(Laughter)

00:03:36.986 --> 00:03:39.990
So the smile is positive,

00:03:39.990 --> 00:03:41.260
a smirk is often negative.

00:03:41.260 --> 00:03:45.136
Sometimes a smirk
can make you become famous.

00:03:45.136 --> 00:03:47.960
But seriously, it's important
for a computer to be able

00:03:47.960 --> 00:03:50.815
to tell the difference
between the two expressions.

00:03:50.815 --> 00:03:52.627
So how do we do that?

00:03:52.627 --> 00:03:54.414
We give our algorithms

00:03:54.414 --> 00:03:58.524
tens of thousands of examples
of people we know to be smiling,

00:03:58.524 --> 00:04:01.589
from different ethnicities, ages, genders,

00:04:01.589 --> 00:04:04.400
and we do the same for smirks.

00:04:04.400 --> 00:04:05.954
And then, using deep learning,

00:04:05.954 --> 00:04:08.810
the algorithm looks for all these
textures and wrinkles

00:04:08.810 --> 00:04:11.390
and shape changes on our face,

00:04:11.390 --> 00:04:14.592
and basically learns that all smiles
have common characteristics,

00:04:14.592 --> 00:04:17.773
all smirks have subtly
different characteristics.

00:04:17.773 --> 00:04:20.141
And the next time it sees a new face,

00:04:20.141 --> 00:04:22.440
it essentially learns that

00:04:22.440 --> 00:04:25.473
this face has the same
characteristics of a smile,

00:04:25.473 --> 00:04:29.751
and it says, "Aha, I recognize this.
This is a smile expression."

00:04:30.381 --> 00:04:33.181
So the best way to demonstrate
how this technology works

00:04:33.181 --> 00:04:35.317
is to try a live demo,

00:04:35.317 --> 00:04:39.230
so I need a volunteer,
preferably somebody with a face.

00:04:39.230 --> 00:04:41.564
(Laughter)

00:04:41.564 --> 00:04:44.335
Cloe's going to be our volunteer today.

00:04:45.325 --> 00:04:49.783
So over the past five years, we've moved
from being a research project at MIT

00:04:49.783 --> 00:04:50.939
to a company,

00:04:50.939 --> 00:04:54.131
where my team has worked really hard
to make this technology work,

00:04:54.131 --> 00:04:56.540
as we like to say, in the wild.

00:04:56.540 --> 00:04:59.210
And we've also shrunk it so that
the core emotion engine

00:04:59.210 --> 00:05:02.530
works on any mobile device
with a camera, like this iPad.

00:05:02.530 --> 00:05:05.316
So let's give this a try.

00:05:06.756 --> 00:05:10.680
As you can see, the algorithm
has essentially found Cloe's face,

00:05:10.680 --> 00:05:12.372
so it's this white bounding box,

00:05:12.372 --> 00:05:14.943
and it's tracking the main
feature points on her face,

00:05:14.943 --> 00:05:17.799
so her eyebrows, her eyes,
her mouth and her nose.

00:05:17.799 --> 00:05:20.786
The question is,
can it recognize her expression?

00:05:20.786 --> 00:05:22.457
So we're going to test the machine.

00:05:22.457 --> 00:05:26.643
So first of all, give me your poker face.
Yep, awesome. (Laughter)

00:05:26.643 --> 00:05:29.456
And then as she smiles,
this is a genuine smile, it's great.

00:05:29.456 --> 00:05:31.756
So you can see the green bar
go up as she smiles.

00:05:31.756 --> 00:05:32.978
Now that was a big smile.

00:05:32.978 --> 00:05:36.021
Can you try a subtle smile
to see if the computer can recognize?

00:05:36.021 --> 00:05:38.352
It does recognize subtle smiles as well.

00:05:38.352 --> 00:05:40.477
We've worked really hard
to make that happen.

00:05:40.477 --> 00:05:43.439
And then eyebrow raised,
indicator of surprise.

00:05:43.439 --> 00:05:47.688
Brow furrow, which is
an indicator of confusion.

00:05:47.688 --> 00:05:51.695
Frown. Yes, perfect.

00:05:51.695 --> 00:05:55.188
So these are all the different
action units. There's many more of them.

00:05:55.188 --> 00:05:57.220
This is just a slimmed-down demo.

00:05:57.220 --> 00:06:00.368
But we call each reading
an emotion data point,

00:06:00.368 --> 00:06:03.337
and then they can fire together
to portray different emotions.

00:06:03.337 --> 00:06:07.990
So on the right side of the demo --
look like you're happy.

00:06:07.990 --> 00:06:09.444
So that's joy. Joy fires up.

00:06:09.444 --> 00:06:11.371
And then give me a disgust face.

00:06:11.371 --> 00:06:15.643
Try to remember what it was like
when Zayn left One Direction.

00:06:15.643 --> 00:06:17.153
(Laughter)

00:06:17.153 --> 00:06:21.495
Yeah, wrinkle your nose. Awesome.

00:06:21.495 --> 00:06:25.226
And the valence is actually quite
negative, so you must have been a big fan.

00:06:25.226 --> 00:06:27.926
So valence is how positive
or negative an experience is,

00:06:27.926 --> 00:06:30.712
and engagement is how
expressive she is as well.

00:06:30.712 --> 00:06:34.126
So imagine if Cloe had access
to this real-time emotion stream,

00:06:34.126 --> 00:06:36.935
and she could share it
with anybody she wanted to.

00:06:36.935 --> 00:06:39.858
Thank you.

00:06:39.858 --> 00:06:44.479
(Applause)

00:06:45.749 --> 00:06:51.019
So, so far, we have amassed
12 billion of these emotion data points.

00:06:51.019 --> 00:06:53.630
It's the largest emotion
database in the world.

00:06:53.630 --> 00:06:56.593
We've collected it
from 2.9 million face videos,

00:06:56.593 --> 00:06:59.193
people who have agreed
to share their emotions with us,

00:06:59.193 --> 00:07:02.398
and from 75 countries around the world.

00:07:02.398 --> 00:07:04.113
It's growing every day.

00:07:04.603 --> 00:07:06.670
It blows my mind away

00:07:06.670 --> 00:07:09.865
that we can now quantify something
as personal as our emotions,

00:07:09.865 --> 00:07:12.100
and we can do it at this scale.

00:07:12.100 --> 00:07:14.277
So what have we learned to date?

00:07:15.057 --> 00:07:17.388
Gender.

00:07:17.388 --> 00:07:21.034
Our data confirms something
that you might suspect.

00:07:21.034 --> 00:07:22.891
Women are more expressive than men.

00:07:22.891 --> 00:07:25.574
Not only do they smile more,
their smiles last longer,

00:07:25.574 --> 00:07:28.478
and we can now really quantify
what it is that men and women

00:07:28.478 --> 00:07:30.614
respond to differently.

00:07:30.614 --> 00:07:32.904
Let's do culture: So in the United States,

00:07:32.904 --> 00:07:36.108
women are 40 percent
more expressive than men,

00:07:36.108 --> 00:07:39.753
but curiously, we don't see any difference
in the U.K. between men and women.

00:07:39.753 --> 00:07:42.259
(Laughter)

00:07:43.296 --> 00:07:47.323
Age: People who are 50 years and older

00:07:47.323 --> 00:07:50.759
are 25 percent more emotive
than younger people.

00:07:51.899 --> 00:07:55.751
Women in their 20s smile a lot more
than men the same age,

00:07:55.751 --> 00:07:59.590
perhaps a necessity for dating.

00:07:59.590 --> 00:08:02.207
But perhaps what surprised us
the most about this data

00:08:02.207 --> 00:08:05.410
is that we happen
to be expressive all the time,

00:08:05.410 --> 00:08:08.243
even when we are sitting
in front of our devices alone,

00:08:08.243 --> 00:08:11.517
and it's not just when we're watching
cat videos on Facebook.

00:08:12.217 --> 00:08:15.227
We are expressive when we're emailing,
texting, shopping online,

00:08:15.227 --> 00:08:17.527
or even doing our taxes.

00:08:17.527 --> 00:08:19.919
Where is this data used today?

00:08:19.919 --> 00:08:22.682
In understanding how we engage with media,

00:08:22.682 --> 00:08:25.166
so understanding virality
and voting behavior;

00:08:25.166 --> 00:08:27.906
and also empowering
or emotion-enabling technology,

00:08:27.906 --> 00:08:32.527
and I want to share some examples
that are especially close to my heart.

00:08:33.197 --> 00:08:36.265
Emotion-enabled wearable glasses
can help individuals

00:08:36.265 --> 00:08:39.493
who are visually impaired
read the faces of others,

00:08:39.493 --> 00:08:43.680
and it can help individuals
on the autism spectrum interpret emotion,

00:08:43.680 --> 00:08:46.458
something that they really struggle with.

00:08:47.918 --> 00:08:50.777
In education, imagine
if your learning apps

00:08:50.777 --> 00:08:53.587
sense that you're confused and slow down,

00:08:53.587 --> 00:08:55.444
or that you're bored, so it's sped up,

00:08:55.444 --> 00:08:58.413
just like a great teacher
would in a classroom.

00:08:59.043 --> 00:09:01.644
What if your wristwatch tracked your mood,

00:09:01.644 --> 00:09:04.337
or your car sensed that you're tired,

00:09:04.337 --> 00:09:06.885
or perhaps your fridge
knows that you're stressed,

00:09:06.885 --> 00:09:12.951
so it auto-locks to prevent you
from binge eating. (Laughter)

00:09:12.951 --> 00:09:15.668
I would like that, yeah.

00:09:15.668 --> 00:09:17.595
What if, when I was in Cambridge,

00:09:17.595 --> 00:09:19.908
I had access to my real-time
emotion stream,

00:09:19.908 --> 00:09:23.437
and I could share that with my family
back home in a very natural way,

00:09:23.437 --> 00:09:27.408
just like I would've if we were all
in the same room together?

00:09:27.408 --> 00:09:30.550
I think five years down the line,

00:09:30.550 --> 00:09:32.887
all our devices are going
to have an emotion chip,

00:09:32.887 --> 00:09:36.951
and we won't remember what it was like
when we couldn't just frown at our device

00:09:36.951 --> 00:09:41.200
and our device would say, "Hmm,
you didn't like that, did you?"

00:09:41.200 --> 00:09:44.961
Our biggest challenge is that there are
so many applications of this technology,

00:09:44.961 --> 00:09:47.864
my team and I realize that we can't
build them all ourselves,

00:09:47.864 --> 00:09:51.360
so we've made this technology available
so that other developers

00:09:51.360 --> 00:09:53.474
can get building and get creative.

00:09:53.474 --> 00:09:57.560
We recognize that
there are potential risks

00:09:57.560 --> 00:09:59.627
and potential for abuse,

00:09:59.627 --> 00:10:02.576
but personally, having spent
many years doing this,

00:10:02.576 --> 00:10:05.548
I believe that the benefits to humanity

00:10:05.548 --> 00:10:07.823
from having emotionally
intelligent technology

00:10:07.823 --> 00:10:11.399
far outweigh the potential for misuse.

00:10:11.399 --> 00:10:13.930
And I invite you all to be
part of the conversation.

00:10:13.930 --> 00:10:16.484
The more people who know
about this technology,

00:10:16.484 --> 00:10:19.661
the more we can all have a voice
in how it's being used.

00:10:21.081 --> 00:10:25.655
So as more and more
of our lives become digital,

00:10:25.655 --> 00:10:29.153
we are fighting a losing battle
trying to curb our usage of devices

00:10:29.153 --> 00:10:31.382
in order to reclaim our emotions.

00:10:32.622 --> 00:10:36.536
So what I'm trying to do instead
is to bring emotions into our technology

00:10:36.536 --> 00:10:38.765
and make our technologies more responsive.

00:10:38.765 --> 00:10:41.435
So I want those devices
that have separated us

00:10:41.435 --> 00:10:43.897
to bring us back together.

00:10:43.897 --> 00:10:48.485
And by humanizing technology,
we have this golden opportunity

00:10:48.485 --> 00:10:51.782
to reimagine how we
connect with machines,

00:10:51.782 --> 00:10:56.263
and therefore, how we, as human beings,

00:10:56.263 --> 00:10:58.167
connect with one another.

00:10:58.167 --> 00:11:00.327
Thank you.

00:11:00.327 --> 00:11:03.640
(Applause)

