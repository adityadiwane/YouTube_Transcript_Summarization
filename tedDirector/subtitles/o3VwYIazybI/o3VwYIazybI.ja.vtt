WEBVTT
Kind: captions
Language: ja

00:00:00.000 --> 00:00:07.000
翻訳: Moe Shoji
校正: Tamami Inoue

00:00:12.556 --> 00:00:16.573
私たちの感情は
生活のあらゆる面に影響します

00:00:16.573 --> 00:00:20.148
健康や学び方
ビジネスのやり方や決定の仕方まで

00:00:20.148 --> 00:00:21.922
大小様々です

00:00:22.672 --> 00:00:26.162
感情は私たちがお互いと
結びつく方法にも影響します

00:00:27.132 --> 00:00:31.108
今まで私たちは
このような世界に順応してきましたが

00:00:31.108 --> 00:00:35.427
現在の世界は どんどん
このようになってきています

00:00:35.427 --> 00:00:38.561
これは昨晩 娘から届いた
携帯メールです

00:00:38.561 --> 00:00:41.301
感情のない世界ですね

00:00:41.301 --> 00:00:43.252
私はそれを変えるべく活動しています

00:00:43.252 --> 00:00:47.343
デジタルでの体験に
感情を取り戻したいのです

00:00:48.223 --> 00:00:51.300
この道を歩み始めたのは15年前―

00:00:51.300 --> 00:00:53.366
エジプトでコンピューターサイエンスを
学んでいたときのことです

00:00:53.366 --> 00:00:57.871
ケンブリッジ大学の博士課程に
合格したところでした

00:00:57.871 --> 00:00:59.984
私が取った行動は

00:00:59.984 --> 00:01:04.209
結婚したての若いイスラム教徒の
エジプト人妻としてはかなり珍しいものでした

00:01:05.599 --> 00:01:08.598
エジプトに残らねばならない
夫のサポートを得て

00:01:08.598 --> 00:01:11.616
私は荷物をまとめて
イングランドにやってきたのです

00:01:11.616 --> 00:01:14.844
故郷から何千キロも離れた
ケンブリッジで

00:01:14.844 --> 00:01:18.257
私は自分が他の人と過ごすよりも
長い時間を

00:01:18.257 --> 00:01:20.486
ノートパソコンと過ごしていることに
気づきました

00:01:20.486 --> 00:01:25.339
とても近しいものでありながら
ノートパソコンに私の感情は分かりません

00:01:25.339 --> 00:01:28.550
私が嬉しかろうと

00:01:28.550 --> 00:01:31.538
ストレスを抱えて混乱し
嫌な１日を過ごそうと伝わらないので

00:01:31.538 --> 00:01:34.460
歯がゆい気持ちになりました

00:01:35.600 --> 00:01:40.831
さらに悪いことに オンラインで
家族と会話をしていても

00:01:41.421 --> 00:01:44.703
感情がすべてサイバー空間に
消えてしまうような気がしました

00:01:44.703 --> 00:01:49.858
ホームシックで寂しい思いをし
泣いたときもありましたが

00:01:49.858 --> 00:01:54.786
感情を表す手段はこれだけでした

00:01:54.786 --> 00:01:56.806
（笑）

00:01:56.806 --> 00:02:01.780
現在のテクノロジーには
IQはあってもEQはありません

00:02:01.780 --> 00:02:04.956
知能指数は高くても
心の知能指数はないのです

00:02:04.956 --> 00:02:07.153
そこで私は考えました

00:02:07.153 --> 00:02:10.777
テクノロジーが感情を
感じ取れるとしたらどうだろう？

00:02:10.777 --> 00:02:14.853
心の知能指数を持った友人のように
電子機器が感情を読み取って

00:02:14.853 --> 00:02:17.866
それに応じて反応したらどうだろう？

00:02:18.666 --> 00:02:22.229
これらの問いによって
私とチームは

00:02:22.229 --> 00:02:26.607
感情を読み取り 対応できる
テクノロジーを開発するに至り

00:02:26.607 --> 00:02:29.697
私たちの出発点は人間の顔でした

00:02:30.577 --> 00:02:33.750
人間の顔は
社会的・感情的状態を伝えるのに

00:02:33.750 --> 00:02:37.766
誰もが用いている
非常に強力な手段のひとつです

00:02:37.766 --> 00:02:40.776
喜びや驚き―

00:02:40.776 --> 00:02:44.979
共感や好奇心まで様々あります

00:02:44.979 --> 00:02:49.907
感情科学では顔面筋肉の動き
それぞれをアクション・ユニットと言います

00:02:49.907 --> 00:02:52.832
例えば アクション・ユニット12は

00:02:52.832 --> 00:02:54.870
ハリウッド映画のタイトルではありませんよ

00:02:54.870 --> 00:02:58.312
これは口の端を引く動きで
笑顔の主な要素です

00:02:58.312 --> 00:03:01.300
試してみて下さい
笑顔になりますね

00:03:01.300 --> 00:03:03.954
もうひとつの例はアクション・ユニット４で
これは眉をひそめる動きです

00:03:03.954 --> 00:03:06.192
両眉をひそめると

00:03:06.192 --> 00:03:08.459
凹凸やしわができますね

00:03:08.459 --> 00:03:12.754
好まれるものではありませんが
否定的な感情を強く示すサインです

00:03:12.754 --> 00:03:14.960
こうしたアクション・ユニットが45個あり

00:03:14.960 --> 00:03:18.350
これらが組み合わさって
何百もの感情を表します

00:03:18.350 --> 00:03:22.251
こうした顔の表情をコンピューターに
読み取らせるのは困難です

00:03:22.251 --> 00:03:25.223
アクション・ユニットは動きが素早く
加減が微妙ですし

00:03:25.223 --> 00:03:27.777
様々に組み合わせられるためです

00:03:27.777 --> 00:03:31.515
例えば 笑顔と
ぎこちない作り笑いです

00:03:31.515 --> 00:03:35.268
どこかしら似てはいますが
意味合いは随分違います

00:03:35.268 --> 00:03:36.986
（笑）

00:03:36.986 --> 00:03:39.990
笑顔はポジティブで

00:03:39.990 --> 00:03:41.260
わざと作った笑顔は
ネガティブなことが多いです

00:03:41.260 --> 00:03:45.136
作り笑いで有名になることもあります

00:03:45.136 --> 00:03:47.960
ですが真面目な話
コンピューターが

00:03:47.960 --> 00:03:50.815
２つの表情の違いを
読み取れることが重要です

00:03:50.815 --> 00:03:52.627
では どうすればいいでしょう？

00:03:52.627 --> 00:03:54.414
私達の開発したアルゴリズムに

00:03:54.414 --> 00:03:58.524
純粋な笑顔を見せている人々の例を
何万例も与えます

00:03:58.524 --> 00:04:01.589
人種 年齢 性別も様々です

00:04:01.589 --> 00:04:04.400
そして作り笑いにも
同様のことを行います

00:04:04.400 --> 00:04:05.954
するとディープラーニング（深層学習）で

00:04:05.954 --> 00:04:08.810
アルゴリズムが
顔面に起こる凹凸やしわや

00:04:08.810 --> 00:04:11.390
形状の変化を探し

00:04:11.390 --> 00:04:14.592
笑顔には共通の特徴があり

00:04:14.592 --> 00:04:17.773
作り笑いには微妙に異なる
特徴があることを学習します

00:04:17.773 --> 00:04:20.141
以降は 未知の顔に遭遇しても

00:04:20.141 --> 00:04:22.440
基本的に この顔が

00:04:22.440 --> 00:04:25.473
笑顔の特徴を
備えていることを認識し

00:04:25.473 --> 00:04:29.750
「あぁわかった　これは笑顔ですね」
と言うのです

00:04:30.380 --> 00:04:33.180
このテクノロジーの働きを
お見せする一番の方法は

00:04:33.180 --> 00:04:35.317
ここで実演することです

00:04:35.317 --> 00:04:39.230
誰か手伝ってもらえませんか
顔がある人がいいのですが

00:04:39.230 --> 00:04:41.564
（笑）

00:04:41.564 --> 00:04:44.335
クロイに手伝ってもらいましょう

00:04:45.325 --> 00:04:49.783
ここ５年間で私たちはMITでの
研究プロジェクトから

00:04:49.783 --> 00:04:50.939
企業へと変化しました

00:04:50.939 --> 00:04:54.131
私のチームはこのテクノロジーが
いわば「外の世界で」

00:04:54.131 --> 00:04:56.540
機能するよう努力してきました

00:04:56.540 --> 00:04:59.210
またコンパクトにすることで
感情エンジンの核となる部分が

00:04:59.210 --> 00:05:02.530
iPadのようなカメラ付き携帯機器で
使えるようにしました

00:05:02.530 --> 00:05:05.316
では やってみましょう

00:05:06.756 --> 00:05:10.680
ご覧のように アルゴリズムが
クロイの顔を認識しました

00:05:10.680 --> 00:05:12.372
白い四角で示されており

00:05:12.372 --> 00:05:14.943
顔の主な部分を追跡しています

00:05:14.943 --> 00:05:17.799
眉、目、口や鼻などです

00:05:17.799 --> 00:05:20.786
さあ 表情を読み取れるでしょうか？

00:05:20.786 --> 00:05:22.457
機械を試してみましょう

00:05:22.457 --> 00:05:26.643
まず ポーカーフェイスを見せて下さい
いいですね （笑）

00:05:26.643 --> 00:05:29.456
それから笑顔です
これは心からの笑顔ですね

00:05:29.456 --> 00:05:31.756
彼女が笑うと
緑色のバーが上がるのが分かります

00:05:31.756 --> 00:05:32.978
満面の笑みでしたね

00:05:32.978 --> 00:05:36.021
コンピューターが認識できるか
微笑みを作ってみましょう

00:05:36.021 --> 00:05:38.352
微笑みでも認識できますね

00:05:38.352 --> 00:05:40.477
これには大変苦労しました

00:05:40.477 --> 00:05:43.439
眉が上がると
驚きの表情のサインです

00:05:43.439 --> 00:05:47.688
眉をひそめると
困惑を示すサインです

00:05:47.688 --> 00:05:51.695
眉をひそめて
完璧です

00:05:51.695 --> 00:05:55.188
これらは全て異なるアクション・ユニットで
まだ他にも多くありますが

00:05:55.188 --> 00:05:57.220
ほんの一部だけお見せしました

00:05:57.220 --> 00:06:00.368
それぞれの感情のデータポイントを
読み取らせることができ

00:06:00.368 --> 00:06:03.337
それらが合わさって
様々な感情を表現することができます

00:06:03.337 --> 00:06:07.990
デモの右側には―
喜びの表情を見せて

00:06:07.990 --> 00:06:09.444
これが喜びですね
「喜び」の項目が上がりました

00:06:09.444 --> 00:06:11.371
それから 嫌悪の表情を見せて下さい

00:06:11.371 --> 00:06:15.643
ゼインがワン・ダイレクションを
脱退したのを思い出して

00:06:15.643 --> 00:06:17.153
（笑）

00:06:17.153 --> 00:06:21.495
鼻にもしわが寄ります
いいですね

00:06:21.495 --> 00:06:25.226
感情価がかなりネガティブでしたから
熱心なファンだったのでしょう

00:06:25.226 --> 00:06:27.926
「感情価」は経験がいかにポジティブ
またはネガティブであったか

00:06:27.926 --> 00:06:30.712
そして「関与度」は
いかに強く表現したかです

00:06:30.712 --> 00:06:34.126
クロイがこのリアルタイムでの
感情ストリームを利用でき

00:06:34.126 --> 00:06:36.935
それを他の人と共有できると
想像してみて下さい

00:06:36.935 --> 00:06:39.858
どうもありがとう

00:06:39.858 --> 00:06:44.479
（拍手）

00:06:45.749 --> 00:06:51.019
これまでに120億の
感情データポイントを収集しました

00:06:51.019 --> 00:06:53.630
世界最大の感情データベースです

00:06:53.630 --> 00:06:56.593
290万の顔の写ったビデオから
集めましたが

00:06:56.593 --> 00:06:59.193
感情を共有することに
同意した人たち―

00:06:59.193 --> 00:07:02.398
世界75か国から集まりました

00:07:02.398 --> 00:07:04.113
これは日々 増えています

00:07:04.603 --> 00:07:06.670
感情というごく個人的なものを

00:07:06.670 --> 00:07:09.865
今やこれほどの規模で
数量化できるなんて

00:07:09.865 --> 00:07:12.100
私の想像を超えています

00:07:12.100 --> 00:07:14.277
では 今日までにわかったことは
何でしょうか？

00:07:15.057 --> 00:07:17.388
ジェンダーです

00:07:17.388 --> 00:07:21.034
皆さんが薄々気づいていることを
データが立証しました

00:07:21.034 --> 00:07:22.891
女性の方が男性より表情豊かです

00:07:22.891 --> 00:07:25.574
より頻繁に笑顔になるだけでなく
笑顔が持続します

00:07:25.574 --> 00:07:28.478
今や男性と女性が
異なる反応を見せるものについて

00:07:28.478 --> 00:07:30.614
数量化することができます

00:07:30.614 --> 00:07:32.904
文化の違いではどうでしょう
アメリカでは―

00:07:32.904 --> 00:07:36.108
女性は男性より40％表情豊かですが

00:07:36.108 --> 00:07:39.753
面白いことにイギリスでは
男女の差異はありません

00:07:39.753 --> 00:07:42.259
（笑）

00:07:43.296 --> 00:07:47.323
年齢です
50歳以上の人々は

00:07:47.323 --> 00:07:50.759
若者よりも25％さらに感情的です

00:07:51.899 --> 00:07:55.751
20代の女性は同じ年代の男性よりも
ずっと頻繁に笑顔になり

00:07:55.751 --> 00:07:59.590
これはデートで
必要となるのかもしれません

00:07:59.590 --> 00:08:02.207
このデータで最も驚かされるのは

00:08:02.207 --> 00:08:05.410
いかに私たちが常に
表情豊かであるかと言うことです

00:08:05.410 --> 00:08:08.243
電子機器の前に
独りで座っているときでもです

00:08:08.243 --> 00:08:11.517
Facebookで猫のビデオを
見ているときだけではありません

00:08:12.217 --> 00:08:15.227
メールや携帯メールを書くときや
オンラインショッピング―

00:08:15.227 --> 00:08:17.527
確定申告の準備中でも
表情豊かなのです

00:08:17.527 --> 00:08:19.919
このデータは今
どこで使われているのでしょう？

00:08:19.919 --> 00:08:22.682
メディアとの関わり方の研究―

00:08:22.682 --> 00:08:25.166
つまりバイラリティーや
投票行動などの理解や

00:08:25.166 --> 00:08:27.906
また 能力を高めたり
感情を利用するテクノロジーにも使えます

00:08:27.906 --> 00:08:32.527
私が特に大切に思う例を
お話ししたいと思います

00:08:33.197 --> 00:08:36.265
感情を理解する
眼鏡型端末があれば

00:08:36.265 --> 00:08:39.493
視覚障害者が
他者の表情を読むのに役立ちます

00:08:39.493 --> 00:08:43.680
自閉症のスペクトラムの人が
非常に苦労している―

00:08:43.680 --> 00:08:46.457
感情の理解にも役立ちます

00:08:47.918 --> 00:08:50.777
教育においては学習アプリが

00:08:50.777 --> 00:08:53.587
生徒の困惑や退屈の表情に気付き

00:08:53.587 --> 00:08:55.443
教室で熟練の教師がするように

00:08:55.443 --> 00:08:58.413
学習のペースを
調整することができるでしょう

00:08:59.043 --> 00:09:01.644
腕時計があなたの気分をモニターしたり

00:09:01.644 --> 00:09:04.337
車があなたの疲労度合いを
感知できたら―

00:09:04.337 --> 00:09:06.885
あるいは冷蔵庫が
ストレスを感知して

00:09:06.885 --> 00:09:12.951
ばか食いを防ぐために
自動ロックしたらどうでしょうか （笑）

00:09:12.951 --> 00:09:15.668
私は良いと思いますね

00:09:15.668 --> 00:09:17.595
私がケンブリッジにいた頃

00:09:17.595 --> 00:09:19.908
リアルタイムの
感情ストリームを利用して

00:09:19.908 --> 00:09:23.437
故郷にいる家族と感情を
ごく自然に―

00:09:23.437 --> 00:09:27.408
まるで同じ部屋にいるように
共有できたらどうだったでしょう？

00:09:27.408 --> 00:09:30.550
今から５年もすれば

00:09:30.550 --> 00:09:32.887
電子機器はすべて
感情チップを搭載し

00:09:32.887 --> 00:09:36.951
電子機器に向かって 眉をひそめれば
「気に入らなかったんだね」と

00:09:36.951 --> 00:09:41.200
言ってくれなかった昔のことなど
忘れてしまうでしょう

00:09:41.200 --> 00:09:44.961
最大の難関はこのテクノロジーには
非常に多くの使い道があり

00:09:44.961 --> 00:09:47.864
私たちのチームだけでは
全てを開発できないと気づいたことです

00:09:47.864 --> 00:09:51.360
そこで このテクノロジーを利用可能にして
他の開発者たちが

00:09:51.360 --> 00:09:53.474
独自に開発を進められるようにしました

00:09:53.474 --> 00:09:57.560
私たちはリスクの可能性も
認識しています

00:09:57.560 --> 00:09:59.627
濫用されるリスクです

00:09:59.627 --> 00:10:02.576
でも個人的には
何年も携わってきて

00:10:02.576 --> 00:10:05.548
心の知能指数の高い
テクノロジーによって

00:10:05.548 --> 00:10:07.823
人類にもたらされる利益の方が

00:10:07.823 --> 00:10:11.399
濫用のリスクよりも
ずっと多いと考えています

00:10:11.399 --> 00:10:13.930
皆さんにも対話に参加をお願いします

00:10:13.930 --> 00:10:16.484
このテクノロジーを
知っている人が多ければ多いほど

00:10:16.484 --> 00:10:19.661
使用法に関する意見を
多く得ることができます

00:10:21.081 --> 00:10:25.655
私たちの生活が
さらにデジタルなものになるにつれ

00:10:25.655 --> 00:10:29.153
私たちは感情を再び手にするために
電子機器の使用を控えようという

00:10:29.153 --> 00:10:31.382
負けが明らかな闘いに挑んでいます

00:10:32.622 --> 00:10:36.536
その代わりに 私が試みているのは
テクノロジーに感情を導入すること

00:10:36.536 --> 00:10:38.765
そしてテクノロジーが
反応するものにすることです

00:10:38.765 --> 00:10:41.435
私たちを引き離した電子機器によって

00:10:41.435 --> 00:10:43.897
再び人々を
結びつけようとしているのです

00:10:43.897 --> 00:10:48.485
テクノロジーに人間性を付与することで
私たちには

00:10:48.485 --> 00:10:51.782
機械との関わりを見つめ直す
絶好の機会が与えられています

00:10:51.782 --> 00:10:56.263
つまり人間として 私たちが

00:10:56.263 --> 00:10:58.167
いかにお互いとつながり合うかを
見つめ直す機会なのです

00:10:58.167 --> 00:11:00.327
ありがとうございました

00:11:00.327 --> 00:11:03.640
（拍手）

