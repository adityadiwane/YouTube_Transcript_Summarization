WEBVTT
Kind: captions
Language: pt-PT

00:00:00.000 --> 00:00:07.000
Tradutor: Marina Cheffe
Revisora: Margarida Ferreira

00:00:13.116 --> 00:00:16.463
As nossas emoções influenciam
todos os aspetos da nossa vida,

00:00:16.463 --> 00:00:18.609
desde a nossa saúde,
a forma como aprendemos

00:00:18.609 --> 00:00:20.638
até como trabalhamos 
e tomamos decisões,

00:00:20.638 --> 00:00:22.458
sejam elas pequenas ou grandes.

00:00:22.867 --> 00:00:27.380
As nossas emoções também influenciam o modo
como nos relacionamos uns com os outros.

00:00:27.356 --> 00:00:30.377
Nós evoluímos para viver
num mundo como este.

00:00:31.196 --> 00:00:35.652
Mas, em vez disso, vivemos
cada vez mais desta forma.

00:00:35.652 --> 00:00:38.349
— esta foi a mensagem 
da minha filha ontem à noite —

00:00:38.921 --> 00:00:41.271
num mundo desprovido de emoção.

00:00:41.271 --> 00:00:43.718
Eu estou numa missão para mudar isto.

00:00:43.718 --> 00:00:46.919
Quero fazer voltar as emoções
às nossas experiências digitais.

00:00:48.512 --> 00:00:50.990
Comecei neste caminho há 15 anos.

00:00:50.990 --> 00:00:53.830
Eu era cientista de informática no Egito,

00:00:53.830 --> 00:00:56.527
e tinha sido aceite 
num programa de doutoramento

00:00:56.527 --> 00:00:58.245
na Universidade de Cambridge.

00:00:58.245 --> 00:01:00.480
Então, fiz uma coisa bastante invulgar

00:01:00.480 --> 00:01:03.903
para uma jovem esposa 
egípcia e muçulmana.

00:01:05.929 --> 00:01:08.664
Com o apoio do meu marido, 
que teve que ficar no Egito,

00:01:08.664 --> 00:01:11.332
fiz as malas e fui para Inglaterra.

00:01:11.723 --> 00:01:14.924
Em Cambridge, 
a milhares de quilómetros de casa,

00:01:14.924 --> 00:01:17.904
percebi que estava a passar
mais tempo com o meu portátil

00:01:17.904 --> 00:01:20.266
do que com qualquer ser humano.

00:01:20.716 --> 00:01:22.334
Mas, apesar da intimidade,

00:01:22.334 --> 00:01:25.470
o meu portátil não fazia ideia nenhuma
de como eu me sentia.

00:01:25.470 --> 00:01:27.917
Não sabia se eu estava feliz,

00:01:28.047 --> 00:01:31.721
se estava a ter um dia mau, 
se estava tensa ou confusa,

00:01:31.721 --> 00:01:33.847
e isso tornou-se frustrante.

00:01:35.659 --> 00:01:39.640
Pior ainda, quando eu comunicava 
com a minha família online,

00:01:41.309 --> 00:01:44.450
eu sentia que as minhas emoções
desapareciam no ciberespaço.

00:01:44.718 --> 00:01:49.890
Eu estava com saudades de casa, solitária,
e, às vezes, até chorava,

00:01:49.890 --> 00:01:54.666
mas tudo o que eu tinha para comunicar
essas emoções era isto.

00:01:55.210 --> 00:01:56.894
(Risos)

00:01:57.130 --> 00:02:01.036
A tecnologia atual tem um alto Q.I,
mas nenhum Q.E.,

00:02:01.036 --> 00:02:05.450
ou seja, uma alta inteligência cognitiva,
mas nenhuma inteligência emocional.

00:02:05.086 --> 00:02:07.590
Então comecei a pensar:

00:02:07.590 --> 00:02:11.058
E se a nossa tecnologia pudesse
reconhecer as nossas emoções?

00:02:11.058 --> 00:02:13.495
E se ela pudesse reconhecer 
os nossos sentimentos

00:02:13.495 --> 00:02:15.226
e reagir de acordo com eles,

00:02:15.226 --> 00:02:18.247
do mesmo modo que um amigo
emocionalmente inteligente reagiria?

00:02:18.742 --> 00:02:22.274
Estas perguntas fizeram 
com que eu e a minha equipa

00:02:22.274 --> 00:02:25.992
criássemos tecnologias capazes 
de ler e de reagir às nossas emoções.

00:02:26.566 --> 00:02:29.411
O nosso ponto de partida
foi o rosto humano.

00:02:30.560 --> 00:02:33.959
O rosto humano é um 
dos mais poderosos canais

00:02:33.959 --> 00:02:37.741
que utilizamos para comunicar
os estados sociais e emocionais,

00:02:37.741 --> 00:02:39.904
como a alegria,

00:02:39.904 --> 00:02:41.462
a surpresa,

00:02:41.462 --> 00:02:43.162
a empatia

00:02:43.162 --> 00:02:44.946
e a curiosidade.

00:02:45.255 --> 00:02:48.202
Na ciência da emoção, 
chamamos "unidade de ação"

00:02:48.202 --> 00:02:50.247
a cada movimento facial muscular ,

00:02:50.247 --> 00:02:52.515
Por exemplo, ''unidade de ação 12'',

00:02:52.515 --> 00:02:54.811
— não é um grande êxito de Hollywood —

00:02:54.811 --> 00:02:58.303
é o levantar do canto do lábio,
o principal componente de um sorriso.

00:02:58.303 --> 00:03:00.654
Tentem, todos vocês. 
Vamos ver alguns sorrisos.

00:03:01.116 --> 00:03:04.197
Outro exemplo é a "unidade de ação 4".
É franzir a testa.

00:03:04.197 --> 00:03:07.934
É quando aproximamos as sobrancelhas,
e criamos várias texturas e rugas.

00:03:07.934 --> 00:03:12.117
Não gostamos delas, mas é 
um forte indicador de emoções negativas.

00:03:12.117 --> 00:03:14.775
Temos cerca de 45 unidades de ação,

00:03:14.775 --> 00:03:17.852
que se combinam para exprimir
centenas de emoções.

00:03:18.345 --> 00:03:21.972
Ensinar um computador a ler 
essas emoções faciais é difícil

00:03:21.972 --> 00:03:25.283
porque essas unidades de ação
podem ser rápidas, podem ser subtis,

00:03:25.283 --> 00:03:27.797
e combinam-se de muita formas diferentes.

00:03:27.797 --> 00:03:30.774
Por exemplo, o sorriso alegre
e o sorriso irónico.

00:03:31.743 --> 00:03:35.441
São parecidos, mas 
significam coisas muito diferentes.

00:03:35.441 --> 00:03:36.905
(Risos)

00:03:37.232 --> 00:03:39.142
O sorriso alegre é positivo,

00:03:39.142 --> 00:03:41.229
o sorriso irónico, geralmente, é negativo.

00:03:41.229 --> 00:03:44.120
Às vezes o sorriso irónico
pode tornar alguém famoso.

00:03:45.140 --> 00:03:50.387
É importante que um computador
consiga distinguir essas duas expressões.

00:03:50.387 --> 00:03:52.270
Então como fazemos isso?

00:03:52.346 --> 00:03:54.466
Damos aos nossos algoritmos

00:03:54.466 --> 00:03:58.614
dezenas de milhares de exemplos
de pessoas que sorriem alegremente,

00:03:58.669 --> 00:04:01.738
pessoas de diferentes etnias,
idades, sexos,

00:04:01.738 --> 00:04:04.320
e fazemos o mesmo com o sorriso irónico.

00:04:04.132 --> 00:04:06.148
Usando a aprendizagem profunda,

00:04:06.148 --> 00:04:08.980
o algoritmo procura todas
essas texturas e rugas

00:04:09.000 --> 00:04:11.264
e as mudanças de forma no nosso rosto,

00:04:11.264 --> 00:04:14.532
e aprende que todos os sorrisos alegres
têm características em comum

00:04:14.532 --> 00:04:17.926
e todos os sorrisos irónicos têm
características subtilmente diferentes.

00:04:17.926 --> 00:04:20.119
E quando voltar a ver um novo rosto,

00:04:20.119 --> 00:04:23.282
ele aprende que esse rosto

00:04:23.282 --> 00:04:26.259
tem as mesmas características
de um sorriso alegre e diz:

00:04:26.259 --> 00:04:29.621
''Aha, eu reconheço isto.
Esta é uma expressão de sorriso alegre.''

00:04:30.581 --> 00:04:33.393
A melhor maneira de demonstrar
como essa tecnologia funciona

00:04:33.393 --> 00:04:35.514
é fazer uma demonstração ao vivo.

00:04:35.514 --> 00:04:38.951
Portanto, preciso de um voluntário,
de preferência alguém com um rosto.

00:04:38.951 --> 00:04:41.270
(Risos)

00:04:42.250 --> 00:04:44.712
Cloe vai ser hoje a nossa voluntária.

00:04:45.343 --> 00:04:49.898
Nos últimos cinco anos, passámos
de um projeto de pesquisa no MIT

00:04:49.898 --> 00:04:51.281
para uma empresa,

00:04:51.281 --> 00:04:54.823
onde o meu grupo tem trabalhado muito
para fazer funcionar esta tecnologia,

00:04:54.823 --> 00:04:56.665
como gostamos de dizer, no dia-a-dia.

00:04:56.665 --> 00:04:59.617
E também a reduzimos para que
o mecanismo de emoção central

00:04:59.617 --> 00:05:02.593
funcione em qualquer aparelho
com câmara, como este iPad.

00:05:02.593 --> 00:05:04.746
Então vamos lá testá-lo.

00:05:07.770 --> 00:05:10.760
Como podem ver, o algoritmo
encontrou o rosto da Cloe

00:05:10.760 --> 00:05:12.656
— é esta caixa delimitadora branca —

00:05:12.656 --> 00:05:15.287
e está a procurar
os pontos principais das feições dela:

00:05:15.287 --> 00:05:17.687
as sobrancelhas, os olhos,
a boca e o nariz.

00:05:17.907 --> 00:05:20.585
A pergunta é: ''Poderá
reconhecer a expressão dela?''

00:05:20.585 --> 00:05:22.289
Vamos testar a máquina.

00:05:22.289 --> 00:05:24.874
Primeiro, faz um rosto inexpressivo.
Isso, fantástico.

00:05:24.874 --> 00:05:26.556
(Risos)

00:05:27.128 --> 00:05:29.742
Quando ela sorri, é um
sorriso genuíno, é ótimo.

00:05:29.742 --> 00:05:31.748
Vejam a barra verde
subir quando ela sorri.

00:05:31.748 --> 00:05:33.123
Esse foi um sorriso enorme.

00:05:33.123 --> 00:05:35.895
Podes sorrir levemente,
para ver se o computador reconhece?

00:05:35.895 --> 00:05:37.718
Ele também reconhece sorrisos subtis.

00:05:37.718 --> 00:05:39.886
Trabalhámos muito para que isto aconteça.

00:05:39.886 --> 00:05:42.892
Agora o levantar das sobrancelhas,
que indica surpresa.

00:05:43.218 --> 00:05:46.758
O franzir da testa,
que indica confusão.

00:05:46.758 --> 00:05:48.266
(Risos)

00:05:48.266 --> 00:05:51.189
O franzir de sobrancelhas. Perfeito.

00:05:51.745 --> 00:05:54.741
Estas são diferentes unidades de ação.
Existem muitas outras.

00:05:54.741 --> 00:05:56.856
Esta é uma demonstração reduzida.

00:05:57.223 --> 00:06:00.325
Nós consideramos cada leitura 
como um dado emocional.

00:06:00.688 --> 00:06:03.612
Podem aparecer juntos
para representar diferentes emoções.

00:06:03.612 --> 00:06:07.450
No lado direito da demonstração
— parece que estamos felizes.

00:06:07.450 --> 00:06:09.283
Isso é alegria. A alegria dispara.

00:06:09.585 --> 00:06:11.523
Agora faz uma cara de desgosto.

00:06:11.523 --> 00:06:14.840
Tenta lembrar-te como te sentiste
quando Zayn saiu dos One Direction.

00:06:14.840 --> 00:06:16.331
(Risos)

00:06:16.331 --> 00:06:19.188
Isso, enruga o nariz. Ótimo.

00:06:21.914 --> 00:06:25.189
A valência está muito negativa, 
devias ser uma grande fã.

00:06:25.189 --> 00:06:28.390
A valência é quão positiva
ou negativa é uma experiência,

00:06:28.390 --> 00:06:30.837
e o envolvimento é
quão expressiva é a pessoa.

00:06:30.837 --> 00:06:34.223
Imaginem se Cloe tivesse acesso
a este fluxo digital de emoções,

00:06:34.223 --> 00:06:36.877
e pudesse partilhar isso
com quem ela quisesse.

00:06:37.610 --> 00:06:38.546
Obrigada.

00:06:38.771 --> 00:06:41.771
(Aplausos)

00:06:45.851 --> 00:06:50.972
Até agora, já acumulámos 12 mil milhões
destes dados emocionais.

00:06:50.972 --> 00:06:53.430
É o maior banco de dados 
emocionais do mundo.

00:06:53.430 --> 00:06:56.979
Fomos buscá-los a 2,9 milhões 
de vídeos de rostos,

00:06:56.979 --> 00:07:00.135
de pessoas que aceitaram partilhar
as suas emoções connosco,

00:07:00.135 --> 00:07:02.601
de 75 países em todo o mundo.

00:07:02.601 --> 00:07:04.643
Ele está a crescer todos os dias.

00:07:04.697 --> 00:07:07.185
Eu fico espantada com o fato de que

00:07:07.185 --> 00:07:10.524
hoje em dia podemos quantificar
uma coisa tão pessoal como as emoções

00:07:10.524 --> 00:07:12.359
e podemos fazer isso a este nível.

00:07:12.359 --> 00:07:14.522
Então o que é que aprendemos até agora?

00:07:15.227 --> 00:07:16.966
Diferenças entre sexos.

00:07:17.356 --> 00:07:20.477
Os nossos dados confirmam uma coisa
que vocês talvez suspeitem.

00:07:20.897 --> 00:07:23.395
As mulheres são mais expressivas
do que os homens.

00:07:23.395 --> 00:07:25.964
Sorriem-se mais, 
e os sorrisos delas duram mais tempo.

00:07:25.964 --> 00:07:28.774
Podemos quantificar 
a que é que homens e mulheres

00:07:28.774 --> 00:07:30.670
reagem de modo diferente.

00:07:30.670 --> 00:07:32.234
Vamos contribuir para a cultura:

00:07:32.234 --> 00:07:33.636
Nos Estados Unidos,

00:07:33.636 --> 00:07:36.184
as mulheres são 50% mais expressivas
do que os homens

00:07:36.184 --> 00:07:40.347
mas, curiosamente, não vemos diferença
no Reino Unido entre homens e mulheres.

00:07:40.456 --> 00:07:42.576
(Risos)

00:07:43.615 --> 00:07:47.540
Idade: As pessoas com 50 anos ou mais

00:07:47.540 --> 00:07:50.496
são 25% mais emotivas 
do que as pessoas jovens.

00:07:51.396 --> 00:07:55.658
As mulheres na faixa dos 20 anos sorriem
mais do que os homens da mesma idade,

00:07:55.658 --> 00:07:58.382
talvez seja uma necessidade
para arranjar namoro.

00:07:59.506 --> 00:08:02.456
Mas talvez o que mais nos surpreendeu
em relação a estes dados

00:08:02.456 --> 00:08:05.294
é que nós somos expressivos o tempo todo,

00:08:05.294 --> 00:08:08.746
mesmo quando estamos sentados sozinhos
em frente dos nossos computadores,

00:08:08.746 --> 00:08:11.780
e não é só quando assistimos
a vídeos de gatos no Facebook.

00:08:11.935 --> 00:08:15.699
Somos expressivos quando enviamos e-mails
e mensagens, quando compramos online,

00:08:15.699 --> 00:08:17.899
e até quando fazemos
a declaração de impostos.

00:08:17.972 --> 00:08:20.088
Onde é que esses dados são usados hoje?

00:08:20.088 --> 00:08:22.561
Para compreender 
como nos envolvemos com os media;

00:08:22.561 --> 00:08:25.682
para compreender os fenómenos virais
e o comportamento eleitoral;

00:08:25.682 --> 00:08:28.893
e também nas tecnologias
que reforçam ou provocam emoções.

00:08:28.893 --> 00:08:32.207
Quero mostrar-vos alguns exemplos
que considero muito especiais.

00:08:33.357 --> 00:08:36.408
Óculos que reconheçam emoções
podem ajudar indivíduos

00:08:36.408 --> 00:08:39.219
que possuem deficiente visão
a ler o rosto dos outros.

00:08:39.562 --> 00:08:43.179
Podem ajudar indivíduos do espetro
autista a interpretar emoções,

00:08:43.179 --> 00:08:45.794
que é uma coisa 
em que eles têm muita dificuldade.

00:08:47.667 --> 00:08:50.954
Na área do ensino, imaginem se
os aplicativos educacionais

00:08:50.954 --> 00:08:53.626
percebam que estamos confusos
e abrandem de velocidade

00:08:53.626 --> 00:08:56.660
ou que estamos aborrecidos
e avancem mais depressa,

00:08:56.660 --> 00:08:58.830
como faria um bom professor na aula.

00:08:59.660 --> 00:09:02.080
E se o nosso relógio detetasse
a nossa disposição?

00:09:02.080 --> 00:09:04.463
Ou se o nosso carro sentisse que
estamos cansados?

00:09:04.463 --> 00:09:07.350
Ou o nosso frigorífico percebesse
que estamos nervosos

00:09:07.350 --> 00:09:10.425
e trancasse automaticamente
para impedir que comamos demais?

00:09:10.443 --> 00:09:11.597
(Risos)

00:09:11.597 --> 00:09:13.283
Eu adoraria isso.

00:09:15.786 --> 00:09:17.615
E se, quando eu estava em Cambridge,

00:09:17.615 --> 00:09:20.462
eu tivesse tido acesso ao
meu fluxo de emoções em tempo real

00:09:20.462 --> 00:09:23.607
e pudesse partilhar isso 
com a minha família de modo natural,

00:09:23.607 --> 00:09:26.880
tal como faria se estivéssemos
todos juntos na mesma sala?

00:09:27.623 --> 00:09:30.314
Eu acho que, daqui a 5 anos,

00:09:30.314 --> 00:09:32.758
todos os nossos aparelhos
terão um chip emocional.

00:09:33.220 --> 00:09:37.226
Nem nos vamos lembrar como era não poder
franzir testa para o computador

00:09:37.226 --> 00:09:40.299
e ele responder: 
''Hmm, não gostaste disto, pois não?.''

00:09:41.407 --> 00:09:44.835
O nosso maior desafio é que há
tantas aplicações desta tecnologia

00:09:44.835 --> 00:09:48.239
que a minha equipa e eu percebemos que
não podemos montar tudo sozinhos.

00:09:48.239 --> 00:09:51.541
Assim, tornámos disponível esta tecnologia
para que outras pessoas

00:09:51.541 --> 00:09:53.820
possam começar a construir e a criar.

00:09:53.820 --> 00:09:57.779
Sabemos que há riscos potenciais

00:09:57.779 --> 00:09:59.871
e há potencial para abusos,

00:09:59.871 --> 00:10:02.776
mas, pessoalmente, como trabalhei
nisto durante muitos anos,

00:10:02.776 --> 00:10:05.206
acredito que os benefícios
para a humanidade

00:10:05.206 --> 00:10:07.936
de termos uma tecnologia
emocionalmente inteligente

00:10:07.936 --> 00:10:10.928
são muito maiores que o potencial
para uso indevido.

00:10:11.441 --> 00:10:14.176
Convido todos vocês para
participar nessa discussão.

00:10:14.176 --> 00:10:16.589
Quanto mais pessoas souberem
sobre esta tecnologia,

00:10:16.589 --> 00:10:19.710
mais força terá a nossa voz
quanto à forma como for usada.

00:10:21.315 --> 00:10:25.695
À medida que a nossa vida
é cada vez mais digital,

00:10:25.695 --> 00:10:29.508
travamos uma luta perdida ao tentar
restringir o nosso uso de tecnologias

00:10:29.508 --> 00:10:31.654
para reivindicar as nossas emoções.

00:10:32.479 --> 00:10:36.744
Em vez disso, quero introduzir
as emoções na nossa tecnologia

00:10:36.744 --> 00:10:39.131
e tornar a nossa tecnologia mais recetível.

00:10:39.131 --> 00:10:41.665
Quero que estes aparelhos
que nos separaram

00:10:41.665 --> 00:10:43.622
nos unam novamente.

00:10:44.518 --> 00:10:48.695
Ao humanizar a tecnologia,
temos uma oportunidade perfeita

00:10:48.695 --> 00:10:51.706
para re-imaginar como 
nos relacionamos com máquinas

00:10:52.287 --> 00:10:56.146
e, portanto, como nós, 
enquanto seres humanos,

00:10:56.146 --> 00:10:58.205
nos relacionamos uns com os outros.

00:10:58.351 --> 00:11:00.133
Obrigada.

00:11:00.345 --> 00:11:02.817
(Aplausos)

