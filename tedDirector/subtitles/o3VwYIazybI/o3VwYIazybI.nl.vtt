WEBVTT
Kind: captions
Language: nl

00:00:00.000 --> 00:00:07.000
Vertaald door: Bonnie Dekker
Nagekeken door: Robert de Ridder

00:00:12.786 --> 00:00:16.533
Onze emoties beïnvloeden
alle aspecten van ons leven,

00:00:16.533 --> 00:00:18.249
van onze gezondheid en hoe we leren,

00:00:18.249 --> 00:00:20.262
tot hoe we zakendoen
en beslissingen nemen,

00:00:20.262 --> 00:00:21.942
grote en kleine.

00:00:22.692 --> 00:00:26.252
Onze emoties hebben ook invloed
op hoe we met elkaar omgaan.

00:00:27.202 --> 00:00:31.218
We zijn geëvolueerd om te leven
in een wereld die er zo uitziet,

00:00:31.218 --> 00:00:35.427
maar in plaats daarvan leven we
ons leven steeds vaker zo

00:00:35.427 --> 00:00:39.031
-- dit is het berichtje 
dat mijn dochter gisteravond stuurde --

00:00:39.031 --> 00:00:41.491
in een wereld zonder emotie.

00:00:41.491 --> 00:00:43.252
Mijn missie is om dat te veranderen.

00:00:43.252 --> 00:00:47.243
Ik wil emoties terugbrengen
in onze digitale ervaringen.

00:00:48.533 --> 00:00:50.980
15 jaar geleden ben ik hiermee begonnen.

00:00:50.980 --> 00:00:53.686
Ik was computerwetenschapper in Egypte,

00:00:53.686 --> 00:00:58.051
en ik was net toegelaten 
tot een PhD-programma in Cambridge.

00:00:58.051 --> 00:01:00.154
Ik deed iets behoorlijk ongewoons

00:01:00.154 --> 00:01:04.049
voor een jonge, pasgetrouwde
Egyptische moslima.

00:01:04.049 --> 00:01:05.699
(Gelach)

00:01:05.699 --> 00:01:08.568
Met de steun van mijn man,
die in Egypte moest blijven,

00:01:08.568 --> 00:01:11.576
pakte ik mijn koffers
en verhuisde ik naar Engeland.

00:01:11.576 --> 00:01:14.904
Daar in Cambridge, 
duizenden kilometers van huis,

00:01:14.904 --> 00:01:18.117
besefte ik dat ik meer uren 
met mijn laptop doorbracht

00:01:18.117 --> 00:01:20.576
dan met andere mensen.

00:01:20.576 --> 00:01:22.009
Ondanks deze vertrouwdheid

00:01:22.009 --> 00:01:25.469
had mijn laptop geen flauw idee 
hoe ik me voelde.

00:01:25.469 --> 00:01:28.550
Hij wist totaal niet of ik blij was,

00:01:28.550 --> 00:01:31.958
een slechte dag had,
gestrest of verward was,

00:01:31.958 --> 00:01:34.460
en dat werd frustrerend.

00:01:35.680 --> 00:01:40.801
Erger nog, bij het online communiceren
met mijn familie thuis

00:01:40.801 --> 00:01:44.813
had ik het gevoel dat al mijn emoties
verdwenen in de cyberspace.

00:01:44.813 --> 00:01:49.908
Ik had heimwee, ik was eenzaam,
op sommige dagen zat ik echt te huilen,

00:01:49.908 --> 00:01:53.836
maar het enige wat ik had
om deze emoties over te brengen

00:01:53.836 --> 00:01:54.946
was dit.

00:01:54.946 --> 00:01:57.036
(Gelach)

00:01:57.036 --> 00:02:01.120
De technologie van vandaag
heeft veel IQ, maar geen EQ:

00:02:01.120 --> 00:02:05.286
veel cognitieve intelligentie,
maar geen emotionele intelligentie.

00:02:05.286 --> 00:02:07.153
Dat zette me aan het denken.

00:02:07.153 --> 00:02:10.967
Wat als onze technologie
onze emoties kon waarnemen?

00:02:10.967 --> 00:02:15.003
Wat als onze apparaten konden aanvoelen
hoe we ons voelden en daarop reageerden,

00:02:15.003 --> 00:02:17.866
zoals emotioneel intelligente vrienden 
dat ook zouden doen?

00:02:18.776 --> 00:02:23.700
Die vragen zetten mij en mijn team 
ertoe aan technologieën te maken

00:02:23.700 --> 00:02:26.587
die onze emoties kunnen lezen
en erop reageren

00:02:26.587 --> 00:02:29.697
en ons uitgangspunt 
was het menselijk gezicht.

00:02:30.757 --> 00:02:33.960
Het menselijk gezicht
is een van de sterkste kanalen

00:02:33.960 --> 00:02:37.766
waarmee we onze sociale 
en emotionele toestand overbrengen.

00:02:37.766 --> 00:02:39.756
Alles van vreugde

00:02:39.756 --> 00:02:41.396
tot verrassing,

00:02:41.396 --> 00:02:42.576
empathie

00:02:42.576 --> 00:02:45.129
en nieuwsgierigheid.

00:02:45.129 --> 00:02:48.647
In de emotiewetenschap noemen we 
elke beweging van de gelaatsspieren

00:02:48.647 --> 00:02:50.157
een actie-eenheid.

00:02:50.157 --> 00:02:52.472
Actie-eenheid 12 bijvoorbeeld,

00:02:52.472 --> 00:02:54.760
dat is geen blockbuster uit Hollywood,

00:02:54.760 --> 00:02:58.392
maar het optrekken van de mondhoeken,
het hoofdonderdeel van een glimlach.

00:02:58.392 --> 00:03:00.990
Laten we dat allemaal even proberen.

00:03:00.990 --> 00:03:02.940
Een ander voorbeeld is actie-eenheid 4,

00:03:02.940 --> 00:03:03.954
de wenkbrauwfrons:

00:03:03.954 --> 00:03:06.082
als je je wenkbrauwen 
tegen elkaar duwt

00:03:06.082 --> 00:03:07.832
en allemaal rimpels maakt.

00:03:07.832 --> 00:03:09.429
We vinden ze niet leuk,

00:03:09.429 --> 00:03:12.184
maar ze zijn een goed signaal
voor een negatieve emotie.

00:03:12.184 --> 00:03:15.430
We hebben ongeveer 45 
van die actie-eenheden

00:03:15.430 --> 00:03:18.350
en samen drukken ze honderden emoties uit.

00:03:18.350 --> 00:03:22.411
Het is moeilijk om een computer te leren
om deze gelaatsuitdrukkingen te lezen,

00:03:22.411 --> 00:03:25.423
want deze actie-eenheden 
kunnen snel zijn, ze zijn subtiel,

00:03:25.423 --> 00:03:27.777
en ze komen voor in allerlei combinaties.

00:03:27.777 --> 00:03:31.665
Neem bijvoorbeeld een glimlach
en een minachtende blik.

00:03:31.665 --> 00:03:35.268
Ze lijken een beetje op elkaar,
maar ze betekenen iets heel anders.

00:03:35.268 --> 00:03:37.176
(Gelach)

00:03:37.176 --> 00:03:39.200
De een is positief,

00:03:39.200 --> 00:03:41.010
de ander vaak negatief.

00:03:41.010 --> 00:03:45.236
Soms kun je er beroemd door worden.

00:03:45.236 --> 00:03:48.800
Maar het is belangrijk
dat een computer het verschil kan zien

00:03:48.800 --> 00:03:50.635
tussen de twee gelaatsexpressies.

00:03:50.635 --> 00:03:52.407
Hoe doen we dat?

00:03:52.407 --> 00:03:54.284
We geven onze algoritmen

00:03:54.284 --> 00:03:58.694
tienduizenden voorbeelden van mensen
waarvan we weten dat ze glimlachen,

00:03:58.694 --> 00:04:01.659
van verschillende etniciteiten,
leeftijden, geslachten,

00:04:01.659 --> 00:04:04.190
en dan doen we hetzelfde 
voor de andere uitdrukking.

00:04:04.190 --> 00:04:07.004
Door middel van deep learning
zoekt het algoritme

00:04:07.004 --> 00:04:09.160
naar alle structuren en rimpels

00:04:09.160 --> 00:04:11.380
en vormveranderingen in ons gezicht.

00:04:11.380 --> 00:04:14.992
Zo leert het dat alle glimlachen
gemeenschappelijke eigenschappen hebben

00:04:14.992 --> 00:04:17.873
die subtiel verschillen 
van minachtende uitdrukkingen.

00:04:17.873 --> 00:04:20.141
De volgende keer 
dat het een nieuw gezicht ziet

00:04:20.141 --> 00:04:25.120
leert het dat dit gezicht ook
de eigenschappen van een glimlach heeft

00:04:25.120 --> 00:04:29.301
en zegt het: "Aha, dit herken ik.
Deze uitdrukking is een glimlach."

00:04:30.291 --> 00:04:33.561
De beste manier om te laten zien 
hoe deze technologie werkt

00:04:33.561 --> 00:04:35.557
is een live-demonstratie,

00:04:35.557 --> 00:04:39.230
dus ik heb een vrijwilliger nodig,
liefst iemand met een gezicht.

00:04:39.230 --> 00:04:41.694
(Gelach)

00:04:41.694 --> 00:04:44.335
Cloe is vandaag onze vrijwilliger.

00:04:45.475 --> 00:04:47.183
In de afgelopen vijf jaar

00:04:47.183 --> 00:04:50.939
zijn we van een onderzoeksproject bij MIT
overgegaan in een bedrijf,

00:04:50.939 --> 00:04:54.371
waar mijn team keihard heeft gewerkt
om deze technologie te laten werken,

00:04:54.371 --> 00:04:56.250
zoals wij dat zeggen, in het wild.

00:04:56.250 --> 00:04:57.900
We hebben hem ook kleiner gemaakt

00:04:57.900 --> 00:05:01.200
zodat de emotiemachine werkt
op elk mobiel apparaat met een camera,

00:05:01.200 --> 00:05:02.760
zoals deze iPad.

00:05:02.760 --> 00:05:05.316
Laten we het eens proberen.

00:05:06.876 --> 00:05:10.770
Zoals je ziet heeft het algoritme
Cloe's gezicht gevonden,

00:05:10.770 --> 00:05:12.252
dat is die witte rechthoek,

00:05:12.252 --> 00:05:14.943
en het volgt de belangrijkste 
punten van haar gezicht,

00:05:14.943 --> 00:05:17.799
dus de wenkbrauwen, ogen,
mond en neus.

00:05:17.799 --> 00:05:20.646
De vraag is: kan het 
haar uitdrukking herkennen?

00:05:20.646 --> 00:05:22.077
We gaan de machine testen.

00:05:22.077 --> 00:05:26.913
Laat eerst eens je pokerface zien.
Ja, goed zo. (Gelach)

00:05:26.913 --> 00:05:29.816
En terwijl ze glimlacht,
dit is een oprechte glimlach,

00:05:29.816 --> 00:05:31.616
zie je de groene balken omhooggaan.

00:05:31.616 --> 00:05:32.908
Dat was een brede glimlach.

00:05:32.908 --> 00:05:35.951
Kun je proberen of de computer
ook een subtiele glimlach herkent?

00:05:35.951 --> 00:05:37.712
De subtiele glimlach herkent hij ook.

00:05:37.712 --> 00:05:39.717
Daar hebben we hard aan gewerkt.

00:05:39.717 --> 00:05:43.439
En dan je wenkbrauwen omhoog,
een teken van verrassing.

00:05:43.439 --> 00:05:48.038
Wenkbrauwen fronsen, 
een teken van verwarring.

00:05:48.038 --> 00:05:51.755
Boos kijken. Ja, perfect.

00:05:51.755 --> 00:05:53.565
Dit zijn verschillende actie-eenheden.

00:05:53.565 --> 00:05:54.758
Er zijn er nog veel meer.

00:05:54.758 --> 00:05:57.220
Dit is maar een beperkte demo.

00:05:57.220 --> 00:06:00.608
We noemen elke meting
een emotiegegevenspunt,

00:06:00.608 --> 00:06:03.377
en in combinatie kunnen ze
verschillende emoties weergeven.

00:06:03.377 --> 00:06:07.240
Aan de rechterkant van de demo --
trek eens een vrolijk gezicht.

00:06:07.240 --> 00:06:09.594
Dat is vreugde. Vreugde gaat aan.

00:06:09.594 --> 00:06:11.371
Laat nu eens afkeer zien.

00:06:11.371 --> 00:06:15.523
Denk aan hoe je je voelde
toen Zayn wegging bij One Direction.

00:06:15.523 --> 00:06:16.943
(Gelach)

00:06:16.943 --> 00:06:22.055
Ja, maak rimpels bij je neus. Goed zo.

00:06:22.055 --> 00:06:25.166
De valentie is behoorlijk negatief,
dus je was vast een groot fan.

00:06:25.166 --> 00:06:27.926
Valentie is hoe positief 
of negatief een ervaring is,

00:06:27.926 --> 00:06:30.892
en betrokkenheid is hoe expressief ze is.

00:06:30.892 --> 00:06:34.186
Stel je voor dat Cloe toegang had
tot deze rechtstreekse emotiestream

00:06:34.186 --> 00:06:36.935
en ze die kon delen
met we ze maar wilde.

00:06:36.935 --> 00:06:39.028
Dankjewel.

00:06:39.028 --> 00:06:44.479
(Applaus)

00:06:45.959 --> 00:06:51.019
Tot nu toe hebben we 12 miljard
van deze emotiegegevens vergaard.

00:06:51.019 --> 00:06:53.470
Het is de grootste 
emotiedatabase ter wereld.

00:06:53.470 --> 00:06:56.913
We hebben hem verzameld
uit 2,9 miljoen filmpjes van gezichten,

00:06:56.913 --> 00:07:00.083
mensen die hun emoties 
met ons hebben willen delen,

00:07:00.083 --> 00:07:02.398
uit 75 landen over de hele wereld.

00:07:02.398 --> 00:07:04.113
Hij groeit elke dag.

00:07:04.643 --> 00:07:06.630
Ik sta ervan versteld

00:07:06.630 --> 00:07:10.195
dat we iets persoonlijks als emoties
nu kunnen kwantificeren

00:07:10.195 --> 00:07:12.290
en dat we het op deze schaal kunnen doen.

00:07:12.290 --> 00:07:14.277
Wat hebben we tot nu toe geleerd?

00:07:15.207 --> 00:07:17.388
Geslacht:

00:07:17.388 --> 00:07:21.094
onze data bevestigen iets
dat je misschien al vermoedde.

00:07:21.094 --> 00:07:22.891
Vrouwen zijn expressiever dan mannen.

00:07:22.891 --> 00:07:25.604
Ze glimlachen niet alleen meer,
maar ook langduriger,

00:07:25.604 --> 00:07:28.278
en we kunnen nu echt vaststellen
wat het precies is

00:07:28.278 --> 00:07:30.614
waarop mannen en vrouwen anders reageren.

00:07:30.614 --> 00:07:31.614
Nu cultuur:

00:07:31.614 --> 00:07:33.004
in de Verenigde Staten

00:07:33.004 --> 00:07:36.108
zijn vrouwen 40 procent 
expressiever dan mannen,

00:07:36.108 --> 00:07:39.753
maar vreemd genoeg zien we in het VK
geen verschil tussen mannen en vrouwen.

00:07:39.753 --> 00:07:42.259
(Gelach)

00:07:43.576 --> 00:07:45.356
Leeftijd:

00:07:45.356 --> 00:07:47.323
mensen van 50 jaar en ouder

00:07:47.323 --> 00:07:50.759
tonen 25 procent meer emotie
dan jongere mensen.

00:07:51.559 --> 00:07:55.741
Vrouwen in de twintig glimlachen veel meer
dan mannen van dezelfde leeftijd,

00:07:55.741 --> 00:07:59.530
misschien moet dat wel tijdens dates.

00:07:59.530 --> 00:08:02.207
Maar wat ons misschien
het meest heeft verrast

00:08:02.207 --> 00:08:05.250
is dat we altijd expressief zijn,

00:08:05.250 --> 00:08:08.283
zelfs als we alleen 
achter een apparaat zitten,

00:08:08.283 --> 00:08:11.517
en niet alleen als we kattenfilmpjes 
kijken op Facebook.

00:08:12.217 --> 00:08:15.227
We zijn expressief als we emailen,
sms'en, online winkelen

00:08:15.227 --> 00:08:17.937
en zelfs als we
onze belastingaangifte doen.

00:08:17.937 --> 00:08:20.139
Waar worden deze data momenteel gebruikt?

00:08:20.139 --> 00:08:22.382
Bij het begrijpen van 
hoe we met media omgaan,

00:08:22.382 --> 00:08:25.166
het begrijpen van viraliteit
en stemgedrag,

00:08:25.166 --> 00:08:28.766
en ook voor technologie
die emoties toegankelijker maakt.

00:08:28.766 --> 00:08:32.527
Ik wil een paar voorbeelden delen
die me bijzonder nauw aan het hart liggen.

00:08:33.197 --> 00:08:34.977
Brillen met emotietechnologie

00:08:34.977 --> 00:08:37.465
kunnen mensen met 
een visuele handicap helpen

00:08:37.465 --> 00:08:39.553
om de gezichten van anderen te lezen,

00:08:39.553 --> 00:08:43.240
en mensen binnen het autismespectrum
helpen om emoties te interpreteren,

00:08:43.240 --> 00:08:46.458
iets waar ze echt moeite mee hebben.

00:08:47.548 --> 00:08:48.998
In het onderwijs:

00:08:48.998 --> 00:08:50.597
stel je voor dat je studeer-apps

00:08:50.597 --> 00:08:53.227
aanvoelen wat je niet begrijpt
en het tempo verlagen,

00:08:53.227 --> 00:08:55.484
of dat je je verveelt,
en dus sneller gaan,

00:08:55.484 --> 00:08:58.413
net als een goede leraar op school.

00:08:59.003 --> 00:09:01.644
Wat als je horloge je stemming bijhield

00:09:01.644 --> 00:09:04.407
of je auto kon merken dat je moe was,

00:09:04.407 --> 00:09:07.165
of je koelkast het wist
als je gestrest was,

00:09:07.165 --> 00:09:11.281
en automatisch op slot ging
zodat je niet te veel kon eten. (Gelach)

00:09:11.281 --> 00:09:13.798
Dat zou ik wel willen, ja.

00:09:15.728 --> 00:09:17.675
Wat als ik in Cambridge

00:09:17.675 --> 00:09:19.608
toegang had gehad tot mijn emotiestream

00:09:19.608 --> 00:09:21.818
en die had kunnen delen 
met mijn familie thuis

00:09:21.818 --> 00:09:23.847
op een heel natuurlijke manier,

00:09:23.847 --> 00:09:27.608
alsof we samen in één kamer zaten?

00:09:27.608 --> 00:09:30.190
Ik denk dat over vijf jaar

00:09:30.190 --> 00:09:33.237
al onze apparaten 
een emotie-chip zullen hebben

00:09:33.237 --> 00:09:37.011
en we niet meer weten hoe het was
om naar je apparaat te fronsen

00:09:37.011 --> 00:09:41.330
zonder dat het terugzei:
"Hmm, dat vond je blijkbaar niet leuk."

00:09:41.330 --> 00:09:44.991
De grootste uitdaging van deze technologie
is dat er zoveel toepassingen zijn,

00:09:44.991 --> 00:09:47.864
dat mijn team en ik ze niet allemaal 
zelf kunnen bouwen.

00:09:47.864 --> 00:09:50.310
Daarom hebben we de technologie
beschikbaar gemaakt

00:09:50.310 --> 00:09:53.794
zodat andere ontwikkelaars
er creatief mee aan de slag kunnen.

00:09:53.794 --> 00:09:57.550
We zien in dat er potentiële risico's zijn

00:09:57.550 --> 00:10:00.087
en de mogelijkheid dat er misbruik
van wordt gemaakt,

00:10:00.087 --> 00:10:02.466
maar nu ik hier vele jaren 
mee bezig ben geweest

00:10:02.466 --> 00:10:05.148
geloof ik persoonlijk
dat de voordelen voor de mensheid

00:10:05.148 --> 00:10:07.703
van emotioneel intelligente technologie

00:10:07.703 --> 00:10:11.269
veel zwaarder wegen
dan het mogelijke misbruik.

00:10:11.269 --> 00:10:14.030
Ik nodig jullie allemaal uit
deel te nemen aan het gesprek.

00:10:14.030 --> 00:10:16.394
Hoe meer mensen
van deze technologie afweten,

00:10:16.394 --> 00:10:19.801
hoe meer we allemaal inspraak hebben
in hoe het wordt gebruikt.

00:10:21.621 --> 00:10:25.835
Nu ons leven steeds digitaler wordt,

00:10:25.835 --> 00:10:27.395
voeren we een hopeloze strijd

00:10:27.395 --> 00:10:29.953
als we ons gebruik van apparaten 
proberen te beperken

00:10:29.953 --> 00:10:31.722
om zo onze emoties terug te winnen.

00:10:32.622 --> 00:10:36.576
In plaats daarvan probeer ik
emoties in onze technologie op te nemen

00:10:36.576 --> 00:10:38.765
en onze technologieën 
beter te laten reageren.

00:10:38.765 --> 00:10:41.515
Ik wil dat de apparaten
die ons hebben gescheiden

00:10:41.515 --> 00:10:44.507
ons weer bij elkaar brengen.

00:10:44.507 --> 00:10:48.735
Door technologie te vermenselijken
hebben we een gouden kans

00:10:48.735 --> 00:10:52.452
om opnieuw te bedenken
hoe we met onze machines omgaan

00:10:52.452 --> 00:10:56.033
en daarmee ook hoe wij, als mensen,

00:10:56.033 --> 00:10:58.087
omgaan met elkaar.

00:10:58.087 --> 00:10:59.217
Dankjewel.

00:10:59.217 --> 00:11:03.640
(Applaus)

