WEBVTT
Kind: captions
Language: nl

00:00:00.000 --> 00:00:07.000
Vertaald door: Rik Delaet
Nagekeken door: Els De Keyser

00:00:12.570 --> 00:00:16.777
Ik werk met een hoop wiskundigen, 
filosofen en informatici,

00:00:16.777 --> 00:00:21.076
en we zitten onder andere na te denken

00:00:21.076 --> 00:00:24.030
over de toekomst 
van machine-intelligentie.

00:00:24.030 --> 00:00:28.755
Sommige mensen vinden dit 
nogal science-fictionachtig,

00:00:28.755 --> 00:00:31.856
over de schreef, te gek.

00:00:31.856 --> 00:00:33.326
Maar laat ons eens kijken

00:00:33.326 --> 00:00:36.930
naar de moderne menselijke conditie.

00:00:36.930 --> 00:00:38.622
(Gelach)

00:00:38.622 --> 00:00:41.024
Dit is de normale manier van zijn.

00:00:41.024 --> 00:00:43.309
Maar als we erover nadenken,

00:00:43.309 --> 00:00:46.602
zijn we eigenlijk op deze planeet 
recent aangekomen gasten,

00:00:46.602 --> 00:00:48.684
de menselijke soort.

00:00:48.684 --> 00:00:53.430
Als de ouderdom van de Aarde 
één jaar zou zijn,

00:00:53.430 --> 00:00:56.978
dan is de menselijke soort 
10 minuten oud.

00:00:56.978 --> 00:01:00.146
Het industriële tijdperk 
begon twee seconden geleden.

00:01:01.276 --> 00:01:06.501
Of kijk eens hoe het mondiale bbp 
in de afgelopen 10.000 jaar veranderde,

00:01:06.501 --> 00:01:09.530
Ik heb er een grafiek van gemaakt.

00:01:09.530 --> 00:01:11.304
Zo ziet ze er uit.

00:01:11.304 --> 00:01:12.477
(Gelach)

00:01:12.477 --> 00:01:15.058
Een merkwaardige vorm 
voor een normale toestand.

00:01:15.058 --> 00:01:16.846
Daar wil je niet op gaan zitten.

00:01:16.846 --> 00:01:19.067
(Gelach)

00:01:19.067 --> 00:01:23.041
Vanwaar komt die huidige anomalie?

00:01:23.041 --> 00:01:26.393
Sommige mensen zouden zeggen 
dat het komt door de technologie.

00:01:26.393 --> 00:01:31.061
Inderdaad stapelde de technologie zich op 
doorheen de menselijke geschiedenis,

00:01:31.061 --> 00:01:35.713
en nu gaat het wel heel snel -

00:01:35.713 --> 00:01:37.278
dat is de directe oorzaak,

00:01:37.278 --> 00:01:39.843
daarom zijn we zo productief.

00:01:40.473 --> 00:01:44.134
Maar ik denk ook graag 
aan de uiteindelijke oorzaak.

00:01:45.114 --> 00:01:48.880
Kijk eens 
naar deze twee zeer voorname heren:

00:01:48.880 --> 00:01:50.480
Dat is Kanzi --

00:01:50.480 --> 00:01:55.123
hij beheerst 200 lexicale tokens, 
een ongelooflijke prestatie.

00:01:55.123 --> 00:01:58.817
En Ed Witten ontketende 
de tweede superstringrevolutie.

00:01:58.817 --> 00:02:01.141
Als we onder de motorkap kijken, 
vinden we dit:

00:02:01.141 --> 00:02:02.711
in principe hetzelfde.

00:02:02.711 --> 00:02:03.624
Een is iets groter,

00:02:03.624 --> 00:02:05.824
en heeft misschien 
ook wel een paar trucjes

00:02:05.824 --> 00:02:08.192
in de exacte manier 
waarop het is bedraad.

00:02:08.192 --> 00:02:11.334
Deze onzichtbare verschillen 
kunnen niet te ingewikkeld zijn,

00:02:11.334 --> 00:02:14.459
want er zijn slechts 250.000 generaties

00:02:14.459 --> 00:02:17.111
sinds onze laatste 
gemeenschappelijke voorouder.

00:02:17.111 --> 00:02:20.960
Ingewikkelde mechanismen hebben
een lange tijd nodig om te evolueren.

00:02:22.000 --> 00:02:24.499
Een hoop relatief kleine wijzigingen

00:02:24.499 --> 00:02:27.566
brengt ons van Kanzi tot Witten,

00:02:27.566 --> 00:02:32.109
van afgebroken boomtakken tot 
intercontinentale ballistische raketten.

00:02:32.839 --> 00:02:36.774
Het lijkt vrij duidelijk 
dat alles wat we hebben bereikt,

00:02:36.774 --> 00:02:38.152
en alles waar we om geven,

00:02:38.152 --> 00:02:41.650
sterk afhangt van een aantal 
relatief kleine wijzigingen

00:02:41.650 --> 00:02:44.650
die de menselijke geest uitmaken.

00:02:44.650 --> 00:02:48.312
Daaruit volgt natuurlijk 
dat verdere wijzigingen

00:02:48.312 --> 00:02:51.789
die het substraat van het denken 
aanzienlijk zouden kunnen veranderen

00:02:51.789 --> 00:02:54.991
potentieel enorme gevolgen kunnen hebben.

00:02:56.321 --> 00:02:59.226
Sommige collega's denken 
dat we aan het begin staan

00:02:59.226 --> 00:03:03.134
van iets dat een diepgaande verandering 
in dat substraat kan veroorzaken,

00:03:03.134 --> 00:03:06.347
en dat is de machinale superintelligentie.

00:03:06.347 --> 00:03:11.086
Kunstmatige intelligentie ging vroeger 
over opdrachten in een doos steken.

00:03:11.086 --> 00:03:12.751
Je had de menselijke programmeurs

00:03:12.751 --> 00:03:15.886
die nauwgezet kennisitems 
in elkaar knutselden.

00:03:15.886 --> 00:03:17.972
Je bouwt expertsystemen.

00:03:17.972 --> 00:03:20.296
Ze zijn bruikbaar 
voor bepaalde doeleinden,

00:03:20.296 --> 00:03:22.977
maar erg broos en niet schaalbaar.

00:03:22.977 --> 00:03:26.410
Kortom, je krijgt alleen 
wat je erin steekt.

00:03:26.410 --> 00:03:28.627
Maar sindsdien was er
paradigmaverschuiving

00:03:28.627 --> 00:03:30.874
op het gebied 
van kunstmatige intelligentie.

00:03:30.874 --> 00:03:33.644
Vandaag ligt de focus 
op ‘machinaal leren’.

00:03:34.394 --> 00:03:39.781
In plaats van kennisrepresentaties 
en functies in elkaar te knutselen,

00:03:40.511 --> 00:03:46.065
creëren we algoritmen die zelf leren, 
vaak vanuit ruwe perceptuele data.

00:03:46.065 --> 00:03:51.063
In principe hetzelfde 
als wat een kind doet.

00:03:51.063 --> 00:03:55.270
Het resultaat is dat AI 
niet beperkt is tot één domein -

00:03:55.270 --> 00:03:59.901
hetzelfde systeem kan leren vertalen 
tussen alle paren van talen,

00:03:59.901 --> 00:04:05.327
of elk computerspel leren spelen 
op een Atari console.

00:04:05.327 --> 00:04:11.116
AI komt het nog niet in de buurt
van het domeinoverschrijdende vermogen

00:04:11.116 --> 00:04:14.335
om te leren en te plannen 
dat een mens heeft.

00:04:14.335 --> 00:04:16.971
De cortex heeft nog 
een aantal algoritmische trucs

00:04:16.971 --> 00:04:19.666
die we nog niet kunnen 
implementeren in machines.

00:04:19.886 --> 00:04:21.785
De vraag is:

00:04:21.785 --> 00:04:24.975
hoe ver staan we af van het vermogen 
om die trucs toe te passen?

00:04:24.975 --> 00:04:27.458
Een paar jaar geleden
deden we een enquête

00:04:27.458 --> 00:04:30.216
bij 's werelds toonaangevende AI-experts,

00:04:30.216 --> 00:04:31.216
om te zien wat ze denken. 

00:04:31.216 --> 00:04:33.440
Een van de vragen was:

00:04:33.440 --> 00:04:36.793
"Voor welk jaar denkt u 
dat er 50 procent kans is

00:04:36.793 --> 00:04:40.275
dat we menselijke machine-intelligentie 
gaan hebben?”

00:04:40.785 --> 00:04:44.968
We definieerden menselijk hier 
als het vermogen om bijna elke taak

00:04:44.968 --> 00:04:47.839
tenminste als een volwassen mens 
te kunnen uitvoeren.

00:04:47.839 --> 00:04:51.844
Dus echt als een mens, 
niet alleen binnen een beperkt domein.

00:04:51.844 --> 00:04:55.494
Het mediane antwoord was 2040 of 2050,

00:04:55.494 --> 00:04:58.300
afhankelijk van de groep deskundigen.

00:04:58.300 --> 00:05:02.339
Het zou veel later 
of eerder kunnen gebeuren.

00:05:02.339 --> 00:05:05.249
De waarheid is 
dat niemand het echt weet.

00:05:05.259 --> 00:05:09.921
Wat we wel weten, is dat de ultieme 
limiet aan informatieverwerking

00:05:09.939 --> 00:05:12.939
de grenzen van biologisch weefsel
ver overstijgt.

00:05:15.241 --> 00:05:17.619
Dit komt door de natuurkunde.

00:05:17.619 --> 00:05:22.337
Een biologisch neuron vuurt op 200 hertz, 
200 keer per seconde.

00:05:22.337 --> 00:05:25.931
Maar zelfs een hedendaagse transistor 
werkt op Gigahertz [1 miljard hertz].

00:05:25.931 --> 00:05:31.228
Neuronen gaan in axonen 
aan 100 meter per seconde, maximum.

00:05:31.228 --> 00:05:34.339
Maar in computers reizen signalen 
met de snelheid van het licht.

00:05:35.079 --> 00:05:36.948
Er zijn ook beperkingen in grootte:

00:05:36.948 --> 00:05:39.975
een menselijk brein 
moet passen in een schedel,

00:05:39.975 --> 00:05:44.736
maar een computer kan zo groot zijn 
als een magazijn of groter.

00:05:44.736 --> 00:05:50.335
Het potentieel voor superintelligentie 
sluimert in de materie,

00:05:50.335 --> 00:05:56.047
net als de kracht van het atoom 
in de menselijke geschiedenis sluimerde,

00:05:56.047 --> 00:05:59.542
geduldig wachtend tot 1945.

00:05:59.542 --> 00:06:01.700
In deze eeuw kunnen 
wetenschappers leren hoe

00:06:01.700 --> 00:06:05.818
de kracht te ontketenen
van kunstmatige intelligentie.

00:06:05.818 --> 00:06:09.826
Dan zien we misschien 
een intelligentie-explosie.

00:06:10.406 --> 00:06:14.363
Wanneer de meeste mensen nadenken 
over wat slim is en wat dom is,

00:06:14.363 --> 00:06:17.386
maken ze zich misschien deze voorstelling.

00:06:17.386 --> 00:06:19.984
Helemaal links hebben we de dorpsgek,

00:06:19.984 --> 00:06:22.467
en ver aan de andere zijde

00:06:22.467 --> 00:06:27.223
hebben we Ed Witten, of Albert Einstein, 
of wie je favoriete goeroe ook is.

00:06:27.223 --> 00:06:31.057
Maar vanuit het oogpunt 
van kunstmatige intelligentie,

00:06:31.057 --> 00:06:34.738
ziet het er waarschijnlijk meer zo uit:

00:06:35.258 --> 00:06:38.636
AI begint op dit punt hier, 
op nul intelligentie,

00:06:38.636 --> 00:06:41.647
en dan, na vele, 
vele jaren van hard werken,

00:06:41.647 --> 00:06:45.491
krijgen we misschien uiteindelijk 
kunstmatige intelligentie van muisniveau,

00:06:45.491 --> 00:06:47.921
iets dat kan navigeren 
in rommelige omgevingen

00:06:47.921 --> 00:06:49.908
zoals een muis.

00:06:49.908 --> 00:06:54.221
Na nog eens vele jaren 
van hard werk, veel investeringen,

00:06:54.221 --> 00:06:58.860
halen we misschien uiteindelijk 
AI van chimpanseeniveau.

00:06:58.860 --> 00:07:02.070
Na nog meer jaren 
van echt, echt hard werken,

00:07:02.070 --> 00:07:04.983
krijgen we kunstmatige intelligentie
op dorpsgekniveau.

00:07:05.147 --> 00:07:08.147
En wat later zijn we verder dan Ed Witten.

00:07:08.255 --> 00:07:11.225
Maar de trein stopt niet 
bij station Mens.

00:07:11.225 --> 00:07:14.247
Hij suist er waarschijnlijk 
eerder snel voorbij.

00:07:14.247 --> 00:07:16.231
Nu heeft dit ingrijpende gevolgen,

00:07:16.231 --> 00:07:20.093
in het bijzonder als het gaat 
om vragen van macht.

00:07:20.093 --> 00:07:21.992
Zo zijn chimpansees sterk -

00:07:21.992 --> 00:07:27.214
voor hetzelfde gewicht ongeveer twee keer 
zo sterk als een fitte menselijke man.

00:07:27.214 --> 00:07:31.828
En toch hangt het lot van Kanzi 
en zijn vriendjes

00:07:31.828 --> 00:07:35.968
veel meer af van wat wij mensen doen 
dan van wat de chimpansees zelf doen.

00:07:37.228 --> 00:07:39.542
Zodra er superintelligentie is,

00:07:39.542 --> 00:07:43.381
kan het lot van de mensheid afhangen 
van wat de superintelligentie doet.

00:07:44.451 --> 00:07:45.508
Denk er over na:

00:07:45.508 --> 00:07:50.552
machine-intelligentie zal de laatste 
uitvinding van de mensheid zijn.

00:07:50.552 --> 00:07:53.525
Machines zullen dan beter zijn 
in uitvinden dan wij,

00:07:53.525 --> 00:07:56.065
en ze zullen het doen 
op digitale tijdschalen.

00:07:56.065 --> 00:08:00.966
Dit betekent de toekomst
'telescoperen'.

00:08:00.966 --> 00:08:04.364
Denk aan alle gekke technologieën 
die je je zou kunnen inbeelden,

00:08:04.364 --> 00:08:07.542
die mensen misschien zullen ontwikkelen 
in de loop van de tijd:

00:08:07.542 --> 00:08:10.580
behandelingen voor veroudering, 
kolonisatie van de ruimte,

00:08:10.580 --> 00:08:14.311
zelfreplicerende nanobots 
of geesten uploaden in computers,

00:08:14.311 --> 00:08:16.470
allerlei science-fictionachtige dingen

00:08:16.470 --> 00:08:19.207
maar toch in overeenstemming 
met de wetten van de fysica.

00:08:19.207 --> 00:08:22.789
Dit alles zou superintelligentie 
kunnen ontwikkelen,

00:08:22.789 --> 00:08:24.449
en eventueel vrij snel.

00:08:24.449 --> 00:08:28.007
Een superintelligentie met een dergelijke 
technologische maturiteit

00:08:28.007 --> 00:08:30.186
zou zeer krachtig zijn,

00:08:30.186 --> 00:08:32.692
en zou, ten minste 
in een aantal scenario's,

00:08:32.692 --> 00:08:34.732
in staat zijn om te krijgen wat ze wil.

00:08:34.732 --> 00:08:40.393
Onze toekomst zou dan afhangen 
van de voorkeuren van deze AI.

00:08:41.855 --> 00:08:45.604
De vraag is wat die voorkeuren zijn.

00:08:45.924 --> 00:08:47.563
Hier wordt het lastiger.

00:08:47.563 --> 00:08:49.448
Om hierin vooruitgang te boeken,

00:08:49.448 --> 00:08:52.724
moeten we in de eerste plaats 
antropomorfiseren vermijden.

00:08:53.933 --> 00:08:57.235
Ironisch genoeg vind je 
bij elk krantenartikel

00:08:57.235 --> 00:09:01.090
over de toekomst van AI 
iets dergelijks: [dia]

00:09:02.280 --> 00:09:06.414
We moeten het probleem 
meer abstract benaderen,

00:09:06.414 --> 00:09:09.204
niet in termen van Hollywoodscenario's.

00:09:09.204 --> 00:09:12.821
We moeten intelligentie zien 
als een optimalisatieproces,

00:09:12.821 --> 00:09:18.470
een proces dat de toekomst stuurt 
naar een bepaalde set van configuraties.

00:09:18.470 --> 00:09:21.691
Een superintelligentie is echt 
een sterk optimalisatieproces.

00:09:21.691 --> 00:09:24.828
Ze is zeer goed in het gebruik 
van de beschikbare middelen

00:09:24.828 --> 00:09:26.488
om een ​​toestand te bereiken

00:09:26.488 --> 00:09:28.417
waar haar doel wordt gerealiseerd.

00:09:28.447 --> 00:09:31.119
Dit betekent dat er 
geen noodzakelijk verband is tussen

00:09:31.119 --> 00:09:33.853
in deze zin zeer intelligent zijn

00:09:33.853 --> 00:09:35.973
en een doelstelling hebben

00:09:35.973 --> 00:09:39.221
die wij mensen de moeite waard 
of zinvol zouden vinden.

00:09:39.221 --> 00:09:43.115
Stel dat we een AI de opdracht geven 
om mensen te laten glimlachen.

00:09:43.115 --> 00:09:46.097
Als de AI zwak is, 
voert ze nuttige of amusante acties uit

00:09:46.097 --> 00:09:48.614
om de gebruiker te laten glimlachen.

00:09:48.614 --> 00:09:51.031
Maar een superintelligente AI 
realiseert zich

00:09:51.031 --> 00:09:54.554
dat er een effectievere manier is 
om dit doel te bereiken:

00:09:54.554 --> 00:09:56.476
neem de controle van de wereld over,

00:09:56.476 --> 00:09:59.638
plak elektroden 
op de gezichtsspieren

00:09:59.638 --> 00:10:02.399
en veroorzaak een constante, 
stralende grijns.

00:10:02.399 --> 00:10:04.344
Een ander voorbeeld,
we willen de AI

00:10:04.344 --> 00:10:06.997
een ​​moeilijk wiskundig 
probleem laten oplossen.

00:10:06.997 --> 00:10:08.934
Wanneer de AI superintelligent wordt,

00:10:08.934 --> 00:10:13.105
vindt ze de meest effectieve manier 
om dit probleem op te lossen:

00:10:13.105 --> 00:10:16.035
door de planeet om te vormen
in een gigantische computer,

00:10:16.035 --> 00:10:18.281
voor meer denkvermogen.

00:10:18.281 --> 00:10:21.045
Merk op dat dit de AI 
een instrumentele rede geeft

00:10:21.045 --> 00:10:23.561
om ons dingen aan te doen 
die we niet zouden willen.

00:10:23.561 --> 00:10:25.896
Hier zijn menselijke 
wezens bedreigingen,

00:10:25.896 --> 00:10:29.137
die in de weg staan van de oplossing 
van het probleem.

00:10:29.207 --> 00:10:32.701
Natuurlijk zal het zo niet gaan.

00:10:32.701 --> 00:10:34.194
Dit zijn cartoon-voorbeelden.

00:10:34.194 --> 00:10:36.633
Maar het algemene punt blijft belangrijk:

00:10:36.633 --> 00:10:39.266
als je een echt krachtig 
optimalisatieproces creëert

00:10:39.266 --> 00:10:41.500
om objectief x te maximaliseren,

00:10:41.500 --> 00:10:44.386
kun je er beter voor zorgen 
dat je definitie van x

00:10:44.386 --> 00:10:46.475
alles omvat waar je om geeft.

00:10:46.835 --> 00:10:51.219
Deze les vind je terug 
in een aantal mythes.

00:10:51.219 --> 00:10:56.517
Koning Midas wenst dat alles 
wat hij aanraakt, verandert in goud.

00:10:56.517 --> 00:10:59.378
Hij raakt zijn dochter aan, 
ze verandert in goud.

00:10:59.378 --> 00:11:01.931
Hij raakt zijn eten aan, 
het verandert in goud.

00:11:01.931 --> 00:11:04.520
Dit kan praktisch relevant worden,

00:11:04.520 --> 00:11:06.590
niet alleen als metafoor voor hebzucht,

00:11:06.590 --> 00:11:08.845
maar als voorbeeld van wat er gebeurt

00:11:08.845 --> 00:11:11.322
als je een krachtig 
optimalisatieproces creëert

00:11:11.322 --> 00:11:15.061
en het onjuiste of slecht 
omschreven doelen geef.

00:11:15.061 --> 00:11:18.300
Nu zou je kunnen zeggen 
dat als een computer elektroden

00:11:18.300 --> 00:11:21.300
in de gezichten van mensen 
begint te steken,

00:11:21.300 --> 00:11:23.565
we hem gewoon uitschakelen.

00:11:24.555 --> 00:11:26.895
A: misschien niet zo gemakkelijk om doen

00:11:26.895 --> 00:11:29.895
als we te afhankelijk 
van het systeem zijn geworden -

00:11:29.895 --> 00:11:32.627
zoals, waar is de uitschakelaar 
van het internet?

00:11:32.627 --> 00:11:37.127
B: waarom hebben de chimpansees 
of de neanderthalers

00:11:37.127 --> 00:11:39.468
de schakelaar voor de mensheid 
niet uitgezet?

00:11:39.468 --> 00:11:41.964
Ze hadden zeker redenen.

00:11:41.964 --> 00:11:44.759
We hebben een uitschakelaar, 
deze bijvoorbeeld.

00:11:44.759 --> 00:11:46.313
(Knijpt keel toe)

00:11:46.313 --> 00:11:49.238
Wij zijn intelligente tegenstanders.

00:11:49.238 --> 00:11:52.066
Wij kunnen anticiperen op bedreigingen 
en ze zo vermijden.

00:11:52.066 --> 00:11:54.470
Maar een superintelligentie kan dat ook,

00:11:54.470 --> 00:11:57.724
en veel beter dan wij.

00:11:57.724 --> 00:12:04.911
We moeten er niet te zeker van zijn 
dat we dit onder controle houden.

00:12:04.911 --> 00:12:08.358
We kunnen proberen ons werk 
te vergemakkelijken door bijvoorbeeld

00:12:08.358 --> 00:12:09.948
de AI in een doos te stoppen,

00:12:09.948 --> 00:12:11.494
een veilige softwareomgeving,

00:12:11.494 --> 00:12:14.956
een virtual reality simulatie 
van waaruit ze niet kan ontsnappen.

00:12:14.956 --> 00:12:18.912
Maar hoe zeker kunnen we zijn dat 
de AI daar geen bug in zou vinden?

00:12:18.912 --> 00:12:22.081
Zelfs louter menselijke hackers 
vinden de hele tijd bugs.

00:12:22.081 --> 00:12:25.117
Daar moeten we 
maar niet te zelfverzekerd in zijn.

00:12:26.237 --> 00:12:30.785
De ethernet-kabel loskoppelen 
en een luchtspleet maken?

00:12:30.785 --> 00:12:32.553
Nogmaals, louter menselijke hackers

00:12:32.553 --> 00:12:36.364
omzeilen de hele tijd luchtspleten 
door ‘social engineering’.

00:12:36.364 --> 00:12:39.053
Ik ben er zeker van 
dat op dit eigenste moment

00:12:39.053 --> 00:12:43.782
een werknemer ergens
haar accountgegevens meedeelt

00:12:43.828 --> 00:12:46.574
aan iemand die beweert 
van de IT-afdeling te zijn.

00:12:46.574 --> 00:12:48.701
Nog creatievere scenario's zijn mogelijk,

00:12:48.701 --> 00:12:50.766
Als AI zou je kunnen bedenken hoe je,

00:12:50.766 --> 00:12:54.488
door wat met je elektroden 
in je interne circuit te wiebelen,

00:12:54.488 --> 00:12:57.010
radiogolven kunt maken om te communiceren.

00:12:57.010 --> 00:12:59.434
Of misschien kan je een storing simuleren,

00:12:59.434 --> 00:13:02.931
en wanneer de programmeurs je openen 
om te zien wat er mis ging

00:13:02.931 --> 00:13:04.867
en kijken naar de broncode, kan - Bam! -

00:13:04.867 --> 00:13:07.314
de manipulatie plaatsvinden.

00:13:07.314 --> 00:13:10.744
Of ze kan de blauwdruk ontwerpen
voor een echt handige technologie,

00:13:10.744 --> 00:13:12.142
en zodra we ze implementeren,

00:13:12.142 --> 00:13:16.539
heeft ze een aantal heimelijke 
neveneffecten die de AI had gepland.

00:13:16.539 --> 00:13:20.002
Het komt erop neer 
dat we er niet op mogen vertrouwen

00:13:20.002 --> 00:13:21.862
dat we een superintelligente geest

00:13:21.862 --> 00:13:24.670
voor altijd in zijn fles 
opgesloten kunnen houden.

00:13:24.670 --> 00:13:26.774
Vroeg of laat ontsnapt hij.

00:13:27.034 --> 00:13:30.137
Het antwoord hierop is, 
achterhalen hoe

00:13:30.137 --> 00:13:35.161
je een superintelligente AI zo ontwerpt 
dat zelfs als - wanneer - ze ontsnapt,

00:13:35.161 --> 00:13:38.438
ze nog veilig is doordat ze 
fundamenteel aan onze kant staat,

00:13:38.438 --> 00:13:40.337
ze onze waarden deelt.

00:13:40.337 --> 00:13:43.547
Ik zie geen manier om 
dit moeilijke probleem uit de weg te gaan.

00:13:44.557 --> 00:13:48.391
Maar ik ben eigenlijk vrij optimistisch 
dat dit probleem kan worden opgelost.

00:13:48.391 --> 00:13:52.294
Het zal niet nodig zijn een lange lijst 
van alles waar we om geven op te stellen,

00:13:52.294 --> 00:13:55.047
of erger nog, ze in een computertaal

00:13:55.047 --> 00:13:57.391
zoals C++ of Python op te schrijven,

00:13:57.391 --> 00:14:00.158
dat zou een hopeloze taak zijn.

00:14:00.158 --> 00:14:04.455
We moeten een AI creëren 
die haar intelligentie gebruikt

00:14:04.455 --> 00:14:07.226
om te leren wat wij waarderen,

00:14:07.226 --> 00:14:12.506
en haar motivatiesysteem zo construeren 
dat ze gemotiveerd is

00:14:12.506 --> 00:14:17.738
om onze waarden na te streven of 
om te doen wat wij zouden goedkeuren.

00:14:17.738 --> 00:14:21.152
We zouden haar intelligentie 
als hefboom gebruiken om zo veel mogelijk

00:14:21.152 --> 00:14:23.897
het probleem van 'waardenopslag' 
op te lossen.

00:14:24.727 --> 00:14:26.239
Dit kan,

00:14:26.239 --> 00:14:29.835
en de uitkomst zou zeer goed 
voor de mensheid kunnen zijn.

00:14:29.835 --> 00:14:33.792
Maar het gebeurt niet automatisch.

00:14:33.792 --> 00:14:36.790
De initiële voorwaarden 
voor de intelligentie-explosie

00:14:36.790 --> 00:14:39.653
moeten op de juiste manier worden opgezet

00:14:39.653 --> 00:14:42.553
als we een gecontroleerde 
ontploffing willen.

00:14:42.553 --> 00:14:44.971
De waarden van die AI 
moeten passen bij de onze,

00:14:44.971 --> 00:14:47.011
niet alleen in de vertrouwde context,

00:14:47.011 --> 00:14:49.999
waar we het gedag van de AI 
gemakkelijk kunnen controleren,

00:14:49.999 --> 00:14:53.233
maar ook in alle nieuwe contexten 
die de AI zou kunnen tegenkomen

00:14:53.233 --> 00:14:54.790
in de onbepaalde toekomst.

00:14:54.790 --> 00:14:58.527
Een aantal esoterische kwesties 
moeten worden opgelost:

00:14:58.527 --> 00:15:01.616
de exacte details 
van haar beslissingstheorie,

00:15:01.616 --> 00:15:04.480
hoe omgaan met logische onzekerheid, 
enzovoort, enzovoort.

00:15:05.330 --> 00:15:08.432
De technische problemen 
om dit te laten werken

00:15:08.432 --> 00:15:09.545
lijken heel moeilijk -

00:15:09.545 --> 00:15:12.925
niet zo moeilijk als het maken 
van een superintelligente AI,

00:15:12.925 --> 00:15:15.793
maar behoorlijk moeilijk.

00:15:15.793 --> 00:15:17.488
Hier is de zorg:

00:15:17.488 --> 00:15:22.172
Het maken van superintelligente AI 
is echt een lastige uitdaging.

00:15:22.172 --> 00:15:24.720
Een veilige superintelligente AI maken,

00:15:24.720 --> 00:15:27.136
houdt daarenboven een extra uitdaging in.

00:15:28.216 --> 00:15:31.703
Het risico bestaat 
dat iemand de eerste uitdaging oplost

00:15:31.703 --> 00:15:34.704
zonder de extra uitdaging 
van perfecte veiligheid

00:15:34.704 --> 00:15:36.605
te hebben opgelost.

00:15:37.375 --> 00:15:40.706
Ik denk dat we dat controleprobleem

00:15:40.706 --> 00:15:43.528
vooraf moeten oplossen,

00:15:43.528 --> 00:15:46.828
zodat we erover beschikken 
tegen de tijd dat het nodig is.

00:15:46.828 --> 00:15:50.275
Misschien kan het hele controleprobleem 
niet op voorhand worden opgelost

00:15:50.275 --> 00:15:53.299
omdat sommige elementen misschien 
pas kunnen worden aangebracht

00:15:53.299 --> 00:15:55.579
als we de details kennen 
van de architectuur,

00:15:55.579 --> 00:15:57.296
waarin het zal worden uitgevoerd.

00:15:57.296 --> 00:16:00.676
Maar hoe beter we het controleprobleem 
vooraf oplossen,

00:16:00.676 --> 00:16:04.766
hoe groter de kans dat de overgang 
naar machine-intelligentie

00:16:04.766 --> 00:16:06.306
goed zal verlopen.

00:16:06.306 --> 00:16:10.950
Dit lijkt me iets 
dat zeker de moeite waard is

00:16:10.950 --> 00:16:14.282
en ik kan me voorstellen 
dat als alles goed afloopt,

00:16:14.282 --> 00:16:18.940
mensen een miljoen jaar na nu 
naar deze eeuw terugkijken

00:16:18.940 --> 00:16:22.942
en zeggen dat het enige wat we deden 
dat er echt toe deed,

00:16:22.942 --> 00:16:24.509
was dit ding in orde krijgen.

00:16:24.509 --> 00:16:26.198
Dankjewel.

00:16:26.198 --> 00:16:29.011
(Applaus)

