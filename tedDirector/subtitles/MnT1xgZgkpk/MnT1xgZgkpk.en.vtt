WEBVTT
Kind: captions
Language: en

00:00:12.570 --> 00:00:16.777
I work with a bunch of mathematicians,
philosophers and computer scientists,

00:00:16.777 --> 00:00:21.986
and we sit around and think about
the future of machine intelligence,

00:00:21.986 --> 00:00:24.030
among other things.

00:00:24.030 --> 00:00:28.755
Some people think that some of these
things are sort of science fiction-y,

00:00:28.755 --> 00:00:31.856
far out there, crazy.

00:00:31.856 --> 00:00:33.326
But I like to say,

00:00:33.326 --> 00:00:36.930
okay, let's look at the modern
human condition.

00:00:36.930 --> 00:00:38.622
(Laughter)

00:00:38.622 --> 00:00:41.024
This is the normal way for things to be.

00:00:41.024 --> 00:00:43.309
But if we think about it,

00:00:43.309 --> 00:00:46.602
we are actually recently arrived
guests on this planet,

00:00:46.602 --> 00:00:48.684
the human species.

00:00:48.684 --> 00:00:53.430
Think about if Earth
was created one year ago,

00:00:53.430 --> 00:00:56.978
the human species, then, 
would be 10 minutes old.

00:00:56.978 --> 00:01:00.146
The industrial era started
two seconds ago.

00:01:01.276 --> 00:01:06.501
Another way to look at this is to think of
world GDP over the last 10,000 years,

00:01:06.501 --> 00:01:09.530
I've actually taken the trouble
to plot this for you in a graph.

00:01:09.530 --> 00:01:11.304
It looks like this.

00:01:11.304 --> 00:01:12.667
(Laughter)

00:01:12.667 --> 00:01:14.818
It's a curious shape
for a normal condition.

00:01:14.818 --> 00:01:16.516
I sure wouldn't want to sit on it.

00:01:16.516 --> 00:01:19.067
(Laughter)

00:01:19.067 --> 00:01:23.841
Let's ask ourselves, what is the cause
of this current anomaly?

00:01:23.841 --> 00:01:26.393
Some people would say it's technology.

00:01:26.393 --> 00:01:31.061
Now it's true, technology has accumulated
through human history,

00:01:31.061 --> 00:01:35.713
and right now, technology
advances extremely rapidly --

00:01:35.713 --> 00:01:37.278
that is the proximate cause,

00:01:37.278 --> 00:01:39.843
that's why we are currently 
so very productive.

00:01:40.473 --> 00:01:44.134
But I like to think back further 
to the ultimate cause.

00:01:45.114 --> 00:01:48.880
Look at these two highly
distinguished gentlemen:

00:01:48.880 --> 00:01:50.480
We have Kanzi --

00:01:50.480 --> 00:01:55.123
he's mastered 200 lexical
tokens, an incredible feat.

00:01:55.123 --> 00:01:58.817
And Ed Witten unleashed the second
superstring revolution.

00:01:58.817 --> 00:02:01.141
If we look under the hood, 
this is what we find:

00:02:01.141 --> 00:02:02.711
basically the same thing.

00:02:02.711 --> 00:02:04.524
One is a little larger,

00:02:04.524 --> 00:02:07.282
it maybe also has a few tricks
in the exact way it's wired.

00:02:07.282 --> 00:02:11.094
These invisible differences cannot
be too complicated, however,

00:02:11.094 --> 00:02:15.379
because there have only
been 250,000 generations

00:02:15.379 --> 00:02:17.111
since our last common ancestor.

00:02:17.111 --> 00:02:20.960
We know that complicated mechanisms
take a long time to evolve.

00:02:22.000 --> 00:02:24.499
So a bunch of relatively minor changes

00:02:24.499 --> 00:02:27.566
take us from Kanzi to Witten,

00:02:27.566 --> 00:02:32.109
from broken-off tree branches
to intercontinental ballistic missiles.

00:02:32.839 --> 00:02:36.774
So this then seems pretty obvious
that everything we've achieved,

00:02:36.774 --> 00:02:38.152
and everything we care about,

00:02:38.152 --> 00:02:43.380
depends crucially on some relatively minor
changes that made the human mind.

00:02:44.650 --> 00:02:48.312
And the corollary, of course,
is that any further changes

00:02:48.312 --> 00:02:51.789
that could significantly change
the substrate of thinking

00:02:51.789 --> 00:02:54.991
could have potentially 
enormous consequences.

00:02:56.321 --> 00:02:59.226
Some of my colleagues 
think we're on the verge

00:02:59.226 --> 00:03:03.134
of something that could cause
a profound change in that substrate,

00:03:03.134 --> 00:03:06.347
and that is machine superintelligence.

00:03:06.347 --> 00:03:11.086
Artificial intelligence used to be
about putting commands in a box.

00:03:11.086 --> 00:03:12.751
You would have human programmers

00:03:12.751 --> 00:03:15.886
that would painstakingly 
handcraft knowledge items.

00:03:15.886 --> 00:03:17.972
You build up these expert systems,

00:03:17.972 --> 00:03:20.296
and they were kind of useful 
for some purposes,

00:03:20.296 --> 00:03:22.977
but they were very brittle,
you couldn't scale them.

00:03:22.977 --> 00:03:26.410
Basically, you got out only
what you put in.

00:03:26.410 --> 00:03:27.407
But since then,

00:03:27.407 --> 00:03:30.874
a paradigm shift has taken place
in the field of artificial intelligence.

00:03:30.874 --> 00:03:33.644
Today, the action is really 
around machine learning.

00:03:34.394 --> 00:03:39.781
So rather than handcrafting knowledge
representations and features,

00:03:40.511 --> 00:03:46.065
we create algorithms that learn,
often from raw perceptual data.

00:03:46.065 --> 00:03:51.063
Basically the same thing
that the human infant does.

00:03:51.063 --> 00:03:55.270
The result is A.I. that is not
limited to one domain --

00:03:55.270 --> 00:03:59.901
the same system can learn to translate 
between any pairs of languages,

00:03:59.901 --> 00:04:05.338
or learn to play any computer game
on the Atari console.

00:04:05.338 --> 00:04:07.117
Now of course,

00:04:07.117 --> 00:04:11.116
A.I. is still nowhere near having
the same powerful, cross-domain

00:04:11.116 --> 00:04:14.335
ability to learn and plan
as a human being has.

00:04:14.335 --> 00:04:16.461
The cortex still has some 
algorithmic tricks

00:04:16.461 --> 00:04:18.816
that we don't yet know
how to match in machines.

00:04:19.886 --> 00:04:21.785
So the question is,

00:04:21.785 --> 00:04:25.285
how far are we from being able
to match those tricks?

00:04:26.245 --> 00:04:27.328
A couple of years ago,

00:04:27.328 --> 00:04:30.216
we did a survey of some of the world's 
leading A.I. experts,

00:04:30.216 --> 00:04:33.440
to see what they think,
and one of the questions we asked was,

00:04:33.440 --> 00:04:36.793
"By which year do you think
there is a 50 percent probability

00:04:36.793 --> 00:04:40.275
that we will have achieved 
human-level machine intelligence?"

00:04:40.785 --> 00:04:44.968
We defined human-level here 
as the ability to perform

00:04:44.968 --> 00:04:47.839
almost any job at least as well
as an adult human,

00:04:47.839 --> 00:04:51.844
so real human-level, not just
within some limited domain.

00:04:51.844 --> 00:04:55.494
And the median answer was 2040 or 2050,

00:04:55.494 --> 00:04:58.300
depending on precisely which 
group of experts we asked.

00:04:58.300 --> 00:05:02.339
Now, it could happen much,
much later, or sooner,

00:05:02.339 --> 00:05:04.279
the truth is nobody really knows.

00:05:05.259 --> 00:05:09.671
What we do know is that the ultimate 
limit to information processing

00:05:09.671 --> 00:05:14.542
in a machine substrate lies far outside 
the limits in biological tissue.

00:05:15.241 --> 00:05:17.619
This comes down to physics.

00:05:17.619 --> 00:05:22.337
A biological neuron fires, maybe, 
at 200 hertz, 200 times a second.

00:05:22.337 --> 00:05:25.931
But even a present-day transistor
operates at the Gigahertz.

00:05:25.931 --> 00:05:31.228
Neurons propagate slowly in axons,
100 meters per second, tops.

00:05:31.228 --> 00:05:34.339
But in computers, signals can travel
at the speed of light.

00:05:35.079 --> 00:05:36.948
There are also size limitations,

00:05:36.948 --> 00:05:39.975
like a human brain has 
to fit inside a cranium,

00:05:39.975 --> 00:05:44.736
but a computer can be the size
of a warehouse or larger.

00:05:44.736 --> 00:05:50.335
So the potential for superintelligence 
lies dormant in matter,

00:05:50.335 --> 00:05:56.047
much like the power of the atom 
lay dormant throughout human history,

00:05:56.047 --> 00:06:00.452
patiently waiting there until 1945.

00:06:00.452 --> 00:06:01.700
In this century,

00:06:01.700 --> 00:06:05.818
scientists may learn to awaken
the power of artificial intelligence.

00:06:05.818 --> 00:06:09.826
And I think we might then see
an intelligence explosion.

00:06:10.406 --> 00:06:14.363
Now most people, when they think
about what is smart and what is dumb,

00:06:14.363 --> 00:06:17.386
I think have in mind a picture
roughly like this.

00:06:17.386 --> 00:06:19.984
So at one end we have the village idiot,

00:06:19.984 --> 00:06:22.467
and then far over at the other side

00:06:22.467 --> 00:06:27.223
we have Ed Witten, or Albert Einstein,
or whoever your favorite guru is.

00:06:27.223 --> 00:06:31.057
But I think that from the point of view
of artificial intelligence,

00:06:31.057 --> 00:06:34.738
the true picture is actually
probably more like this:

00:06:35.258 --> 00:06:38.636
AI starts out at this point here,
at zero intelligence,

00:06:38.636 --> 00:06:41.647
and then, after many, many 
years of really hard work,

00:06:41.647 --> 00:06:45.491
maybe eventually we get to
mouse-level artificial intelligence,

00:06:45.491 --> 00:06:47.921
something that can navigate 
cluttered environments

00:06:47.921 --> 00:06:49.908
as well as a mouse can.

00:06:49.908 --> 00:06:54.221
And then, after many, many more years
of really hard work, lots of investment,

00:06:54.221 --> 00:06:58.860
maybe eventually we get to
chimpanzee-level artificial intelligence.

00:06:58.860 --> 00:07:02.070
And then, after even more years 
of really, really hard work,

00:07:02.070 --> 00:07:04.983
we get to village idiot 
artificial intelligence.

00:07:04.983 --> 00:07:08.255
And a few moments later, 
we are beyond Ed Witten.

00:07:08.255 --> 00:07:11.225
The train doesn't stop
at Humanville Station.

00:07:11.225 --> 00:07:14.247
It's likely, rather, to swoosh right by.

00:07:14.247 --> 00:07:16.231
Now this has profound implications,

00:07:16.231 --> 00:07:20.093
particularly when it comes 
to questions of power.

00:07:20.093 --> 00:07:21.992
For example, chimpanzees are strong --

00:07:21.992 --> 00:07:27.214
pound for pound, a chimpanzee is about
twice as strong as a fit human male.

00:07:27.214 --> 00:07:31.828
And yet, the fate of Kanzi 
and his pals depends a lot more

00:07:31.828 --> 00:07:35.968
on what we humans do than on
what the chimpanzees do themselves.

00:07:37.228 --> 00:07:39.542
Once there is superintelligence,

00:07:39.542 --> 00:07:43.381
the fate of humanity may depend
on what the superintelligence does.

00:07:44.451 --> 00:07:45.508
Think about it:

00:07:45.508 --> 00:07:50.552
Machine intelligence is the last invention
that humanity will ever need to make.

00:07:50.552 --> 00:07:53.525
Machines will then be better 
at inventing than we are,

00:07:53.525 --> 00:07:56.065
and they'll be doing so 
on digital timescales.

00:07:56.065 --> 00:08:00.966
What this means is basically
a telescoping of the future.

00:08:00.966 --> 00:08:04.524
Think of all the crazy technologies 
that you could have imagined

00:08:04.524 --> 00:08:07.322
maybe humans could have developed
in the fullness of time:

00:08:07.322 --> 00:08:10.580
cures for aging, space colonization,

00:08:10.580 --> 00:08:14.311
self-replicating nanobots or uploading
of minds into computers,

00:08:14.311 --> 00:08:16.470
all kinds of science fiction-y stuff

00:08:16.470 --> 00:08:19.207
that's nevertheless consistent 
with the laws of physics.

00:08:19.207 --> 00:08:23.419
All of this superintelligence could 
develop, and possibly quite rapidly.

00:08:24.449 --> 00:08:28.007
Now, a superintelligence with such 
technological maturity

00:08:28.007 --> 00:08:30.186
would be extremely powerful,

00:08:30.186 --> 00:08:34.732
and at least in some scenarios,
it would be able to get what it wants.

00:08:34.732 --> 00:08:40.393
We would then have a future that would
be shaped by the preferences of this A.I.

00:08:41.855 --> 00:08:45.604
Now a good question is,
what are those preferences?

00:08:46.244 --> 00:08:48.013
Here it gets trickier.

00:08:48.013 --> 00:08:49.448
To make any headway with this,

00:08:49.448 --> 00:08:52.724
we must first of all
avoid anthropomorphizing.

00:08:53.934 --> 00:08:57.235
And this is ironic because 
every newspaper article

00:08:57.235 --> 00:09:01.090
about the future of A.I.
has a picture of this:

00:09:02.280 --> 00:09:06.414
So I think what we need to do is
to conceive of the issue more abstractly,

00:09:06.414 --> 00:09:09.204
not in terms of vivid Hollywood scenarios.

00:09:09.204 --> 00:09:12.821
We need to think of intelligence 
as an optimization process,

00:09:12.821 --> 00:09:18.470
a process that steers the future
into a particular set of configurations.

00:09:18.470 --> 00:09:21.981
A superintelligence is
a really strong optimization process.

00:09:21.981 --> 00:09:26.098
It's extremely good at using 
available means to achieve a state

00:09:26.098 --> 00:09:28.007
in which its goal is realized.

00:09:28.447 --> 00:09:31.119
This means that there is no necessary
connection between

00:09:31.119 --> 00:09:33.853
being highly intelligent in this sense,

00:09:33.853 --> 00:09:38.515
and having an objective that we humans
would find worthwhile or meaningful.

00:09:39.321 --> 00:09:43.115
Suppose we give an A.I. the goal 
to make humans smile.

00:09:43.115 --> 00:09:46.097
When the A.I. is weak, it performs useful
or amusing actions

00:09:46.097 --> 00:09:48.614
that cause its user to smile.

00:09:48.614 --> 00:09:51.031
When the A.I. becomes superintelligent,

00:09:51.031 --> 00:09:54.554
it realizes that there is a more
effective way to achieve this goal:

00:09:54.554 --> 00:09:56.476
take control of the world

00:09:56.476 --> 00:09:59.638
and stick electrodes into the facial
muscles of humans

00:09:59.638 --> 00:10:02.579
to cause constant, beaming grins.

00:10:02.579 --> 00:10:03.614
Another example,

00:10:03.614 --> 00:10:06.997
suppose we give A.I. the goal to solve
a difficult mathematical problem.

00:10:06.997 --> 00:10:08.934
When the A.I. becomes superintelligent,

00:10:08.934 --> 00:10:13.105
it realizes that the most effective way 
to get the solution to this problem

00:10:13.105 --> 00:10:16.035
is by transforming the planet
into a giant computer,

00:10:16.035 --> 00:10:18.281
so as to increase its thinking capacity.

00:10:18.281 --> 00:10:21.045
And notice that this gives the A.I.s
an instrumental reason

00:10:21.045 --> 00:10:23.561
to do things to us that we
might not approve of.

00:10:23.561 --> 00:10:25.496
Human beings in this model are threats,

00:10:25.496 --> 00:10:28.417
we could prevent the mathematical
problem from being solved.

00:10:29.207 --> 00:10:32.701
Of course, perceivably things won't 
go wrong in these particular ways;

00:10:32.701 --> 00:10:34.454
these are cartoon examples.

00:10:34.454 --> 00:10:36.393
But the general point here is important:

00:10:36.393 --> 00:10:39.266
if you create a really powerful
optimization process

00:10:39.266 --> 00:10:41.500
to maximize for objective x,

00:10:41.500 --> 00:10:43.776
you better make sure 
that your definition of x

00:10:43.776 --> 00:10:46.245
incorporates everything you care about.

00:10:46.835 --> 00:10:51.219
This is a lesson that's also taught
in many a myth.

00:10:51.219 --> 00:10:56.517
King Midas wishes that everything
he touches be turned into gold.

00:10:56.517 --> 00:10:59.378
He touches his daughter, 
she turns into gold.

00:10:59.378 --> 00:11:01.931
He touches his food, it turns into gold.

00:11:01.931 --> 00:11:04.520
This could become practically relevant,

00:11:04.520 --> 00:11:06.590
not just as a metaphor for greed,

00:11:06.590 --> 00:11:08.485
but as an illustration of what happens

00:11:08.485 --> 00:11:11.322
if you create a powerful
optimization process

00:11:11.322 --> 00:11:16.111
and give it misconceived 
or poorly specified goals.

00:11:16.111 --> 00:11:21.300
Now you might say, if a computer starts
sticking electrodes into people's faces,

00:11:21.300 --> 00:11:23.565
we'd just shut it off.

00:11:24.555 --> 00:11:29.895
A, this is not necessarily so easy to do
if we've grown dependent on the system --

00:11:29.895 --> 00:11:32.627
like, where is the off switch 
to the Internet?

00:11:32.627 --> 00:11:37.747
B, why haven't the chimpanzees
flicked the off switch to humanity,

00:11:37.747 --> 00:11:39.298
or the Neanderthals?

00:11:39.298 --> 00:11:41.964
They certainly had reasons.

00:11:41.964 --> 00:11:44.759
We have an off switch, 
for example, right here.

00:11:44.759 --> 00:11:46.313
(Choking)

00:11:46.313 --> 00:11:49.238
The reason is that we are 
an intelligent adversary;

00:11:49.238 --> 00:11:51.966
we can anticipate threats 
and plan around them.

00:11:51.966 --> 00:11:54.470
But so could a superintelligent agent,

00:11:54.470 --> 00:11:57.724
and it would be much better 
at that than we are.

00:11:57.724 --> 00:12:04.911
The point is, we should not be confident
that we have this under control here.

00:12:04.911 --> 00:12:08.358
And we could try to make our job
a little bit easier by, say,

00:12:08.358 --> 00:12:09.948
putting the A.I. in a box,

00:12:09.948 --> 00:12:11.744
like a secure software environment,

00:12:11.744 --> 00:12:14.766
a virtual reality simulation
from which it cannot escape.

00:12:14.766 --> 00:12:18.912
But how confident can we be that
the A.I. couldn't find a bug.

00:12:18.912 --> 00:12:22.081
Given that merely human hackers
find bugs all the time,

00:12:22.081 --> 00:12:25.117
I'd say, probably not very confident.

00:12:26.237 --> 00:12:30.785
So we disconnect the ethernet cable
to create an air gap,

00:12:30.785 --> 00:12:33.453
but again, like merely human hackers

00:12:33.453 --> 00:12:36.834
routinely transgress air gaps
using social engineering.

00:12:36.834 --> 00:12:38.093
Right now, as I speak,

00:12:38.093 --> 00:12:40.482
I'm sure there is some employee
out there somewhere

00:12:40.482 --> 00:12:43.828
who has been talked into handing out 
her account details

00:12:43.828 --> 00:12:46.574
by somebody claiming to be
from the I.T. department.

00:12:46.574 --> 00:12:48.701
More creative scenarios are also possible,

00:12:48.701 --> 00:12:50.016
like if you're the A.I.,

00:12:50.016 --> 00:12:53.548
you can imagine wiggling electrodes
around in your internal circuitry

00:12:53.548 --> 00:12:57.010
to create radio waves that you
can use to communicate.

00:12:57.010 --> 00:12:59.434
Or maybe you could pretend to malfunction,

00:12:59.434 --> 00:13:02.931
and then when the programmers open
you up to see what went wrong with you,

00:13:02.931 --> 00:13:04.867
they look at the source code -- Bam! --

00:13:04.867 --> 00:13:07.314
the manipulation can take place.

00:13:07.314 --> 00:13:10.744
Or it could output the blueprint
to a really nifty technology,

00:13:10.744 --> 00:13:12.142
and when we implement it,

00:13:12.142 --> 00:13:16.539
it has some surreptitious side effect
that the A.I. had planned.

00:13:16.539 --> 00:13:20.002
The point here is that we should 
not be confident in our ability

00:13:20.002 --> 00:13:23.810
to keep a superintelligent genie
locked up in its bottle forever.

00:13:23.810 --> 00:13:26.064
Sooner or later, it will out.

00:13:27.034 --> 00:13:30.137
I believe that the answer here
is to figure out

00:13:30.137 --> 00:13:35.161
how to create superintelligent A.I.
such that even if -- when -- it escapes,

00:13:35.161 --> 00:13:38.438
it is still safe because it is
fundamentally on our side

00:13:38.438 --> 00:13:40.337
because it shares our values.

00:13:40.337 --> 00:13:43.547
I see no way around 
this difficult problem.

00:13:44.557 --> 00:13:48.391
Now, I'm actually fairly optimistic
that this problem can be solved.

00:13:48.391 --> 00:13:52.294
We wouldn't have to write down 
a long list of everything we care about,

00:13:52.294 --> 00:13:55.937
or worse yet, spell it out 
in some computer language

00:13:55.937 --> 00:13:57.391
like C++ or Python,

00:13:57.391 --> 00:14:00.158
that would be a task beyond hopeless.

00:14:00.158 --> 00:14:04.455
Instead, we would create an A.I.
that uses its intelligence

00:14:04.455 --> 00:14:07.226
to learn what we value,

00:14:07.226 --> 00:14:12.506
and its motivation system is constructed
in such a way that it is motivated

00:14:12.506 --> 00:14:17.738
to pursue our values or to perform actions
that it predicts we would approve of.

00:14:17.738 --> 00:14:21.152
We would thus leverage 
its intelligence as much as possible

00:14:21.152 --> 00:14:23.897
to solve the problem of value-loading.

00:14:24.727 --> 00:14:26.239
This can happen,

00:14:26.239 --> 00:14:29.835
and the outcome could be 
very good for humanity.

00:14:29.835 --> 00:14:33.792
But it doesn't happen automatically.

00:14:33.792 --> 00:14:36.790
The initial conditions 
for the intelligence explosion

00:14:36.790 --> 00:14:39.653
might need to be set up 
in just the right way

00:14:39.653 --> 00:14:43.183
if we are to have a controlled detonation.

00:14:43.183 --> 00:14:45.801
The values that the A.I. has
need to match ours,

00:14:45.801 --> 00:14:47.561
not just in the familiar context,

00:14:47.561 --> 00:14:49.999
like where we can easily check
how the A.I. behaves,

00:14:49.999 --> 00:14:53.233
but also in all novel contexts
that the A.I. might encounter

00:14:53.233 --> 00:14:54.790
in the indefinite future.

00:14:54.790 --> 00:14:59.527
And there are also some esoteric issues
that would need to be solved, sorted out:

00:14:59.527 --> 00:15:01.616
the exact details of its decision theory,

00:15:01.616 --> 00:15:04.480
how to deal with logical
uncertainty and so forth.

00:15:05.330 --> 00:15:08.432
So the technical problems that need
to be solved to make this work

00:15:08.432 --> 00:15:09.545
look quite difficult --

00:15:09.545 --> 00:15:12.925
not as difficult as making 
a superintelligent A.I.,

00:15:12.925 --> 00:15:15.793
but fairly difficult.

00:15:15.793 --> 00:15:17.488
Here is the worry:

00:15:17.488 --> 00:15:22.172
Making superintelligent A.I.
is a really hard challenge.

00:15:22.172 --> 00:15:24.720
Making superintelligent A.I. that is safe

00:15:24.720 --> 00:15:27.136
involves some additional 
challenge on top of that.

00:15:28.216 --> 00:15:31.703
The risk is that if somebody figures out
how to crack the first challenge

00:15:31.703 --> 00:15:34.704
without also having cracked 
the additional challenge

00:15:34.704 --> 00:15:36.605
of ensuring perfect safety.

00:15:37.375 --> 00:15:40.706
So I think that we should
work out a solution

00:15:40.706 --> 00:15:43.528
to the control problem in advance,

00:15:43.528 --> 00:15:46.188
so that we have it available 
by the time it is needed.

00:15:46.768 --> 00:15:50.275
Now it might be that we cannot solve
the entire control problem in advance

00:15:50.275 --> 00:15:53.299
because maybe some elements
can only be put in place

00:15:53.299 --> 00:15:57.296
once you know the details of the 
architecture where it will be implemented.

00:15:57.296 --> 00:16:00.676
But the more of the control problem
that we solve in advance,

00:16:00.676 --> 00:16:04.766
the better the odds that the transition
to the machine intelligence era

00:16:04.766 --> 00:16:06.306
will go well.

00:16:06.306 --> 00:16:10.950
This to me looks like a thing
that is well worth doing

00:16:10.950 --> 00:16:14.282
and I can imagine that if 
things turn out okay,

00:16:14.282 --> 00:16:18.940
that people a million years from now
look back at this century

00:16:18.940 --> 00:16:22.942
and it might well be that they say that
the one thing we did that really mattered

00:16:22.942 --> 00:16:24.509
was to get this thing right.

00:16:24.509 --> 00:16:26.198
Thank you.

00:16:26.198 --> 00:16:29.011
(Applause)

