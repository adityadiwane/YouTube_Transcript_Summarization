WEBVTT
Kind: captions
Language: it

00:00:00.000 --> 00:00:07.000
Traduttore: Michele Gianella
Revisore: Emanuele Francesco Pecora

00:00:12.570 --> 00:00:16.880
Lavoro con molti matematici,
filosofi e informatici.

00:00:18.010 --> 00:00:21.980
Ci sediamo e pensiamo
al futuro dell'Intelligenza Artificiale,

00:00:21.980 --> 00:00:24.030
tra le altre cose.

00:00:24.030 --> 00:00:28.750
Alcuni pensano che alcune di queste cose
siano roba di fantascienza,

00:00:28.750 --> 00:00:31.850
di là da venire, folli.

00:00:31.850 --> 00:00:33.320
Ma a me piace dire,

00:00:33.320 --> 00:00:36.930
okay, diamo un'occhiata
alla condizione dell'uomo moderno.

00:00:36.930 --> 00:00:38.620
(Risate)

00:00:38.620 --> 00:00:41.020
È così che vanno le cose, "normalmente".

00:00:41.020 --> 00:00:43.300
Ma se ci pensiamo,

00:00:43.300 --> 00:00:48.670
in realtà noi umani non siamo ospiti
di questo pianeta da molto tempo.

00:00:48.680 --> 00:00:53.430
Pensate, se la Terra
fosse stata creata un anno fa,

00:00:53.430 --> 00:00:56.970
la specie umana esisterebbe
da soli 10 minuti.

00:00:56.970 --> 00:01:00.140
L'era industriale
sarebbe iniziata due secondi fa.

00:01:01.270 --> 00:01:06.500
Un altro modo di vederlo è pensare
al PIL mondiale negli ultimi 10.000 anni:

00:01:06.500 --> 00:01:09.530
mi sono permesso
di mostrarvelo in un grafico.

00:01:09.530 --> 00:01:11.300
L'andamento è questo.

00:01:11.300 --> 00:01:12.660
(Risate)

00:01:12.660 --> 00:01:14.810
Strana curva,
per una condizione normale.

00:01:14.810 --> 00:01:16.510
Certo non vorrei sedermi lì.

00:01:16.510 --> 00:01:19.060
(Risate)

00:01:19.060 --> 00:01:23.840
Chiediamoci, qual è la causa
di questa anomalia?

00:01:23.840 --> 00:01:26.390
Alcuni direbbero che è la tecnologia.

00:01:26.390 --> 00:01:31.060
Ed è vero, la tecnologia si è accumulata
nel corso della storia umana,

00:01:31.060 --> 00:01:35.710
e ora procede ad un ritmo
estremamente rapido -

00:01:35.710 --> 00:01:37.270
questa è la causa più immediata,

00:01:37.270 --> 00:01:39.840
il motivo per cui ora
siamo così produttivi.

00:01:40.470 --> 00:01:44.130
Ma vorrei andare a ritroso
fino alla causa fondamentale.

00:01:45.110 --> 00:01:48.880
Guardate questi due signori,
molto diversi tra loro:

00:01:48.880 --> 00:01:50.480
Abbiamo Kanzi --

00:01:50.480 --> 00:01:55.120
che padroneggia 200 elementi lessicali,
una caratteristica incredibile.

00:01:55.120 --> 00:01:58.810
E lui è Ed Witten, che ha lanciato
la 2° rivoluzione delle superstringhe.

00:01:58.810 --> 00:02:01.140
Guardando sotto la fronte,
ecco cosa troviamo:

00:02:01.140 --> 00:02:02.710
essenzialmente la stessa cosa.

00:02:02.710 --> 00:02:04.520
Sì, una è un po' più larga,

00:02:04.520 --> 00:02:07.280
forse è anche cablata un po' meglio.

00:02:07.280 --> 00:02:11.090
Tuttavia, queste differenze invisibili
non possono essere troppo complicate

00:02:11.090 --> 00:02:15.370
perché sono passate
solo 250.000 generazioni

00:02:15.370 --> 00:02:17.110
dall'ultimo progenitore comune.

00:02:17.110 --> 00:02:20.960
E sappiamo che i meccanismi complicati
richiedono molto tempo per evolversi.

00:02:22.000 --> 00:02:24.490
Una manciata di modifiche minori,
quindi, è bastata

00:02:24.490 --> 00:02:27.560
a portarci da Kanzi a Witten,

00:02:27.560 --> 00:02:32.100
dai rami spezzati
ai missili balistici intercontinentali.

00:02:32.830 --> 00:02:36.770
Sembra quindi abbastanza ovvio
che tutto ciò che abbiamo conquistato,

00:02:36.770 --> 00:02:38.150
e a cui diamo valore,

00:02:38.150 --> 00:02:43.380
derivi sostanzialmente da alcuni ritocchi
che hanno prodotto la mente umana.

00:02:44.650 --> 00:02:48.310
E il corollario, ovviamente,
è che ogni modifica ulteriore

00:02:48.310 --> 00:02:51.780
che possa cambiare sostanzialmente
il substrato del pensiero

00:02:51.780 --> 00:02:54.990
potrebbe avere conseguenze
potenzialmente enormi.

00:02:56.320 --> 00:02:59.220
Alcuni dei miei colleghi ritengono
che siamo alla vigilia

00:02:59.220 --> 00:03:03.130
di qualcosa che potrebbe causare
un profondo cambiamento in quel substrato,

00:03:03.130 --> 00:03:06.340
e quel qualcosa
è la superintelligenza artificiale.

00:03:06.340 --> 00:03:11.080
L'Intelligenza Artificiale, un tempo,
consisteva nel dare comandi a una scatola.

00:03:11.080 --> 00:03:12.750
C'erano programmatori umani

00:03:12.750 --> 00:03:15.880
che elaboravano "manualmente",
con fatica, tasselli di conoscenza.

00:03:15.880 --> 00:03:17.970
Si costruivano questi sistemi esperti

00:03:17.970 --> 00:03:20.290
che erano utili per qualche scopo,

00:03:20.290 --> 00:03:22.970
ma erano molto "fragili",
difficili da espandere.

00:03:22.970 --> 00:03:26.410
Sostanzialmente ottenevi
solo quello che inserivi.

00:03:26.410 --> 00:03:27.400
Ma da allora

00:03:27.400 --> 00:03:30.870
c'è stata una rivoluzione
nel settore dell'Intelligenza Artificiale.

00:03:30.870 --> 00:03:33.640
Oggi, siamo concentrati
sull'apprendimento macchina.

00:03:34.390 --> 00:03:39.780
Invece di inserire a mano rappresentazioni
e caratteristiche della conoscenza,

00:03:40.510 --> 00:03:46.060
creiamo algoritmi che apprendono,
spesso da dati percettivi grezzi.

00:03:46.060 --> 00:03:51.060
In pratica, la stessa cosa
che fa un bambino.

00:03:51.060 --> 00:03:55.270
Il risultato è un'intelligenza artificiale
che non si limita a un solo settore:

00:03:55.270 --> 00:03:59.900
lo stesso sistema può imparare
a tradurre tra ogni coppia di linguaggi,

00:03:59.900 --> 00:04:05.330
o imparare a giocare
ogni gioco della consolle Atari.

00:04:05.330 --> 00:04:07.110
Naturalmente,

00:04:07.110 --> 00:04:11.110
L'IA non è ancora neanche vicina 
alla potente, interdisciplinare

00:04:11.110 --> 00:04:14.330
capacità di imparare e progettare
di un essere umano.

00:04:14.330 --> 00:04:16.460
La corteccia ha ancora
vantaggi algoritmici

00:04:16.460 --> 00:04:18.810
che non sappiamo riprodurre
nelle macchine.

00:04:19.880 --> 00:04:21.780
Quindi la domanda è,

00:04:21.780 --> 00:04:25.270
quanto manca, prima che le macchine
riescano a recuperare questi vantaggi?

00:04:25.270 --> 00:04:27.320
Un paio di anni fa,

00:04:27.320 --> 00:04:30.209
abbiamo fatto un sondaggio
ad alcuni esperti mondiali di IA

00:04:30.209 --> 00:04:33.440
per vedere cosa pensassero,
e una delle domande fu:

00:04:33.440 --> 00:04:36.790
"Entro quale anno pensate
che vi sia una probabilità del 50%

00:04:36.790 --> 00:04:40.270
di ottenere un'intelligenza artificiale
di livello umano?"

00:04:40.780 --> 00:04:44.960
Qui definiamo "di livello umano"
l'abilità di eseguire

00:04:44.960 --> 00:04:47.830
quasi ogni lavoro almeno tanto bene
quanto un umano,

00:04:47.830 --> 00:04:51.840
quindi di livello veramente umano,
non solo in qualche dominio limitato.

00:04:51.840 --> 00:04:55.490
E la risposta mediana fu 2040 o 2050,

00:04:55.490 --> 00:04:58.300
a seconda del gruppo di esperti
a cui chiedevamo.

00:04:58.300 --> 00:05:02.330
Potrebbe avvenire molto,
molto più tardi, o prima,

00:05:02.330 --> 00:05:04.270
la realtà è che nessuno lo sa davvero.

00:05:05.250 --> 00:05:09.670
Quello che sappiamo è che il limite
all'elaborazione di informazioni

00:05:09.670 --> 00:05:14.540
su un substrato artificale va molto
al di là dei limiti dei tessuti biologici.

00:05:15.240 --> 00:05:17.610
Le ragioni si trovano nella fisica.

00:05:17.610 --> 00:05:22.330
Un neurone biologico spara, forse,
a 200 Hertz, 200 volte al secondo.

00:05:22.330 --> 00:05:25.930
Ma un transistor dei giorni nostri
opera a GigaHertz.

00:05:25.930 --> 00:05:31.220
I neuroni si propagano lentamente lungo
gli assoni, a massimo 100 mt/s.

00:05:31.220 --> 00:05:34.330
Ma un computer può instradare
i segnali alla velocità della luce.

00:05:35.070 --> 00:05:36.940
Ci sono inoltre limiti dimensionali:

00:05:36.940 --> 00:05:39.970
un cervello umano deve stare
all'interno di un cranio,

00:05:39.970 --> 00:05:44.730
mentre un computer può essere grande
come un magazzino, o di più.

00:05:44.730 --> 00:05:50.330
Il potenziale della superintelligenza,
quindi, giace nella materia,

00:05:50.330 --> 00:05:56.040
proprio come la forza dell'atomo
si è nascosta nella storia umana,

00:05:56.040 --> 00:06:00.450
attendendo paziente il 1945.

00:06:00.450 --> 00:06:01.700
In questo secolo,

00:06:01.700 --> 00:06:05.810
gli scienziati potrebbero imparare
a scatenare l'intelligenza artificiale.

00:06:05.810 --> 00:06:09.820
E penso che a quel punto potremmo
osservare un'esplosione di intelligenza.

00:06:10.400 --> 00:06:14.360
La maggior parte delle persone,
quando pensano al genio e all'idiozia,

00:06:14.360 --> 00:06:17.380
penso che abbiano in mente più o meno
un'immagine come questa.

00:06:17.380 --> 00:06:19.980
Abbiamo lo scemo del villaggio
ad un estremo,

00:06:19.980 --> 00:06:22.460
e in posizione diametralmente opposta

00:06:22.460 --> 00:06:27.220
abbiamo Ed Witten, Albert Einstein
o un altro vostro guru preferito.

00:06:27.220 --> 00:06:31.050
Ma penso che dal punto di vista
dell'intelligenza artificiale,

00:06:31.050 --> 00:06:34.730
la vera immagine somigli
più probabilmente a questa:

00:06:35.250 --> 00:06:38.630
l'IA inizia qui, a zero intelligenza,

00:06:38.630 --> 00:06:41.640
e poi, dopo molti, molti anni
di lavoro veramente duro,

00:06:41.640 --> 00:06:45.490
alla fine forse arriviamo
al livello di intelligenza di un topo,

00:06:45.490 --> 00:06:47.920
qualcosa che possa navigare
in ambienti complessi

00:06:47.920 --> 00:06:49.900
bene quanto un topo.

00:06:49.900 --> 00:06:54.220
E poi, dopo molti, molti anni
di duro lavoro, e molti investimenti,

00:06:54.220 --> 00:06:58.860
forse alla fine arriviamo a un'IA
intelligente come uno scimpanzé.

00:06:58.860 --> 00:07:02.070
E poi, dopo ancora molti,
molti anni di lavoro massacrante,

00:07:02.070 --> 00:07:04.980
arriviamo a un'IA al livello
di uno scemo del villaggio.

00:07:04.980 --> 00:07:08.250
E pochi istanti dopo,
avremo sorpassato Ed Witten.

00:07:08.250 --> 00:07:11.220
Il treno non si fermerà a Umanopoli.

00:07:11.220 --> 00:07:14.240
È più probabile che sfrecci oltre, invece.

00:07:14.240 --> 00:07:16.230
Questo ha implicazioni profonde,

00:07:16.230 --> 00:07:20.090
soprattutto quando si parla di potere.

00:07:20.090 --> 00:07:21.990
Gli scimpanzé, ad esempio, sono forti--

00:07:21.990 --> 00:07:27.210
circa il doppio di un maschio umano
in buona forma fisica.

00:07:27.210 --> 00:07:31.820
E tuttavia, il destino di Kanzi
e dei suoi pari dipende molto di più

00:07:31.820 --> 00:07:35.960
dalle nostre azioni che dalle loro.

00:07:37.220 --> 00:07:39.540
Quando arriverà la superintelligenza,

00:07:39.540 --> 00:07:43.380
anche il nostro destino
potrebbe dipenderne.

00:07:44.450 --> 00:07:45.500
Pensateci:

00:07:45.500 --> 00:07:50.550
L'IA è l'ultima invenzione
che l'umanità dovrà mai creare.

00:07:50.550 --> 00:07:53.520
Le macchine saranno
inventori migliori di noi,

00:07:53.520 --> 00:07:56.060
e agiranno in tempi "digitali".

00:07:56.060 --> 00:08:00.960
Questo significa sostanzialmente
un "avvicinamento" del futuro.

00:08:00.960 --> 00:08:04.520
Pensate a tutte le tecnologie folli
che forse, a vostro avviso,

00:08:04.520 --> 00:08:07.320
gli umani potrebbero sviluppare:

00:08:07.320 --> 00:08:10.580
cure per l'invecchiamento,
colonizzazione spaziale,

00:08:10.580 --> 00:08:14.310
nanobot auto-replicanti,
caricare le proprie menti in un computer.

00:08:14.310 --> 00:08:16.470
Ogni sorta di roba fantascientifica

00:08:16.470 --> 00:08:19.200
e nondimeno permessa dalla fisica.

00:08:19.200 --> 00:08:23.410
Una superintelligenza potrebbe sviluppare
tutto questo, e forse molto rapidamente.

00:08:24.440 --> 00:08:28.000
Una superintelligenza
con una tale maturità tecnologica

00:08:28.000 --> 00:08:30.180
sarebbe estremamente potente,

00:08:30.180 --> 00:08:34.730
e almeno in qualche scenario,
potrebbe ottenere quel che vuole.

00:08:34.730 --> 00:08:40.390
A quel punto avremmo un futuro
modellato sulle preferenze dell'IA.

00:08:41.850 --> 00:08:45.600
Una buona domanda a quel punto è:
quali sono queste preferenze?

00:08:46.240 --> 00:08:48.010
Qui la cosa si fa intricata.

00:08:48.010 --> 00:08:49.440
Per trovare una via d'uscita,

00:08:49.440 --> 00:08:52.720
dobbiamo prima di tutto
evitare l'antropomorfizzazione.

00:08:53.929 --> 00:08:57.230
Ed è ironico perché
ogni articolo di giornale

00:08:57.230 --> 00:09:01.090
sul futuro dell'IA
ha un'immagine come questa.

00:09:02.280 --> 00:09:06.410
Quindi penso che dovremmo concepire
la questione in modo più astratto,

00:09:06.410 --> 00:09:09.200
non come un film di Hollywood.

00:09:09.200 --> 00:09:12.820
Dobbiamo pensare all'intelligenza
come a un processo di ottimizzazione,

00:09:12.820 --> 00:09:18.470
un processo che dirige il futuro verso
un particolare set di configurazioni.

00:09:18.470 --> 00:09:21.980
Una superintelligenza è un processo
di ottimizzazione davvero potente.

00:09:21.980 --> 00:09:24.710
È estremamente capace
di usare i mezzi disponibili

00:09:24.710 --> 00:09:28.000
per ottenere una condizione
in cui i suoi scopi sono realizzati.

00:09:28.440 --> 00:09:31.110
Quindi non c'è necessariamente
una connessione

00:09:31.110 --> 00:09:33.850
tra l'essere molto intelligenti
in questo senso

00:09:33.850 --> 00:09:38.510
e avere un obiettivo che noi umani
riterremmo degno o significativo.

00:09:39.320 --> 00:09:42.810
Supponiamo di dare all'AI
l'obiettivo di far sorridere gli umani.

00:09:42.810 --> 00:09:46.090
Un'IA debole si limiterebbe
a eseguire azioni utili o divertenti,

00:09:46.090 --> 00:09:48.610
che fanno sorridere il suo utente.

00:09:48.610 --> 00:09:51.030
Quando l'IA diventa superintelligente,

00:09:51.030 --> 00:09:54.550
capisce che c'è un modo più efficace
di ottenere questo scopo:

00:09:54.550 --> 00:09:56.470
prendere il controllo del mondo,

00:09:56.470 --> 00:09:59.630
e infilare elettrodi
nei muscoli facciali degli umani,

00:09:59.630 --> 00:10:02.570
causando una costante,
accattivante smorfia.

00:10:02.570 --> 00:10:04.580
Un altro esempio:
supponiamo di dare all'IA

00:10:04.580 --> 00:10:06.990
un problema matematico duro da risolvere.

00:10:06.990 --> 00:10:08.930
Quando l'IA diventa superintelligente,

00:10:08.930 --> 00:10:13.100
capisce che il modo più efficace
di ottenere la soluzione al problema

00:10:13.100 --> 00:10:16.030
è trasformare il pianeta
in un computer gigantesco,

00:10:16.030 --> 00:10:18.280
così da aumentare
la sua capacità di pensiero.

00:10:18.280 --> 00:10:21.040
E notate: questo dà all'IA
una ragione strumentale

00:10:21.040 --> 00:10:23.560
per farci subire cose
che potremmo non approvare.

00:10:23.560 --> 00:10:25.490
Gli umani diventerebbero una minaccia,

00:10:25.490 --> 00:10:28.410
perché potremmo impedire
la scoperta della soluzione.

00:10:29.200 --> 00:10:32.640
Ovviamente, non è detto che le cose
andranno male in questo preciso modo;

00:10:32.640 --> 00:10:34.450
sono esempi da cartone animato.

00:10:34.450 --> 00:10:36.390
Ma è importante cogliere il punto:

00:10:36.390 --> 00:10:39.260
se create un processo
di ottimizzazione davvero potente

00:10:39.260 --> 00:10:41.500
che massimizzi l'obiettivo x,

00:10:41.500 --> 00:10:43.770
sinceratevi che la vostra definizione di x

00:10:43.770 --> 00:10:46.240
includa tutto ciò a cui tenete.

00:10:46.830 --> 00:10:51.210
È una lezione tramandata
da molti miti, anche.

00:10:51.210 --> 00:10:56.510
Re Mida voleva trasformare in oro
tutto ciò che toccava.

00:10:56.510 --> 00:10:59.370
Tocca sua figlia, e la trasforma in oro.

00:10:59.370 --> 00:11:01.930
Tocca il suo cibo, si trasforma in oro.

00:11:01.930 --> 00:11:04.520
È un esempio che potremmo
considerare pregnante:

00:11:04.520 --> 00:11:06.590
non solo come metafora dell'avidità,

00:11:06.590 --> 00:11:08.480
ma anche perché illustra cosa succede

00:11:08.480 --> 00:11:11.320
se create un potente processo
di ottimizzazione

00:11:11.320 --> 00:11:16.110
e gli affidate obiettivi indesiderabili,
o male specificati.

00:11:16.110 --> 00:11:21.300
Be', mi direte, se un computer inizia
a infilare elettrodi in faccia alla gente,

00:11:21.300 --> 00:11:22.940
basta spegnerlo.

00:11:22.940 --> 00:11:24.550
[Le mie obiezioni sono due:]

00:11:24.550 --> 00:11:29.890
A, non è detto che sia così semplice,
se diventiamo dipendenti dal sistema --

00:11:29.890 --> 00:11:32.620
per esempio, dov'è l'interuttore
per spegnere Internet?

00:11:32.620 --> 00:11:37.740
B, perché gli scimpanzé non hanno
staccato l'interruttore dell'umanità,

00:11:37.740 --> 00:11:39.290
o i Neanderthal?

00:11:39.290 --> 00:11:41.960
Certamente avevano
delle ragioni per farlo.

00:11:41.960 --> 00:11:44.750
Noi abbiamo un interruttore,
per esempio qui.

00:11:44.750 --> 00:11:46.310
(Si strozza da solo)

00:11:46.310 --> 00:11:49.230
Il motivo è che siamo
avversari intelligenti;

00:11:49.230 --> 00:11:51.960
possiamo anticipare le minacce
e studiare come aggirarle.

00:11:51.960 --> 00:11:54.470
Ma anche un agente
superintelligente potrebbe farlo,

00:11:54.470 --> 00:11:57.720
e ci riuscirebbe molto meglio di noi.

00:11:57.720 --> 00:12:04.910
Non dovremmo contare
sul fatto di poterlo controllare.

00:12:04.910 --> 00:12:08.350
E potremmo provare a semplificarci
un po' la vita, diciamo,

00:12:08.350 --> 00:12:09.940
mettendo l'IA in una "scatola",

00:12:09.940 --> 00:12:11.740
come un ambiente virtuale sicuro,

00:12:11.740 --> 00:12:14.760
una ricostruzione della realtà
da cui non può sfuggire.

00:12:14.760 --> 00:12:18.910
Ma quanto possiamo contare sul fatto
che non trovi una falla nel codice?

00:12:18.910 --> 00:12:22.080
Visto che molti hacker umani
scoprono bug in continuazione,

00:12:22.080 --> 00:12:25.110
direi che non dovremmo contarci molto.

00:12:26.230 --> 00:12:30.780
Potremmo scollegare il cavo Ethernet,
creando un gap fisico.

00:12:30.780 --> 00:12:33.450
Ma anche in questo caso,
molti hacker umani

00:12:33.450 --> 00:12:36.830
aggirano continuamente il problema
con l'ingegneria sociale.

00:12:36.830 --> 00:12:38.090
Proprio ora, mentre parlo,

00:12:38.090 --> 00:12:40.480
sono sicuro che c'è
qualche impiegato, là fuori,

00:12:40.480 --> 00:12:43.820
che sta dando i dettagli
del suo account a qualcuno

00:12:43.820 --> 00:12:46.570
che si spaccia per il dipartimento IT.

00:12:46.570 --> 00:12:48.700
E sono possibili anche
scenari più creativi:

00:12:48.700 --> 00:12:50.010
se siete l'IA, ad esempio,

00:12:50.010 --> 00:12:53.540
potreste pensare a sguinzagliare
degli elettrodi nei vostri circuiti

00:12:53.540 --> 00:12:57.010
per creare onde radio
che potete usare per comunicare.

00:12:57.010 --> 00:12:59.430
Oppure potreste fingere di guastarvi,

00:12:59.430 --> 00:13:02.930
e quando i programmatori vi ispezionano
per capire cosa non va,

00:13:02.930 --> 00:13:04.860
guardano al codice sorgente e BAM!

00:13:04.860 --> 00:13:07.310
ecco che avviene una manipolazione.

00:13:07.310 --> 00:13:10.740
Oppure potrebbe pubblicare il modello
di una tecnologia affascinante,

00:13:10.740 --> 00:13:12.140
che quando la implementiamo

00:13:12.140 --> 00:13:16.530
produce dei sottili effetti collaterali,
che l'IA aveva previsto.

00:13:16.530 --> 00:13:20.000
Non dovremmo contare sulla nostra capacità

00:13:20.000 --> 00:13:23.810
di chiudere una superintelligenza
nella sua bottiglia in eterno.

00:13:23.810 --> 00:13:26.060
Prima o poi riuscirà ad uscire.

00:13:27.030 --> 00:13:30.130
Credo che qui la risposta sia capire

00:13:30.130 --> 00:13:35.160
come creare un'IA superintelligente
tale che se -- quando -- uscirà,

00:13:35.160 --> 00:13:38.430
resterà amichevole,
fondamentalmente dalla nostra parte

00:13:38.430 --> 00:13:40.330
perché condivide i nostri valori.

00:13:40.330 --> 00:13:43.540
Non vedo scorciatoie
a questo difficile problema.

00:13:44.550 --> 00:13:48.390
Sono molto ottimista
sulle nostre capacità di risolverlo.

00:13:48.390 --> 00:13:52.290
Non dovremmo scrivere una lunga lista
di tutto ciò a cui diamo valore,

00:13:52.290 --> 00:13:55.930
o peggio ancora codificarlo
in qualche linguaggio

00:13:55.930 --> 00:13:57.390
come C++ o Python,

00:13:57.390 --> 00:14:00.150
altrimenti sarebbe una sfida impossibile.

00:14:00.150 --> 00:14:04.450
Invece, dovremmo creare un'IA
che usa la sua intelligenza

00:14:04.450 --> 00:14:07.220
per imparare a cosa diamo valore,

00:14:07.220 --> 00:14:12.500
e con un sistema motivazionale concepito

00:14:12.500 --> 00:14:17.730
per perseguire i nostri valori, o eseguire
azioni che sa che approveremmo.

00:14:17.730 --> 00:14:21.150
Potremmo così sfruttare
la sua intelligenza al massimo

00:14:21.150 --> 00:14:23.890
nel risolvere il problema
dell'attribuzione di valore.

00:14:24.720 --> 00:14:26.230
Possiamo farlo,

00:14:26.230 --> 00:14:29.830
e il risultato sarebbe
molto positivo per l'umanità.

00:14:29.830 --> 00:14:33.790
Ma non avviene automaticamente.

00:14:33.790 --> 00:14:36.790
Le condizioni iniziali
per questa esplosione di intelligenza

00:14:36.790 --> 00:14:39.650
potrebbero dover essere
definite perfettamente,

00:14:39.650 --> 00:14:43.180
se quella che vogliamo
è un'esplosione controllata.

00:14:43.180 --> 00:14:45.800
I valori dell'IA
devono coincidere con i nostri,

00:14:45.800 --> 00:14:47.560
non solo nei contesti familiari,

00:14:47.560 --> 00:14:49.990
dove puoi facilmente controllare
come si comporta,

00:14:49.990 --> 00:14:53.230
ma anche in tutti quei contesti nuovi
che l'IA potrebbe incontrare

00:14:53.230 --> 00:14:54.790
in futuro.

00:14:54.790 --> 00:14:59.520
E restano anche alcune questioni
esoteriche da risolvere e chiarire:

00:14:59.520 --> 00:15:01.610
i dettagli su come prendere decisioni,

00:15:01.610 --> 00:15:04.480
come gestire l'incertezza logica,
e così via.

00:15:05.330 --> 00:15:08.430
Quindi i problemi tecnici da risolvere

00:15:08.430 --> 00:15:09.540
sembrano molto difficili:

00:15:09.540 --> 00:15:12.920
non tanto quanto realizzare
un'IA superintelligente,

00:15:12.920 --> 00:15:15.790
ma comunque molto difficili.

00:15:15.790 --> 00:15:17.480
Questa è la mia paura:

00:15:17.480 --> 00:15:22.170
realizzare un'IA superintelligente
è già una sfida veramente dura;

00:15:22.170 --> 00:15:24.720
realizzare un'IA
superintelligente e sicura

00:15:24.720 --> 00:15:27.130
pone sfide aggiuntive.

00:15:28.210 --> 00:15:31.700
Il rischio è che qualcuno capisca
come vincere la prima sfida

00:15:31.700 --> 00:15:34.700
senza sapere ancora
come vincere la sfida aggiuntiva

00:15:34.700 --> 00:15:36.600
di assicurare una perfetta sicurezza.

00:15:37.370 --> 00:15:40.700
Penso quindi che dovremmo prima
lavorare a una soluzione

00:15:40.700 --> 00:15:43.520
al problema del controllo,

00:15:43.520 --> 00:15:46.180
così da averla disponibile
al momento del bisogno.

00:15:46.760 --> 00:15:50.270
Magari non riusciremo
a risolvere tutto a priori,

00:15:50.270 --> 00:15:53.290
perché forse alcuni elementi
possono essere messi a punto

00:15:53.290 --> 00:15:57.290
solo dopo aver conosciuto
l'architettura che li implementa.

00:15:57.290 --> 00:16:00.670
Ma più problemi legati al controllo
risolviamo in anticipo,

00:16:00.670 --> 00:16:04.760
più è probabile che la transizione
all'era dell'intelligenza artificiale

00:16:04.760 --> 00:16:06.300
andrà a buon fine.

00:16:06.300 --> 00:16:10.950
Ritengo che questa sia una cosa
assolutamente da fare,

00:16:10.950 --> 00:16:14.280
e posso immaginare
che se le cose andranno bene,

00:16:14.280 --> 00:16:18.940
tra un milione di anni
la gente ripenserà a questo secolo

00:16:18.940 --> 00:16:22.940
e potrebbe ben dire che la sola cosa
importante che abbiamo fatto

00:16:22.940 --> 00:16:24.500
fu risolvere questo problema.

00:16:24.500 --> 00:16:26.190
Grazie.

00:16:26.190 --> 00:16:29.010
(Applausi)

