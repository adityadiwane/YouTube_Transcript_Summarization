WEBVTT
Kind: captions
Language: en

00:00:12.973 --> 00:00:17.516
We are built out of very small stuff,

00:00:17.516 --> 00:00:20.031
and we are embedded in
a very large cosmos,

00:00:20.031 --> 00:00:24.582
and the fact is that we are not
very good at understanding reality

00:00:24.582 --> 00:00:26.161
at either of those scales,

00:00:26.161 --> 00:00:27.763
and that's because our brains

00:00:27.763 --> 00:00:32.157
haven't evolved to understand 
the world at that scale.

00:00:32.157 --> 00:00:36.377
Instead, we're trapped on this
very thin slice of perception

00:00:36.377 --> 00:00:38.143
right in the middle.

00:00:38.723 --> 00:00:43.191
But it gets strange, because even at 
that slice of reality that we call home,

00:00:43.191 --> 00:00:46.176
we're not seeing most
of the action that's going on.

00:00:46.176 --> 00:00:49.566
So take the colors of our world.

00:00:49.566 --> 00:00:54.279
This is light waves, electromagnetic
radiation that bounces off objects

00:00:54.279 --> 00:00:57.716
and it hits specialized receptors
in the back of our eyes.

00:00:57.716 --> 00:01:01.361
But we're not seeing
all the waves out there.

00:01:01.361 --> 00:01:03.056
In fact, what we see

00:01:03.056 --> 00:01:07.119
is less than a 10 trillionth
of what's out there.

00:01:07.119 --> 00:01:10.486
So you have radio waves and microwaves

00:01:10.486 --> 00:01:13.783
and X-rays and gamma rays
passing through your body right now

00:01:13.783 --> 00:01:16.732
and you're completely unaware of it,

00:01:16.732 --> 00:01:19.913
because you don't come with
the proper biological receptors

00:01:19.913 --> 00:01:21.121
for picking it up.

00:01:21.631 --> 00:01:24.198
There are thousands
of cell phone conversations

00:01:24.198 --> 00:01:25.754
passing through you right now,

00:01:25.754 --> 00:01:28.055
and you're utterly blind to it.

00:01:28.055 --> 00:01:31.953
Now, it's not that these things
are inherently unseeable.

00:01:31.953 --> 00:01:36.852
Snakes include some infrared
in their reality,

00:01:36.852 --> 00:01:40.730
and honeybees include ultraviolet
in their view of the world,

00:01:40.730 --> 00:01:43.655
and of course we build machines
in the dashboards of our cars

00:01:43.655 --> 00:01:46.883
to pick up on signals
in the radio frequency range,

00:01:46.883 --> 00:01:50.575
and we built machines in hospitals
to pick up on the X-ray range.

00:01:50.575 --> 00:01:53.965
But you can't sense
any of those by yourself,

00:01:53.965 --> 00:01:55.474
at least not yet,

00:01:55.474 --> 00:01:59.421
because you don't come equipped
with the proper sensors.

00:01:59.421 --> 00:02:03.902
Now, what this means is that
our experience of reality

00:02:03.902 --> 00:02:07.362
is constrained by our biology,

00:02:07.362 --> 00:02:09.916
and that goes against
the common sense notion

00:02:09.916 --> 00:02:12.179
that our eyes and our ears
and our fingertips

00:02:12.179 --> 00:02:16.394
are just picking up
the objective reality that's out there.

00:02:16.394 --> 00:02:22.013
Instead, our brains are sampling
just a little bit of the world.

00:02:22.013 --> 00:02:24.080
Now, across the animal kingdom,

00:02:24.080 --> 00:02:27.400
different animals pick up
on different parts of reality.

00:02:27.400 --> 00:02:30.349
So in the blind
and deaf world of the tick,

00:02:30.349 --> 00:02:34.830
the important signals
are temperature and butyric acid;

00:02:34.830 --> 00:02:37.756
in the world of the black ghost knifefish,

00:02:37.756 --> 00:02:42.655
its sensory world is lavishly colored
by electrical fields;

00:02:42.655 --> 00:02:45.116
and for the echolocating bat,

00:02:45.116 --> 00:02:49.156
its reality is constructed
out of air compression waves.

00:02:49.156 --> 00:02:53.521
That's the slice of their ecosystem
that they can pick up on,

00:02:53.521 --> 00:02:55.379
and we have a word for this in science.

00:02:55.403 --> 00:02:56.911
It's called the umwelt,

00:02:56.911 --> 00:03:00.603
which is the German word
for the surrounding world.

00:03:00.603 --> 00:03:03.598
Now, presumably, every animal assumes

00:03:03.598 --> 00:03:07.987
that its umwelt is the entire
objective reality out there,

00:03:07.987 --> 00:03:10.281
because why would you ever stop to imagine

00:03:10.281 --> 00:03:12.802
that there's something beyond
what we can sense.

00:03:13.412 --> 00:03:16.126
Instead, what we all do
is we accept reality

00:03:16.126 --> 00:03:18.772
as it's presented to us.

00:03:19.222 --> 00:03:21.717
Let's do a consciousness-raiser on this.

00:03:21.717 --> 00:03:24.373
Imagine that you are a bloodhound dog.

00:03:24.973 --> 00:03:27.178
Your whole world is about smelling.

00:03:27.178 --> 00:03:31.590
You've got a long snout that has
200 million scent receptors in it,

00:03:31.590 --> 00:03:36.094
and you have wet nostrils
that attract and trap scent molecules,

00:03:36.094 --> 00:03:40.088
and your nostrils even have slits
so you can take big nosefuls of air.

00:03:40.088 --> 00:03:43.362
Everything is about smell for you.

00:03:43.362 --> 00:03:47.263
So one day, you stop in your tracks
with a revelation.

00:03:47.263 --> 00:03:50.583
You look at your human owner
and you think,

00:03:50.583 --> 00:03:55.393
"What is it like to have the pitiful,
impoverished nose of a human?

00:03:55.393 --> 00:03:57.083
(Laughter)

00:03:57.083 --> 00:04:00.335
What is it like when you take
a feeble little noseful of air?

00:04:00.335 --> 00:04:04.384
How can you not know that there's
a cat 100 yards away,

00:04:04.384 --> 00:04:07.718
or that your neighbor was on
this very spot six hours ago?"

00:04:07.718 --> 00:04:10.458
(Laughter)

00:04:10.458 --> 00:04:12.757
So because we're humans,

00:04:12.757 --> 00:04:15.404
we've never experienced
that world of smell,

00:04:15.404 --> 00:04:18.083
so we don't miss it,

00:04:18.083 --> 00:04:22.114
because we are firmly settled
into our umwelt.

00:04:22.114 --> 00:04:25.777
But the question is,
do we have to be stuck there?

00:04:26.317 --> 00:04:30.845
So as a neuroscientist, I'm interested
in the way that technology

00:04:30.845 --> 00:04:33.468
might expand our umwelt,

00:04:33.468 --> 00:04:37.108
and how that's going to change
the experience of being human.

00:04:38.228 --> 00:04:41.781
So we already know that we can marry
our technology to our biology,

00:04:41.781 --> 00:04:45.565
because there are hundreds of thousands
of people walking around

00:04:45.565 --> 00:04:49.164
with artificial hearing
and artificial vision.

00:04:49.164 --> 00:04:53.553
So the way this works is, you take
a microphone and you digitize the signal,

00:04:53.553 --> 00:04:57.291
and you put an electrode strip
directly into the inner ear.

00:04:57.291 --> 00:04:59.590
Or, with the retinal implant,
you take a camera

00:04:59.590 --> 00:05:02.864
and you digitize the signal,
and then you plug an electrode grid

00:05:02.864 --> 00:05:05.882
directly into the optic nerve.

00:05:05.882 --> 00:05:09.806
And as recently as 15 years ago,

00:05:09.806 --> 00:05:13.544
there were a lot of scientists who thought
these technologies wouldn't work.

00:05:13.544 --> 00:05:18.723
Why? It's because these technologies
speak the language of Silicon Valley,

00:05:18.723 --> 00:05:24.295
and it's not exactly the same dialect
as our natural biological sense organs.

00:05:24.295 --> 00:05:26.710
But the fact is that it works;

00:05:26.710 --> 00:05:31.299
the brain figures out
how to use the signals just fine.

00:05:31.719 --> 00:05:33.233
Now, how do we understand that?

00:05:33.763 --> 00:05:35.458
Well, here's the big secret:

00:05:35.458 --> 00:05:40.728
Your brain is not hearing
or seeing any of this.

00:05:40.728 --> 00:05:47.183
Your brain is locked in a vault of silence
and darkness inside your skull.

00:05:47.183 --> 00:05:50.991
All it ever sees are
electrochemical signals

00:05:50.991 --> 00:05:53.540
that come in along different data cables,

00:05:53.540 --> 00:05:57.992
and this is all it has to work with,
and nothing more.

00:05:58.672 --> 00:06:00.924
Now, amazingly,

00:06:00.924 --> 00:06:03.687
the brain is really good
at taking in these signals

00:06:03.687 --> 00:06:07.238
and extracting patterns
and assigning meaning,

00:06:07.238 --> 00:06:11.292
so that it takes this inner cosmos
and puts together a story

00:06:11.292 --> 00:06:16.179
of this, your subjective world.

00:06:16.179 --> 00:06:18.129
But here's the key point:

00:06:18.129 --> 00:06:21.519
Your brain doesn't know,
and it doesn't care,

00:06:21.519 --> 00:06:24.561
where it gets the data from.

00:06:24.561 --> 00:06:29.414
Whatever information comes in,
it just figures out what to do with it.

00:06:29.414 --> 00:06:31.852
And this is a very efficient
kind of machine.

00:06:31.852 --> 00:06:36.008
It's essentially a general purpose
computing device,

00:06:36.008 --> 00:06:38.423
and it just takes in everything

00:06:38.423 --> 00:06:41.023
and figures out
what it's going to do with it,

00:06:41.023 --> 00:06:44.669
and that, I think, frees up Mother Nature

00:06:44.669 --> 00:06:49.452
to tinker around with different
sorts of input channels.

00:06:49.452 --> 00:06:52.284
So I call this the P.H. 
model of evolution,

00:06:52.284 --> 00:06:54.328
and I don't want to get
too technical here,

00:06:54.328 --> 00:06:57.369
but P.H. stands for Potato Head,

00:06:57.369 --> 00:07:01.200
and I use this name to emphasize
that all these sensors

00:07:01.200 --> 00:07:04.451
that we know and love, like our eyes
and our ears and our fingertips,

00:07:04.451 --> 00:07:08.770
these are merely peripheral
plug-and-play devices:

00:07:08.770 --> 00:07:12.044
You stick them in, and you're good to go.

00:07:12.044 --> 00:07:17.153
The brain figures out what to do
with the data that comes in.

00:07:18.243 --> 00:07:20.449
And when you look across
the animal kingdom,

00:07:20.449 --> 00:07:23.096
you find lots of peripheral devices.

00:07:23.096 --> 00:07:27.206
So snakes have heat pits
with which to detect infrared,

00:07:27.206 --> 00:07:30.456
and the ghost knifefish has
electroreceptors,

00:07:30.456 --> 00:07:33.057
and the star-nosed mole has this appendage

00:07:33.057 --> 00:07:35.704
with 22 fingers on it

00:07:35.704 --> 00:07:39.373
with which it feels around and constructs
a 3D model of the world,

00:07:39.373 --> 00:07:43.297
and many birds have magnetite
so they can orient

00:07:43.297 --> 00:07:45.792
to the magnetic field of the planet.

00:07:45.792 --> 00:07:49.664
So what this means is that
nature doesn't have to continually

00:07:49.664 --> 00:07:52.079
redesign the brain.

00:07:52.079 --> 00:07:56.560
Instead, with the principles
of brain operation established,

00:07:56.560 --> 00:08:01.239
all nature has to worry about
is designing new peripherals.

00:08:01.239 --> 00:08:04.164
Okay. So what this means is this:

00:08:04.164 --> 00:08:06.184
The lesson that surfaces

00:08:06.184 --> 00:08:09.853
is that there's nothing
really special or fundamental

00:08:09.853 --> 00:08:12.848
about the biology that we
come to the table with.

00:08:12.848 --> 00:08:14.915
It's just what we have inherited

00:08:14.915 --> 00:08:18.142
from a complex road of evolution.

00:08:18.142 --> 00:08:21.671
But it's not what we have to stick with,

00:08:21.671 --> 00:08:23.715
and our best proof of principle of this

00:08:23.715 --> 00:08:26.315
comes from what's called
sensory substitution.

00:08:26.315 --> 00:08:29.543
And that refers to feeding
information into the brain

00:08:29.543 --> 00:08:32.329
via unusual sensory channels,

00:08:32.329 --> 00:08:35.208
and the brain just figures out
what to do with it.

00:08:35.208 --> 00:08:37.669
Now, that might sound speculative,

00:08:37.669 --> 00:08:42.621
but the first paper demonstrating this was
published in the journal Nature in 1969.

00:08:43.985 --> 00:08:46.353
So a scientist named Paul Bach-y-Rita

00:08:46.353 --> 00:08:49.581
put blind people
in a modified dental chair,

00:08:49.581 --> 00:08:51.926
and he set up a video feed,

00:08:51.926 --> 00:08:54.178
and he put something
in front of the camera,

00:08:54.178 --> 00:08:56.639
and then you would feel that

00:08:56.639 --> 00:08:59.565
poked into your back
with a grid of solenoids.

00:08:59.565 --> 00:09:02.049
So if you wiggle a coffee cup
in front of the camera,

00:09:02.049 --> 00:09:04.394
you're feeling that in your back,

00:09:04.394 --> 00:09:07.482
and amazingly, blind people
got pretty good

00:09:07.482 --> 00:09:11.035
at being able to determine
what was in front of the camera

00:09:11.035 --> 00:09:14.820
just by feeling it
in the small of their back.

00:09:14.820 --> 00:09:18.326
Now, there have been many
modern incarnations of this.

00:09:18.326 --> 00:09:21.600
The sonic glasses take a video feed
right in front of you

00:09:21.600 --> 00:09:24.455
and turn that into a sonic landscape,

00:09:24.455 --> 00:09:26.932
so as things move around,
and get closer and farther,

00:09:26.956 --> 00:09:29.030
it sounds like "Bzz, bzz, bzz."

00:09:29.030 --> 00:09:31.003
It sounds like a cacophony,

00:09:31.003 --> 00:09:34.997
but after several weeks, blind people
start getting pretty good

00:09:34.997 --> 00:09:37.319
at understanding what's in front of them

00:09:37.319 --> 00:09:39.966
just based on what they're hearing.

00:09:39.966 --> 00:09:41.966
And it doesn't have to be
through the ears:

00:09:41.990 --> 00:09:45.354
this system uses an electrotactile grid
on the forehead,

00:09:45.354 --> 00:09:49.044
so whatever's in front of the video feed,
you're feeling it on your forehead.

00:09:49.044 --> 00:09:51.897
Why the forehead? Because you're not
using it for much else.

00:09:51.897 --> 00:09:56.103
The most modern incarnation
is called the brainport,

00:09:56.103 --> 00:09:59.852
and this is a little electrogrid
that sits on your tongue,

00:09:59.852 --> 00:10:03.968
and the video feed gets turned into
these little electrotactile signals,

00:10:03.968 --> 00:10:10.455
and blind people get so good at using this
that they can throw a ball into a basket,

00:10:10.455 --> 00:10:14.471
or they can navigate
complex obstacle courses.

00:10:15.311 --> 00:10:19.525
They can come to see through their tongue.

00:10:19.525 --> 00:10:21.731
Now, that sounds completely insane, right?

00:10:21.731 --> 00:10:24.540
But remember, all vision ever is

00:10:24.540 --> 00:10:28.557
is electrochemical signals
coursing around in your brain.

00:10:28.557 --> 00:10:31.251
Your brain doesn't know
where the signals come from.

00:10:31.251 --> 00:10:34.687
It just figures out what to do with them.

00:10:34.687 --> 00:10:40.493
So my interest in my lab
is sensory substitution for the deaf,

00:10:40.493 --> 00:10:43.232
and this is a project I've undertaken

00:10:43.232 --> 00:10:46.227
with a graduate student
in my lab, Scott Novich,

00:10:46.227 --> 00:10:48.526
who is spearheading this for his thesis.

00:10:48.526 --> 00:10:50.522
And here is what we wanted to do:

00:10:50.522 --> 00:10:54.516
we wanted to make it so that
sound from the world gets converted

00:10:54.516 --> 00:10:59.392
in some way so that a deaf person
can understand what is being said.

00:10:59.392 --> 00:11:03.920
And we wanted to do this, given the power
and ubiquity of portable computing,

00:11:03.920 --> 00:11:08.796
we wanted to make sure that this
would run on cell phones and tablets,

00:11:08.796 --> 00:11:11.094
and also we wanted
to make this a wearable,

00:11:11.094 --> 00:11:14.136
something that you could wear
under your clothing.

00:11:14.136 --> 00:11:15.816
So here's the concept.

00:11:17.326 --> 00:11:22.402
So as I'm speaking, my sound
is getting captured by the tablet,

00:11:22.402 --> 00:11:28.160
and then it's getting mapped onto a vest
that's covered in vibratory motors,

00:11:28.160 --> 00:11:31.597
just like the motors in your cell phone.

00:11:31.597 --> 00:11:33.988
So as I'm speaking,

00:11:33.988 --> 00:11:40.327
the sound is getting translated
to a pattern of vibration on the vest.

00:11:40.327 --> 00:11:41.906
Now, this is not just conceptual:

00:11:41.906 --> 00:11:47.014
this tablet is transmitting Bluetooth,
and I'm wearing the vest right now.

00:11:47.014 --> 00:11:49.323
So as I'm speaking -- (Applause) --

00:11:50.033 --> 00:11:55.966
the sound is getting translated
into dynamic patterns of vibration.

00:11:55.966 --> 00:12:01.340
I'm feeling the sonic world around me.

00:12:01.340 --> 00:12:05.404
So, we've been testing this
with deaf people now,

00:12:05.404 --> 00:12:08.910
and it turns out that after
just a little bit of time,

00:12:08.910 --> 00:12:12.300
people can start feeling,
they can start understanding

00:12:12.300 --> 00:12:14.970
the language of the vest.

00:12:14.970 --> 00:12:19.753
So this is Jonathan. He's 37 years old.
He has a master's degree.

00:12:19.753 --> 00:12:22.098
He was born profoundly deaf,

00:12:22.098 --> 00:12:26.208
which means that there's a part
of his umwelt that's unavailable to him.

00:12:26.208 --> 00:12:30.596
So we had Jonathan train with the vest
for four days, two hours a day,

00:12:30.596 --> 00:12:33.876
and here he is on the fifth day.

00:12:33.876 --> 00:12:36.012
Scott Novich: You.

00:12:36.012 --> 00:12:39.226
David Eagleman: So Scott says a word,
Jonathan feels it on the vest,

00:12:39.226 --> 00:12:42.282
and he writes it on the board.

00:12:42.282 --> 00:12:46.168
SN: Where. Where.

00:12:46.168 --> 00:12:49.805
DE: Jonathan is able to translate
this complicated pattern of vibrations

00:12:49.805 --> 00:12:52.684
into an understanding
of what's being said.

00:12:52.684 --> 00:12:56.283
SN: Touch. Touch.

00:12:56.283 --> 00:13:00.723
DE: Now, he's not doing this --

00:13:00.723 --> 00:13:06.784
(Applause) --

00:13:07.944 --> 00:13:12.030
Jonathan is not doing this consciously,
because the patterns are too complicated,

00:13:12.030 --> 00:13:17.510
but his brain is starting to unlock
the pattern that allows it to figure out

00:13:17.510 --> 00:13:19.786
what the data mean,

00:13:19.786 --> 00:13:23.988
and our expectation is that,
after wearing this for about three months,

00:13:23.988 --> 00:13:28.586
he will have a direct
perceptual experience of hearing

00:13:28.586 --> 00:13:32.765
in the same way that when a blind person
passes a finger over braille,

00:13:32.765 --> 00:13:38.362
the meaning comes directly off the page
without any conscious intervention at all.

00:13:38.941 --> 00:13:42.494
Now, this technology has the potential
to be a game-changer,

00:13:42.494 --> 00:13:46.278
because the only other solution
for deafness is a cochlear implant,

00:13:46.278 --> 00:13:49.181
and that requires an invasive surgery.

00:13:49.181 --> 00:13:54.335
And this can be built for 40 times cheaper
than a cochlear implant,

00:13:54.335 --> 00:13:59.234
which opens up this technology globally,
even for the poorest countries.

00:14:00.052 --> 00:14:05.171
Now, we've been very encouraged
by our results with sensory substitution,

00:14:05.171 --> 00:14:09.374
but what we've been thinking a lot about
is sensory addition.

00:14:09.374 --> 00:14:14.803
How could we use a technology like this
to add a completely new kind of sense,

00:14:14.803 --> 00:14:17.937
to expand the human umvelt?

00:14:17.937 --> 00:14:22.186
For example, could we feed
real-time data from the Internet

00:14:22.186 --> 00:14:24.067
directly into somebody's brain,

00:14:24.067 --> 00:14:27.945
and can they develop a direct
perceptual experience?

00:14:27.945 --> 00:14:30.482
So here's an experiment
we're doing in the lab.

00:14:30.482 --> 00:14:34.376
A subject is feeling a real-time
streaming feed from the Net of data

00:14:34.376 --> 00:14:36.187
for five seconds.

00:14:36.187 --> 00:14:39.456
Then, two buttons appear,
and he has to make a choice.

00:14:39.456 --> 00:14:41.145
He doesn't know what's going on.

00:14:41.145 --> 00:14:43.841
He makes a choice,
and he gets feedback after one second.

00:14:43.841 --> 00:14:45.046
Now, here's the thing:

00:14:45.046 --> 00:14:47.690
The subject has no idea
what all the patterns mean,

00:14:47.690 --> 00:14:51.361
but we're seeing if he gets better
at figuring out which button to press.

00:14:51.361 --> 00:14:53.428
He doesn't know that what we're feeding

00:14:53.428 --> 00:14:56.609
is real-time data from the stock market,

00:14:56.609 --> 00:14:59.116
and he's making buy and sell decisions.

00:14:59.116 --> 00:15:00.870
(Laughter)

00:15:01.490 --> 00:15:04.792
And the feedback is telling him
whether he did the right thing or not.

00:15:04.792 --> 00:15:07.661
And what we're seeing is,
can we expand the human umvelt

00:15:07.661 --> 00:15:10.656
so that he comes to have,
after several weeks,

00:15:10.656 --> 00:15:16.763
a direct perceptual experience
of the economic movements of the planet.

00:15:16.763 --> 00:15:20.129
So we'll report on that later
to see how well this goes.

00:15:20.129 --> 00:15:21.950
(Laughter)

00:15:22.730 --> 00:15:24.820
Here's another thing we're doing:

00:15:24.820 --> 00:15:29.417
During the talks this morning,
we've been automatically scraping Twitter

00:15:29.417 --> 00:15:31.855
for the TED2015 hashtag,

00:15:31.855 --> 00:15:34.548
and we've been doing
an automated sentiment analysis,

00:15:34.548 --> 00:15:39.123
which means, are people using positive
words or negative words or neutral?

00:15:39.123 --> 00:15:41.567
And while this has been going on,

00:15:41.567 --> 00:15:44.562
I have been feeling this,

00:15:44.562 --> 00:15:48.835
and so I am plugged in
to the aggregate emotion

00:15:48.835 --> 00:15:52.991
of thousands of people in real time,

00:15:52.991 --> 00:15:56.729
and that's a new kind of human experience,
because now I can know

00:15:56.729 --> 00:16:00.026
how everyone's doing
and how much you're loving this.

00:16:00.026 --> 00:16:05.159
(Laughter) (Applause)

00:16:06.899 --> 00:16:11.255
It's a bigger experience
than a human can normally have.

00:16:11.845 --> 00:16:14.538
We're also expanding the umvelt of pilots.

00:16:14.538 --> 00:16:18.624
So in this case, the vest is streaming
nine different measures

00:16:18.624 --> 00:16:20.250
from this quadcopter,

00:16:20.250 --> 00:16:23.617
so pitch and yaw and roll
and orientation and heading,

00:16:23.617 --> 00:16:27.703
and that improves
this pilot's ability to fly it.

00:16:27.703 --> 00:16:32.998
It's essentially like he's extending
his skin up there, far away.

00:16:32.998 --> 00:16:34.553
And that's just the beginning.

00:16:34.553 --> 00:16:40.357
What we're envisioning is taking
a modern cockpit full of gauges

00:16:40.357 --> 00:16:44.908
and instead of trying
to read the whole thing, you feel it.

00:16:44.908 --> 00:16:47.393
We live in a world of information now,

00:16:47.393 --> 00:16:51.201
and there is a difference
between accessing big data

00:16:51.201 --> 00:16:54.289
and experiencing it.

00:16:54.289 --> 00:16:58.114
So I think there's really no end
to the possibilities

00:16:58.114 --> 00:17:00.436
on the horizon for human expansion.

00:17:00.436 --> 00:17:05.358
Just imagine an astronaut
being able to feel

00:17:05.358 --> 00:17:08.679
the overall health
of the International Space Station,

00:17:08.679 --> 00:17:13.555
or, for that matter, having you feel
the invisible states of your own health,

00:17:13.555 --> 00:17:17.494
like your blood sugar
and the state of your microbiome,

00:17:17.494 --> 00:17:23.121
or having 360-degree vision
or seeing in infrared or ultraviolet.

00:17:23.121 --> 00:17:26.616
So the key is this:
As we move into the future,

00:17:26.616 --> 00:17:31.515
we're going to increasingly be able
to choose our own peripheral devices.

00:17:31.515 --> 00:17:35.369
We no longer have to wait
for Mother Nature's sensory gifts

00:17:35.369 --> 00:17:37.227
on her timescales,

00:17:37.227 --> 00:17:41.499
but instead, like any good parent,
she's given us the tools that we need

00:17:41.499 --> 00:17:45.632
to go out and define our own trajectory.

00:17:45.632 --> 00:17:47.373
So the question now is,

00:17:47.373 --> 00:17:52.598
how do you want to go out
and experience your universe?

00:17:52.598 --> 00:17:54.641
Thank you.

00:17:54.641 --> 00:18:02.977
(Applause)

00:18:11.365 --> 00:18:13.553
Chris Anderson: Can you feel it?
DE: Yeah.

00:18:13.553 --> 00:18:16.943
Actually, this was the first time
I felt applause on the vest.

00:18:16.943 --> 00:18:19.102
It's nice. It's like a massage. (Laughter)

00:18:19.102 --> 00:18:22.747
CA: Twitter's going crazy.
Twitter's going mad.

00:18:22.747 --> 00:18:25.040
So that stock market experiment.

00:18:25.040 --> 00:18:29.568
This could be the first experiment
that secures its funding forevermore,

00:18:29.568 --> 00:18:31.563
right, if successful?

00:18:31.563 --> 00:18:34.715
DE: Well, that's right, I wouldn't
have to write to NIH anymore.

00:18:34.715 --> 00:18:37.532
CA: Well look, just to be
skeptical for a minute,

00:18:37.532 --> 00:18:40.702
I mean, this is amazing,
but isn't most of the evidence so far

00:18:40.702 --> 00:18:43.049
that sensory substitution works,

00:18:43.049 --> 00:18:45.156
not necessarily 
that sensory addition works?

00:18:45.156 --> 00:18:48.793
I mean, isn't it possible that the
blind person can see through their tongue

00:18:48.793 --> 00:18:53.971
because the visual cortex is still there,
ready to process,

00:18:53.971 --> 00:18:55.790
and that that is needed as part of it?

00:18:55.790 --> 00:18:58.434
DE: That's a great question.
We actually have no idea

00:18:58.434 --> 00:19:02.330
what the theoretical limits are of what
kind of data the brain can take in.

00:19:02.330 --> 00:19:05.378
The general story, though,
is that it's extraordinarily flexible.

00:19:05.402 --> 00:19:09.207
So when a person goes blind,
what we used to call their visual cortex

00:19:09.207 --> 00:19:14.265
gets taken over by other things,
by touch, by hearing, by vocabulary.

00:19:14.265 --> 00:19:18.327
So what that tells us is that
the cortex is kind of a one-trick pony.

00:19:18.327 --> 00:19:20.975
It just runs certain kinds
of computations on things.

00:19:20.975 --> 00:19:24.076
And when we look around
at things like braille, for example,

00:19:24.076 --> 00:19:27.165
people are getting information
through bumps on their fingers.

00:19:27.165 --> 00:19:30.820
So I don't think we have any reason
to think there's a theoretical limit

00:19:30.820 --> 00:19:32.334
that we know the edge of.

00:19:33.244 --> 00:19:36.508
CA: If this checks out,
you're going to be deluged.

00:19:36.508 --> 00:19:39.759
There are so many
possible applications for this.

00:19:39.759 --> 00:19:43.690
Are you ready for this? What are you most
excited about, the direction it might go?

00:19:43.690 --> 00:19:46.267
DE: I mean, I think there's
a lot of applications here.

00:19:46.267 --> 00:19:49.715
In terms of beyond sensory substitution,
the things I started mentioning

00:19:49.715 --> 00:19:54.085
about astronauts on the space station,
they spend a lot of their time

00:19:54.085 --> 00:19:57.304
monitoring things, and they could instead
just get what's going on,

00:19:57.304 --> 00:20:00.764
because what this is really good for
is multidimensional data.

00:20:00.764 --> 00:20:05.547
The key is this: Our visual systems
are good at detecting blobs and edges,

00:20:05.547 --> 00:20:07.995
but they're really bad
at what our world has become,

00:20:07.995 --> 00:20:10.182
which is screens
with lots and lots of data.

00:20:10.182 --> 00:20:12.585
We have to crawl that
with our attentional systems.

00:20:12.585 --> 00:20:15.255
So this is a way of just
feeling the state of something,

00:20:15.255 --> 00:20:18.849
just like the way you know the state
of your body as you're standing around.

00:20:18.849 --> 00:20:22.028
So I think heavy machinery, safety,
feeling the state of a factory,

00:20:22.028 --> 00:20:25.092
of your equipment, that's one place
it'll go right away.

00:20:25.092 --> 00:20:28.797
CA: David Eagleman, that was one
mind-blowing talk. Thank you very much.

00:20:28.797 --> 00:20:33.576
DE: Thank you, Chris.
(Applause)

