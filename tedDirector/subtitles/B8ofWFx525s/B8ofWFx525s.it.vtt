WEBVTT
Kind: captions
Language: it

00:00:00.000 --> 00:00:07.000
Traduttore: Daniele Buratti
Revisore: Elena Montrasio

00:00:15.260 --> 00:00:17.260
Mark Zuckerberg,

00:00:17.260 --> 00:00:20.260
un giornalista gli aveva posto una domanda sulla logica delle notizie proposte.

00:00:20.260 --> 00:00:22.260
La domanda era,

00:00:22.260 --> 00:00:24.260
"Perché è tanto importante?"

00:00:24.260 --> 00:00:26.260
E Zuckerberg,

00:00:26.260 --> 00:00:28.260
"Uno scoiattolo morente nel tuo giardino

00:00:28.260 --> 00:00:31.260
potrebbe essere più vicino ai tuoi interessi, in questo momento,

00:00:31.260 --> 00:00:34.260
di quanto lo sia la gente che muore in Africa."

00:00:34.260 --> 00:00:36.260
Io vi voglio parlare di quali risvolti

00:00:36.260 --> 00:00:39.260
potrebbe avere una Rete che si basi sull'aspetto di rilevanza delle notizie.

00:00:40.260 --> 00:00:42.260
Durante la mia adolescenza

00:00:42.260 --> 00:00:44.260
in una zona rurale del Maine,

00:00:44.260 --> 00:00:47.260
Internet aveva per me un significato molto diverso.

00:00:47.260 --> 00:00:49.260
Era una connessione con il mondo.

00:00:49.260 --> 00:00:52.260
Qualcosa che ci avrebbe unito tutti.

00:00:52.260 --> 00:00:55.260
Ero sicuro che sarebbe stato grandioso per la democrazia

00:00:55.260 --> 00:00:58.260
e per la società.

00:00:58.260 --> 00:01:00.260
Ma ora c'è questo cambiamento

00:01:00.260 --> 00:01:02.260
del modo in cui le informazioni vengono fornite,

00:01:02.260 --> 00:01:05.260
ed è impercettibile.

00:01:05.260 --> 00:01:07.260
Se non gli prestiamo attenzione

00:01:07.260 --> 00:01:10.260
ci potremmo trovare presto nei guai.

00:01:10.260 --> 00:01:13.260
Ho cominciato a rendermene conto in un luogo in cui passo molto tempo --

00:01:13.260 --> 00:01:15.260
la mia pagina di Facebook.

00:01:15.260 --> 00:01:18.260
Politicamente -- senti senti -- sono progressista --

00:01:18.260 --> 00:01:20.260
ma sono sempre stato aperto alle idee dei conservatori.

00:01:20.260 --> 00:01:22.260
Mi piace ascoltare come la pensino;

00:01:22.260 --> 00:01:24.260
vedere i loro riferimenti;

00:01:24.260 --> 00:01:26.260
mi piace sempre imparare qualcosa.

00:01:26.260 --> 00:01:29.260
Perciò un giorno sono rimasto sorpreso nel vedere

00:01:29.260 --> 00:01:32.260
che i conservatori erano scomparsi dal mio aggregatore di notizie su Facebook.

00:01:33.260 --> 00:01:35.260
E quello che ho scoperto

00:01:35.260 --> 00:01:39.260
è che Facebook prendeva nota dei siti che visitavo

00:01:39.260 --> 00:01:41.260
determinando che, effettivamente,

00:01:41.260 --> 00:01:43.260
visitavo maggiormente i siti dei miei amici liberali

00:01:43.260 --> 00:01:46.260
rispetto a quelli degli amici conservatori.

00:01:46.260 --> 00:01:48.260
E senza alcun preavviso

00:01:48.260 --> 00:01:50.260
li ha esclusi dal mio mondo.

00:01:50.260 --> 00:01:53.260
Scomparsi.

00:01:54.260 --> 00:01:56.260
Ma Facebook non è l'unico

00:01:56.260 --> 00:01:58.260
a operare questa selezione algoritmica e invisibile

00:01:58.260 --> 00:02:01.260
dei contenuti della Rete.

00:02:01.260 --> 00:02:03.260
Anche Google lo fa.

00:02:03.260 --> 00:02:06.260
Se io e te facciamo una ricerca,

00:02:06.260 --> 00:02:08.260
proprio adesso e contemporaneamente,

00:02:08.260 --> 00:02:11.260
potremmo ottenere risultati molto diversi.

00:02:11.260 --> 00:02:14.260
Anche se non sei connesso, mi ha detto un ingegnere,

00:02:14.260 --> 00:02:16.260
ci sono 57 segnali

00:02:16.260 --> 00:02:19.260
che vengono analizzati da Google --

00:02:19.260 --> 00:02:22.260
qualsiasi cosa, dal computer che stai utilizzando

00:02:22.260 --> 00:02:24.260
al tuo programma di navigazione,

00:02:24.260 --> 00:02:26.260
e anche dove ti trovi --

00:02:26.260 --> 00:02:29.260
li usa per adattare a te i risultati della ricerca.

00:02:29.260 --> 00:02:31.260
Pensateci un attimo:

00:02:31.260 --> 00:02:35.260
non c'è più un Google uguale per tutti.

00:02:35.260 --> 00:02:38.260
E sapete, la cosa divertente è che non è facile accorgersene.

00:02:38.260 --> 00:02:40.260
Voi non potete vedere quanto i risultati della vostra ricerca

00:02:40.260 --> 00:02:42.260
siano diversi da quelli degli altri.

00:02:42.260 --> 00:02:44.260
Un paio di settimane fa

00:02:44.260 --> 00:02:47.260
ho chiesto ad alcuni amici di Google "Egitto"

00:02:47.260 --> 00:02:50.260
di inviarmi le loro schermate della pagina principale del sito.

00:02:50.260 --> 00:02:53.260
Questa è quella del mio amico Scott.

00:02:54.260 --> 00:02:57.260
E questa è quella di Daniel.

00:02:57.260 --> 00:02:59.260
Se le confrontate

00:02:59.260 --> 00:03:01.260
non c'è nemmeno bisogno di leggere i collegamenti

00:03:01.260 --> 00:03:03.260
per notare quanto siano diverse.

00:03:03.260 --> 00:03:05.260
Ma se poi leggete i collegamenti

00:03:05.260 --> 00:03:08.260
scoprirete qualcosa di interessante.

00:03:09.260 --> 00:03:12.260
Sul sito di Daniel non c'erano riferimenti alle proteste in Egitto

00:03:12.260 --> 00:03:14.260
nella prima pagina di risultati.

00:03:14.260 --> 00:03:16.260
Mentre quella di Scott ne era piena.

00:03:16.260 --> 00:03:18.260
Questa era la notizia del giorno all'epoca.

00:03:18.260 --> 00:03:21.260
Ecco quanto possono essere diversi i risultati.

00:03:21.260 --> 00:03:24.260
E non è limitato a Google e Facebook.

00:03:24.260 --> 00:03:26.260
Sta succedendo dappertutto nella Rete.

00:03:26.260 --> 00:03:29.260
C'è un gran numero di aziende che sta operando questa personalizzazione.

00:03:29.260 --> 00:03:32.260
Yahoo News, il maggiore sito di notizie di Internet

00:03:32.260 --> 00:03:35.260
ora è personalizzato -- persone diverse ottengono informazioni diverse.

00:03:36.260 --> 00:03:39.260
L'Huffington Post, il Wasihngton Post, il New York Times --

00:03:39.260 --> 00:03:42.260
tutti, in vari modi, civettano con la personalizzazione.

00:03:42.260 --> 00:03:45.260
E ci troveremo presto

00:03:45.260 --> 00:03:47.260
in un mondo in cui

00:03:47.260 --> 00:03:51.260
Internet ci mostrerà ciò che pensa noi vogliamo vedere,

00:03:51.260 --> 00:03:54.260
ma non necessariamente quello che dovremmo vedere.

00:03:54.260 --> 00:03:57.260
Come ha detto Eric Schmidt,

00:03:57.260 --> 00:04:00.260
"Sarà molto difficile per le persone vedere o consumare qualcosa

00:04:00.260 --> 00:04:02.260
che non sia stato in qualche modo

00:04:02.260 --> 00:04:05.260
fatto su misura per loro."

00:04:05.260 --> 00:04:07.260
E penso che questo sia un problema.

00:04:07.260 --> 00:04:10.260
Penso che, se mettiamo insieme tutti questi filtri,

00:04:10.260 --> 00:04:12.260
tutti gli algoritmi,

00:04:12.260 --> 00:04:15.260
otteniamo qualcosa che chiamo "gabbia di filtri".

00:04:16.260 --> 00:04:19.260
E la vostra gabbia rappresenta il vostro universo

00:04:19.260 --> 00:04:21.260
personale e unico di informazioni

00:04:21.260 --> 00:04:23.260
che vivete online.

00:04:23.260 --> 00:04:26.260
Ciò che troverete nella vostra gabbia

00:04:26.260 --> 00:04:29.260
dipende da chi siete e da che cosa fate.

00:04:29.260 --> 00:04:33.260
Il problema è che voi non prendete decisioni su cosa debba entrare.

00:04:33.260 --> 00:04:35.260
Ancora più importante,

00:04:35.260 --> 00:04:38.260
non riuscite a vedere cosa ne resta fuori.

00:04:38.260 --> 00:04:40.260
Dunque uno dei problemi di questa gabbia

00:04:40.260 --> 00:04:43.260
è stato scoperto da alcuni ricercatori di Netflix.

00:04:43.260 --> 00:04:46.260
Analizzando le richieste di noleggio di film degli utenti si sono accorti di qualcosa di strano

00:04:46.260 --> 00:04:48.260
che forse hanno notato anche molti di voi,

00:04:48.260 --> 00:04:50.260
ossia che ci sono alcuni titoli

00:04:50.260 --> 00:04:53.260
che appaiono o scompaiono dalle nostre liste.

00:04:53.260 --> 00:04:56.260
Entrano in lista ma all'improvviso vengono tolti.

00:04:56.260 --> 00:04:58.260
E magari "Iron Man" scompare

00:04:58.260 --> 00:05:00.260
mentre "Aspettando Superman"

00:05:00.260 --> 00:05:02.260
ci rimane per molto tempo.

00:05:02.260 --> 00:05:04.260
Si sono accorti

00:05:04.260 --> 00:05:06.260
che tra le liste di Netflix

00:05:06.260 --> 00:05:09.260
è in corso una battaglia epica

00:05:09.260 --> 00:05:12.260
tra le nostre aspirazioni future

00:05:12.260 --> 00:05:15.260
e quello che vogliamo impulsivamente adesso.

00:05:15.260 --> 00:05:17.260
Sapete, noi tutti vorremmo

00:05:17.260 --> 00:05:19.260
aver visto "Rashomon",

00:05:19.260 --> 00:05:21.260
ma in questo momento

00:05:21.260 --> 00:05:24.260
vogliamo guardare "Ace Ventura" per la quarta volta.

00:05:24.260 --> 00:05:27.260
(Risate)

00:05:27.260 --> 00:05:29.260
Certo il massimo sarebbe avere un po' di entrambi.

00:05:29.260 --> 00:05:31.260
Un po' di Justin Bieber

00:05:31.260 --> 00:05:33.260
e un po' di Afghanistan.

00:05:33.260 --> 00:05:35.260
Un po' di contorno

00:05:35.260 --> 00:05:38.260
e un po' di dessert.

00:05:38.260 --> 00:05:40.260
La sfida con questo tipo di filtri algoritmici,

00:05:40.260 --> 00:05:42.260
questi filtri personalizzati,

00:05:42.260 --> 00:05:44.260
è che, dato che rilevano prevalentemente

00:05:44.260 --> 00:05:48.260
cosa viene cliccato per primo,

00:05:48.260 --> 00:05:52.260
possono alterare questo equilibrio.

00:05:52.260 --> 00:05:55.260
E invece di informazioni equilibrate

00:05:55.260 --> 00:05:57.260
alla fine avrete

00:05:57.260 --> 00:05:59.260
informazioni spazzatura.

00:05:59.260 --> 00:06:01.260
Questo ci suggerisce

00:06:01.260 --> 00:06:04.260
che forse abbiamo interpretato male la storia di Internet.

00:06:04.260 --> 00:06:06.260
In una società di trasmisisoni --

00:06:06.260 --> 00:06:08.260
è così che nascono i miti --

00:06:08.260 --> 00:06:10.260
in una società di trasmissioni --

00:06:10.260 --> 00:06:12.260
c'erano i guardiani, gli editori,

00:06:12.260 --> 00:06:15.260
che controllavano i flussi di informazioni.

00:06:15.260 --> 00:06:18.260
E poi venne Internet che li spazzò via

00:06:18.260 --> 00:06:20.260
e permise a tutti noi di collegarci l'un l'altro,

00:06:20.260 --> 00:06:22.260
ed era fantastico.

00:06:22.260 --> 00:06:25.260
Ma non è quello che sta succedendo adesso.

00:06:26.260 --> 00:06:29.260
Ciò a cui stiamo assistendo è più che altro il passaggio del testimone

00:06:29.260 --> 00:06:31.260
dai guardiani umani

00:06:31.260 --> 00:06:34.260
a quelli algoritmici.

00:06:34.260 --> 00:06:37.260
Il problema è che gli algoritmi

00:06:37.260 --> 00:06:40.260
non hanno ancora incorporato i principi etici

00:06:40.260 --> 00:06:43.260
propri degli editori.

00:06:43.260 --> 00:06:46.260
Dunque se gli algoritmi dovranno prendersi cura del mondo in nostra vece,

00:06:46.260 --> 00:06:49.260
se ci diranno cosa vedere e cosa non vedere,

00:06:49.260 --> 00:06:51.260
allora dobbiamo assicurarci

00:06:51.260 --> 00:06:54.260
che non siano guidati unicamente dalla rilevanza.

00:06:54.260 --> 00:06:56.260
Dobbiamo far sì che ci mostrino anche cose

00:06:56.260 --> 00:06:59.260
scomode o stimolanti o importanti --

00:06:59.260 --> 00:07:01.260
è quello che fa TED --

00:07:01.260 --> 00:07:03.260
punti di vista differenti.

00:07:03.260 --> 00:07:05.260
Abbiamo già vissuto questa esperienza

00:07:05.260 --> 00:07:07.260
come società.

00:07:08.260 --> 00:07:11.260
Nel 1915 non è che i giornali si preoccupassero molto

00:07:11.260 --> 00:07:14.260
della loro responsabilità civile.

00:07:14.260 --> 00:07:16.260
Ma poi la gente si accorse

00:07:16.260 --> 00:07:19.260
che svolgevano un ruolo davvero importante.

00:07:19.260 --> 00:07:21.260
Che, in effetti, non si può avere

00:07:21.260 --> 00:07:23.260
una democrazia che funzioni

00:07:23.260 --> 00:07:27.260
se i cittadini non hanno accesso all'informazione.

00:07:28.260 --> 00:07:31.260
Che i giornali erano fondamentali perché agivano da filtro,

00:07:31.260 --> 00:07:33.260
e poi è nata l'etica del giornalismo.

00:07:33.260 --> 00:07:35.260
Non era perfetta,

00:07:35.260 --> 00:07:38.260
ma ci ha fatto attraversare lo scorso secolo.

00:07:38.260 --> 00:07:40.260
Ed ora

00:07:40.260 --> 00:07:43.260
ci troviamo nel 1915 della Rete.

00:07:44.260 --> 00:07:47.260
Abbiamo bisogno di nuovi guardiani

00:07:47.260 --> 00:07:49.260
che incorporino quel tipo di responsabilità

00:07:49.260 --> 00:07:51.260
nel codice che stanno scrivendo.

00:07:51.260 --> 00:07:54.260
So che tra i presenti c'è gente di Facebook e Google --

00:07:54.260 --> 00:07:56.260
Larry e Sergey --

00:07:56.260 --> 00:07:58.260
gente che ha contribuito a costruire la Rete come è oggi,

00:07:58.260 --> 00:08:00.260
e sono loro grato per questo.

00:08:00.260 --> 00:08:03.260
Ma c'è davvero bisogno di far sì che

00:08:03.260 --> 00:08:06.260
questi algoritmi contengano

00:08:06.260 --> 00:08:09.260
un senso di vita pubblica, di responsabilità civile.

00:08:09.260 --> 00:08:12.260
Dovete fare in modo che siano abbastanza trasparenti

00:08:12.260 --> 00:08:14.260
da consentirci di vedere le regole

00:08:14.260 --> 00:08:17.260
che determinano cosa può passare attraverso i filtri.

00:08:17.260 --> 00:08:19.260
E dovete darci la facoltà di controllo,

00:08:19.260 --> 00:08:21.260
per decidere

00:08:21.260 --> 00:08:24.260
cosa può passare e cosa deve essere bloccato.

00:08:24.260 --> 00:08:26.260
Perché penso

00:08:26.260 --> 00:08:28.260
che abbiamo bisogno che Internet sia davvero

00:08:28.260 --> 00:08:30.260
quella cosa che abbiamo sempre sognato.

00:08:30.260 --> 00:08:33.260
Ne abbiamo bisogno per essere connessi tra di noi.

00:08:33.260 --> 00:08:36.260
Perché ci presenti nuove idee,

00:08:36.260 --> 00:08:39.260
nuove persone e prospettive differenti.

00:08:40.260 --> 00:08:42.260
E questo non sarà possibile

00:08:42.260 --> 00:08:45.260
se rimarremo isolati in una Rete individuale.

00:08:45.260 --> 00:08:47.260
Grazie.

00:08:47.260 --> 00:08:58.260
(Applausi)

