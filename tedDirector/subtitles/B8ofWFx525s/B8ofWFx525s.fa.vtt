WEBVTT
Kind: captions
Language: fa

00:00:00.000 --> 00:00:07.000
Translator: Sepideh Sekehchi
Reviewer: soheila Jafari

00:00:15.260 --> 00:00:17.260
یک روزنامه‌نگار

00:00:17.260 --> 00:00:20.260
از "مارک زاکربرگ " (مؤسس فیس‌بوک)

00:00:20.260 --> 00:00:22.260
دربارۀ خوراک خبری پرسید.

00:00:22.260 --> 00:00:24.260
« چرا این اینقدر مهم است؟ »

00:00:24.260 --> 00:00:26.260
و "مارک زاکربرگ " گفت:

00:00:26.260 --> 00:00:28.260
« یک سنجاب در حال مرگ در حیاط خانۀ شما

00:00:28.260 --> 00:00:31.260
ممکن است در حال حاضر بیشتر مورد علاقه و توجه شما

00:00:31.260 --> 00:00:34.260
تا مردم در حال مرگ در آفریقا »

00:00:34.260 --> 00:00:36.260
و من می‌خواهم بگویم اینترنت بر اساس

00:00:36.260 --> 00:00:39.260
ایدۀ مرتبط بودن با علاقه، چه شکلی خواهد بود.

00:00:40.260 --> 00:00:42.260
وقتی در یک ناحیه روستایی در " مین "

00:00:42.260 --> 00:00:44.260
بزرگ می‌شدم،

00:00:44.260 --> 00:00:47.260
اینترنت برای من معنای خیلی متفاوتی داشت.

00:00:47.260 --> 00:00:49.260
به معنی اتصال با دنیا بود.

00:00:49.260 --> 00:00:52.260
به معنی چیزی بود که همۀ ما را به هم متصل می‌کند.

00:00:52.260 --> 00:00:55.260
و مطمئن بودم که برای دموکراسی و برای جامعۀ ما

00:00:55.260 --> 00:00:58.260
فوق‌العاده خواهد بود.

00:00:58.260 --> 00:01:00.260
اما تغییری در

00:01:00.260 --> 00:01:02.260
نحوۀ جریان اطلاعات آنلاین

00:01:02.260 --> 00:01:05.260
به وجود آمده که نامرئی است.

00:01:05.260 --> 00:01:07.260
و اگر به آن توجه نکنیم

00:01:07.260 --> 00:01:10.260
می‌تواند یک مشکل اساسی شود.

00:01:10.260 --> 00:01:13.260
من اولین بار در جایی که زمان خیلی زیادی را در صفحه فیس بوک صرف کردم

00:01:13.260 --> 00:01:15.260
این را متوجه شدم.

00:01:15.260 --> 00:01:18.260
من از لحاظ سیاسی ترقی‌خواه هستم -- شگفتی بزرگ--

00:01:18.260 --> 00:01:20.260
ولی همیشه برای ملاقات با محافظه‌کاران مسیرم را کج کرده‌ام.

00:01:20.260 --> 00:01:22.260
دوست دارم بشنوم به چی فکر می‌کنند.

00:01:22.260 --> 00:01:24.260
دوست دارم ببینم به چی لینک می‌دهند.

00:01:24.260 --> 00:01:26.260
دوست دارم یکی دوتا چیز یاد بگیرم.

00:01:26.260 --> 00:01:29.260
پس سوپرایز شدم وقتی یک روز متوجه شدم

00:01:29.260 --> 00:01:32.260
که محافظه‌کارها از خوراک فیس بوک من حذف شده‌اند.

00:01:33.260 --> 00:01:35.260
و معلوم شد که جریان این بوده که

00:01:35.260 --> 00:01:39.260
فیس بوک به لینک‌‌هایی که روی آنها کلیک می‌کردم

00:01:39.260 --> 00:01:41.260
نگاه می‌کرده و متوجه شده که ، در واقع

00:01:41.260 --> 00:01:43.260
من بیشتر روی لینک‌‌های دوستان لیبرالم کلیک می‌کردم

00:01:43.260 --> 00:01:46.260
تا روی لینک‌‌های دوستان محافظه‌کارم.

00:01:46.260 --> 00:01:48.260
و بدون مشورت با من

00:01:48.260 --> 00:01:50.260
آنان را حذف کرده بود.

00:01:50.260 --> 00:01:53.260
آنان ناپدید شدند.

00:01:54.260 --> 00:01:56.260
خب فیس بوک تنها جایی نیست که

00:01:56.260 --> 00:01:58.260
این ویرایش نامرئی و الگوریتمی

00:01:58.260 --> 00:02:01.260
را انجام می‌دهد.

00:02:01.260 --> 00:02:03.260
گوگل هم این کار را می‌کند.

00:02:03.260 --> 00:02:06.260
اگر من چیزی را جستجو کنم و شما هم جستجو کنید

00:02:06.260 --> 00:02:08.260
حتی اگر هم زمان باشد

00:02:08.260 --> 00:02:11.260
احتمالا" نتایج خیلی متفاوتی را می گیریم.

00:02:11.260 --> 00:02:14.260
یک مهندس به من گفت

00:02:14.260 --> 00:02:16.260
حتی اگر خارج از حساب کاربریتان باشید،

00:02:16.260 --> 00:02:19.260
گوگل به 57 نشانه نگاه می‌کند.

00:02:19.260 --> 00:02:22.260
همه چیز از نوع کامپیوتری که با آن به شبکه وصل شده‌اید

00:02:22.260 --> 00:02:24.260
تا مرورگری که از آن استفاده می‌کنید

00:02:24.260 --> 00:02:26.260
و تا موقعیت مکانی شما.

00:02:26.260 --> 00:02:29.260
که گوگل از آنها برای انجام اصلاحات بر روی نتایج جستجو استفاده می‌کند.

00:02:29.260 --> 00:02:31.260
یک ثانیه به این فکر کنید

00:02:31.260 --> 00:02:35.260
دیگر هیچ گوگل استانداردی وجود ندارد.

00:02:35.260 --> 00:02:38.260
و مسخره‌ترین چیز این است که این سخت قابل تشخیص است.

00:02:38.260 --> 00:02:40.260
شما نمی‌توانید ببینید نتایج جستجوی شما

00:02:40.260 --> 00:02:42.260
چقدر با بقیه تفاوت دارد.

00:02:42.260 --> 00:02:44.260
ولی چند هفته پیش

00:02:44.260 --> 00:02:47.260
من از چندتا دوست خواستم " مصر " را در گوگل جستچو کنند

00:02:47.260 --> 00:02:50.260
و تصویر صفحۀ نتایج را برای من بفرستند.

00:02:50.260 --> 00:02:53.260
خب این تصویر صفحه نمایش دوستم " اسکات " است.

00:02:54.260 --> 00:02:57.260
و این تصویر صفحه دوستم " دانیل " است.

00:02:57.260 --> 00:02:59.260
اگر آنها را کنار هم بگذارید

00:02:59.260 --> 00:03:01.260
حتی لازم نیست لینک‌‌ها را بخوانید تا

00:03:01.260 --> 00:03:03.260
متوجه بشوید چقدر این دو صفحه با هم فرق دارند.

00:03:03.260 --> 00:03:05.260
ولی وقتی که لینک‌‌ها را بخوانید

00:03:05.260 --> 00:03:08.260
این تفاوت واقعا چشمگیر است.

00:03:09.260 --> 00:03:12.260
" دانیل " در صفحۀ اول نتایج گوگل

00:03:12.260 --> 00:03:14.260
هیچ اطلاعاتی از تظاهرات در " مصر" نگرفت.

00:03:14.260 --> 00:03:16.260
نتایج جستجوی " اسکات " پر از آنها بود.

00:03:16.260 --> 00:03:18.260
و این تظاهرات خبر اصلی روز در آن زمان بود.

00:03:18.260 --> 00:03:21.260
این نتایج تا این اندازه در حال متفاوت شدن هستند.

00:03:21.260 --> 00:03:24.260
خوب فقط گوگل و فیس بوک نیستند.

00:03:24.260 --> 00:03:26.260
این چیزی است که در سراسر اینترنت پخش شده است.

00:03:26.260 --> 00:03:29.260
انبوهی از شرکت‌‌ها هستند که این نوع شخصی سازی را انجام می‌دهند.

00:03:29.260 --> 00:03:32.260
" یاهو نیوز " بزرگترین سایت خبری اینترنت

00:03:32.260 --> 00:03:35.260
دیگر الان شخصی سازی شده -- افراد مختلف، اخبار مختلف می‌گیرند.

00:03:36.260 --> 00:03:39.260
" هافینگتون پست "، " واشنگتن پست "، " نیویورک تایمز "

00:03:39.260 --> 00:03:42.260
همه از راه‌های متنوع با شخصی سازی سر و کار دارند.

00:03:42.260 --> 00:03:45.260
و این ما را به سرعت به طرف

00:03:45.260 --> 00:03:47.260
جهانی سوق می‌دهد که در آن

00:03:47.260 --> 00:03:51.260
اینترنت چیزی را به ما نشان می‌دهد که فکر می‌کند می‌خواهیم ببینیم

00:03:51.260 --> 00:03:54.260
ولی نه لزوما چیزی را که نیاز داریم ببینیم.

00:03:54.260 --> 00:03:57.260
همانطور که " اریک اشمیت " گفت:

00:03:57.260 --> 00:04:00.260
« برای مردم خیلی سخت خواهد بود که چیزی را تماشا کنند یا مصرف کنند

00:04:00.260 --> 00:04:02.260
که احساس نمی کنند

00:04:02.260 --> 00:04:05.260
برای آنان تهیه شده است »

00:04:05.260 --> 00:04:07.260
من واقعا فکر می‌کنم این یک مشکل است.

00:04:07.260 --> 00:04:10.260
و فکر می‌کنم اگر همۀ این فیلترها،

00:04:10.260 --> 00:04:12.260
همۀ این الگوریتم‌ها را در نظر بگیرید

00:04:12.260 --> 00:04:15.260
به همان حباب‌‌ فیلتری که می‌گویم می‌رسید.

00:04:16.260 --> 00:04:19.260
و حباب فیلتر شما، همان دنیای شخصی و خاص شما

00:04:19.260 --> 00:04:21.260
از اطلاعات است که

00:04:21.260 --> 00:04:23.260
در فضای مجازی در آن زندگی می‌کنید.

00:04:23.260 --> 00:04:26.260
و چیزی که در حباب فیلتر شما وجود دارد

00:04:26.260 --> 00:04:29.260
بستگی به این دارد که شما چه کسی هستید و چکارمی کننید.

00:04:29.260 --> 00:04:33.260
ولی مسئله این است که شما دربارۀ محتوای آن تصمیم نمی‌گیرید.

00:04:33.260 --> 00:04:35.260
و مهمتر اینکه

00:04:35.260 --> 00:04:38.260
شما در واقع نمی‌بینید چه چیزی حذف شده.

00:04:38.260 --> 00:04:40.260
خب یک مشکل ناشی از این حباب‌‌های فیلتر

00:04:40.260 --> 00:04:43.260
توسط چند محقق در " نتفلیکس" ( شرکت اجارۀ فیلم) کشف شد.

00:04:43.260 --> 00:04:46.260
وقتی به لیست‌‌های نتفلیکس نگاه می‌کردند متوجه چیز جالبی شدند

00:04:46.260 --> 00:04:48.260
که بیشتر ما احتمالا متوجه شده‌ایم.

00:04:48.260 --> 00:04:50.260
اینکه بعضی فیلم‌ها

00:04:50.260 --> 00:04:53.260
به سرعت به خانه‌های ما وارد و به سرعت خارج می‌شوند.

00:04:53.260 --> 00:04:56.260
وارد لیست می‌شوند و سریع می‌پرند بیرون.

00:04:56.260 --> 00:04:58.260
" مرد آهنی " می‌پره بیرون

00:04:58.260 --> 00:05:00.260
ولی " انتظار برای سوپرمن "

00:05:00.260 --> 00:05:02.260
مدت‌‌ها در لیست می‌ماند.

00:05:02.260 --> 00:05:04.260
چیزی که آنان کشف کردند

00:05:04.260 --> 00:05:06.260
این بود که در لیست‌‌های نتفلیکس ما،

00:05:06.260 --> 00:05:09.260
کشمکش عظیمی میان

00:05:09.260 --> 00:05:12.260
خودِ هدفدار و آرزومند ما در آینده

00:05:12.260 --> 00:05:15.260
و خودِ بلهوس ما در زمان حال، وجود دارد.

00:05:15.260 --> 00:05:17.260
ما همه می‌خواهیم کسی باشیم که

00:05:17.260 --> 00:05:19.260
" راشمون " (فیلم جنایی ژاپنی) را دیده است

00:05:19.260 --> 00:05:21.260
ولی الان

00:05:21.260 --> 00:05:24.260
می‌خواهیم " آیس ونتورا " ( کمدی) را برای چهارمین بار تماشا کنیم.

00:05:24.260 --> 00:05:27.260
( خنده )

00:05:27.260 --> 00:05:29.260
پس بهترین ویرایش ، به ما کمی از هر دو نوع را می‌دهد.

00:05:29.260 --> 00:05:31.260
کمی به ما " جاستین بیبر " ( خواننده- بازیگر نوجوان) را می‌دهد

00:05:31.260 --> 00:05:33.260
و کمی هم از افغانستان در اختیار ما می‌گذارد.

00:05:33.260 --> 00:05:35.260
مقداری اطلاعاتِ سبزیجات مانند،

00:05:35.260 --> 00:05:38.260
و مقداری اطلاعاتِ دسر مانند، را می دهد.

00:05:38.260 --> 00:05:40.260
و چالش موجود در این نوع فیلترهای الگوریتمی

00:05:40.260 --> 00:05:42.260
این فیلترهای شخصی شده ،

00:05:42.260 --> 00:05:44.260
که عموما" آنها به دنبال آنند،

00:05:44.260 --> 00:05:48.260
آنهایی هستند که شما اول بر روی آنها کلیک می کنید.

00:05:48.260 --> 00:05:52.260
این می تواند تعادل را بر هم زند.

00:05:52.260 --> 00:05:55.260
و به جای یک رژیم اطلاعاتی متعادل،

00:05:55.260 --> 00:05:57.260
شما ممکن است در میان

00:05:57.260 --> 00:05:59.260
اطلاعاتِ هله هوله مانند، قرار بگیرید.

00:05:59.260 --> 00:06:01.260
این نشان می‌دهد که در واقع

00:06:01.260 --> 00:06:04.260
داستانی که ما از اینترنت داریم اشتباه است.

00:06:04.260 --> 00:06:06.260
در یک جامعۀ رادیو تلویزیونی

00:06:06.260 --> 00:06:08.260
آنطور که در اساطیر آمده است،

00:06:08.260 --> 00:06:10.260
دربان‌هایی وجود داشتند،

00:06:10.260 --> 00:06:12.260
یعنی تدوین‌کننده‌ها یا سانسورچی‌ ها،

00:06:12.260 --> 00:06:15.260
و آنان جریان اطلاعات را کنترل می‌کردند

00:06:15.260 --> 00:06:18.260
تا اینکه اینترنت از راه رسید و آنان را از سر راه برداشت

00:06:18.260 --> 00:06:20.260
و این به ما اجازه داد به هم وصل شویم

00:06:20.260 --> 00:06:22.260
و همه چیز عالی بود.

00:06:22.260 --> 00:06:25.260
ولی این آن چیزی نیست که الان در جریان است.

00:06:26.260 --> 00:06:29.260
آنچه ما شاهدش هستیم دست به دست شدن مشعل

00:06:29.260 --> 00:06:31.260
از دربان‌های انسان

00:06:31.260 --> 00:06:34.260
به دربان‌های الگوریتمی است.

00:06:34.260 --> 00:06:37.260
و نکته این است که این الگوریتم‌ها

00:06:37.260 --> 00:06:40.260
آن اصول اخلاقی نهادینه شده را

00:06:40.260 --> 00:06:43.260
که تصحیح کننده‌ها داشتند، ندارند.

00:06:43.260 --> 00:06:46.260
پس اگر قرار است الگوریتم‌ها، دنیا را برای ما سازماندهی کنند،

00:06:46.260 --> 00:06:49.260
اگر قرار است آنها تصمیم بگیرند چه چیزی را ببینیم و چه چیز را نبینیم،

00:06:49.260 --> 00:06:51.260
ما باید مطمئن شویم

00:06:51.260 --> 00:06:54.260
که آنها تنها برای ربطِ علایق، تنظیم نشده باشند.

00:06:54.260 --> 00:06:56.260
باید مطمئن شویم آنها به ما چیزهایی را که

00:06:56.260 --> 00:06:59.260
ناراحت کننده، چالش برانگیز و مهم هستند، نشان می‌دهند.

00:06:59.260 --> 00:07:01.260
این کاری است که TED انجام می‌دهد

00:07:01.260 --> 00:07:03.260
-- یعنی نشان دادن نقطه نظرهای دیگر --

00:07:03.260 --> 00:07:05.260
و موضوع این است که ما قبلا به عنوان یک جامعه

00:07:05.260 --> 00:07:07.260
این را تجربه کرده‌ایم.

00:07:08.260 --> 00:07:11.260
در سال 1915، روزنامه‌ها خیلی هم برای

00:07:11.260 --> 00:07:14.260
مسئولیت‌‌های شهروندی خود عرق نمی‌ریختند.

00:07:14.260 --> 00:07:16.260
بعد مردم متوجه شدند که

00:07:16.260 --> 00:07:19.260
کار روزنامه‌ها بسیار مهم است.

00:07:19.260 --> 00:07:21.260
شما در واقع نمی‌توانید

00:07:21.260 --> 00:07:23.260
یک دموکراسی کارآمد داشته باشید

00:07:23.260 --> 00:07:27.260
اگر مردم یک جریان خوب اطلاعاتی نداشته باشند.

00:07:28.260 --> 00:07:31.260
وضعیت روزنامه‌ها وخیم بود چون آنها مانند فیلتر عمل می‌کردند.

00:07:31.260 --> 00:07:33.260
آن وقت بود که اخلاق روزنامه نگاری پرورانده شد.

00:07:33.260 --> 00:07:35.260
بی‌نقص نبود

00:07:35.260 --> 00:07:38.260
ولی ما را از قرن گذشته به سلامت عبور داد.

00:07:38.260 --> 00:07:40.260
و بنابراین حالا

00:07:40.260 --> 00:07:43.260
تا حدی در اینترنت به شرایط 1915 برگشته‌ایم

00:07:44.260 --> 00:07:47.260
و به دربان‌های جدید نیاز داریم

00:07:47.260 --> 00:07:49.260
که همان نوع تعهد را در کدها و اصولی که مشغول نوشتنشان هستند، وارد کنند.

00:07:49.260 --> 00:07:51.260
که همان نوع تعهد را در کدها و اصولی که مشغول نوشتنشان هستند، وارد کنند.

00:07:51.260 --> 00:07:54.260
می‌دانم که در گوگل و فیس بوک افراد زیادی مثل

00:07:54.260 --> 00:07:56.260
" لری " و " سرگی " هستند که

00:07:56.260 --> 00:07:58.260
برای ساختن اینترنت به صورتی که امروز هست، کمک کرده‌اند

00:07:58.260 --> 00:08:00.260
و من بسیار بابت آن سپاسگزار هستم

00:08:00.260 --> 00:08:03.260
ولی ما نیاز داریم که مطمئن شویم

00:08:03.260 --> 00:08:06.260
درک و فهم زندگی اجتماعی و حس مسئولیت شهروندی

00:08:06.260 --> 00:08:09.260
در این الگوریتم‌ها کد و وارد شده است.

00:08:09.260 --> 00:08:12.260
ما باید مطمئن شویم که آنها به اندازۀ کافی شفاف هستند

00:08:12.260 --> 00:08:14.260
که ما بتوانیم قوانینی که مشخص می‌کنند

00:08:14.260 --> 00:08:17.260
چه چیزی از فیلترها عبور کنند، ببینیم.

00:08:17.260 --> 00:08:19.260
و نیاز است که به ما این امکان داده شود

00:08:19.260 --> 00:08:21.260
تا بتوانیم تصمیم بگیریم

00:08:21.260 --> 00:08:24.260
چه چیزی از فیلتر عبور کند و چه چیزی عبور نکند.

00:08:24.260 --> 00:08:26.260
چون فکر می‌کنم

00:08:26.260 --> 00:08:28.260
ما واقعا نیاز داریم اینترنت همان چیزی باشد که

00:08:28.260 --> 00:08:30.260
همۀ ما رویای آن را در سر داریم.

00:08:30.260 --> 00:08:33.260
احتیاج داریم همۀ ما را به هم وصل کند

00:08:33.260 --> 00:08:36.260
و ایده‌های نو، افراد جدید

00:08:36.260 --> 00:08:39.260
و چشم‌اندازهای جدید به ما نشان دهد.

00:08:40.260 --> 00:08:42.260
و این کار انجام نخواهد شد

00:08:42.260 --> 00:08:45.260
اگر اینترنت ما را در فضای مجازی یک نفره ، از بقیه جدا کند.

00:08:45.260 --> 00:08:47.260
ممنون.

00:08:47.260 --> 00:08:58.260
( تشویق )

