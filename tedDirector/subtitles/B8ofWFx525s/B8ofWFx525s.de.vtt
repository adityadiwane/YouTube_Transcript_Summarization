WEBVTT
Kind: captions
Language: de

00:00:00.000 --> 00:00:07.000
Übersetzung: Judith Funke
Lektorat: Alex Boos

00:00:15.260 --> 00:00:17.260
Mark Zuckerberg

00:00:17.260 --> 00:00:20.260
wurde von einem Journalisten zum News Feed bei Facebook befragt.

00:00:20.260 --> 00:00:22.260
Der Journalist fragte ihn,

00:00:22.260 --> 00:00:24.260
"Warum ist er so wichtig?"

00:00:24.260 --> 00:00:26.260
Und Zuckerberg sagte,

00:00:26.260 --> 00:00:28.260
"Ein Eichhörnchen, das in Ihrem Vorgarten stirbt

00:00:28.260 --> 00:00:31.260
kann für Sie jetzt gerade relevanter sein

00:00:31.260 --> 00:00:34.260
als sterbende Menschen in Afrika."

00:00:34.260 --> 00:00:36.260
Und ich möchte darüber sprechen

00:00:36.260 --> 00:00:39.260
wie ein Internet aussähe, das auf dieser Auffassung von Relevanz basiert.

00:00:40.260 --> 00:00:42.260
Während meiner Jugend

00:00:42.260 --> 00:00:44.260
in einer sehr ländlichen Gegend in Maine

00:00:44.260 --> 00:00:47.260
war das Internet für mich etwas völlig anderes.

00:00:47.260 --> 00:00:49.260
Es war eine Verbindung zur Welt.

00:00:49.260 --> 00:00:52.260
Etwas, das uns alle miteinander verbinden würde.

00:00:52.260 --> 00:00:55.260
Und ich war sicher, dass es großartig für die Demokratie sein würde

00:00:55.260 --> 00:00:58.260
und für unsere Gesellschaft.

00:00:58.260 --> 00:01:00.260
Aber es gibt eine Verschiebung darin

00:01:00.260 --> 00:01:02.260
wie Informationen online fließen,

00:01:02.260 --> 00:01:05.260
und zwar eine unsichtbare.

00:01:05.260 --> 00:01:07.260
Und wenn wir nicht aufpassen,

00:01:07.260 --> 00:01:10.260
könnte sie ein echtes Problem werden.

00:01:10.260 --> 00:01:13.260
Mir ist sie zuerst an einem Ort aufgefallen, an dem ich viel Zeit verbringe --

00:01:13.260 --> 00:01:15.260
meiner Facebook-Seite.

00:01:15.260 --> 00:01:18.260
Ich bin politisch progressiv -- überraschenderweise --

00:01:18.260 --> 00:01:20.260
aber ich habe mich immer bemüht, Konservativen zu begegnen.

00:01:20.260 --> 00:01:22.260
Ich mag es, zu hören worüber sie nachdenken;

00:01:22.260 --> 00:01:24.260
Ich mag es, zu sehen was sie verlinken;

00:01:24.260 --> 00:01:26.260
Ich mag es, ein-zwei Dinge zu lernen.

00:01:26.260 --> 00:01:29.260
Also war ich überrascht, als ich eines Tages gemerkt habe

00:01:29.260 --> 00:01:32.260
dass die Konservativen aus meinen Facebook-Neuigkeiten verschwunden waren.

00:01:33.260 --> 00:01:35.260
Wie sich herausstellte

00:01:35.260 --> 00:01:39.260
hatte Facebook registriert, auf welche Links ich klickte,

00:01:39.260 --> 00:01:41.260
und hatte festgestellt, dass ich

00:01:41.260 --> 00:01:43.260
mehr auf die Links meiner liberalen Freunde klickte

00:01:43.260 --> 00:01:46.260
als auf die meiner konservativen Freunde.

00:01:46.260 --> 00:01:48.260
Und ohne dass ich gefragt wurde

00:01:48.260 --> 00:01:50.260
wurden sie aussortiert.

00:01:50.260 --> 00:01:53.260
Sie verschwanden.

00:01:54.260 --> 00:01:56.260
Facebook ist nicht der einzige Ort

00:01:56.260 --> 00:01:58.260
an dem das Netz auf diese unsichtbare,

00:01:58.260 --> 00:02:01.260
algorithmische Weise redigiert wird.

00:02:01.260 --> 00:02:03.260
Google macht das gleiche.

00:02:03.260 --> 00:02:06.260
Wenn ich nach etwas suche, und Sie nach etwas suchen,

00:02:06.260 --> 00:02:08.260
jetzt gerade, genau gleichzeitig,

00:02:08.260 --> 00:02:11.260
können wir sehr unterschiedliche Ergebnisse bekommen.

00:02:11.260 --> 00:02:14.260
Sogar wenn man ausgeloggt ist, hat mir ein Spezialist gesagt,

00:02:14.260 --> 00:02:16.260
gibt es noch 57 Signale

00:02:16.260 --> 00:02:19.260
die Google berücksichtigt --

00:02:19.260 --> 00:02:22.260
von der Art des Computers

00:02:22.260 --> 00:02:24.260
über den Browser, den man benutzt

00:02:24.260 --> 00:02:26.260
bis zum eigenen Standort --

00:02:26.260 --> 00:02:29.260
die benutzt werden um die Ergebnisse zu personalisieren.

00:02:29.260 --> 00:02:31.260
Denken sie mal drüber nach:

00:02:31.260 --> 00:02:35.260
es gibt kein Standard-Google mehr.

00:02:35.260 --> 00:02:38.260
Und das komische daran ist, dass es kaum sichtbar ist.

00:02:38.260 --> 00:02:40.260
Man sieht nicht, wie unterschiedlich die eigenen Suchtreffer

00:02:40.260 --> 00:02:42.260
von denen der anderen sind.

00:02:42.260 --> 00:02:44.260
Aber vor ein paar Wochen

00:02:44.260 --> 00:02:47.260
habe ich ein paar Freunde gebeten, "Ägypten" zu googlen

00:02:47.260 --> 00:02:50.260
und mir Screenshots der Ergebnisse zu schicken.

00:02:50.260 --> 00:02:53.260
Hier ist der Screenshot meines Freundes Scott.

00:02:54.260 --> 00:02:57.260
Und hier ist der von meinem Freund Daniel.

00:02:57.260 --> 00:02:59.260
Wenn man sie nebeneinander stellt,

00:02:59.260 --> 00:03:01.260
muss man noch nicht mal die Links lesen

00:03:01.260 --> 00:03:03.260
um zu sehen, wie unterschiedlich die beiden Seiten sind.

00:03:03.260 --> 00:03:05.260
Aber wenn man die Links liest,

00:03:05.260 --> 00:03:08.260
ist es schon bemerkenswert.

00:03:09.260 --> 00:03:12.260
Daniel hat auf seiner ersten Trefferseite überhaupt nichts

00:03:12.260 --> 00:03:14.260
über die Proteste in Ägypten bekommen.

00:03:14.260 --> 00:03:16.260
Scott's Ergebnisse waren voll davon.

00:03:16.260 --> 00:03:18.260
Und das war zu dieser Zeit die Nachricht des Tages.

00:03:18.260 --> 00:03:21.260
So unterschiedlich werden die Ergebnisse jetzt.

00:03:21.260 --> 00:03:24.260
Und es geht nicht nur um Google und Facebook.

00:03:24.260 --> 00:03:26.260
Das ist etwas, das das ganze Internet durchzieht.

00:03:26.260 --> 00:03:29.260
Es gibt eine ganze Menge Firmen, die diese Art von Personalisierung machen.

00:03:29.260 --> 00:03:32.260
Yahoo News, die größte Nachrichtenseite im Internet,

00:03:32.260 --> 00:03:35.260
ist schon personalisiert -- verschiedene Menschen bekommen verschiedene Dinge.

00:03:36.260 --> 00:03:39.260
Huffington Post, die Washington Post, die New York Times --

00:03:39.260 --> 00:03:42.260
sie flirten alle irgendwie mit Personalisierung.

00:03:42.260 --> 00:03:45.260
Und das bringt uns sehr schnell

00:03:45.260 --> 00:03:47.260
zu einer Welt in der das Internet

00:03:47.260 --> 00:03:51.260
uns das zeigt, wovon es denkt, dass wir es sehen wollen,

00:03:51.260 --> 00:03:54.260
aber nicht zwangsläufig, was wir sehen sollten.

00:03:54.260 --> 00:03:57.260
Eric Schmidt hat gesagt,

00:03:57.260 --> 00:04:00.260
"Es wird für die Menschen schwierig werden, etwas zu sehen oder zu konsumieren,

00:04:00.260 --> 00:04:02.260
das nicht in irgendeiner Weise

00:04:02.260 --> 00:04:05.260
auf sie zugeschnitten ist."

00:04:05.260 --> 00:04:07.260
Und ich glaube, dass das ein Problem ist.

00:04:07.260 --> 00:04:10.260
Ich glaube, wenn man all diese Filter zusammen nimmt,

00:04:10.260 --> 00:04:12.260
all diese Algorithmen,

00:04:12.260 --> 00:04:15.260
dann bekommt man, was ich eine "Filterblase" nenne.

00:04:16.260 --> 00:04:19.260
Ihre Filterblase ist Ihr ganz persönliches

00:04:19.260 --> 00:04:21.260
einzigartiges Informationsuniversum,

00:04:21.260 --> 00:04:23.260
in dem Sie online leben.

00:04:23.260 --> 00:04:26.260
Und was in Ihrer Filterblase ist,

00:04:26.260 --> 00:04:29.260
hängt davon ab, wer Sie sind und was Sie tun.

00:04:29.260 --> 00:04:33.260
Aber Sie bestimmen nicht, was hineinkommt.

00:04:33.260 --> 00:04:35.260
Und, noch wichtiger,

00:04:35.260 --> 00:04:38.260
Sie sehen nie, was aussortiert wird.

00:04:38.260 --> 00:04:40.260
Eines der Probleme der Filterblase

00:04:40.260 --> 00:04:43.260
wurde von Forschern bei Netflix entdeckt.

00:04:43.260 --> 00:04:46.260
Sie schauten sich die Film-Bestelllisten dort an, und ihnen fiel etwas komisches auf,

00:04:46.260 --> 00:04:48.260
das wohl schon viele von uns kennen,

00:04:48.260 --> 00:04:50.260
nämlich dass manche Filme sofort

00:04:50.260 --> 00:04:53.260
nach oben auf die Liste kommen und ab zu uns nach Hause.

00:04:53.260 --> 00:04:56.260
Sie kommen neu oben auf die Liste, und sofort geht es los.

00:04:56.260 --> 00:04:58.260
"Iron Man" geht sofort raus,

00:04:58.260 --> 00:05:00.260
und bei "Waiting for Superman"

00:05:00.260 --> 00:05:02.260
kann es ziemlich lange dauern.

00:05:02.260 --> 00:05:04.260
Was sie herausgefunden haben

00:05:04.260 --> 00:05:06.260
ist, dass in unseren Netflix-Wartelisten

00:05:06.260 --> 00:05:09.260
ein epischer Kampf stattfindet --

00:05:09.260 --> 00:05:12.260
zwischen unserem zukünftigen, anspruchsvolleren Ich

00:05:12.260 --> 00:05:15.260
und unserem impulsiveren, gegenwärtigen Ich.

00:05:15.260 --> 00:05:17.260
Sie wissen ja, wir möchten alle jemand sein

00:05:17.260 --> 00:05:19.260
der "Rashomon" gesehen hat,

00:05:19.260 --> 00:05:21.260
aber jetzt gerade

00:05:21.260 --> 00:05:24.260
wollen wir "Ace Ventura" zum vierten Mal anschauen.

00:05:24.260 --> 00:05:27.260
(Gelächter)

00:05:27.260 --> 00:05:29.260
Die beste Zusammenstellung für uns wäre von allem etwas.

00:05:29.260 --> 00:05:31.260
Ein kleines bißchen Justin Bieber

00:05:31.260 --> 00:05:33.260
und ein kleines bißchen Afghanistan.

00:05:33.260 --> 00:05:35.260
Etwas Informations-Gemüse,

00:05:35.260 --> 00:05:38.260
und ein wenig Informations-Nachtisch.

00:05:38.260 --> 00:05:40.260
Und die Herausforderung für algorithmische Filter,

00:05:40.260 --> 00:05:42.260
diese personalisierten Filter,

00:05:42.260 --> 00:05:44.260
ist, dass sie diese Balance umkippen,

00:05:44.260 --> 00:05:48.260
weil sie vor allem danach gehen,

00:05:48.260 --> 00:05:52.260
worauf man zuerst klickt.

00:05:52.260 --> 00:05:55.260
Und statt einer ausgewogenen Informationsdiät

00:05:55.260 --> 00:05:57.260
findet man sich am Ende inmitten

00:05:57.260 --> 00:05:59.260
von Informations-Junk Food wieder.

00:05:59.260 --> 00:06:01.260
Was das heißen könnte ist,

00:06:01.260 --> 00:06:04.260
dass wir die Sache mit dem Internet vielleicht falsch verstanden haben.

00:06:04.260 --> 00:06:06.260
In einer Sendergesellschaft --

00:06:06.260 --> 00:06:08.260
so geht der Gründungsmythos --

00:06:08.260 --> 00:06:10.260
in einer Sendergesellschaft

00:06:10.260 --> 00:06:12.260
gab es Torwächter, die Redakteure,

00:06:12.260 --> 00:06:15.260
die den Informationsfluss kontrolliert haben.

00:06:15.260 --> 00:06:18.260
Und dann kam das Internet und hat sie aus dem Weg gefegt,

00:06:18.260 --> 00:06:20.260
und uns allen ermöglicht, uns miteinander zu verbinden,

00:06:20.260 --> 00:06:22.260
und es war wunderbar.

00:06:22.260 --> 00:06:25.260
Aber das ist nicht, was gerade passiert.

00:06:26.260 --> 00:06:29.260
Was wir erleben ist eher eine Übergabe der Fackel

00:06:29.260 --> 00:06:31.260
von menschlichen Torwächtern

00:06:31.260 --> 00:06:34.260
an die Algorithmen.

00:06:34.260 --> 00:06:37.260
Nur dass den Algorithmen

00:06:37.260 --> 00:06:40.260
noch keine Ethik eingebaut wurde,

00:06:40.260 --> 00:06:43.260
wie sie die Redakteure haben.

00:06:43.260 --> 00:06:46.260
Wenn also Algorithmen die Welt für uns kuratieren,

00:06:46.260 --> 00:06:49.260
wenn sie entscheiden, was wir zu sehen bekommen und was nicht,

00:06:49.260 --> 00:06:51.260
dann müssen wir sicherstellen,

00:06:51.260 --> 00:06:54.260
dass sie nicht nur nach Relevanz auswählen.

00:06:54.260 --> 00:06:56.260
Wir müssen garantieren, dass sie uns auch Dinge zeigen,

00:06:56.260 --> 00:06:59.260
die unbequem oder herausfordernd oder wichtig sind --

00:06:59.260 --> 00:07:01.260
so wie das TED tut --

00:07:01.260 --> 00:07:03.260
andere Sichtweisen.

00:07:03.260 --> 00:07:05.260
Und an diesem Punkt waren wir ja schon einmal,

00:07:05.260 --> 00:07:07.260
als Gesellschaft.

00:07:08.260 --> 00:07:11.260
Im Jahre 1915 haben sich die Zeitungen nicht gerade verausgabt

00:07:11.260 --> 00:07:14.260
um ihrer bürgerlichen Verantwortung nachzukommen.

00:07:14.260 --> 00:07:16.260
Dann hat man bemerkt

00:07:16.260 --> 00:07:19.260
dass sie eine sehr wichtige Funktion hatten.

00:07:19.260 --> 00:07:21.260
Dass eine funktionierende Demokratie

00:07:21.260 --> 00:07:23.260
gar nicht möglich war

00:07:23.260 --> 00:07:27.260
ohne einen guten Informationsfluss für die Bürger.

00:07:28.260 --> 00:07:31.260
Dass die Zeitungen mit ihrer Filterwirkung von entscheidender Bedeutung waren --

00:07:31.260 --> 00:07:33.260
und dann entwickelte sich eine journalistische Ethik.

00:07:33.260 --> 00:07:35.260
Die war nicht perfekt,

00:07:35.260 --> 00:07:38.260
aber sie hat uns durch das letzte Jahrhundert gebracht.

00:07:38.260 --> 00:07:40.260
Und jetzt sind wir

00:07:40.260 --> 00:07:43.260
mit dem Internet wieder im Jahr 1915.

00:07:44.260 --> 00:07:47.260
Und die neuen Torwächter müssen

00:07:47.260 --> 00:07:49.260
in den Code, den sie schreiben,

00:07:49.260 --> 00:07:51.260
diese Verantwortung für uns mit einbauen.

00:07:51.260 --> 00:07:54.260
Ich weiß, dass hier viele Leute von Facebook und von Google sind --

00:07:54.260 --> 00:07:56.260
Larry und Sergey --

00:07:56.260 --> 00:07:58.260
Leute, die das Netz in seiner heutigen Form mitgebaut haben,

00:07:58.260 --> 00:08:00.260
und dafür bin ich dankbar.

00:08:00.260 --> 00:08:03.260
Aber wir müssen wirklich sicherstellen,

00:08:03.260 --> 00:08:06.260
dass diese Algorithmen einen Sinn für öffentliches Leben,

00:08:06.260 --> 00:08:09.260
für bürgerliche Verantwortung einprogrammiert haben.

00:08:09.260 --> 00:08:12.260
Wir müssen dafür sorgen, dass sie transparent genug sind,

00:08:12.260 --> 00:08:14.260
so dass wir die Regeln sehen können,

00:08:14.260 --> 00:08:17.260
die bestimmen, was durch unsere Filter kommt.

00:08:17.260 --> 00:08:19.260
Und Ihr müsst uns Kontrolle darüber geben,

00:08:19.260 --> 00:08:21.260
so dass wir entscheiden können

00:08:21.260 --> 00:08:24.260
was durchkommen soll und was nicht.

00:08:24.260 --> 00:08:26.260
Denn ich glaube

00:08:26.260 --> 00:08:28.260
dass wir das Internet wirklich so brauchen

00:08:28.260 --> 00:08:30.260
wie wir es uns erträumt haben.

00:08:30.260 --> 00:08:33.260
Es soll uns alle miteinander verbinden.

00:08:33.260 --> 00:08:36.260
Es soll uns an neue Ideen heranführen

00:08:36.260 --> 00:08:39.260
und an neue Leute und fremde Perspektiven.

00:08:40.260 --> 00:08:42.260
Und das wird nicht geschehen,

00:08:42.260 --> 00:08:45.260
wenn wir alle isoliert werden, in einem vereinzelten Netz.

00:08:45.260 --> 00:08:47.260
Danke.

00:08:47.260 --> 00:08:58.260
(Applaus)

