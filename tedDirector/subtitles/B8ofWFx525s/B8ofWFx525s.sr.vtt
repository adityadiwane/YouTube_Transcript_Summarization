WEBVTT
Kind: captions
Language: sr

00:00:00.000 --> 00:00:07.000
Prevodilac: Sanja Drakulovic
Lektor: Sandra Gojic

00:00:15.260 --> 00:00:17.260
Mark Cukerberg,

00:00:17.260 --> 00:00:20.260
novinar mu je postavio pitanje u vezi news feed-a (na facebooku).

00:00:20.260 --> 00:00:22.260
I novinar ga je pitao,

00:00:22.260 --> 00:00:24.260
"zašto je ovo tako bitno?"

00:00:24.260 --> 00:00:26.260
A Cukerberg je odgovorio,

00:00:26.260 --> 00:00:28.260
"Veverica koja umire na vašem travnjaku

00:00:28.260 --> 00:00:31.260
može biti relevantnija za vas u ovom trenutku

00:00:31.260 --> 00:00:34.260
nego ljudi koji umiru u Africi."

00:00:34.260 --> 00:00:36.260
Želeo bih da govorim o tome

00:00:36.260 --> 00:00:39.260
kako bi izgledao internet zasnovan na toj ideji o relevantnosti.

00:00:40.260 --> 00:00:42.260
Tokom mog odrastanja

00:00:42.260 --> 00:00:44.260
u stvarno ruralnoj sredini u Mejnu,

00:00:44.260 --> 00:00:47.260
Internet je meni značio nešto sasvim drugačije.

00:00:47.260 --> 00:00:49.260
Značio je vezu sa svetom.

00:00:49.260 --> 00:00:52.260
Predstavljao je nešto što bi nas sve povezalo.

00:00:52.260 --> 00:00:55.260
I bio sam siguran da će to biti sjajno za demokratiju

00:00:55.260 --> 00:00:58.260
i za društvo.

00:00:58.260 --> 00:01:00.260
Ali tu postoji taj preokret

00:01:00.260 --> 00:01:02.260
u načinu na koji informacija putuje online,

00:01:02.260 --> 00:01:05.260
koji je nevidljiv.

00:01:05.260 --> 00:01:07.260
I ako ne obratimo pažnju na njega,

00:01:07.260 --> 00:01:10.260
može postati realan problem.

00:01:10.260 --> 00:01:13.260
Prvi put sam ovo primetio na mestu na kom provodim dosta vremena --

00:01:13.260 --> 00:01:15.260
moja Facebook stranica.

00:01:15.260 --> 00:01:18.260
Moja politička shvatanja su progresivna -- kakvo iznenađenje --

00:01:18.260 --> 00:01:20.260
ali sam se uvek trudio da upoznam konzervativce.

00:01:20.260 --> 00:01:22.260
Volim da čujem o čemu razmišljaju;

00:01:22.260 --> 00:01:24.260
Volim da vidim koje linkove postavljaju;

00:01:24.260 --> 00:01:26.260
Volim da naučim par stvari.

00:01:26.260 --> 00:01:29.260
Tako da sam bio iznenađen kad sam jednog dana primetio

00:01:29.260 --> 00:01:32.260
da su konzervativci nestali iz mog Facebook feeda.

00:01:33.260 --> 00:01:35.260
A ono što se ispostavilo je

00:01:35.260 --> 00:01:39.260
da je Facebook pratio koje linkove sam kliknuo,

00:01:39.260 --> 00:01:41.260
i primećivao je da sam, u stvari,

00:01:41.260 --> 00:01:43.260
više kliktao na linkove mojih liberalnih prijatelja

00:01:43.260 --> 00:01:46.260
nego na linkove mojih konzervativnih prijatelja.

00:01:46.260 --> 00:01:48.260
I bez konsultacije sa mnom,

00:01:48.260 --> 00:01:50.260
izbacio ih je.

00:01:50.260 --> 00:01:53.260
Oni su nestali.

00:01:54.260 --> 00:01:56.260
Ali Facebook nije jedino mesto

00:01:56.260 --> 00:01:58.260
koje radi ovo nevidljivo, algoritamsko

00:01:58.260 --> 00:02:01.260
menjanje Weba.

00:02:01.260 --> 00:02:03.260
I Google to radi.

00:02:03.260 --> 00:02:06.260
Kad ja tražim neki pojam, i vi tražite neki pojam,

00:02:06.260 --> 00:02:08.260
čak i sada, u isto vreme,

00:02:08.260 --> 00:02:11.260
možemo dobiti različite rezultate pretrage.

00:02:11.260 --> 00:02:14.260
Čak i ako niste ulogovani, jedan inženjer mi je rekao,

00:02:14.260 --> 00:02:16.260
postoji 57 signala

00:02:16.260 --> 00:02:19.260
koje Google prati --

00:02:19.260 --> 00:02:22.260
sve od toga na kakvom ste kompjuteru

00:02:22.260 --> 00:02:24.260
do toga koji pretraživač koristite

00:02:24.260 --> 00:02:26.260
i toga gde se nalazite --

00:02:26.260 --> 00:02:29.260
koje koristi da bi vam lično skrojio rezultate pretrage.

00:02:29.260 --> 00:02:31.260
Razmislite o tome na momenat:

00:02:31.260 --> 00:02:35.260
više ne postoji standardni Google.

00:02:35.260 --> 00:02:38.260
Znate, zanimljivo u vezi toga je da je to vrlo teško primetiti.

00:02:38.260 --> 00:02:40.260
Vi ne možete videti koliko se vaši rezultati pretrage

00:02:40.260 --> 00:02:42.260
razlikuju od bilo čijih drugih.

00:02:42.260 --> 00:02:44.260
Ali pre par nedelja

00:02:44.260 --> 00:02:47.260
pitao sam grupu prijatelja da guglaju "Egipat"

00:02:47.260 --> 00:02:50.260
i da mi pošalju snimak ekrana šta su dobili.

00:02:50.260 --> 00:02:53.260
Evo snimka ekrana mog prijatelja Skota.

00:02:54.260 --> 00:02:57.260
A evo snimka ekrana mog prijatelja Danijela.

00:02:57.260 --> 00:02:59.260
Kad ih stavite jedan pored drugog,

00:02:59.260 --> 00:03:01.260
ne morate čak ni da čitate linkove

00:03:01.260 --> 00:03:03.260
da biste videli koliko se razlikuju.

00:03:03.260 --> 00:03:05.260
Ali kad pročitate linkove,

00:03:05.260 --> 00:03:08.260
stvarno je vrlo upadljivo.

00:03:09.260 --> 00:03:12.260
Danijel nije dobio ništa u vezi protesta u Egiptu

00:03:12.260 --> 00:03:14.260
na svojoj prvoj stranici Google rezultata.

00:03:14.260 --> 00:03:16.260
Skotovi rezultati su bili puni protesta.

00:03:16.260 --> 00:03:18.260
A to je bila najvažnija vest dana u tom trenutku.

00:03:18.260 --> 00:03:21.260
Toliko ti rezultati postaju različiti.

00:03:21.260 --> 00:03:24.260
To nisu čak ni samo Google i Facebook.

00:03:24.260 --> 00:03:26.260
To je nešto što se širi mrežom.

00:03:26.260 --> 00:03:29.260
Postoji mnoštvo kompanija koje rade ovakvu vrstu personalizacije.

00:03:29.260 --> 00:03:32.260
Yahoo News, najveći news sajt na Internetu,

00:03:32.260 --> 00:03:35.260
je sada personalizovan -- različiti ljudi dobijaju različite stvari.

00:03:36.260 --> 00:03:39.260
Huffington Post, Vašington Post, New York Times --

00:03:39.260 --> 00:03:42.260
svi flertuju sa personalizacijom na različite načine.

00:03:42.260 --> 00:03:45.260
I to nas vrlo brzo vodi

00:03:45.260 --> 00:03:47.260
ka svetu u kome

00:03:47.260 --> 00:03:51.260
nam Internet prikazuje stvari koje misli da želimo da vidimo,

00:03:51.260 --> 00:03:54.260
ali ne nužno i stvari koje bi trebalo da vidimo.

00:03:54.260 --> 00:03:57.260
Kao što je Erik Šmit rekao,

00:03:57.260 --> 00:04:00.260
"Biće vrlo teško ljudima da gledaju ili konzumiraju nešto

00:04:00.260 --> 00:04:02.260
što nije na neki način

00:04:02.260 --> 00:04:05.260
skrojeno baš za njih."

00:04:05.260 --> 00:04:07.260
Tako da stvarno smatram da je to problem.

00:04:07.260 --> 00:04:10.260
I mislim da, ako skupite sve te filtere,

00:04:10.260 --> 00:04:12.260
uzmete sve te algoritme,

00:04:12.260 --> 00:04:15.260
dobijete ono što zovem filter mehurić.

00:04:16.260 --> 00:04:19.260
Vaš filter mehurić je vaš lični

00:04:19.260 --> 00:04:21.260
jedinstveni univerzum informacija

00:04:21.260 --> 00:04:23.260
u kome živite online.

00:04:23.260 --> 00:04:26.260
A šta se nalazi u vašem filter mehiruću

00:04:26.260 --> 00:04:29.260
zavisi od toga ko ste, i zavisi od toga čime se bavite.

00:04:29.260 --> 00:04:33.260
Ali stvar je u tome da vi ne odlučujete šta ulazi unutra.

00:04:33.260 --> 00:04:35.260
I još važnije,

00:04:35.260 --> 00:04:38.260
ne možete da vidite šta vam je uklonjeno.

00:04:38.260 --> 00:04:40.260
Tako da je jedan od problema sa filter mehurićem

00:04:40.260 --> 00:04:43.260
pronađen od strane nekih istraživača iz Netflixa.

00:04:43.260 --> 00:04:46.260
Posmatrali su Netflix redoslede, i primetili nešto zanimljivo

00:04:46.260 --> 00:04:48.260
što su mnogi od nas verovatno primetili,

00:04:48.260 --> 00:04:50.260
a to je da postoje neki filmovi

00:04:50.260 --> 00:04:53.260
koji na neki način iskoče pravo u naše domove.

00:04:53.260 --> 00:04:56.260
Uđu u redosled, i odmah iskoče.

00:04:56.260 --> 00:04:58.260
Tako "Iron Man" odmah iskoči,

00:04:58.260 --> 00:05:00.260
a "Waiting for Superman"

00:05:00.260 --> 00:05:02.260
može da čeka prilično dugo vremena.

00:05:02.260 --> 00:05:04.260
Ono što su otkrili

00:05:04.260 --> 00:05:06.260
je da se u Netflix redosledu

00:05:06.260 --> 00:05:09.260
dešava epska bitka

00:05:09.260 --> 00:05:12.260
između našeg budućeg sebe kojem težimo

00:05:12.260 --> 00:05:15.260
i našeg sadašnjeg, impulsivnijeg sebe.

00:05:15.260 --> 00:05:17.260
Znate, svi bismo želeli da budemo neko

00:05:17.260 --> 00:05:19.260
ko je gledao "Rašomona",

00:05:19.260 --> 00:05:21.260
ali trenutno

00:05:21.260 --> 00:05:24.260
želimo da gledamo "Ejs Venturu" po četvrti put.

00:05:24.260 --> 00:05:27.260
(Smeh)

00:05:27.260 --> 00:05:29.260
Tako da nam najbolje uređivanje daje pomalo od oboje.

00:05:29.260 --> 00:05:31.260
Daje nam pomalo Džastin Bibera

00:05:31.260 --> 00:05:33.260
i pomalo Avganistana.

00:05:33.260 --> 00:05:35.260
Daje nam neke biljke informacija,

00:05:35.260 --> 00:05:38.260
i daje nam neke dezerte informacija.

00:05:38.260 --> 00:05:40.260
Nedostatak ove vrste algoritamskih filtera,

00:05:40.260 --> 00:05:42.260
ovih personalizovanih filtera,

00:05:42.260 --> 00:05:44.260
je u tome što, pošto pretežno prate

00:05:44.260 --> 00:05:48.260
na šta prvo klikćete,

00:05:48.260 --> 00:05:52.260
mogu da poremete tu ravnotežu.

00:05:52.260 --> 00:05:55.260
Umesto balansirane informacijske dijete,

00:05:55.260 --> 00:05:57.260
možete završiti okruženi

00:05:57.260 --> 00:05:59.260
informacijskom lošom hranom.

00:05:59.260 --> 00:06:01.260
Ovo u stvari ukazuje na to da

00:06:01.260 --> 00:06:04.260
smo možda promašili celu priču sa Internetom.

00:06:04.260 --> 00:06:06.260
U društvu širokopojasnog emitovanja --

00:06:06.260 --> 00:06:08.260
ovako glasi osnivačka mitologija --

00:06:08.260 --> 00:06:10.260
u društvu širokopojasnog emitovanja,

00:06:10.260 --> 00:06:12.260
postojali su ti vratari, urednici,

00:06:12.260 --> 00:06:15.260
koji su kontrolisali protok informacija.

00:06:15.260 --> 00:06:18.260
I onda se pojavio Internet koji ih je oduvao sa puta,

00:06:18.260 --> 00:06:20.260
i omogućio svima nama da se međusobno povežemo,

00:06:20.260 --> 00:06:22.260
što je bilo fenomenalno.

00:06:22.260 --> 00:06:25.260
Ali to u stvari nije ono što se upravo dešava.

00:06:26.260 --> 00:06:29.260
Ono što sada posmatramo je više prelazak štafete

00:06:29.260 --> 00:06:31.260
od ljudskih vratara

00:06:31.260 --> 00:06:34.260
ka algoritamskim.

00:06:34.260 --> 00:06:37.260
A stvar je u tome da algoritmi

00:06:37.260 --> 00:06:40.260
još uvek nemaju ugrađenu etiku

00:06:40.260 --> 00:06:43.260
koju su imali urednici.

00:06:43.260 --> 00:06:46.260
Ako će algoritmi da nam budu tutori o svetu,

00:06:46.260 --> 00:06:49.260
ako će da odlučuju šta možemo a šta ne možemo da vidimo,

00:06:49.260 --> 00:06:51.260
onda moramo da se pobrinemo

00:06:51.260 --> 00:06:54.260
da nisu naštimovani samo na relevantnost.

00:06:54.260 --> 00:06:56.260
Moramo da obezbedimo da nam prikazuju i stvari

00:06:56.260 --> 00:06:59.260
koje su neprijatne ili teške ili bitne --

00:06:59.260 --> 00:07:01.260
to je ono što TED radi --

00:07:01.260 --> 00:07:03.260
druge tačke gledišta.

00:07:03.260 --> 00:07:05.260
Stvar je u tome da smo već bili na ovom mestu

00:07:05.260 --> 00:07:07.260
kao društvo.

00:07:08.260 --> 00:07:11.260
1915. godine, novine se nisu mnogo brinule

00:07:11.260 --> 00:07:14.260
o svojoj odgovornosti prema građanima.

00:07:14.260 --> 00:07:16.260
Onda su ljudi primetili

00:07:16.260 --> 00:07:19.260
da one rade nešto vrlo značajno.

00:07:19.260 --> 00:07:21.260
Da, u stvari, ne možete imati

00:07:21.260 --> 00:07:23.260
funkcionalnu demokratiju

00:07:23.260 --> 00:07:27.260
ako građani nemaju dobar priliv informacija.

00:07:28.260 --> 00:07:31.260
Da su novine kritične, jer su funkcionisale kao filter,

00:07:31.260 --> 00:07:33.260
i onda se razvila novinarska etika.

00:07:33.260 --> 00:07:35.260
Nije bila savršena,

00:07:35.260 --> 00:07:38.260
ali nam je poslužila kroz prošli vek.

00:07:38.260 --> 00:07:40.260
I tako smo danas,

00:07:40.260 --> 00:07:43.260
u neku ruku ponovo u 1915. na mreži.

00:07:44.260 --> 00:07:47.260
I potrebni su nam novi vratari

00:07:47.260 --> 00:07:49.260
da utkaju tu vrstu odgovornosti

00:07:49.260 --> 00:07:51.260
u kod koji ispisuju.

00:07:51.260 --> 00:07:54.260
Znam da ima dosta ljudi ovde iz Facebooka ili Googla --

00:07:54.260 --> 00:07:56.260
Lari i Sergej --

00:07:56.260 --> 00:07:58.260
ljudi koji su pomogli u izgradnji Weba kakav je danas,

00:07:58.260 --> 00:08:00.260
i ja sam im zahvalan zbog toga.

00:08:00.260 --> 00:08:03.260
Ali stvarno želimo da obezbedite

00:08:03.260 --> 00:08:06.260
da ti algoritmi imaju ugrađen u sebi

00:08:06.260 --> 00:08:09.260
osećaj javnog života, osećaj građanske odgovornosti.

00:08:09.260 --> 00:08:12.260
Želimo da obezbedite da su dovoljno transparentni

00:08:12.260 --> 00:08:14.260
da možemo da vidimo koja su pravila

00:08:14.260 --> 00:08:17.260
koja određuju šta će proći kroz naše filtere.

00:08:17.260 --> 00:08:19.260
I želimo da nam date neku kontrolu,

00:08:19.260 --> 00:08:21.260
da možemo da odlučimo

00:08:21.260 --> 00:08:24.260
šta će proći a šta neće.

00:08:24.260 --> 00:08:26.260
Jer smatram da

00:08:26.260 --> 00:08:28.260
nam je stvarno potrebno da Internet bude to

00:08:28.260 --> 00:08:30.260
što smo svi sanjali da će biti.

00:08:30.260 --> 00:08:33.260
Potreban nam je da nas sve poveže.

00:08:33.260 --> 00:08:36.260
Potreban nam je da nas upozna sa novim idejama

00:08:36.260 --> 00:08:39.260
i novim ljudima i različitim perspektivama.

00:08:40.260 --> 00:08:42.260
A to neće uraditi

00:08:42.260 --> 00:08:45.260
ako nas sve ostavi izolovane u pojedinačnim mrežama.

00:08:45.260 --> 00:08:47.260
Hvala Vam.

00:08:47.260 --> 00:08:58.260
(Aplauz)

