WEBVTT
Kind: captions
Language: sk

00:00:00.000 --> 00:00:07.000
Translator: Martin Ukrop
Reviewer: Martin Francis Gilbert Máik

00:00:15.260 --> 00:00:17.260
Marka Zuckerberga

00:00:17.260 --> 00:00:20.260
sa raz novinár spýtal na panel noviniek.

00:00:20.260 --> 00:00:22.260
Tento novinár sa spýtal:

00:00:22.260 --> 00:00:24.260
„Prečo je to také dôležité?"

00:00:24.260 --> 00:00:26.260
A Zuckrerberg mu odvetil:

00:00:26.260 --> 00:00:28.260
„Veverička, ktorá práve zdochla pred vaším domom

00:00:28.260 --> 00:00:31.260
môže byť pre vás práve teraz dôležitejšia,

00:00:31.260 --> 00:00:34.260
ako ľudia umierajúci v Afrike."

00:00:34.260 --> 00:00:36.260
A ja by som chcel hovoriť o tom,

00:00:36.260 --> 00:00:39.260
ako by vyzeral web založený na takýchto princípoch relevantnosti.

00:00:40.260 --> 00:00:42.260
Keď som vyrastal

00:00:42.260 --> 00:00:44.260
na ozajstnom vidieku v štáte Maine,

00:00:44.260 --> 00:00:47.260
internet bol pre mňa niečím špeciálnym.

00:00:47.260 --> 00:00:49.260
Predstavoval spojenie so svetom.

00:00:49.260 --> 00:00:52.260
Predstavoval niečo, čo nás všetkých spojí dohromady.

00:00:52.260 --> 00:00:55.260
A bol som si istý, že to bude skvelá vec pre demokraciu

00:00:55.260 --> 00:00:58.260
a celkovo pre našu spoločnosť.

00:00:58.260 --> 00:01:00.260
Ale existuje istý posun v tom,

00:01:00.260 --> 00:01:02.260
ako informácie plynú po internete --

00:01:02.260 --> 00:01:05.260
je však neviditeľný.

00:01:05.260 --> 00:01:07.260
A, ak tomu nebudeme venovať pozornosť,

00:01:07.260 --> 00:01:10.260
mohol by sa z toho stať vážny problém.

00:01:10.260 --> 00:01:13.260
Prvýkrát som si to všimol na mieste, kde trávim mnoho času --

00:01:13.260 --> 00:01:15.260
na Facebooku.

00:01:15.260 --> 00:01:18.260
Snažím sa byť pokrokový, aj politicky -- veľké prekvapenie --

00:01:18.260 --> 00:01:20.260
ale vždy som sa stýkal aj s konzervatívcami.

00:01:20.260 --> 00:01:22.260
Rád počúvam, o čom rozmýšľajú;

00:01:22.260 --> 00:01:24.260
rád sa dívam, aké odkazy rozširujú;

00:01:24.260 --> 00:01:26.260
rád sa priučím vec či dve.

00:01:26.260 --> 00:01:29.260
A tak som zostal veľmi prekvapený, keď som si raz uvedomil,

00:01:29.260 --> 00:01:32.260
že všetci konzervatívci z mojich Facebookových noviniek zmizli.

00:01:33.260 --> 00:01:35.260
Nakoniec sa ukázalo,

00:01:35.260 --> 00:01:39.260
že Facebook sledoval, na ktoré odkazy klikám,

00:01:39.260 --> 00:01:41.260
a všímal si, že vlastne

00:01:41.260 --> 00:01:43.260
viac klikám na odkazy svojich liberálnych priateľov

00:01:43.260 --> 00:01:46.260
ako na tie, ktoré boli od konzervatívcov.

00:01:46.260 --> 00:01:48.260
A bez toho, aby sa ma spýtal,

00:01:48.260 --> 00:01:50.260
ich jednoducho vynechal.

00:01:50.260 --> 00:01:53.260
Jednoducho zmizli.

00:01:54.260 --> 00:01:56.260
A Facebook nie je jediné miesto,

00:01:56.260 --> 00:01:58.260
kde sa odohráva takáto neviditeľná, algoritmická

00:01:58.260 --> 00:02:01.260
korektúra webového obsahu.

00:02:01.260 --> 00:02:03.260
Google to robí tiež.

00:02:03.260 --> 00:02:06.260
Ak by som ja niečo hľadal a hľadali by ste to aj vy,

00:02:06.260 --> 00:02:08.260
hoci aj tu a teraz v rovnakom momente,

00:02:08.260 --> 00:02:11.260
je možné, že dostaneme výrazne rôzne výsledky.

00:02:11.260 --> 00:02:14.260
Jeden technik mi povedal, že aj keď ste odhlásení,

00:02:14.260 --> 00:02:16.260
je ďalších 57 faktorov,

00:02:16.260 --> 00:02:19.260
ktoré Google berie do úvahy --

00:02:19.260 --> 00:02:22.260
úplne všetko, od toho za akým počítačom sedíte,

00:02:22.260 --> 00:02:24.260
cez typ browsera, ktorý používate,

00:02:24.260 --> 00:02:26.260
až po miesto, kde sa nachádzate --

00:02:26.260 --> 00:02:29.260
tak to všetko sa používa, aby boli výsledky ušité na mieru pre vás.

00:02:29.260 --> 00:02:31.260
Zamyslite sa nad tým na chvíľu:

00:02:31.260 --> 00:02:35.260
už neexistuje žiadny „štandardný Google."

00:02:35.260 --> 00:02:38.260
A najnezvyčajnejšie na tom je, že to skoro nie je vidieť.

00:02:38.260 --> 00:02:40.260
Nevidíte, ako veľmi sa to, čo dostanete vy,

00:02:40.260 --> 00:02:42.260
líši od toho, čo dostanú ostatní.

00:02:42.260 --> 00:02:44.260
Pred pár týždňami som poprosil

00:02:44.260 --> 00:02:47.260
zopár svojich známych, aby zadali do Googlu výraz „Egypt"

00:02:47.260 --> 00:02:50.260
a poslali mi screenshot toho, čo dostanú.

00:02:50.260 --> 00:02:53.260
Takto to vyzeralo u môjho priateľa Scotta.

00:02:54.260 --> 00:02:57.260
A takto zase u Daniela.

00:02:57.260 --> 00:02:59.260
Keď si ich dáte vedľa seba pre porovnanie,

00:02:59.260 --> 00:03:01.260
nemusíte ani len čítať tie odkazy,

00:03:01.260 --> 00:03:03.260
aby ste si uvedomili, ako veľmi rôzne tie stránky sú.

00:03:03.260 --> 00:03:05.260
Ale keď si prečítate, aké odkazy to sú,

00:03:05.260 --> 00:03:08.260
všimnete si niečo pozoruhodné.

00:03:09.260 --> 00:03:12.260
U Daniela sa vôbec nič ohľadom terajších protestov v Egypte

00:03:12.260 --> 00:03:14.260
nedostalo na prvú stranu.

00:03:14.260 --> 00:03:16.260
U Scotta toho bolo veľa.

00:03:16.260 --> 00:03:18.260
A to boli v tom čase najdôležitejšie novinky.

00:03:18.260 --> 00:03:21.260
A presne toto sa s výsledkami vyhľadávania deje.

00:03:21.260 --> 00:03:24.260
Ale nie je to iba Google a Facebook.

00:03:24.260 --> 00:03:26.260
Podobné veci sa tiahnu celým webom.

00:03:26.260 --> 00:03:29.260
Je obrovské množstvo firiem, ktoré robia tento druh personalizácie.

00:03:29.260 --> 00:03:32.260
Yahoo News, najväčší spravodajský web na internete

00:03:32.260 --> 00:03:35.260
je už tiež personalizovaný -- rozdielni ľudia vidia rozdielne správy.

00:03:36.260 --> 00:03:39.260
Huffington Post, the Washington Post, the New York Times --

00:03:39.260 --> 00:03:42.260
všetci sa nejakým spôsobom pohrávajú s personalizáciou.

00:03:42.260 --> 00:03:45.260
A tým sa veľmi rýchlo

00:03:45.260 --> 00:03:47.260
dostávame do doby,

00:03:47.260 --> 00:03:51.260
kedy nám internet bude ukazovať to, čo si myslí, že chceme vidieť,

00:03:51.260 --> 00:03:54.260
ale nie nutne to, čo potrebujeme vidieť.

00:03:54.260 --> 00:03:57.260
Ako povedal Eric Schmidt:

00:03:57.260 --> 00:04:00.260
„Bude pre ľudí veľmi ťažké sledovať alebo konzumovať čokoľvek,

00:04:00.260 --> 00:04:02.260
čo nebolo v nejakom zmysle

00:04:02.260 --> 00:04:05.260
prispôsobené práve im."

00:04:05.260 --> 00:04:07.260
Ja si naozaj myslím, že je to problém.

00:04:07.260 --> 00:04:10.260
A myslím si, že keď zoberiete všetky tieto filtre,

00:04:10.260 --> 00:04:12.260
všetky tieto filtrovacie algoritmy,

00:04:12.260 --> 00:04:15.260
získate niečo, čo nazývam „informačná bublina."

00:04:16.260 --> 00:04:19.260
Táto informačná bublina predstavuje váš osobný,

00:04:19.260 --> 00:04:21.260
unikátny vesmír informácií,

00:04:21.260 --> 00:04:23.260
v ktorom žijete, keď ste online.

00:04:23.260 --> 00:04:26.260
Obsah vašej informačnej bubliny

00:04:26.260 --> 00:04:29.260
závisí od toho, kto ste a ako sa správate.

00:04:29.260 --> 00:04:33.260
Nemôžete si však vybrať, čo sa dostane dnu.

00:04:33.260 --> 00:04:35.260
Ale hlavne,

00:04:35.260 --> 00:04:38.260
nevidíte to, čo bolo vynechané.

00:04:38.260 --> 00:04:40.260
Jeden z problémov informačných bublín

00:04:40.260 --> 00:04:43.260
objavili výskumníci z firmy Netflix.

00:04:43.260 --> 00:04:46.260
Pozorovali sled výpožičiek filmov zákazníkov Netflixu a všimli si niečo zvláštne,

00:04:46.260 --> 00:04:48.260
niečo, čo si pravdepodobne všimlo mnoho z nás --

00:04:48.260 --> 00:04:50.260
sú filmy, ktoré sa akosi

00:04:50.260 --> 00:04:53.260
hneď dostanú medzi ľudí, do ich domovov.

00:04:53.260 --> 00:04:56.260
Hneď ako sa dostanú do zoznamu filmov na pozretie si ich aj pozrieme.

00:04:56.260 --> 00:04:58.260
Tak rýchlo prejde napríklad „Iron Man,"

00:04:58.260 --> 00:05:00.260
ale na dokumentárny film „Waiting for Superman"

00:05:00.260 --> 00:05:02.260
si asi počkáme celkom dlho.

00:05:02.260 --> 00:05:04.260
A oni zistili, že v tých filmových zoznamoch

00:05:04.260 --> 00:05:06.260
zákazníkov firmy Netflix

00:05:06.260 --> 00:05:09.260
prebieha epický súboj

00:05:09.260 --> 00:05:12.260
medzi našimi „ja", ktoré sa usilujú o budúcnosť,

00:05:12.260 --> 00:05:15.260
a našimi prítomnými a oveľa impulzívnejšími „ja".

00:05:15.260 --> 00:05:17.260
Určite to poznáte -- všetci chceme byť tí,

00:05:17.260 --> 00:05:19.260
ktorí už videli film „Rašomón,"

00:05:19.260 --> 00:05:21.260
ale práve teraz by sme si

00:05:21.260 --> 00:05:24.260
radšej pozreli „Ace Ventura: zvierací detektív" po štvrtýkrát.

00:05:24.260 --> 00:05:27.260
(Smiech)

00:05:27.260 --> 00:05:29.260
A najlepšia zostava je taká, ktorá nám dá niečo od každého.

00:05:29.260 --> 00:05:31.260
Dá nám trochu Justina Biebera

00:05:31.260 --> 00:05:33.260
a súčasne niečo z Afganistanu.

00:05:33.260 --> 00:05:35.260
Dá nám trochu informačnej zeleniny,

00:05:35.260 --> 00:05:38.260
ale aj informačný zákusok.

00:05:38.260 --> 00:05:40.260
A problém s týmto druhom algoritmických filtrov,

00:05:40.260 --> 00:05:42.260
týchto personalizačných filtrov,

00:05:42.260 --> 00:05:44.260
je ten, že keďže zohľadňujú predovšetkým to,

00:05:44.260 --> 00:05:48.260
na čo sme klikli prvé,

00:05:48.260 --> 00:05:52.260
môžu veľmi rýchlo stratiť rovnováhu.

00:05:52.260 --> 00:05:55.260
A namiesto vyváženej informačnej stravy

00:05:55.260 --> 00:05:57.260
sa k vám dostane

00:05:57.260 --> 00:05:59.260
len informačný fast food.

00:05:59.260 --> 00:06:01.260
Snaží sa to naznačiť,

00:06:01.260 --> 00:06:04.260
že možno sme to rozprávanie o internete pochopili zle.

00:06:04.260 --> 00:06:06.260
V spoločnosti s rozhlasom a televíziou --

00:06:06.260 --> 00:06:08.260
presne tak začína báj o vzniku --

00:06:08.260 --> 00:06:10.260
tak v tej spoločnosti

00:06:10.260 --> 00:06:12.260
existovali vrátnici, editori, korektori,

00:06:12.260 --> 00:06:15.260
ktorí kontrolovali tok informácií.

00:06:15.260 --> 00:06:18.260
Potom však prišiel internet, ktorý ich zlikvidoval

00:06:18.260 --> 00:06:20.260
a dovolil nám všetkým nadviazať kontakt so svetom

00:06:20.260 --> 00:06:22.260
a bolo to úžasné.

00:06:22.260 --> 00:06:25.260
Ale to nie je úplne presne to, čo sa deje teraz.

00:06:26.260 --> 00:06:29.260
Sme skôr svedkami momentu, keď sa štafeta predáva

00:06:29.260 --> 00:06:31.260
od ľudských strážcov

00:06:31.260 --> 00:06:34.260
tým algoritmickým.

00:06:34.260 --> 00:06:37.260
A problémom je, že tie algoritmy

00:06:37.260 --> 00:06:40.260
ešte nemajú zabudovanú akýsi etický rozmer,

00:06:40.260 --> 00:06:43.260
ktorý tí ľudia mali.

00:06:43.260 --> 00:06:46.260
A ak budú za nás spravovať svet algoritmy,

00:06:46.260 --> 00:06:49.260
ak budú oni rozhodovať, čo budeme vidieť a čo nie,

00:06:49.260 --> 00:06:51.260
potom sa musíme ubezpečiť,

00:06:51.260 --> 00:06:54.260
že nebudú hľadieť len na významnosť daných udalostí.

00:06:54.260 --> 00:06:56.260
Musíme sa uistiť, že nám ukážu aj veci,

00:06:56.260 --> 00:06:59.260
ktoré sú nepríjemné, provokatívne alebo inak závažné --

00:06:59.260 --> 00:07:01.260
a to je to, o čo sa snaží TED --

00:07:01.260 --> 00:07:03.260
iný pohľad na vec.

00:07:03.260 --> 00:07:05.260
V skutočnosti sme my, ako spoločnosť,

00:07:05.260 --> 00:07:07.260
v takejto situácií už boli.

00:07:08.260 --> 00:07:11.260
Rok 1915 -- nedá sa povedať, že by sa noviny

00:07:11.260 --> 00:07:14.260
išli pretrhnúť kvôli svojim občianskym povinnostiam.

00:07:14.260 --> 00:07:16.260
Potom si ľudia uvedomili,

00:07:16.260 --> 00:07:19.260
že noviny zastávali naozaj dôležitú funkciu.

00:07:19.260 --> 00:07:21.260
Uvedomili si, že nie je možné

00:07:21.260 --> 00:07:23.260
mať fungujúcu demokraciu

00:07:23.260 --> 00:07:27.260
bez toho, aby mali obyvatelia dobrý prísun informácií.

00:07:28.260 --> 00:07:31.260
Uvedomili si, že noviny boli kritické, pretože fungovali ako filter,

00:07:31.260 --> 00:07:33.260
a preto sa rozvinula novinárska etika.

00:07:33.260 --> 00:07:35.260
Nebolo to dokonalé,

00:07:35.260 --> 00:07:38.260
ale pomohlo nám to dostať sa cez posledné storočie.

00:07:38.260 --> 00:07:40.260
A tak aj teraz

00:07:40.260 --> 00:07:43.260
sme na webe ako keby znova v roku 1915.

00:07:44.260 --> 00:07:47.260
A potrebujeme nových vrátnikov a strážcov,

00:07:47.260 --> 00:07:49.260
aby zabudovali tento typ zodpovednosti

00:07:49.260 --> 00:07:51.260
do algoritmov, ktoré píšu.

00:07:51.260 --> 00:07:54.260
Viem, že je tu veľa ľudí z Facebooku a Googlu --

00:07:54.260 --> 00:07:56.260
Larry a Sergey --

00:07:56.260 --> 00:07:58.260
sú to ľudia, ktorí pomohli vystavať web tak, ako ho poznáme

00:07:58.260 --> 00:08:00.260
a som im za to vďačný.

00:08:00.260 --> 00:08:03.260
Ale naozaj sa musíme ubezpečiť,

00:08:03.260 --> 00:08:06.260
že tieto algoritmy budú mať v sebe zabudovaný

00:08:06.260 --> 00:08:09.260
zmysel pre verejný život, akúsi občiansku zodpovednosť.

00:08:09.260 --> 00:08:12.260
Musíme sa uistiť, že budú dostatočne transparentné,

00:08:12.260 --> 00:08:14.260
aby bolo možné vidieť, aké pravidlá

00:08:14.260 --> 00:08:17.260
rozhodujú, čo sa cez tie filtre dostane.

00:08:17.260 --> 00:08:19.260
A musíte dostať možnosť

00:08:19.260 --> 00:08:21.260
rozhodnúť sa,

00:08:21.260 --> 00:08:24.260
čo sa dnu dostane a čo nie.

00:08:24.260 --> 00:08:26.260
Pretože si naozaj myslím,

00:08:26.260 --> 00:08:28.260
že potrebujeme, aby bol internet tým,

00:08:28.260 --> 00:08:30.260
čím sme si vysnívali, že bude.

00:08:30.260 --> 00:08:33.260
Potrebujeme ho, aby sme boli prepojený so všetkými ostatnými.

00:08:33.260 --> 00:08:36.260
Potrebujeme ho na to, aby nám ukázal nové myšlienky,

00:08:36.260 --> 00:08:39.260
nových ľudí a iné pohľady na vec.

00:08:40.260 --> 00:08:42.260
A to nebude možné,

00:08:42.260 --> 00:08:45.260
ak zostaneme izolovaní v našich individuálnych svetoch.

00:08:45.260 --> 00:08:47.260
Ďakujem.

00:08:47.260 --> 00:08:58.260
(Potlesk)

