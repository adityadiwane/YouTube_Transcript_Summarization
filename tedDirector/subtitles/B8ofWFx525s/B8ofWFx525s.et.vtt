WEBVTT
Kind: captions
Language: et

00:00:00.000 --> 00:00:07.000
Translator: Alo-Jarmo Küppas
Reviewer: Piret Hion

00:00:15.260 --> 00:00:17.260
Mark Zukerbergi käest

00:00:17.260 --> 00:00:20.260
küsis ajakirjanik facebooki uudistevoo kohta.

00:00:20.260 --> 00:00:22.260
Ajakirjanik küsis:

00:00:22.260 --> 00:00:24.260
"Miks see üldse nii oluline on?"

00:00:24.260 --> 00:00:26.260
Ja Zukerberg vastas:

00:00:26.260 --> 00:00:28.260
"Teie hoovis surev orav

00:00:28.260 --> 00:00:31.260
võib olla teie huvidest lähtuvalt olulisem

00:00:31.260 --> 00:00:34.260
kui inimesed, kes surevad Aafrikas."

00:00:34.260 --> 00:00:36.260
Ja ma tahaksin rääkida milline

00:00:36.260 --> 00:00:39.260
sellisel ideel põhinev internet välja võiks näha.

00:00:40.260 --> 00:00:42.260
Ma kasvasin üles

00:00:42.260 --> 00:00:44.260
Maine osariigi maapiirkonnas

00:00:44.260 --> 00:00:47.260
ja internet oli minu jaoks midagi erilist.

00:00:47.260 --> 00:00:49.260
See oli ühenduseks maailmaga.

00:00:49.260 --> 00:00:52.260
See oli midagi, mis meid kõiki omavahel ühendab.

00:00:52.260 --> 00:00:55.260
Ma olin kindel, et see tuleb suuresti kasuks demokraatiale

00:00:55.260 --> 00:00:58.260
ja kogu meie ühiskonnale.

00:00:58.260 --> 00:01:00.260
Kuid nüüd on toimumas muutus

00:01:00.260 --> 00:01:02.260
kuidas informatsioon internetis liigub

00:01:02.260 --> 00:01:05.260
ja see on nähtamatu.

00:01:05.260 --> 00:01:07.260
Ning kui me sellele tähelepanu ei pööra

00:01:07.260 --> 00:01:10.260
võib see probleemiks saada.

00:01:10.260 --> 00:01:13.260
Märkasin seda esimesena kohas, kus palju aega veedan -

00:01:13.260 --> 00:01:15.260
oma Facebooki lehel.

00:01:15.260 --> 00:01:18.260
Olen poliitiliselt liberaal - suur üllatus -

00:01:18.260 --> 00:01:20.260
aga olen teadlikult ka konservatiividega suhelnud.

00:01:20.260 --> 00:01:22.260
Mulle meeldib kuulata millest nad mõtlevad,

00:01:22.260 --> 00:01:24.260
mulle meeldib näha mida nad jagavad,

00:01:24.260 --> 00:01:26.260
mulle meeldib uusi asju teada saada.

00:01:26.260 --> 00:01:29.260
Ja ma olin üllatunud ühel päeval märgates,

00:01:29.260 --> 00:01:32.260
et mu konservatiividest sõbrad olid Facebooki uudistevoost kadunud.

00:01:33.260 --> 00:01:35.260
Tuli välja, et

00:01:35.260 --> 00:01:39.260
Facebook vaatas, millistele linkidele ma vajutan

00:01:39.260 --> 00:01:41.260
ja nägi, et tegelikult,

00:01:41.260 --> 00:01:43.260
vajutan ma liberaalidest sõprade linkidele rohkem

00:01:43.260 --> 00:01:46.260
kui konservatiivide omadele.

00:01:46.260 --> 00:01:48.260
Ja ilma minuga nõu pidamata,

00:01:48.260 --> 00:01:50.260
oli ta nad välja jätnud.

00:01:50.260 --> 00:01:53.260
Nad olid kadunud.

00:01:54.260 --> 00:01:56.260
Ega Facebook pole ainuke koht,

00:01:56.260 --> 00:01:58.260
mis sarnast nähtamatut, algoritmilist

00:01:58.260 --> 00:02:01.260
toimetamist kasutab.

00:02:01.260 --> 00:02:03.260
Google teeb sedasama.

00:02:03.260 --> 00:02:06.260
Kui mina ja teie praegu midagi otsiksime,

00:02:06.260 --> 00:02:08.260
koos ja samal ajal,

00:02:08.260 --> 00:02:11.260
võime saada täiesti erinevad otsingutulemused.

00:02:11.260 --> 00:02:14.260
Isegi juhul kui te ei ole sisse loginud, rääkis üks insener mulle,

00:02:14.260 --> 00:02:16.260
et on 57 signaali,

00:02:16.260 --> 00:02:19.260
mida Google kontrollib -

00:02:19.260 --> 00:02:22.260
alates millist arvutit kasutatakse,

00:02:22.260 --> 00:02:24.260
millist veebilehitsejat,

00:02:24.260 --> 00:02:26.260
kuni otsija asukohani -

00:02:26.260 --> 00:02:29.260
nende abil pannakse kokku otsingu tulemused.

00:02:29.260 --> 00:02:31.260
Mõelge natuke selle peale:

00:02:31.260 --> 00:02:35.260
tavalist Google'it ei ole enam olemas.

00:02:35.260 --> 00:02:38.260
Teate, naljakas asja juures on see, et seda on keeruline näha.

00:02:38.260 --> 00:02:40.260
Te ei saa ju näha, kui palju teie otsingutulemused

00:02:40.260 --> 00:02:42.260
teiste omadest erinevad.

00:02:42.260 --> 00:02:44.260
Kuid paar nädalat tagasi

00:02:44.260 --> 00:02:47.260
palusin oma sõpradel googeldada sõna "Egiptus".

00:02:47.260 --> 00:02:50.260
Palusin, et nad saadaksid mulle pildid vastustest.

00:02:50.260 --> 00:02:53.260
Siin on minu sõber Scott oma vastustega.

00:02:54.260 --> 00:02:57.260
Ja siin on Daniel enda omadega.

00:02:57.260 --> 00:02:59.260
Kui need kõrvuti panna,

00:02:59.260 --> 00:03:01.260
ei pea isegi linke lähemalt lugema,

00:03:01.260 --> 00:03:03.260
et aru saada kui erinevad tulemused on.

00:03:03.260 --> 00:03:05.260
Kuid lähemalt vaadates

00:03:05.260 --> 00:03:08.260
on see päris erakordne.

00:03:09.260 --> 00:03:12.260
Daniel ei saanud Egiptuse meeleavalduste kohta

00:03:12.260 --> 00:03:14.260
esimesel lehel mitte midagi.

00:03:14.260 --> 00:03:16.260
Scotti vastused olid neid aga täis.

00:03:16.260 --> 00:03:18.260
Need olid sel ajal aga kõige kuumemad uudised.

00:03:18.260 --> 00:03:21.260
Vaadake, kui erinevaks tulemused on muutumas.

00:03:21.260 --> 00:03:24.260
Ning see ei ole ainult Google ja Facebook.

00:03:24.260 --> 00:03:26.260
See on midagi, mis toimub kogu internetis.

00:03:26.260 --> 00:03:29.260
On suur hulk ettevõtteid, kes kasutavad samasugust isikustamist.

00:03:29.260 --> 00:03:32.260
Yahoo News, interneti suurim uudistelehekülg

00:03:32.260 --> 00:03:35.260
kasutab nüüd isikustamist - erinevad inimesed näevad erinevaid asju.

00:03:36.260 --> 00:03:39.260
Huffington Post, Washington Post ja New York Times -

00:03:39.260 --> 00:03:42.260
semmivad kõik erineval moel isikustamisega.

00:03:42.260 --> 00:03:45.260
Ja seeläbi liigume kiiresti

00:03:45.260 --> 00:03:47.260
olukorrani, kus

00:03:47.260 --> 00:03:51.260
internet näitab meile ainult neid asju mida ta eeldab, et näha tahame,

00:03:51.260 --> 00:03:54.260
mis ei pruugi alati olla asjad, mida nägema peame.

00:03:54.260 --> 00:03:57.260
Eric Smith on öelnud:

00:03:57.260 --> 00:04:00.260
"Inimeste jaoks on keeruline vaadata või tarbida midagi,

00:04:00.260 --> 00:04:02.260
mis mingis plaanis

00:04:02.260 --> 00:04:05.260
ei ole nende jaoks sobivaks tehtud."

00:04:05.260 --> 00:04:07.260
Ja ma arvan, et see on probleem.

00:04:07.260 --> 00:04:10.260
Ma arvan ka, et kui võtta kõik need filtrid üheskoos,

00:04:10.260 --> 00:04:12.260
kui võtta kõik need algoritmid,

00:04:12.260 --> 00:04:15.260
saame midagi, mida võiks nimetada filtreeritud mulliks.

00:04:16.260 --> 00:04:19.260
Ja see filtreeritud mull on Sinu isiklik,

00:04:19.260 --> 00:04:21.260
ainulaadne inforuum,

00:04:21.260 --> 00:04:23.260
milles internetis olles elad.

00:04:23.260 --> 00:04:26.260
See, mis seal filtreeritud mullis on,

00:04:26.260 --> 00:04:29.260
sõltub sellest, kes Sa oled ja millega tegeled.

00:04:29.260 --> 00:04:33.260
Kuid asi on selles, et Sa ei saa otsustada, mis sinna mulli pääseb.

00:04:33.260 --> 00:04:35.260
Veel olulisem on,

00:04:35.260 --> 00:04:38.260
Sa ei saa ka tegelikult näha, mis sealt mullist välja jäetakse.

00:04:38.260 --> 00:04:40.260
Üks filtreeritud mulli probleeme

00:04:40.260 --> 00:04:43.260
avastati Netflixi analüütikute poolt.

00:04:43.260 --> 00:04:46.260
Nad vaatasid Netflixi videote järjekordi ja avastasid midagi päris huvitavat,

00:04:46.260 --> 00:04:48.260
mida me ilmselt ka ise oleme märganud,

00:04:48.260 --> 00:04:50.260
et on mõned filmid

00:04:50.260 --> 00:04:53.260
mis otsejoones meile kodudesse jõuavad.

00:04:53.260 --> 00:04:56.260
Nad on järjekorras ja hüppavad kohe välja.

00:04:56.260 --> 00:04:58.260
Näiteks "Raudmees" hüppab kohe välja,

00:04:58.260 --> 00:05:00.260
aga "Oodates Supermani"

00:05:00.260 --> 00:05:02.260
võib päris kaua aega oodata.

00:05:02.260 --> 00:05:04.260
Nad avastasid,

00:05:04.260 --> 00:05:06.260
et meie Netflixi järjekordades

00:05:06.260 --> 00:05:09.260
käib suur võitlus

00:05:09.260 --> 00:05:12.260
meie tuleviku poole püüdlevate "minade"

00:05:12.260 --> 00:05:15.260
ja meie impulsiivsemate, olevikus elavate "minade" vahel.

00:05:15.260 --> 00:05:17.260
Et me kõik tahaksime öelda, et

00:05:17.260 --> 00:05:19.260
oleme juba näinud kultusfilmi "Rashomon",

00:05:19.260 --> 00:05:21.260
kuid praegu

00:05:21.260 --> 00:05:24.260
tahame juba neljandat korda vaadata filmi "Ace Ventura".

00:05:24.260 --> 00:05:27.260
(Naer)

00:05:27.260 --> 00:05:29.260
Ehk parim filtreerimine annab meile pisut mõlemat.

00:05:29.260 --> 00:05:31.260
See annab natukene Justin Bieberit

00:05:31.260 --> 00:05:33.260
ja pisut Afganistani.

00:05:33.260 --> 00:05:35.260
See annab meile olulist teavet

00:05:35.260 --> 00:05:38.260
ja pisut kollast sinna hulka.

00:05:38.260 --> 00:05:40.260
Selliste algoritmiliste filtrite probleem,

00:05:40.260 --> 00:05:42.260
selliste isikustatud filtrite väljakutse

00:05:42.260 --> 00:05:44.260
on, et nad vaatavad põhiliselt,

00:05:44.260 --> 00:05:48.260
kuhu vajutatakse esimesena

00:05:48.260 --> 00:05:52.260
ja võivad tasakaalust välja minna.

00:05:52.260 --> 00:05:55.260
Ning tasakaalustatud info asemel

00:05:55.260 --> 00:05:57.260
võite olla ümbritsetud

00:05:57.260 --> 00:05:59.260
informatsioon kiirtoidust.

00:05:59.260 --> 00:06:01.260
See viitab tõsiasjale,

00:06:01.260 --> 00:06:04.260
et oleme tegelikult internetist valesti aru saanud.

00:06:04.260 --> 00:06:06.260
Teabeühiskonnas -

00:06:06.260 --> 00:06:08.260
selline müüt on käibel -

00:06:08.260 --> 00:06:10.260
teabeühiskonnas

00:06:10.260 --> 00:06:12.260
olid nn. "väravavalvurid", toimetajad,

00:06:12.260 --> 00:06:15.260
kes kontrollisid kogu informatsiooni liikumist.

00:06:15.260 --> 00:06:18.260
Koos internetiga muutusid nad tähtsusetuks,

00:06:18.260 --> 00:06:20.260
see andis kõigile võimaluse omavahel ühenduses olla

00:06:20.260 --> 00:06:22.260
ja see oli super.

00:06:22.260 --> 00:06:25.260
Kuid nüüd toimub hoopis midagi muud.

00:06:26.260 --> 00:06:29.260
Nüüd oleme tunnistajaks,

00:06:29.260 --> 00:06:31.260
et inimestest "väravavalvurid"

00:06:31.260 --> 00:06:34.260
lihtsalt asendatakse algoritmilistega.

00:06:34.260 --> 00:06:37.260
Pprobleem on selles, et algoritmidel

00:06:37.260 --> 00:06:40.260
ei ole veel sisse ehitatud eetilist kompassi,

00:06:40.260 --> 00:06:43.260
mis toimetajatel olemas oli.

00:06:43.260 --> 00:06:46.260
Ehk, kui algoritmid maailma meie eest kureerivad,

00:06:46.260 --> 00:06:49.260
kui nemad otsustavad, mida me näeme ja mida mitte,

00:06:49.260 --> 00:06:51.260
siis peame olema kindlad,

00:06:51.260 --> 00:06:54.260
et nad ei otsiks ainult olulisi samasusi.

00:06:54.260 --> 00:06:56.260
Peame olema kindlad, et nad näitaksid meile ka asju,

00:06:56.260 --> 00:06:59.260
mis võivad olla ebamugavad, või keerulised, või tähtsad -

00:06:59.260 --> 00:07:01.260
see on see, mida TED teeb -

00:07:01.260 --> 00:07:03.260
teistsugused seisukohad.

00:07:03.260 --> 00:07:05.260
Me oleme seda korra juba läbi teinud

00:07:05.260 --> 00:07:07.260
ühiskonnana.

00:07:08.260 --> 00:07:11.260
1915.a. ei mõelnud ajalehed just palju

00:07:11.260 --> 00:07:14.260
oma sotsiaalse vastutuse peale.

00:07:14.260 --> 00:07:16.260
Kuid siis märkasid inimesed,

00:07:16.260 --> 00:07:19.260
et ajalehtedel on tegelikult oluline roll.

00:07:19.260 --> 00:07:21.260
Sest ei saa olla

00:07:21.260 --> 00:07:23.260
toimivat demokraatiat

00:07:23.260 --> 00:07:27.260
ilma, et kodanikeni jõuaks olulist teavet.

00:07:28.260 --> 00:07:31.260
Ajalehed olid tähtsad, sest toimisid sellise filtrina

00:07:31.260 --> 00:07:33.260
ja tekkiski ajakirjanduslik eetika.

00:07:33.260 --> 00:07:35.260
See ei olnud ideaalne,

00:07:35.260 --> 00:07:38.260
aga see aitas meid läbi eelmise sajandi.

00:07:38.260 --> 00:07:40.260
Nüüdsiis

00:07:40.260 --> 00:07:43.260
oleme internetiga justkui aastas 1915.

00:07:44.260 --> 00:07:47.260
Me vajame uusi "väravahoidjaid",

00:07:47.260 --> 00:07:49.260
kes kirjutaksid sellise vastutustunde

00:07:49.260 --> 00:07:51.260
ka loodavatesse koodidesse.

00:07:51.260 --> 00:07:54.260
Tean, et siin on palju inimesi Facebookist ja Googlest -

00:07:54.260 --> 00:07:56.260
Larry ja Sergei -

00:07:56.260 --> 00:07:58.260
inimesed, kes on aidanud interneti ehitada selliseks nagu ta täna on,

00:07:58.260 --> 00:08:00.260
ja ma olen selle eest tänulik.

00:08:00.260 --> 00:08:03.260
Kuid me peame olema kindlad,

00:08:03.260 --> 00:08:06.260
et nende poolt kirjutatud algoritmid sisaldavad

00:08:06.260 --> 00:08:09.260
arusaama avalikust elust, mingisugust kodanikuvastutust.

00:08:09.260 --> 00:08:12.260
Peame kindlad olema, et nad on piisavalt avatud,

00:08:12.260 --> 00:08:14.260
et aru saada, milliste reeglitega on tegu,

00:08:14.260 --> 00:08:17.260
mis filtreerimist reguleerivad.

00:08:17.260 --> 00:08:19.260
Me tahame, et te annaksite osa kontrollist meile,

00:08:19.260 --> 00:08:21.260
et saaksime ise otsustada,

00:08:21.260 --> 00:08:24.260
mis filtrist läbi pääseb ja mis mitte.

00:08:24.260 --> 00:08:26.260
Sest ma arvan,

00:08:26.260 --> 00:08:28.260
et internet peab olema see koht,

00:08:28.260 --> 00:08:30.260
millest me kõik unistanud oleme.

00:08:30.260 --> 00:08:33.260
See peab meid kõiki omavahel ühendama.

00:08:33.260 --> 00:08:36.260
See peab tutvustama meile uusi ideid,

00:08:36.260 --> 00:08:39.260
uusi inimesi, erinevaid seisukohti.

00:08:40.260 --> 00:08:42.260
Seda ei juhtu,

00:08:42.260 --> 00:08:45.260
kui internet meid hoopis isoleerima hakkab.

00:08:45.260 --> 00:08:47.260
Aitäh.

00:08:47.260 --> 00:08:58.260
(Aplaus)

