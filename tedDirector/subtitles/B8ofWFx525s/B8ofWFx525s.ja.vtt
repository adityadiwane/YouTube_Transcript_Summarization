WEBVTT
Kind: captions
Language: ja

00:00:00.000 --> 00:00:07.000
翻訳: Sawa Horibe
校正: Takafusa Kitazume

00:00:15.260 --> 00:00:17.260
マーク・ザッカーバーグが

00:00:17.260 --> 00:00:20.260
ニュース配信について質問されていました

00:00:20.260 --> 00:00:22.260
「なぜそんなに重要なんですか？」

00:00:22.260 --> 00:00:24.260
ジャーナリストが聞くと

00:00:24.260 --> 00:00:26.260
ザッカーバーグは答えました

00:00:26.260 --> 00:00:28.260
「今あなたの関心事は

00:00:28.260 --> 00:00:31.260
アフリカで死に瀕した人々よりも

00:00:31.260 --> 00:00:34.260
家の庭で死にかけているリスかもしれないでしょう」

00:00:34.260 --> 00:00:36.260
この関連性の考え方をもとにした

00:00:36.260 --> 00:00:39.260
ウェブとはどういうことか話したいと思います

00:00:40.260 --> 00:00:42.260
メイン州の片田舎に住んでいた

00:00:42.260 --> 00:00:44.260
僕にとって 子供の頃

00:00:44.260 --> 00:00:47.260
インターネットは全く違う意味を持ったものでした

00:00:47.260 --> 00:00:49.260
世界へのドアであり

00:00:49.260 --> 00:00:52.260
人々を繋げるものでした

00:00:52.260 --> 00:00:55.260
民主主義と僕たちの社会に

00:00:55.260 --> 00:00:58.260
大きく役立つに違いないと思っていました

00:00:58.260 --> 00:01:00.260
でも見えないところで

00:01:00.260 --> 00:01:02.260
インターネットでの

00:01:02.260 --> 00:01:05.260
情報の流れに変化がありました

00:01:05.260 --> 00:01:07.260
注意しないと

00:01:07.260 --> 00:01:10.260
大きな問題になるかもしれません

00:01:10.260 --> 00:01:13.260
これに初めて気付いたのは僕がいつも使っている

00:01:13.260 --> 00:01:15.260
Facebookのページでした

00:01:15.260 --> 00:01:18.260
見た目どおり 僕は政党的には進歩派ですが

00:01:18.260 --> 00:01:20.260
進んで保守派と交流しています

00:01:20.260 --> 00:01:22.260
保守派の考えを聞いたり

00:01:22.260 --> 00:01:24.260
何にリンクしているか見たり

00:01:24.260 --> 00:01:26.260
何か学びたいと思っています

00:01:26.260 --> 00:01:29.260
ですから ある日僕のFacebookの配信から

00:01:29.260 --> 00:01:32.260
保守派の人たちが消えてしまって驚きました

00:01:33.260 --> 00:01:35.260
何が起こったのかと言うと

00:01:35.260 --> 00:01:39.260
僕がどのリンクをクリックしているかFacebookがチェックしていて

00:01:39.260 --> 00:01:41.260
実際に保守派より

00:01:41.260 --> 00:01:43.260
リベラル派の友達のリンクを

00:01:43.260 --> 00:01:46.260
クリックすることが多いと気づき

00:01:46.260 --> 00:01:48.260
僕に何の相談もなく

00:01:48.260 --> 00:01:50.260
保守派を削除していたのです

00:01:50.260 --> 00:01:53.260
保守派はいなくなってしまいました

00:01:54.260 --> 00:01:56.260
このように目に見えないところで

00:01:56.260 --> 00:01:58.260
アルゴリズムによるウェブの

00:01:58.260 --> 00:02:01.260
編集をしているのはFacebookだけではありません

00:02:01.260 --> 00:02:03.260
Googleもしています

00:02:03.260 --> 00:02:06.260
何かの検索をするにしても 僕とあなたでは

00:02:06.260 --> 00:02:08.260
たとえ今同時に検索しても

00:02:08.260 --> 00:02:11.260
かなり違う結果になる可能性があります

00:02:11.260 --> 00:02:14.260
あるエンジニアによると ログオンしてなくても

00:02:14.260 --> 00:02:16.260
57個のシグナルを

00:02:16.260 --> 00:02:19.260
Googleはチェックしているそうです

00:02:19.260 --> 00:02:22.260
どんなPCを使っているか

00:02:22.260 --> 00:02:24.260
どのブラウザを使っているか

00:02:24.260 --> 00:02:26.260
現在地は何処かなどによって

00:02:26.260 --> 00:02:29.260
検索結果を調節しているのです

00:02:29.260 --> 00:02:31.260
ちょっと考えてみてください

00:02:31.260 --> 00:02:35.260
もう標準のGoogle検索はないのです

00:02:35.260 --> 00:02:38.260
そして妙なことに これは認識しにくい現象です

00:02:38.260 --> 00:02:40.260
自分の検索結果がどれほど

00:02:40.260 --> 00:02:42.260
他人のものと違うか分かりません

00:02:42.260 --> 00:02:44.260
でも数週間前

00:02:44.260 --> 00:02:47.260
Googleで「エジプト」と検索したスクリーンショットを

00:02:47.260 --> 00:02:50.260
送ってもらうよう大勢の友人に頼みました

00:02:50.260 --> 00:02:53.260
これが友人スコットのスクリーンショットです

00:02:54.260 --> 00:02:57.260
こちらはダニエルからのスクリーンショットです

00:02:57.260 --> 00:02:59.260
並べて見てみると

00:02:59.260 --> 00:03:01.260
リンクを読むまでもなく

00:03:01.260 --> 00:03:03.260
２つのページの違いに気付きます

00:03:03.260 --> 00:03:05.260
でも実際リンクを読むと

00:03:05.260 --> 00:03:08.260
本当に驚きます

00:03:09.260 --> 00:03:12.260
ダニエルのGoogle検索結果の最初のページには

00:03:12.260 --> 00:03:14.260
エジプトのデモ関連記事が全くなく

00:03:14.260 --> 00:03:16.260
スコットの方はそればかりでした

00:03:16.260 --> 00:03:18.260
この頃大きな話題だったのにです

00:03:18.260 --> 00:03:21.260
これほど検索結果は異なってきています

00:03:21.260 --> 00:03:24.260
GoogleとFacebookだけではありません

00:03:24.260 --> 00:03:26.260
これはウェブ全体で起こっています

00:03:26.260 --> 00:03:29.260
多くの企業がこのようなカスタマイズをしています

00:03:29.260 --> 00:03:32.260
Yahoo！ニュースはネット最大のニュースサイトですが

00:03:32.260 --> 00:03:35.260
今ではカスタマイズされた違う内容を提示しています

00:03:36.260 --> 00:03:39.260
Huffington Postや Washington Postや New York Timesも

00:03:39.260 --> 00:03:42.260
様々な形でカスタマイズを試みています

00:03:42.260 --> 00:03:45.260
その結果 インターネットは私たちが見たいものを

00:03:45.260 --> 00:03:47.260
予測して見せているが

00:03:47.260 --> 00:03:51.260
それは必ずしも私たちが見る必要があるものでない

00:03:51.260 --> 00:03:54.260
という状況に急速に変わってきています

00:03:54.260 --> 00:03:57.260
エリック・シュミットが言うように

00:03:57.260 --> 00:04:00.260
「全く何のカスタマイズもされていないものを

00:04:00.260 --> 00:04:02.260
人々が見たり利用したりするのは

00:04:02.260 --> 00:04:05.260
とても難しくなるでしょう」

00:04:05.260 --> 00:04:07.260
ですからこれは問題だと思います

00:04:07.260 --> 00:04:10.260
このようなフィルターやアルゴリズムを

00:04:10.260 --> 00:04:12.260
全部集めてできるのは

00:04:12.260 --> 00:04:15.260
「フィルターに囲まれた世界」だと思うのです

00:04:16.260 --> 00:04:19.260
そしてこの「囲まれた世界」がネット上での

00:04:19.260 --> 00:04:21.260
あなた個人独特の

00:04:21.260 --> 00:04:23.260
情報世界となるわけです

00:04:23.260 --> 00:04:26.260
さらに自分の世界に何が含まれるかは 自分が

00:04:26.260 --> 00:04:29.260
どんな人で何をしているかによって決まります

00:04:29.260 --> 00:04:33.260
でも何が取り入れられるかは自分次第でないのが問題です

00:04:33.260 --> 00:04:35.260
もっと重要なのは

00:04:35.260 --> 00:04:38.260
何が削除されるか自分には見えないことです

00:04:38.260 --> 00:04:40.260
フィルターによる問題の１つは

00:04:40.260 --> 00:04:43.260
Netflixのデータアナリストたちに発見されました

00:04:43.260 --> 00:04:46.260
発送待ち段階で妙なことが起こっていたのです

00:04:46.260 --> 00:04:48.260
気付いていた人も多いでしょうか

00:04:48.260 --> 00:04:50.260
映画によってはDVDが

00:04:50.260 --> 00:04:53.260
すぐに発送されて家に届きます

00:04:53.260 --> 00:04:56.260
オーダーが入るとすぐに発送されます

00:04:56.260 --> 00:04:58.260
ですから「アイアンマン」はすぐ届くのに

00:04:58.260 --> 00:05:00.260
「ウェイティング・フォー・スーパーマン（原題）」は

00:05:00.260 --> 00:05:02.260
待ち時間がとても長いことがあります

00:05:02.260 --> 00:05:04.260
データアナリストたちが

00:05:04.260 --> 00:05:06.260
DVDの発送待ち段階で見たのは

00:05:06.260 --> 00:05:09.260
利用者の計画的な向上心と

00:05:09.260 --> 00:05:12.260
もっと衝動的な今の欲望が

00:05:12.260 --> 00:05:15.260
大きく対立している状況でした

00:05:15.260 --> 00:05:17.260
「羅生門」を観たことがある人に

00:05:17.260 --> 00:05:19.260
なりたいと思う一方

00:05:19.260 --> 00:05:21.260
今は ４回目の

00:05:21.260 --> 00:05:24.260
「エース・ベンチュラ」を観たいわけです

00:05:24.260 --> 00:05:27.260
（笑）

00:05:27.260 --> 00:05:29.260
最良の編集は両面を見せるものです

00:05:29.260 --> 00:05:31.260
ジャスティン・ビーバーを少し

00:05:31.260 --> 00:05:33.260
アフガニスタンを少し

00:05:33.260 --> 00:05:35.260
ヘルシーな情報と

00:05:35.260 --> 00:05:38.260
デザート的情報も提供するわけです

00:05:38.260 --> 00:05:40.260
アルゴリズムでカスタマイズされた

00:05:40.260 --> 00:05:42.260
フィルターの問題点は

00:05:42.260 --> 00:05:44.260
利用者が何を最初に

00:05:44.260 --> 00:05:48.260
クリックしたかを主に参考にしているため

00:05:48.260 --> 00:05:52.260
そのようなバランスを崩してしまうことです

00:05:52.260 --> 00:05:55.260
バランスのとれた情報摂取の代わりに

00:05:55.260 --> 00:05:57.260
ジャンク情報ばかりに

00:05:57.260 --> 00:05:59.260
囲まれてしまうこともあり得ます

00:05:59.260 --> 00:06:01.260
つまりインターネットに対する

00:06:01.260 --> 00:06:04.260
我々の見解は実は間違っているのかもということです

00:06:04.260 --> 00:06:06.260
放送社会の

00:06:06.260 --> 00:06:08.260
創設神話はこんな感じです

00:06:08.260 --> 00:06:10.260
「放送社会では

00:06:10.260 --> 00:06:12.260
門番である編集者によって

00:06:12.260 --> 00:06:15.260
情報の流れがコントロールされていました

00:06:15.260 --> 00:06:18.260
でもインターネットが現れ 門番を追い払ったので

00:06:18.260 --> 00:06:20.260
素晴らしいことに 私たちは

00:06:20.260 --> 00:06:22.260
繋がり合えるようになりました」

00:06:22.260 --> 00:06:25.260
でも今 実際にはそうなっていません

00:06:26.260 --> 00:06:29.260
私たちが目にしているのは どちらかと言うと

00:06:29.260 --> 00:06:31.260
人間の門番からアルゴリズムの門番に

00:06:31.260 --> 00:06:34.260
バトンが渡されている状況です

00:06:34.260 --> 00:06:37.260
そして問題なのはアルゴリズムは

00:06:37.260 --> 00:06:40.260
編集者が持ち合わせていた倫理観念を

00:06:40.260 --> 00:06:43.260
まだ持っていないということです

00:06:43.260 --> 00:06:46.260
ですから世界の情報をアルゴリズムが監修して

00:06:46.260 --> 00:06:49.260
私たちが何を見て 何を見ないか決めるのなら

00:06:49.260 --> 00:06:51.260
アルゴリズムが関連性以外の要素も

00:06:51.260 --> 00:06:54.260
使用するようにしなくてはなりません

00:06:54.260 --> 00:06:56.260
見たくないものや難しいもの

00:06:56.260 --> 00:06:59.260
重要なものなども提示するようにしなくてはだめです

00:06:59.260 --> 00:07:01.260
TEDが異なる視点を

00:07:01.260 --> 00:07:03.260
見せていることと同じです

00:07:03.260 --> 00:07:05.260
過去我々は同じ問題に

00:07:05.260 --> 00:07:07.260
直面しています

00:07:08.260 --> 00:07:11.260
1915年当時 新聞は市民としての義務について

00:07:11.260 --> 00:07:14.260
あまり考えていませんでした

00:07:14.260 --> 00:07:16.260
でも人々は新聞が

00:07:16.260 --> 00:07:19.260
重要な役割を果たしていることに気付きました

00:07:19.260 --> 00:07:21.260
つまり 市民が

00:07:21.260 --> 00:07:23.260
適切な情報を得ていないと

00:07:23.260 --> 00:07:27.260
民主主義は機能しないということです　よって

00:07:28.260 --> 00:07:31.260
情報のフィルターを行う新聞は重要なのです

00:07:31.260 --> 00:07:33.260
そしてジャーナリズムの倫理ができました

00:07:33.260 --> 00:07:35.260
完璧ではありませんでしたが

00:07:35.260 --> 00:07:38.260
１世紀の間このやり方をしてきました

00:07:38.260 --> 00:07:40.260
ですから現在私たちは

00:07:40.260 --> 00:07:43.260
ウェブ上での1915年に直面しているようなものです

00:07:44.260 --> 00:07:47.260
プログラムを書く上で このような責任を

00:07:47.260 --> 00:07:49.260
組み込んでくれる

00:07:49.260 --> 00:07:51.260
新しい門番が必要なのです

00:07:51.260 --> 00:07:54.260
会場にはFacebookとGoogleからの出席者も大勢います

00:07:54.260 --> 00:07:56.260
ラリーやサーゲイのように

00:07:56.260 --> 00:07:58.260
今あるウェブの構築に

00:07:58.260 --> 00:08:00.260
貢献した人々には感謝しています

00:08:00.260 --> 00:08:03.260
でも私たちは彼らに これらのアルゴリズムに

00:08:03.260 --> 00:08:06.260
生活や市民の義務が組み込まれているように

00:08:06.260 --> 00:08:09.260
しっかり確認してもらいたいのです

00:08:09.260 --> 00:08:12.260
アルゴリズムが明白で何が入ってくるかを決める

00:08:12.260 --> 00:08:14.260
フィルターのルールが

00:08:14.260 --> 00:08:17.260
理解できるようにして欲しいのです

00:08:17.260 --> 00:08:19.260
さらに自分で何が削除されて

00:08:19.260 --> 00:08:21.260
何がされないか決められるように

00:08:21.260 --> 00:08:24.260
管理できるオプションを提供してもらいたいです

00:08:24.260 --> 00:08:26.260
これも インターネットが私たちの

00:08:26.260 --> 00:08:28.260
思い描いていたようなものに

00:08:28.260 --> 00:08:30.260
なってもらう必要があると思うからです

00:08:30.260 --> 00:08:33.260
みんなを繋ぐものであり

00:08:33.260 --> 00:08:36.260
新しいアイデアや人々 そして

00:08:36.260 --> 00:08:39.260
異なる視点を提示するものであるべきです

00:08:40.260 --> 00:08:42.260
これを達成するには

00:08:42.260 --> 00:08:45.260
ウェブ上で私たちが孤立しないようにするべきです

00:08:45.260 --> 00:08:47.260
ありがとう

00:08:47.260 --> 00:08:58.260
(拍手)

