WEBVTT
Kind: captions
Language: ca

00:00:00.000 --> 00:00:07.000
Translator: Xavi Ivars
Reviewer: Pol Monsó Purtí

00:00:15.260 --> 00:00:17.260
Mark Zuckerberg,

00:00:17.260 --> 00:00:20.260
un periodista li feia una pregunta referent al "mur de notícies".

00:00:20.260 --> 00:00:22.260
El periodista li estava preguntant:

00:00:22.260 --> 00:00:24.260
"Per què és el "mur" tan important?"

00:00:24.260 --> 00:00:26.260
I Zuckerberg va dir,

00:00:26.260 --> 00:00:28.260
"Un esquirol morint davant de ta casa

00:00:28.260 --> 00:00:31.260
pot ser més rellevant per al teu interès en aquest moment

00:00:31.260 --> 00:00:34.260
que gent morint a l'Àfrica."

00:00:34.260 --> 00:00:36.260
I vull parlar sobre

00:00:36.260 --> 00:00:39.260
com seria una Web basada en eixa idea de rellevància.

00:00:40.260 --> 00:00:42.260
Quan estava creixent

00:00:42.260 --> 00:00:44.260
en una àrea rural a Maine,

00:00:44.260 --> 00:00:47.260
'Internet' significava una cosa molt diferent per mi.

00:00:47.260 --> 00:00:49.260
Significava una connexió amb el món.

00:00:49.260 --> 00:00:52.260
Significava quelcom per connectar-nos a tots.

00:00:52.260 --> 00:00:55.260
I estava segur que anava a ser quelcom molt bo per a la democràcia

00:00:55.260 --> 00:00:58.260
i per a la nostra societat.

00:00:58.260 --> 00:01:00.260
Però ha aparegut aquest desplaçament

00:01:00.260 --> 00:01:02.260
respecte a com circula la informació a la xarxa,

00:01:02.260 --> 00:01:05.260
i que és invisible.

00:01:05.260 --> 00:01:07.260
I que si no hi estem atents

00:01:07.260 --> 00:01:10.260
podria ser un problema real.

00:01:10.260 --> 00:01:13.260
El primer lloc on em vaig adonar d'això va ser en un lloc on passe molt de temps --

00:01:13.260 --> 00:01:15.260
la meva pàgina a Facebook.

00:01:15.260 --> 00:01:18.260
Sóc progressista, ideològicament -- gran sorpresa --

00:01:18.260 --> 00:01:20.260
però sempre m'he preocupat per trobar-me amb conservadors.

00:01:20.260 --> 00:01:22.260
M'agrada escoltar de què parlem;

00:01:22.260 --> 00:01:24.260
m'agrada veure què enllancen;

00:01:24.260 --> 00:01:26.260
m'agrada aprendre coses d'ells.

00:01:26.260 --> 00:01:29.260
I per això em vaig sorprendre el dia que em vaig adonar

00:01:29.260 --> 00:01:32.260
que els conservadors havien desaparegut del meu mur de Facebook.

00:01:33.260 --> 00:01:35.260
I el què havia ocorregut era

00:01:35.260 --> 00:01:39.260
que Facebook estava controlant sobre quins enllaços feia clic,

00:01:39.260 --> 00:01:41.260
i estava descobrint que, de fet,

00:01:41.260 --> 00:01:43.260
feia més clics als enllaços dels meus amics progressistes

00:01:43.260 --> 00:01:46.260
que als enllaços dels meus amics conservadors.

00:01:46.260 --> 00:01:48.260
I sense preguntar-m'ho,

00:01:48.260 --> 00:01:50.260
els havia eliminat.

00:01:50.260 --> 00:01:53.260
Havien desaparegut.

00:01:54.260 --> 00:01:56.260
Però Facebook no és l'únic lloc

00:01:56.260 --> 00:01:58.260
que està fent aquesta mena d'edició algorítmica invisible

00:01:58.260 --> 00:02:01.260
de la Web.

00:02:01.260 --> 00:02:03.260
Google també ho està fent.

00:02:03.260 --> 00:02:06.260
Si jo cerque alguna cosa, i vosaltres cerqueu el mateix,

00:02:06.260 --> 00:02:08.260
fins i tot ara mateix, simultàniament,

00:02:08.260 --> 00:02:11.260
podríem obtenir resultats molt diferents.

00:02:11.260 --> 00:02:14.260
Fins i tot amb la sessió tancada, segons em va dir un enginyer,

00:02:14.260 --> 00:02:16.260
hi ha 57 senyals

00:02:16.260 --> 00:02:19.260
que Google té en compte --

00:02:19.260 --> 00:02:22.260
des d'allò relacionat amb el tipus d'ordinador en el què esteu

00:02:22.260 --> 00:02:24.260
fins al tipus de navegador que utilitzeu

00:02:24.260 --> 00:02:26.260
o on esteu físicament --

00:02:26.260 --> 00:02:29.260
i utilitza aquesta informació per confeccionar els resultats de la vostra cerca.

00:02:29.260 --> 00:02:31.260
Penseu-ho un moment:

00:02:31.260 --> 00:02:35.260
ja no hi ha un Google estàndar.

00:02:35.260 --> 00:02:38.260
I el pitjor d'això és que és complicat de veure.

00:02:38.260 --> 00:02:40.260
No podeu veure la diferència entre els vostres resultats

00:02:40.260 --> 00:02:42.260
i els de qualsevol altra persona.

00:02:42.260 --> 00:02:44.260
Però fa un parell de setmanes,

00:02:44.260 --> 00:02:47.260
els vaig demanar a un grapat d'amics que cercaren "Egipte" al Google

00:02:47.260 --> 00:02:50.260
i que m'enviaren en una captura de pantalla del què havien obtingut.

00:02:50.260 --> 00:02:53.260
Ací podeu veure la captura del meu amic Scott.

00:02:54.260 --> 00:02:57.260
I aci la del meu amic Daniel.

00:02:57.260 --> 00:02:59.260
Si les poseu una al costat de l'altra

00:02:59.260 --> 00:03:01.260
no cal ni que llegiu els enllaços

00:03:01.260 --> 00:03:03.260
per a observar com de diferents són ambdues pàgines.

00:03:03.260 --> 00:03:05.260
Però si llegiu els enllaços

00:03:05.260 --> 00:03:08.260
és realment molt gran la diferència.

00:03:09.260 --> 00:03:12.260
Daniel no va rebre res relatiu a les protestes d'Egipte

00:03:12.260 --> 00:03:14.260
en la primera pàgina de resultats de Google.

00:03:14.260 --> 00:03:16.260
Els resultats de Scott n'estaven plens.

00:03:16.260 --> 00:03:18.260
I aquesta era la història del dia en aquell moment.

00:03:18.260 --> 00:03:21.260
Així és com de diferents s'estan convertint aquestos resultats.

00:03:21.260 --> 00:03:24.260
I tampoc passa només amb Google i Facebook.

00:03:24.260 --> 00:03:26.260
Açò és una cosa que està arrasant la web.

00:03:26.260 --> 00:03:29.260
Hi ha tot un munt d'empreses que estan fent aquest tipus de personalització.

00:03:29.260 --> 00:03:32.260
Yahoo News, la major web de notícies a Internet,

00:03:32.260 --> 00:03:35.260
és ara personalitzada -- diferents persones reben diferents notícies.

00:03:36.260 --> 00:03:39.260
Huffington Post, el Washington Post, el New York Times --

00:03:39.260 --> 00:03:42.260
tots juguen amb la personalització d'alguna manera.

00:03:42.260 --> 00:03:45.260
I açó ens duu molt ràpidament

00:03:45.260 --> 00:03:47.260
cap a un mon en què

00:03:47.260 --> 00:03:51.260
Internet ens mostra el que creu que volem veure,

00:03:51.260 --> 00:03:54.260
però no necessàriament el que necessitem veure.

00:03:54.260 --> 00:03:57.260
Tal i com va dir Eric Schmidt,

00:03:57.260 --> 00:04:00.260
"Serà molt complicat per a la gent veure o consumir alguna cosa

00:04:00.260 --> 00:04:02.260
que no ha estat, d'alguna manera,

00:04:02.260 --> 00:04:05.260
preparat per a ells."

00:04:05.260 --> 00:04:07.260
I jo crec que açò és un problema.

00:04:07.260 --> 00:04:10.260
I pense que, si unim tots aquestos filtres,

00:04:10.260 --> 00:04:12.260
si agafem tots aquestos algoritmes,

00:04:12.260 --> 00:04:15.260
aconseguim el que jo anomene "bombolla de filtres".

00:04:16.260 --> 00:04:19.260
I la vostra "bombolla de filtres" és el vostre personal

00:04:19.260 --> 00:04:21.260
i únic univers d'informació

00:04:21.260 --> 00:04:23.260
en el què viviu en la xarxa.

00:04:23.260 --> 00:04:26.260
I el que hi ha en la vostra bombolla de filtres

00:04:26.260 --> 00:04:29.260
depén de qui sou, i depén de què feu.

00:04:29.260 --> 00:04:33.260
Però la cosa més important és que no decidiu que entra.

00:04:33.260 --> 00:04:35.260
I més important,

00:04:35.260 --> 00:04:38.260
no veieu què es queda fora.

00:04:38.260 --> 00:04:40.260
Un dels problemes de la bombolla de filtres

00:04:40.260 --> 00:04:43.260
va ser descobert per uns investigadors a Netflix.

00:04:43.260 --> 00:04:46.260
I quan estaven mirant les cues a Netflix, es van adonar d'una cosa graciosa

00:04:46.260 --> 00:04:48.260
que probablement molts ja havíem notat,

00:04:48.260 --> 00:04:50.260
i és que hi ha algunes pel·lícules

00:04:50.260 --> 00:04:53.260
que entren i de seguida ixen de les nostres cases.

00:04:53.260 --> 00:04:56.260
Entren a la cua, i ixen de seguida.

00:04:56.260 --> 00:04:58.260
Per exemple, "Iron Man" ix de seguida

00:04:58.260 --> 00:05:00.260
i "Esperant a Superman"

00:05:00.260 --> 00:05:02.260
pot estar esperant molt de temps.

00:05:02.260 --> 00:05:04.260
El que han descobert

00:05:04.260 --> 00:05:06.260
és que a les cues de Netflix

00:05:06.260 --> 00:05:09.260
hi ha aquesta batalla èpica

00:05:09.260 --> 00:05:12.260
entre les nostres aspiracions futures

00:05:12.260 --> 00:05:15.260
i els nostres impulsos presents.

00:05:15.260 --> 00:05:17.260
Tots volem ser algú

00:05:17.260 --> 00:05:19.260
que ha vist "Rashomon",

00:05:19.260 --> 00:05:21.260
però ara mateix

00:05:21.260 --> 00:05:24.260
volem veure per quarta vegada "Ace Ventura".

00:05:24.260 --> 00:05:27.260
(Rises)

00:05:27.260 --> 00:05:29.260
La millor edició ens dóna una mica de cada.

00:05:29.260 --> 00:05:31.260
Ens dona un poc de Justin Bieber

00:05:31.260 --> 00:05:33.260
i una miqueta d'Afganistan.

00:05:33.260 --> 00:05:35.260
Ens dóna certa informació sobre verdures;

00:05:35.260 --> 00:05:38.260
ens dona també informació sobre les postres.

00:05:38.260 --> 00:05:40.260
I la part complicada d'aquest tipus de filtres algorítmics,

00:05:40.260 --> 00:05:42.260
d'aquestos filtres personalitzats,

00:05:42.260 --> 00:05:44.260
és que, degut a que principalment es fixen

00:05:44.260 --> 00:05:48.260
en què cliqueu primer,

00:05:48.260 --> 00:05:52.260
poden trencar aquest equilibri.

00:05:52.260 --> 00:05:55.260
I en lloc d'informació per a una dieta equilibrada,

00:05:55.260 --> 00:05:57.260
podeu acabar rodejats

00:05:57.260 --> 00:05:59.260
d'informació de menjar escombraria.

00:05:59.260 --> 00:06:01.260
El que açò suggereix

00:06:01.260 --> 00:06:04.260
és que potser ens hem equivocat en la història sobre Internet.

00:06:04.260 --> 00:06:06.260
En una societat de la transmissió --

00:06:06.260 --> 00:06:08.260
així és com es funda una mitologia --

00:06:08.260 --> 00:06:10.260
en una societat de la transmissió,

00:06:10.260 --> 00:06:12.260
hi havia aquestos guardians, els editors,

00:06:12.260 --> 00:06:15.260
i ells controlaven els fluxes d'informació.

00:06:15.260 --> 00:06:18.260
I quan va arribar Internet i els va apartar,

00:06:18.260 --> 00:06:20.260
i ens va permetre connectar-nos junts,

00:06:20.260 --> 00:06:22.260
va ser increïble.

00:06:22.260 --> 00:06:25.260
Però no és el que està passant ara mateixa.

00:06:26.260 --> 00:06:29.260
El que estem veient és més una substitució

00:06:29.260 --> 00:06:31.260
dels guardians humans

00:06:31.260 --> 00:06:34.260
a guardians algorítmics.

00:06:34.260 --> 00:06:37.260
I el que passa és que els algorítmics

00:06:37.260 --> 00:06:40.260
encara no tenen integrats els conceptes ètics

00:06:40.260 --> 00:06:43.260
que si que tenien els editors.

00:06:43.260 --> 00:06:46.260
De manera que si els algoritmes van a administrar-nos el món,

00:06:46.260 --> 00:06:49.260
si van a decidir que anem a veure i què no,

00:06:49.260 --> 00:06:51.260
hem d'estar segurs

00:06:51.260 --> 00:06:54.260
que no són programats només per a la rellevància.

00:06:54.260 --> 00:06:56.260
Hem d'estar segurs que també ens mostren coses

00:06:56.260 --> 00:06:59.260
que són incòmodes o qüestionables o importants --

00:06:59.260 --> 00:07:01.260
açò és el que fa TED --

00:07:01.260 --> 00:07:03.260
altres punts de vista.

00:07:03.260 --> 00:07:05.260
I, a més, resulta que ja hem estat ací

00:07:05.260 --> 00:07:07.260
com a societat.

00:07:08.260 --> 00:07:11.260
El 1915 els periòdics no estaven massa implicats

00:07:11.260 --> 00:07:14.260
amb les seues responsabilitats cíviques.

00:07:14.260 --> 00:07:16.260
Aleshores la gent es va adonar

00:07:16.260 --> 00:07:19.260
que feien una cosa realment important.

00:07:19.260 --> 00:07:21.260
Que, de fet, no podia existir

00:07:21.260 --> 00:07:23.260
una democràcia que funcionara

00:07:23.260 --> 00:07:27.260
si els ciutadans no rebien un bon fluxe d'informació,

00:07:28.260 --> 00:07:31.260
i que els periòdics eren crucials degut a que actuaven com a filtre,

00:07:31.260 --> 00:07:33.260
i l'ètica periodística es va desenvolupar.

00:07:33.260 --> 00:07:35.260
No era perfecte,

00:07:35.260 --> 00:07:38.260
però ens va acompanyar tot el segle passat.

00:07:38.260 --> 00:07:40.260
I ara,

00:07:40.260 --> 00:07:43.260
tornem a estar al 1915 a la xarxa.

00:07:44.260 --> 00:07:47.260
I necessitem els guardians

00:07:47.260 --> 00:07:49.260
amb aquesta responsabilitat

00:07:49.260 --> 00:07:51.260
dins del codi que estan escrivint.

00:07:51.260 --> 00:07:54.260
Sé que hi ha molta gent ací de Facebook i de Google --

00:07:54.260 --> 00:07:56.260
Larry i Sergey --

00:07:56.260 --> 00:07:58.260
gent que ha ajudat a construir la Web com és ara,

00:07:58.260 --> 00:08:00.260
i els estic agraït.

00:08:00.260 --> 00:08:03.260
Però realment necessitem assegurar-nos

00:08:03.260 --> 00:08:06.260
que aquests algoritmes contenen

00:08:06.260 --> 00:08:09.260
un sentit de la vida pública, un sentit de la reponsabilitat cívica.

00:08:09.260 --> 00:08:12.260
Necessitem que ens assegurem que són suficientment transparents

00:08:12.260 --> 00:08:14.260
que podem veure quines regles són

00:08:14.260 --> 00:08:17.260
les que determinen què passa a través dels filtres.

00:08:17.260 --> 00:08:19.260
I necessitem que ens doneu una mica de control

00:08:19.260 --> 00:08:21.260
per a que puguem decidir

00:08:21.260 --> 00:08:24.260
què passa i què no.

00:08:24.260 --> 00:08:26.260
Perquè crec que

00:08:26.260 --> 00:08:28.260
necessitem que Internet siga

00:08:28.260 --> 00:08:30.260
allò que tots hem somniat que siga.

00:08:30.260 --> 00:08:33.260
Necessitem connectar-nos tots junts.

00:08:33.260 --> 00:08:36.260
Necessitem que ens presente noves idees

00:08:36.260 --> 00:08:39.260
i nova gent i perspectives diferents.

00:08:40.260 --> 00:08:42.260
I açò no passarà

00:08:42.260 --> 00:08:45.260
si ens deixa sols en una web d'un.

00:08:45.260 --> 00:08:47.260
Gràcies

00:08:47.260 --> 00:08:58.260
(Aplaudiments)

