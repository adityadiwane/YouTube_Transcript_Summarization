WEBVTT
Kind: captions
Language: cs

00:00:00.000 --> 00:00:07.000
Překladatel: Jiřina Vítů
Korektor: Martin Francis Gilbert Máik

00:00:15.260 --> 00:00:17.260
Mark Zuckeberg, vynálezce Facebooku,

00:00:17.260 --> 00:00:20.260
mluvil s jedním novinářem o "výpisu novinek".

00:00:20.260 --> 00:00:22.260
Ten novinář se ho ptal,

00:00:22.260 --> 00:00:24.260
"Proč je tak důležitý?"

00:00:24.260 --> 00:00:26.260
A Zuckerberg řekl,

00:00:26.260 --> 00:00:28.260
"Veverka umírající před vaším domem

00:00:28.260 --> 00:00:31.260
pro vás může být právě teď mnohem důležitější,

00:00:31.260 --> 00:00:34.260
než lidé umírající v Africe."

00:00:34.260 --> 00:00:36.260
A já teď chci mluvit o tom,

00:00:36.260 --> 00:00:39.260
jak může vypadat Web založený na tom, co je důležité pro jeho uživatele.

00:00:40.260 --> 00:00:42.260
Když jsem vyrůstal,

00:00:42.260 --> 00:00:44.260
na venkově v Maine,

00:00:44.260 --> 00:00:47.260
internet pro mě znamenal něco úplně jiného než dnes.

00:00:47.260 --> 00:00:49.260
Znamenal spojení se světem.

00:00:49.260 --> 00:00:52.260
Bylo to něco, co propojí nás všechny.

00:00:52.260 --> 00:00:55.260
A já si byl jistý, že bude výborný pro demokracii

00:00:55.260 --> 00:00:58.260
a pro naši společnost.

00:00:58.260 --> 00:01:00.260
Ale objevil se značný posun v tom

00:01:00.260 --> 00:01:02.260
jak informace po internetu proudí.

00:01:02.260 --> 00:01:05.260
A ten posun je neviditelný.

00:01:05.260 --> 00:01:07.260
A pokud mu nebudeme věnovat pozornost,

00:01:07.260 --> 00:01:10.260
může to být opravdu problém.

00:01:10.260 --> 00:01:13.260
Poprvé jsem si toho všiml tam, kde trávím spoustu času --

00:01:13.260 --> 00:01:15.260
na Facebooku.

00:01:15.260 --> 00:01:18.260
Jsem progresivista, politicky -- velké překvapení --

00:01:18.260 --> 00:01:20.260
ale snažím se nepředpojatě bavit i s konzervativci.

00:01:20.260 --> 00:01:22.260
Rád si poslechnu, co si myslí,

00:01:22.260 --> 00:01:24.260
jaké odkazy posílají,

00:01:24.260 --> 00:01:26.260
rád se od nich pár věcí přiučím.

00:01:26.260 --> 00:01:29.260
Takže jsem byl překvapený, když jsem si všiml,

00:01:29.260 --> 00:01:32.260
že konzervativci najednou zmizeli z mé facebookové stránky.

00:01:33.260 --> 00:01:35.260
Zjistil jsem, že je to proto,

00:01:35.260 --> 00:01:39.260
že Facebook sleduje, na jaké odkazy klikám,

00:01:39.260 --> 00:01:41.260
a všiml si, že daleko častěji

00:01:41.260 --> 00:01:43.260
otevírám odkazy svých liberálních přátel,

00:01:43.260 --> 00:01:46.260
než svých konzervativních přátel.

00:01:46.260 --> 00:01:48.260
A aniž by se mě na to ptal,

00:01:48.260 --> 00:01:50.260
odstranil mi je ze zdi.

00:01:50.260 --> 00:01:53.260
Prostě zmizeli.

00:01:54.260 --> 00:01:56.260
A Facebook není jediný,

00:01:56.260 --> 00:01:58.260
kdo dělá tyhle neviditelné, algoritmické

00:01:58.260 --> 00:02:01.260
úpravy Webu.

00:02:01.260 --> 00:02:03.260
Google to dělá taky.

00:02:03.260 --> 00:02:06.260
Pokud budu něco vyhledávat já a vy,

00:02:06.260 --> 00:02:08.260
i zrovna teď, ve stejnou chvíli,

00:02:08.260 --> 00:02:11.260
možná dostaneme naprosto rozdílné výsledky.

00:02:11.260 --> 00:02:14.260
Jeden inženýr mi řekl, že i když jste odhlášení,

00:02:14.260 --> 00:02:16.260
vysíláte 57 signálů,

00:02:16.260 --> 00:02:19.260
na které se Google dívá --

00:02:19.260 --> 00:02:22.260
k personalizaci vašeho vyhledávání

00:02:22.260 --> 00:02:24.260
používá vše od značky vašeho počítače,

00:02:24.260 --> 00:02:26.260
prohlížeče, který používáte,

00:02:26.260 --> 00:02:29.260
po místo, kde se zrovna nacházíte.

00:02:29.260 --> 00:02:31.260
Zamyslete se nad tím na chvíli:

00:02:31.260 --> 00:02:35.260
už neexistuje žádný standartní Google.

00:02:35.260 --> 00:02:38.260
Víte, nejlegračnější na tom je, že to nepoznáte.

00:02:38.260 --> 00:02:40.260
Nevidíte, jak se liší vaše vyhledávání,

00:02:40.260 --> 00:02:42.260
od výsledků někoho jiného.

00:02:42.260 --> 00:02:44.260
Ale před pár týdny

00:02:44.260 --> 00:02:47.260
jsem poprosil pár přátel, aby vygooglili "Egypt"

00:02:47.260 --> 00:02:50.260
a poslali mi snímek své obrazovky.

00:02:50.260 --> 00:02:53.260
Tady je obrazovka mého přítele Scotta,

00:02:54.260 --> 00:02:57.260
a tady Daniela.

00:02:57.260 --> 00:02:59.260
Když je dáte vedle sebe,

00:02:59.260 --> 00:03:01.260
nemusíte ani číst odkazy,

00:03:01.260 --> 00:03:03.260
abyste si všimli, jak rozdílné ty stránky jsou.

00:03:03.260 --> 00:03:05.260
A když si ty odkazy přečtete,

00:03:05.260 --> 00:03:08.260
uvidíte opravdu propastný rozdíl.

00:03:09.260 --> 00:03:12.260
Daniel neměl na celé první stránce

00:03:12.260 --> 00:03:14.260
ani slovo o protestech v Egyptě.

00:03:14.260 --> 00:03:16.260
Scott jich tam má spoustu.

00:03:16.260 --> 00:03:18.260
V těch dnech toho zrovna byly plné zprávy.

00:03:18.260 --> 00:03:21.260
Tady vidíte jak rozdílné ty výsledky jsou.

00:03:21.260 --> 00:03:24.260
A není to jen Google a Facebook.

00:03:24.260 --> 00:03:26.260
Šíří se to po celém internetu.

00:03:26.260 --> 00:03:29.260
Velké množství společností teď zavádí personalizaci.

00:03:29.260 --> 00:03:32.260
Yahoo News, největší internetové zpravodajství,

00:03:32.260 --> 00:03:35.260
je teď personalizované -- různí lidé vidí různé věci.

00:03:36.260 --> 00:03:39.260
Huffington Post, Washington Post, New York Times --

00:03:39.260 --> 00:03:42.260
všichni se snaží různě se personalizovat.

00:03:42.260 --> 00:03:45.260
A to nás velmi rychle posunuje

00:03:45.260 --> 00:03:47.260
do světa, ve kterém nám

00:03:47.260 --> 00:03:51.260
internet bude dávat to, co chceme vidět,

00:03:51.260 --> 00:03:54.260
ale ne nutně to, co vidět potřebujeme.

00:03:54.260 --> 00:03:57.260
Jak řekl Eric Schmidt,

00:03:57.260 --> 00:04:00.260
"Pro lidi bude velmi těžké sledovat a konzumovat něco,

00:04:00.260 --> 00:04:02.260
co pro ně v určitém ohledu

00:04:02.260 --> 00:04:05.260
nebylo vytvořeno."

00:04:05.260 --> 00:04:07.260
A já si myslím, že tohle je problém.

00:04:07.260 --> 00:04:10.260
Myslím si, že když spojíte všechny tyhle filtry dohromady,

00:04:10.260 --> 00:04:12.260
vezmete všechny ty algoritmy,

00:04:12.260 --> 00:04:15.260
dostanete informační bublinu.

00:04:16.260 --> 00:04:19.260
A vaše informační bublina je váš osobní,

00:04:19.260 --> 00:04:21.260
jedinečný informační vesmír,

00:04:21.260 --> 00:04:23.260
ve kterém žijete když jste online.

00:04:23.260 --> 00:04:26.260
Co je ve vaší informační bublině,

00:04:26.260 --> 00:04:29.260
záleží na tom, kdo jste a co děláte.

00:04:29.260 --> 00:04:33.260
A vy vůbec nerozhodujete o tom, co se dostane dovnitř.

00:04:33.260 --> 00:04:35.260
A ještě důležitější je,

00:04:35.260 --> 00:04:38.260
že nevidíte, co se dovnitř nedostane.

00:04:38.260 --> 00:04:40.260
Jeden z problémů informačních bublin objevili pracovníci Netflixu,

00:04:40.260 --> 00:04:43.260
amerického provozovatele online televize a půjčovny DVD.

00:04:43.260 --> 00:04:46.260
Dívali se na to, o jaké filmy si zákazníci žádají, a všimli si něčeho zvláštního,

00:04:46.260 --> 00:04:48.260
čeho si možná všimla i spousta dalších lidí.

00:04:48.260 --> 00:04:50.260
Že některé filmy si lidé dávají do fronty

00:04:50.260 --> 00:04:53.260
vždy na začátek - aby je dostali nejdřív.

00:04:53.260 --> 00:04:56.260
(Vždy je možné půjčit si jen jeden film naráz)

00:04:56.260 --> 00:04:58.260
Filmy jako "Iron Man" bývají na začátku,

00:04:58.260 --> 00:05:00.260
a "Čekání na Supermana"

00:05:00.260 --> 00:05:02.260
někde na konci.

00:05:02.260 --> 00:05:04.260
Objevili,

00:05:04.260 --> 00:05:06.260
že v Netflixových frontách

00:05:06.260 --> 00:05:09.260
probíhá obrovský boj

00:05:09.260 --> 00:05:12.260
mezi naším cílevědomým já hledícím do budoucnosti

00:05:12.260 --> 00:05:15.260
a naším impulzivním současným já.

00:05:15.260 --> 00:05:17.260
Víte, všichni chceme

00:05:17.260 --> 00:05:19.260
jednou vidět film Rašomón,

00:05:19.260 --> 00:05:21.260
ale zrovna teď

00:05:21.260 --> 00:05:24.260
se jdeme počtvrté koukat na Ace Venturu.

00:05:24.260 --> 00:05:27.260
(Smích)

00:05:27.260 --> 00:05:29.260
Ten nejlepší filtr by nám měl dát trochu od všeho.

00:05:29.260 --> 00:05:31.260
Trochu Justina Biebera

00:05:31.260 --> 00:05:33.260
a trochu Afghánistánu.

00:05:33.260 --> 00:05:35.260
Nějaké výživné informace

00:05:35.260 --> 00:05:38.260
a nějaký informační zákusek.

00:05:38.260 --> 00:05:40.260
A největší záludnost těchto algoritmických,

00:05:40.260 --> 00:05:42.260
personalizovaných filtrů je právě to,

00:05:42.260 --> 00:05:44.260
že sledují hlavně,

00:05:44.260 --> 00:05:48.260
na co klikáme jako první,

00:05:48.260 --> 00:05:52.260
což může narušit tu požadovanou rovnováhu.

00:05:52.260 --> 00:05:55.260
A místo vyvážené informační stravy

00:05:55.260 --> 00:05:57.260
dostaneme jen

00:05:57.260 --> 00:05:59.260
informační menu z MacDonalda.

00:05:59.260 --> 00:06:01.260
Což naznačuje, že jsme se možná

00:06:01.260 --> 00:06:04.260
s našimi představami o internetu pěkně spletli.

00:06:04.260 --> 00:06:06.260
Ve vysílacích společnostech --

00:06:06.260 --> 00:06:08.260
tak to říká mytologie vysílání --

00:06:08.260 --> 00:06:10.260
tak v těch vysílacích společnostech

00:06:10.260 --> 00:06:12.260
byli správci, editoři,

00:06:12.260 --> 00:06:15.260
kteří kontrolovali proudění informací.

00:06:15.260 --> 00:06:18.260
A pak přišel internet a smetl je z cesty,

00:06:18.260 --> 00:06:20.260
umožnil všem navzájem se propojit,

00:06:20.260 --> 00:06:22.260
a bylo to skvělé.

00:06:22.260 --> 00:06:25.260
Ale to už se teď neděje.

00:06:26.260 --> 00:06:29.260
Teď můžeme sledovat, jak živí strážci

00:06:29.260 --> 00:06:31.260
předávají štafetu

00:06:31.260 --> 00:06:34.260
algoritmickým strážcům.

00:06:34.260 --> 00:06:37.260
A problém je v tom, že algoritmy

00:06:37.260 --> 00:06:40.260
nemají zabudovanou etiku

00:06:40.260 --> 00:06:43.260
těch bývalých živých strážců.

00:06:43.260 --> 00:06:46.260
Takže pokud za nás mají svět hlídat algoritmy,

00:06:46.260 --> 00:06:49.260
pokud to budou ony, kdo rozhoduje, co vidíme a co nevidíme,

00:06:49.260 --> 00:06:51.260
pak si musíme dát pozor,

00:06:51.260 --> 00:06:54.260
aby se neřídily automaticky jen podle našeho výběru.

00:06:54.260 --> 00:06:56.260
Musíme zajistit, aby nám také nabízely věci,

00:06:56.260 --> 00:06:59.260
které jsou nepříjemné, podnětné a důležité,

00:06:59.260 --> 00:07:01.260
aby nám ukazovaly jiné úhly pohledu

00:07:01.260 --> 00:07:03.260
-- jako to dělá třeba TED.

00:07:03.260 --> 00:07:05.260
V podobné situaci

00:07:05.260 --> 00:07:07.260
už společnost jednou byla.

00:07:08.260 --> 00:07:11.260
V roce 1915 se noviny příliš nestaraly

00:07:11.260 --> 00:07:14.260
o nějakou občanskou zodpovědnost.

00:07:14.260 --> 00:07:16.260
A pak si lidé všimli,

00:07:16.260 --> 00:07:19.260
že ony mají velmi důležitý úkol.

00:07:19.260 --> 00:07:21.260
Že nemůžete mít

00:07:21.260 --> 00:07:23.260
fungující demokracii,

00:07:23.260 --> 00:07:27.260
pokud občané nemají kvalitní přísun informací.

00:07:28.260 --> 00:07:31.260
Noviny byly kritické, protože fungovaly jako filtr,

00:07:31.260 --> 00:07:33.260
a tak se vyvinula novinářská etika.

00:07:33.260 --> 00:07:35.260
Nebyla dokonalá,

00:07:35.260 --> 00:07:38.260
ale pomohla nám přežít minulé století.

00:07:38.260 --> 00:07:40.260
A teď jsme s internetem

00:07:40.260 --> 00:07:43.260
zpátky v situaci z roku 1915.

00:07:44.260 --> 00:07:47.260
A potřebujeme, aby noví strážci

00:07:47.260 --> 00:07:49.260
zabudovali ten smysl zodpovědnosti

00:07:49.260 --> 00:07:51.260
do kódu, který vytvářejí.

00:07:51.260 --> 00:07:54.260
Vím, že je tu přítomno mnoho lidí z Facebooku i z Googlu --

00:07:54.260 --> 00:07:56.260
Larry a Sergey --

00:07:56.260 --> 00:07:58.260
lidé, kteří pomohli vybudovat Web do dnešní podoby,

00:07:58.260 --> 00:08:00.260
a já jsem jim za to vděčný.

00:08:00.260 --> 00:08:03.260
Ale musíme si teď dát velký pozor,

00:08:03.260 --> 00:08:06.260
aby algoritmy měly v sobě zabudovaný

00:08:06.260 --> 00:08:09.260
smysl pro veřejný život, pro občanskou zodpovědnost.

00:08:09.260 --> 00:08:12.260
Musíme se ujistit, že jsou dostatečně průhledné,

00:08:12.260 --> 00:08:14.260
že známe pravidla,

00:08:14.260 --> 00:08:17.260
která rozhodují o tom, co projde našimi filtry.

00:08:17.260 --> 00:08:19.260
A my potřebujeme, abyste nám vy dali kontrolu,

00:08:19.260 --> 00:08:21.260
abychom mohli rozhodovat,

00:08:21.260 --> 00:08:24.260
co se k nám dostane a co ne.

00:08:24.260 --> 00:08:26.260
Protože jsem předvědčený,

00:08:26.260 --> 00:08:28.260
že internet opravdu potřebujeme takový,

00:08:28.260 --> 00:08:30.260
jaký jsme si ho kdysi vysnili.

00:08:30.260 --> 00:08:33.260
Potřebujeme, aby nás všechny spojoval.

00:08:33.260 --> 00:08:36.260
Potřebujeme, aby přinášel nové nápady,

00:08:36.260 --> 00:08:39.260
nové lidi a nové úhly pohledu.

00:08:40.260 --> 00:08:42.260
A to se nestane,

00:08:42.260 --> 00:08:45.260
pokud budeme izolováni v naší vlastní "síti".

00:08:45.260 --> 00:08:47.260
Děkuji.

00:08:47.260 --> 00:08:58.260
(Potlesk)

