WEBVTT
Kind: captions
Language: da

00:00:00.000 --> 00:00:07.000
Translator: David J. Kreps Finnemann
Reviewer: Anders Finn Jørgensen

00:00:15.260 --> 00:00:17.260
Mark Zuckerberg,

00:00:17.260 --> 00:00:20.260
en journalist stillede ham spørgsmål om nyheds feeds.

00:00:20.260 --> 00:00:22.260
Og journalisten spurgte ham,

00:00:22.260 --> 00:00:24.260
"Hvorfor er det her så vigtigt?"

00:00:24.260 --> 00:00:26.260
Og Zuckerberg svarede,

00:00:26.260 --> 00:00:28.260
"Et egern der dør i din have,

00:00:28.260 --> 00:00:31.260
er nok mere relevant for dine interesser lige nu,

00:00:31.260 --> 00:00:34.260
end folk der dør i Afrika".

00:00:34.260 --> 00:00:36.260
Og jeg vil tale om

00:00:36.260 --> 00:00:39.260
hvordan et internet baseret på den ide om relevans kunne se ud.

00:00:40.260 --> 00:00:42.260
Så da jeg voksede op

00:00:42.260 --> 00:00:44.260
I et virkelig landligt område i Maine,

00:00:44.260 --> 00:00:47.260
betød internettet noget helt andet for mig.

00:00:47.260 --> 00:00:49.260
Det betød en forbindelse til verden.

00:00:49.260 --> 00:00:52.260
Det betød, at noget forbandt os alle sammen.

00:00:52.260 --> 00:00:55.260
Og jeg var sikker på, at det ville være rigtig godt for demokratiet

00:00:55.260 --> 00:00:58.260
og for vores samfund.

00:00:58.260 --> 00:01:00.260
Men der er et skift på vej

00:01:00.260 --> 00:01:02.260
i hvordan information er tilgængelig online,

00:01:02.260 --> 00:01:05.260
og det er usynligt.

00:01:05.260 --> 00:01:07.260
Og hvis vi ikke er opmærksomme på det,

00:01:07.260 --> 00:01:10.260
kunne det blive et reelt problem.

00:01:10.260 --> 00:01:13.260
Så jeg lagde først mærke til det her, et sted hvor jeg bruger meget tid --

00:01:13.260 --> 00:01:15.260
min Facebook side.

00:01:15.260 --> 00:01:18.260
Jeg er progresiv, politisk set -- stor overraskelse --

00:01:18.260 --> 00:01:20.260
men jeg har altid sørget for at interessere mig for at møde konservative.

00:01:20.260 --> 00:01:22.260
Jeg kan godt lide at høre hvad de tænker på;

00:01:22.260 --> 00:01:24.260
jeg kan godt lide at se hvad de linker til;

00:01:24.260 --> 00:01:26.260
jeg kan godt lide at lære en ting eller to.

00:01:26.260 --> 00:01:29.260
Så jeg blev overrasket en dag, da jeg lagde mærke til,

00:01:29.260 --> 00:01:32.260
at de konservative var forsvundet fra mit Facebook feed.

00:01:33.260 --> 00:01:35.260
Og det der viste sig at ske,

00:01:35.260 --> 00:01:39.260
var at Facebook kiggede på hvilke link jeg klikkede på,

00:01:39.260 --> 00:01:41.260
og det lagde mærke til, faktisk,

00:01:41.260 --> 00:01:43.260
at jeg klikkede mere på mine liberale venners links

00:01:43.260 --> 00:01:46.260
end på mine konservative venners links.

00:01:46.260 --> 00:01:48.260
Og uden at spørge mig om det,

00:01:48.260 --> 00:01:50.260
havde den fjernet dem.

00:01:50.260 --> 00:01:53.260
De forsvandt.

00:01:54.260 --> 00:01:56.260
Men Facebook er ikke det eneste sted

00:01:56.260 --> 00:01:58.260
der udfører denne slags usynlig, algoritmisk

00:01:58.260 --> 00:02:01.260
redigering af nettet.

00:02:01.260 --> 00:02:03.260
Google gør det også.

00:02:03.260 --> 00:02:06.260
Hvis jeg søger efter noget, og du søger efter noget,

00:02:06.260 --> 00:02:08.260
selv lige nu, på præcis det samme tidspunkt,

00:02:08.260 --> 00:02:11.260
får vi måske vidt forskellige resultater.

00:02:11.260 --> 00:02:14.260
Selv hvis man er logget ud, fortalte en ingeniør mig,

00:02:14.260 --> 00:02:16.260
er der 57 signaler

00:02:16.260 --> 00:02:19.260
som Google kigger efter --

00:02:19.260 --> 00:02:22.260
alt fra hvilken slags computer man sidder ved,

00:02:22.260 --> 00:02:24.260
til hvilken browser man benytter,

00:02:24.260 --> 00:02:26.260
til hvor man sidder --

00:02:26.260 --> 00:02:29.260
som den bruger til at skræddersy ens søgeresultater.

00:02:29.260 --> 00:02:31.260
Tænk lige på det et øjeblik:

00:02:31.260 --> 00:02:35.260
Der er ikke nogen standard Google mere.

00:02:35.260 --> 00:02:38.260
Og I ved, det sjove ved det er, at det er virkelig svært at se.

00:02:38.260 --> 00:02:40.260
Man kan ikke se hvor forskellige ens søgeresultater er

00:02:40.260 --> 00:02:42.260
fra nogle andres.

00:02:42.260 --> 00:02:44.260
Men for et par uger siden,

00:02:44.260 --> 00:02:47.260
bad jeg en flok venner at Google "Ægypten"

00:02:47.260 --> 00:02:50.260
og at sende mig screenshots af resultatet.

00:02:50.260 --> 00:02:53.260
Så her er min ven Scotts screenshot.

00:02:54.260 --> 00:02:57.260
Og her er min ven Daniels screenshot.

00:02:57.260 --> 00:02:59.260
Når man stiller dem op ved siden af hinanden,

00:02:59.260 --> 00:03:01.260
behøver man ikke engang at læse linksene,

00:03:01.260 --> 00:03:03.260
for at se hvor forskellige de to sider er.

00:03:03.260 --> 00:03:05.260
Men hvis man læser linksene,

00:03:05.260 --> 00:03:08.260
er det ret opsigtsvækkende.

00:03:09.260 --> 00:03:12.260
Daniel fik ikke noget om protesterne i Ægypten overhovedet,

00:03:12.260 --> 00:03:14.260
på den første side af Google resultater.

00:03:14.260 --> 00:03:16.260
Scotts resultater var fyldt med dem.

00:03:16.260 --> 00:03:18.260
Og det her var dagens store historie på det tidspunkt.

00:03:18.260 --> 00:03:21.260
Så forskellige begynder resultaterne at blive.

00:03:21.260 --> 00:03:24.260
Men det er heller ikke kun Google og Facebook.

00:03:24.260 --> 00:03:26.260
Det er noget der indtager nettet.

00:03:26.260 --> 00:03:29.260
Der er en hel række af firmaer der laver denne slags personliggørelse.

00:03:29.260 --> 00:03:32.260
Yahoo News, det største nyhedssite på internettet,

00:03:32.260 --> 00:03:35.260
er nu personliggjort -- forskellige mennesker får forskellige ting.

00:03:36.260 --> 00:03:39.260
Huffington Post, Washington Post, New York Times --

00:03:39.260 --> 00:03:42.260
er alle i gang med at personliggøre på forskellige måder.

00:03:42.260 --> 00:03:45.260
Og det flytter os rigtig hurtigt

00:03:45.260 --> 00:03:47.260
imod en verden hvor

00:03:47.260 --> 00:03:51.260
internettet viser os hvad det tror vi vil se,

00:03:51.260 --> 00:03:54.260
men ikke nødvendigvis hvad vi har brug for at se.

00:03:54.260 --> 00:03:57.260
Som Eric Schmidt sagde,

00:03:57.260 --> 00:04:00.260
"Det vil blive rigtig svært for mennesker at se eller forbruge noget

00:04:00.260 --> 00:04:02.260
der ikke på den ene eller anden måde

00:04:02.260 --> 00:04:05.260
er blevet skræddersyet til dem".

00:04:05.260 --> 00:04:07.260
Så jeg mener det her er et problem.

00:04:07.260 --> 00:04:10.260
Og jeg mener, at hvis man kigger på alle disse filtre,

00:04:10.260 --> 00:04:12.260
hvis man kigger på alle disse algoritmer,

00:04:12.260 --> 00:04:15.260
får man det jeg kalder en filterboble.

00:04:16.260 --> 00:04:19.260
Og ens filterboble er ens egen, personlige,

00:04:19.260 --> 00:04:21.260
unikke univers med information

00:04:21.260 --> 00:04:23.260
som man lever i online.

00:04:23.260 --> 00:04:26.260
Og hvad der er i ens filterboble,

00:04:26.260 --> 00:04:29.260
afhænger af hvem man er, og det afhænger af hvad man gør.

00:04:29.260 --> 00:04:33.260
Men det der er ved det er, at man ikke kan bestemme hvad der er i den.

00:04:33.260 --> 00:04:35.260
Og endnu vigtigere,

00:04:35.260 --> 00:04:38.260
man kan ikke se hvad der bliver redigeret væk.

00:04:38.260 --> 00:04:40.260
Så en af problemerne med filterboblen

00:04:40.260 --> 00:04:43.260
blev opdaget af nogle forskere ved Netflix.

00:04:43.260 --> 00:04:46.260
Og de kiggede på køerne ved Netflix, og de lagde mærke til noget ret spøjst,

00:04:46.260 --> 00:04:48.260
noget som mange af os sikkert har lagt mærke til,

00:04:48.260 --> 00:04:50.260
hvilket er at der er nogle film

00:04:50.260 --> 00:04:53.260
der egentlig bare suser afsted til vores hjem.

00:04:53.260 --> 00:04:56.260
De kommer i køen, og de suser bare afsted.

00:04:56.260 --> 00:04:58.260
Så "Iron Man" suser afsted,

00:04:58.260 --> 00:05:00.260
og "Waiting for Superman"

00:05:00.260 --> 00:05:02.260
kan vente i rigtig lang tid.

00:05:02.260 --> 00:05:04.260
Det de fandt ud af var

00:05:04.260 --> 00:05:06.260
at i vores Netflix kø

00:05:06.260 --> 00:05:09.260
foregår der en episk kamp mellem

00:05:09.260 --> 00:05:12.260
vores fremtidige, ideelle selv

00:05:12.260 --> 00:05:15.260
og vores mere impulsive nutidige selv.

00:05:15.260 --> 00:05:17.260
I ved, vi vil alle gerne være en person

00:05:17.260 --> 00:05:19.260
der har set "Rashomon",

00:05:19.260 --> 00:05:21.260
men lige nu

00:05:21.260 --> 00:05:24.260
vil vi gerne se "Ace Venture" en fjerde gang.

00:05:24.260 --> 00:05:27.260
(Latter)

00:05:27.260 --> 00:05:29.260
Så den bedste redigering, giver os en smule af hvert.

00:05:29.260 --> 00:05:31.260
Det giver os en smule Justin Bieber,

00:05:31.260 --> 00:05:33.260
og en smule Afghanistan.

00:05:33.260 --> 00:05:35.260
Det giver os nogle informationsgrøntsager;

00:05:35.260 --> 00:05:38.260
det giver os noget informationsdessert.

00:05:38.260 --> 00:05:40.260
Og udfordringen med denne slags algoritmiske filtre,

00:05:40.260 --> 00:05:42.260
disse personaliserede filtre,

00:05:42.260 --> 00:05:44.260
er at, fordi de hovedsageligt kigger

00:05:44.260 --> 00:05:48.260
på hvad man klikker på først,

00:05:48.260 --> 00:05:52.260
kan det forstyrre balancen.

00:05:52.260 --> 00:05:55.260
Og i stedet for en balanceret informationskost,

00:05:55.260 --> 00:05:57.260
kan det ende med at man er omgivet

00:05:57.260 --> 00:05:59.260
af informations junk food.

00:05:59.260 --> 00:06:01.260
Hvad dette indeholder

00:06:01.260 --> 00:06:04.260
er egentlig, at vi måske har misforstået historien om internettet.

00:06:04.260 --> 00:06:06.260
I et online samfund --

00:06:06.260 --> 00:06:08.260
sådan her er skabelses mytologien --

00:06:08.260 --> 00:06:10.260
i et online samfund,

00:06:10.260 --> 00:06:12.260
var der de her kontrollører, redaktørerne,

00:06:12.260 --> 00:06:15.260
og de kontrollerede informationsstrømmen.

00:06:15.260 --> 00:06:18.260
Og pludselig kom internettet og fjernede dem,

00:06:18.260 --> 00:06:20.260
og det tillod os alle at blive forbundet med hinanden,

00:06:20.260 --> 00:06:22.260
og det var fedt.

00:06:22.260 --> 00:06:25.260
Men det er faktisk ikke det der foregår lige nu.

00:06:26.260 --> 00:06:29.260
Det vi ser nu, er mere end slags videregiven

00:06:29.260 --> 00:06:31.260
fra de menneskelige kontrollører

00:06:31.260 --> 00:06:34.260
til de algoritmiske.

00:06:34.260 --> 00:06:37.260
Og det der er med de algoritmiske er,

00:06:37.260 --> 00:06:40.260
at de har endnu ikke den slags medfødte etik

00:06:40.260 --> 00:06:43.260
som redaktørerne havde.

00:06:43.260 --> 00:06:46.260
Så hvis algoritmerne vil organisere verden for os,

00:06:46.260 --> 00:06:49.260
hvis de skal beslutte hvad vi kommer til at se, og hvad vi ikke kommer til at se

00:06:49.260 --> 00:06:51.260
så må vi være sikre på

00:06:51.260 --> 00:06:54.260
at de ikke udelukkende kigger på relevans.

00:06:54.260 --> 00:06:56.260
Vi må sørge for, at de også viser os ting

00:06:56.260 --> 00:06:59.260
der er ubehagelige eller udfordrende, eller vigtige --

00:06:59.260 --> 00:07:01.260
det er hvad TED gør --

00:07:01.260 --> 00:07:03.260
andre synspunkter.

00:07:03.260 --> 00:07:05.260
Og det der er ved det er, at vi faktisk har været her før

00:07:05.260 --> 00:07:07.260
som samfund.

00:07:08.260 --> 00:07:11.260
I 1915, var det ikke fordi aviserne svedte

00:07:11.260 --> 00:07:14.260
over deres borgerlige pligter.

00:07:14.260 --> 00:07:16.260
Så lagde folk mærke til

00:07:16.260 --> 00:07:19.260
at de gjorde noget rigtig vigtigt.

00:07:19.260 --> 00:07:21.260
At man, faktisk, ikke kunne have

00:07:21.260 --> 00:07:23.260
et velfungerende demokrati

00:07:23.260 --> 00:07:27.260
hvis borgerne ikke fik en god informationsstrøm,

00:07:28.260 --> 00:07:31.260
at aviserne var afgørende, fordi de opførte sig som filteret,

00:07:31.260 --> 00:07:33.260
og så udviklede journalisternes etik sig.

00:07:33.260 --> 00:07:35.260
Det var ikke perfekt,

00:07:35.260 --> 00:07:38.260
men det første os gennem det sidste århundrede.

00:07:38.260 --> 00:07:40.260
Så nu,

00:07:40.260 --> 00:07:43.260
er vi på en måde tilbage i 1915 med nettet.

00:07:44.260 --> 00:07:47.260
Og vi har brug for at de nye kontrollører

00:07:47.260 --> 00:07:49.260
koder den form for ansvar ind

00:07:49.260 --> 00:07:51.260
i den kode de skriver.

00:07:51.260 --> 00:07:54.260
Jeg ved at der er mange mennesker fra Facebook og Google til stede --

00:07:54.260 --> 00:07:56.260
Larry og Sergey --

00:07:56.260 --> 00:07:58.260
mennesker der har hjulpet os med at bygge nettet som det er,

00:07:58.260 --> 00:08:00.260
og det er jeg taknemmelig for.

00:08:00.260 --> 00:08:03.260
Men vi skal virkelig være sikre på

00:08:03.260 --> 00:08:06.260
at disse algoritmer har indkodet

00:08:06.260 --> 00:08:09.260
en sans for det offentlige liv, en sans for de borgerlige pligter.

00:08:09.260 --> 00:08:12.260
Vi må være sikre på, at de er gennemsigtige nok

00:08:12.260 --> 00:08:14.260
til at vi kan se hvad reglerne er

00:08:14.260 --> 00:08:17.260
der afgører hvad der kommer gennem vores filtre.

00:08:17.260 --> 00:08:19.260
Og der er brug for at I giver os den samme kontrol

00:08:19.260 --> 00:08:21.260
så vi kan afgøre

00:08:21.260 --> 00:08:24.260
hvad der kommer igennem og hvad der ikke gør.

00:08:24.260 --> 00:08:26.260
Fordi jeg mener

00:08:26.260 --> 00:08:28.260
at vi virkelig har brug for at internettet er den ting

00:08:28.260 --> 00:08:30.260
som vi alle har brug for det er.

00:08:30.260 --> 00:08:33.260
Vi har brug for, at det forbinder os alle sammen.

00:08:33.260 --> 00:08:36.260
Vi har brug for at det introducerer os for nye ideer

00:08:36.260 --> 00:08:39.260
og nye mennesker og forskellige synsvinkler.

00:08:40.260 --> 00:08:42.260
Og det kommer det ikke til at gøre

00:08:42.260 --> 00:08:45.260
hvis det efterlader os alle sammen i et net bestående af én.

00:08:45.260 --> 00:08:47.260
Tak.

00:08:47.260 --> 00:08:58.260
(Bifald)

