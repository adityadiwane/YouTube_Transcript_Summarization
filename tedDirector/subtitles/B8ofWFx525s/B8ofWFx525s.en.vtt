WEBVTT
Kind: captions
Language: en

00:00:15.260 --> 00:00:17.260
Mark Zuckerberg,

00:00:17.260 --> 00:00:20.260
a journalist was asking him a question about the news feed.

00:00:20.260 --> 00:00:22.260
And the journalist was asking him,

00:00:22.260 --> 00:00:24.260
"Why is this so important?"

00:00:24.260 --> 00:00:26.260
And Zuckerberg said,

00:00:26.260 --> 00:00:28.260
"A squirrel dying in your front yard

00:00:28.260 --> 00:00:31.260
may be more relevant to your interests right now

00:00:31.260 --> 00:00:34.260
than people dying in Africa."

00:00:34.260 --> 00:00:36.260
And I want to talk about

00:00:36.260 --> 00:00:39.260
what a Web based on that idea of relevance might look like.

00:00:40.260 --> 00:00:42.260
So when I was growing up

00:00:42.260 --> 00:00:44.260
in a really rural area in Maine,

00:00:44.260 --> 00:00:47.260
the Internet meant something very different to me.

00:00:47.260 --> 00:00:49.260
It meant a connection to the world.

00:00:49.260 --> 00:00:52.260
It meant something that would connect us all together.

00:00:52.260 --> 00:00:55.260
And I was sure that it was going to be great for democracy

00:00:55.260 --> 00:00:58.260
and for our society.

00:00:58.260 --> 00:01:00.260
But there's this shift

00:01:00.260 --> 00:01:02.260
in how information is flowing online,

00:01:02.260 --> 00:01:05.260
and it's invisible.

00:01:05.260 --> 00:01:07.260
And if we don't pay attention to it,

00:01:07.260 --> 00:01:10.260
it could be a real problem.

00:01:10.260 --> 00:01:13.260
So I first noticed this in a place I spend a lot of time --

00:01:13.260 --> 00:01:15.260
my Facebook page.

00:01:15.260 --> 00:01:18.260
I'm progressive, politically -- big surprise --

00:01:18.260 --> 00:01:20.260
but I've always gone out of my way to meet conservatives.

00:01:20.260 --> 00:01:22.260
I like hearing what they're thinking about;

00:01:22.260 --> 00:01:24.260
I like seeing what they link to;

00:01:24.260 --> 00:01:26.260
I like learning a thing or two.

00:01:26.260 --> 00:01:29.260
And so I was surprised when I noticed one day

00:01:29.260 --> 00:01:32.260
that the conservatives had disappeared from my Facebook feed.

00:01:33.260 --> 00:01:35.260
And what it turned out was going on

00:01:35.260 --> 00:01:39.260
was that Facebook was looking at which links I clicked on,

00:01:39.260 --> 00:01:41.260
and it was noticing that, actually,

00:01:41.260 --> 00:01:43.260
I was clicking more on my liberal friends' links

00:01:43.260 --> 00:01:46.260
than on my conservative friends' links.

00:01:46.260 --> 00:01:48.260
And without consulting me about it,

00:01:48.260 --> 00:01:50.260
it had edited them out.

00:01:50.260 --> 00:01:53.260
They disappeared.

00:01:54.260 --> 00:01:56.260
So Facebook isn't the only place

00:01:56.260 --> 00:01:58.260
that's doing this kind of invisible, algorithmic

00:01:58.260 --> 00:02:01.260
editing of the Web.

00:02:01.260 --> 00:02:03.260
Google's doing it too.

00:02:03.260 --> 00:02:06.260
If I search for something, and you search for something,

00:02:06.260 --> 00:02:08.260
even right now at the very same time,

00:02:08.260 --> 00:02:11.260
we may get very different search results.

00:02:11.260 --> 00:02:14.260
Even if you're logged out, one engineer told me,

00:02:14.260 --> 00:02:16.260
there are 57 signals

00:02:16.260 --> 00:02:19.260
that Google looks at --

00:02:19.260 --> 00:02:22.260
everything from what kind of computer you're on

00:02:22.260 --> 00:02:24.260
to what kind of browser you're using

00:02:24.260 --> 00:02:26.260
to where you're located --

00:02:26.260 --> 00:02:29.260
that it uses to personally tailor your query results.

00:02:29.260 --> 00:02:31.260
Think about it for a second:

00:02:31.260 --> 00:02:35.260
there is no standard Google anymore.

00:02:35.260 --> 00:02:38.260
And you know, the funny thing about this is that it's hard to see.

00:02:38.260 --> 00:02:40.260
You can't see how different your search results are

00:02:40.260 --> 00:02:42.260
from anyone else's.

00:02:42.260 --> 00:02:44.260
But a couple of weeks ago,

00:02:44.260 --> 00:02:47.260
I asked a bunch of friends to Google "Egypt"

00:02:47.260 --> 00:02:50.260
and to send me screen shots of what they got.

00:02:50.260 --> 00:02:53.260
So here's my friend Scott's screen shot.

00:02:54.260 --> 00:02:57.260
And here's my friend Daniel's screen shot.

00:02:57.260 --> 00:02:59.260
When you put them side-by-side,

00:02:59.260 --> 00:03:01.260
you don't even have to read the links

00:03:01.260 --> 00:03:03.260
to see how different these two pages are.

00:03:03.260 --> 00:03:05.260
But when you do read the links,

00:03:05.260 --> 00:03:08.260
it's really quite remarkable.

00:03:09.260 --> 00:03:12.260
Daniel didn't get anything about the protests in Egypt at all

00:03:12.260 --> 00:03:14.260
in his first page of Google results.

00:03:14.260 --> 00:03:16.260
Scott's results were full of them.

00:03:16.260 --> 00:03:18.260
And this was the big story of the day at that time.

00:03:18.260 --> 00:03:21.260
That's how different these results are becoming.

00:03:21.260 --> 00:03:24.260
So it's not just Google and Facebook either.

00:03:24.260 --> 00:03:26.260
This is something that's sweeping the Web.

00:03:26.260 --> 00:03:29.260
There are a whole host of companies that are doing this kind of personalization.

00:03:29.260 --> 00:03:32.260
Yahoo News, the biggest news site on the Internet,

00:03:32.260 --> 00:03:35.260
is now personalized -- different people get different things.

00:03:36.260 --> 00:03:39.260
Huffington Post, the Washington Post, the New York Times --

00:03:39.260 --> 00:03:42.260
all flirting with personalization in various ways.

00:03:42.260 --> 00:03:45.260
And this moves us very quickly

00:03:45.260 --> 00:03:47.260
toward a world in which

00:03:47.260 --> 00:03:51.260
the Internet is showing us what it thinks we want to see,

00:03:51.260 --> 00:03:54.260
but not necessarily what we need to see.

00:03:54.260 --> 00:03:57.260
As Eric Schmidt said,

00:03:57.260 --> 00:04:00.260
"It will be very hard for people to watch or consume something

00:04:00.260 --> 00:04:02.260
that has not in some sense

00:04:02.260 --> 00:04:05.260
been tailored for them."

00:04:05.260 --> 00:04:07.260
So I do think this is a problem.

00:04:07.260 --> 00:04:10.260
And I think, if you take all of these filters together,

00:04:10.260 --> 00:04:12.260
you take all these algorithms,

00:04:12.260 --> 00:04:15.260
you get what I call a filter bubble.

00:04:16.260 --> 00:04:19.260
And your filter bubble is your own personal,

00:04:19.260 --> 00:04:21.260
unique universe of information

00:04:21.260 --> 00:04:23.260
that you live in online.

00:04:23.260 --> 00:04:26.260
And what's in your filter bubble

00:04:26.260 --> 00:04:29.260
depends on who you are, and it depends on what you do.

00:04:29.260 --> 00:04:33.260
But the thing is that you don't decide what gets in.

00:04:33.260 --> 00:04:35.260
And more importantly,

00:04:35.260 --> 00:04:38.260
you don't actually see what gets edited out.

00:04:38.260 --> 00:04:40.260
So one of the problems with the filter bubble

00:04:40.260 --> 00:04:43.260
was discovered by some researchers at Netflix.

00:04:43.260 --> 00:04:46.260
And they were looking at the Netflix queues, and they noticed something kind of funny

00:04:46.260 --> 00:04:48.260
that a lot of us probably have noticed,

00:04:48.260 --> 00:04:50.260
which is there are some movies

00:04:50.260 --> 00:04:53.260
that just sort of zip right up and out to our houses.

00:04:53.260 --> 00:04:56.260
They enter the queue, they just zip right out.

00:04:56.260 --> 00:04:58.260
So "Iron Man" zips right out,

00:04:58.260 --> 00:05:00.260
and "Waiting for Superman"

00:05:00.260 --> 00:05:02.260
can wait for a really long time.

00:05:02.260 --> 00:05:04.260
What they discovered

00:05:04.260 --> 00:05:06.260
was that in our Netflix queues

00:05:06.260 --> 00:05:09.260
there's this epic struggle going on

00:05:09.260 --> 00:05:12.260
between our future aspirational selves

00:05:12.260 --> 00:05:15.260
and our more impulsive present selves.

00:05:15.260 --> 00:05:17.260
You know we all want to be someone

00:05:17.260 --> 00:05:19.260
who has watched "Rashomon,"

00:05:19.260 --> 00:05:21.260
but right now

00:05:21.260 --> 00:05:24.260
we want to watch "Ace Ventura" for the fourth time.

00:05:24.260 --> 00:05:27.260
(Laughter)

00:05:27.260 --> 00:05:29.260
So the best editing gives us a bit of both.

00:05:29.260 --> 00:05:31.260
It gives us a little bit of Justin Bieber

00:05:31.260 --> 00:05:33.260
and a little bit of Afghanistan.

00:05:33.260 --> 00:05:35.260
It gives us some information vegetables;

00:05:35.260 --> 00:05:38.260
it gives us some information dessert.

00:05:38.260 --> 00:05:40.260
And the challenge with these kinds of algorithmic filters,

00:05:40.260 --> 00:05:42.260
these personalized filters,

00:05:42.260 --> 00:05:44.260
is that, because they're mainly looking

00:05:44.260 --> 00:05:48.260
at what you click on first,

00:05:48.260 --> 00:05:52.260
it can throw off that balance.

00:05:52.260 --> 00:05:55.260
And instead of a balanced information diet,

00:05:55.260 --> 00:05:57.260
you can end up surrounded

00:05:57.260 --> 00:05:59.260
by information junk food.

00:05:59.260 --> 00:06:01.260
What this suggests

00:06:01.260 --> 00:06:04.260
is actually that we may have the story about the Internet wrong.

00:06:04.260 --> 00:06:06.260
In a broadcast society --

00:06:06.260 --> 00:06:08.260
this is how the founding mythology goes --

00:06:08.260 --> 00:06:10.260
in a broadcast society,

00:06:10.260 --> 00:06:12.260
there were these gatekeepers, the editors,

00:06:12.260 --> 00:06:15.260
and they controlled the flows of information.

00:06:15.260 --> 00:06:18.260
And along came the Internet and it swept them out of the way,

00:06:18.260 --> 00:06:20.260
and it allowed all of us to connect together,

00:06:20.260 --> 00:06:22.260
and it was awesome.

00:06:22.260 --> 00:06:25.260
But that's not actually what's happening right now.

00:06:26.260 --> 00:06:29.260
What we're seeing is more of a passing of the torch

00:06:29.260 --> 00:06:31.260
from human gatekeepers

00:06:31.260 --> 00:06:34.260
to algorithmic ones.

00:06:34.260 --> 00:06:37.260
And the thing is that the algorithms

00:06:37.260 --> 00:06:40.260
don't yet have the kind of embedded ethics

00:06:40.260 --> 00:06:43.260
that the editors did.

00:06:43.260 --> 00:06:46.260
So if algorithms are going to curate the world for us,

00:06:46.260 --> 00:06:49.260
if they're going to decide what we get to see and what we don't get to see,

00:06:49.260 --> 00:06:51.260
then we need to make sure

00:06:51.260 --> 00:06:54.260
that they're not just keyed to relevance.

00:06:54.260 --> 00:06:56.260
We need to make sure that they also show us things

00:06:56.260 --> 00:06:59.260
that are uncomfortable or challenging or important --

00:06:59.260 --> 00:07:01.260
this is what TED does --

00:07:01.260 --> 00:07:03.260
other points of view.

00:07:03.260 --> 00:07:05.260
And the thing is, we've actually been here before

00:07:05.260 --> 00:07:07.260
as a society.

00:07:08.260 --> 00:07:11.260
In 1915, it's not like newspapers were sweating a lot

00:07:11.260 --> 00:07:14.260
about their civic responsibilities.

00:07:14.260 --> 00:07:16.260
Then people noticed

00:07:16.260 --> 00:07:19.260
that they were doing something really important.

00:07:19.260 --> 00:07:21.260
That, in fact, you couldn't have

00:07:21.260 --> 00:07:23.260
a functioning democracy

00:07:23.260 --> 00:07:27.260
if citizens didn't get a good flow of information,

00:07:28.260 --> 00:07:31.260
that the newspapers were critical because they were acting as the filter,

00:07:31.260 --> 00:07:33.260
and then journalistic ethics developed.

00:07:33.260 --> 00:07:35.260
It wasn't perfect,

00:07:35.260 --> 00:07:38.260
but it got us through the last century.

00:07:38.260 --> 00:07:40.260
And so now,

00:07:40.260 --> 00:07:43.260
we're kind of back in 1915 on the Web.

00:07:44.260 --> 00:07:47.260
And we need the new gatekeepers

00:07:47.260 --> 00:07:49.260
to encode that kind of responsibility

00:07:49.260 --> 00:07:51.260
into the code that they're writing.

00:07:51.260 --> 00:07:54.260
I know that there are a lot of people here from Facebook and from Google --

00:07:54.260 --> 00:07:56.260
Larry and Sergey --

00:07:56.260 --> 00:07:58.260
people who have helped build the Web as it is,

00:07:58.260 --> 00:08:00.260
and I'm grateful for that.

00:08:00.260 --> 00:08:03.260
But we really need you to make sure

00:08:03.260 --> 00:08:06.260
that these algorithms have encoded in them

00:08:06.260 --> 00:08:09.260
a sense of the public life, a sense of civic responsibility.

00:08:09.260 --> 00:08:12.260
We need you to make sure that they're transparent enough

00:08:12.260 --> 00:08:14.260
that we can see what the rules are

00:08:14.260 --> 00:08:17.260
that determine what gets through our filters.

00:08:17.260 --> 00:08:19.260
And we need you to give us some control

00:08:19.260 --> 00:08:21.260
so that we can decide

00:08:21.260 --> 00:08:24.260
what gets through and what doesn't.

00:08:24.260 --> 00:08:26.260
Because I think

00:08:26.260 --> 00:08:28.260
we really need the Internet to be that thing

00:08:28.260 --> 00:08:30.260
that we all dreamed of it being.

00:08:30.260 --> 00:08:33.260
We need it to connect us all together.

00:08:33.260 --> 00:08:36.260
We need it to introduce us to new ideas

00:08:36.260 --> 00:08:39.260
and new people and different perspectives.

00:08:40.260 --> 00:08:42.260
And it's not going to do that

00:08:42.260 --> 00:08:45.260
if it leaves us all isolated in a Web of one.

00:08:45.260 --> 00:08:47.260
Thank you.

00:08:47.260 --> 00:08:58.260
(Applause)

