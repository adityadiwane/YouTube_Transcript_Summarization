WEBVTT
Kind: captions
Language: sv

00:00:00.000 --> 00:00:07.000
Översättare: Anders Björk
Granskare: Daniel Lundberg

00:00:15.260 --> 00:00:17.260
Mark Zuckerberg,

00:00:17.260 --> 00:00:20.260
utfrågades av en journalist om nyhetsströmmen (feeden).

00:00:20.260 --> 00:00:22.260
Och journalisten frågade honom,

00:00:22.260 --> 00:00:24.260
"Varför är denna så viktig?"

00:00:24.260 --> 00:00:26.260
Och Zuckerberg svarade,

00:00:26.260 --> 00:00:28.260
"En ekorre som dör framför din trappa

00:00:28.260 --> 00:00:31.260
kan vara mer relevant för vad du föredrar just nu

00:00:31.260 --> 00:00:34.260
än människor som dör i Afrika."

00:00:34.260 --> 00:00:36.260
Och vad jag vill prata om är

00:00:36.260 --> 00:00:39.260
hur en webbaserad idé för relevans kan se ut.

00:00:40.260 --> 00:00:42.260
Så när jag växte upp

00:00:42.260 --> 00:00:44.260
i ett verkligen lantligt område i Maine,

00:00:44.260 --> 00:00:47.260
så betydde verkligen Internet något väldigt speciellt för mig.

00:00:47.260 --> 00:00:49.260
Det innebar en anslutning ut i världen.

00:00:49.260 --> 00:00:52.260
Det betydde någonting som skulle sammanbinda oss alla.

00:00:52.260 --> 00:00:55.260
Och jag var säker att det skulle vara positivt för demokratin

00:00:55.260 --> 00:00:58.260
och för vårt samhälle.

00:00:58.260 --> 00:01:00.260
Men det håller på skifta,

00:01:00.260 --> 00:01:02.260
på vilket sätt information flödar på nätet,

00:01:02.260 --> 00:01:05.260
och det sker utan att vi ser det.

00:01:05.260 --> 00:01:07.260
Och om vi inte lägger till märke till detta,

00:01:07.260 --> 00:01:10.260
så skulle detta kunna bli ett verkligt problem.

00:01:10.260 --> 00:01:13.260
Jag märkte detta första gången på en plats jag spendera mycket tid på

00:01:13.260 --> 00:01:15.260
-- på min Facebooksida.

00:01:15.260 --> 00:01:18.260
Jag är progressiv, politisk, -- stor överraskning --

00:01:18.260 --> 00:01:20.260
men jag alltid försökt på mitt sätt att möta konservativa

00:01:20.260 --> 00:01:22.260
Jag tycker om att höra vad de funderar kring;

00:01:22.260 --> 00:01:24.260
Jag vill se vad de länkar till;

00:01:24.260 --> 00:01:26.260
Jag gillar att lära mig saker.

00:01:26.260 --> 00:01:29.260
Så jag vart väldigt förvånad när jag en dag förstod

00:01:29.260 --> 00:01:32.260
att de konservativa hade försvunnit från min Facebook-nyhetsström (feed).

00:01:33.260 --> 00:01:35.260
Och visade sig

00:01:35.260 --> 00:01:39.260
att Facebook tittade på vilka länkar jag klickade på

00:01:39.260 --> 00:01:41.260
och siten noterade i själva verket,

00:01:41.260 --> 00:01:43.260
att jag klickade mer på min liberala vänners länkar

00:01:43.260 --> 00:01:46.260
än på mina konservativa vänners länkar.

00:01:46.260 --> 00:01:48.260
Och utan att tillfråga mig,

00:01:48.260 --> 00:01:50.260
hade siten tagit bort dem.

00:01:50.260 --> 00:01:53.260
De hade försvunnit.

00:01:54.260 --> 00:01:56.260
Och Facebook är inte ensam om

00:01:56.260 --> 00:01:58.260
att göra detta osynligt med algoritmer,

00:01:58.260 --> 00:02:01.260
att redigera webben.

00:02:01.260 --> 00:02:03.260
Även Google gör det.

00:02:03.260 --> 00:02:06.260
Om jag söker något och du söker desamma,

00:02:06.260 --> 00:02:08.260
samtidigt i denna stund,

00:02:08.260 --> 00:02:11.260
så kan vi få väldigt olika sökresultat.

00:02:11.260 --> 00:02:14.260
Detta även om du är utloggad, en ingenjör berättade för mig

00:02:14.260 --> 00:02:16.260
att det finns 57 signaler

00:02:16.260 --> 00:02:19.260
som Google tittar på --

00:02:19.260 --> 00:02:22.260
allt i från vilken typ av dator du använder

00:02:22.260 --> 00:02:24.260
till vilken webbläsare du använder

00:02:24.260 --> 00:02:26.260
till vart du är lokaliserad --

00:02:26.260 --> 00:02:29.260
detta används för att skräddarsy just dina sökresultat.

00:02:29.260 --> 00:02:31.260
Betänk detta för en sekund:

00:02:31.260 --> 00:02:35.260
det finns inget standard-Google längre.

00:02:35.260 --> 00:02:38.260
Och vet du, det lustiga är att det är svårt att se det.

00:02:38.260 --> 00:02:40.260
Du kan inte se hur olika dina sökresultat är

00:02:40.260 --> 00:02:42.260
jämfört med någon annans.

00:02:42.260 --> 00:02:44.260
Men för ett par veckor sedan,

00:02:44.260 --> 00:02:47.260
så bad jag några vänner att Googla "Egypt"

00:02:47.260 --> 00:02:50.260
och skicka mig några skärmdumpar på vad de fick.

00:02:50.260 --> 00:02:53.260
Här är min vän Scotts skärmdump-

00:02:54.260 --> 00:02:57.260
Och här är min vän Daniels skärmdump.

00:02:57.260 --> 00:02:59.260
När man lägger dem sida vid sida,

00:02:59.260 --> 00:03:01.260
så behöver du inte ens läsa länkarna

00:03:01.260 --> 00:03:03.260
för se hur olika dessa två sidor är.

00:03:03.260 --> 00:03:05.260
Men när du läser länkarna,

00:03:05.260 --> 00:03:08.260
är det verkligen anmärkningsvärt.

00:03:09.260 --> 00:03:12.260
Daniel fick ingenting om protesterna i Egypten överhuvudtaget

00:03:12.260 --> 00:03:14.260
på sin första sida med Google-resultat.

00:03:14.260 --> 00:03:16.260
Scotts resultat var full av dem.

00:03:16.260 --> 00:03:18.260
Och detta var den stora nyheten den dagen vid den tiden på dagen.

00:03:18.260 --> 00:03:21.260
Sökresultaten håller på att bli så olika.

00:03:21.260 --> 00:03:24.260
Och det är inte bara Google och Facebook heller.

00:03:24.260 --> 00:03:26.260
Det är något som sprider sig över hela webben.

00:03:26.260 --> 00:03:29.260
Det är en mängd företag som håller på med den typ av personalisering.

00:03:29.260 --> 00:03:32.260
Yahoo News, den största nyhetssiten på Internet,

00:03:32.260 --> 00:03:35.260
är nu personaliserad -- olika människor får olika saker.

00:03:36.260 --> 00:03:39.260
Huffington Post, the Washington Post, the New York Times --

00:03:39.260 --> 00:03:42.260
lockar med personalisering på flera olika sätt.

00:03:42.260 --> 00:03:45.260
Och leder oss mycket snabbt

00:03:45.260 --> 00:03:47.260
mot en värld där

00:03:47.260 --> 00:03:51.260
Internet visar oss vad det tror vi vill se,

00:03:51.260 --> 00:03:54.260
men inte nödvändigtvis vad vi behöver se.

00:03:54.260 --> 00:03:57.260
Som Eric Schmidt sa,

00:03:57.260 --> 00:04:00.260
"Det kommer bli väldigt svårt att få se något eller konsumera något

00:04:00.260 --> 00:04:02.260
som inte i någon mening

00:04:02.260 --> 00:04:05.260
skräddarsytts"

00:04:05.260 --> 00:04:07.260
Så jag anser detta är ett problem.

00:04:07.260 --> 00:04:10.260
Och jag anser, om du lägger samman alla dessa filter,

00:04:10.260 --> 00:04:12.260
och du tar alla dessa algoritmer,

00:04:12.260 --> 00:04:15.260
får du vad jag kallar en "filterbubbla".

00:04:16.260 --> 00:04:19.260
Och din egen "filterbubbla" är ditt egna

00:04:19.260 --> 00:04:21.260
unika universum av information

00:04:21.260 --> 00:04:23.260
som du lever i på nätet.

00:04:23.260 --> 00:04:26.260
Och vad som är i din Filterbubbla

00:04:26.260 --> 00:04:29.260
beror på vem du är och den beror vad du gör.

00:04:29.260 --> 00:04:33.260
Men grejen är det är inte du som bestämmer vad som kommer in.

00:04:33.260 --> 00:04:35.260
Än viktigare,

00:04:35.260 --> 00:04:38.260
är att du verkligen inte ser vad som klipps bort.

00:04:38.260 --> 00:04:40.260
Så ett av problemen med "Filterbubblor"

00:04:40.260 --> 00:04:43.260
vilket upptäcktes av några forskare på Netflix.

00:04:43.260 --> 00:04:46.260
Och de tittade på Netflix-köerna och de märkte något väldigt märkligt

00:04:46.260 --> 00:04:48.260
något som en hel del av oss märkt,

00:04:48.260 --> 00:04:50.260
vilket är att det är några filmer

00:04:50.260 --> 00:04:53.260
som far direkt upp och ut till hushållen.

00:04:53.260 --> 00:04:56.260
De går in i kön, och viner direkt ut.

00:04:56.260 --> 00:04:58.260
Så "Iron Man" viner rätt ut,

00:04:58.260 --> 00:05:00.260
och "Waiting for Superman"

00:05:00.260 --> 00:05:02.260
kan få vänta ganska lång tid.

00:05:02.260 --> 00:05:04.260
Vad de upptäckte

00:05:04.260 --> 00:05:06.260
var att i våra Netflix-köer

00:05:06.260 --> 00:05:09.260
pågår en episk kamp

00:05:09.260 --> 00:05:12.260
mellan vårt framtida strävande jag

00:05:12.260 --> 00:05:15.260
och vår mer impulsiva jag i nuet.

00:05:15.260 --> 00:05:17.260
Du vet att vi alla vill vara någon

00:05:17.260 --> 00:05:19.260
som tittat på "Rashomon"

00:05:19.260 --> 00:05:21.260
men just nu

00:05:21.260 --> 00:05:24.260
vill vi bara se "Ace Ventura" för fjärde gången.

00:05:24.260 --> 00:05:27.260
(Skratt)

00:05:27.260 --> 00:05:29.260
Så det bästa urvalet ger oss lite av varje.

00:05:29.260 --> 00:05:31.260
Det ger oss en lite del av Justin Bieber

00:05:31.260 --> 00:05:33.260
och en liten del av Afghanistan.

00:05:33.260 --> 00:05:35.260
Och det ger oss lite "informationsgrönsaker"

00:05:35.260 --> 00:05:38.260
och det ger oss lite "informationsefterrätter".

00:05:38.260 --> 00:05:40.260
Och utmaningarna med dessa typer av algoritmiska filter,

00:05:40.260 --> 00:05:42.260
dessa personaliserade filter,

00:05:42.260 --> 00:05:44.260
är att de på grund av att de tittar mest på

00:05:44.260 --> 00:05:48.260
vad du klickat först på,

00:05:48.260 --> 00:05:52.260
kan dessa komma ur balans.

00:05:52.260 --> 00:05:55.260
Och istället för en balanserad informationsdiet,

00:05:55.260 --> 00:05:57.260
kan du tillslut vara omgiven

00:05:57.260 --> 00:05:59.260
med informationsskräpmat.

00:05:59.260 --> 00:06:01.260
Vad detta pekar på

00:06:01.260 --> 00:06:04.260
är att vi helt enkelt skrivit sagan om Internet felaktigt.

00:06:04.260 --> 00:06:06.260
I ett etermediasamhället --

00:06:06.260 --> 00:06:08.260
låter den grundande mytologin på detta sätt --

00:06:08.260 --> 00:06:10.260
i ett etermediasamhälle

00:06:10.260 --> 00:06:12.260
fanns dessa grindvakter, dessa redaktörer,

00:06:12.260 --> 00:06:15.260
och de kontrollerade informationsflödet.

00:06:15.260 --> 00:06:18.260
Och så kom Internet och sköljde bort dem,

00:06:18.260 --> 00:06:20.260
och tillät oss att kommunicera tillsammans,

00:06:20.260 --> 00:06:22.260
och det var underbart.

00:06:22.260 --> 00:06:25.260
Men det är sannerligen inte vad som händer just nu.

00:06:26.260 --> 00:06:29.260
Vad vi ser är att facklan skickas vidare

00:06:29.260 --> 00:06:31.260
från mänskliga grindvakter

00:06:31.260 --> 00:06:34.260
till algoritmiska vakter.

00:06:34.260 --> 00:06:37.260
Och saken med algoritmerna är att

00:06:37.260 --> 00:06:40.260
de har inte än har någon typ av inbyggd etik

00:06:40.260 --> 00:06:43.260
som redaktörerna hade.

00:06:43.260 --> 00:06:46.260
Så om algoritmerna ska kunna skapa en värld för oss,

00:06:46.260 --> 00:06:49.260
om de ska kunna bestämma vad vi ska få se och vi inte ska få se,

00:06:49.260 --> 00:06:51.260
då bör vi säkerställa

00:06:51.260 --> 00:06:54.260
att de inte bara har relevans som enda måttet.

00:06:54.260 --> 00:06:56.260
Vi behöver säkerställa att de visar oss saker

00:06:56.260 --> 00:06:59.260
som är obehagliga eller utmanande eller viktiga --

00:06:59.260 --> 00:07:01.260
det är vad TED gör --

00:07:01.260 --> 00:07:03.260
en annan synvinkel.

00:07:03.260 --> 00:07:05.260
Och saken är den att vi varit där förut

00:07:05.260 --> 00:07:07.260
som samhället.

00:07:08.260 --> 00:07:11.260
Det är inte som 1915 när tidningarna hade det tufft

00:07:11.260 --> 00:07:14.260
kring sitt samhällsansvar.

00:07:14.260 --> 00:07:16.260
Då märkte folk

00:07:16.260 --> 00:07:19.260
att de gjorde något verkligt viktigt.

00:07:19.260 --> 00:07:21.260
Faktum är de kunde inte blivit

00:07:21.260 --> 00:07:23.260
en fungerande demokrati

00:07:23.260 --> 00:07:27.260
om inte medborgarna hade fått ett bra informationsflöde.

00:07:28.260 --> 00:07:31.260
Att tidningarna var kritiska för att de agerade som filtret,

00:07:31.260 --> 00:07:33.260
och att den journalistiska etiken utvecklades.

00:07:33.260 --> 00:07:35.260
Den var inte perfekt.

00:07:35.260 --> 00:07:38.260
men den tog oss genom förra seklet.

00:07:38.260 --> 00:07:40.260
Och nu är,

00:07:40.260 --> 00:07:43.260
vi typ tillbaka på 1915-års nivån på webben.

00:07:44.260 --> 00:07:47.260
Och vi behöver de nya grindvakterna

00:07:47.260 --> 00:07:49.260
för att koda den typen av ansvar

00:07:49.260 --> 00:07:51.260
in i programkoden de skriver.

00:07:51.260 --> 00:07:54.260
Jag vet det är en hel del människor från Facebook och från Google här --

00:07:54.260 --> 00:07:56.260
Larry och Sergey --

00:07:56.260 --> 00:07:58.260
människor som hjälpt till att bygga upp webben som den är,

00:07:58.260 --> 00:08:00.260
och jag är tacksam för detta.

00:08:00.260 --> 00:08:03.260
Men vad vi verkligen måste försäkra oss om

00:08:03.260 --> 00:08:06.260
är att dessa algoritmer har inbyggt

00:08:06.260 --> 00:08:09.260
en känsla för det offentliga livet, en känsla av medborgerligt ansvar.

00:08:09.260 --> 00:08:12.260
Vi måste se till att de är tillräckligt transparenta

00:08:12.260 --> 00:08:14.260
så vi kan se vilka reglerna är

00:08:14.260 --> 00:08:17.260
som bestämmer vad som passerar vårt filter.

00:08:17.260 --> 00:08:19.260
Och vi att ni ger oss lite kontroll över dessa,

00:08:19.260 --> 00:08:21.260
så att vi kan bestämma

00:08:21.260 --> 00:08:24.260
vad som går igenom och vad som inte går igenom.

00:08:24.260 --> 00:08:26.260
För att jag anser

00:08:26.260 --> 00:08:28.260
det är viktigt för oss att Internet blir det

00:08:28.260 --> 00:08:30.260
som det vi alla drömde om att det skulle vara.

00:08:30.260 --> 00:08:33.260
Vi behöver detta för att binda oss samman.

00:08:33.260 --> 00:08:36.260
Vi behöver det för att introducerar oss till nya idéer

00:08:36.260 --> 00:08:39.260
och nya människor och olika perspektiv.

00:08:40.260 --> 00:08:42.260
Och det kommer inte att göra detta

00:08:42.260 --> 00:08:45.260
om det lämnar oss alla i en isolerad personspecifik webb.

00:08:45.260 --> 00:08:47.260
Tack så mycket.

00:08:47.260 --> 00:08:58.260
(Applåder)

