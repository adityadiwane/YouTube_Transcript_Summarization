WEBVTT
Kind: captions
Language: es

00:00:00.000 --> 00:00:07.000
Traductor: Sebastian Betti
Revisor: Eduardo Sierra

00:00:15.260 --> 00:00:17.260
A Mark Zuckerberg

00:00:17.260 --> 00:00:20.260
un periodista le preguntó sobre la redifusión de contenidos web.

00:00:20.260 --> 00:00:22.260
La pregunta era:

00:00:22.260 --> 00:00:24.260
¿por qué es tan importante?

00:00:24.260 --> 00:00:26.260
Y Zuckerberg le contestó:

00:00:26.260 --> 00:00:28.260
"Saber que una ardilla se muere en tu jardín

00:00:28.260 --> 00:00:31.260
puede ser más relevante en este momento para tus intereses

00:00:31.260 --> 00:00:34.260
que saber que muere gente en África".

00:00:34.260 --> 00:00:36.260
Quiero hablar de

00:00:36.260 --> 00:00:39.260
cómo sería la Red si se basara en esa idea de relevancia.

00:00:40.260 --> 00:00:42.260
Crecí

00:00:42.260 --> 00:00:44.260
en una zona rural de Maine

00:00:44.260 --> 00:00:47.260
y entonces, para mí, Internet era algo muy distinto.

00:00:47.260 --> 00:00:49.260
Era una conexión con el mundo.

00:00:49.260 --> 00:00:52.260
Era algo que nos conectaba a todos.

00:00:52.260 --> 00:00:55.260
Y estaba seguro de que sería genial para la democracia

00:00:55.260 --> 00:00:58.260
y para nuestra sociedad.

00:00:58.260 --> 00:01:00.260
Pero ha cambiado la manera

00:01:00.260 --> 00:01:02.260
en la que circula la información en la red

00:01:02.260 --> 00:01:05.260
y este cambio es imperceptible.

00:01:05.260 --> 00:01:07.260
Y si no prestamos atención

00:01:07.260 --> 00:01:10.260
puede convertirse en un problema grave.

00:01:10.260 --> 00:01:13.260
Noté el cambio por primera vez en una página en la que paso mucho tiempo:

00:01:13.260 --> 00:01:15.260
Facebook.

00:01:15.260 --> 00:01:18.260
En política soy progresista (¡vaya sorpresa!)

00:01:18.260 --> 00:01:20.260
pero siempre estoy abierto a las ideas de los conservadores.

00:01:20.260 --> 00:01:22.260
Me gusta escuchar sus ideas;

00:01:22.260 --> 00:01:24.260
me gusta ver los enlaces que comparten;

00:01:24.260 --> 00:01:26.260
me gusta enterarme de algunas cosas.

00:01:26.260 --> 00:01:29.260
Por eso me sorprendió darme cuenta un día

00:01:29.260 --> 00:01:32.260
de que los conservadores habían desaparecido de las novedades de Facebook.

00:01:33.260 --> 00:01:35.260
Lo que había pasado

00:01:35.260 --> 00:01:39.260
era que Facebook estaba controlando en qué enlaces hacía clic

00:01:39.260 --> 00:01:41.260
y que se había dado cuenta de que, realmente,

00:01:41.260 --> 00:01:43.260
hacia clic con más frecuencia en los enlaces de mis amigos progresistas

00:01:43.260 --> 00:01:46.260
que en los de mis amigos conservadores.

00:01:46.260 --> 00:01:48.260
Y sin consultarme

00:01:48.260 --> 00:01:50.260
excluyó a los últimos.

00:01:50.260 --> 00:01:53.260
Desaparecieron.

00:01:54.260 --> 00:01:56.260
Pero Facebook no es la única página

00:01:56.260 --> 00:01:58.260
que hace esta edición invisible

00:01:58.260 --> 00:02:01.260
y algorítmica de la Red.

00:02:01.260 --> 00:02:03.260
Google también lo hace.

00:02:03.260 --> 00:02:06.260
Si yo realizo una búsqueda y ustedes realizan una búsqueda,

00:02:06.260 --> 00:02:08.260
incluso si lo hacemos al mismo tiempo,

00:02:08.260 --> 00:02:11.260
podríamos obtener resultados de búsqueda muy diferentes.

00:02:11.260 --> 00:02:14.260
Un ingeniero me contó que, incluso sin estar conectado,

00:02:14.260 --> 00:02:16.260
hay 57 indicios

00:02:16.260 --> 00:02:19.260
que Google tiene en cuenta

00:02:19.260 --> 00:02:22.260
-desde el tipo de computadora y explorador

00:02:22.260 --> 00:02:24.260
que se está usando,

00:02:24.260 --> 00:02:26.260
hasta la ubicación-

00:02:26.260 --> 00:02:29.260
para personalizar los resultados.

00:02:29.260 --> 00:02:31.260
Piénsenlo durante un segundo,

00:02:31.260 --> 00:02:35.260
ya no existe un Google estándar.

00:02:35.260 --> 00:02:38.260
¿Y saben qué? Lo más gracioso es que es difícil de ver.

00:02:38.260 --> 00:02:40.260
Uno no puede ver lo diferentes que son sus búsquedas

00:02:40.260 --> 00:02:42.260
de las de los demás.

00:02:42.260 --> 00:02:44.260
Pero hace un par de semanas

00:02:44.260 --> 00:02:47.260
le pedí a un puñado de amigos que googlearan "Egipto"

00:02:47.260 --> 00:02:50.260
y que me enviaran capturas de pantalla de los resultados.

00:02:50.260 --> 00:02:53.260
Esta es la captura de pantalla de mi amigo Scott.

00:02:54.260 --> 00:02:57.260
Y esta la de mi amigo Daniel.

00:02:57.260 --> 00:02:59.260
Si las ponemos lado a lado

00:02:59.260 --> 00:03:01.260
ni siquiera tenemos que leer los enlaces

00:03:01.260 --> 00:03:03.260
para ver lo diferentes que son.

00:03:03.260 --> 00:03:05.260
Pero si leemos los enlaces

00:03:05.260 --> 00:03:08.260
es muy notable.

00:03:09.260 --> 00:03:12.260
A Daniel no le aparece nada de las protestas en Egipto

00:03:12.260 --> 00:03:14.260
en su portada de resultados de Google.

00:03:14.260 --> 00:03:16.260
En los resultados de Scott aparece mucho.

00:03:16.260 --> 00:03:18.260
Y esa era la historia del día en ese momento.

00:03:18.260 --> 00:03:21.260
Así de diferentes se están volviendo los resultados.

00:03:21.260 --> 00:03:24.260
Y no se trata sólo de Google y Facebook.

00:03:24.260 --> 00:03:26.260
Esto está arrasando la Red.

00:03:26.260 --> 00:03:29.260
Hay toda una serie de empresas que están haciendo este tipo de personalización.

00:03:29.260 --> 00:03:32.260
Yahoo News, el sitio más grande de noticias de Internet,

00:03:32.260 --> 00:03:35.260
ahora es personalizado; distintas personas obtienen distintas cosas.

00:03:36.260 --> 00:03:39.260
Huffington Post, Washington Post, New York Times

00:03:39.260 --> 00:03:42.260
todos coquetean con algún tipo de personalización.

00:03:42.260 --> 00:03:45.260
Y esto marcha muy rápido

00:03:45.260 --> 00:03:47.260
hacia un mundo en el cual

00:03:47.260 --> 00:03:51.260
Internet nos va a mostrar lo que piense que queremos ver

00:03:51.260 --> 00:03:54.260
y no necesariamente lo que tenemos que ver.

00:03:54.260 --> 00:03:57.260
Como dijo Eric Schmidt:

00:03:57.260 --> 00:04:00.260
"Va a ser muy difícil que las personas miren o consuman algo

00:04:00.260 --> 00:04:02.260
que en alguna medida no haya

00:04:02.260 --> 00:04:05.260
sido hecho a medida para ellas".

00:04:05.260 --> 00:04:07.260
Creo que esto es un problema.

00:04:07.260 --> 00:04:10.260
Si uno junta todos estos filtros,

00:04:10.260 --> 00:04:12.260
todos estos algoritmos,

00:04:12.260 --> 00:04:15.260
obtiene lo que llamo la «burbuja de filtros».

00:04:16.260 --> 00:04:19.260
La burbuja de filtros es el universo propio,

00:04:19.260 --> 00:04:21.260
personal, único, de información

00:04:21.260 --> 00:04:23.260
que uno vive en la red.

00:04:23.260 --> 00:04:26.260
Y lo que haya en la burbuja de filtros

00:04:26.260 --> 00:04:29.260
depende de quién uno es, y de lo que uno hace.

00:04:29.260 --> 00:04:33.260
Pero la cosa es que uno no decide que es lo que entra.

00:04:33.260 --> 00:04:35.260
Y, más importante aún,

00:04:35.260 --> 00:04:38.260
no vemos qué es lo que se elimina.

00:04:38.260 --> 00:04:40.260
Unos investigadores de Netflix

00:04:40.260 --> 00:04:43.260
detectaron problemas con la burbuja de filtros.

00:04:43.260 --> 00:04:46.260
Estaban mirando las listas de Netflix y notaron algo gracioso,

00:04:46.260 --> 00:04:48.260
que a muchos seguro nos ha pasado,

00:04:48.260 --> 00:04:50.260
y es que algunas películas

00:04:50.260 --> 00:04:53.260
aparecen y desaparecen de nuestras listas.

00:04:53.260 --> 00:04:56.260
Entran a la lista y desaparecen enseguida.

00:04:56.260 --> 00:04:58.260
"Iron Man" desaparece

00:04:58.260 --> 00:05:00.260
y "Esperando a Súperman"

00:05:00.260 --> 00:05:02.260
puede quedar mucho tiempo.

00:05:02.260 --> 00:05:04.260
Lo que descubrieron

00:05:04.260 --> 00:05:06.260
es que en nuestras listas de Netflix

00:05:06.260 --> 00:05:09.260
ocurren estas batallas épicas

00:05:09.260 --> 00:05:12.260
entre nuestras aspiraciones futuras

00:05:12.260 --> 00:05:15.260
y nuestro yo impulsivo del momento.

00:05:15.260 --> 00:05:17.260
Ya saben, a todos nos gustaría

00:05:17.260 --> 00:05:19.260
haber visto "Rashōmon"

00:05:19.260 --> 00:05:21.260
pero en este momento

00:05:21.260 --> 00:05:24.260
queremos ver "Ace Ventura" por cuarta vez.

00:05:24.260 --> 00:05:27.260
(Risas)

00:05:27.260 --> 00:05:29.260
Por eso la mejor edición nos da lo mejor de ambas cosas.

00:05:29.260 --> 00:05:31.260
Nos da un poco de Justin Bieber

00:05:31.260 --> 00:05:33.260
y un poco de Afganistán.

00:05:33.260 --> 00:05:35.260
Nos da algunos vegetales informativos

00:05:35.260 --> 00:05:38.260
y nos da algunos postres informativos.

00:05:38.260 --> 00:05:40.260
El desafío de estos filtros algorítmicos,

00:05:40.260 --> 00:05:42.260
de estos filtros personalizados,

00:05:42.260 --> 00:05:44.260
es que al basarse principalmente

00:05:44.260 --> 00:05:48.260
en lo que uno cliquea primero

00:05:48.260 --> 00:05:52.260
pueden alterar ese equilibrio.

00:05:52.260 --> 00:05:55.260
Y en vez de tener una dieta informativa balanceada

00:05:55.260 --> 00:05:57.260
uno termine rodeado

00:05:57.260 --> 00:05:59.260
de comida chatarra informativa.

00:05:59.260 --> 00:06:01.260
Esto sugiere

00:06:01.260 --> 00:06:04.260
que quizá hemos interpretado mal la historia de Internet.

00:06:04.260 --> 00:06:06.260
En una sociedad de la difusión

00:06:06.260 --> 00:06:08.260
-eso dice el mito fundador-

00:06:08.260 --> 00:06:10.260
en una sociedad de la difusión

00:06:10.260 --> 00:06:12.260
estaban estos porteros, los editores,

00:06:12.260 --> 00:06:15.260
que controlaban el flujo de la información.

00:06:15.260 --> 00:06:18.260
Y luego aparece Internet y arrasa con ellos

00:06:18.260 --> 00:06:20.260
y nos permite a todos nosotros conectarnos unos a otros,

00:06:20.260 --> 00:06:22.260
y eso fue genial.

00:06:22.260 --> 00:06:25.260
Pero eso no es lo que está sucediendo ahora.

00:06:26.260 --> 00:06:29.260
Lo que estamos viendo se parece más a un pasaje de antorcha

00:06:29.260 --> 00:06:31.260
entre los porteros humanos

00:06:31.260 --> 00:06:34.260
y los algorítmicos.

00:06:34.260 --> 00:06:37.260
Y el problema es que los algoritmos

00:06:37.260 --> 00:06:40.260
todavía no tienen incorporados los principios éticos

00:06:40.260 --> 00:06:43.260
que tenían los editores.

00:06:43.260 --> 00:06:46.260
Entonces, si los algoritmos nos van a seleccionar el contenido,

00:06:46.260 --> 00:06:49.260
si van a decidir qué veremos y qué no,

00:06:49.260 --> 00:06:51.260
entonces tenemos que asegurarnos

00:06:51.260 --> 00:06:54.260
de que no sólo se guían por la relevancia.

00:06:54.260 --> 00:06:56.260
Tenemos que asegurarnos de que también nos muestran cosas

00:06:56.260 --> 00:06:59.260
incómodas, estimulantes o importantes

00:06:59.260 --> 00:07:01.260
-eso hace TED-

00:07:01.260 --> 00:07:03.260
otros puntos de vista.

00:07:03.260 --> 00:07:05.260
El punto es que hemos pasado por esto antes

00:07:05.260 --> 00:07:07.260
como sociedad.

00:07:08.260 --> 00:07:11.260
No es que en 1915 los periódicos se preocuparan mucho

00:07:11.260 --> 00:07:14.260
por sus responsabilidades cívicas.

00:07:14.260 --> 00:07:16.260
Después, la gente se dio cuenta

00:07:16.260 --> 00:07:19.260
de que servían para algo muy importante.

00:07:19.260 --> 00:07:21.260
Que, de hecho, no se puede tener

00:07:21.260 --> 00:07:23.260
una democracia que funcione

00:07:23.260 --> 00:07:27.260
si los ciudadanos no acceden a un buen flujo de información.

00:07:28.260 --> 00:07:31.260
Que los periódicos eran críticos porque actuaban de filtro

00:07:31.260 --> 00:07:33.260
y entonces nace la ética periodística.

00:07:33.260 --> 00:07:35.260
No era perfecta

00:07:35.260 --> 00:07:38.260
pero con eso pudimos atravesar el siglo pasado.

00:07:38.260 --> 00:07:40.260
Y ahora

00:07:40.260 --> 00:07:43.260
es como que estamos en el 1915 de la Red.

00:07:44.260 --> 00:07:47.260
Y necesitamos que los nuevos porteros

00:07:47.260 --> 00:07:49.260
incluyan este tipo de responsabilidad

00:07:49.260 --> 00:07:51.260
en el código que están escribiendo.

00:07:51.260 --> 00:07:54.260
Sé que entre los presentes hay gente de Facebook y Google

00:07:54.260 --> 00:07:56.260
-Larry y Sergey-

00:07:56.260 --> 00:07:58.260
personas que han ayudado a construir la Red tal como es

00:07:58.260 --> 00:08:00.260
y les agradezco eso.

00:08:00.260 --> 00:08:03.260
Pero realmente necesitamos que nos aseguren

00:08:03.260 --> 00:08:06.260
que estos algoritmos contienen

00:08:06.260 --> 00:08:09.260
un sentido de la vida pública, un sentido de responsabilidad cívica.

00:08:09.260 --> 00:08:12.260
Necesitamos que nos aseguren que son suficientemente transparentes,

00:08:12.260 --> 00:08:14.260
que podemos ver cuáles son las reglas

00:08:14.260 --> 00:08:17.260
que determinan lo que pasa por nuestros filtros.

00:08:17.260 --> 00:08:19.260
Y necesitamos que nos den algún control

00:08:19.260 --> 00:08:21.260
para poder decidir

00:08:21.260 --> 00:08:24.260
qué pasa y que no pasa.

00:08:24.260 --> 00:08:26.260
Porque creo

00:08:26.260 --> 00:08:28.260
que realmente necesitamos que Internet sea eso

00:08:28.260 --> 00:08:30.260
que todos soñamos que fuera.

00:08:30.260 --> 00:08:33.260
Necesitamos que nos conecte a todos.

00:08:33.260 --> 00:08:36.260
Necesitamos que nos presente nuevas ideas,

00:08:36.260 --> 00:08:39.260
nuevas personas y distintas perspectivas.

00:08:40.260 --> 00:08:42.260
Y esto no va a ser posible

00:08:42.260 --> 00:08:45.260
si nos aísla en una Red unipersonal.

00:08:45.260 --> 00:08:47.260
Gracias.

00:08:47.260 --> 00:08:58.260
(Aplausos)

