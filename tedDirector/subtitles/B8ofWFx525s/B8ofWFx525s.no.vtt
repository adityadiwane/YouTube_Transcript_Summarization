WEBVTT
Kind: captions
Language: no

00:00:00.000 --> 00:00:07.000
Translator: Fredrik Randsborg Bølstad
Reviewer: Martin Hassel

00:00:15.260 --> 00:00:17.260
Mark Zuckerberg,

00:00:17.260 --> 00:00:20.260
en journalist spurte ham et spørsmål om nyhetsstrømmen.

00:00:20.260 --> 00:00:22.260
Og det journalisten spurte var,

00:00:22.260 --> 00:00:24.260
"Hvorfor er dette så viktig?"

00:00:24.260 --> 00:00:26.260
Og Zuckerberg svarte,

00:00:26.260 --> 00:00:28.260
"Et ekorn som dør i din hage

00:00:28.260 --> 00:00:31.260
kan være mer relevant for dine interesser akkurat nå

00:00:31.260 --> 00:00:34.260
enn at folk dør i Afrika."

00:00:34.260 --> 00:00:36.260
Og jeg vil snakke om

00:00:36.260 --> 00:00:39.260
hvordan et Internett basert på den idéen om relevanse kan se ut.

00:00:40.260 --> 00:00:42.260
Da jeg vokste opp

00:00:42.260 --> 00:00:44.260
i et virkelig øde område i Maine,

00:00:44.260 --> 00:00:47.260
betydde Internett noe veldig annet for meg.

00:00:47.260 --> 00:00:49.260
Det betydde en forbindelse til verden.

00:00:49.260 --> 00:00:52.260
Det betydde noe som ville koble oss alle sammen.

00:00:52.260 --> 00:00:55.260
Og jeg var sikker på at det kom til å være flott for demokratiet

00:00:55.260 --> 00:00:58.260
og for vårt samfunn.

00:00:58.260 --> 00:01:00.260
Men det er denne endringen

00:01:00.260 --> 00:01:02.260
i hvordan informasjon beveger seg på nettet,

00:01:02.260 --> 00:01:05.260
og den er usynlig.

00:01:05.260 --> 00:01:07.260
Og hvis vi ikke holder øye med den,

00:01:07.260 --> 00:01:10.260
kan det bli et virkelig problem.

00:01:10.260 --> 00:01:13.260
Først la jeg merke til dette på et sted der jeg tilbringer mye tid –

00:01:13.260 --> 00:01:15.260
Facebook-siden min.

00:01:15.260 --> 00:01:18.260
Politisk er jeg progressiv – stor overraskelse –

00:01:18.260 --> 00:01:20.260
men jeg har alltid gjort mitt beste for å treffe konservative.

00:01:20.260 --> 00:01:22.260
Jeg liker å høre hva de tenker på;

00:01:22.260 --> 00:01:24.260
jeg liker å se hva de linker til;

00:01:24.260 --> 00:01:26.260
jeg liker å lære en ting eller to.

00:01:26.260 --> 00:01:29.260
Så jeg ble overrasket da jeg en dag oppdaget

00:01:29.260 --> 00:01:32.260
at de konservative hadde forsvunnet fra nyhetsstrømmen min på Facebook.

00:01:33.260 --> 00:01:35.260
Og det som viste seg å være årsaken

00:01:35.260 --> 00:01:39.260
var at Facebook registrerte hvilke lenker jeg klikket på,

00:01:39.260 --> 00:01:41.260
og la merke til at jeg faktisk

00:01:41.260 --> 00:01:43.260
klikket mer på mine liberale venners lenker

00:01:43.260 --> 00:01:46.260
enn mine konservative venners lenker.

00:01:46.260 --> 00:01:48.260
Og uten å rådføre seg med meg om det,

00:01:48.260 --> 00:01:50.260
filtrerte de dem ut.

00:01:50.260 --> 00:01:53.260
De forsvant.

00:01:54.260 --> 00:01:56.260
Facebook er ikke de eneste

00:01:56.260 --> 00:01:58.260
som driver med denne usynlige, algoritmiske

00:01:58.260 --> 00:02:01.260
filtreringen av Internett.

00:02:01.260 --> 00:02:03.260
Google gjør det også.

00:02:03.260 --> 00:02:06.260
Hvis jeg søker etter noe og du søker etter noe,

00:02:06.260 --> 00:02:08.260
selv akkurat nå på nøyaktig samme tidspunkt,

00:02:08.260 --> 00:02:11.260
kan vi få veldig forskjellige resultater.

00:02:11.260 --> 00:02:14.260
Selv når du er logget ut, fortalte en utvikler meg,

00:02:14.260 --> 00:02:16.260
er det 57 signaler

00:02:16.260 --> 00:02:19.260
som Google ser på –

00:02:19.260 --> 00:02:22.260
alt fra hva slags datamaskin du bruker,

00:02:22.260 --> 00:02:24.260
til hvilken nettleser du bruker,

00:02:24.260 --> 00:02:26.260
til hvor du befinner deg –

00:02:26.260 --> 00:02:29.260
som blir brukt til å personlig skreddersy dine søkeresultater.

00:02:29.260 --> 00:02:31.260
Tenk på det et øyeblikk:

00:02:31.260 --> 00:02:35.260
det er ingen standard Google lengre.

00:02:35.260 --> 00:02:38.260
Og du vet, det rare med dette er at det er vanskelig å se.

00:02:38.260 --> 00:02:40.260
Du kan ikke se hvor annerledes dine søkeresultater er

00:02:40.260 --> 00:02:42.260
i forhold til alle andre sine.

00:02:42.260 --> 00:02:44.260
Men for et par uker siden,

00:02:44.260 --> 00:02:47.260
spurte jeg en gruppe venner om å Google "Egypt"

00:02:47.260 --> 00:02:50.260
og sende meg skjermbilder av resultatene sine.

00:02:50.260 --> 00:02:53.260
Her er min venn Scott sitt skjermbilde.

00:02:54.260 --> 00:02:57.260
Og her er min venn Daniel sitt skjermbilde.

00:02:57.260 --> 00:02:59.260
Når du plasserer dem side om side,

00:02:59.260 --> 00:03:01.260
trenger du ikke en gang lese lenkene

00:03:01.260 --> 00:03:03.260
for å se hvor forskjellig disse to sidene er.

00:03:03.260 --> 00:03:05.260
Men når du leser lenkene,

00:03:05.260 --> 00:03:08.260
er det faktisk veldig oppsiktsvekkende.

00:03:09.260 --> 00:03:12.260
Daniel fikk ikke noen treff om protestene i Egypt

00:03:12.260 --> 00:03:14.260
på sin første side med Google-resultater.

00:03:14.260 --> 00:03:16.260
Scott sine resultater var fulle av dem.

00:03:16.260 --> 00:03:18.260
Og dette var den store nyhetssaken på dette tidspunktet.

00:03:18.260 --> 00:03:21.260
Det er såpass forskjellig disse resultatene er i ferd med å bli.

00:03:21.260 --> 00:03:24.260
Dette gjelder ikke bare Google og Facebook heller.

00:03:24.260 --> 00:03:26.260
Dette er en trend som går igjen på Internett.

00:03:26.260 --> 00:03:29.260
Det er en hel rekke av bedrifter som bedriver denne type personalisering.

00:03:29.260 --> 00:03:32.260
Yahoo News, den største nyhetssiden på Internett,

00:03:32.260 --> 00:03:35.260
er nå personalisert – forskjellige folk ser forskjellige ting.

00:03:36.260 --> 00:03:39.260
Huffington Post, the Washington Post, the New York Times –

00:03:39.260 --> 00:03:42.260
alle flørter med personalisering på forskjellige måter.

00:03:42.260 --> 00:03:45.260
Og det beveger oss veldig raskt

00:03:45.260 --> 00:03:47.260
mot en verden der

00:03:47.260 --> 00:03:51.260
Internett viser oss hva det tror vi ønsker å se,

00:03:51.260 --> 00:03:54.260
men ikke nødvendigvis det vi trenger å se.

00:03:54.260 --> 00:03:57.260
Som Eric Schmidt sa,

00:03:57.260 --> 00:04:00.260
"Det kommer til å bli veldig vanskelig for folk å se eller konsumere noe

00:04:00.260 --> 00:04:02.260
som ikke på noen måte

00:04:02.260 --> 00:04:05.260
er blitt skreddersydd til dem."

00:04:05.260 --> 00:04:07.260
Jeg syntes at dette er et problem.

00:04:07.260 --> 00:04:10.260
Og jeg tror at dersom du kombinerer alle disse filtrene,

00:04:10.260 --> 00:04:12.260
alle disse algoritmene,

00:04:12.260 --> 00:04:15.260
så får du det jeg kaller en filterboble.

00:04:16.260 --> 00:04:19.260
Din filterboble er ditt eget personlige,

00:04:19.260 --> 00:04:21.260
unike univers av informasjon

00:04:21.260 --> 00:04:23.260
som du lever i på nettet.

00:04:23.260 --> 00:04:26.260
Hva som finnes i din filterboble

00:04:26.260 --> 00:04:29.260
kommer an på hvem du er og det kommer an på hva du gjør.

00:04:29.260 --> 00:04:33.260
Men greia er at du får ikke bestemme hva som får komme inn.

00:04:33.260 --> 00:04:35.260
Og enda viktigere,

00:04:35.260 --> 00:04:38.260
du får faktisk ikke se hva som blir filtrert ut.

00:04:38.260 --> 00:04:40.260
Så et av problemene med filterboblen

00:04:40.260 --> 00:04:43.260
ble oppdaget av noen forskere hos Netflix.

00:04:43.260 --> 00:04:46.260
De så på Netflix-køen, og la merke til noe litt rart

00:04:46.260 --> 00:04:48.260
som mange av oss antakeligvis har lagt merke til,

00:04:48.260 --> 00:04:50.260
nemlig at det er noen filmer

00:04:50.260 --> 00:04:53.260
som bare sklir til topps og sendes direkte ut.

00:04:53.260 --> 00:04:56.260
De kommer inn i køen og sklir rett til topps.

00:04:56.260 --> 00:04:58.260
Så "Iron Man" sklir rett til topps,

00:04:58.260 --> 00:05:00.260
og "Waiting for Superman"

00:05:00.260 --> 00:05:02.260
kan vente veldig lenge.

00:05:02.260 --> 00:05:04.260
Det de oppdaget

00:05:04.260 --> 00:05:06.260
var at i vår Netflix-kø

00:05:06.260 --> 00:05:09.260
foregår det en storslått kamp

00:05:09.260 --> 00:05:12.260
mellom vår fremtidige ambisiøse selv

00:05:12.260 --> 00:05:15.260
og vår mer impulsive nåværende selv.

00:05:15.260 --> 00:05:17.260
Dere vet at vi alle ønsker å være en

00:05:17.260 --> 00:05:19.260
som har sett "Rashomon,"

00:05:19.260 --> 00:05:21.260
men akkurat nå

00:05:21.260 --> 00:05:24.260
vil vi heller se "Ace Ventura" for fjerde gang.

00:05:24.260 --> 00:05:27.260
(Latter)

00:05:27.260 --> 00:05:29.260
Så det beste filteret gir oss litt av begge deler.

00:05:29.260 --> 00:05:31.260
Det gir oss en liten bit av Justin Bieber

00:05:31.260 --> 00:05:33.260
og en liten bit av Afghanistan.

00:05:33.260 --> 00:05:35.260
Det gir oss noe informasjon om grønnsaker;

00:05:35.260 --> 00:05:38.260
det gir oss noe informasjon om dessert.

00:05:38.260 --> 00:05:40.260
Og utfordringen med denne type algoritmiske filtre,

00:05:40.260 --> 00:05:42.260
disse personaliserte filtrene,

00:05:42.260 --> 00:05:44.260
er at, fordi de hovedsakelig ser

00:05:44.260 --> 00:05:48.260
på hva du klikker på først,

00:05:48.260 --> 00:05:52.260
kan de sette denne balansen ute av spill.

00:05:52.260 --> 00:05:55.260
Og i stedet for en balansert informasjonsdiett

00:05:55.260 --> 00:05:57.260
kan du ende opp omringet

00:05:57.260 --> 00:05:59.260
av informasjonssøppelmat.

00:05:59.260 --> 00:06:01.260
Det dette antyder

00:06:01.260 --> 00:06:04.260
er at vi faktisk kan ha fått historien om Internett feil.

00:06:04.260 --> 00:06:06.260
I et kringkastingssamfunn –

00:06:06.260 --> 00:06:08.260
dette er slik grunnleggingsmytologien går –

00:06:08.260 --> 00:06:10.260
I et kringkastingssamfunn,

00:06:10.260 --> 00:06:12.260
der var det disse portvaktene, redaktørene,

00:06:12.260 --> 00:06:15.260
og de kontrollerte informasjonsflyten.

00:06:15.260 --> 00:06:18.260
Og så kom Internett på banen og feide dem ut av veien,

00:06:18.260 --> 00:06:20.260
og det tillot oss alle å koble oss sammen,

00:06:20.260 --> 00:06:22.260
og det var fantastisk.

00:06:22.260 --> 00:06:25.260
Men det er ikke akkurat det som skjer nå.

00:06:26.260 --> 00:06:29.260
Det vi ser er snarere en videreføring av ansvaret

00:06:29.260 --> 00:06:31.260
vekk fra menneskelige portvakter

00:06:31.260 --> 00:06:34.260
til algoritmiske.

00:06:34.260 --> 00:06:37.260
Og greia er at algoritmer

00:06:37.260 --> 00:06:40.260
ikke enda har den formen for innebygd etikk

00:06:40.260 --> 00:06:43.260
som redaktørene hadde.

00:06:43.260 --> 00:06:46.260
Så hvis algoritmer skal håndplukke verden for oss,

00:06:46.260 --> 00:06:49.260
hvis de skal få bestemme hva vi får se og hva vi ikke får se,

00:06:49.260 --> 00:06:51.260
da er vi nødt til å sørge for

00:06:51.260 --> 00:06:54.260
at de ikke kun er knyttet til relevans.

00:06:54.260 --> 00:06:56.260
Vi må sørge for at de også viser oss ting

00:06:56.260 --> 00:06:59.260
som er ubehagelig eller utfordrende eller viktig –

00:06:59.260 --> 00:07:01.260
det er dette TED gjør –

00:07:01.260 --> 00:07:03.260
andre synspunkter.

00:07:03.260 --> 00:07:05.260
Det spesielle er at vi faktisk har vært i denne situasjonen før

00:07:05.260 --> 00:07:07.260
som et samfunn.

00:07:08.260 --> 00:07:11.260
I 1915 hadde ikke akkurat avisene et anstrengt forhold

00:07:11.260 --> 00:07:14.260
til sitt samfunnsansvar.

00:07:14.260 --> 00:07:16.260
Så la folk merke til

00:07:16.260 --> 00:07:19.260
at de gjorde noe veldig viktig.

00:07:19.260 --> 00:07:21.260
At man faktisk ikke kunne ha

00:07:21.260 --> 00:07:23.260
et fungerende demokrati

00:07:23.260 --> 00:07:27.260
hvis borgerne ikke fikk en god strøm av informasjon,

00:07:28.260 --> 00:07:31.260
at avisene var viktige fordi de opptrådte som filteret,

00:07:31.260 --> 00:07:33.260
og så ble journalistisk etikk utviklet.

00:07:33.260 --> 00:07:35.260
Det var ikke perfekt,

00:07:35.260 --> 00:07:38.260
men det fikk oss gjennom det forrige århundret.

00:07:38.260 --> 00:07:40.260
Så nå,

00:07:40.260 --> 00:07:43.260
er vi på en måte tilbake i 1915 på Internett.

00:07:44.260 --> 00:07:47.260
Og vi trenger de nye portvaktene

00:07:47.260 --> 00:07:49.260
til å hardkode et slikt ansvar

00:07:49.260 --> 00:07:51.260
inn i koden de skriver.

00:07:51.260 --> 00:07:54.260
Jeg vet at det er mange mennesker her fra Facebook og fra Google –

00:07:54.260 --> 00:07:56.260
Larry og Sergey –

00:07:56.260 --> 00:07:58.260
mennesker som har hjulpet med å bygge Internett slik det er,

00:07:58.260 --> 00:08:00.260
og jeg er takknemlig for det.

00:08:00.260 --> 00:08:03.260
Men vi trenger virkelig at dere sørger for

00:08:03.260 --> 00:08:06.260
at disse algoritmene har kodet inn i seg

00:08:06.260 --> 00:08:09.260
en oppfatning av det offentlige liv, en forstand for et samfunnsansvar.

00:08:09.260 --> 00:08:12.260
Vi trenger at dere sørger for at de er såpass åpne

00:08:12.260 --> 00:08:14.260
slik at vi kan se hva reglene er

00:08:14.260 --> 00:08:17.260
for å avgjøre hva som kommer gjennom våre filtre.

00:08:17.260 --> 00:08:19.260
Og vi trenger at dere gir oss noe kontroll

00:08:19.260 --> 00:08:21.260
så vi kan få bestemme

00:08:21.260 --> 00:08:24.260
hva som kommer gjennom og hva som stoppes.

00:08:24.260 --> 00:08:26.260
Fordi jeg tror

00:08:26.260 --> 00:08:28.260
at vi virkelig trenger at Internett blir den tingen

00:08:28.260 --> 00:08:30.260
vi alle drømte at det skulle bli.

00:08:30.260 --> 00:08:33.260
Vi trenger at det kobler oss alle sammen.

00:08:33.260 --> 00:08:36.260
Vi trenger det for å introdusere oss for nye idéer

00:08:36.260 --> 00:08:39.260
og nye mennesker og forskjellige perspektiver.

00:08:40.260 --> 00:08:42.260
Og det kommer ikke til å gjøre det

00:08:42.260 --> 00:08:45.260
om det etterlater oss alle isolert i et nett av en.

00:08:45.260 --> 00:08:47.260
Takk.

00:08:47.260 --> 00:08:58.260
(Applaus)

