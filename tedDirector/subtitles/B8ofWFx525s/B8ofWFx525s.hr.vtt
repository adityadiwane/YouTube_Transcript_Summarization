WEBVTT
Kind: captions
Language: hr

00:00:00.000 --> 00:00:07.000
Prevoditelj: Matija Stepic
Recezent: Tilen Pigac - EFZG

00:00:15.260 --> 00:00:17.260
Mark Zuckerberg,

00:00:17.260 --> 00:00:20.260
novinar mu je postavio pitanje u vezi news feed-a.

00:00:20.260 --> 00:00:22.260
I novinar ga je pitao,

00:00:22.260 --> 00:00:24.260
"Zašto je ovo toliko važno?"

00:00:24.260 --> 00:00:26.260
A Zuckerber odgovara,

00:00:26.260 --> 00:00:28.260
"Vjeverica koja umire na vašem travnjaku

00:00:28.260 --> 00:00:31.260
može biti relevantnija za vas u ovom trenutku

00:00:31.260 --> 00:00:34.260
nego ljudi koji umiru u Africi."

00:00:34.260 --> 00:00:36.260
Želio bih govoriti o tome

00:00:36.260 --> 00:00:39.260
kako bi izgledao internet utemeljen na toj ideji o relevantnosti.

00:00:40.260 --> 00:00:42.260
Tokom mog odrastanja

00:00:42.260 --> 00:00:44.260
u stvarno ruralnoj sredini u Maine-u,

00:00:44.260 --> 00:00:47.260
internet mi je značio nešto sasvim drukčije.

00:00:47.260 --> 00:00:49.260
Značio je vezu sa svijetom.

00:00:49.260 --> 00:00:52.260
Značio je nešto što bi nas sve povezalo.

00:00:52.260 --> 00:00:55.260
I bio sam siguran da će to biti odlično za demokraciju

00:00:55.260 --> 00:00:58.260
i za naše društvo.

00:00:58.260 --> 00:01:00.260
Ali tu postoji taj preokret

00:01:00.260 --> 00:01:02.260
u načinu na koji informacija putuje online,

00:01:02.260 --> 00:01:05.260
koji je nevidljiv.

00:01:05.260 --> 00:01:07.260
I ako ne obratimo pažnju na njega,

00:01:07.260 --> 00:01:10.260
može postati ozbiljan problem.

00:01:10.260 --> 00:01:13.260
Prvi put sam ovo primjetio na mjestu na kojem provodim dosta vremena --

00:01:13.260 --> 00:01:15.260
moja Facebook stranica.

00:01:15.260 --> 00:01:18.260
Moja politička razumjevanja su progresivna -- kakvo iznenađenje --

00:01:18.260 --> 00:01:20.260
ali sam se uvijek trudio upoznati konzervativce.

00:01:20.260 --> 00:01:22.260
Volim čuti o čemu razmišljaju;

00:01:22.260 --> 00:01:24.260
Volim vidjeti koje linkove postavljaju;

00:01:24.260 --> 00:01:26.260
Volim naučiti nekoliko stvari.

00:01:26.260 --> 00:01:29.260
Tako sam se iznenadio kada sam jednog dana primjetio

00:01:29.260 --> 00:01:32.260
da su konzervativci nestali iz mog Facebook feeda.

00:01:33.260 --> 00:01:35.260
A ono što se ispostavilo je

00:01:35.260 --> 00:01:39.260
da je Facebook pratio koje linkove sam kliknuo,

00:01:39.260 --> 00:01:41.260
i primjećivao je da sam, u stvari,

00:01:41.260 --> 00:01:43.260
više klikao na linkove mojih liberalnih prijatelja

00:01:43.260 --> 00:01:46.260
nego na linkove mojih konzervativnih prijatelja.

00:01:46.260 --> 00:01:48.260
I bez konzultacije samnom,

00:01:48.260 --> 00:01:50.260
izbacio ih je.

00:01:50.260 --> 00:01:53.260
Oni su nestali.

00:01:54.260 --> 00:01:56.260
Ali Facebook nije jedino mjesto

00:01:56.260 --> 00:01:58.260
koje radi ovo nevidljivo, algoritamsko

00:01:58.260 --> 00:02:01.260
mjenjanje weba.

00:02:01.260 --> 00:02:03.260
I Google to radi.

00:02:03.260 --> 00:02:06.260
Kad ja tražim neki pojam, i vi tražite neki pojam,

00:02:06.260 --> 00:02:08.260
čak i sada, u isto vrijeme,

00:02:08.260 --> 00:02:11.260
možemo dobiti različite rezultate pretrage.

00:02:11.260 --> 00:02:14.260
Čak i ako niste ulogirani, jedan inžinjer mi je rekao,

00:02:14.260 --> 00:02:16.260
postoji 57 signala

00:02:16.260 --> 00:02:19.260
koje Google prati --

00:02:19.260 --> 00:02:22.260
sve od toga na kakvom ste računalu

00:02:22.260 --> 00:02:24.260
do toga koji pretraživač koristite

00:02:24.260 --> 00:02:26.260
i toga gdje se nalazite --

00:02:26.260 --> 00:02:29.260
koje služi da bi vam osobno skrojio rezultate pretrage.

00:02:29.260 --> 00:02:31.260
Razmislite o tome na trenutak:

00:02:31.260 --> 00:02:35.260
više ne postoji standardni Google.

00:02:35.260 --> 00:02:38.260
Znate, zanimljivo u vezi toga je što je to vrlo teško primjetiti.

00:02:38.260 --> 00:02:40.260
Vi ne možete vidjeti koliko se vaši rezultati pretrage

00:02:40.260 --> 00:02:42.260
razlikuju od bilo kojih drugih.

00:02:42.260 --> 00:02:44.260
Ali prije nekoliko tjedana

00:02:44.260 --> 00:02:47.260
pitao sam grupu prijatelja da guglaju "Egipat"

00:02:47.260 --> 00:02:50.260
i da mi pošalju snimak ekrana koji su dobili.

00:02:50.260 --> 00:02:53.260
Evo snimka ekrana mog prijatelja Scott-a.

00:02:54.260 --> 00:02:57.260
A evo snimka ekrana mog prijatelja Daniel-a.

00:02:57.260 --> 00:02:59.260
Kad ih stavite jedan pored drugog,

00:02:59.260 --> 00:03:01.260
ne morate čak ni čitati linkove

00:03:01.260 --> 00:03:03.260
da biste vidjeli koliko se razlikuju.

00:03:03.260 --> 00:03:05.260
Ali kad pročitate linkove,

00:03:05.260 --> 00:03:08.260
stvarno je vrijedno pažnje.

00:03:09.260 --> 00:03:12.260
Daniel nije dobio ništa u vezi prosvjeda u Egiptu

00:03:12.260 --> 00:03:14.260
na svojoj prvoj stranici Google rezultata.

00:03:14.260 --> 00:03:16.260
Scottovi rezultati su bili puni prosvjeda.

00:03:16.260 --> 00:03:18.260
A to je bila najvažnija vijest dana u tom trenutku.

00:03:18.260 --> 00:03:21.260
Toliko ti rezultati postaju različiti.

00:03:21.260 --> 00:03:24.260
To nisu čak ni samo Google i Facebook.

00:03:24.260 --> 00:03:26.260
To je nešto što se širi mrežom.

00:03:26.260 --> 00:03:29.260
Postoji mnoštvo kompanija koje rade ovakvu vrstu presonalizacije.

00:03:29.260 --> 00:03:32.260
Yahoo News, najveći news site na Internetu,

00:03:32.260 --> 00:03:35.260
je sada personaliziran -- različiti ljudi dobiju različite stvari.

00:03:36.260 --> 00:03:39.260
Huffington Post, Washington Post, New York Times --

00:03:39.260 --> 00:03:42.260
svi flertuju s personalizacijom na različite načine.

00:03:42.260 --> 00:03:45.260
I to nas vrlo brzo vodi

00:03:45.260 --> 00:03:47.260
ka svijetu u kojem

00:03:47.260 --> 00:03:51.260
nam Internet prikazuje stvari koje misli da želimo vidjeti,

00:03:51.260 --> 00:03:54.260
ali ne nužno i stvari koje bi trebali vidjeti.

00:03:54.260 --> 00:03:57.260
Kao što je Eric Schmidt rekao,

00:03:57.260 --> 00:04:00.260
"Bit će vrlo teško ljudima gledati ili konzumirati nešto

00:04:00.260 --> 00:04:02.260
što nije na neki način

00:04:02.260 --> 00:04:05.260
skrojeno baš za njih."

00:04:05.260 --> 00:04:07.260
Tako da stvarno smatram da je to problem.

00:04:07.260 --> 00:04:10.260
I mislim da, ako skupite sve te filtere,

00:04:10.260 --> 00:04:12.260
uzmete sve te algoritme,

00:04:12.260 --> 00:04:15.260
dobijete ono što zovem filter mjehurić.

00:04:16.260 --> 00:04:19.260
Vaš filter mjehurić je vaš osobni

00:04:19.260 --> 00:04:21.260
jedinstveni univerzum informacija

00:04:21.260 --> 00:04:23.260
u kojem živite online.

00:04:23.260 --> 00:04:26.260
A što se nalazi u vašem filter mjehuriću

00:04:26.260 --> 00:04:29.260
zavisi od toga tko ste, i zavisi od toga čime se bavite.

00:04:29.260 --> 00:04:33.260
Ali stvar je u tome da vi ne odlučujete što ulazi unutra.

00:04:33.260 --> 00:04:35.260
I još važnije,

00:04:35.260 --> 00:04:38.260
ne možete vidjeti što vam je uklonjeno.

00:04:38.260 --> 00:04:40.260
Tako da je jedan od problema s filter mjehurićem

00:04:40.260 --> 00:04:43.260
otkriven od strane nekih istraživača iz Netflixa.

00:04:43.260 --> 00:04:46.260
Promatrali su Netflix redoslijede, i primjetili nešto zanimljivo

00:04:46.260 --> 00:04:48.260
što su mnogi od nas vjerovatno primjetili,

00:04:48.260 --> 00:04:50.260
a to je da postoje neki filmovi

00:04:50.260 --> 00:04:53.260
koji na neki način iskoče odmah u naše domove.

00:04:53.260 --> 00:04:56.260
Uđu u redoslijed, i odmah iskoče.

00:04:56.260 --> 00:04:58.260
Tako "Iron Man" odmah iskoči,

00:04:58.260 --> 00:05:00.260
a "Waiting for Superman"

00:05:00.260 --> 00:05:02.260
može čekati prilično dugo vremena.

00:05:02.260 --> 00:05:04.260
Ono što su otkrili

00:05:04.260 --> 00:05:06.260
je da se u Netflix redoslijedu

00:05:06.260 --> 00:05:09.260
događa epska bitka

00:05:09.260 --> 00:05:12.260
između našeg budućeg sebe kojem težimo

00:05:12.260 --> 00:05:15.260
i našeg sadašnjeg, impulzivnijeg sebe.

00:05:15.260 --> 00:05:17.260
Znate, svi bi mi željeli biti netko

00:05:17.260 --> 00:05:19.260
tko je gledao "Rashomon,"

00:05:19.260 --> 00:05:21.260
ali trenutno

00:05:21.260 --> 00:05:24.260
želimo gledati "Ace Venturu" po četvrti put.

00:05:24.260 --> 00:05:27.260
(Smijeh)

00:05:27.260 --> 00:05:29.260
Tako da nam najbolje uređivanje daje ponešto od obojeg.

00:05:29.260 --> 00:05:31.260
Daje nam pomalo Justin Biebera

00:05:31.260 --> 00:05:33.260
i pomalo Afganistana.

00:05:33.260 --> 00:05:35.260
Daje nam nekakve biljke informacija,

00:05:35.260 --> 00:05:38.260
i daje nam neke informacijske poslastice.

00:05:38.260 --> 00:05:40.260
Nedostatak ovakve vrste algoritamskih filtera,

00:05:40.260 --> 00:05:42.260
ovih personaliziranih filtera,

00:05:42.260 --> 00:05:44.260
je u tome što, pošto pretežno prate

00:05:44.260 --> 00:05:48.260
na što prvo kliknete,

00:05:48.260 --> 00:05:52.260
mogu promijeniti tu ravnotežu.

00:05:52.260 --> 00:05:55.260
Umjesto balansirane informacijske dijete,

00:05:55.260 --> 00:05:57.260
možete završiti okruženi

00:05:57.260 --> 00:05:59.260
informacijskim smećem od hrane.

00:05:59.260 --> 00:06:01.260
Ovo ustvari ukazuje na to da

00:06:01.260 --> 00:06:04.260
smo možda promašili cijelu priču s Internetom.

00:06:04.260 --> 00:06:06.260
U širokopojasnom društvu --

00:06:06.260 --> 00:06:08.260
ovako glasi osnivačka mitologija --

00:06:08.260 --> 00:06:10.260
u širokopojasnom društvu,

00:06:10.260 --> 00:06:12.260
postojali su ti vratari, urednici,

00:06:12.260 --> 00:06:15.260
koji su kontrolirali protok informacija.

00:06:15.260 --> 00:06:18.260
I onda se pojavio Internet koji ih je pomeo s puta,

00:06:18.260 --> 00:06:20.260
i omogućio svima nama da se međusobno povežemo,

00:06:20.260 --> 00:06:22.260
što je bilo fenomenalno.

00:06:22.260 --> 00:06:25.260
Ali to zapravo nije ono što se upravo događa.

00:06:26.260 --> 00:06:29.260
Ono što sada vidimo je više predavanje štafete

00:06:29.260 --> 00:06:31.260
od ljudskih vratara

00:06:31.260 --> 00:06:34.260
ka algoritamskim.

00:06:34.260 --> 00:06:37.260
A stvar je u tome da algoritmi

00:06:37.260 --> 00:06:40.260
još uvijek nemaju ugrađenu etiku

00:06:40.260 --> 00:06:43.260
koju su imali urednici.

00:06:43.260 --> 00:06:46.260
Ako će nam algoritmi biti tutori o svijetu,

00:06:46.260 --> 00:06:49.260
ako će odlučivati što možemo a što ne možemo vidjeti,

00:06:49.260 --> 00:06:51.260
onda se moramo pobriniti

00:06:51.260 --> 00:06:54.260
da nisu naštimani samo na relevantnost.

00:06:54.260 --> 00:06:56.260
Moramo osigurati da nam prikazuju i stvari

00:06:56.260 --> 00:06:59.260
koje su neugodne ili teške ili bitne --

00:06:59.260 --> 00:07:01.260
to je ono što TED radi --

00:07:01.260 --> 00:07:03.260
druge točke gledišta.

00:07:03.260 --> 00:07:05.260
Stvar je u tome da smo već bili na ovom mjestu

00:07:05.260 --> 00:07:07.260
kao društvo.

00:07:08.260 --> 00:07:11.260
1915. godine, novine se nisu puno brinule

00:07:11.260 --> 00:07:14.260
o svojoj odgovornosti prema građanima.

00:07:14.260 --> 00:07:16.260
Onda su ljudi primjetili

00:07:16.260 --> 00:07:19.260
da one rade nešto vrlo značajno.

00:07:19.260 --> 00:07:21.260
Da, u stvari, ne možete imati

00:07:21.260 --> 00:07:23.260
funkcionalnu demokraciju

00:07:23.260 --> 00:07:27.260
ako građani nemaju dobar priljev informacija.

00:07:28.260 --> 00:07:31.260
Da su novine kritične, jer su funkcionirale kao filter,

00:07:31.260 --> 00:07:33.260
i onda se razvila novinarska etika.

00:07:33.260 --> 00:07:35.260
Nije bila savršena,

00:07:35.260 --> 00:07:38.260
ali nam je poslužila kroz prošli vijek.

00:07:38.260 --> 00:07:40.260
I tako smo danas,

00:07:40.260 --> 00:07:43.260
u neku ruku ponovno u 1915. na Webu.

00:07:44.260 --> 00:07:47.260
I potrebni su nam novi vratari

00:07:47.260 --> 00:07:49.260
da utkaju tu vrstu odgovornosti

00:07:49.260 --> 00:07:51.260
u kod koji ispisuju.

00:07:51.260 --> 00:07:54.260
Znam da ima dosta ljudi ovdje iz Facebooka ili Googlea --

00:07:54.260 --> 00:07:56.260
Larry i Sergey --

00:07:56.260 --> 00:07:58.260
ljudi koji su pomogli u izgradnji Weba kakav je danas,

00:07:58.260 --> 00:08:00.260
i ja sam im zahvalan zbog toga.

00:08:00.260 --> 00:08:03.260
Ali stvarno želimo da osigurate

00:08:03.260 --> 00:08:06.260
da ti algoritmi imaju u sebi ugrađen

00:08:06.260 --> 00:08:09.260
osjećaj javnog života, osjećaj građanske odgovornosti.

00:08:09.260 --> 00:08:12.260
Želimo da osigurate da budu dovoljno transparentni

00:08:12.260 --> 00:08:14.260
kako bi vidjeli koja su pravila

00:08:14.260 --> 00:08:17.260
koja određuju što će proći kroz naše filtere.

00:08:17.260 --> 00:08:19.260
I želimo da nam date neku kontrolu,

00:08:19.260 --> 00:08:21.260
da možemo odlučiti

00:08:21.260 --> 00:08:24.260
što će proći a što ne.

00:08:24.260 --> 00:08:26.260
Jer smatram da

00:08:26.260 --> 00:08:28.260
nam je stvarno potrebno da Internet bude to

00:08:28.260 --> 00:08:30.260
što smo svi sanjali da će biti.

00:08:30.260 --> 00:08:33.260
Potreban nam je da nas sve poveže.

00:08:33.260 --> 00:08:36.260
Potreban nam je da nas upozna s novim idejama

00:08:36.260 --> 00:08:39.260
i novim ljudima i različitim perspektivama.

00:08:40.260 --> 00:08:42.260
A to neće napraviti

00:08:42.260 --> 00:08:45.260
ako nas sve ostavi izolirane u pojedinim mrežama.

00:08:45.260 --> 00:08:47.260
Hvala Vam.

00:08:47.260 --> 00:08:58.260
(Pljesak)

