WEBVTT
Kind: captions
Language: nl

00:00:00.000 --> 00:00:07.000
Vertaald door: Christel Foncke
Nagekeken door: Erik Renes

00:00:15.260 --> 00:00:17.260
Mark Zuckerberg,

00:00:17.260 --> 00:00:20.260
een journalist stelde hem een ​​vraag over de nieuwsfeed.

00:00:20.260 --> 00:00:22.260
De journalist vroeg hem,

00:00:22.260 --> 00:00:24.260
"Waarom is dit zo belangrijk?"

00:00:24.260 --> 00:00:26.260
Zuckerberg zei,

00:00:26.260 --> 00:00:28.260
"Een stervende eekhoorn in je voortuin

00:00:28.260 --> 00:00:31.260
is voor jou op dit moment soms meer relevant

00:00:31.260 --> 00:00:34.260
dan mensen die sterven in Afrika."

00:00:34.260 --> 00:00:36.260
Ik wil het hebben over hoe een Web

00:00:36.260 --> 00:00:39.260
gebaseerd op dat idee over relevantie eruit zou kunnen zien.

00:00:40.260 --> 00:00:42.260
Toen ik opgroeide

00:00:42.260 --> 00:00:44.260
in een zeer landelijk gebied in Maine,

00:00:44.260 --> 00:00:47.260
betekende het internet iets heel anders voor mij.

00:00:47.260 --> 00:00:49.260
Het betekende een verbinding met de wereld.

00:00:49.260 --> 00:00:52.260
Het betekende iets dat ons allemaal zou verbinden.

00:00:52.260 --> 00:00:55.260
Ik was ervan overtuigd dat het geweldig zou zijn voor de democratie

00:00:55.260 --> 00:00:58.260
en voor onze samenleving.

00:00:58.260 --> 00:01:00.260
Maar er is een verschuiving

00:01:00.260 --> 00:01:02.260
in de manier waarop informatie online stroomt

00:01:02.260 --> 00:01:05.260
en het is onzichtbaar.

00:01:05.260 --> 00:01:07.260
En als we niet opletten,

00:01:07.260 --> 00:01:10.260
kan het een echt probleem worden.

00:01:10.260 --> 00:01:13.260
Ik merkte het voor het eerst op in een plaats waar ik veel tijd besteed --

00:01:13.260 --> 00:01:15.260
mijn Facebook-pagina.

00:01:15.260 --> 00:01:18.260
Ik ben progressief, politiek - grote verrassing -

00:01:18.260 --> 00:01:20.260
maar ik heb er altijd alles aan gedaan om ook conservatieven te ontmoeten.

00:01:20.260 --> 00:01:22.260
Ik wil horen waaraan ze denken;

00:01:22.260 --> 00:01:24.260
Ik wil zien waarnaar ze verwijzen;

00:01:24.260 --> 00:01:26.260
Ik leer graag een paar dingen bij.

00:01:26.260 --> 00:01:29.260
Dus was ik verrast toen ik op een dag merkte

00:01:29.260 --> 00:01:32.260
dat de conservatieven verdwenen waren uit mijn Facebookfeed.

00:01:33.260 --> 00:01:35.260
Wat er aan de hand bleek te zijn

00:01:35.260 --> 00:01:39.260
was dat Facebook bijhield op welke links ik klikte,

00:01:39.260 --> 00:01:41.260
en het merkte op

00:01:41.260 --> 00:01:43.260
dat ik meer op de links van mijn liberale vrienden klikte

00:01:43.260 --> 00:01:46.260
dan op die van mijn conservatieve vrienden.

00:01:46.260 --> 00:01:48.260
En zonder mij te raadplegen

00:01:48.260 --> 00:01:50.260
werden deze links gewist.

00:01:50.260 --> 00:01:53.260
Ze verdwenen.

00:01:54.260 --> 00:01:56.260
Facebook is niet de enige plek

00:01:56.260 --> 00:01:58.260
die dit soort onzichtbare, algoritmische

00:01:58.260 --> 00:02:01.260
bewerking doet op het Web.

00:02:01.260 --> 00:02:03.260
Google doet het ook.

00:02:03.260 --> 00:02:06.260
Als ik naar iets zoek en jij zoekt iets,

00:02:06.260 --> 00:02:08.260
zelfs nu, op precies hetzelfde moment,

00:02:08.260 --> 00:02:11.260
krijgen we waarschijnlijk heel andere resultaten.

00:02:11.260 --> 00:02:14.260
Zelfs als je uitgelogd bent, vertelde een ingenieur me,

00:02:14.260 --> 00:02:16.260
zijn er 57 signalen

00:02:16.260 --> 00:02:19.260
waar Google naar kijkt -

00:02:19.260 --> 00:02:22.260
alles van wat voor soort computer je op werkt

00:02:22.260 --> 00:02:24.260
tot welke browser je gebruikt

00:02:24.260 --> 00:02:26.260
tot de plaats waar je je bevindt -

00:02:26.260 --> 00:02:29.260
gebruikt het om persoonlijke zoekresultaten weer te geven.

00:02:29.260 --> 00:02:31.260
Denk er eens een moment over na:

00:02:31.260 --> 00:02:35.260
er is geen standaard Google meer.

00:02:35.260 --> 00:02:38.260
En weet je, het grappige hiervan is dat het moeilijk te zien is.

00:02:38.260 --> 00:02:40.260
Je kunt niet zien hoeveel jouw zoekresultaten verschillen

00:02:40.260 --> 00:02:42.260
van die van iemand anders.

00:02:42.260 --> 00:02:44.260
Een paar weken geleden,

00:02:44.260 --> 00:02:47.260
vroeg ik een stel vrienden "Egypte" te googelen

00:02:47.260 --> 00:02:50.260
en mij screenshots te sturen van wat ze kregen.

00:02:50.260 --> 00:02:53.260
Dus hier is de screenshot van mijn vriend Scott.

00:02:54.260 --> 00:02:57.260
En hier die van mijn vriend Daniel.

00:02:57.260 --> 00:02:59.260
Als je ze naast elkaar zet,

00:02:59.260 --> 00:03:01.260
hoef je niet eens de links te lezen

00:03:01.260 --> 00:03:03.260
om te zien hoe verschillend deze twee pagina's zijn.

00:03:03.260 --> 00:03:05.260
Maar als je de links leest,

00:03:05.260 --> 00:03:08.260
is het echt heel opmerkelijk.

00:03:09.260 --> 00:03:12.260
Daniel kreeg helemaal niets over de protesten in Egypte

00:03:12.260 --> 00:03:14.260
in zijn eerste pagina met Google resultaten.

00:03:14.260 --> 00:03:16.260
Scotts resultaten stonden er vol van.

00:03:16.260 --> 00:03:18.260
En dit was het grote nieuws van de dag op dat moment.

00:03:18.260 --> 00:03:21.260
Dat is hoe verschillend de resultaten kunnen worden.

00:03:21.260 --> 00:03:24.260
Het is niet alleen Google of Facebook.

00:03:24.260 --> 00:03:26.260
Dit is iets dat rondgaat op het Web.

00:03:26.260 --> 00:03:29.260
Er zijn een hele reeks bedrijven die aan dit soort van personalisatie doen.

00:03:29.260 --> 00:03:32.260
Yahoo News, de grootste nieuwssite op het internet,

00:03:32.260 --> 00:03:35.260
is nu gepersonaliseerd - verschillende mensen krijgen verschillende dingen.

00:03:36.260 --> 00:03:39.260
Huffington Post, de Washington Post, de New York Times -

00:03:39.260 --> 00:03:42.260
allen flirten ze met personalisatie op verschillende manieren.

00:03:42.260 --> 00:03:45.260
En dit brengt ons zeer snel

00:03:45.260 --> 00:03:47.260
naar een wereld waarin

00:03:47.260 --> 00:03:51.260
het internet ons toont wat het denkt dat we willen zien,

00:03:51.260 --> 00:03:54.260
maar niet noodzakelijkerwijs wat we nodig hebben.

00:03:54.260 --> 00:03:57.260
Zoals Eric Schmidt zei,

00:03:57.260 --> 00:04:00.260
"Het zal heel moeilijk worden voor mensen om naar iets te kijken

00:04:00.260 --> 00:04:02.260
of te consumeren dat niet op een bepaalde manier

00:04:02.260 --> 00:04:05.260
op maat gemaakt is voor hen."

00:04:05.260 --> 00:04:07.260
Ik denk dat dit een probleem is.

00:04:07.260 --> 00:04:10.260
En ik denk dat, als je al deze filters samenvoegt,

00:04:10.260 --> 00:04:12.260
al deze algoritmen,

00:04:12.260 --> 00:04:15.260
je iets krijgt wat ik een filterbubble noem.

00:04:16.260 --> 00:04:19.260
En je filterbubble is je eigen persoonlijke

00:04:19.260 --> 00:04:21.260
unieke universum aan informatie

00:04:21.260 --> 00:04:23.260
waarin je online leeft.

00:04:23.260 --> 00:04:26.260
En wat er in jouw filterbubble zit

00:04:26.260 --> 00:04:29.260
hangt af van wie je bent en wat je doet.

00:04:29.260 --> 00:04:33.260
Maar zelf kan je niet bepalen wat er in komt.

00:04:33.260 --> 00:04:35.260
En nog belangrijker,

00:04:35.260 --> 00:04:38.260
je ziet niet echt wat er eruit wordt geknipt.

00:04:38.260 --> 00:04:40.260
Een van de problemen van de filterbubble

00:04:40.260 --> 00:04:43.260
werd ontdekt door enkele onderzoekers bij Netflix.

00:04:43.260 --> 00:04:46.260
Ze keken naar de Netflix wachtrijen en ze merkten iets grappigs op

00:04:46.260 --> 00:04:48.260
dat velen van ons waarschijnlijk al opgemerkt hadden,

00:04:48.260 --> 00:04:50.260
dat er een aantal films zijn

00:04:50.260 --> 00:04:53.260
die gewoon opduiken en in onze huizen terecht komen.

00:04:53.260 --> 00:04:56.260
Ze komen in de wachtrij en ritsen er weer onmiddellijk uit.

00:04:56.260 --> 00:04:58.260
"Iron Man" ritst er onmiddellijk uit,

00:04:58.260 --> 00:05:00.260
en "Waiting for Superman"

00:05:00.260 --> 00:05:02.260
kan een hele lange tijd wachten.

00:05:02.260 --> 00:05:04.260
Wat ze ontdekten

00:05:04.260 --> 00:05:06.260
was dat in onze Netflix wachtrijen

00:05:06.260 --> 00:05:09.260
een epische strijd gaande is

00:05:09.260 --> 00:05:12.260
tussen ons toekomstige te bereiken zelf

00:05:12.260 --> 00:05:15.260
en ons meer impulsieve huidige zelf.

00:05:15.260 --> 00:05:17.260
We willen allemaal iemand zijn

00:05:17.260 --> 00:05:19.260
die "Rashomon" gezien heeft

00:05:19.260 --> 00:05:21.260
maar nu

00:05:21.260 --> 00:05:24.260
willen we voor de vierde keer naar "Ace Ventura" kijken.

00:05:24.260 --> 00:05:27.260
(Gelach)

00:05:27.260 --> 00:05:29.260
De beste editing geeft ons een beetje van beide.

00:05:29.260 --> 00:05:31.260
Het geeft ons een beetje Justin Bieber

00:05:31.260 --> 00:05:33.260
en een beetje Afghanistan.

00:05:33.260 --> 00:05:35.260
Het geeft ons wat informatie als groenten,

00:05:35.260 --> 00:05:38.260
het geeft ons wat informatie als dessert.

00:05:38.260 --> 00:05:40.260
En de uitdaging aan deze vormen van algoritmische filters,

00:05:40.260 --> 00:05:42.260
deze gepersonaliseerde filters,

00:05:42.260 --> 00:05:44.260
omdat ze vooral op zoek zijn

00:05:44.260 --> 00:05:48.260
naar waar je het eerst op klikt,

00:05:48.260 --> 00:05:52.260
is dat er geen evenwicht meer is.

00:05:52.260 --> 00:05:55.260
In plaats van een evenwichtig informatiedieet,

00:05:55.260 --> 00:05:57.260
eindig je omgeven

00:05:57.260 --> 00:05:59.260
door informatie junkfood.

00:05:59.260 --> 00:06:01.260
Wat dit suggereert

00:06:01.260 --> 00:06:04.260
is eigenlijk dat we het internetverhaal verkeerd hebben begrepen.

00:06:04.260 --> 00:06:06.260
In een televisie samenleving -

00:06:06.260 --> 00:06:08.260
is dit hoe de oprichters mythologie klinkt -

00:06:08.260 --> 00:06:10.260
In een televisie samenleving -

00:06:10.260 --> 00:06:12.260
waren er poortwachters, de redactie,

00:06:12.260 --> 00:06:15.260
en ze beheersten de stromen van informatie.

00:06:15.260 --> 00:06:18.260
Toen kwam het internet en veegde ze uit de weg,

00:06:18.260 --> 00:06:20.260
en liet ons toe met elkaar te verbinden,

00:06:20.260 --> 00:06:22.260
en het was geweldig.

00:06:22.260 --> 00:06:25.260
Maar dat is eigenlijk niet wat er nu gebeurt.

00:06:26.260 --> 00:06:29.260
Wat we zien is meer een doorgeven van de fakkel

00:06:29.260 --> 00:06:31.260
van menselijke poortwachters

00:06:31.260 --> 00:06:34.260
naar algoritmische.

00:06:34.260 --> 00:06:37.260
En deze algoritmes

00:06:37.260 --> 00:06:40.260
hebben nog niet de geïntegreerde ethiek

00:06:40.260 --> 00:06:43.260
dat de redactie had.

00:06:43.260 --> 00:06:46.260
Dus als algoritmes de wereld voor ons gaan indelen,

00:06:46.260 --> 00:06:49.260
als ze gaan beslissen wat we wel of niet te zien krijgen,

00:06:49.260 --> 00:06:51.260
dan moeten we ervoor zorgen

00:06:51.260 --> 00:06:54.260
dat ze niet alleen ingesteld zijn op relevantie.

00:06:54.260 --> 00:06:56.260
We moeten ervoor zorgen dat zij ons ook dingen laten zien

00:06:56.260 --> 00:06:59.260
die ongemakkelijk of uitdagend of belangrijk zijn -

00:06:59.260 --> 00:07:01.260
dit is wat TED doet -

00:07:01.260 --> 00:07:03.260
andere standpunten.

00:07:03.260 --> 00:07:05.260
We waren hier al eerder

00:07:05.260 --> 00:07:07.260
als samenleving.

00:07:08.260 --> 00:07:11.260
In 1915, het is niet alsof kranten veel zweet lieten

00:07:11.260 --> 00:07:14.260
over hun maatschappelijke verantwoordelijkheden.

00:07:14.260 --> 00:07:16.260
Dan viel het mensen op

00:07:16.260 --> 00:07:19.260
dat ze iets echt belangrijks deden.

00:07:19.260 --> 00:07:21.260
Dat je in feite geen

00:07:21.260 --> 00:07:23.260
functionerende democratie kon hebben

00:07:23.260 --> 00:07:27.260
als burgers geen goede doorstroming van informatie krijgen.

00:07:28.260 --> 00:07:31.260
Dat de kranten kritisch waren omdat zij functioneerden als filter,

00:07:31.260 --> 00:07:33.260
en dan ontwikkelde de journalistieke ethiek zich.

00:07:33.260 --> 00:07:35.260
Het was niet perfect,

00:07:35.260 --> 00:07:38.260
maar we kwamen er de vorige eeuw mee door.

00:07:38.260 --> 00:07:40.260
En dus nu,

00:07:40.260 --> 00:07:43.260
zijn we terug in 1915 op het web.

00:07:44.260 --> 00:07:47.260
De nieuwe poortwachters moeten

00:07:47.260 --> 00:07:49.260
dat soort van verantwoordelijkheid coderen

00:07:49.260 --> 00:07:51.260
in de code die ze schrijven.

00:07:51.260 --> 00:07:54.260
Ik weet dat er hier een heleboel mensen zijn van Facebook en van Google -

00:07:54.260 --> 00:07:56.260
Larry en Sergey --

00:07:56.260 --> 00:07:58.260
mensen die hebben geholpen het Web op te bouwen zoals het is,

00:07:58.260 --> 00:08:00.260
en ik ben er dankbaar voor.

00:08:00.260 --> 00:08:03.260
Maar het is nodig dat jullie ervoor zorgen

00:08:03.260 --> 00:08:06.260
dat in deze algoritmen een gevoel voor het openbare leven

00:08:06.260 --> 00:08:09.260
en maatschappelijke verantwoordelijkheid wordt gecodeerd.

00:08:09.260 --> 00:08:12.260
We hebben het nodig dat jullie ervoor zorgen dat ze transparant genoeg zijn

00:08:12.260 --> 00:08:14.260
dat we kunnen zien wat de regels zijn

00:08:14.260 --> 00:08:17.260
die bepalen wat er door onze filters komt.

00:08:17.260 --> 00:08:19.260
We hebben het nodig dat jullie ons enige controle geven,

00:08:19.260 --> 00:08:21.260
zodat we kunnen beslissen

00:08:21.260 --> 00:08:24.260
wat erdoor komt en wat niet.

00:08:24.260 --> 00:08:26.260
Omdat ik denk dat

00:08:26.260 --> 00:08:28.260
we het nodig hebben dat het internet dat wordt

00:08:28.260 --> 00:08:30.260
waar we allemaal van gedroomd hebben.

00:08:30.260 --> 00:08:33.260
We hebben het nodig om ons allemaal te verbinden.

00:08:33.260 --> 00:08:36.260
We hebben het nodig om ons nieuwe ideeën te introduceren

00:08:36.260 --> 00:08:39.260
en nieuwe mensen en verschillende perspectieven.

00:08:40.260 --> 00:08:42.260
Dat zal niet gebeuren als

00:08:42.260 --> 00:08:45.260
het ons isoleert in een Web van één.

00:08:45.260 --> 00:08:47.260
Dank je wel

00:08:47.260 --> 00:08:58.260
(Applaus)

