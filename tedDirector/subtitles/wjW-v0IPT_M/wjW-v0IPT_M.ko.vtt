WEBVTT
Kind: captions
Language: ko

00:00:00.000 --> 00:00:07.000
번역: Jun Ha Park
검토: Eunyoung Lim

00:00:16.260 --> 00:00:19.260
안녕하세요, 저는 데이빗 한슨 박사입니다. 저는 "마음"이 있는 로봇을 만들죠.

00:00:19.260 --> 00:00:21.260
아 제 말은

00:00:21.260 --> 00:00:23.260
저는 "마음"이 있는 로봇을 개발하고,

00:00:23.260 --> 00:00:26.260
또한 그 로봇이 곧

00:00:26.260 --> 00:00:28.260
감정을 통할수 있게 하려 한다는 것이죠.

00:00:28.260 --> 00:00:30.260
우리는 얼굴을 보고, 시선을 맞추고,

00:00:30.260 --> 00:00:34.260
다양한 표정을 짓고, 언어를 알아들으며,

00:00:34.260 --> 00:00:36.260
감정을 이해하고, 상대방이 누구인지 인지하고,

00:00:36.260 --> 00:00:39.260
함께 관계를 만들어 나갈수 있는,

00:00:39.260 --> 00:00:43.260
대화가 가능하고 감정이 있는 로봇을

00:00:43.260 --> 00:00:46.260
다양한 기술을 이용해 만들고 있습니다.

00:00:46.260 --> 00:00:48.260
저는 로봇이 전보다 더 자연스러운 표정을 만들 수 있는

00:00:48.260 --> 00:00:51.260
일련의 기술을 개발하였고,

00:00:51.260 --> 00:00:53.260
이 기술은 저전력으로도 직립보행이 가능한 로봇인

00:00:53.260 --> 00:00:57.260
최초의 안드로이드를 가능하게 하였습니다.

00:00:57.260 --> 00:00:59.260
사람 얼굴에 있는 주요 근육들을 시뮬레이션하여

00:00:59.260 --> 00:01:01.260
다양한 얼굴 표정을 만들 수 있는데,

00:01:01.260 --> 00:01:03.260
이것은 아주 작은 전지,

00:01:03.260 --> 00:01:05.260
초경량 전지로 구동됩니다.

00:01:05.260 --> 00:01:08.260
전지로 얼굴 표정을 작동할 수 있게 하는 물질은

00:01:08.260 --> 00:01:10.260
'프러버'라 불리는 소재입니다.

00:01:10.260 --> 00:01:12.260
사실 이런 일을 가능하게 하는 그 물질에는

00:01:12.260 --> 00:01:14.260
세가지 주요 혁신 기술이 있습니다.

00:01:14.260 --> 00:01:16.260
하나는 계층적 기공이고,

00:01:16.260 --> 00:01:20.260
또 다른 건 이 재질의 고분자 나노 기공성입니다.

00:01:20.260 --> 00:01:23.260
저기 로봇이 걷기 시작하네요.

00:01:23.260 --> 00:01:26.260
저것은 KAIST에서 만든거죠.

00:01:26.260 --> 00:01:30.260
제가 머리를 만들었습니다. KAIST에서 몸통을 만들었고요.

00:01:30.260 --> 00:01:33.260
여기서의 목표는 이 로봇이 지각력을 갖게 하는 것입니다.

00:01:33.260 --> 00:01:37.260
그뿐만 아니라 감정도 함께요.

00:01:37.260 --> 00:01:39.260
우리는 UC 샌디에고에 있는 '기계지각연구소'와

00:01:39.260 --> 00:01:41.260
함께 연구하고 있습니다.

00:01:41.260 --> 00:01:44.260
이 연구소는 얼굴 표정을 인식하는

00:01:44.260 --> 00:01:46.260
즉, 상대방이 어떤 표정을 짓는지를 인식하는

00:01:46.260 --> 00:01:48.260
매우 놀랄만한 표정 기술을 보유하고 있습니다.

00:01:48.260 --> 00:01:51.260
또한 상대방이 어디를 보는지, 머리의 방향도 인식합니다.

00:01:51.260 --> 00:01:53.260
우리는 주요 얼굴 표정들을 모두 구현하고

00:01:53.260 --> 00:01:55.260
그리고 '캐릭터 엔진'이라 불리는 소프트웨어를 이용하여

00:01:55.260 --> 00:01:57.260
표정을 조종합니다.

00:01:57.260 --> 00:02:01.260
그리고 여기 그 기술이 적용되어있죠.

00:02:01.260 --> 00:02:09.260
여기에 이걸 연결하고, 그리고 이걸 연결합니다.

00:02:09.260 --> 00:02:12.260
자 이제, 이 녀석이 제 표정을 인식하는 지를 봅시다.

00:02:12.260 --> 00:02:17.260
자, 제가 웃어봅니다 :D

00:02:17.260 --> 00:02:19.260
(웃음)

00:02:19.260 --> 00:02:21.260
그리고 찡그려봅니다.

00:02:21.260 --> 00:02:25.260
이거 백라이트가 진짜 강하네요.

00:02:25.260 --> 00:02:27.260
자, 갑니다

00:02:27.260 --> 00:02:29.260
오, 슬퍼하네요.

00:02:29.260 --> 00:02:32.260
자, 웃고, 찡그리고.

00:02:32.260 --> 00:02:34.260
상대방의 감정 상태를 지각하는 능력은

00:02:34.260 --> 00:02:38.260
로봇이 감정을 가지는데 매우 중요한 요솝니다.

00:02:38.260 --> 00:02:41.260
로봇들은 '살인'과 같은 파괴적인

00:02:41.260 --> 00:02:45.260
능력을 가지게 됩니다, 맞죠?

00:02:45.260 --> 00:02:47.260
그런 기계들은 감정이란게 없죠.

00:02:47.260 --> 00:02:49.260
몇십억 달러의 돈이 그런 곳에 쓰이고 있죠.

00:02:49.260 --> 00:02:51.260
캐릭터 로봇공학은 진짜 감정을 가진 로봇을

00:02:51.260 --> 00:02:53.260
만들 '씨앗을 심는' 계기가 될수 있습니다.

00:02:53.260 --> 00:02:55.260
그래서 그 로봇들이 사람 정도의 지능을 가지게 될 때,

00:02:55.260 --> 00:02:59.260
또는 사람보다 너 나은 지능을 가지게 될 때,

00:02:59.260 --> 00:03:02.260
그것이 우리 미래의 '희망'이 될수 있는 '씨앗'일수도 있죠.

00:03:02.260 --> 00:03:06.260
우리는 제가 박사 과정에 있는 지난 8년동안 20개 정도의 로봇을 만들었습니다.

00:03:06.260 --> 00:03:08.260
그리고나서 저는 '한슨 로보틱스'란 회사를 출범시켜,

00:03:08.260 --> 00:03:12.260
이 로봇들을 대량 생산용으로 개발하여 왔습니다.

00:03:12.260 --> 00:03:14.260
이것이 'Wired NextFest'에 몇 년 전에 선보인

00:03:14.260 --> 00:03:16.260
저희가 만든 로봇 중 하나입니다.

00:03:16.260 --> 00:03:19.260
이 녀석은 한 장면에서 여러 명의 사람을 보면

00:03:19.260 --> 00:03:21.260
그 각자가 어디에 있는지 기억하고,

00:03:21.260 --> 00:03:25.260
한 사람씩 기억하면서 살펴봅니다.

00:03:25.260 --> 00:03:27.260
여기에는 두 가지가 수반됩니다.

00:03:27.260 --> 00:03:29.260
하나는, 사람에 대한 지각입니다.

00:03:29.260 --> 00:03:33.260
두번째는 내츄럴 인터페이스,

00:03:33.260 --> 00:03:35.260
즉, 자연스러운 형식의 인터페이스로

00:03:35.260 --> 00:03:38.260
로봇과의 상호작용이 더 직관적입니다.

00:03:38.260 --> 00:03:41.260
우리는 이 녀석이 살아있고, 의식이 있는 것으로 믿기 시작했습니다.

00:03:41.260 --> 00:03:44.260
제 마음에 드는 프로젝트 중 하나는 이 모든 것들을 함께 묶어

00:03:44.260 --> 00:03:47.260
공상과학소설가 Philip K. Dick의

00:03:47.260 --> 00:03:49.260
안드로이드를 예술적으로 표현한 것입니다.

00:03:49.260 --> 00:03:52.260
Philip K. Dick은 '안드로이드는 전기양을 꿈꾸는가?'와 같은 대단한 작품들을 썼고,

00:03:52.260 --> 00:03:54.260
이 책은 영화 "블레이드러너"의 원작입니다.

00:03:54.260 --> 00:03:57.260
이 이야기들에서는 로봇들은 자신들이 사람이라 생각하죠.

00:03:57.260 --> 00:03:59.260
뭐, 생명을 얻는 것과 비슷하기도 해요.

00:03:59.260 --> 00:04:02.260
그래서 우린 그의 글, 편지,

00:04:02.260 --> 00:04:05.260
그의 인터뷰, 서신들을

00:04:05.260 --> 00:04:07.260
수 천 페이지의 거대한 데이터 베이스로 정리하고

00:04:07.260 --> 00:04:09.260
자연어 처리를 이용하여

00:04:09.260 --> 00:04:11.260
그와 대화하는 것을 가능케 했습니다.

00:04:11.260 --> 00:04:13.260
좀 오싹했어요. 그가 꼭 상대방의 말을

00:04:13.260 --> 00:04:16.260
이해해서 대답하는 것 같으니까요.

00:04:16.260 --> 00:04:19.260
이것은 우리가 하고 있는 가장 흥미로운 프로젝트 중 하나로,

00:04:19.260 --> 00:04:22.260
인간-친화적 인공지능, 인간-친화적 기계지능을 가진

00:04:22.260 --> 00:04:25.260
‘Spokesbot’(말하는로봇) 캐릭터입니다.

00:04:25.260 --> 00:04:27.260
우리는 이 녀석을 대량 생산할 것입니다.

00:04:27.260 --> 00:04:30.260
우리가 매우 낮은 가격의 재료를 가지고도

00:04:30.260 --> 00:04:33.260
생산할수 있게 만들어 ($299)

00:04:33.260 --> 00:04:37.260
아이들의 어린시절 친구가 될수 있게 만들려 합니다.

00:04:37.260 --> 00:04:40.260
인터넷과 연결하여 매년 더 똑똑하게 만들죠.

00:04:40.260 --> 00:04:43.260
인공지능은 진화하고 있습니다. 이 녀석의 지능도 말입니다.

00:04:43.260 --> 00:04:45.260
크리스 엔더스: 감사합니다. 정말 놀라웠어요.

00:04:45.260 --> 00:04:52.260
(박수)

