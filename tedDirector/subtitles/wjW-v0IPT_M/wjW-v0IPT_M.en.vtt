WEBVTT
Kind: captions
Language: en

00:00:16.260 --> 00:00:19.260
I'm Dr. David Hanson, and I build robots with character.

00:00:19.260 --> 00:00:21.260
And by that, I mean

00:00:21.260 --> 00:00:23.260
that I develop robots that are characters,

00:00:23.260 --> 00:00:26.260
but also robots that will eventually

00:00:26.260 --> 00:00:28.260
come to empathize with you.

00:00:28.260 --> 00:00:30.260
So we're starting with a variety of technologies

00:00:30.260 --> 00:00:34.260
that have converged into these conversational character robots

00:00:34.260 --> 00:00:36.260
that can see faces, make eye contact with you,

00:00:36.260 --> 00:00:39.260
make a full range of facial expressions, understand speech

00:00:39.260 --> 00:00:43.260
and begin to model how you're feeling

00:00:43.260 --> 00:00:46.260
and who you are, and build a relationship with you.

00:00:46.260 --> 00:00:48.260
I developed a series of technologies

00:00:48.260 --> 00:00:51.260
that allowed the robots to make more realistic facial expressions

00:00:51.260 --> 00:00:53.260
than previously achieved, on lower power,

00:00:53.260 --> 00:00:57.260
which enabled the walking biped robots, the first androids.

00:00:57.260 --> 00:00:59.260
So, it's a full range of facial expressions

00:00:59.260 --> 00:01:01.260
simulating all the major muscles in the human face,

00:01:01.260 --> 00:01:03.260
running on very small batteries,

00:01:03.260 --> 00:01:05.260
extremely lightweight.

00:01:05.260 --> 00:01:08.260
The materials that allowed the battery-operated facial expressions

00:01:08.260 --> 00:01:10.260
is a material that we call Frubber,

00:01:10.260 --> 00:01:12.260
and it actually has three major innovations

00:01:12.260 --> 00:01:14.260
in the material that allow this to happen.

00:01:14.260 --> 00:01:16.260
One is hierarchical pores,

00:01:16.260 --> 00:01:20.260
and the other is a macro-molecular nanoscale porosity in the material.

00:01:20.260 --> 00:01:23.260
There he's starting to walk.

00:01:23.260 --> 00:01:26.260
This is at the Korean Advanced Institute of Science and Technology.

00:01:26.260 --> 00:01:30.260
I built the head. They built the body.

00:01:30.260 --> 00:01:33.260
So the goal here is to achieve sentience in machines,

00:01:33.260 --> 00:01:37.260
and not just sentience, but empathy.

00:01:37.260 --> 00:01:39.260
We're working with the Machine Perception Laboratory

00:01:39.260 --> 00:01:41.260
at the U.C. San Diego.

00:01:41.260 --> 00:01:44.260
They have this really remarkable facial expression technology

00:01:44.260 --> 00:01:46.260
that recognizes facial expressions,

00:01:46.260 --> 00:01:48.260
what facial expressions you're making.

00:01:48.260 --> 00:01:51.260
It also recognizes where you're looking, your head orientation.

00:01:51.260 --> 00:01:53.260
We're emulating all the major facial expressions,

00:01:53.260 --> 00:01:55.260
and then controlling it with the software

00:01:55.260 --> 00:01:57.260
that we call the Character Engine.

00:01:57.260 --> 00:02:01.260
And here is a little bit of the technology that's involved in that.

00:02:01.260 --> 00:02:09.260
In fact, right now -- plug it from here, and then plug it in here,

00:02:09.260 --> 00:02:12.260
and now let's see if it gets my facial expressions.

00:02:12.260 --> 00:02:17.260
Okay. So I'm smiling.

00:02:17.260 --> 00:02:19.260
(Laughter)

00:02:19.260 --> 00:02:21.260
Now I'm frowning.

00:02:21.260 --> 00:02:25.260
And this is really heavily backlit.

00:02:25.260 --> 00:02:27.260
Okay, here we go.

00:02:27.260 --> 00:02:29.260
Oh, it's so sad.

00:02:29.260 --> 00:02:32.260
Okay, so you smile, frowning.

00:02:32.260 --> 00:02:34.260
So his perception of your emotional states

00:02:34.260 --> 00:02:38.260
is very important for machines to effectively become empathetic.

00:02:38.260 --> 00:02:41.260
Machines are becoming devastatingly capable

00:02:41.260 --> 00:02:45.260
of things like killing. Right?

00:02:45.260 --> 00:02:47.260
Those machines have no place for empathy.

00:02:47.260 --> 00:02:49.260
And there is billions of dollars being spent on that.

00:02:49.260 --> 00:02:51.260
Character robotics could plant the seed

00:02:51.260 --> 00:02:53.260
for robots that actually have empathy.

00:02:53.260 --> 00:02:55.260
So, if they achieve human level intelligence

00:02:55.260 --> 00:02:59.260
or, quite possibly, greater than human levels of intelligence,

00:02:59.260 --> 00:03:02.260
this could be the seeds of hope for our future.

00:03:02.260 --> 00:03:06.260
So, we've made 20 robots in the last eight years, during the course of getting my Ph.D.

00:03:06.260 --> 00:03:08.260
And then I started Hanson Robotics,

00:03:08.260 --> 00:03:12.260
which has been developing these things for mass manufacturing.

00:03:12.260 --> 00:03:14.260
This is one of our robots

00:03:14.260 --> 00:03:16.260
that we showed at Wired NextFest a couple of years ago.

00:03:16.260 --> 00:03:19.260
And it sees multiple people in a scene,

00:03:19.260 --> 00:03:21.260
remembers where individual people are,

00:03:21.260 --> 00:03:25.260
and looks from person to person, remembering people.

00:03:25.260 --> 00:03:27.260
So, we're involving two things.

00:03:27.260 --> 00:03:29.260
One, the perception of people,

00:03:29.260 --> 00:03:33.260
and two, the natural interface,

00:03:33.260 --> 00:03:35.260
the natural form of the interface,

00:03:35.260 --> 00:03:38.260
so that it's more intuitive for you to interact with the robot.

00:03:38.260 --> 00:03:41.260
You start to believe that it's alive and aware.

00:03:41.260 --> 00:03:44.260
So one of my favorite projects was bringing all this stuff together

00:03:44.260 --> 00:03:47.260
in an artistic display of an android portrait

00:03:47.260 --> 00:03:49.260
of science-fiction writer Philip K. Dick,

00:03:49.260 --> 00:03:52.260
who wrote great works like, "Do Androids Dream of Electric Sheep?"

00:03:52.260 --> 00:03:54.260
which was the basis of the movie "Bladerunner."

00:03:54.260 --> 00:03:57.260
In these stories, robots often think

00:03:57.260 --> 00:03:59.260
that they're human, and they sort of come to life.

00:03:59.260 --> 00:04:02.260
So we put his writings, letters,

00:04:02.260 --> 00:04:05.260
his interviews, correspondences,

00:04:05.260 --> 00:04:07.260
into a huge database of thousands of pages,

00:04:07.260 --> 00:04:09.260
and then used some natural language processing

00:04:09.260 --> 00:04:11.260
to allow you to actually have a conversation with him.

00:04:11.260 --> 00:04:13.260
And it was kind of spooky, because he would say these things

00:04:13.260 --> 00:04:16.260
that just sounded like they really understood you.

00:04:16.260 --> 00:04:19.260
And this is one of the most exciting projects that we're developing,

00:04:19.260 --> 00:04:22.260
which is a little character that's a spokesbot

00:04:22.260 --> 00:04:25.260
for friendly artificial intelligence, friendly machine intelligence.

00:04:25.260 --> 00:04:27.260
And we're getting this mass-manufactured.

00:04:27.260 --> 00:04:30.260
We specked it out to actually be doable

00:04:30.260 --> 00:04:33.260
with a very, very low-cost bill of materials,

00:04:33.260 --> 00:04:37.260
so that it can become a childhood companion for kids.

00:04:37.260 --> 00:04:40.260
Interfacing with the Internet, it gets smarter over the years.

00:04:40.260 --> 00:04:43.260
As artificial intelligence evolves, so does his intelligence.

00:04:43.260 --> 00:04:45.260
Chris Anderson: Thank you so much. That's incredible.

00:04:45.260 --> 00:04:52.260
(Applause)

