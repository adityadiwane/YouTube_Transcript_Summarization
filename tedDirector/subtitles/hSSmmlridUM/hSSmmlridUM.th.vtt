WEBVTT
Kind: captions
Language: th

00:00:00.000 --> 00:00:07.000
Translator: Teerachart Prasert
Reviewer: Sritala Dhanasarnsombut

00:00:12.559 --> 00:00:16.681
ดิฉันเริ่มทำงานครั้งแรก 
เป็นคอมพิวเตอร์ โปรแกรมเมอร์

00:00:17.283 --> 00:00:18.661
ในปีแรกที่เรียนมหาวิทยาลัย

00:00:18.685 --> 00:00:20.192
ตั้งแต่ยังวัยรุ่น

00:00:20.709 --> 00:00:22.441
เริ่มทำงานเขียนโปรแกรม

00:00:22.465 --> 00:00:24.075
ให้บริษัทได้ไม่นาน

00:00:24.619 --> 00:00:28.254
ผู้จัดการคนหนึ่งในบริษัทนั้น
มาหาดิฉัน

00:00:28.278 --> 00:00:29.546
แล้วกระซิบถามว่า

00:00:30.049 --> 00:00:32.910
"มันจับโกหกผมได้หรือเปล่า?"

00:00:33.626 --> 00:00:35.703
ตอนนั้น ไม่มีคนอื่นอยู่ในห้องอีก

00:00:36.852 --> 00:00:41.241
"ใครที่ไหนจับโกหกคุณได้เหรอ?
แล้วเราจะกระซิบกันทำไม?"

00:00:42.086 --> 00:00:45.193
ผู้จัดการก็ชี้
ไปที่คอมพิวเตอร์

00:00:45.217 --> 00:00:48.313
"มันจับโกหกผมได้หรือเปล่า?"

00:00:49.433 --> 00:00:53.795
พอดีว่า ผู้จัดการคนนั้น
เป็นชู้อยู่กับพนักงานต้อนรับ

00:00:53.819 --> 00:00:54.931
(เสียงหัวเราะ)

00:00:54.955 --> 00:00:56.721
ตอนนั้น ฉันยังเป็นแค่วัยรุ่น

00:00:57.267 --> 00:00:59.286
ฉันเลยกระซิบดัง ๆ กลับไปว่า

00:00:59.310 --> 00:01:02.934
"ใช่ ๆ คอมพิวเตอร์มันจับโกหกได้ค่ะ"

00:01:02.958 --> 00:01:04.764
(เสียงหัวเราะ)

00:01:04.788 --> 00:01:07.711
ดิฉันอำไปขำ ๆ แต่ตอนนี้
ดิฉันกลับขำไม่ออกเสียเอง

00:01:07.735 --> 00:01:11.003
ปัจจุบัน มีระบบคอมพิวเตอร์

00:01:11.027 --> 00:01:14.575
ที่สามารถตรวจจับอารมณ์
หรือกระทั่งคำโกหกได้

00:01:14.599 --> 00:01:16.643
โดยการประมวลผลใบหน้ามนุษย์

00:01:17.068 --> 00:01:21.221
ทั้งบริษัทโฆษณาและรัฐบาล
ต่างก็สนใจอย่างมาก

00:01:22.139 --> 00:01:24.001
ดิฉันทำอาชีพคอมพิวเตอร์ โปรแกรมเมอร์

00:01:24.025 --> 00:01:27.138
เพราะตอนเป็นเด็ก ดิฉันชอบคณิตศาสตร์
และวิทยาศาสตร์มาก ๆ

00:01:27.762 --> 00:01:30.870
แต่พอโตขึ้น
ดิฉันก็ได้รู้เรื่องอาวุธนิวเคลียร์

00:01:30.894 --> 00:01:33.846
ซึ่งทำให้ดิฉันไม่สบายใจเรื่อง
จริยธรรมทางวิทยาศาสตร์เอามาก ๆ

00:01:33.870 --> 00:01:35.074
ดิฉันรู้สึกไม่ดีเลย

00:01:35.098 --> 00:01:37.739
อย่างไรก็ตาม ด้วยความจำเป็นของครอบครัว

00:01:37.763 --> 00:01:41.061
ดิฉันต้องรีบหางานทำให้เร็วที่สุดให้ได้ด้วย

00:01:41.085 --> 00:01:44.384
ดิฉันเลยคิดว่า เอาอาชีพเชิงเทคนิค

00:01:44.408 --> 00:01:46.204
แบบที่ดิฉันจะได้งานง่าย ๆ

00:01:46.228 --> 00:01:50.246
และก็ไม่ต้องมาปวดหัวกับ
ปัญหาจริยธรรมอะไรก็แล้วกัน

00:01:50.842 --> 00:01:52.371
ดิฉันเลยเลือกคอมพิวเตอร์

00:01:52.395 --> 00:01:53.499
(เสียงหัวเราะ)

00:01:53.523 --> 00:01:56.933
ฮ่า ฮ่า ฮ่า
ดิฉันนี่เองกลับขำไม่ออก

00:01:56.957 --> 00:01:59.711
ปัจจุบัน นักวิทยาศาสตร์คอมพิวเตอร์
กำลังสร้างแพลตฟอร์ม

00:01:59.735 --> 00:02:03.944
ที่ควบคุมสื่อซึ่งประชากร
เป็นพันล้านคนดูอยู่ทุกวัน

00:02:04.872 --> 00:02:08.694
พวกเขากำลังสร้างรถ
ที่จะตัดสินใจว่าใครที่จะวิ่งชนได้

00:02:09.527 --> 00:02:12.740
พวกเขาสร้างแม้กระทั่งเครื่องจักร และอาวุธ

00:02:12.764 --> 00:02:15.049
ที่คร่าชีวิตมนุษย์ในสงครามได้

00:02:15.073 --> 00:02:17.844
สิ่งเหล่านี้ ล้วนเกี่ยวกับจริยธรรม

00:02:19.003 --> 00:02:21.061
เครื่องจักร มีสติปัญญาแล้ว

00:02:21.643 --> 00:02:25.117
เราใช้คอมพิวเตอร์คำนวณ
เพื่อตัดสินใจเรื่องสารพัดอย่าง

00:02:25.141 --> 00:02:27.027
รวมถึงการตัดสินใจรูปแบบใหม่ ๆ ด้วย

00:02:27.051 --> 00:02:32.223
เราเริ่มใช้คอมพิวเตอร์
เพื่อตอบคำถามที่ไม่มีคำตอบตายตัว

00:02:32.247 --> 00:02:33.449
คำถามอัตนัย

00:02:33.473 --> 00:02:35.798
ปลายเปิด และต้องใช้มุมมองในการตอบ

00:02:35.822 --> 00:02:37.580
คำถามอย่างเช่น

00:02:37.604 --> 00:02:39.254
"บริษัทควรจ้างใครดี"

00:02:39.916 --> 00:02:42.675
"ควรแสดงอัพเดตของเฟรนด์คนไหนดี"

00:02:42.699 --> 00:02:44.965
"นักโทษคนไหนมีแนวโน้มทำผิดซ้ำอีก"

00:02:45.334 --> 00:02:48.388
"ควรแนะนำข่าวหรือหนังเรื่องอะไรให้คนดูดี"

00:02:48.412 --> 00:02:51.784
ใช่ค่ะ
เราใช้คอมพิวเตอร์กันมาระยะนึงแล้ว

00:02:51.808 --> 00:02:53.325
แต่นี่ไม่เหมือนแต่ก่อน

00:02:53.349 --> 00:02:55.416
นี่คือจุดพลิกผันครั้งประวัติศาสตร์

00:02:55.440 --> 00:03:00.777
เพราะเป็นไปไม่ได้ ที่เราจะใช้คอมพิวเตอร์
มาช่วยแก้ปัญหาเชิงอัตวิสัย

00:03:00.801 --> 00:03:06.221
แบบเดียวกับที่ใช้ช่วยสร้างเครื่องบิน
สร้างสะพาน

00:03:06.245 --> 00:03:07.504
หรือไปดวงจันทร์

00:03:08.269 --> 00:03:11.528
เครื่องบินปลอดภัยขึ้นมั้ย
สะพานจะแกว่งหรือพังมั้ย

00:03:11.552 --> 00:03:16.050
เรื่องแบบนี้ เรามีจุดตรวจสอบ
ที่จับต้อง เห็นตรงกันได้

00:03:16.074 --> 00:03:18.313
และเรามีกฎธรรมชาติช่วยนำทางให้

00:03:18.337 --> 00:03:21.731
แต่เราไม่มีหมุดหมายหรือจุดตรวจสอบเช่นนั้น

00:03:21.755 --> 00:03:25.718
สำหรับการตัดสิน
เรื่องธุระที่ยุ่งเหยิงของมนุษย์

00:03:25.742 --> 00:03:29.979
เรื่องยิ่งยุ่งยากขึ้นอีก
เมื่อซอฟท์แวร์ของเราทรงพลังขึ้น

00:03:30.003 --> 00:03:33.776
แต่ขณะเดียวกัน ก็โปร่งใสลดลง
และซับซ้อนมากขึ้น

00:03:34.362 --> 00:03:36.402
ในทศวรรษที่ผ่านมา

00:03:36.426 --> 00:03:39.155
อัลกอริธึ่มระดับซับซ้อน
ก้าวล้ำรุดหน้าอย่างยิ่ง

00:03:39.179 --> 00:03:41.169
พวกมันตรวจจับใบหน้ามนุษย์ได้

00:03:41.805 --> 00:03:43.860
แกะลายมือได้

00:03:44.256 --> 00:03:46.322
ตรวจหาการโกงบัตรเครดิต

00:03:46.346 --> 00:03:47.535
และบล็อกสแปมได้

00:03:47.559 --> 00:03:49.596
แปลภาษาได้หลากหลาย

00:03:49.620 --> 00:03:52.194
ค้นหาเนื้องอกผ่านระบบฉายภาพได้

00:03:52.218 --> 00:03:54.423
เล่นหมากรุกและโกะชนะมนุษย์ได้

00:03:55.084 --> 00:03:59.588
ความล้ำหน้าเหล่านี้เกิดจากกระบวนการ
"แมคชีน เลิร์นนิ่ง" (Machine Learning)

00:03:59.995 --> 00:04:03.182
แมคชีน เลิร์นนิ่ง ต่างจาก
การเขียนโปรแกรมทั่วไป

00:04:03.206 --> 00:04:06.791
ที่คุณเขียนคำสั่งป้อนคอมพิวเตอร์
อย่างละเอียด เจาะจง และรัดกุม

00:04:07.198 --> 00:04:11.380
กรณีนี้ เป็นเหมือนการป้อนข้อมูลจำนวนมาก
เข้าสู่ระบบ

00:04:11.404 --> 00:04:13.060
ซึ่งรวมถึงข้อมูลที่ไม่มีโครงสร้าง

00:04:13.084 --> 00:04:15.362
แบบที่เกิดขึ้น เวลาเล่นอินเทอร์เนต

00:04:15.386 --> 00:04:18.116
จากนั้น ระบบจะเรียนรู้
โดยประมวลย่อยข้อมูลเหล่านั้น

00:04:18.489 --> 00:04:20.015
และที่สำคัญ

00:04:20.039 --> 00:04:24.419
ระบบพวกนี้ไม่ได้ทำงาน
โดยใช้ชุดตรรกะที่ให้คำตอบตายตัว

00:04:24.443 --> 00:04:27.402
พวกมันจะไม่สร้างคำตอบแบบพื้น ๆ
แต่เป็นคำตอบที่อิงความน่าจะเป็น

00:04:27.426 --> 00:04:30.909
"คำตอบนี้น่าจะเป็นสิ่งที่คุณมองหาอยู่"

00:04:31.843 --> 00:04:34.913
ซึ่งมีด้านดีคือ
วิธีการนี้ทรงพลังเอามาก ๆ

00:04:34.937 --> 00:04:37.013
หัวหน้าระบบเอไอของกูเกิ้ลเรียกมันว่า

00:04:37.037 --> 00:04:39.234
"ประสิทธิผลอย่างไม่น่าเชื่อของดาต้า"

00:04:39.611 --> 00:04:40.964
แต่ด้าบลบก็คือ

00:04:41.558 --> 00:04:44.629
เรายังไม่เข้าใจเลยว่า
อะไรคือสิ่งที่ระบบเรียนรู้

00:04:44.653 --> 00:04:46.240
ซึ่งนี่เองคือจุดเด่นของมัน

00:04:46.766 --> 00:04:50.564
นี่ไม่ใช่การป้อนคำสั่งให้คอมพิวเตอร์

00:04:51.020 --> 00:04:55.084
แต่เป็นเหมือนการฝึกลูกหมา
ที่เผอิญเป็นคอมพิวเตอร์

00:04:55.108 --> 00:04:57.479
เราไม่ได้เข้าใจหรือควบคุมอะไรมันได้เลย

00:04:58.182 --> 00:04:59.733
นี่เองคือปัญหาของเรา

00:05:00.247 --> 00:05:04.509
จะเกิดปัญหาขึ้น เมื่อระบบเอไอ
พวกนี้ตีความข้อมูลผิด

00:05:04.533 --> 00:05:08.073
และจะเกิดปัญหาเช่นกัน
แม้เมื่อมันตีความถูก

00:05:08.097 --> 00:05:11.725
เพราะเราไม่รู้ด้วยซ้ำว่าอะไรผิดหรือถูก
เมื่อพูดถึงปัญหาแบบอัตนัย

00:05:11.749 --> 00:05:14.088
เราไม่รู้ว่าเจ้าสิ่งนี้คิดอะไรอยู่กันแน่

00:05:15.313 --> 00:05:18.996
สมมติว่ามีอัลกอริธึ่มสำหรับจ้างพนักงาน

00:05:19.943 --> 00:05:24.254
ระบบที่ตัดสินใจจ้างพนักงาน
โดยใช้วิธีแมคชีน เลิร์นนิ่ง

00:05:24.872 --> 00:05:28.451
ระบบเช่นนี้ จะถูกฝึกผ่านการป้อนข้อมูล
พนักงานที่เคยจ้างในอดีต

00:05:28.475 --> 00:05:31.066
และถูกสั่งให้ค้นหาและว่าจ้าง

00:05:31.090 --> 00:05:34.128
คนที่มีคุณสมบัติเหมือนพนักงาน
ระดับเยี่ยมที่มีอยู่ในปัจจุบัน

00:05:34.634 --> 00:05:35.787
ฟังดูดี

00:05:35.811 --> 00:05:37.810
ครั้งหนึ่ง ดิฉันเข้าประชุม

00:05:37.834 --> 00:05:40.959
งานเสวนาที่รวมเอา
ผู้จัดการและผู้บริหารด้านทรัพยากรมนุษย์

00:05:40.983 --> 00:05:42.189
คนระดับสูง ๆ

00:05:42.213 --> 00:05:43.772
ที่ใช้ระบบแบบนี้ไว้จ้างงาน

00:05:43.796 --> 00:05:45.442
พวกเขาตื่นเต้นมาก ๆ

00:05:45.466 --> 00:05:50.119
พวกเขาคิดว่า ระบบจะทำให้การจ้างงาน
ตรงตามข้อเท็จจริงมากขึ้น มีอคติน้อยลง

00:05:50.143 --> 00:05:53.143
ทำให้ผู้หญิงและคนกลุ่มน้อย
มีโอกาสได้จ้างดีขึ้น

00:05:53.167 --> 00:05:55.355
เมื่อเทียบกับใช้คนจ้างที่อาจมีอคติ

00:05:55.379 --> 00:05:58.222
การจ้างงานโดยใช้มนุษย์นั้นมีอคติค่ะ

00:05:58.919 --> 00:06:00.104
ฉันรู้ดี

00:06:00.128 --> 00:06:03.133
ช่วงที่ฉันเป็นโปรแกรมเมอร์ใหม่ ๆ

00:06:03.157 --> 00:06:07.025
บางครั้ง หัวหน้าของฉัน
จะมาหาฉัน

00:06:07.049 --> 00:06:10.802
ตอนเช้ามาก ๆ หรือไม่ก็
ตอนบ่ายมาก ๆ

00:06:10.826 --> 00:06:13.888
เธอบอกว่า "เซย์เน็ป
ไปกินข้าวเที่ยงกัน!"

00:06:14.544 --> 00:06:16.711
พอดูเวลา ฉันก็จะงง

00:06:16.735 --> 00:06:18.864
บ่าย 4 ข้าวเที่ยงเหรอ?

00:06:18.888 --> 00:06:21.982
ตอนนั้น ฉันไม่มีเงิน
ฉะนั้น ข้าวเที่ยงฟรี ฉันพร้อมเสมอ

00:06:22.438 --> 00:06:24.505
ต่อมา ดิฉันถึงเข้าใจเบื้องหลัง

00:06:24.529 --> 00:06:29.075
หัวหน้าของฉันไม่ได้
บอกผู้ใหญ่ตามตรงว่า

00:06:29.099 --> 00:06:32.212
โปรแกรมเมอร์ที่พวกเขาจ้าง
มานั้น เป็นแค่วัยรุ่นผู้หญิง

00:06:32.236 --> 00:06:36.166
ที่ใส่กางเกงยีนส์ รองเท้าผ้าใบมาทำงาน

00:06:36.994 --> 00:06:39.196
ดิฉันทำผลงานดี
แต่รูปลักษณ์นั้นไม่ผ่าน

00:06:39.220 --> 00:06:40.919
ไม่ผ่านทั้งอายุ และเพศ

00:06:40.943 --> 00:06:44.289
แน่นอนว่า การจ้างงานโดย
ไม่คำนึงถึงเพศ และเชื้อชาติ

00:06:44.313 --> 00:06:46.178
ถูกใจฉันแน่นอน

00:06:46.851 --> 00:06:50.192
แต่ถ้าเราใช้ระบบนี้
จะมีเรื่องยุ่งยากเพิ่มขึ้นค่ะ

00:06:50.788 --> 00:06:56.579
เพราะในปัจจุบันระบบคอมพิวเตอร์
สามารถคาดเดาเรื่องของคุณได้สารพัด

00:06:56.603 --> 00:06:58.475
จากร่องรอยทางดิจิตอลของคุณ

00:06:58.499 --> 00:07:00.832
ถึงแม้คุณจะไม่เปิดเผย
ข้อมูลเหล่านั้นเลยก็ตาม

00:07:01.326 --> 00:07:04.253
พวกมันคาดเดาได้
ทั้งรสนิยมทางเพศของคุณ

00:07:04.814 --> 00:07:06.120
บุคลิกนิสัย

00:07:06.679 --> 00:07:08.052
แนวโน้มทางการเมือง

00:07:08.650 --> 00:07:12.335
พวกมันสามารถทำนายได้
ด้วยความแม่นยำสูง

00:07:13.182 --> 00:07:15.760
อย่าลืมค่ะ นี่แค่เรื่อง
ที่คุณไม่ได้เปิดเผยเสียด้วยซ้ำ

00:07:15.784 --> 00:07:17.375
เป็นแค่การคาดเดา

00:07:17.399 --> 00:07:20.660
ดิฉันมีเพื่อนที่พัฒนา
ระบบคอมพิวเตอร์แบบนี้

00:07:20.684 --> 00:07:24.325
เพื่อทำนายความน่าจะเป็น
ของการเป็นโรคซึมเศร้ารุนแรงหรือหลังคลอดลูก

00:07:24.349 --> 00:07:25.765
โดยใช้ข้อมูลโซเชียลมีเดีย

00:07:26.496 --> 00:07:27.923
ผลที่ได้ น่าประทับใจมาก

00:07:28.312 --> 00:07:31.669
ระบบของเธอสามารถทำนาย
แนวโน้มการเป็นโรคซึมเศร้า

00:07:31.693 --> 00:07:35.596
ได้ล่วงหน้าหลายเดือนก่อนเกิดอาการขึ้นจริง

00:07:35.620 --> 00:07:36.993
รู้ล่วงหน้าหลายเดือน

00:07:37.017 --> 00:07:39.263
ยังไม่มีอาการ แต่เป็นแค่การทำนาย

00:07:39.287 --> 00:07:44.099
เธอหวังว่าจะใช้ระบบนี้
เพื่อป้องกันโรคแต่เนิ่น ๆ ดีค่ะ!

00:07:44.731 --> 00:07:46.771
ทีนี้ ลองเอาเรื่องนี้มาคิดกับการจ้างคน

00:07:47.847 --> 00:07:50.893
ที่งานประชุมของผู้จัดการทรัพยากรมนุษย์
งานนี้

00:07:50.917 --> 00:07:55.626
ดิฉันพูดคุยกับผู้บริหารระดับสูง
ของบริษัทใหญ่ยักษ์แห่งหนึ่ง

00:07:55.650 --> 00:08:00.228
ดิฉันถามเธอว่า "สมมติว่า
โดยที่คุณไม่ล่วงรู้"

00:08:00.252 --> 00:08:06.801
ระบบคัดคนที่มีแนวโน้ม
เป็นโรคซึมเศร้าในอนาคตสูงออกไป

00:08:07.581 --> 00:08:10.957
ไม่ใช่ว่าซึมเศร้าตอนนี้นะ
แค่อาจจะซึมเศร้าในอนาคต

00:08:11.743 --> 00:08:15.149
ถ้ามันคัดผู้หญิงที่มีแนวโน้มสูง
ว่าจะตั้งครรภ์

00:08:15.173 --> 00:08:17.759
ในอีกปีสองปี
แต่ไม่ได้ตั้งครรภ์ตอนนี้ออกไปล่ะ

00:08:18.664 --> 00:08:24.300
สมมติว่า มันจ้างคนบุคลิกก้าวร้าว
เพราะว่ามันเป็นวัฒนธรรมองค์กรของคุณล่ะ

00:08:24.993 --> 00:08:27.684
เรื่องเหล่านี้ คุณดูไม่ออก
โดยใช้แค่เพศจำแนก

00:08:27.708 --> 00:08:29.210
มันอาจเหมือน ๆ กันก็ได้

00:08:29.234 --> 00:08:32.791
และเพราะว่านี่เป็น แมคชีน เลิร์นนิ่ง
ไม่ใช่การโปรแกรมแบบปกติ

00:08:32.815 --> 00:08:37.722
จึงไม่มีตัวแปรที่ระบุว่า
"ความเสี่ยงซึมเศร้าสูง"

00:08:37.746 --> 00:08:39.579
"ความเสี่ยงตั้งครรภ์สูง"

00:08:39.603 --> 00:08:41.337
"ระดับความก้าวร้าวสูง"

00:08:41.815 --> 00:08:45.494
คุณไม่เพียงไม่รู้ว่า
ระบบเลือกอะไรมา

00:08:45.518 --> 00:08:47.841
คุณไม่รู้ด้วยซ้ำ
ว่าจะเริ่มหาจากตรงไหนดี

00:08:47.865 --> 00:08:49.111
มันเหมือนกล่องดำ

00:08:49.135 --> 00:08:51.942
มันสามารถทำนายได้
แต่เราไม่อาจเข้าใจมันได้

00:08:52.306 --> 00:08:54.675
ฉันถาม "คุณจะใช้วิธีป้องกันใด

00:08:54.699 --> 00:08:58.372
เพื่อให้แน่ใจว่า กล่องดำของคุณ
ไม่ทำอะไรนอกลู่นอกทาง

00:09:00.683 --> 00:09:04.561
เธอมองดิฉันกลับเหมือนดิฉันกำลัง
เหยียบหางลูกหมาอยู่ 10 ตัว

00:09:04.585 --> 00:09:05.833
(เสียงหัวเราะ)

00:09:05.857 --> 00:09:07.898
เธอจ้องดิฉันกลับ และพูดว่า

00:09:08.376 --> 00:09:12.709
"อย่ามาพูดอะไรแบบนี้
ให้ฉันฟังอีกนะคะ"

00:09:13.278 --> 00:09:15.312
แล้วเธอก็หันหน้า เดินหนีไป

00:09:15.884 --> 00:09:17.370
เธอไม่ได้หยาบคายหรอกค่ะ

00:09:17.394 --> 00:09:23.702
แค่ว่า อะไรที่ฉันไม่รู้ ไม่ใช่ปัญหาของฉัน
ไปไกล ๆ ไล่ด้วยสายตา

00:09:23.726 --> 00:09:24.972
(เสียงหัวเราะ)

00:09:25.682 --> 00:09:29.521
ระบบแบบนั้น
อาจมีอคติน้อยกว่า

00:09:29.545 --> 00:09:31.648
ผู้จัดการมนุษย์ในบางด้าน

00:09:31.672 --> 00:09:33.818
และอาจคุ้มค่าทางการเงินด้วยก็จริง

00:09:34.393 --> 00:09:36.043
แต่ก็อาจเป็นต้นเหตุ

00:09:36.067 --> 00:09:40.815
ที่ค่อย ๆ นำไปสู่การกีดกัน
คนที่มีแนวโน้มซึมเศร้า

00:09:40.839 --> 00:09:43.132
ออกจากตลาดแรงงานไปเลย
โดยเราไม่รู้ตัว

00:09:43.573 --> 00:09:46.169
นี่คือสังคมแบบที่เราอยาก
สร้างขึ้น

00:09:46.193 --> 00:09:48.478
โดยที่เราไม่รู้ตัว เพราะเราให้การตัดสินใจ

00:09:48.502 --> 00:09:52.466
อยู่ในมือของเครื่องกลที่เราเอง
ก็ไม่เข้าใจอย่างถ่องแท้อย่างนั้นหรือ

00:09:53.085 --> 00:09:54.543
อีกปัญหาหนึ่งคือ

00:09:55.134 --> 00:09:59.586
ระบบเหล่านี้ มักถูกฝึกฝน
โดยใช้ข้อมูลที่เกิดจากพฤติกรรมของเรา

00:09:59.610 --> 00:10:01.426
พฤติกรรมของมนุษย์

00:10:02.008 --> 00:10:05.816
ฉะนั้น มันอาจรับเอาอคติของเรา

00:10:05.840 --> 00:10:09.433
แล้วก็หยิบอคติเหล่านั้นออกมา

00:10:09.457 --> 00:10:10.770
ขยายให้รุนแรงขึ้น

00:10:10.794 --> 00:10:12.212
แล้วก็แสดงผลกลับมาให้เราดู

00:10:12.236 --> 00:10:13.698
ขณะที่เราพร่ำบอกตัวเองว่า

00:10:13.722 --> 00:10:16.839
"เรากำลังคำนวณอย่างเป็นกลาง
ตามข้อเท็จจริง"

00:10:18.134 --> 00:10:20.811
นักวิจัยพบว่าบนกูเกิ้ล

00:10:21.954 --> 00:10:27.267
โฆษณางานรายได้สูงมีแนวโน้มจะแสดง
ให้ผู้หญิงเห็นน้อยกว่าให้ผู้ชาย

00:10:28.283 --> 00:10:30.813
และถ้าเราเสิร์ชหาชื่อคนแอฟริกัน-อเมริกัน

00:10:30.837 --> 00:10:35.543
ก็มีแนวโน้มที่โฆษณาซึ่งเกี่ยวข้อง
กับประวัติอาชญากรรมจะแสดงขึ้นมา

00:10:35.567 --> 00:10:37.134
แม้ว่าคนนั้น จะไม่มีประวัติเลยก็ตาม

00:10:38.513 --> 00:10:42.062
อคติซ่อนเร้น
และอัลกอริธึ่มกล่องดำแบบนี้เอง

00:10:42.086 --> 00:10:46.059
ซึ่งบางครั้ง นักวิจัยก็เผอิญค้นพบ
แต่บางครั้งก็ไม่อาจรู้ได้

00:10:46.083 --> 00:10:48.744
ที่อาจสร้างผลกระทบ
ถึงขั้นเปลี่ยนชีวิตคนได้

00:10:49.778 --> 00:10:53.937
ในวิสคอนซิน ผู้ต้องหาคนหนึ่ง
ถูกตัดสินจำคุกหกปี

00:10:53.961 --> 00:10:55.316
ข้อหาหลบหนีตำรวจ

00:10:56.644 --> 00:10:57.830
เรื่องนี้คุณอาจไม่รู้

00:10:57.854 --> 00:11:01.852
แต่เราให้อัลกอริธึ่มตัดสินใจเรื่อง
การสั่งทัณฑ์บนหรือจำคุกมากขึ้นเรื่อย ๆ

00:11:01.876 --> 00:11:04.831
ผู้ต้องหาคนนั้นอยากรู้ว่า
คะแนนนี้มีวิธีคำนวณอย่างไร

00:11:05.615 --> 00:11:07.280
อัลกอริธึ่มนีี้เป็นของบริษัทแสวงกำไร

00:11:07.304 --> 00:11:11.509
บริษัทนั้นปฎิเสธที่จะให้
ศาลไต่สวนอัลกอริธึ่มของตนเอง

00:11:12.216 --> 00:11:17.748
แต่ โพรพับลิก้า (ProPublica) หน่วยงานสืบสวนไม่หวังกำไร
ตรวจสอบอัลกอริธึ่มนั้น

00:11:17.772 --> 00:11:19.788
โดยใช้ข้อมูลสาธารณะเท่าที่หาได้

00:11:19.812 --> 00:11:22.128
และพบว่า ผลลัพธ์ที่ได้มีอคติอยู่

00:11:22.152 --> 00:11:25.781
ความสามารถในการทำนายย่ำแย่
ดีกว่าเดาสุ่มแค่นิดเดียว

00:11:25.805 --> 00:11:30.221
มันให้ผลลัพธ์ที่ผิดพลาดว่า
ผู้ต้องหาผิวดำมีแนวโน้มเป็นอาชญากรในอนาคต

00:11:30.245 --> 00:11:34.140
สูงเป็นสองเท่าของผู้ต้องหาผิวขาว

00:11:35.711 --> 00:11:37.275
ลองดูอีกคดีหนึ่ง

00:11:37.923 --> 00:11:41.775
ผู้หญิงคนนี้ ไปรับน้องสาวบุญธรรม
กลับจากโรงเรียน

00:11:41.799 --> 00:11:43.874
ในบรอเวิร์ด เคาน์ตี้ ฟลอริด้า
สายกว่าเวลา

00:11:44.577 --> 00:11:46.933
เธอกับเพื่อน ๆ
วิ่งกันไปตามถนน

00:11:46.957 --> 00:11:51.056
แล้วเจอจักรยานเด็ก
และสกูตเตอร์วางทิ้งไว้

00:11:51.080 --> 00:11:52.712
แล้วก็เอามาใช้อย่างไม่คิด

00:11:52.736 --> 00:11:55.335
ขณะที่กำลังจะปั่นจักรยานไป
ผู้หญิงคนหนึ่งก็ออกมาตะโกนว่า

00:11:55.359 --> 00:11:57.564
"เฮ้ นั่นของลูกฉันนะ"

00:11:57.588 --> 00:12:00.882
พวกเธอเลยทิ้งจักรยาน
รีบเดินหนี แล้วก็ถูกจับกุม

00:12:00.906 --> 00:12:04.543
เธอผิดจริง เธอโง่เขลา
แต่เธอก็อายุแค่ 18 ด้วย

00:12:04.567 --> 00:12:07.111
เธอเคยถูกจับด้วยข้อหาเล็ก ๆ น้อย ๆ มาก่อน

00:12:07.628 --> 00:12:12.813
ส่วนผู้ชายคนนั้น ถูกจับกุม
ข้อหาขโมยของในโฮมดีโป้

00:12:12.837 --> 00:12:15.761
มูลค่า 85 ดอลลาร์
เป็นอาชญากรรมเล็ก ๆ น้อย ๆ เช่นกัน

00:12:16.586 --> 00:12:21.145
แต่เขามีประวัติกรรโชคทรัพย์
มาแล้วถึงสองครั้ง

00:12:21.775 --> 00:12:25.257
แต่อัลกอริธึ่มกลับให้คะแนน
ความเสี่ยงเธอสูง ไม่ใช่เขา

00:12:26.566 --> 00:12:30.440
สองปีต่อมา โปรพับลิก้า พบว่า
เธอไม่ได้กระทำผิดซ้ำอีก

00:12:30.464 --> 00:12:33.014
แต่ด้วยประวัติของเธอ
ทำให้เธอหางานทำได้ยาก

00:12:33.038 --> 00:12:35.114
แต่ผู้ชายนั้น กลับทำความผิดซ้ำ

00:12:35.138 --> 00:12:38.974
และตอนนี้ ต้องโทษจำคุกแปดปี
จากการทำผิดหนหลัง

00:12:39.908 --> 00:12:43.277
ฉะนั้น เราต้องตรวจสอบกล่องดำของเรา

00:12:43.301 --> 00:12:45.916
และไม่ปล่อยให้มันมีอำนาจโดย
ไม่มีการถ่วงดุลเช่นนี้

00:12:45.940 --> 00:12:48.819
(เสียงปรบมือ)

00:12:49.907 --> 00:12:54.149
การตรวจสอบเป็นเรื่องที่ดีและสำคัญ
แต่ก็ไม่ได้แก้ปัญหาได้ทุกเรื่อง

00:12:54.173 --> 00:12:56.921
ลองดูฟีดข่าวทรงอิทธิพล
ของเฟซบุ้กเป็นตัวอย่าง

00:12:56.945 --> 00:13:01.788
อันที่จัดลำดับข่าวสารทุกอย่าง
แล้วตัดสินใจว่าจะโชว์อะไรให้คุณดู

00:13:01.812 --> 00:13:04.096
โดยดูจากเพื่อนและเพจที่คุณติดตามนี่แหละ

00:13:04.718 --> 00:13:06.993
จะโชว์อะไรดีล่ะ รูปเด็กเล็ก

00:13:07.017 --> 00:13:08.213
(เสียงหัวเราะ)

00:13:08.237 --> 00:13:10.833
หรือโน้ตขึงขังที่คนรู้จักเขียน

00:13:11.269 --> 00:13:13.125
หรือจะเป็นข่าวชิ้นสำคัญแต่อ่านยาก

00:13:13.149 --> 00:13:14.631
ไม่มีคำตอบผิดถูก

00:13:14.655 --> 00:13:17.314
เฟซบุ้กให้ความสำคัญกับ
การมีปฏิสัมพันธ์ต่อคอนเทนต์

00:13:17.338 --> 00:13:18.753
ไลค์ แชร์ คอมเมนต์

00:13:19.988 --> 00:13:22.684
ในเดือนสิงหาคม ค.ศ. 2014

00:13:22.708 --> 00:13:25.370
เกิดการประท้วงที่ เฟอร์กูสัน รัฐมิสซูรี่

00:13:25.394 --> 00:13:29.811
หลังจากเหตุการณ์ที่ตำรวจผิวขาว
ฆ่าวัยรุ่นแอฟริกันอเมริกันตาย

00:13:29.835 --> 00:13:31.405
ในสภาพการณ์ที่ีน่าสงสัย

00:13:31.794 --> 00:13:33.801
ข่าวการประท้วงผุดเต็มฟีด

00:13:33.825 --> 00:13:36.510
บนหน้าทวิตเตอร์
ที่ไม่มีอัลกอริธึ่มคัดกรองของดิฉัน

00:13:36.534 --> 00:13:38.484
แต่หาไม่เจอบนเฟซบุ้กเลย

00:13:39.002 --> 00:13:40.736
เพราะเพื่อนบนเฟซบุ้กของฉันหรือ?

00:13:40.760 --> 00:13:42.792
ดิฉันบล็อกอัลกอริธึ่มของเฟซบุ้กเสีย

00:13:43.292 --> 00:13:46.140
ซึ่งทำได้ยาก เพราะเฟซบุ้ก
จะยืนกรานแต่ให้คุณ

00:13:46.164 --> 00:13:48.200
อยู่ใต้บงการของอัลกอริธึ่ม

00:13:48.224 --> 00:13:50.462
แล้วก็พบว่า เพื่อน ๆ ของฉัน
ต่างก็พูดถึงเรื่องนี้

00:13:50.486 --> 00:13:52.995
แต่อัลกอริธึ่มมันไม่ยอม
โชว์ให้ดิฉันดู

00:13:53.019 --> 00:13:56.061
ฉันค้นคว้าเรื่องนี้ต่อ
และพบว่านี่เป็นปัญหาในวงกว้าง

00:13:56.085 --> 00:13:59.898
ข่าวเฟอร์กูสัน
ไม่เป็นมิตรกับอัลกอริธึ่ม

00:13:59.922 --> 00:14:01.093
เพราะมัน "ไม่มีคนไลค์"

00:14:01.117 --> 00:14:02.669
ใครจะไปกดไลค์ล่ะ

00:14:03.320 --> 00:14:05.526
ขนาดคอมเมนต์ยังยากเลย

00:14:05.550 --> 00:14:06.921
เมื่อไม่มีไลค์ หรือคอมเมนต์

00:14:06.945 --> 00:14:10.237
อัลกอริธึ่มจึงมีแนวโน้ม
จะโชว์ให้คนดูน้อยลงไปอีก

00:14:10.261 --> 00:14:11.803
เราเลยไม่เห็นข่าวนี้เลย

00:14:12.766 --> 00:14:13.994
แต่ในสัปดาห์นั้น

00:14:14.018 --> 00:14:16.316
สิ่งที่อัลกอริธึ่มของเฟซบุ้กให้ความสำคัญ

00:14:16.340 --> 00:14:18.566
คือ การท้าราดน้ำแข็งสู้โรค ALS

00:14:18.590 --> 00:14:22.332
จุดประสงค์ดี มีการราดน้ำแข็ง
ได้บริจาคช่วยคน ดีค่ะ

00:14:22.356 --> 00:14:24.260
แต่ข่าวนี้ อัลกอริธึ่มก็จะชอบมาก ๆ ไปเลย

00:14:25.039 --> 00:14:27.652
เครื่องจักรตัดสินใจเรื่องแบบนี้ให้พวกเรา

00:14:27.676 --> 00:14:31.173
การสนทนาประเด็นยาก ๆ
แต่สำคัญมาก ๆ แบบนี้

00:14:31.197 --> 00:14:32.752
อาจถูกกำจัดออก

00:14:32.776 --> 00:14:35.472
หากมีเฟซบุ้กเป็นสื่อเพียงช่องทางเดียว

00:14:35.937 --> 00:14:39.734
และท้ายที่สุด ระบบพวกนี้
ก็อาจผิดพลาด

00:14:39.758 --> 00:14:42.494
ในแบบที่ต่างจากระบบของมนุษย์ไปเลยก็ได้

00:14:42.518 --> 00:14:45.440
พวกคุณคงรู้จักวัตสัน
ระบบสมองประดิษฐ์ของ IBM

00:14:45.464 --> 00:14:48.592
ที่เอาชนะคู่แข่งมนุษย์
แบบขาดลอยในรายการเจพเปอร์ดี้

00:14:48.951 --> 00:14:50.379
มันเล่นเกมเก่งมาก

00:14:50.403 --> 00:14:53.972
แต่ในเจพเปอร์ดี้รอบสุดท้าย
วัตสันต้องตอบคำถามนี้ค่ะ

00:14:54.479 --> 00:14:57.411
"สนามบินที่ใหญ่ที่สุดของมัน
ตั้งชื่อตามวีรบุรุษสงครามโลกครั้งที่ 2

00:14:57.435 --> 00:14:59.687
และเป็นสมรภูมิใหญ่เป็นที่สอง
ในสงครามโลกครั้งที่ 2"

00:14:59.711 --> 00:15:01.089
(ฮัมดนตรีรายการเจพเปอร์ดี้รอบสุดท้าย)

00:15:01.402 --> 00:15:02.584
ชิคาโก้ค่ะ

00:15:02.608 --> 00:15:03.978
มนุษย์สองคนตอบถูกต้อง

00:15:04.517 --> 00:15:08.865
แต่วัตสัน ตอบว่า "โตรอนโต้"

00:15:08.889 --> 00:15:10.707
ทั้งที่คำถามเป็นหมวด เมืองในสหรัฐฯ!

00:15:11.416 --> 00:15:14.317
ระบบที่เก่งกาจ ยังรู้จักพลาดพลั้ง

00:15:14.341 --> 00:15:17.992
ในเรื่องที่มนุษย์ทั่วไป
หรือกระทั้งเด็กประถมไม่มีทางพลาด

00:15:18.643 --> 00:15:21.752
สมองประดิษฐ์อาจทำผิดพลาดได้

00:15:21.776 --> 00:15:24.876
ด้วยพฤติการณ์คนละแบบกับของมนุษย์

00:15:24.900 --> 00:15:27.850
ในรูปแบบที่เราจะคาดไม่ถึง
และไม่ได้เตรียมรับมือ

00:15:27.874 --> 00:15:31.512
การไม่ได้งานทั้งที่คุณ
มีคุณสมบัติพร้อมก็แย่อยู่แล้ว

00:15:31.536 --> 00:15:35.263
แต่คงแย่สุด ๆ ถ้าไม่ได้งาน
เพราะแค่เกิด สแต็ค โอเวอร์โฟลว์

00:15:35.287 --> 00:15:36.719
ในซับรูทีนบางตัว

00:15:36.743 --> 00:15:38.322
(เสียงหัวเราะ)

00:15:38.346 --> 00:15:41.132
เมื่อเดือนพฤษภาคม 2010

00:15:41.156 --> 00:15:45.200
ตลาดหุ้นวอลล์สตรีท เกิดล่มกระทันหัน
เนื่องจากลูปป้อนกลับ

00:15:45.224 --> 00:15:48.252
ในอัลกอริธึ่มของวอลล์สตรีท

00:15:48.276 --> 00:15:52.460
ทำให้เงินในตลาดหายไป
กว่าหนึ่งล้านล้านดอลลาร์ใน 36 นาที

00:15:53.542 --> 00:15:55.729
ดิฉันไม่อยากคิดเลยว่า
"ข้อบกพร่อง" จะเป็นแบบไหน

00:15:55.753 --> 00:15:59.342
ถ้าเกิดเปลี่ยนเป็น
อาวุธอันตรายแบบอัตโนมัติแทน

00:16:01.714 --> 00:16:05.504
มนุษย์ล้วนมีอคติกันทั้งนั้น

00:16:05.528 --> 00:16:07.704
ทั้งผู้ตัดสินใจ และผู้ดูแลกฎ

00:16:07.728 --> 00:16:11.221
ทั้งในศาล ในข่าว และในสงคราม

00:16:11.245 --> 00:16:14.283
พวกเขาผิดพลาดได้ทั้งนั้น
แต่นั่นแหละ คือประเด็นของดิฉัน

00:16:14.307 --> 00:16:17.828
เราหลบเลี่ยงคำถามยาก ๆ แบบนี้ไม่ได้

00:16:18.416 --> 00:16:21.932
เราจะถ่ายโอนความรับผิดชอบ
ของเราเองนี้ไปให้คอมพิวเตอร์ไม่ได้

00:16:22.496 --> 00:16:26.704
(เสียงปรบมือ)

00:16:28.909 --> 00:16:33.356
ปัญญาประดิษฐ์ ไม่ได้ทำให้เรา
เป็นอิสระจากปัญหาเชิงจริยธรรม

00:16:34.562 --> 00:16:37.943
นักวิทยาศาสตร์ข้อมูล เฟรด เบเนนสัน
เรียกเรื่องนี้ว่า แม็ธวอชชิ่ง (math-washing)

00:16:37.967 --> 00:16:39.356
เราต้องทำตรงกันข้าม

00:16:39.380 --> 00:16:44.768
เราต้องส่งเสริมการสีบสวน
ตรวจตรา ตั้งข้อสงสัยในอัลกอริธึ่ม

00:16:45.200 --> 00:16:48.398
เราต้องมั่นใจได้ว่าอัลกอริธึ่มที่ใช้
สามารถตรวจสอบย้อนกลับได้

00:16:48.422 --> 00:16:50.867
ออดิทได้ และมีความโปร่งใสอย่างเป็นรูปธรรม

00:16:51.200 --> 00:16:54.434
เราต้องยอมรับว่า
การเอาคณิตศาสตร์และคอมพิวเตอร์

00:16:54.458 --> 00:16:57.428
มาใช้แก้ปัญหาของมนุษย์ที่กำกวม
และแฝงความเชื่อทางสังคม

00:16:57.452 --> 00:16:59.836
ไม่ได้ทำให้เกิดความเที่ยงตรง
อย่างภววิสัย

00:16:59.860 --> 00:17:03.493
แต่ความซับซ้อนของปัญหามนุษย์
จะเล่นงานอัลกอริธึ่มแทน

00:17:03.968 --> 00:17:07.455
จริงค่ะ คอมพิวเตอร์นั้น
เราใช้ได้ และควรใช้ด้วย

00:17:07.479 --> 00:17:09.493
เพื่อให้เราตัดสินใจได้ดีขึ้น

00:17:09.517 --> 00:17:14.849
แต่เราต้องไม่ลืมความรับผิดชอบทางศีลธรรม
ในการตัดสินใจนั้น

00:17:14.873 --> 00:17:17.691
แล้วใช้อัลกอริธึ่มภายในขอบเขตนั้น

00:17:17.715 --> 00:17:22.650
ไม่ใช่เพื่อปลดเปลื้อง หรือถ่ายโอน
ความรับผิดชอบของเรา

00:17:22.674 --> 00:17:25.128
ที่มีต่อผู้อื่น ในฐานะมนุษย์ด้วยกัน

00:17:25.627 --> 00:17:28.236
เรามีสมองประดิษฐ์ไว้ใช้แล้ว

00:17:28.260 --> 00:17:31.681
นั่นหมายความว่า เรายิ่งต้องยึดมั่น

00:17:31.705 --> 00:17:33.852
ในค่านิยมของมนุษย์ และจริยธรรมของมนุษย์

00:17:33.876 --> 00:17:35.030
ขอบคุณค่ะ

00:17:35.054 --> 00:17:40.074
(เสียงปรบมือ)

