WEBVTT
Kind: captions
Language: pt-PT

00:00:00.000 --> 00:00:07.000
Tradutor: Manuel Cardoso
Revisora: Margarida Ferreira

00:00:13.387 --> 00:00:16.700
Comecei a trabalhar
como programadora informática

00:00:16.733 --> 00:00:18.946
no meu primeiro ano de faculdade

00:00:18.980 --> 00:00:20.649
— basicamente uma adolescente.

00:00:20.709 --> 00:00:22.660
Pouco depois de começar a trabalhar,

00:00:22.712 --> 00:00:24.570
a programar software numa empresa.

00:00:24.619 --> 00:00:28.254
um gestor que trabalhava na empresa

00:00:28.278 --> 00:00:29.974
veio ter comigo e segredou-me:

00:00:30.049 --> 00:00:33.071
"Ele consegue saber se eu estou a mentir?"

00:00:33.626 --> 00:00:35.969
Não havia mais ninguém na sala.

00:00:36.852 --> 00:00:41.526
"Quem poderá saber que você está a mentir?
Porque é que está a segredar?"

00:00:42.086 --> 00:00:45.193
O gestor apontou
para o computador na sala.

00:00:45.302 --> 00:00:48.798
"Ele consegue saber se eu estou a mentir?"

00:00:50.337 --> 00:00:53.795
Aquele gestor tinha
um caso romântico com a rececionista.

00:00:53.923 --> 00:00:55.131
(Risos)

00:00:55.155 --> 00:00:56.939
Eu ainda era uma adolescente.

00:00:57.267 --> 00:00:59.390
Por isso, sussurrei-gritei:

00:00:59.510 --> 00:01:02.934
"Sim, o computador sabe
que você está a mentir."

00:01:03.062 --> 00:01:04.697
(Risos)

00:01:04.788 --> 00:01:07.815
Bem, eu ri-me, mas, na verdade,
hoje riem-se de mim.

00:01:08.580 --> 00:01:11.164
Hoje em dia, há sistemas informáticos

00:01:11.227 --> 00:01:14.775
que conseguem detetar
estados emocionais e mesmo a mentira

00:01:14.818 --> 00:01:16.965
processando apenas os rostos humanos.

00:01:17.068 --> 00:01:21.544
Os publicitários e mesmo os governos
estão muito interessados nesta tecnologia.

00:01:22.139 --> 00:01:24.162
Tornei-me programadora informática

00:01:24.205 --> 00:01:27.214
porque era uma criança
louca por matemática e ciências.

00:01:27.762 --> 00:01:30.965
Mas, ao longo do caminho
descobri as armas nucleares

00:01:31.017 --> 00:01:33.846
e fiquei muito preocupada
com a ética da ciência.

00:01:33.876 --> 00:01:35.274
Fiquei perturbada.

00:01:35.307 --> 00:01:37.996
No entanto, 
devido a circunstâncias familiares,

00:01:38.039 --> 00:01:41.061
também precisava de começar a trabalhar
o mais cedo possível.

00:01:41.189 --> 00:01:44.422
Então pensei:
"Bem, vou escolher uma área técnica

00:01:44.474 --> 00:01:46.423
"onde facilmente consiga um emprego

00:01:46.466 --> 00:01:50.522
"e onde não tenha que lidar
com essas questões incómodas da ética".

00:01:50.842 --> 00:01:52.485
Escolhi a informática.

00:01:52.566 --> 00:01:53.737
(Risos)

00:01:53.780 --> 00:01:57.280
Bem, ha, ha, ha!
Riam-se todos de mim.

00:01:57.371 --> 00:02:00.434
Hoje, os cientistas de informática
estão a construir plataformas

00:02:00.463 --> 00:02:04.210
que controlam o que mil milhões
de pessoas veem todos os dias.

00:02:04.872 --> 00:02:08.922
Estão a desenvolver carros
que podem decidir quem atropelar.

00:02:09.527 --> 00:02:13.129
Estão inclusive a construir
máquinas, armas,

00:02:13.180 --> 00:02:15.268
que poderão matar
seres humanos, em guerras.

00:02:15.339 --> 00:02:17.986
Há ética por todo o lado.

00:02:19.003 --> 00:02:21.337
A inteligência artificial já chegou.

00:02:21.643 --> 00:02:25.117
Estamos a usar a informática
para tomar todo o tipo de decisões,

00:02:25.141 --> 00:02:27.198
mas também novos tipos de decisões.

00:02:27.260 --> 00:02:32.223
Estamos a fazer perguntas
que não têm uma resposta certa,

00:02:32.247 --> 00:02:33.649
que são subjetivas,

00:02:33.696 --> 00:02:36.036
estão em aberto e assentam em valores.

00:02:36.079 --> 00:02:37.770
Fazemos perguntas como:

00:02:38.040 --> 00:02:39.954
"Quem é que a empresa deve contratar?"

00:02:40.039 --> 00:02:42.455
"Que notícias de que amigo
devemos mostrar?"

00:02:42.720 --> 00:02:45.307
"Qual o prisioneiro
que reincidirá mais facilmente?""

00:02:45.334 --> 00:02:48.388
"Que notícia ou filme
deve ser recomendado?"

00:02:48.659 --> 00:02:51.784
Sim, já utilizamos
os computadores há algum tempo,

00:02:51.808 --> 00:02:53.458
mas isto é diferente.

00:02:53.491 --> 00:02:55.539
Esta é uma mudança histórica,

00:02:55.582 --> 00:02:58.658
porque não podemos
apoiar-nos na informática

00:02:58.696 --> 00:03:00.953
para decisões tão subjetivas,

00:03:00.991 --> 00:03:03.387
do mesmo modo que podemos
apoiar-nos na informática

00:03:03.445 --> 00:03:06.511
para pôr aviões no ar,
para construir pontes,

00:03:06.530 --> 00:03:08.180
para voar até à lua.

00:03:08.269 --> 00:03:11.528
Os aviões são mais seguros?
Será que a ponte vai balançar e cair?

00:03:11.647 --> 00:03:16.145
Nestes casos, chegámos a acordo
sobre referências bastante claras,

00:03:16.178 --> 00:03:18.541
tendo as leis da natureza para nos guiar.

00:03:18.594 --> 00:03:22.350
Não temos esses apoios e referências

00:03:22.389 --> 00:03:26.410
para as decisões sobre
os complexos assuntos humanos.

00:03:26.351 --> 00:03:30.074
Para complicar ainda mais,
o software está cada vez mais poderoso,

00:03:30.126 --> 00:03:33.776
mas também está a ficar
menos transparente e mais complexo.

00:03:34.362 --> 00:03:36.563
Recentemente, na última década,

00:03:36.635 --> 00:03:39.155
os algoritmos complexos
alcançaram grandes feitos.

00:03:39.312 --> 00:03:41.445
Conseguem reconhecer rostos humanos.

00:03:41.805 --> 00:03:44.079
Podem decifrar a caligrafia.

00:03:44.256 --> 00:03:47.531
Detetam fraudes de cartões de crédito
e bloqueiam "spam".

00:03:47.578 --> 00:03:49.596
Conseguem traduzir idiomas.

00:03:49.620 --> 00:03:52.194
Conseguem detetar 
tumores em imagens médicas.

00:03:52.218 --> 00:03:54.689
Vencem os humanos no xadrez e no "Go".

00:03:55.084 --> 00:03:57.442
Grande parte deste progresso obteve-se

00:03:57.499 --> 00:03:59.995
com o método chamado
"aprendizagem de máquina."

00:04:00.109 --> 00:04:03.410
A "aprendizagem de máquina"
é diferente da programação tradicional,

00:04:03.458 --> 00:04:07.095
na qual se dá ao computador instruções
detalhadas, meticulosas e exatas.

00:04:07.198 --> 00:04:11.427
Com este novo método, disponibilizam-se
grandes quantidades de dados ao sistema,

00:04:11.470 --> 00:04:13.279
— incluindo dados não estruturados,

00:04:13.303 --> 00:04:15.581
como os que geramos
na nossa vida digital.

00:04:15.614 --> 00:04:18.277
O sistema aprende
analisando esses dados.

00:04:18.489 --> 00:04:20.148
Para além disso,

00:04:20.200 --> 00:04:24.419
estes sistemas não funcionam
sob uma lógica de resposta única.

00:04:24.566 --> 00:04:27.640
Não produzem uma resposta única;
é mais probabilista:

00:04:27.702 --> 00:04:31.147
"Isto, provavelmente,
está mais próximo do que procura."

00:04:31.938 --> 00:04:35.460
A vantagem é que
este método é muito poderoso.

00:04:34.937 --> 00:04:37.574
O chefe de sistemas da IA
do Google, chamou-lhe:

00:04:37.670 --> 00:04:39.529
"A eficácia irracional dos dados".

00:04:39.611 --> 00:04:41.354
A desvantagem é que

00:04:41.558 --> 00:04:44.629
não entendemos concretamente
o que o sistema aprendeu.

00:04:44.653 --> 00:04:46.525
Na verdade, é essa a sua força.

00:04:46.766 --> 00:04:50.792
É menos como dar instruções ao computador

00:04:51.058 --> 00:04:55.236
e mais como treinar
um filhote de máquina-criatura

00:04:55.326 --> 00:04:57.669
que não entendemos nem controlamos.

00:04:58.182 --> 00:05:00.056
Portanto, este é o problema.

00:05:00.247 --> 00:05:04.337
É problemático quando a IA
compreende mal as coisas.

00:05:04.533 --> 00:05:08.311
É também um problema
quando as compreende bem,

00:05:08.344 --> 00:05:12.020
porque nem sabemos o que é o quê
quando se trata de um problema subjetivo.

00:05:12.082 --> 00:05:14.649
Nós não sabemos 
o que a máquina está a pensar.

00:05:15.313 --> 00:05:19.176
Assim, considerem
um algoritmo de contratação

00:05:19.943 --> 00:05:24.377
— um sistema usado para contratar pessoas,
utilizando a aprendizagem de máquina.

00:05:24.872 --> 00:05:28.451
Tal sistema teria sido treinado
com dados dos empregados anteriores

00:05:28.475 --> 00:05:31.389
e instruído para encontrar
e contratar pessoas

00:05:31.427 --> 00:05:34.366
semelhantes aos melhores profissionais
da empresa.

00:05:34.634 --> 00:05:35.686
Parece bem.

00:05:36.010 --> 00:05:37.981
Uma vez, fui a uma conferência

00:05:38.014 --> 00:05:40.893
que juntou gestores
de recursos humanos e executivos.

00:05:40.908 --> 00:05:43.779
Pessoas de alto nível, que usam
esses sistemas para contratar

00:05:43.815 --> 00:05:45.670
Estavam super entusiasmados,

00:05:45.713 --> 00:05:50.223
Achavam que isto tornaria a contratação
mais objetiva, menos tendenciosa,

00:05:50.257 --> 00:05:53.143
e daria mais hipóteses
às mulheres e minorias,

00:05:53.167 --> 00:05:55.526
ao contrário dos gestores
de RH tendenciosos.

00:05:55.569 --> 00:05:58.450
Notem, a contratação humana
é tendenciosa.

00:05:58.919 --> 00:06:00.227
Eu bem sei.

00:06:00.356 --> 00:06:03.371
Num dos meus primeiros empregos
como programadora,

00:06:03.452 --> 00:06:07.177
a minha chefe imediata,
às vezes vinha ter comigo,

00:06:07.220 --> 00:06:10.802
muito cedo de manhã
ou muito ao final da tarde,

00:06:10.826 --> 00:06:13.888
e dizia:
"Zeynep, vamos almoçar!"

00:06:14.544 --> 00:06:16.930
Eu ficava intrigada
com aquele horário estranho.

00:06:16.982 --> 00:06:19.073
São 16 horas. Almoço?

00:06:19.116 --> 00:06:22.277
Como estava sem dinheiro 
— almoço grátis — aceitava sempre.

00:06:22.438 --> 00:06:24.619
Mais tarde percebi porquê:

00:06:24.700 --> 00:06:29.075
Os meus chefes diretos
não tinham informado os superiores

00:06:29.099 --> 00:06:31.622
que o programador contratado
para um trabalho sério

00:06:31.622 --> 00:06:33.439
era uma garota adolescente

00:06:33.439 --> 00:06:36.632
que usava "jeans" e ténis para trabalhar.

00:06:36.708 --> 00:06:39.624
Eu fazia um bom trabalho,
mas tinha um aspeto não convencional

00:06:39.681 --> 00:06:41.347
e tinha a idade e sexo errados.

00:06:41.395 --> 00:06:44.479
Logo, contratar sem olhar
ao sexo e à etnia

00:06:44.513 --> 00:06:46.616
claro que me soa bem.

00:06:46.851 --> 00:06:50.401
Mas com estes sistemas,
é mais complicado. E porquê?

00:06:50.788 --> 00:06:53.422
Atualmente, os sistemas informáticos

00:06:53.460 --> 00:06:56.603
conseguem inferir 
todo o tipo de coisas sobre a pessoa

00:06:56.603 --> 00:06:58.598
a partir das suas migalhas digitais,

00:06:58.641 --> 00:07:01.241
mesmo que a pessoa não tenha
divulgado essas coisas.

00:07:01.326 --> 00:07:04.481
Conseguem inferir 
a sua orientação sexual,

00:07:04.814 --> 00:07:06.529
os seus traços de personalidade,

00:07:06.679 --> 00:07:08.432
as inclinações políticas.

00:07:08.650 --> 00:07:12.335
Conseguem prever
com elevados níveis de precisão.

00:07:13.182 --> 00:07:16.007
Notem... coisas que nem sequer
divulgámos conscientemente.

00:07:16.050 --> 00:07:17.565
Isto é dedução.

00:07:17.618 --> 00:07:20.707
Tenho uma amiga que desenvolveu
esses sistemas informáticos,

00:07:20.760 --> 00:07:24.391
para prever a probabilidade
de depressão clínica ou pós-parto

00:07:24.453 --> 00:07:26.412
a partir de dados de redes sociais.

00:07:26.496 --> 00:07:28.284
Os resultados são impressionantes.

00:07:28.312 --> 00:07:31.669
O sistema consegue prever
a probabilidade de depressão

00:07:31.693 --> 00:07:35.596
meses antes do início
de quaisquer sintomas

00:07:35.620 --> 00:07:36.993
— meses antes.

00:07:37.188 --> 00:07:39.272
Não há sintomas. Há previsão.

00:07:39.648 --> 00:07:44.422
Ela espera que isso seja usado
para intervenção precoce. Ótimo!

00:07:44.731 --> 00:07:47.447
Mas agora coloquem isto
no contexto da contratação.

00:07:47.847 --> 00:07:51.016
Na conferência de gestores
de recursos humanos,

00:07:51.097 --> 00:07:56.511
aproximei-me de uma gestora de alto nível
de uma grande empresa e disse-lhe:

00:07:56.992 --> 00:08:00.370
"O que acha se, sem o seu conhecimento,

00:08:00.423 --> 00:08:07.001
"o seu sistema estiver a excluir pessoas
com alto risco de futura depressão?

00:08:07.581 --> 00:08:11.366
"Não estão deprimidas agora,
mas talvez no futuro, seja mais provável.

00:08:11.743 --> 00:08:15.263
"E se está a excluir as mulheres
com maior probabilidade de engravidar

00:08:15.306 --> 00:08:18.416
"dentro de um ou dois anos
mas que não estão grávidas agora?

00:08:18.664 --> 00:08:24.347
"E se está a contratar pessoas agressivas
porque essa é a cultura da empresa?

00:08:24.802 --> 00:08:27.757
"Não nos apercebemos disso
olhando par a repartição por sexos.

00:08:27.757 --> 00:08:29.548
Aí até pode estar equilibrado.

00:08:29.586 --> 00:08:33.000
Como isto é aprendizagem de máquina,
e não codificação tradicional,

00:08:33.024 --> 00:08:37.607
não há lá nenhuma variável
intitulada "maior risco de depressão",

00:08:37.679 --> 00:08:39.693
"maior risco de gravidez",

00:08:39.726 --> 00:08:41.651
"escala de agressividade".

00:08:41.815 --> 00:08:45.398
Não só não se sabe
o que é que o sistema está a selecionar,

00:08:45.518 --> 00:08:47.840
como não se sabe
por onde começar a procurar.

00:08:47.930 --> 00:08:49.415
É uma caixa preta.

00:08:49.468 --> 00:08:52.236
Tem poder preditivo,
mas não conseguimos entendê-la.

00:08:52.306 --> 00:08:55.460
"Que garantia você tem", perguntei,

00:08:55.790 --> 00:08:58.600
"de que a sua caixa negra
não está a fazer algo sombrio?"

00:09:00.683 --> 00:09:04.503
Ela olhou para mim
como se eu a tivesse ofendido.

00:09:04.585 --> 00:09:05.833
(Risos)

00:09:05.857 --> 00:09:08.155
Olhou para mim e disse:

00:09:08.376 --> 00:09:12.756
"Não quero ouvir
nem mais uma palavra sobre isso."

00:09:13.128 --> 00:09:15.682
Virou-me as costas e foi-se embora.

00:09:15.884 --> 00:09:18.512
Ela não foi indelicada. 
Foi claramente:

00:09:18.660 --> 00:09:23.702
"O que eu não sei, não é problema meu.
Desapareça." Olhar de morte.

00:09:23.973 --> 00:09:25.219
(Risos)

00:09:25.682 --> 00:09:29.521
Notem que estes sistemas
podem ser menos tendenciosos

00:09:29.545 --> 00:09:31.743
do que os gestores humanos,
nalguns aspetos.

00:09:31.795 --> 00:09:34.255
E isso pode fazer sentido economicamente.

00:09:34.393 --> 00:09:36.309
Mas também pode levar

00:09:36.357 --> 00:09:40.815
ao fecho constante mas furtivo
do mercado de trabalho

00:09:40.839 --> 00:09:43.332
para as pessoas
com maior risco de depressão.

00:09:43.573 --> 00:09:46.169
Será este o tipo de sociedade
que queremos construir,

00:09:46.193 --> 00:09:48.478
sem sequer saber que o fizemos,

00:09:48.502 --> 00:09:51.151
porque demos às máquinas
a tomada de decisões

00:09:51.151 --> 00:09:53.151
que não compreendemos totalmente?

00:09:53.285 --> 00:09:54.933
Outro problema é o seguinte:

00:09:55.134 --> 00:09:59.586
estes sistemas são treinados frequentemente
com dados gerados pelas nossas ações,

00:09:59.610 --> 00:10:01.663
impressões humanas.

00:10:02.008 --> 00:10:05.816
Assim, poderão estar
a refletir os nossos preconceitos.

00:10:05.840 --> 00:10:09.433
poderão estar 
a aprender os nossos preconceitos,

00:10:09.457 --> 00:10:12.227
a amplificá-los e a mostrá-los de novo,

00:10:12.236 --> 00:10:14.500
enquanto nós pensamos:

00:10:13.922 --> 00:10:16.762
"Estamos a ser objetivos,
a informática é neutra".

00:10:18.134 --> 00:10:21.163
Investigadores verificaram que, no Google,

00:10:21.954 --> 00:10:25.083
os anúncios para empregos
com salários elevados

00:10:25.083 --> 00:10:28.092
aparecem mais para homens
do que para mulheres.

00:10:28.283 --> 00:10:31.600
E se procurarmos nomes de afro-americano

00:10:30.951 --> 00:10:35.543
encontramos mais resultados de anúncios
sugerindo antecedentes criminais,

00:10:35.567 --> 00:10:37.553
mesmo quando não há nenhum.

00:10:38.513 --> 00:10:42.062
Estes preconceitos escondidos
e algoritmos de caixa-negra

00:10:42.086 --> 00:10:46.163
que os investigadores por vezes descobrem,
mas que por vezes nós não descobrimos,

00:10:46.216 --> 00:10:48.944
podem ter consequências
capazes de mudar uma vida.

00:10:49.778 --> 00:10:54.051
No Wisconsin, um réu
foi condenado a seis anos de prisão

00:10:54.094 --> 00:10:55.782
por fugir da polícia.

00:10:56.644 --> 00:10:57.944
Podem não saber,

00:10:58.006 --> 00:10:59.618
mas os algoritmos são utilizados

00:10:59.656 --> 00:11:02.209
para a liberdade condicional 
e as condenações.

00:11:02.304 --> 00:11:05.288
Este réu queria saber
como era calculada a pontuação.

00:11:05.615 --> 00:11:07.499
É uma caixa preta comercial.

00:11:07.532 --> 00:11:11.956
A empresa recusou-se a ver o seu algoritmo
questionado em tribunal aberto.

00:11:12.216 --> 00:11:16.133
Mas a ProPublica, uma agência
de investigação sem fins lucrativos,

00:11:16.181 --> 00:11:19.762
auditou esse algoritmo
com os dados públicos que encontrou,

00:11:19.812 --> 00:11:22.594
e descobriu que os resultados
eram tendenciosos

00:11:22.642 --> 00:11:25.781
e o poder de previsão
era pouco melhor que o acaso.

00:11:26.140 --> 00:11:30.221
E que estava a sinalizar erradamente
réus negros como futuros criminosos

00:11:30.245 --> 00:11:34.140
duas vezes mais do que réus brancos.

00:11:35.711 --> 00:11:37.703
Considerem o seguinte caso:

00:11:37.923 --> 00:11:41.775
Esta mulher estava atrasada
para ir buscar a afilhada

00:11:42.081 --> 00:11:44.350
a uma escola no condado
de Broward, Flórida

00:11:44.577 --> 00:11:46.933
e corria pela rua abaixo com uma amiga.

00:11:47.052 --> 00:11:51.056
Avistaram uma bicicleta e uma scooter,
não amarradas, numa varanda

00:11:51.080 --> 00:11:53.007
e, parvoíce... levaram-nas.

00:11:53.031 --> 00:11:55.715
Quando estavam a acelerar,
apareceu uma mulher que disse:

00:11:55.792 --> 00:11:58.590
"Ei! Essa bicicleta é do meu filho!"

00:11:57.940 --> 00:12:01.024
Largaram-nas e seguiram caminho,
mas acabaram por ser presas.

00:12:01.096 --> 00:12:04.666
Foi errado, foi uma parvoíce,
mas ela só tinha 18 anos.

00:12:04.767 --> 00:12:07.272
Tinha alguns delitos juvenis.

00:12:07.628 --> 00:12:09.617
Entretanto, aquele homem

00:12:09.665 --> 00:12:12.932
já tinha sido preso
por furto no Home Depot

00:12:12.979 --> 00:12:16.046
— um roubo de 85 dólares,
um crime menor.

00:12:16.586 --> 00:12:21.554
Mas, para além disso, já tinha
duas sentenças por assalto à mão armada.

00:12:21.775 --> 00:12:25.523
Mas o algoritmo marcou-a
como de alto risco, e não a ele.

00:12:26.566 --> 00:12:30.163
Dois anos depois, a ProPublica verificou
que ela não tinha reincidido,

00:12:30.178 --> 00:12:33.309
mas tinha dificuldade em conseguir
um emprego, devido ao cadastro.

00:12:33.342 --> 00:12:35.294
Ele, por outro lado, reincidiu

00:12:35.328 --> 00:12:39.412
e agora está a cumprir pena de oito anos
por um crime cometido mais tarde.

00:12:39.908 --> 00:12:43.277
Necessitamos, claramente, 
de auditar as nossas caixas-negras

00:12:43.301 --> 00:12:45.811
e não deixá-las ter
este poder sem controlo.

00:12:46.035 --> 00:12:48.587
(Aplausos)

00:12:49.907 --> 00:12:54.149
As auditorias são ótimas e importantes,
mas não solucionam todos os problemas.

00:12:54.211 --> 00:12:57.025
Considerem o poderoso algoritmo
de notícias do Facebook,

00:12:57.078 --> 00:13:01.026
aquele que classifica
e decide tudo o que vos mostram

00:13:01.421 --> 00:13:04.096
de todos os amigos e das páginas
que vocês seguem.

00:13:04.718 --> 00:13:06.993
Deverão mostrar-vos
outra foto de um bebé?

00:13:07.055 --> 00:13:08.289
(Risos)

00:13:08.370 --> 00:13:11.061
Uma nota mal-humorado de um conhecido?

00:13:11.269 --> 00:13:13.229
Uma notícia importante, 
mas incomodativa?

00:13:13.282 --> 00:13:14.920
Não há uma resposta certa.

00:13:15.260 --> 00:13:17.637
O Facebook otimiza
para manter-vos ligado ao site:

00:13:17.680 --> 00:13:19.476
"likes", partilhas, comentários.

00:13:19.578 --> 00:13:22.684
Em agosto de 2014,

00:13:22.708 --> 00:13:25.370
estalaram manifestações
em Ferguson, no Missouri,

00:13:25.394 --> 00:13:29.811
após a morte de um jovem afro-americano
infligida por um polícia branco,

00:13:29.835 --> 00:13:31.690
em circunstâncias pouco claras.

00:13:31.794 --> 00:13:34.277
As notícias dos protestos
apareceram de todos o lados

00:13:34.324 --> 00:13:36.967
na minha conta de Twitter
sem filtro de algoritmos,

00:13:36.991 --> 00:13:38.998
mas em parte alguma no meu Facebook.

00:13:39.045 --> 00:13:40.974
Foram os meus amigos no Facebook?

00:13:41.017 --> 00:13:43.163
Desativei o algoritmo do Facebook

00:13:43.196 --> 00:13:46.349
— o que é difícil, pois o Facebook
insiste em voltar a incluir-nos

00:13:46.392 --> 00:13:48.276
sob o controlo do algoritmo —

00:13:48.319 --> 00:13:50.595
e vi que os meus amigos falavam disso.

00:13:50.628 --> 00:13:52.995
O algoritmo é que
não me mostrava essa informação.

00:13:53.019 --> 00:13:56.700
Fiz pesquisas e verifiquei
que era um problema generalizado.

00:13:56.085 --> 00:13:59.793
A história de Ferguson
não agradava ao algoritmo.

00:13:59.980 --> 00:14:01.350
Não era "simpática".

00:14:01.383 --> 00:14:03.183
Quem ia clicar em "Gosto"?"

00:14:03.320 --> 00:14:05.526
Nem é fácil de comentar.

00:14:05.550 --> 00:14:07.263
Sem "gosto" e sem comentários,

00:14:07.301 --> 00:14:10.237
o algoritmo, provavelmente,
mostrá-la-ia a menos pessoas,

00:14:10.261 --> 00:14:12.279
e assim nós não a conseguimos ver.

00:14:12.766 --> 00:14:14.194
Em vez disso, nessa semana,

00:14:14.236 --> 00:14:16.516
o algoritmo do Facebook destacava isto,

00:14:16.559 --> 00:14:18.756
o Desafio do Balde de Água Gelada.

00:14:18.790 --> 00:14:22.465
Uma causa digna: despejar água gelada;
doar para a caridade. Tudo bem.

00:14:22.517 --> 00:14:25.600
Mas, para o algoritmo, era super amigável.

00:14:25.039 --> 00:14:27.490
A máquina tomou essa decisão por nós.

00:14:27.676 --> 00:14:31.173
Uma conversa muito importante
mas também difícil

00:14:31.197 --> 00:14:32.952
poderia ter sido abafada,

00:14:33.004 --> 00:14:35.805
se o Facebook fosse o único canal.

00:14:35.937 --> 00:14:39.734
Finalmente, estes sistemas
também podem cometer erros

00:14:39.758 --> 00:14:42.494
diferentes dos erros dos sistemas humanos.

00:14:42.518 --> 00:14:45.525
Lembram-se do Watson,
a máquina de IA da IBM

00:14:45.559 --> 00:14:48.792
que eliminou os concorrentes
humanos no Jeopardy?

00:14:48.951 --> 00:14:50.531
Foi um grande jogador.

00:14:50.574 --> 00:14:53.972
Mas depois, na final do Jeopardy,
quando perguntaram ao Watson:

00:14:54.355 --> 00:14:57.411
"O maior aeroporto com o nome
de um herói da II Guerra Mundial,

00:14:57.435 --> 00:15:00.639
"o segundo maior com o nome
de uma batalha da II Guerra Mundial."

00:15:01.525 --> 00:15:02.717
Chicago.

00:15:02.788 --> 00:15:04.416
Os dois seres humanos acertaram.

00:15:04.517 --> 00:15:08.693
Watson, por outro lado,
respondeu "Toronto"

00:15:08.889 --> 00:15:11.287
— para uma pergunta sobre cidades dos EUA!

00:15:11.416 --> 00:15:14.469
O impressionante sistema
também cometeu um erro

00:15:14.531 --> 00:15:18.411
que um ser humano nunca iria fazer,
que uma criança do 1.º ciclo não faria.

00:15:18.643 --> 00:15:21.771
A inteligência artificial pode falhar

00:15:21.795 --> 00:15:25.066
de formas que não se encaixam
nos padrões de erro dos seres humanos.

00:15:25.109 --> 00:15:27.850
de formas inesperadas e imprevistas.

00:15:27.978 --> 00:15:31.512
Seria péssimo não conseguir um emprego
para o qual estamos qualificados,

00:15:31.536 --> 00:15:35.263
mas seria três vezes pior
se fosse por causa de um erro

00:15:35.287 --> 00:15:37.242
de processamento de alguma sub-rotina.

00:15:37.295 --> 00:15:38.512
(Risos)

00:15:38.574 --> 00:15:41.132
Em maio de 2010,

00:15:41.156 --> 00:15:45.380
um acidente relâmpago em Wall Street
provocado por uma auto alimentação

00:15:45.443 --> 00:15:48.413
no algoritmo de "vender", em Wall Street,

00:15:48.447 --> 00:15:52.660
fez perder um bilião de dólares
em 36 minutos.

00:15:53.542 --> 00:15:56.620
Eu nem quero pensar
o que significa "erro"

00:15:55.943 --> 00:15:59.465
no contexto de armas mortais autónomas.

00:16:01.714 --> 00:16:05.504
Sim. Os seres humanos
sempre alimentaram preconceitos.

00:16:05.528 --> 00:16:07.780
Quem toma decisões e controla

00:16:08.120 --> 00:16:12.097
nos tribunais, nas notícias, na guerra ...
comete erros.

00:16:12.178 --> 00:16:14.444
É esse exatamente o meu ponto.

00:16:14.507 --> 00:16:18.008
Nós não podemos fugir
destas perguntas difíceis.

00:16:18.416 --> 00:16:22.008
Não podemos atribuir 
as nossas responsabilidades às máquinas.

00:16:23.067 --> 00:16:26.218
(Aplausos)

00:16:28.909 --> 00:16:33.356
A inteligência artificial não nos dá
um cartão "Liberte-se da ética".

00:16:34.562 --> 00:16:37.943
O cientista de dados Fred Benenson
chama-lhe "lavagem de matemática".

00:16:37.967 --> 00:16:39.660
Precisamos é do oposto.

00:16:39.732 --> 00:16:44.815
Temos de cultivar algoritmos de suspeita,
de análise e de investigação.

00:16:45.085 --> 00:16:48.607
Precisamos de garantir que assumimos
a responsabilidade dos algoritmos,

00:16:48.650 --> 00:16:51.019
da auditoria e da transparência relevante.

00:16:51.200 --> 00:16:54.595
Precisamos de aceitar
que trazer a matemática e a informática

00:16:54.600 --> 00:16:57.570
para os assuntos humanos,
complicados e carregados de valores,

00:16:57.613 --> 00:16:59.836
não nos dá objetividade.

00:17:00.021 --> 00:17:03.711
Em vez disso, a complexidade
dos assuntos humanos invade os algoritmos.

00:17:03.968 --> 00:17:07.455
Sim, podemos e devemos usar a informática

00:17:07.479 --> 00:17:09.578
para nos ajudar a tomar melhores decisões.

00:17:09.650 --> 00:17:14.849
Mas temos que assumir 
a responsabilidade moral do julgamento.

00:17:14.873 --> 00:17:17.691
e usar os algoritmos nesse âmbito,

00:17:17.715 --> 00:17:22.649
não como um meio para abdicar
e subcontratar a nossa responsabilidade

00:17:22.674 --> 00:17:25.299
de ser humano para ser humano.

00:17:25.627 --> 00:17:28.235
A inteligência artificial já chegou.

00:17:28.260 --> 00:17:31.681
O que significa que mais que nunca
temos de nos agarrar afincadamente

00:17:31.705 --> 00:17:33.851
aos valores e à ética humana.

00:17:33.971 --> 00:17:35.106
Obrigada.

00:17:35.176 --> 00:17:38.273
(Aplausos)

