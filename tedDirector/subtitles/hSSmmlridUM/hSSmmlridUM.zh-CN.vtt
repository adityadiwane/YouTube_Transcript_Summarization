WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:07.000
翻译人员: Yangyang Liu
校对人员: Junyi Sha

00:00:12.559 --> 00:00:16.681
我的第一份工作是程序员，

00:00:16.705 --> 00:00:18.661
那是在我刚上大学的时候，

00:00:18.685 --> 00:00:20.192
不到二十岁。

00:00:20.709 --> 00:00:22.441
我刚开始工作不久，

00:00:22.465 --> 00:00:24.075
正当在公司写程序，

00:00:24.619 --> 00:00:28.254
公司的一位经理来到我旁边，

00:00:28.278 --> 00:00:29.546
他悄悄的对我说，

00:00:30.049 --> 00:00:32.910
“他能看出来我在撒谎吗？”

00:00:33.626 --> 00:00:36.483
当时屋子里没有别人。

00:00:36.852 --> 00:00:41.951
“你是指谁能看出你在撒谎？
还有，我们干嘛要悄悄地说话？”

00:00:42.086 --> 00:00:45.193
那个经理指着屋子里的电脑，说：

00:00:45.217 --> 00:00:48.313
“他能看出我在撒谎吗？”

00:00:49.433 --> 00:00:53.795
其实，那个经理和前台有一腿。

00:00:53.819 --> 00:00:54.931
（笑声）

00:00:54.955 --> 00:00:56.721
当时我只有十来岁，

00:00:57.267 --> 00:00:59.286
我低声地回答他，

00:00:59.310 --> 00:01:02.934
“是的，电脑什么都知道。”

00:01:02.958 --> 00:01:04.764
（笑声）

00:01:04.788 --> 00:01:07.711
我笑了，但其实我是在笑自己，

00:01:07.735 --> 00:01:11.003
现在，计算机系统已经可以

00:01:11.027 --> 00:01:14.039
通过分析人脸来辨别人的情绪，

00:01:14.039 --> 00:01:16.643
甚至包括是否在撒谎。

00:01:17.068 --> 00:01:21.221
广告商，甚至政府
都对此很感兴趣。

00:01:22.139 --> 00:01:24.001
我选择成为电脑程序员，

00:01:24.025 --> 00:01:27.138
因为我是那种
痴迷于数学和科学孩子。

00:01:27.762 --> 00:01:30.870
其间我也学习过核武器，

00:01:30.894 --> 00:01:33.846
我也非常关心科学伦理。

00:01:33.870 --> 00:01:35.074
我曾经很困惑。

00:01:35.098 --> 00:01:37.739
但是，因为家庭原因，

00:01:37.763 --> 00:01:41.061
我需要尽快参加工作。

00:01:41.085 --> 00:01:44.384
我对自己说，嘿，
选一个容易找工作

00:01:44.408 --> 00:01:46.204
的科技领域吧，

00:01:46.228 --> 00:01:50.246
并且找个不需要
操心伦理问题的。

00:01:50.842 --> 00:01:52.371
所以我选了计算机。

00:01:52.395 --> 00:01:53.499
（笑声）

00:01:53.523 --> 00:01:56.933
哈哈哈，
我多可笑。

00:01:56.957 --> 00:01:59.711
如今，计算机科学控制着

00:01:59.735 --> 00:02:03.944
十亿人每天能看到的信息，

00:02:04.872 --> 00:02:08.694
它们可以控制汽车朝哪里开，

00:02:09.527 --> 00:02:12.740
它们可以建造机器、武器，

00:02:12.764 --> 00:02:15.049
那些在战争中
用于杀人的武器。

00:02:15.073 --> 00:02:17.844
说到底，
都是伦理问题。

00:02:19.003 --> 00:02:21.061
机器智能来了。

00:02:21.643 --> 00:02:25.117
我们用计算机来做各种决策，

00:02:25.141 --> 00:02:27.027
包括人们面临的新决策。

00:02:27.051 --> 00:02:32.223
我们向计算机询问多解的、

00:02:32.247 --> 00:02:33.449
主观的、

00:02:33.473 --> 00:02:35.798
开放性的或意义深远的问题。

00:02:35.822 --> 00:02:37.580
我们会问，

00:02:37.604 --> 00:02:39.834
“我们公司应该聘请谁？”

00:02:39.916 --> 00:02:42.675
“你该关注哪个朋友
的哪条状态？”

00:02:42.699 --> 00:02:44.965
“哪种犯罪更容易再犯？”

00:02:45.334 --> 00:02:48.388
“应该给人们推荐
哪条新闻或是电影？”

00:02:48.412 --> 00:02:51.784
看，是的，我们使用计算机
已经有一段时间了，

00:02:51.808 --> 00:02:53.325
但现在不一样了。

00:02:53.349 --> 00:02:55.416
这是历史性的转折，

00:02:55.440 --> 00:03:00.777
因为我们在这些主观决策上
无法主导计算机，

00:03:00.801 --> 00:03:04.835
不像我们在
管理飞机、建造桥梁、

00:03:04.835 --> 00:03:08.014
登月等问题上，
可以主导它们。

00:03:08.269 --> 00:03:11.528
飞机会更安全吗？
桥梁会摇晃或倒塌吗？

00:03:11.552 --> 00:03:16.050
在这些问题上，我们
有统一而清晰的判断标准，

00:03:16.074 --> 00:03:18.313
我们有自然定律来指导。

00:03:18.337 --> 00:03:21.731
但是在复杂的人类事务上，

00:03:21.755 --> 00:03:25.718
我们没有这样的客观标准。

00:03:25.742 --> 00:03:29.979
让问题变得更复杂的，
是我们的软件正越来越强大，

00:03:30.003 --> 00:03:33.776
同时也变得更加不透明，
更加复杂。

00:03:34.362 --> 00:03:36.402
最近的几十年，

00:03:36.426 --> 00:03:39.155
复杂算法已
取得了长足发展，

00:03:39.179 --> 00:03:41.169
它们可以识别人脸，

00:03:41.805 --> 00:03:43.860
它们可以破解笔迹，

00:03:44.256 --> 00:03:46.322
它们可以识别信用卡欺诈，

00:03:46.346 --> 00:03:47.535
可以屏蔽垃圾信息，

00:03:47.559 --> 00:03:49.596
它们可以翻译语言，

00:03:49.620 --> 00:03:52.194
他们可以通过
医学图像识别肿瘤，

00:03:52.218 --> 00:03:55.083
它们可以在国际象棋
和围棋上击败人类。

00:03:55.084 --> 00:03:59.588
类似的很多发展，
都来自一种叫“机器学习”的方法。

00:03:59.995 --> 00:04:03.182
机器学习不像传统程序一样，

00:04:03.206 --> 00:04:06.791
需要给计算机详细、
准确的逐条指令。

00:04:07.198 --> 00:04:11.380
它更像是你给系统
喂了很多数据，

00:04:11.404 --> 00:04:13.060
包括非结构化数据，

00:04:13.084 --> 00:04:15.362
比如我们在数字生活中
产生的数据。

00:04:15.386 --> 00:04:18.116
系统扎进这些数据中学习，

00:04:18.489 --> 00:04:20.015
重要的是，

00:04:20.039 --> 00:04:24.419
这些系统不再局限单一答案。

00:04:24.443 --> 00:04:27.402
他们得出的不是一个
简单的答案，而是概率性的：

00:04:27.426 --> 00:04:30.909
“这个更像是你在寻找的。”

00:04:31.843 --> 00:04:34.913
它的优势是：
它真的非常强大。

00:04:34.937 --> 00:04:37.013
Google 人工智能系统的
负责人称它为：

00:04:37.037 --> 00:04:39.234
“不可思议的数据效率”。

00:04:39.611 --> 00:04:40.964
缺点在于，

00:04:41.558 --> 00:04:44.629
我们无法清楚的了解
系统学到了什么，

00:04:44.653 --> 00:04:46.780
事实上，这也正是
它的强大之处。

00:04:46.780 --> 00:04:50.564
不像是给计算机下达指令，

00:04:51.020 --> 00:04:55.084
更像是在训练一个机器狗，

00:04:55.108 --> 00:04:58.109
我们无法精确的
了解和控制它。

00:04:58.182 --> 00:04:59.733
这就是我们遇到的问题。

00:05:00.247 --> 00:05:04.509
人工智能会出错，
这是一个问题。

00:05:04.533 --> 00:05:08.073
但他们得出正确答案，
又是另一种问题。

00:05:08.097 --> 00:05:11.725
因为我们面对主观问题，
是不应该有答案的。

00:05:11.749 --> 00:05:14.088
我们不知道
这些机器在想什么。

00:05:15.313 --> 00:05:18.996
所以，考虑一下招聘算法－

00:05:19.943 --> 00:05:24.254
通过机器学习构建的招聘系统。

00:05:24.872 --> 00:05:28.451
这样的系统会用员工
现有的数据进行自我培训，

00:05:28.475 --> 00:05:31.066
参照公司的优秀员工

00:05:31.090 --> 00:05:34.128
来寻找和招聘新人。

00:05:34.634 --> 00:05:35.787
听起来很好。

00:05:35.811 --> 00:05:37.810
有次我参加了一个会议，

00:05:37.834 --> 00:05:40.959
会上聚集了很多
人力资源部的经理和总监，

00:05:40.983 --> 00:05:42.189
都是高管，

00:05:42.213 --> 00:05:43.772
让他们使用这样的招聘系统。

00:05:43.796 --> 00:05:45.442
他们都非常兴奋，

00:05:45.466 --> 00:05:50.119
认为这可以让招聘变得
更加客观，从而减少偏见，

00:05:50.143 --> 00:05:53.143
给女性和少数族裔
更多的机会，

00:05:53.167 --> 00:05:55.355
减少他们自身的偏见。

00:05:55.379 --> 00:05:58.222
你知道的，
招聘是存在偏见的，

00:05:58.919 --> 00:06:00.104
我也很清楚。

00:06:00.128 --> 00:06:03.133
在我刚开始做程序员的时候，

00:06:03.157 --> 00:06:07.025
我的直接主管会来找我，

00:06:07.049 --> 00:06:10.802
在早晨很早或下午很晚的时候，

00:06:10.826 --> 00:06:13.888
说，“ 图费, 我们去吃午饭！”

00:06:14.544 --> 00:06:16.711
我就被这奇怪的时间
给搞糊涂了，

00:06:16.735 --> 00:06:18.864
现在是下午4点，吃午饭？

00:06:18.888 --> 00:06:21.982
我当时很穷，所以
不会放过免费的午餐。

00:06:22.438 --> 00:06:24.505
后来我才想明白原因，

00:06:24.529 --> 00:06:29.075
我的主管们没有
向他们的上级坦白，

00:06:29.099 --> 00:06:32.212
他们雇了一个十多岁的小女孩
来做重要的编程工作，

00:06:32.236 --> 00:06:36.166
一个穿着牛仔裤，
运动鞋工作的女孩。

00:06:36.994 --> 00:06:39.196
我的工作做得很好，
我只是看起来不合适，

00:06:39.220 --> 00:06:40.919
年龄和性别也不合适。

00:06:40.943 --> 00:06:44.289
所以，忽略性别和种族的招聘，

00:06:44.313 --> 00:06:46.178
听起来很适合我。

00:06:46.851 --> 00:06:50.192
但是这样的系统会带来更多问题，

00:06:50.788 --> 00:06:56.579
当前，计算机系统
能根据零散的数据，

00:06:56.603 --> 00:06:58.475
推断出关于你的一切，

00:06:58.499 --> 00:07:00.832
甚至你没有公开的事。

00:07:01.326 --> 00:07:04.253
它们可以推断你的性取向，

00:07:04.814 --> 00:07:06.120
你的性格特点，

00:07:06.679 --> 00:07:08.052
你的政治倾向。

00:07:08.650 --> 00:07:12.335
它们有高准确度的预测能力，

00:07:13.182 --> 00:07:15.760
记住，是你没有公开的事情，

00:07:15.784 --> 00:07:17.375
这就是推断。

00:07:17.399 --> 00:07:20.660
我有个朋友
就是开发这种系统，

00:07:20.684 --> 00:07:23.819
从社交媒体的数据中，

00:07:23.819 --> 00:07:26.425
推断患临床或
产后抑郁症的可能性。

00:07:26.496 --> 00:07:27.923
结果令人印象深刻，

00:07:28.312 --> 00:07:31.669
她的系统可以
在症状出现前几个月

00:07:31.693 --> 00:07:35.596
成功预测到
患抑郁的可能性，

00:07:35.620 --> 00:07:36.993
提前几个月。

00:07:37.017 --> 00:07:39.263
在有症状之前，
就可以预测到，

00:07:39.287 --> 00:07:44.099
她希望这可以用于
临床早期干预，这很棒！

00:07:44.731 --> 00:07:46.771
现在我们把这项技术
放到招聘中来看。

00:07:47.847 --> 00:07:50.893
在那次人力资源管理会议中，

00:07:50.917 --> 00:07:55.626
我接近了一位大公司的高管，

00:07:55.650 --> 00:08:00.228
我对她说，“看，如果这个系统
在不通知你的情况下，

00:08:00.252 --> 00:08:06.801
就剔除了未来
有可能抑郁的人，怎么办？

00:08:07.581 --> 00:08:10.957
他们现在不抑郁，
只是未来有可能。

00:08:11.743 --> 00:08:15.149
如果它剔除了
有可能怀孕的女性，怎么办？

00:08:15.173 --> 00:08:17.759
她们现在没怀孕，
但未来一两年有可能。

00:08:18.664 --> 00:08:24.300
如果因为你的公司文化，
它只雇佣激进的候选人怎么办？”

00:08:24.993 --> 00:08:27.684
只看性别比例，
你发现不了这些问题，

00:08:27.708 --> 00:08:29.210
性别比例是可以被调整的。

00:08:29.234 --> 00:08:32.791
并且因为这是机器学习，
不是传统的代码，

00:08:32.815 --> 00:08:37.722
不会有一个变量来标识
“高抑郁风险”、

00:08:37.746 --> 00:08:39.579
“高怀孕风险”、

00:08:39.603 --> 00:08:41.337
“人员的激进程度”。

00:08:41.815 --> 00:08:45.494
你不仅无法了解系统
在选什么样的人，

00:08:45.518 --> 00:08:47.841
你甚至不知道
从哪里入手了解。

00:08:47.865 --> 00:08:49.111
它是个暗箱。

00:08:49.135 --> 00:08:51.942
它有预测的能力，
但你不了解它。

00:08:52.306 --> 00:08:54.675
我问，“你有什么措施
可以保证，

00:08:54.699 --> 00:08:58.372
你的暗箱没有
在做些见不得人的事？”

00:09:00.683 --> 00:09:04.561
她看着我，就好像
我刚踩了10只小狗的尾巴。

00:09:04.585 --> 00:09:05.833
（笑声）

00:09:05.857 --> 00:09:07.898
她瞪着我说：

00:09:08.376 --> 00:09:12.709
“我不想再听你多说一个字。”

00:09:13.278 --> 00:09:15.312
然后她转身走开了。

00:09:15.884 --> 00:09:17.370
其实，
她不是无礼，

00:09:17.394 --> 00:09:23.702
她想表达的其实是：我不知道，
这不是我的错，走开，不然我瞪死你。

00:09:23.726 --> 00:09:24.972
（笑声）

00:09:25.682 --> 00:09:29.521
看，这样的系统
可能在某些方面

00:09:29.545 --> 00:09:31.648
比人类高管
怀有更少偏见，

00:09:31.672 --> 00:09:33.818
而且可以创造经济价值。

00:09:34.393 --> 00:09:36.043
但它也可能

00:09:36.067 --> 00:09:40.219
用一种顽固且隐秘的方式，

00:09:40.219 --> 00:09:43.132
把高抑郁风险的人清出职场。

00:09:43.573 --> 00:09:46.169
这是我们想要的未来吗？

00:09:46.193 --> 00:09:48.478
把决策权给予我们
并不完全了解的机器，

00:09:48.502 --> 00:09:52.466
在我们不知情的状况下
构建一种新的社会？

00:09:53.085 --> 00:09:54.543
另一个问题是，

00:09:55.134 --> 00:09:59.110
这些系统通常使用

00:09:59.110 --> 00:10:01.426
我们真实的
行为数据来训练。

00:10:02.008 --> 00:10:05.816
它们可能只是在
反馈我们的偏见，

00:10:05.840 --> 00:10:09.433
这些系统会
继承我们的偏见，

00:10:09.457 --> 00:10:10.770
并把它们放大，

00:10:10.794 --> 00:10:12.212
然后反馈给我们。

00:10:12.236 --> 00:10:13.698
我们骗自己说，

00:10:13.722 --> 00:10:16.839
“我们只做客观、
中立的预测。”

00:10:18.134 --> 00:10:21.961
研究者发现，在 Google 上，

00:10:21.961 --> 00:10:27.267
高收入工作的广告
更多的被展示给男性用户。

00:10:28.283 --> 00:10:30.813
搜索非裔美国人的名字，

00:10:30.837 --> 00:10:35.543
更可能出现
关于犯罪史的广告，

00:10:35.567 --> 00:10:37.844
即使某些根本不存在。

00:10:38.513 --> 00:10:42.062
这些潜在的偏见
以及暗箱中的算法，

00:10:42.086 --> 00:10:46.059
有些会被研究者揭露，
有些根本不会被发现，

00:10:46.083 --> 00:10:49.244
它的后果可能是
改变一个人的人生。

00:10:49.778 --> 00:10:52.781
在威斯康星，一个被告

00:10:52.781 --> 00:10:56.186
因逃避警察被判刑六年。

00:10:56.644 --> 00:10:57.830
你可能不知道，

00:10:57.854 --> 00:11:01.852
但计算机算法正越来越多的
被应用在假释及量刑裁定上。

00:11:01.876 --> 00:11:04.831
他想要弄清楚，这个
得分是怎么算出来的？

00:11:05.615 --> 00:11:07.280
这是个商业暗箱，

00:11:07.304 --> 00:11:11.509
这家公司拒绝在公开法庭上
讨论他们的算法。

00:11:12.216 --> 00:11:16.612
但是一家叫 ProPublica 
的非盈利机构，

00:11:16.612 --> 00:11:19.788
根据公开数据，
对这个算法进行了评估，

00:11:19.812 --> 00:11:22.128
他们发现这个算法
的结论是有偏见的，

00:11:22.152 --> 00:11:25.781
它的预测能力很差，
比碰运气强不了多少，

00:11:25.805 --> 00:11:30.221
并且它错误的把黑人被告
未来犯罪的可能性

00:11:30.245 --> 00:11:35.040
标记为白人的两倍。

00:11:35.711 --> 00:11:37.895
看下这个案例：

00:11:37.923 --> 00:11:41.775
这个女人急着去佛罗里达州,
布劳沃德县的一所学校，

00:11:41.799 --> 00:11:44.614
去接她的干妹妹。

00:11:44.614 --> 00:11:46.933
女人和她的朋友在街上狂奔，

00:11:46.957 --> 00:11:51.056
她们看到门廊上一辆没上锁的
儿童自行车，和一辆电瓶车，

00:11:51.080 --> 00:11:52.712
于是就愚蠢的骑上了车。

00:11:52.736 --> 00:11:55.335
正在她们要骑走的时候，
另一个女人出来，喊道：

00:11:55.359 --> 00:11:57.564
“嘿！那是我孩子的自行车！”

00:11:57.588 --> 00:12:00.882
她们扔掉车走开，
但还是被抓住了。

00:12:00.906 --> 00:12:04.543
她做错了，她很愚蠢，
但她也才刚满18岁，

00:12:04.567 --> 00:12:07.561
她之前有不少
青少年轻罪的记录。

00:12:07.628 --> 00:12:12.813
与此同时，这个男人
在连锁超市偷窃被捕了，

00:12:12.837 --> 00:12:16.581
偷了价值85美金的东西，
同样的轻微犯罪，

00:12:16.586 --> 00:12:21.145
但他有两次持枪抢劫的案底。

00:12:21.775 --> 00:12:26.517
这个程序将这位女性判定为
高风险，而这位男性则不是。

00:12:26.566 --> 00:12:30.440
两年后，ProPublica 
发现她没有再次犯罪，

00:12:30.464 --> 00:12:33.014
但这个记录
使她很难找到工作。

00:12:33.038 --> 00:12:35.114
而这位男性，却再次犯罪，

00:12:35.138 --> 00:12:38.974
并因此被判八年监禁。

00:12:39.908 --> 00:12:43.277
显然，我们需要
审查这些暗箱，

00:12:43.301 --> 00:12:45.916
确保它们不再有这样
不加限制的权限。

00:12:45.940 --> 00:12:48.819
（掌声）

00:12:49.907 --> 00:12:54.149
审查是很重要的，
但不能解决所有的问题。

00:12:54.173 --> 00:12:56.921
拿 Facebook 的强大的
新闻流算法来说，

00:12:56.945 --> 00:13:00.842
就是通过你的朋友圈
和你浏览过的页面，

00:13:00.842 --> 00:13:04.686
决定你的
“推荐内容”的算法。

00:13:04.718 --> 00:13:06.993
它会决定要不要
再推一张婴儿照片给你，

00:13:07.017 --> 00:13:08.213
（笑声）

00:13:08.237 --> 00:13:10.833
要不要推一条熟人
的沮丧状态？

00:13:11.269 --> 00:13:13.125
要不要推一条重要
但艰涩的新闻？

00:13:13.149 --> 00:13:14.631
这个问题没有正解。

00:13:14.655 --> 00:13:17.314
Facebook 会根据
网站的参与度来优化：

00:13:17.338 --> 00:13:19.463
喜欢、分享、评论。

00:13:19.988 --> 00:13:22.684
在2014年8月，

00:13:22.708 --> 00:13:25.370
密苏里州弗格森市爆发了游行，

00:13:25.394 --> 00:13:28.785
一个白人警察在不明状况下

00:13:28.785 --> 00:13:31.405
杀害了一位非裔少年。

00:13:31.794 --> 00:13:33.801
关于游行的新闻

00:13:33.825 --> 00:13:36.510
在我的未经算法过滤的
Twitter 上大量出现，

00:13:36.534 --> 00:13:38.994
但 Facebook 上却没有。

00:13:39.002 --> 00:13:40.736
是因为我的 Facebook 好友
不关注这事吗？

00:13:40.760 --> 00:13:42.792
我禁用了 Facebook 的算法，

00:13:43.292 --> 00:13:46.140
这是很麻烦的一键事，
因为 Facebook 希望

00:13:46.164 --> 00:13:48.200
你一直在它的算法
控制下使用,

00:13:48.224 --> 00:13:50.462
希望我的朋友持续
地谈论这件事。

00:13:50.486 --> 00:13:52.995
只是算法没法
给我这些信息。

00:13:53.019 --> 00:13:56.061
我研究了这个现象，
发现这是个普遍的问题。

00:13:56.085 --> 00:13:59.292
弗格森事件
对算法是不适用的，

00:13:59.292 --> 00:14:01.093
它不是值得“赞”的新闻，

00:14:01.117 --> 00:14:03.279
谁会在这样
的文章下点“赞”呢？

00:14:03.320 --> 00:14:05.526
甚至这新闻都不好被评论。

00:14:05.550 --> 00:14:06.921
因为没有“赞”和评论，

00:14:06.945 --> 00:14:10.237
算法会减少
这些新闻的曝光，

00:14:10.261 --> 00:14:12.623
所以我们无法看到。

00:14:12.766 --> 00:14:13.994
相反的，在同一周，

00:14:14.018 --> 00:14:16.316
Facebook 的算法热推了

00:14:16.340 --> 00:14:18.566
ALS 冰桶挑战的信息。

00:14:18.590 --> 00:14:22.332
这很有意义，倒冰水，
为慈善捐款，很好。

00:14:22.356 --> 00:14:25.040
这个事件对算法是很适用的，

00:14:25.040 --> 00:14:27.652
机器帮我们做了这个决定。

00:14:27.676 --> 00:14:31.173
非常重要但艰涩的新闻事件

00:14:31.197 --> 00:14:32.752
可能会被埋没掉，

00:14:32.776 --> 00:14:36.002
因为 Facebook 已经成为
主要的信息来源。

00:14:36.002 --> 00:14:39.734
最后，这些系统
也可能会在一些

00:14:39.758 --> 00:14:42.494
不同于人力系统
的那些事情上搞错。

00:14:42.518 --> 00:14:46.450
你们记得 Watson 吧，
那个在智力竞赛《危险边缘》中

00:14:46.450 --> 00:14:49.092
横扫人类选手的 
IBM 机器智能系统，

00:14:49.092 --> 00:14:50.379
它是个很厉害的选手。

00:14:50.403 --> 00:14:53.972
但是，在最后一轮比赛中，
Watson 被问道：

00:14:54.479 --> 00:14:57.411
“它最大的机场是以
二战英雄命名的，

00:14:57.435 --> 00:14:59.687
它第二大机场是以
二战战场命名的。”

00:14:59.711 --> 00:15:01.089
（哼唱《危险边缘》插曲）

00:15:01.402 --> 00:15:02.584
芝加哥。

00:15:02.608 --> 00:15:03.978
两位人类选手答对了，

00:15:04.517 --> 00:15:08.865
但 Watson 答的是，
“多伦多”，

00:15:08.889 --> 00:15:11.427
这是个猜美国城市的环节！

00:15:11.427 --> 00:15:14.317
这个厉害的系统也会犯

00:15:14.341 --> 00:15:18.682
人类都不会犯的，二年级
小孩都不会犯的错误。

00:15:18.682 --> 00:15:21.752
我们的机器智能系统,

00:15:21.776 --> 00:15:24.876
会在一些不符合人类
出错模式的问题上出错，

00:15:24.900 --> 00:15:27.850
这些问题都是我们
无法预料和准备的。

00:15:27.874 --> 00:15:31.512
丢失一份完全有能力胜任
的工作时，人们会感到很糟，

00:15:31.536 --> 00:15:35.263
但如果是因为机器
子程序的过度堆积，

00:15:35.287 --> 00:15:36.719
就简直糟透了。

00:15:36.743 --> 00:15:38.322
（笑声）

00:15:38.346 --> 00:15:41.132
在2010年五月，

00:15:41.156 --> 00:15:44.624
华尔街出现一次
股票闪电崩盘，

00:15:44.624 --> 00:15:48.252
原因是“卖出”算法
的反馈回路导致，

00:15:48.276 --> 00:15:52.460
在36分钟内
损失了几十亿美金。

00:15:53.542 --> 00:15:55.729
我甚至不敢想，
致命的自动化武器

00:15:55.753 --> 00:15:59.342
发生“错误”会是什么后果。

00:16:01.714 --> 00:16:05.504
是的，人类总是会有偏见，

00:16:05.528 --> 00:16:07.704
法庭上、新闻机构、战争中的，

00:16:07.728 --> 00:16:11.221
决策者、看门人…

00:16:11.245 --> 00:16:14.283
他们都会犯错，
但这恰恰是我要说的。

00:16:14.307 --> 00:16:17.938
我们无法抛开
这些困难的问题，

00:16:17.938 --> 00:16:22.842
我们不能把我们自身
该承担的责任推给机器。

00:16:22.842 --> 00:16:26.704
（掌声）

00:16:28.909 --> 00:16:34.516
人工智能不会给我们
一张“伦理免责卡”。

00:16:34.562 --> 00:16:37.943
数据科学家 Fred Benenson 
称之为“数学粉饰”。

00:16:37.967 --> 00:16:39.356
我们需要是相反的东西。

00:16:39.380 --> 00:16:44.768
我们需要培养算法的
怀疑、复查和调研能力。

00:16:45.200 --> 00:16:48.398
我们需要确保
有人为算法负责，

00:16:48.422 --> 00:16:50.867
为算法审查，
并切实的公开透明。

00:16:51.200 --> 00:16:54.434
我们必须认识到，
把数学和计算引入

00:16:54.458 --> 00:16:57.428
解决复杂的、高价值
的人类事务中，

00:16:57.452 --> 00:16:59.836
并不能带来客观性，

00:16:59.860 --> 00:17:03.493
相反，人类事务
的复杂性会扰乱算法。

00:17:03.968 --> 00:17:07.455
是的，我们可以
并且需要使用计算机

00:17:07.479 --> 00:17:09.493
来帮助我们做更好的决策，

00:17:09.517 --> 00:17:14.849
但我们也需要在判断中
加入道德义务，

00:17:14.873 --> 00:17:17.691
在这个框架下使用算法，

00:17:17.715 --> 00:17:21.724
而不是像人与人
之间相互推卸那样，

00:17:21.724 --> 00:17:25.128
就把责任转移给机器。

00:17:25.627 --> 00:17:28.236
人工智能到来了，

00:17:28.260 --> 00:17:30.885
这意味着
我们要格外坚守

00:17:30.885 --> 00:17:33.852
人类的价值观和伦理。

00:17:33.876 --> 00:17:35.030
谢谢。

00:17:35.054 --> 00:17:40.074
（掌声）

