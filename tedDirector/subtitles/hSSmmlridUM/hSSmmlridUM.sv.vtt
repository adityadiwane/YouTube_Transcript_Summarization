WEBVTT
Kind: captions
Language: sv

00:00:00.000 --> 00:00:07.000
Översättare: Anette Smedberg
Granskare: Annika Bidner

00:00:12.739 --> 00:00:16.861
Mitt första jobb var som dataprogrammerare

00:00:16.885 --> 00:00:18.841
under mitt första år på högskolan -

00:00:18.865 --> 00:00:20.372
jag var tonåring då.

00:00:20.889 --> 00:00:22.621
Strax efter att jag börjat jobba,

00:00:22.645 --> 00:00:24.255
med att utveckla programvara,

00:00:24.799 --> 00:00:28.434
kom en chef på företaget ner där jag satt,

00:00:28.458 --> 00:00:29.726
och viskade till mig,

00:00:30.229 --> 00:00:33.090
"Kan han se om jag ljuger?"

00:00:33.806 --> 00:00:35.883
Där fanns ingen annan i rummet.

00:00:37.032 --> 00:00:41.421
"Kan vem se om du ljuger?
Och varför viskar vi?"

00:00:42.266 --> 00:00:45.373
Chefen pekade på datorn som stod i rummet.

00:00:45.397 --> 00:00:48.493
"Kan han se om jag ljuger?"

00:00:49.613 --> 00:00:53.975
Den här chefen hade en affär
med receptionisten.

00:00:53.999 --> 00:00:55.111
(Skratt)

00:00:55.135 --> 00:00:56.901
Och jag var fortfarande tonåring.

00:00:57.447 --> 00:00:59.466
Så jag visk-skrek tillbaka,

00:00:59.490 --> 00:01:03.114
"Ja, datorn kan se om du ljuger."

00:01:03.138 --> 00:01:04.944
(Skratt)

00:01:04.968 --> 00:01:07.891
Jag skrattade, men egentligen
är det jag som är skämtet.

00:01:07.915 --> 00:01:11.183
Idag finns det beräkningssystem

00:01:11.207 --> 00:01:14.755
som kan sortera ut känslor och även lögner

00:01:14.779 --> 00:01:16.823
genom att processa data från ansikten.

00:01:17.248 --> 00:01:21.401
Annonsörer och till och med regeringar
är mycket intresserade.

00:01:22.319 --> 00:01:24.181
Jag blev programmerare

00:01:24.205 --> 00:01:27.318
för som barn var jag som tokig
i matematik och vetenskap.

00:01:27.942 --> 00:01:31.050
Men någonstans längs vägen
fick jag vetskap om kärnvapen,

00:01:31.074 --> 00:01:34.026
och blev mycket oroad
över etiken i forskningen.

00:01:34.050 --> 00:01:35.254
Jag var bekymrad.

00:01:35.278 --> 00:01:37.919
Emellertid, på grund av
familjeomständigheter,

00:01:37.943 --> 00:01:41.241
var jag också tvungen
att börja arbeta tidigt.

00:01:41.265 --> 00:01:44.564
Jag tänkte för mig själv;
jag väljer ett tekniskt område

00:01:44.588 --> 00:01:46.384
där jag lätt kan få arbete och där

00:01:46.408 --> 00:01:50.426
jag inte behöver bekymra mig om
några svårlösta etiska frågor.

00:01:51.022 --> 00:01:52.551
Så jag valde datorer.

00:01:52.575 --> 00:01:53.679
(Skratt)

00:01:53.703 --> 00:01:57.113
Ja, ha, ha, ha!
Skratta gärna åt mig.

00:01:57.137 --> 00:01:59.891
Idag bygger datorforskare plattformar

00:01:59.915 --> 00:02:04.124
som kontrollerar vad som visas
för miljarder människor varje dag.

00:02:05.052 --> 00:02:06.707
De utvecklar bilar

00:02:06.707 --> 00:02:09.223
som skulle kunna bestämma
vem de ska köra över.

00:02:09.707 --> 00:02:12.920
De bygger till och med maskiner, vapen,

00:02:12.944 --> 00:02:15.229
som kan döda människor i krig.

00:02:15.253 --> 00:02:18.024
Det handlar om etik i hela ledet.

00:02:19.183 --> 00:02:21.241
De intelligenta maskinerna är här.

00:02:21.823 --> 00:02:25.297
Idag använder vi beräkningssystem
för alla sorters beslut,

00:02:25.321 --> 00:02:27.207
men också för nya typer av beslut.

00:02:27.231 --> 00:02:32.403
Vi ställer frågor till algoritmerna
som inte har ett entydigt korrekt svar,

00:02:32.427 --> 00:02:33.629
frågor som är subjektiva,

00:02:33.653 --> 00:02:35.978
öppna och värdeladdade.

00:02:36.002 --> 00:02:37.760
Vi ställer frågor som,

00:02:37.784 --> 00:02:39.434
"Vem ska företaget anställa?"

00:02:39.716 --> 00:02:41.546
"Vilken av dina vänners uppdateringar

00:02:41.546 --> 00:02:42.879
ska du kunna se?"

00:02:42.879 --> 00:02:45.145
"Vem återfaller troligast i brott?"

00:02:45.514 --> 00:02:48.568
"Vilka nyheter eller filmer
ska rekommenderas till folk?"

00:02:48.592 --> 00:02:51.964
Visst, vi har använt datorer ett tag,

00:02:51.988 --> 00:02:53.505
men det här är annorlunda.

00:02:53.529 --> 00:02:55.596
Det är en historisk vridning,

00:02:55.620 --> 00:03:00.957
för vi kan inte verifiera beräkningarna
för sådana subjektiva beslut

00:03:00.981 --> 00:03:06.401
på samma sätt som vi kan verifiera dem
vi gör för flygplan, brokonstruktioner,

00:03:06.425 --> 00:03:07.684
eller månfärder.

00:03:08.449 --> 00:03:11.708
Är flygplan säkrare?
Började bron självsvänga och rasa?

00:03:11.732 --> 00:03:16.230
Där har vi kommit överens om
ganska tydliga ramverk,

00:03:16.254 --> 00:03:18.493
och vi har lagar som vägleder oss.

00:03:18.517 --> 00:03:21.911
Såna ramverk eller normer finns inte

00:03:21.935 --> 00:03:25.898
för beslut gällande
krångliga mänskliga relationer.

00:03:25.922 --> 00:03:30.159
För att göra det än mer komplicerat,
blir vår mjukvara kraftfullare,

00:03:30.183 --> 00:03:33.956
samtidigt som den blir
både mindre transparent och mer komplex.

00:03:34.542 --> 00:03:36.582
Under det senaste årtiondet,

00:03:36.606 --> 00:03:39.335
har komplexa algoritmer
gjort enorma framsteg.

00:03:39.359 --> 00:03:41.349
De kan känna igen ansikten.

00:03:41.985 --> 00:03:44.040
De kan tolka handskriven text.

00:03:44.436 --> 00:03:46.502
De kan upptäcka kontokortsbedrägerier,

00:03:46.526 --> 00:03:47.715
blockera spam

00:03:47.739 --> 00:03:49.776
och de kan översätta språk.

00:03:49.800 --> 00:03:52.374
De kan upptäcka tumörer
genom medicinsk bildteknik.

00:03:52.398 --> 00:03:54.603
De kan slå människor i schack och Go.

00:03:55.264 --> 00:03:59.768
Många av de här framstegen härrör från
en metod som kallas "maskininlärning."

00:04:00.175 --> 00:04:03.362
Maskininlärning skiljer sig
från traditionell programmering,

00:04:03.386 --> 00:04:06.971
där du ger datorn detaljerade,
exakta, noggranna instruktioner.

00:04:07.378 --> 00:04:11.560
Det här är mer som att du
matar systemet med en massa data,

00:04:11.584 --> 00:04:13.240
inkluderat ostrukturerad data,

00:04:13.264 --> 00:04:15.542
till exempel från våra digitala liv.

00:04:15.566 --> 00:04:18.296
Systemet lär sig genom att
bearbeta datamängderna.

00:04:18.669 --> 00:04:20.195
Avgörande är också,

00:04:20.219 --> 00:04:24.599
att systemen inte kommer fram till
något entydigt svar.

00:04:24.623 --> 00:04:28.002
Du får inte ett enkelt svar,
utan en sannolikhetsbedömning;

00:04:28.002 --> 00:04:31.089
"Det här är sannolikt det du letar efter."

00:04:32.023 --> 00:04:35.093
Fördelen är att metoden är kraftfull.

00:04:35.117 --> 00:04:37.403
Chefen för Googles AI-system kallade det

00:04:37.403 --> 00:04:39.564
"datas orimliga effektivitet."

00:04:39.791 --> 00:04:41.144
Nackdelen är

00:04:41.738 --> 00:04:44.809
att vi inte förstår vad systemet lär sig.

00:04:44.833 --> 00:04:46.820
Faktum är att det är dess makt.

00:04:46.946 --> 00:04:50.744
Det är mindre av
att ge en dator instruktioner,

00:04:51.200 --> 00:04:55.264
och mer som att träna en Tamagotschi

00:04:55.288 --> 00:04:57.659
som vi varken förstår eller kontrollerar.

00:04:58.362 --> 00:04:59.913
Det är vårt stora problem.

00:05:00.427 --> 00:05:04.689
Det är problematiskt när såna här system
får saker och ting om bakfoten.

00:05:04.713 --> 00:05:08.253
Det är också ett problem
när de får till det rätt,

00:05:08.277 --> 00:05:11.905
för vi vet inte vad som är vad,
när frågeställningen är subjektiv.

00:05:11.929 --> 00:05:14.648
Vi vet inte vad den här tingesten tänker.

00:05:15.493 --> 00:05:19.176
Föreställ er en algoritm
för anställning -

00:05:20.123 --> 00:05:24.434
ett maskininlärningssystem
som används för att anställa människor.

00:05:25.052 --> 00:05:28.631
Ett sådant system har lärt sig
av data över tidigare anställda

00:05:28.655 --> 00:05:31.246
och instruerats att hitta och anställa

00:05:31.270 --> 00:05:34.308
likadana högpresterare
som redan är anställda i företaget.

00:05:34.814 --> 00:05:35.967
Det låter ju bra.

00:05:35.991 --> 00:05:37.990
Jag deltog en gång i en konferens

00:05:38.014 --> 00:05:41.139
med personalchefer och chefer,

00:05:41.163 --> 00:05:42.369
högt uppsatta människor,

00:05:42.393 --> 00:05:43.952
som använder såna system.

00:05:43.976 --> 00:05:45.622
De var otroligt entusiastiska.

00:05:45.646 --> 00:05:49.063
De trodde att det här skulle göra
anställningsprocessen objektiv,

00:05:49.063 --> 00:05:50.433
och mindre fördomsfull,

00:05:50.433 --> 00:05:53.323
och ge kvinnor och minoriteter
en bättre chans

00:05:53.347 --> 00:05:55.755
mot fördomsfulla, partiska
personalchefer.

00:05:55.755 --> 00:05:58.582
Men lyssna här -
anställningar är fördomsfulla.

00:05:59.099 --> 00:06:00.284
Jag vet.

00:06:00.308 --> 00:06:03.313
Jag menar, under mitt första jobb
som programmerare,

00:06:03.337 --> 00:06:07.205
kom ibland min närmsta chef förbi

00:06:07.229 --> 00:06:10.982
antingen väldigt tidigt på morgonen
eller mycket sent på eftermiddagen,

00:06:11.006 --> 00:06:14.068
och sade; "Zeynep, nu går vi på lunch!"

00:06:14.724 --> 00:06:16.891
Jag var brydd av den märkliga tajmingen.

00:06:16.915 --> 00:06:19.044
Klockan är fyra på eftermiddagen. Lunch?

00:06:19.068 --> 00:06:22.162
Jag var pank, så gratis lunch.
Jag följde alltid med.

00:06:22.618 --> 00:06:24.685
Senare förstod jag vad som hände.

00:06:24.709 --> 00:06:29.255
Min närmaste chef hade inte erkänt
för de högre cheferna

00:06:29.279 --> 00:06:33.632
att programmeraren de anställt
var en tonårstjej

00:06:33.646 --> 00:06:36.706
i jeans och gymnastikskor.

00:06:37.174 --> 00:06:39.486
Jag gjorde ett bra jobb,
jag såg bara fel ut

00:06:39.486 --> 00:06:41.099
och var ung och kvinna.

00:06:41.123 --> 00:06:44.469
Så att anställa utan att
ta hänsyn till kön och ras

00:06:44.493 --> 00:06:46.358
låter verkligen bra i mina öron.

00:06:47.031 --> 00:06:50.372
Men med de här systemen
blir det mer komplicerat;

00:06:50.968 --> 00:06:56.759
Idag kan beräkningssystem
dra alla möjliga slutsatser om dig

00:06:56.783 --> 00:06:58.655
utifrån dina digitala avtryck,

00:06:58.679 --> 00:07:01.012
även om du inte har avslöjat dem.

00:07:01.506 --> 00:07:04.433
De kan dra slutsatser
om din sexuella läggning,

00:07:04.994 --> 00:07:06.300
dina karaktärsdrag,

00:07:06.859 --> 00:07:08.232
dina politiska böjelser.

00:07:08.830 --> 00:07:12.515
De kan prognostisera med hög noggrannhet.

00:07:13.362 --> 00:07:15.940
Kom ihåg - kring saker du inte avslöjat.

00:07:15.964 --> 00:07:17.555
Det är slutledningsförmåga.

00:07:17.579 --> 00:07:20.980
Jag har en vän som utvecklat
ett beräkningssystem

00:07:20.980 --> 00:07:24.505
för att beräkna sannolikheten
för klinisk- eller förlossningsdepression

00:07:24.529 --> 00:07:25.945
utifrån digitala avtryck.

00:07:26.676 --> 00:07:28.103
Resultaten är imponerande.

00:07:28.492 --> 00:07:31.849
Hennes system kan beräkna
sannolikheten för depression

00:07:31.873 --> 00:07:35.776
månader innan symtomen visar sig -

00:07:35.800 --> 00:07:37.173
månader i förväg.

00:07:37.197 --> 00:07:39.443
Inga symtom, bara förutsägelse.

00:07:39.467 --> 00:07:43.321
Hon hoppas att det ska användas
för behandling i ett tidigt stadium.

00:07:43.341 --> 00:07:44.911
Jättebra!

00:07:44.911 --> 00:07:47.671
Men sätt in det
i ett anställningssammanhang.

00:07:48.027 --> 00:07:51.073
På den här personalchefskonferensen,

00:07:51.097 --> 00:07:55.806
frågade jag en högt uppsatt chef
i ett mycket stort företag,

00:07:55.830 --> 00:08:00.408
"Du, tänk om, utan att du vet,

00:08:00.432 --> 00:08:03.051
ert system gallrar ut

00:08:03.051 --> 00:08:07.309
personer med hög sannolikhet
att drabbas av depression?

00:08:07.761 --> 00:08:11.367
De är inte deprimerade nu,
men kanske blir någon gång i framtiden.

00:08:11.923 --> 00:08:15.599
Tänk om det gallrar ut kvinnor
som kan komma att bli gravida

00:08:15.599 --> 00:08:18.189
inom ett till två år
men som inte är gravida nu?

00:08:18.844 --> 00:08:24.700
Tänk om aggressiva människor anställs för
att det är en del av er företagskultur?"

00:08:25.173 --> 00:08:27.864
Du kan inte bedöma det
utifrån könsfördelningen.

00:08:27.888 --> 00:08:29.390
De kan vara i balans.

00:08:29.414 --> 00:08:32.971
Eftersom detta gäller maskininlärning,
och inte traditionell kodning,

00:08:32.995 --> 00:08:37.902
finns det ingen variabel som säger
"högre risk för depression,"

00:08:37.926 --> 00:08:39.759
"högre risk för graviditet,"

00:08:39.783 --> 00:08:41.517
"macho-tendenser."

00:08:41.995 --> 00:08:43.555
Du vet varken

00:08:43.555 --> 00:08:45.698
vad ditt system selekterar på,

00:08:45.698 --> 00:08:48.021
eller var du ska börja titta.

00:08:48.045 --> 00:08:49.291
Det är en svart låda.

00:08:49.315 --> 00:08:52.122
Det har en förutsägbar kraft,
men du förstår inte det.

00:08:52.486 --> 00:08:54.855
"Vilka garantier, frågade jag, har du

00:08:54.889 --> 00:08:58.552
för att försäkra dig om
att din svarta låda inte gör något skumt?"

00:09:00.863 --> 00:09:04.741
Hon tittade på mig som om jag
just trampat på hennes hund.

00:09:04.765 --> 00:09:06.013
(Skratt)

00:09:06.037 --> 00:09:08.078
Hon stirrade på mig och sade,

00:09:08.396 --> 00:09:12.889
"Jag vill inte höra ett enda ord till."

00:09:13.458 --> 00:09:15.532
Hon vände sig om och gick.

00:09:15.724 --> 00:09:18.020
Kom ihåg - hon var inte oförskämd.

00:09:18.020 --> 00:09:23.882
Det var tydligt: Det jag inte vet
är inte mitt problem, stick iväg.

00:09:23.906 --> 00:09:25.152
(Skratt)

00:09:25.862 --> 00:09:29.701
Ett sånt system kan vara mindre subjektivt

00:09:29.725 --> 00:09:31.828
än personalchefer på vissa sätt.

00:09:31.852 --> 00:09:33.998
Och det kan vara ekonomiskt rimligt.

00:09:34.573 --> 00:09:36.653
Men det kan också leda till

00:09:36.677 --> 00:09:40.995
ett smygande utestängande
från arbetsmarknaden

00:09:41.019 --> 00:09:43.312
av människor med högre risk
för psykisk ohälsa.

00:09:43.753 --> 00:09:46.349
Vill vi bygga den sortens samhälle,

00:09:46.373 --> 00:09:48.658
utan att ens märka att vi gör det,

00:09:48.682 --> 00:09:50.825
för att vi låter maskiner ta besluten?

00:09:50.825 --> 00:09:53.265
Maskiner som vi inte begriper oss på?

00:09:53.265 --> 00:09:54.723
Ett annat problem är att

00:09:55.314 --> 00:09:59.766
dessa system ofta reagerar på data
som genererats av våra aktiviteter,

00:09:59.790 --> 00:10:01.606
våra digitala avtryck.

00:10:02.188 --> 00:10:05.996
De kan ju bara reflektera våra fördomar,

00:10:06.020 --> 00:10:09.613
och dessa system
kan plocka upp fördomarna,

00:10:09.637 --> 00:10:10.950
förstärka dem

00:10:10.974 --> 00:10:12.392
och återspegla dem för oss,

00:10:12.416 --> 00:10:13.878
alltmedan vi intalar oss,

00:10:13.902 --> 00:10:17.019
att "Vi gör bara objektiva
neutrala beräkningar."

00:10:18.314 --> 00:10:20.991
Forskare fann att på Google,

00:10:22.134 --> 00:10:27.447
är det mindre troligt att kvinnor får se
annonser för välbetalda jobb än män.

00:10:28.463 --> 00:10:30.993
Söker man på afro-amerikanska namn

00:10:31.017 --> 00:10:35.723
är det mer troligt att man får annonser
som antyder kriminell bakgrund,

00:10:35.747 --> 00:10:38.054
även när det inte finns någon koppling.

00:10:38.693 --> 00:10:42.242
Sådana dolda fördomar
och svarta lådor-algoritmer

00:10:42.266 --> 00:10:46.239
som forskare ibland upptäcker,
och ibland inte,

00:10:46.263 --> 00:10:48.924
kan få livsavgörande konsekvenser.

00:10:49.958 --> 00:10:54.117
I Wisconsin dömdes en åtalad
till sex års fängelse

00:10:54.141 --> 00:10:55.886
för att ha kört från polisen.

00:10:56.824 --> 00:10:58.010
Du vet det kanske inte,

00:10:58.034 --> 00:11:02.062
men algoritmer används mer och mer
vid förelägganden och villkorliga straff.

00:11:02.062 --> 00:11:05.011
Han ville veta:
Hur beräknades straffsatsen?

00:11:05.795 --> 00:11:07.460
Av en kommersiell svart låda.

00:11:07.484 --> 00:11:10.496
Företaget vägrade
att få sin algoritm bedömd

00:11:10.536 --> 00:11:12.396
i en offentlig rättegång.

00:11:12.396 --> 00:11:17.928
Men det granskande, icke-vinstdrivande
företaget Pro-Publica, jämförde den

00:11:17.952 --> 00:11:20.458
med den offentliga data
de kunde hitta, och fann

00:11:20.478 --> 00:11:22.428
att algoritmens utfall var partiskt

00:11:22.438 --> 00:11:25.961
och dess kraftfulla beräkningar usla,
knappt bättre än slumpen,

00:11:25.985 --> 00:11:30.401
och att den felaktigt pekade ut
svarta åtalade som presumtiva brottslingar

00:11:30.425 --> 00:11:34.650
dubbelt så ofta som vita åtalade.

00:11:35.891 --> 00:11:37.455
Så, tänk över det här fallet:

00:11:38.103 --> 00:11:41.955
Den här kvinnan var sen
när hon skulle hämta sitt gudbarn

00:11:41.979 --> 00:11:44.054
på en skola i Broward County i Florida,

00:11:44.757 --> 00:11:47.113
hon och hennes vänner sprang nerför gatan.

00:11:47.137 --> 00:11:51.236
De fick syn på en olåst barncykel
och en sparkcykel på en veranda

00:11:51.260 --> 00:11:52.892
och tog dem dumt nog.

00:11:52.916 --> 00:11:55.515
När de stack iväg,
kom en kvinna ut och sade,

00:11:55.539 --> 00:11:57.744
"Hallå! Det där är mitt barns prylar."

00:11:57.768 --> 00:12:01.062
De släppte dem, gick därifrån,
men blev anhållna.

00:12:01.086 --> 00:12:04.723
Hon gjorde fel, hon betedde sig dumt,
men hon var också bara arton år.

00:12:04.747 --> 00:12:06.797
Hon hade några ungsdomsförseelser

00:12:06.797 --> 00:12:08.028
sedan tidigare.

00:12:08.028 --> 00:12:13.003
Samtidigt hade den här mannen
blivit anhållen för snatteri i Home Depot,

00:12:13.017 --> 00:12:16.131
prylar för 85 dollar,
ett ganska litet brott.

00:12:16.766 --> 00:12:21.325
Men han hade tidigare blivit dömd
för två väpnade rån.

00:12:21.955 --> 00:12:25.657
Men algoritmen bedömde henne
som en högre risk än honom.

00:12:26.746 --> 00:12:30.620
Två år senare fann ProPublica
att hon inte återfallit i brott.

00:12:30.644 --> 00:12:33.194
Hon hade bara svårt
att få jobb med sin bakgrund.

00:12:33.218 --> 00:12:35.294
Han, å andra sidan,
återföll i brottslighet

00:12:35.318 --> 00:12:37.968
och avtjänar nu
ett åttaårigt fängelsestraff

00:12:37.968 --> 00:12:40.088
för ett nytt brott.

00:12:40.088 --> 00:12:43.457
Det är tydligt att vi måste revidera
våra svarta lådor

00:12:43.481 --> 00:12:46.346
och inte låta dem få
sådan här okontrollerad makt.

00:12:46.346 --> 00:12:48.999
(Applåder)

00:12:50.087 --> 00:12:54.329
Kontroller är bra och viktiga,
men de löser inte alla våra problem.

00:12:54.353 --> 00:12:57.251
Ta Facebooks kraftfulla algoritm
för vårt nyhetsflöde -

00:12:57.251 --> 00:13:01.968
ni vet, den som rangordnar allt
och bestämmer vad som ska visas för dig

00:13:01.992 --> 00:13:04.276
från alla vänner och sidor du följer.

00:13:04.898 --> 00:13:07.173
Ska du få se ännu en spädbarnsbild?

00:13:07.197 --> 00:13:08.393
(Skratt)

00:13:08.417 --> 00:13:11.013
En vresig kommentar från en bekant?

00:13:11.449 --> 00:13:13.305
En viktig men svår nyhetsnotis?

00:13:13.329 --> 00:13:14.811
Det finns inget rätt svar.

00:13:14.835 --> 00:13:17.494
Facebook optimerar flödet
för att få engagemang:

00:13:17.518 --> 00:13:19.263
gilla, dela, kommentera.

00:13:20.168 --> 00:13:22.864
I augusti 2014

00:13:22.888 --> 00:13:25.550
bröt protester ut i Ferguson, Missouri,

00:13:25.574 --> 00:13:29.991
efter att en vit polis dödat
en afro-amerikansk tonåring,

00:13:30.015 --> 00:13:31.585
under märkliga omständigheter.

00:13:31.974 --> 00:13:33.981
Nyheten om protesterna fanns överallt

00:13:34.005 --> 00:13:36.690
i mitt i stort sett
algoritmfria Twitter-flöde,

00:13:36.714 --> 00:13:38.664
men inte alls på min Facebook.

00:13:39.182 --> 00:13:40.916
Berodde det på mina Facebook-vänner?

00:13:40.940 --> 00:13:42.972
Jag inaktiverade Facebooks algoritm,

00:13:43.472 --> 00:13:46.320
vilket är svårt för de vill ha dig

00:13:46.344 --> 00:13:48.380
under algoritmens kontroll,

00:13:48.404 --> 00:13:50.522
och såg att mina vänner
pratade om händelsen.

00:13:50.526 --> 00:13:52.835
Det var bara det att algoritmen
inte visade det.

00:13:52.845 --> 00:13:56.631
Jag undersökte saken och fann
att det är ett vanligt problem.

00:13:56.631 --> 00:14:00.078
Händelsen i Ferguson
var inte algoritm-vänlig.

00:14:00.102 --> 00:14:01.413
Den var inte "sympatisk."

00:14:01.427 --> 00:14:02.849
Vem skulle klicka på "gilla?"

00:14:03.500 --> 00:14:05.706
Den är inte ens lätt att kommentera.

00:14:05.730 --> 00:14:07.301
Utan gillanden och kommentarer

00:14:07.321 --> 00:14:10.417
skulle algoritmen troligen visa den
för ännu färre människor,

00:14:10.441 --> 00:14:12.193
så därför fick vi inte se den.

00:14:12.946 --> 00:14:14.174
Istället, den veckan,

00:14:14.198 --> 00:14:16.496
lyfte Facebooks algoritm fram det här,

00:14:16.520 --> 00:14:18.746
ALS Ice Bucket Challenge.

00:14:18.770 --> 00:14:22.512
Häll iskallt vatten över dig,
och skänk pengar för ett gott syfte.

00:14:22.536 --> 00:14:24.870
Den var superalgoritmvänlig.

00:14:25.219 --> 00:14:27.832
Maskinen tog beslutet åt oss.

00:14:27.856 --> 00:14:31.353
En mycket viktig men svår diskussion

00:14:31.377 --> 00:14:32.932
skulle förmodligen kvävts

00:14:32.956 --> 00:14:35.792
om Facebook hade varit 
den enda nyhetskällan.

00:14:36.117 --> 00:14:39.914
Till sist, de här systemen
kan också göra fel

00:14:39.938 --> 00:14:42.674
på sätt som människor inte skulle göra.

00:14:42.698 --> 00:14:45.620
Kommer ni ihåg Watson,
IBMs intelligenta dator

00:14:45.644 --> 00:14:48.772
som sopade mattan
med deltagarna i Jeopardy?

00:14:48.841 --> 00:14:50.559
Den var en duktig motståndare.

00:14:50.583 --> 00:14:54.152
Men sen, som sista fråga i Jeopardy,
fick Watson följande fråga:

00:14:54.519 --> 00:14:57.771
"Största flygplatsen är döpt
efter en hjälte i 2:a världskriget,

00:14:57.775 --> 00:15:00.137
den andra största,
från ett slag i samma krig."

00:15:00.151 --> 00:15:01.579
(Nynnar Jeopardy-vinjetten)

00:15:01.582 --> 00:15:02.764
Chicago.

00:15:02.778 --> 00:15:04.668
De två människorna svarade rätt.

00:15:04.697 --> 00:15:09.045
Watson, å sin sida, svarade "Toronto" -

00:15:09.069 --> 00:15:10.887
i kategorin amerikanska städer!

00:15:11.336 --> 00:15:14.307
Den imponerande maskinen
gjorde också fel

00:15:14.341 --> 00:15:18.452
som en människa aldrig skulle göra,
en lågstadieelev aldrig skulle göra.

00:15:18.823 --> 00:15:21.932
Våra intelligenta maskiner kan misslyckas

00:15:21.956 --> 00:15:25.056
på sätt som inte följer
mänskliga mönster,

00:15:25.080 --> 00:15:28.030
på sätt som vi inte förväntar oss
och är förberedda för.

00:15:28.054 --> 00:15:31.692
Det är botten att inte få
ett jobb man är kvalificerad för,

00:15:31.716 --> 00:15:35.303
men det skulle suga om det
berodde på minneshanteringen

00:15:35.317 --> 00:15:36.899
i någon subrutin.

00:15:36.923 --> 00:15:38.502
(Skratt)

00:15:38.526 --> 00:15:41.312
I maj 2010

00:15:41.336 --> 00:15:45.380
drabbades Wall Street av en "blixtkrasch"
som förstärktes av en loop

00:15:45.404 --> 00:15:48.432
i Wall Streets sälj-algoritm

00:15:48.456 --> 00:15:52.640
som på 36 minuter
raderade en miljard dollar.

00:15:53.722 --> 00:15:55.909
Jag vill inte tänka på vad "error" betyder

00:15:55.933 --> 00:15:59.852
i samband med automatiserade vapensystem.

00:16:01.894 --> 00:16:05.684
Visst, människor har alltid
gjort partiska antaganden.

00:16:05.708 --> 00:16:07.884
Beslutsfattare och andra grindvakter,

00:16:07.908 --> 00:16:11.401
i domstolar, i nyheter, i krig ...

00:16:11.425 --> 00:16:14.463
de gör misstag;
och det är det här jag menar.

00:16:14.487 --> 00:16:18.198
Vi kan inte rymma från
de här svåra frågeställningarna.

00:16:18.596 --> 00:16:19.676
Vi kan inte

00:16:19.676 --> 00:16:22.436
lämna över ansvaret till maskiner.

00:16:22.676 --> 00:16:25.674
(Applåder)

00:16:29.089 --> 00:16:33.806
Artificiell intelligens ger oss inte
ett frikort när det gäller etik.

00:16:34.742 --> 00:16:38.123
Dataanalytikern Fred Benenson
kallar det för matematik-tvätt.

00:16:38.147 --> 00:16:39.536
Vi behöver motsatsen.

00:16:39.560 --> 00:16:44.948
Vi måste förfina algoritmerna,
granska och kontrollera dem.

00:16:45.380 --> 00:16:48.578
Vi måste se till att ha
ansvarsfulla algoritmer,

00:16:48.602 --> 00:16:51.047
revideringar och meningsfull transparens.

00:16:51.230 --> 00:16:54.864
Vi måste förstå att användande av
matematik och maskinberäkningar

00:16:54.874 --> 00:16:57.608
i krångliga, värdeladdade
mänskliga relationer

00:16:57.632 --> 00:17:00.016
inte skapar objektivitet;

00:17:00.040 --> 00:17:03.673
utan snarare, att komplexa handlingar
påverkar algoritmerna.

00:17:04.148 --> 00:17:07.635
Ja, vi kan och vi bör
använda maskinberäkningar

00:17:07.659 --> 00:17:09.673
som en hjälp för att ta bättre beslut.

00:17:09.697 --> 00:17:15.029
Men vi måste ta vårt moraliska ansvar
i beaktande i bedömningarna

00:17:15.053 --> 00:17:17.871
och använda algoritmerna i det ramverket,

00:17:17.895 --> 00:17:22.830
och inte som ett sätt att frånsäga oss
eller outsourca vårt ansvar

00:17:22.854 --> 00:17:25.308
till varandra som människa till människa.

00:17:25.807 --> 00:17:28.416
Maskinintelligens är här för att stanna.

00:17:28.440 --> 00:17:31.861
Det betyder att vi måste
hålla ännu hårdare

00:17:31.885 --> 00:17:34.032
i våra värderingar och vår etik.

00:17:34.056 --> 00:17:35.210
Tack.

00:17:35.234 --> 00:17:38.206
(Applåder)

