WEBVTT
Kind: captions
Language: uk

00:00:00.000 --> 00:00:07.000
Перекладач: Nataliia Levchuk
Утверджено: Hanna Leliv

00:00:12.739 --> 00:00:16.861
Отже, я почала працювати
програмісткою,

00:00:16.885 --> 00:00:18.841
коли навчалась на першому курсі коледжу

00:00:18.865 --> 00:00:20.372
і була, по суті, ще тінейджеркою.

00:00:20.889 --> 00:00:22.621
Після того, як я почала працювати,

00:00:22.645 --> 00:00:24.255
пишучи програмне забезпечення,

00:00:24.799 --> 00:00:28.434
менеджер, який теж там працював,
підійшов до мене

00:00:28.458 --> 00:00:29.726
і прошепотів:

00:00:30.229 --> 00:00:33.090
"Чи може він сказати, що я брешу?"

00:00:33.806 --> 00:00:35.883
У кабінеті крім нас нікого не було.

00:00:37.032 --> 00:00:41.421
"Хто може сказати, що ти брешеш?
І чому ми говоримо пошепки?"

00:00:42.266 --> 00:00:45.373
Менеджер показав пальцем 
на комп'ютер.

00:00:45.397 --> 00:00:48.493
"Чи може він сказати, що я брешу?"

00:00:49.613 --> 00:00:53.975
Річ у тім, що в мого колеги був 
роман із секретаркою.

00:00:53.999 --> 00:00:55.111
(Сміх)

00:00:55.135 --> 00:00:56.901
А я все ще була тінейджеркою.

00:00:57.447 --> 00:00:59.466
Тому голосно прошепотіла:

00:00:59.490 --> 00:01:03.114
"Так, комп'ютер може сказати,
що ти брешеш".

00:01:03.138 --> 00:01:04.944
(Сміх)

00:01:04.968 --> 00:01:07.891
Ну, я сміялась, але насправді
сміялась над собою.

00:01:07.915 --> 00:01:11.183
Сьогодні існують комп'ютерні системи,

00:01:11.207 --> 00:01:14.755
які можуть розпізнати емоції
і навіть брехню,

00:01:14.779 --> 00:01:16.823
опрацювавши обличчя людини.

00:01:17.248 --> 00:01:21.401
Рекламодавці і навіть влада
в цьому дуже зацікавлені.

00:01:22.319 --> 00:01:24.181
Я стала програмісткою,

00:01:24.205 --> 00:01:27.318
тому, що була однією з тих дітей,
які шаленіли від точних наук.

00:01:27.942 --> 00:01:31.050
Але десь у процесі я дізналась
про ядерну зброю,

00:01:31.074 --> 00:01:34.026
і мене справді схвилювало 
питання наукової етики.

00:01:34.050 --> 00:01:35.254
Я була стривожена.

00:01:35.278 --> 00:01:37.919
Однак через сімейні обставини

00:01:37.943 --> 00:01:41.241
мені також треба було 
почати працювати якнайшвидше.

00:01:41.265 --> 00:01:44.564
Тому я подумала: "Ей, обирай
технічну сферу,

00:01:44.588 --> 00:01:46.384
де можна легко знайти роботу

00:01:46.408 --> 00:01:50.426
і де не треба мати справу з 
болісними етичними проблемами".

00:01:51.022 --> 00:01:52.551
Тому я обрала комп'ютери.

00:01:52.575 --> 00:01:53.679
(Сміх)

00:01:53.703 --> 00:01:57.113
Ха-ха-ха!
Всі наді мною сміялися.

00:01:57.137 --> 00:01:59.891
Сьогодні програмісти будують платформи,

00:01:59.915 --> 00:02:04.124
які регулюють те, що мільйони людей
щодня бачать.

00:02:05.052 --> 00:02:08.874
Вони удосконалюють автомобілі,
які можуть вирішити, кого переїхати.

00:02:09.707 --> 00:02:12.920
Вони навіть конструюють машини і зброю,

00:02:12.944 --> 00:02:15.229
які можуть вбивати людей на війні.

00:02:15.253 --> 00:02:18.024
Це все одно проблема моралі.

00:02:19.183 --> 00:02:21.241
Це машинний інтелект.

00:02:21.823 --> 00:02:25.297
Сьогодні ми використовуємо комп'ютеризацію,
щоб ухвалювати будь-які рішення,

00:02:25.321 --> 00:02:27.207
а також для нових рішень.

00:02:27.231 --> 00:02:32.403
Ми ставимо запитання машинам,
які не мають чітких правильних відповідей,

00:02:32.427 --> 00:02:33.629
суб'єктивних,

00:02:33.653 --> 00:02:35.978
відкритих і заснованих
на ідеях цінностей.

00:02:36.002 --> 00:02:37.760
Ми ставимо запитання на кшталт:

00:02:37.784 --> 00:02:39.434
"Кого компанії варто найняти?"

00:02:40.096 --> 00:02:42.855
"Новини якого друга потрібно показати?"

00:02:42.879 --> 00:02:45.145
"Який злочинець може
знову порушити закон?"

00:02:45.514 --> 00:02:48.568
"Яку новину або фільм
слід рекомендувати людям?"

00:02:48.592 --> 00:02:51.964
Так, ми постійно 
використовували комп'ютери,

00:02:51.988 --> 00:02:53.505
але це інше.

00:02:53.529 --> 00:02:55.596
Це історичний поворот,

00:02:55.620 --> 00:03:00.957
адже не можна передати комп'ютерам
відповідальність за такі суб'єктивні рішення,

00:03:00.981 --> 00:03:06.401
так як передаємо за управління літаками,
будування мостів,

00:03:06.425 --> 00:03:07.684
польоти на Місяць.

00:03:08.449 --> 00:03:11.708
Хіба літаки безпечніші?
Мости не хиталися і не падали?

00:03:11.732 --> 00:03:16.230
У нас є загальноприйняті, 
цілком зрозумілі критерії порівняння,

00:03:16.254 --> 00:03:18.493
і є закони природи, які нами керують.

00:03:18.517 --> 00:03:21.911
У нас немає таких підстав та критеріїв,

00:03:21.935 --> 00:03:25.898
щоб ухвалювати рішення стосовно
брудних людських справ.

00:03:25.922 --> 00:03:30.159
Програмне забезпечення стає потужнішим,
щоб зробити деякі речі складнішими,

00:03:30.183 --> 00:03:33.956
але водночас стає прозорішим 
і заплутанішим.

00:03:34.542 --> 00:03:36.582
За останніх десять років

00:03:36.606 --> 00:03:39.335
заплутані алгоритми досягли
великого успіху.

00:03:39.359 --> 00:03:41.349
Вони можуть розпізнавати обличчя людей.

00:03:41.985 --> 00:03:44.040
Можуть розшифрувати почерк.

00:03:44.436 --> 00:03:46.502
Можуть знайти кредитних шахраїв,

00:03:46.526 --> 00:03:47.715
блокувати спам,

00:03:47.739 --> 00:03:49.776
перекладати з мови на мову.

00:03:49.800 --> 00:03:52.374
Вони можуть виявити пухлини на рентгені.

00:03:52.398 --> 00:03:54.603
Обіграти людей в шахи чи "Ґо".

00:03:55.264 --> 00:03:59.768
Велика частина цього прогресу
прийшла з методу "машинного навчання".

00:04:00.175 --> 00:04:03.362
Машинне навчання відрізняється від 
традиційного програмування,

00:04:03.386 --> 00:04:06.971
коли ви даєте комп'ютеру детальні,
точні, досконалі інструкції.

00:04:07.378 --> 00:04:11.560
Це більше схоже на те, коли ви берете
систему і наповнюєте її інформацією,

00:04:11.584 --> 00:04:13.240
зокрема неструктурованою,

00:04:13.264 --> 00:04:15.542
схожою на ту, що ми генеруємо
у цифровому житті.

00:04:15.566 --> 00:04:18.296
І система вчиться на обробці 
цієї інформації.

00:04:18.669 --> 00:04:20.195
Також важливо,

00:04:20.219 --> 00:04:24.599
що системи не виконують завдання, пов'язані
із необхідністю знайти єдину відповідь.

00:04:24.623 --> 00:04:27.582
Вони не дають просту відповідь,
виходячи з теорії імовірностей.

00:04:27.606 --> 00:04:31.089
"Ось щось схоже на те,
що ви шукаєте".

00:04:32.023 --> 00:04:35.093
Тепер позитивний аспект:
цей метод справді потужний.

00:04:35.117 --> 00:04:37.193
Директор Google's AI systems називає його

00:04:37.217 --> 00:04:39.414
"ірраціональною ефективністю даних".

00:04:39.791 --> 00:04:41.144
Мінусом є те,

00:04:41.738 --> 00:04:44.809
що ми насправді не розуміємо,
про що дізнається система.

00:04:44.833 --> 00:04:46.420
Насправді, в цьому сила системи.

00:04:46.946 --> 00:04:50.744
Це не схоже на ситуацію, коли
ми даємо комп'ютеру інструкції,

00:04:51.200 --> 00:04:55.264
а, скоріше, на формування
маленької машинної істоти,

00:04:55.288 --> 00:04:57.659
яку ми насправді не розуміємо і 
не контролюємо.

00:04:58.362 --> 00:04:59.913
Тому це наша проблема.

00:05:00.427 --> 00:05:04.689
Проблемно, коли система штучного 
інтелекту засвоює інформацію неправильно.

00:05:04.713 --> 00:05:08.253
Також проблемно, коли вона засвоює
інформацію правильно,

00:05:08.277 --> 00:05:11.905
тому що ми не знаємо, хто є хто,
коли це суб'єктивна проблема.

00:05:11.929 --> 00:05:14.268
Ми не знаємо, про що ця річ думає.

00:05:15.493 --> 00:05:19.176
Розглянемо алгоритм найму на роботу -

00:05:20.123 --> 00:05:24.434
система, яку застосовують для найму людей,
використовує системи машинного навчання.

00:05:25.052 --> 00:05:28.631
Така система підготовлена
на даних попередніх працівників

00:05:28.655 --> 00:05:31.246
і навчена шукати і наймати

00:05:31.270 --> 00:05:34.308
людей, подібних до найкращих
працівників компанії.

00:05:34.814 --> 00:05:35.967
Звучить добре.

00:05:35.991 --> 00:05:37.990
Одного разу я відвідала конференцію,

00:05:38.014 --> 00:05:41.139
яку проводили керівники відділів кадрів
та члени правління,

00:05:41.163 --> 00:05:42.369
люди на високих посадах,

00:05:42.393 --> 00:05:43.952
які використовують такі системи.

00:05:43.976 --> 00:05:45.622
Вони були в захопленні.

00:05:45.646 --> 00:05:50.299
Вони думали, що це зробить прийом на
роботу більш об'єктивним, менш упередженим,

00:05:50.323 --> 00:05:53.323
і дасть жінкам і представникам меншин
більше шансів,

00:05:53.347 --> 00:05:55.535
ніж упереджені менеджери.

00:05:55.559 --> 00:05:58.402
Найм людей на роботу - 
справа упереджена.

00:05:59.099 --> 00:06:00.284
Я знаю.

00:06:00.308 --> 00:06:03.313
На одній із перших компаній,
де я програмувала,

00:06:03.337 --> 00:06:07.205
моя безпосередня керівничка 
деколи підходила до мене

00:06:07.229 --> 00:06:10.982
рано-вранці або пізно ввечері,

00:06:11.006 --> 00:06:14.068
і казала: "Зейнеп, ідемо обідати!"

00:06:14.724 --> 00:06:16.891
Я була здивована дивним вибором часу.

00:06:16.915 --> 00:06:19.044
16:00. Обід?

00:06:19.068 --> 00:06:22.162
Ці сумнівні обіди мене ледь не розорили.
Але я завжди йшла.

00:06:22.618 --> 00:06:24.685
Пізніше я усвідомила, що відбувалося.

00:06:24.709 --> 00:06:29.255
Мої безпосередні начальники не могли
зізнатися вищому керівництву,

00:06:29.279 --> 00:06:32.392
що найняли для серйозної роботи 
дівчинку-підлітка,

00:06:32.416 --> 00:06:36.346
яка на роботу ходить у джинсах і кедах.

00:06:37.174 --> 00:06:39.376
Я добре працювала, 
але виглядала неправильно

00:06:39.400 --> 00:06:41.099
і була неправильного віку і статі.

00:06:41.123 --> 00:06:44.469
Тому найм без врахування статі і раси

00:06:44.493 --> 00:06:46.358
безсумнівно мені підходив.

00:06:47.031 --> 00:06:50.372
Але з такими системами цей процес 
ще заплутаніший, і ось чому:

00:06:50.968 --> 00:06:56.759
останнім часом комп'ютерні системи можуть
зробити будь-які висновки про вас,

00:06:56.783 --> 00:06:58.655
враховуючи цифрові дрібниці,

00:06:58.679 --> 00:07:01.012
навіть якщо ви про ці висновки
не підозрюєте.

00:07:01.506 --> 00:07:04.433
Вони можуть робити висновки про 
сексуальну орієнтацію,

00:07:04.994 --> 00:07:06.300
персональні якості,

00:07:06.859 --> 00:07:08.232
політичні погляди.

00:07:08.830 --> 00:07:12.515
Вони можуть передбачати
з високим рівнем точності.

00:07:13.362 --> 00:07:15.940
Пам'ятайте - речі, про які
ви навіть не підозрюєте.

00:07:15.964 --> 00:07:17.555
Це припущення.

00:07:17.579 --> 00:07:20.840
У мене є подруга, яка удосконалює
такі системи,

00:07:20.864 --> 00:07:24.505
щоб передбачити ймовірність
клінічної чи післяпологової депресії,

00:07:24.529 --> 00:07:25.945
аналізуючи соціальні мережі.

00:07:26.676 --> 00:07:28.103
Результати вражаючі.

00:07:28.492 --> 00:07:31.849
ЇЇ система може передбачити
ймовірність депресії

00:07:31.873 --> 00:07:35.776
за місяці перед проявом симптомів -

00:07:35.800 --> 00:07:37.173
за місяці.

00:07:37.197 --> 00:07:39.443
Передбачення без симптомів.

00:07:39.467 --> 00:07:44.279
Вона сподівається, що систему застосують
для втручання на ранній стадії. Чудово!

00:07:44.911 --> 00:07:46.951
Розгляньте це в контексті найму на роботу.

00:07:48.027 --> 00:07:51.073
На конференції керівників кадрових служб

00:07:51.097 --> 00:07:55.806
я спілкувалася з топ-менеджеркою
дуже великої компанії

00:07:55.830 --> 00:08:00.408
і сказала їй: "А якщо без вашого відома

00:08:00.432 --> 00:08:06.981
система видаляє людей з високою
ймовірністю появи депресії?

00:08:07.761 --> 00:08:11.137
Наразі вони не пригнічені, але можливо
стануть такими в майбутньому.

00:08:11.923 --> 00:08:15.329
Що як вона видаляє жінок, які, 
можливо, завагітніють

00:08:15.353 --> 00:08:17.939
за кілька наступних років,
але не вагітні зараз?

00:08:18.844 --> 00:08:24.480
Що як вона наймає на роботу агресивних 
людей через корпоративну культуру?"

00:08:25.173 --> 00:08:27.864
Ви не можете цього сказати, дивлячись на 
гендерний баланс.

00:08:27.888 --> 00:08:29.390
Їх можна врівноважити.

00:08:29.414 --> 00:08:32.971
І так як це машинне навчання,
а не традиційне програмування,

00:08:32.995 --> 00:08:37.902
немає змінних ярликів на кшталт:
"високий ризик депресії",

00:08:37.926 --> 00:08:39.759
"високий ризик вагітності"

00:08:39.783 --> 00:08:41.517
чи "шкала агресивності".

00:08:41.995 --> 00:08:45.674
Ви не тільки не знаєте, що саме система
візьме до уваги,

00:08:45.698 --> 00:08:48.021
ви навіть не знаєте,
де це починати шукати.

00:08:48.045 --> 00:08:49.291
Це чорна скринька.

00:08:49.315 --> 00:08:52.122
Система має силу передбачення,
але ви її не розумієте.

00:08:52.486 --> 00:08:54.855
Я запитала: 
"Які запобіжні заходи у вас є,

00:08:54.879 --> 00:08:58.552
щоб упевнитись, що ця чорна скринька
не робить нічого підозрілого?"

00:09:00.863 --> 00:09:04.741
Вона подивилась на мене так, ніби
я наступила щеняті на хвіст.

00:09:04.765 --> 00:09:06.013
(Сміх)

00:09:06.037 --> 00:09:08.078
Пильно подивилася на мене і сказала:

00:09:08.556 --> 00:09:12.889
"Я більше не хочу нічого про це чути".

00:09:13.458 --> 00:09:15.492
А тоді повернулася і пішла геть.

00:09:16.064 --> 00:09:17.550
Зауважте - вона була вихована.

00:09:17.574 --> 00:09:23.882
Було зрозуміло: те, чого я не знаю -
не моя проблема. Іди геть; вбивчий погляд.

00:09:23.906 --> 00:09:25.152
(Сміх)

00:09:25.862 --> 00:09:29.701
Деколи такі системи можуть бути 
менш упереджені,

00:09:29.725 --> 00:09:31.828
ніж керівники відділів кадрів.

00:09:31.852 --> 00:09:33.998
І це може бути матеріально виправдано.

00:09:34.573 --> 00:09:36.223
Але також це може призвести

00:09:36.247 --> 00:09:40.995
до постійного і приховуваного виштовхування
з ринку праці

00:09:41.019 --> 00:09:43.312
людей з високим ризиком розвитку
депресії.

00:09:43.753 --> 00:09:46.349
Це таке суспільство,
яке ми хочемо будувати,

00:09:46.373 --> 00:09:48.658
навіть не знаючи, що ми це зробили,

00:09:48.682 --> 00:09:52.646
тому що ми доручили ухвалення рішень
машинам, яких повністю не розуміємо.

00:09:53.265 --> 00:09:54.723
Є й інша проблема:

00:09:55.314 --> 00:09:59.766
часто ці програми базуються на
інформації, пов'язаній з нашими діями,

00:09:59.790 --> 00:10:01.606
на людських враженнях.

00:10:02.188 --> 00:10:05.996
Вони можуть відображати
наші упередження,

00:10:06.020 --> 00:10:09.613
можуть їх засвоїти

00:10:09.637 --> 00:10:10.950
і підсилити

00:10:10.974 --> 00:10:12.392
і повернути їх проти нас,

00:10:12.416 --> 00:10:13.878
тоді як ми кажемо:

00:10:13.902 --> 00:10:17.019
"Ми робимо об'єктивний,
нейтральний розрахунок".

00:10:18.314 --> 00:10:20.991
Дослідники виявили, що в Google

00:10:22.134 --> 00:10:27.447
жінкам рідше пропонують оголошення
про високооплачувану роботу.

00:10:28.463 --> 00:10:30.993
І під час пошуку афро-американських імен

00:10:31.017 --> 00:10:35.723
частіше з'являються рекламні оголошення
про кримінальні історії,

00:10:35.747 --> 00:10:37.314
які навіть не стосуються пошуку.

00:10:38.693 --> 00:10:42.242
Такі приховані упередження
і алгоритми "чорної скриньки",

00:10:42.266 --> 00:10:46.239
що їх дослідники деколи виявляють,
а деколи ми про них навіть не знаємо,

00:10:46.263 --> 00:10:48.924
можуть мати життєвоважливі наслідки.

00:10:49.958 --> 00:10:54.117
У Вісконсині підсудний був засуджений
до шести років у в'язниці

00:10:54.141 --> 00:10:55.496
за непокору поліції.

00:10:56.824 --> 00:10:58.010
Можливо, ви цього не знаєте,

00:10:58.034 --> 00:11:02.032
але алгоритми все частіше використовують 
для визначення міри покарання.

00:11:02.056 --> 00:11:05.011
Він хотів знати,
як визначили цей показник.

00:11:05.795 --> 00:11:07.460
Вигідна "чорна скринька".

00:11:07.484 --> 00:11:11.689
Компанію, яка відмовилась від цього 
алгоритму, викликали на судове засідання.

00:11:12.396 --> 00:11:17.928
Некомерційна слідча компанія ProPublica
перевірила той самий алгоритм

00:11:17.952 --> 00:11:19.968
з публічними даними, які вони могли знайти,

00:11:19.992 --> 00:11:22.308
і з'ясувала, що такі висновки
були необ'єктивні,

00:11:22.332 --> 00:11:25.961
а здатність передбачення була мізерна,
мало відрізнялася від випадковості

00:11:25.985 --> 00:11:30.401
і помилково маркувала чорношкірих
обвинувачуваних, як майбутніх злочинців,

00:11:30.425 --> 00:11:34.320
в двічі частіше, ніж білих 
обвинувачуваних.

00:11:35.891 --> 00:11:37.455
Розглянемо такий випадок:

00:11:38.103 --> 00:11:41.955
ця дівчина трохи запізно приїхала 
забирати свою сестру

00:11:41.979 --> 00:11:44.054
зі школи в окрузі Бровард, Флорида,

00:11:44.757 --> 00:11:47.113
і бігла по вулиці
зі своїми друзями.

00:11:47.137 --> 00:11:51.236
Вони помітили неприщіпнутий на замок 
дитячий велосипед і самокат на терасі

00:11:51.260 --> 00:11:52.892
і здуру стрибнули на нього.

00:11:52.916 --> 00:11:55.515
Коли вони втікали, вийшла жінка
і сказала:

00:11:55.539 --> 00:11:57.744
"Ей! Це велосипед моєї дитини!"

00:11:57.768 --> 00:12:01.062
Вони кинули велосипед і втікли,
але їх арештували.

00:12:01.086 --> 00:12:04.723
Дівчина вчинила неправильно і нерозважливо,
але ж їй було всього 18.

00:12:04.747 --> 00:12:07.291
Вона скоїла декілька 
незначних правопорушень.

00:12:07.808 --> 00:12:12.993
Тим часом цього чоловіка арештували за
крадіжку в магазині Home Depot -

00:12:13.017 --> 00:12:15.941
він вкрав речей на 85 доларів,
такий же маленький злочин.

00:12:16.766 --> 00:12:21.325
Але до цього його двічі засудили
за збройні напади.

00:12:21.955 --> 00:12:25.437
Однак алгоритм визначив, що вона 
небезпечніша за нього.

00:12:26.746 --> 00:12:30.620
Через два роки ProPublica з'ясувала,
що вона знову порушила закон.

00:12:30.644 --> 00:12:33.194
І з такими даними їй було важко
отримати роботу.

00:12:33.218 --> 00:12:35.294
З іншого боку, той чоловік 
знову порушив закон

00:12:35.318 --> 00:12:39.154
і тепер відбуває восьмирічний термін
за останній злочин.

00:12:40.088 --> 00:12:43.457
Нам треба перевірити ці "чорні скриньки"

00:12:43.481 --> 00:12:46.096
і не наділяти їх такою
неперевіреною силою.

00:12:46.120 --> 00:12:48.999
(Оплески)

00:12:50.087 --> 00:12:54.329
Перевірки - це дуже важливо,
але вони не розв'язують всіх наших проблем.

00:12:54.353 --> 00:12:57.101
Розглянемо потужний алгоритм
стрічки новин у Фейсбуці -

00:12:57.125 --> 00:13:01.968
той, що упорядковує все і вирішує,
що вам показати

00:13:01.992 --> 00:13:04.276
з усіх сторінок, на які ви підписані.

00:13:04.898 --> 00:13:07.173
Розказати про ще одну "дитячу картину"?

00:13:07.197 --> 00:13:08.393
(Сміх)

00:13:08.417 --> 00:13:11.013
Сумний пост знайомого?

00:13:11.449 --> 00:13:13.305
Важлива, але неприємна новина?

00:13:13.329 --> 00:13:14.811
Немає правильної відповіді.

00:13:14.835 --> 00:13:17.494
Фейсбук враховує активність на сайті:

00:13:17.518 --> 00:13:18.933
лайки, репости, коментарі.

00:13:20.168 --> 00:13:22.864
У серпні 2014

00:13:22.888 --> 00:13:25.550
почалися протести у місті Ферґюсон,
штат Міссуррі,

00:13:25.574 --> 00:13:29.991
після того. як білий поліцейський вбив
афро-американського підлітка

00:13:30.015 --> 00:13:31.585
за незрозумілих обставин.

00:13:31.974 --> 00:13:33.981
Новини про ці протести заполонили

00:13:34.005 --> 00:13:36.690
мою невідфільтровану стрічку
новин у Твіттері,

00:13:36.714 --> 00:13:38.664
але їх не було на моєму Фейсбуці.

00:13:39.182 --> 00:13:40.916
Справа у моїх друзях на Фейсбуці?

00:13:40.940 --> 00:13:42.972
Алгоритм Фейсбука вибив мене з колії,

00:13:43.472 --> 00:13:46.320
він складний, тому що Фейсбук підтримує
бажання

00:13:46.344 --> 00:13:48.380
знаходитися під контролем алгоритму,

00:13:48.404 --> 00:13:50.642
і бачити, що мої друзі говорили про це.

00:13:50.666 --> 00:13:53.175
Це саме те, що алгоритм не показав мені.

00:13:53.199 --> 00:13:56.241
Я досліджувала цю проблему, і виявилось, що
вона досить поширена.

00:13:56.265 --> 00:14:00.078
Історія про Ферґюсон не подобалась
алгоритму.

00:14:00.102 --> 00:14:01.273
Її не "лайкали".

00:14:01.297 --> 00:14:02.849
Хто вподобає такий запис?

00:14:03.500 --> 00:14:05.706
Її навіть непросто прокоментувати.

00:14:05.730 --> 00:14:07.101
Без вподобань і коментарів

00:14:07.125 --> 00:14:10.417
алгоритм показував цей запис
жменьці людей,

00:14:10.441 --> 00:14:11.983
тому ми його не побачили.

00:14:12.946 --> 00:14:14.174
Навпаки, того тижня

00:14:14.198 --> 00:14:16.496
алгоритм Фейсбука виділяв те,

00:14:16.520 --> 00:14:18.746
що називають Ice Bucket Challenge.

00:14:18.770 --> 00:14:22.512
Суспільно значуща справа: обливаєш себе 
холодною водою заради благодійності.

00:14:22.536 --> 00:14:24.440
Але це дуже подобалось алгоритму.

00:14:25.219 --> 00:14:27.832
За нас це рішення прийняла машина.

00:14:27.856 --> 00:14:31.353
Дуже важливе, але заплутане обговорення

00:14:31.377 --> 00:14:32.932
могло бути придушене,

00:14:32.956 --> 00:14:35.652
якби Фейсбук мав лише одну стрічку новин.

00:14:36.117 --> 00:14:39.914
Зрештою, ці системи 
можуть помилятися

00:14:39.938 --> 00:14:42.674
так, як цього не допустить 
людський ресурс.

00:14:42.698 --> 00:14:45.620
Пам'ятаєте систему штучного 
інтелекту Watson,

00:14:45.644 --> 00:14:48.772
що витирала підлогу, змагаючись з
людьми на телегрі Jeopardy?

00:14:49.131 --> 00:14:50.559
Вона була чудовим гравцем.

00:14:50.583 --> 00:14:54.152
Але у фіналі Watson запитали:

00:14:54.659 --> 00:14:57.591
"Його найбільший аеропорт назвали
на честь героя Другої світової війни,

00:14:57.615 --> 00:14:59.867
а другий за розміром - на честь
битви Другої світової війни".

00:14:59.891 --> 00:15:01.269
(Музика з фіналу Jeopardy)

00:15:01.582 --> 00:15:02.764
Чикаго.

00:15:02.788 --> 00:15:04.158
Дві людини відповіли правильно.

00:15:04.697 --> 00:15:09.045
Натомість Watson відповів
"Торонто" -

00:15:09.069 --> 00:15:10.887
і це в категорії міст США!

00:15:11.596 --> 00:15:14.497
Вражаючі системи також робили помилки,

00:15:14.521 --> 00:15:18.172
яких людина ніколи не допустила б,
яких не зробив би навіть другокласник.

00:15:18.823 --> 00:15:21.932
Штучний інтелект може провалитися так,

00:15:21.956 --> 00:15:25.056
як люди зазвичай не помиляються,

00:15:25.080 --> 00:15:28.030
так, як ми не будемо сподіватися
і не будемо до цього готові.

00:15:28.054 --> 00:15:31.692
Кепсько не отримати роботу,
до якої підходить твоя кваліфікація,

00:15:31.716 --> 00:15:35.443
але ще гірше не отримати її через
переповнення стека

00:15:35.467 --> 00:15:36.899
в якійсь програмі.

00:15:36.923 --> 00:15:38.502
(Сміх)

00:15:38.526 --> 00:15:41.312
У травні 2010

00:15:41.336 --> 00:15:45.380
різкий обвал на Волл-Стріт, спричинений
циклом зворотнього зв'язку

00:15:45.404 --> 00:15:48.432
в "торговому" алгоритмі,

00:15:48.456 --> 00:15:52.640
знищив трильйон доларів
за 36 хвилин.

00:15:53.722 --> 00:15:55.909
Я навіть не хочу думати,
що "помилка" означає

00:15:55.933 --> 00:15:59.522
в контексті атомної системи
летального озброєння.

00:16:01.894 --> 00:16:05.684
Так, люди завжди досить упереджені.

00:16:05.708 --> 00:16:07.884
Ті, хто ухвалює рішення і цензори

00:16:07.908 --> 00:16:11.401
в судах, у новинах, на війні...

00:16:11.425 --> 00:16:14.463
вони помиляються;
але у цьому й річ.

00:16:14.487 --> 00:16:18.008
Ми не можемо уникнути цих
заплутаних питань.

00:16:18.596 --> 00:16:22.112
Ми не можемо перекладати відповідальність
на машини.

00:16:22.676 --> 00:16:26.884
(Оплески)

00:16:29.089 --> 00:16:33.536
Штучний інтелект не дає нам
картку "звільнення від моральних норм".

00:16:34.742 --> 00:16:38.123
Фахівець з обробки даних Фред Бененсон
називає це математичною чисткою.

00:16:38.147 --> 00:16:39.536
Нам треба щось протилежне.

00:16:39.560 --> 00:16:44.948
Нам треба удосконалювати підозри,
вивчення і дослідження алгоритмів.

00:16:45.380 --> 00:16:48.578
Треба впевнитись, що у нас є
алгоритмічна звітність,

00:16:48.602 --> 00:16:51.047
перевірка і повноцінна прозорість.

00:16:51.380 --> 00:16:54.614
Треба прийняти те, що залучення
комп'ютерів до безладних

00:16:54.638 --> 00:16:57.608
людських справ, заснованих на цінностях,

00:16:57.632 --> 00:17:00.016
не гарантують об'єктивності;

00:17:00.040 --> 00:17:03.673
алгоритмам, навпаки, передалася 
заплутаність людських справ.

00:17:04.148 --> 00:17:07.635
Так, нам можна і треба використовувати
комп'ютери,

00:17:07.659 --> 00:17:09.673
щоб ухвалювати кращі рішення.

00:17:09.697 --> 00:17:15.029
Але ми маємо підкорятися моральній
відповідальності за вироки суду

00:17:15.053 --> 00:17:17.871
і використовувати алгоритми 
максимум як структуру,

00:17:17.895 --> 00:17:22.830
а не як засіб відмовитися і перекласти
відповідальність

00:17:22.854 --> 00:17:25.308
на когось ще, як людина на людину.

00:17:25.807 --> 00:17:28.416
Штучний інтелект тут.

00:17:28.440 --> 00:17:31.861
І це означає, що ми мусимо триматись
ще сильніше

00:17:31.885 --> 00:17:34.032
за людські цінності і людську мораль.

00:17:34.056 --> 00:17:35.210
Дякую.

00:17:35.234 --> 00:17:40.254
(Оплески)

