WEBVTT
Kind: captions
Language: fa

00:00:00.000 --> 00:00:07.000
Translator: morteza homayounfar
Reviewer: soheila Jafari

00:00:12.739 --> 00:00:16.861
خب، من اولین شغلم را به عنوان یک 
برنامه نویس کامپیوتر شروع کردم

00:00:16.885 --> 00:00:18.841
در اولین سال کالجم

00:00:18.865 --> 00:00:20.372
در واقع، به عنوان یک نوجوان

00:00:20.889 --> 00:00:22.621
درست پس از شروع به کار در

00:00:22.645 --> 00:00:24.255
یک شزکت به عنوان برنامه‌نویس کامپیوتر ،

00:00:24.799 --> 00:00:28.434
یک مدیر که در همان جا کار می‌کرد
نزد من آمد،

00:00:28.458 --> 00:00:29.726
و با صدای آرام به من گفت:

00:00:30.229 --> 00:00:33.090
"او می تونه بگه اگه من دروغ بگم؟"

00:00:33.806 --> 00:00:35.883
در اتاق هیچ کسی وجود نداشت.

00:00:37.032 --> 00:00:41.421
"کسی میتونه بگه اگه تو دروغ میگی؟
و چرا ما در حال پچ پچ کردن هستیم؟"

00:00:42.266 --> 00:00:45.373
مدیر کامپیوتر در اتاق را نشان داد.

00:00:45.397 --> 00:00:48.493
"او می‌تونه بگه اگه من دروغ بگم؟"

00:00:49.613 --> 00:00:53.975
خب، آن مدیر رابطه نامشروع با منشی‌اش داشت.

00:00:53.999 --> 00:00:55.111
(خنده حاضرین)

00:00:55.135 --> 00:00:56.901
و من هم هنوز یک نوجوان بودم

00:00:57.447 --> 00:00:59.466
سپس من با صدای بلندی گفتم:

00:00:59.490 --> 00:01:03.114
"بله، کامپیوتر می تونه بگه 
اگه تو دروغ بگی"

00:01:03.138 --> 00:01:04.944
(خنده حاضرین)

00:01:04.968 --> 00:01:07.891
خب، من خندیدم، ولی در واقع،
خنده به خودم

00:01:07.915 --> 00:01:11.183
امروزه، سامانه‌های محاسباتی وجود دارد

00:01:11.207 --> 00:01:14.755
که حالت احساسی و حتی دروغ رو 
از طریق تحلیل صورت انسان

00:01:14.779 --> 00:01:16.823
می تونه بفهمه

00:01:17.248 --> 00:01:21.401
تبلیغ کننده‌ها و حتی دولت‌ها خیلی
جالبند.

00:01:22.319 --> 00:01:24.181
من یه برنامه نویس کامپیوتر
شده بودم

00:01:24.205 --> 00:01:27.318
زیرا من یکی از آن بچه‌های دیوانه 
ریاضی و علم بودم

00:01:27.942 --> 00:01:31.050
اما در مسیر زندگی من در مورد
سلاح‌های هسته‌ای چیزهایی یادگرفتم‌،

00:01:31.074 --> 00:01:34.026
و واقعا در مورد اخلاق علمی نگران شدم.

00:01:34.050 --> 00:01:35.254
من وحشت زده بودم

00:01:35.278 --> 00:01:37.919
به هر حال، به دلیل موقعیت خانواده‌ام

00:01:37.943 --> 00:01:41.241
نیاز داشتم تا
در اسرع وقت- کارم را شروع کنم

00:01:41.265 --> 00:01:44.564
خُب با خودم فکر کردم
بگذار تا یک رشته تکنیکی را بردارم

00:01:44.588 --> 00:01:46.384
که شغل راحتی باشه

00:01:46.408 --> 00:01:50.426
و من با هیچ پرسش سخت اخلاقی 
مواجه نشوم؟

00:01:51.022 --> 00:01:52.551
پس کامپیوترها را انتخاب کردم.

00:01:52.575 --> 00:01:53.679
(خنده حاضرین)

00:01:53.703 --> 00:01:57.113
خب، ها، ها، ها!
همه خنده‌ها برای من هستند.

00:01:57.137 --> 00:01:59.891
امروزه، دانشمندان کامپیوتر
در حال ساخت یک سیستم عامل هستند

00:01:59.915 --> 00:02:04.124
که آنچه یک میلیارد آدم
هر روز می‌بینند روکنترل می‌کنه

00:02:05.052 --> 00:02:08.874
آنها خودروهایی را ساختند که می‌توانند
تصمیم بگیرند که چه کسی زیر بگیرند.

00:02:09.707 --> 00:02:12.920
آن ها حتی در حال ساخت ماشین‌هایی هستند،
تسلیحاتی

00:02:12.944 --> 00:02:15.229
که ممکنه آدم ها رو در جنگ بکشه.

00:02:15.253 --> 00:02:18.024
همه اینها سقوط اخلاق‌ است.

00:02:19.183 --> 00:02:21.241
هوشمندی ماشین اینجاست.

00:02:21.823 --> 00:02:25.297
ما در حال استفاده از محاسباتی
هستیم که همه تصمیمات ما را مرتب می‌کنه،

00:02:25.321 --> 00:02:27.207
همچنین نوع‌های مختلف تصمیم‌ها را مرتب می‌کند.

00:02:27.231 --> 00:02:32.403
ما سوالاتی را می پرسیم هستیم که 
هیچ جواب معین درستی ندارند،

00:02:32.427 --> 00:02:33.629
آنها ذهنی هستند

00:02:33.653 --> 00:02:35.978
و بدون جواب معین 
و پر محتوا هستند.

00:02:36.002 --> 00:02:37.760
ما پرسش‌هایی شبیه اینها را می‌پرسیم:

00:02:37.784 --> 00:02:39.434
«شرکت کی باید استخدام کند؟»

00:02:40.096 --> 00:02:42.855
«بکدام بروزرسانی از کدام دوست 
باید نشان داده شود؟»

00:02:42.879 --> 00:02:45.145
«کدام متهم بیشتر شبیه خلافکارهاست؟»

00:02:45.514 --> 00:02:48.568
«کدام بخش خبرها یا فیلم باید به مردم 
توصیه بشه؟»

00:02:48.592 --> 00:02:51.964
ببیند، بله، ما مدت‌هاست است که در حال 
استفاده از کامپیوترها هستیم

00:02:51.988 --> 00:02:53.505
اما این بار فرق داره.

00:02:53.529 --> 00:02:55.596
این یک چرخش تاریخی است،

00:02:55.620 --> 00:03:00.957
زیرا ما نمی‌توانیم محاسبات را 
برای تصمیم‌های ذهنی نگه داریم

00:03:00.981 --> 00:03:06.401
مانند روش محاسباتی
برای پرواز هواپیما، ساخت پل‌ها

00:03:06.425 --> 00:03:07.684
و به ماه رفتن.

00:03:08.449 --> 00:03:11.708
آیا هواپیماها امن هستند؟
آیا این پل فرو می‌ریزد؟

00:03:11.732 --> 00:03:16.230
این چنین است، ما منصفانه و براساس معیارهای
روشن توافق کردیم

00:03:16.254 --> 00:03:18.493
و ما قوانین طبیعت را برای راهنمایی داریم

00:03:18.517 --> 00:03:21.911
ما چیزی شبیه مجری ها و معیارها 
برای تصمیم گیری

00:03:21.935 --> 00:03:25.898
درکارهای انسان آشفته نداریم.

00:03:25.922 --> 00:03:30.159
برای انجام کارهای پیچیده‌تر،
تا نرم افزارهای ما بیشتر قدرتمند می‌شوند،

00:03:30.183 --> 00:03:33.956
اما این می تواند کمتر شفاف 
و بیشتر پیچیده باشد.

00:03:34.542 --> 00:03:36.582
اخیرا، در دهه گذشته

00:03:36.606 --> 00:03:39.335
الگوریتم های پیچیده
با گام‌های بلندی ساخته شده‌اند.

00:03:39.359 --> 00:03:41.349
آنها می‌توانند صورت انسان را
بازشناسایی کنند.

00:03:41.985 --> 00:03:44.040
آنها می‌توانند دست خط را تشخیص بدهند.

00:03:44.436 --> 00:03:46.502
آنها می‌توانند تقلب در کارت اعتباری را
کشف کنند

00:03:46.526 --> 00:03:47.715
و اسپم‌ها را مسدود کنند

00:03:47.739 --> 00:03:49.776
و آنها می‌توانند زبان‌ها را ترجمه کنند،

00:03:49.800 --> 00:03:52.374
می توانند تومورها را در تصاویر پزشکی
کشف کنند.

00:03:52.398 --> 00:03:54.603
آنها می‌توانند در بازیهای
شطرنج و گو از آدمها ببرند.

00:03:55.264 --> 00:03:59.768
بیشتر این پیشرفت‌ها از روشی به نام 
"یادگیری ماشین" آمده‌اند.

00:04:00.175 --> 00:04:03.362
یادگیری ماشین با برنامه نویسی سنتی 
متفاوت هست،

00:04:03.386 --> 00:04:06.971
که به کامپیوتر جزئیات دقیق
دستورات پر زحمت را میدهید.

00:04:07.378 --> 00:04:11.560
این بیشتر شبیه اینه که شما یک سیستم
گرفته‌اید و اطلاعات زیادی به آن می‌خورانید

00:04:11.584 --> 00:04:13.240
شامل اطلاعات بدون ساختار،

00:04:13.264 --> 00:04:15.542
مانند اطلاعاتی که ما در زندگی دیجیتال خود
تولید می‌کنیم.

00:04:15.566 --> 00:04:18.296
و سیستمی که بوسیله گردش در بین اطلاعات
یاد میگیرد.

00:04:18.669 --> 00:04:20.195
و همچنین بحرانی

00:04:20.219 --> 00:04:24.599
آن سیستم‌هایی که زیر یک پاسخ سیگنال
منطقی عمل نمی کنند

00:04:24.623 --> 00:04:27.582
آن ها یک پاسخ ساده تولید نمی کنند
این ها بیشتر "احتمال" هستند

00:04:27.606 --> 00:04:31.089
"این یک احتمالی است که بیشتر شبیه
آنچه شما دنبال آن هستید "

00:04:32.023 --> 00:04:35.093
حالا، بالاتر این است که: این شیوه
واقعا قدرتمند است.

00:04:35.117 --> 00:04:37.193
رییس سیستم‌های هوش مصنوعی (AI) گوگل
این را نام گذاری کرد:

00:04:37.217 --> 00:04:39.414
"اثر غیر منطقی اطلاعات"

00:04:39.791 --> 00:04:41.144
قسمت بدترش این است که:

00:04:41.738 --> 00:04:44.809
ما واقعا نمی‌فهمیم 
سیستم چه یاد می‌گیرد.

00:04:44.833 --> 00:04:46.420
در حقیقت این قدرت آن است.

00:04:46.946 --> 00:04:50.744
این کمتر شبیهِ دادنِ دستورالعمل
به کامپیوتر است،

00:04:51.200 --> 00:04:55.264
این بیشتر شبیه یاد دادن به یک
توله ماشین زندهاست

00:04:55.288 --> 00:04:57.659
ما واقعا نمی‌فهمیم و کنترل نمی‌کنیم.

00:04:58.362 --> 00:04:59.913
خب این مشکل ماست.

00:05:00.427 --> 00:05:04.689
این یک مشکل است وقتی که سیستم هوش مصنوعی
چیزها را اشتباه یاد میگیرد.

00:05:04.713 --> 00:05:08.253
این همچنین یک مشکل است وقتی که 
این چیزها را درست یاد میگیرد،

00:05:08.277 --> 00:05:11.905
زیرا ما حتی نمی دانیم کدام به کدام است
وقتی که این یک مشکل درونی است.

00:05:11.929 --> 00:05:14.268
ما نمی‌دانیم در حال فکر کردن 
به چه چیزی است

00:05:15.493 --> 00:05:19.176
یک الگوریتم استخدام را فرض کنید--

00:05:20.123 --> 00:05:24.434
یک سیستمی که مردم را با استفاده از 
سیستم یادگیری ماشین استخدام می کند.

00:05:25.052 --> 00:05:28.631
مانند یک سیستمی که بر اساس اطلاعات
کارمندان قبلی آموزش دیده شده است

00:05:28.655 --> 00:05:31.246
و دستور دارد که پیدا کند و استخدام کند

00:05:31.270 --> 00:05:34.308
مردمی که بهروری بالایی در شرکت
دارند.

00:05:34.814 --> 00:05:35.967
به نظر خوب میاد.

00:05:35.991 --> 00:05:37.990
من یک بار در کنفرانسی حضور داشتم

00:05:38.014 --> 00:05:41.139
که مدیران منابع انسانی و مدیران اجرایی
دور هم جمع شده بودند،

00:05:41.163 --> 00:05:42.369
افراد رده بالای شرکت‌ها

00:05:42.393 --> 00:05:43.952
که از این سیستم‌های برای استخدام
استفاده می کردند.

00:05:43.976 --> 00:05:45.622
آنها خیلی هیجان زده بودند

00:05:45.646 --> 00:05:50.299
آنها فکر می کردند که این استخدام را بیشتر هدفمند 
و کمتر مغروضانه خواهند بود،

00:05:50.323 --> 00:05:53.323
و به خانم ها و اقلیت 
یک شانس بهتری میدهد

00:05:53.347 --> 00:05:55.535
بر خلاف غرض‌ورزی مدیران منابع انسانی

00:05:55.559 --> 00:05:58.402
ببینید--
استخدام افراد غرض ورزانه است.

00:05:59.099 --> 00:06:00.284
من میدانم.

00:06:00.308 --> 00:06:03.313
منظورم اینه، در اولین شغل من به عنوان
برنامه نویس

00:06:03.337 --> 00:06:07.205
مدیر بخش من گاهی اوقات

00:06:07.229 --> 00:06:10.982
در اول صبح یا آخر عصر پیش من می‌آمد

00:06:11.006 --> 00:06:14.068
و میگفت: "زینب بیا بریم ناهار"

00:06:14.724 --> 00:06:16.891
من به خاطر زمان‌های عجیب گیج می‌شدم

00:06:16.915 --> 00:06:19.044
الان ساعت ۴ است، ناهار؟

00:06:19.068 --> 00:06:22.162
من شکست می‌خوردم
من همیشه برای ناهار مجانی می‌رفتم

00:06:22.618 --> 00:06:24.685
بعدها فهمیدم که چه اتفاقی می‌افتاد

00:06:24.709 --> 00:06:29.255
مدیران بالایی من در انتخابشان برای استخدام 

00:06:29.279 --> 00:06:32.392
یک دختر نوجوان که کفش کتانی
و جین در محل کار می‌پوشید

00:06:32.416 --> 00:06:36.346
برای انجام یک کار جدی
اشتباه نکرده بودند.

00:06:37.174 --> 00:06:39.376
من خوب کار می‌کردم،
اما به نظرمیآمد که من مناسب نیستم

00:06:39.400 --> 00:06:41.099
سن و جنسیتم نیز اشتباه بود.

00:06:41.123 --> 00:06:44.469
خُب نادیده گرفتن شدن جنسیت و نژاد من

00:06:44.493 --> 00:06:46.358
قطعا چیزی خوبی برای من بود.

00:06:47.031 --> 00:06:50.372
اما با این سیستم ها
بغرنج‌تر و پیچیده‌تر شده و دلیلیش اینجاست:

00:06:50.968 --> 00:06:56.759
اخیرا، سیستم‌های محاسبه‌گر می‌توانند 
به همه چیزهای شما

00:06:56.783 --> 00:06:58.655
از طریق خرده اطلاعات
دیجیتالی شما پی‌ببرند،

00:06:58.679 --> 00:07:01.012
حتی اگر شما آن چیزها را فاش نکرده باشید.

00:07:01.506 --> 00:07:04.433
آنها به گرایش‌های جنسی‌تان ،

00:07:04.994 --> 00:07:06.300
ویژگی‌های شخصی‌تان،

00:07:06.859 --> 00:07:08.232
دانسته‌های سیاسی‌تان پی‌ببرند.

00:07:08.830 --> 00:07:12.515
آنها قدرت پیش بینی با صحت بالایی را دارند.

00:07:13.362 --> 00:07:15.940
به یاد داشته باشید، برای چیزهایی که شما
حتی آنها را فاش نکرده‌اید

00:07:15.964 --> 00:07:17.555
نتیجه گیری‌ و استنتاج است.

00:07:17.579 --> 00:07:20.840
من یک دوستی دارم که سیستم های 
محاسبه گری را توسعه می‌دهد

00:07:20.864 --> 00:07:24.505
تا شانس افسردگی بالینی یا بعد از وضع حمل
را پیش بینی کند

00:07:24.529 --> 00:07:25.945
با استفاده از اطلاعات رسانه های اجتماعی

00:07:26.676 --> 00:07:28.103
نتیجه ها هیجان انگیز هستند.

00:07:28.492 --> 00:07:31.849
سیستمش احتمال افسردگی را
می‌توان

00:07:31.873 --> 00:07:35.776
ماه‌های قبل از شروع علائم 
بیماری را پیش‌بینی کند--

00:07:35.800 --> 00:07:37.173
ماه‌های قبل.

00:07:37.197 --> 00:07:39.443
هیچ علامتی از بیماری نیست، 
ولی پیش بینی می‌شود.

00:07:39.467 --> 00:07:44.279
او امیدوار است بزودی این را برای مداخلات
(روانشناسی) استفاده کند . عالیه.

00:07:44.911 --> 00:07:46.951
اما حالا این در فضای استخدام قرار دهید.

00:07:48.027 --> 00:07:51.073
بنابراین در این کنفرانس منبع مدیران انسانی

00:07:51.097 --> 00:07:55.806
من به یک مدیر سطح بالا 
در یک شرکت بزرگ نزدیک شدم

00:07:55.830 --> 00:08:00.408
و به او گفتم: "ببین، چه میشد اگر من برای 
تو فردی ناشناخته می‌شدم؟"

00:08:00.432 --> 00:08:06.981
آیا سیستم تو در حال حذف مردم 
با احتمال بالای افسردگی در آینده، است ؟

00:08:07.761 --> 00:08:11.137
آن ها الان افسرده نیستند، فقط شاید در 
آینده به احتمال زیاد دچار افسردگی شوند.

00:08:11.923 --> 00:08:15.329
یا اگر زنانی که احتمال دارد در یکی 
دو سال آینده باردار شوند

00:08:15.353 --> 00:08:17.939
ولی الان حامله نیستد
را کنار گذارده شوند؟

00:08:18.844 --> 00:08:24.480
اگر این افراد پرخاشکر را استخدام شوند
زیرا آن فرهنگ محیط کاریت است

00:08:25.173 --> 00:08:27.864
تو با استفاده از نگاه کردن به
تقسیم بندی جنسبت نمی توانی بگویی

00:08:27.888 --> 00:08:29.390
آن ها ممکنه متعادل باشند.

00:08:29.414 --> 00:08:32.971
و چون این یادگیری ماشین است و 
برنامه نویسی سنتی نیست

00:08:32.995 --> 00:08:37.902
هیچ متغیری وجود ندارد که 
"بیشترین خطر افسردگی " نام گذاری شود

00:08:37.926 --> 00:08:39.759
"بیشترین خطر حاملگی"

00:08:39.783 --> 00:08:41.517
"مقیاس پرخاشگری مردان"

00:08:41.995 --> 00:08:45.674
نه تنها نمی دانست 
چگونه سیستم شما، انتخاب می کنه

00:08:45.698 --> 00:08:48.021
شما حتی نمی دانی که
در کجا جستجو می‌کند

00:08:48.045 --> 00:08:49.291
این یک جعبه سیاه است.

00:08:49.315 --> 00:08:52.122
این قدرت پیش گویی دارد
اما شما این را نمی‌فهمی

00:08:52.486 --> 00:08:54.855
من پرسسیدم"حفاظت چیست؟"

00:08:54.879 --> 00:08:58.552
« آیا باید مطمئن شوی که جعبه سیاه تان
کار مشکوکی انجام نمی‌دهد؟»

00:09:00.863 --> 00:09:04.741
او به من به نگاه کرد مثل اینکه من
پا روی دُم ده تا توله گذاشتم!

00:09:04.765 --> 00:09:06.013
(خنده حاضرین)

00:09:06.037 --> 00:09:08.078
او به من خیره شد و گفت:

00:09:08.556 --> 00:09:12.889
«نمی‌خوام کلمه دیگری در این باره بشنوم»

00:09:13.458 --> 00:09:15.492
و او برگشت و قدم زنان دور شد.

00:09:16.064 --> 00:09:17.550
به خاطر داشته باشید او بی‌ادب نبود.

00:09:17.574 --> 00:09:23.882
کاملا روشن بود با نگاهش می‌گفت: نمی‌دونم،
این مشکل من نیست، برو.

00:09:23.906 --> 00:09:25.152
(خنده حاضرین)

00:09:25.862 --> 00:09:29.701
نگاه کنید، یک سیستم ممکن است
حتی کمتر جانبدارانه باشد

00:09:29.725 --> 00:09:31.828
تا مدیران انسانی در همان زمینه.

00:09:31.852 --> 00:09:33.998
و این میتونه یک حس مالی ایجاد کنه

00:09:34.573 --> 00:09:36.223
اما این می‌تونه منجر بشه

00:09:36.247 --> 00:09:40.995
به یک یکنواختی اما یواشکی بستن
بازار کار مردم

00:09:41.019 --> 00:09:43.312
که با ریسک بالای افسردگی همراه هستند.

00:09:43.753 --> 00:09:46.349
آیا این نوع اجتماعی است 
که ما می‌خواهیم بسازیم؟

00:09:46.373 --> 00:09:48.658
بدون حتی دانستن اینکه ما این را
انجام دادیم

00:09:48.682 --> 00:09:52.646
زیرا ما ساختن تصمیم را تبدیل کردیم
به ماشین که ما سرانجامش را نمی‌فهمیم

00:09:53.265 --> 00:09:54.723
و مسئله دیگر این است:

00:09:55.314 --> 00:09:59.766
این سیستم ها اغلب روی اطلاعات تولید شده
توسط کارهای ما آموزش داده می‌شوند،

00:09:59.790 --> 00:10:01.606
آثار به جای مانده از انسان.

00:10:02.188 --> 00:10:05.996
خب، آنها فقط می‌توانند
تمایلات ما را منعکس کنند،

00:10:06.020 --> 00:10:09.613
و این سیستم‌های می‌توانند
تمایلات ما را انتخاب کنند

00:10:09.637 --> 00:10:10.950
و آن را تقویت کرده

00:10:10.974 --> 00:10:12.392
و آن را دوباره به ما نشان دهند،

00:10:12.416 --> 00:10:13.878
در حالی که به خودمان می‌گویم،

00:10:13.902 --> 00:10:17.019
«ما فقط در حال بررسی هستیم.»

00:10:18.314 --> 00:10:20.991
محققان در شرکت گوگل دریافتند،

00:10:22.134 --> 00:10:27.447
زنان نسبت به مردان احتمال کمتری دارد که
برای مشاغل با حقوق بالاتر قدام کنند.

00:10:28.463 --> 00:10:30.993
و نام‌های آفریقایی-آمریکایی را که جستجو کنید

00:10:31.017 --> 00:10:35.723
احتمال بیشتر دارد که پیشینه جرم نشان دهد،

00:10:35.747 --> 00:10:37.314
حتی وقتی که واقعا جرمی وجود ندارد.

00:10:38.693 --> 00:10:42.242
مانند تمایلات پنهان و الگوریتم جعبه سیاه

00:10:42.266 --> 00:10:46.239
که گاهی محققات آن را تحت پوشش قرار نمی دهند
و ما از آن گاهی اطلاع نداریم.

00:10:46.263 --> 00:10:48.924
که می توانیم پر‌آمدهایی زندگی داشته باشد.

00:10:49.958 --> 00:10:54.117
در ویسکانسین یک متهم به شش سال زندان 
محکوم شد

00:10:54.141 --> 00:10:55.496
برای فرار از پلیس.

00:10:56.824 --> 00:10:58.010
شما ممکنه این را ندانید،

00:10:58.034 --> 00:11:02.032
اما الگوریتم به طور افزاینده‌ای در آزادی
مشروط و صدور حکم در حال استفاده هستند

00:11:02.056 --> 00:11:05.011
او می خواهد بداند:
چگونه این نمره محاسبه می شود؟

00:11:05.795 --> 00:11:07.460
این یک جعبه سیاه تجاری است

00:11:07.484 --> 00:11:11.689
شرکت درخواست اینکه الگوریتم 
در دادگاه به چالش کشیده بشود را رد کرد.

00:11:12.396 --> 00:11:17.928
اما پروپابلیکا، یک موسسه تحقیقاتی
غیرانتفاعی خیلی از الگوریتم ها را

00:11:17.952 --> 00:11:19.968
با اطلاعات عمومی ای که آن ها می‌توانند 
پیدا کنند بررسی می‌کنند.

00:11:19.992 --> 00:11:22.308
و دریافتند که این یک نتیجه از تمایلات بوده

00:11:22.332 --> 00:11:25.961
و این قدرت پیش‌بینی اشتباه، 
نه فقظ شانسی( بلکه به عمد)

00:11:25.985 --> 00:11:30.401
و به طور اشتباه متهمان سیاه را به عنوان
مجرمان آینده دوبرابر نرخ مجرمان سفید

00:11:30.425 --> 00:11:34.320
برچست گذاری کرده بود.

00:11:35.891 --> 00:11:37.455
خب، به این مورد دقت کنید:

00:11:38.103 --> 00:11:41.955
این خانم برای برداشتن 
دخترخوانده‌اش دیر رسید

00:11:41.979 --> 00:11:44.054
از یک مدرسه در بروارد ایالت فلوریدا،

00:11:44.757 --> 00:11:47.113
با دوستش از خیابان می دوید.

00:11:47.137 --> 00:11:51.236
آن ها یک دوچرخه بچه و یک اسکوتر
روی ایوان که قفل نشده بود را نشان دادند

00:11:51.260 --> 00:11:52.892
و احمقانه روی آن پرید

00:11:52.916 --> 00:11:55.515
وقتی آنها در حال سرعت گرفتن بودند
یک زن آمد و گفت

00:11:55.539 --> 00:11:57.744
هی! این دوچرخه بچه من است

00:11:57.768 --> 00:12:01.062
آن ها دوچرخه را رها کردن و دور شدند
ولی آن ها دستگیر شدند

00:12:01.086 --> 00:12:04.723
او اشتباه کرد و او احمق بود 
ولی او تنها ۱۸ سال داشت

00:12:04.747 --> 00:12:07.291
او یک تعدادی جرم‌های کوچک داشت.

00:12:07.808 --> 00:12:12.993
ضمنا، آن مرد برای سرقت از فروشگاه
هوم دیپو دستگیر شده بود--

00:12:13.017 --> 00:12:15.941
۸۵ دلار، که ارزشش به اندازه جرم کوچک بود.

00:12:16.766 --> 00:12:21.325
اما او محکومیت دو سرقت مسلحانه داشت.

00:12:21.955 --> 00:12:25.437
اما الگوریتم احتمال بالای جرم برای این زن
نشان می‌داد، و نه برای این مرد.

00:12:26.746 --> 00:12:30.620
دو سال بعد، پروپابلیکا یافت
که این نباید در حبس باشد.

00:12:30.644 --> 00:12:33.194
و با سایقه‌ای را که داشت
برای او پیدا کردن شغل مشگل بود.

00:12:33.218 --> 00:12:35.294
از طرف دیگر این مرد زندانی شد

00:12:35.318 --> 00:12:39.154
و برای گناه گذشته‌اش 
برای هشت سال زندانی خواهد بود.

00:12:40.088 --> 00:12:43.457
روشن است، ما نیاز داریم تا 
جعبه سیاهمان را بازبینی و بررسی کنیم

00:12:43.481 --> 00:12:46.096
و نه آن ها را، بلکه این نوع 
قدرت چک نشده را حسابرسی کنیم

00:12:46.120 --> 00:12:48.999
(تشویق حضار)

00:12:50.087 --> 00:12:54.329
بررسی و باربینی خوب و مهم است 
اما آنها تمام مشکلات ما را حل نمی کنند.

00:12:54.353 --> 00:12:57.101
الگوریتم قدرت خبری فیسبوک
را در نظر بگیرید.

00:12:57.125 --> 00:13:01.968
می‌دانید، کسی که همه چیز را رتبه بندی می‌کند
و تصمیم میگیرد که چه به شما نشان دهد

00:13:01.992 --> 00:13:04.276
از همه دوستان و همه 
صفحه‌هایی که شما دنبال می‌کنید.

00:13:04.898 --> 00:13:07.173
باید به شما عکس بچه‌ی دیگه را نشان دهد؟

00:13:07.197 --> 00:13:08.393
(خنده)

00:13:08.417 --> 00:13:11.013
یک یادداشت عبوس از یک آشنا؟

00:13:11.449 --> 00:13:13.305
یک خبر مهم اما قسمت‌های سختش؟

00:13:13.329 --> 00:13:14.811
هیچ جواب درستی وجود ندارد.

00:13:14.835 --> 00:13:17.494
فیس بوک برای مشغولیت بیشتر
در سایت بهینه شده :

00:13:17.518 --> 00:13:18.933
لایک، اشتراگ گذاری، کامنت

00:13:20.168 --> 00:13:22.864
در آگوست ۲۰۱۴

00:13:22.888 --> 00:13:25.550
در شهر فرگوست ایالت میسوری معترضان

00:13:25.574 --> 00:13:29.991
بعد از کشتن یک نوجوان آفریقایی- آمریکایی
به وسیله یک پلیس سفید پوست

00:13:30.015 --> 00:13:31.585
زیر رویداد مبهم شورش کردند،

00:13:31.974 --> 00:13:33.981
خبرهای معترضان در همه جا بود

00:13:34.005 --> 00:13:36.690
الگورتیم من فید‌های تویتر را فیلتر نکرد

00:13:36.714 --> 00:13:38.664
اما در فیس بوکم هیچ جا باز نبود.

00:13:39.182 --> 00:13:40.916
آیا اینها دوستان فیسبوکی من بودند؟

00:13:40.940 --> 00:13:42.972
من الگوریتم فیسبوکم را
غیر فعال کردم،

00:13:43.472 --> 00:13:46.320
که سخت بود زیرا فیسبوک
خواسته های شما را حفظ میکند

00:13:46.344 --> 00:13:48.380
تا شما را زیر کنترل الگوریتم ها نگه دارد،

00:13:48.404 --> 00:13:50.642
و می بینید که دوستان من در حال 
صحبت کردن درباره این حادثه بودند.

00:13:50.666 --> 00:13:53.175
این فقط الگوریتمی بود که این را
به من نشان من داد.

00:13:53.199 --> 00:13:56.241
من تحقیق کردم و فهمیدم
که این یه مشکل شایع بود

00:13:56.265 --> 00:14:00.078
حادثه فرگوست یک الگوریتم دوستانه نبود.

00:14:00.102 --> 00:14:01.273
دوست داشتنی نبود

00:14:01.297 --> 00:14:02.849
چه کسی می خواد تا
روی "like" کلیک کنید؟

00:14:03.500 --> 00:14:05.706
این حتی ساده نیست 
تا کامنتی روی آن قرار دهید.

00:14:05.730 --> 00:14:07.101
بدون "like" و کامنت

00:14:07.125 --> 00:14:10.417
احتمالا الگوریتم برای افراد
کمتری را نشان داده می‌شد،

00:14:10.441 --> 00:14:11.983
بنابراین ما نتوانستیم این را ببینیم

00:14:12.946 --> 00:14:14.174
در عوض، آن هفته،

00:14:14.198 --> 00:14:16.496
الگوریتم فیس بوک این را برجسته کرده بود:

00:14:16.520 --> 00:14:18.746
این چالش سطل آب یخ است.

00:14:18.770 --> 00:14:22.512
علت ارزش، خالی کردن آب یخ، 
کمک به موسسه خیریه خوب است

00:14:22.536 --> 00:14:24.440
اما این الگوریتم دوستانه عالی بود.

00:14:25.219 --> 00:14:27.832
که ماشین این تصمیم را برای ما گرفت.

00:14:27.856 --> 00:14:31.353
و خیلی مهم است اما گفتگوی سختی است

00:14:31.377 --> 00:14:32.932
ممکن است خفه شده باشد.

00:14:32.956 --> 00:14:35.652
آیا فیسبوک فقط یک کانال داشت؟

00:14:36.117 --> 00:14:39.914
خُب، این سیستم ها 
می‌توانند اشتباه باشند

00:14:39.938 --> 00:14:42.674
در راه هایی که شباهت
به سیستم انسانی ندارد.

00:14:42.698 --> 00:14:45.620
آیا شما واتسون، سیستم 
ماشین هوشمند آی بی ام

00:14:45.644 --> 00:14:48.772
که با انسان مسابقه جوپرتری را داد
را به خاطر دارید؟

00:14:49.131 --> 00:14:50.559
واتسون بازیگر خوبی در مسابقه بود.

00:14:50.583 --> 00:14:54.152
اما در آخرین جوپرتی، از واتسون پرسیده شد

00:14:54.659 --> 00:14:57.591
«آیا بزرگترین فرودگاه که برای یک قهرمان
جنگ جهانی دوم نام گذاری شد"

00:14:57.615 --> 00:14:59.867
این دومین فرودگاه بزرگ برای 
مبارزه جنگ جهانی دوم نامگذاری شد.»

00:14:59.891 --> 00:15:01.269
(...)

00:15:01.582 --> 00:15:02.764
شیکاگو.

00:15:02.788 --> 00:15:04.158
دو فرد درست جواب دادند

00:15:04.697 --> 00:15:09.045
واتسون در طرف دیگر پاسخ داد «تورنتو»--

00:15:09.069 --> 00:15:10.887
برای یک شهر آمریکایی!

00:15:11.596 --> 00:15:14.497
سیستم موثر همچنین یک اشتباه کرد

00:15:14.521 --> 00:15:18.172
که یک فرد هیچ وقت این اشتباه را
نخواهد کرد، حتی یک کلاس دومی.

00:15:18.823 --> 00:15:21.932
ماشین هوشمند ما میتواند شکست بخورد

00:15:21.956 --> 00:15:25.056
در جاهایی که نمی تواند
الگوی خطای انسان ها را متناسب کند

00:15:25.080 --> 00:15:28.030
در جاهایی که ما انتظار
و آمادگی برای آن نخواهیم داشت

00:15:28.054 --> 00:15:31.692
این نکبت بار خواهد بود که کسی
که واجد شرایط هست شفلی را نگیرد،

00:15:31.716 --> 00:15:35.443
اما سه برابر آن بدتر اینکه به دلیل خرده رفتارها

00:15:35.467 --> 00:15:36.899
در روال عادی زندگی فرد آن شغل را نگیرد.

00:15:36.923 --> 00:15:38.502
(خنده حضار)

00:15:38.526 --> 00:15:41.312
در ماه می ۲۰۱۰،

00:15:41.336 --> 00:15:45.380
یک خرابی کوتاه مدت ناشی 
از یک حلقه فیدبک تقویت شد در وال استریت

00:15:45.404 --> 00:15:48.432
در الگوریتم« فروش» وال استریت

00:15:48.456 --> 00:15:52.640
یک تریلیون دلار ارزش را
در ۳۶ دقیقه از بین برد.

00:15:53.722 --> 00:15:55.909
من حتی نمی‌خوام در مورد
معنی‌های «خطا»

00:15:55.933 --> 00:15:59.522
در زمینه سلاح های کشنده خودکار فکر کنم.

00:16:01.894 --> 00:16:05.684
خب بله، انسان‌ها همیشه تمایلات را می‌سازند.

00:16:05.708 --> 00:16:07.884
تصمیم گیرنده‌ها و دربان‌ها

00:16:07.908 --> 00:16:11.401
در دادگاه ها و در خبر ها و در جنگ...

00:16:11.425 --> 00:16:14.463
آنها اشتباه می کنند،
اما این دقیقا نکته مورد نظر من است.

00:16:14.487 --> 00:16:18.008
ما نمی توانیم از این سوالهای
مشگل فرارکنیم.

00:16:18.596 --> 00:16:22.112
ما نمی‌توانیم مسئولیت هایمان
را در قبال ماشین ها نادیده بگیریم.

00:16:22.676 --> 00:16:26.884
(تشویق)

00:16:29.089 --> 00:16:33.536
هوش مصنوعی نمی‌تواند به ما یک کارت 
«خارج شدن از اخلاق به صورت رایگان» بدهد

00:16:34.742 --> 00:16:38.123
دانشمند اطلاعات، فرد بنسون
این را "شستشوی ریاضی" می‌نامد

00:16:38.147 --> 00:16:39.536
ما به این تضاد نیاز داریم.

00:16:39.560 --> 00:16:44.948
ما نیاز داریم تا یک الگوریتم بد گمانی را 
با بررسی دقیق و موشکافانه رشد دهیم.

00:16:45.380 --> 00:16:48.578
ما نیاز داریم تا مطمئن باشیم
که مسولیت الگوریتمی داریم،

00:16:48.602 --> 00:16:51.047
حسابرسی و شفافیت معنایی نیاز داریم.

00:16:51.380 --> 00:16:54.614
ما نیاز داریم تا قبول کنیم که
آورده های ریاضی و محاسباتی

00:16:54.638 --> 00:16:57.608
برای ارزش انباشته و به هم ریخته و امور انسانی

00:16:57.632 --> 00:17:00.016
عینیت ندارد.

00:17:00.040 --> 00:17:03.673
بلکه ، پیچیدگی امور انسانی به
الگوریتم ها حتما غلبه می‌کند.

00:17:04.148 --> 00:17:07.635
بله ما می توانیم و ما باید
از محاسبات استفاده کنیم

00:17:07.659 --> 00:17:09.673
تا برای داشتن تصمیمات بهتر
به خودمان کمک کنیم

00:17:09.697 --> 00:17:15.029
اما ما اعتراف می‌کنیم 
به مسئولیت اخلاقی و قضاوت

00:17:15.053 --> 00:17:17.871
و استفااه از الگورتیم هایی با آن چارچوب

00:17:17.895 --> 00:17:22.830
نه به عنوان وسیله ای برای کناره گیری
و واگذاری مسئولیت هایمان

00:17:22.854 --> 00:17:25.308
به یک انسان دیگر.

00:17:25.807 --> 00:17:28.416
هوش مصنوعی اینجاست.

00:17:28.440 --> 00:17:31.861
این بدان معناست که ما باید محکم‌تر

00:17:31.885 --> 00:17:34.032
ارزش‌ها و اخلاق انسانی را نگه داریم.

00:17:34.056 --> 00:17:35.210
متشکرم

00:17:35.234 --> 00:17:40.254
(تشویق)

