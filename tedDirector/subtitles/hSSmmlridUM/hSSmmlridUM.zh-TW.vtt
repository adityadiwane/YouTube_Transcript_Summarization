WEBVTT
Kind: captions
Language: zh-TW

00:00:00.000 --> 00:00:07.000
譯者: Helen Chang
審譯者: SF Huang

00:00:12.559 --> 00:00:18.681
大一時我開始了第一份工作：
程式設計師，

00:00:18.685 --> 00:00:20.192
當時我還算是個青少女。

00:00:20.709 --> 00:00:24.081
開始為軟體公司寫程式後不久，

00:00:24.619 --> 00:00:28.254
公司裡的一個經理走到我身邊，

00:00:28.278 --> 00:00:29.546
悄悄地問：

00:00:30.049 --> 00:00:32.910
「他能判斷我是否說謊嗎？」

00:00:33.626 --> 00:00:35.703
當時房裡沒別人。

00:00:36.772 --> 00:00:41.161
「『誰』能不能判斷你說謊與否？
而且，我們為什麼耳語呢？」

00:00:42.086 --> 00:00:45.193
經理指著房裡的電腦，問：

00:00:45.217 --> 00:00:48.313
「『他』能判斷我是否說謊嗎？」

00:00:49.433 --> 00:00:53.795
當時那經理與接待員有曖昧關係。

00:00:53.819 --> 00:00:54.931
（笑聲）

00:00:54.955 --> 00:00:56.721
那時我仍是個青少女。

00:00:57.267 --> 00:00:59.286
所以，我用耳語大聲地回答他：

00:00:59.310 --> 00:01:02.934
「能，電腦能判斷你撒謊與否。」

00:01:02.958 --> 00:01:04.764
（笑聲）

00:01:04.788 --> 00:01:07.711
沒錯，我笑了，
但可笑的人是我。

00:01:07.759 --> 00:01:11.027
如今，有些計算系統

00:01:11.027 --> 00:01:14.859
靠分析、判讀面部表情，
就能判斷出情緒狀態，

00:01:14.859 --> 00:01:16.823
甚至判斷是否說謊。

00:01:17.428 --> 00:01:21.581
廣告商，甚至政府也對此很感興趣。

00:01:22.139 --> 00:01:24.001
我之所以成為程式設計師

00:01:24.025 --> 00:01:27.138
乃因自幼便極為喜愛數學和科學。

00:01:27.762 --> 00:01:30.870
過程中我學到核子武器，

00:01:30.894 --> 00:01:33.846
因而變得非常關心科學倫理。

00:01:33.870 --> 00:01:35.074
我很苦惱。

00:01:35.098 --> 00:01:37.739
但由於家庭狀況，

00:01:37.763 --> 00:01:41.061
我必須儘早就業。

00:01:41.085 --> 00:01:42.688
因此我告訴自己，

00:01:42.688 --> 00:01:46.204
選擇一個在科技領域中
能簡單地找到頭路，

00:01:46.228 --> 00:01:50.346
又無需處理涉及倫理道德
這類麻煩問題的工作吧。

00:01:50.842 --> 00:01:52.371
所以我選擇了電腦。

00:01:52.395 --> 00:01:53.499
（笑聲）

00:01:53.523 --> 00:01:56.933
是啊，哈哈哈！大家都笑我。

00:01:56.957 --> 00:01:58.995
如今，電腦科學家

00:01:58.995 --> 00:02:03.944
正建構著可控制數十億人
每天接收訊息的平台。

00:02:04.872 --> 00:02:08.694
他們設計的汽車
可以決定要輾過哪些人。

00:02:09.527 --> 00:02:15.060
他們甚至建造能殺人的
戰爭機器和武器。

00:02:15.073 --> 00:02:17.844
從頭到尾都是倫理的問題。

00:02:19.003 --> 00:02:21.061
機器智慧已經在此。

00:02:21.667 --> 00:02:25.141
我們利用計算
來做各種決策，

00:02:25.141 --> 00:02:27.027
同時也是種新形態的決策。

00:02:27.051 --> 00:02:32.223
我們以計算來尋求解答，
但問題沒有單一的正解，

00:02:32.247 --> 00:02:35.789
而是主觀、開放、具價值觀的答案。

00:02:35.822 --> 00:02:37.580
問題像是，

00:02:37.604 --> 00:02:39.564
「公司應該聘誰？」

00:02:39.916 --> 00:02:42.675
「應該顯示哪個朋友的哪項更新？」

00:02:42.699 --> 00:02:44.965
「哪個罪犯更可能再犯？」

00:02:45.334 --> 00:02:48.388
「應該推薦哪項新聞或哪部電影？」

00:02:48.412 --> 00:02:51.784
我們使用電腦雖有一段時間了，

00:02:51.808 --> 00:02:53.325
但這是不同的。

00:02:53.349 --> 00:02:55.416
這是歷史性的轉折，

00:02:55.440 --> 00:03:00.777
因我們不能主導計算機
如何去做這樣的主觀決定，

00:03:00.801 --> 00:03:06.221
無法像主導計算機去開飛機、造橋樑

00:03:06.221 --> 00:03:07.480
或登陸月球那樣。

00:03:08.269 --> 00:03:11.528
飛機會更安全嗎？
橋樑會搖擺或倒塌嗎？

00:03:11.552 --> 00:03:16.050
那兒已有相當明確的基準共識，

00:03:16.074 --> 00:03:18.313
有自然的法則指引著我們。

00:03:18.337 --> 00:03:20.055
但我們沒有

00:03:20.055 --> 00:03:25.718
判斷凌亂人事的錨點或基準。

00:03:25.852 --> 00:03:29.979
使事情變得更為複雜的是，
因軟體越來越強大，

00:03:30.003 --> 00:03:33.776
但也越來越不透明，越複雜難懂。

00:03:34.362 --> 00:03:36.402
過去十年

00:03:36.426 --> 00:03:39.155
複雜的演算法有長足的進步：

00:03:39.179 --> 00:03:41.169
能辨識人臉，

00:03:41.805 --> 00:03:43.860
能解讀手寫的字，

00:03:44.256 --> 00:03:46.322
能檢測信用卡欺詐，

00:03:46.346 --> 00:03:47.535
阻擋垃圾郵件，

00:03:47.559 --> 00:03:49.596
能翻譯不同的語言，

00:03:49.620 --> 00:03:52.194
能判讀醫學影像查出腫瘤，

00:03:52.218 --> 00:03:54.553
能在西洋棋和圍棋賽中
擊敗人類棋手。

00:03:55.084 --> 00:03:59.588
這些進步主要來自所謂的
「機器學習」法。

00:03:59.995 --> 00:04:03.182
機器學習不同於傳統的程式編寫。

00:04:03.206 --> 00:04:06.791
編寫程式是下詳細、精確、
齊全的計算機指令；

00:04:07.198 --> 00:04:11.380
機器學習更像是
餵大量的數據給系統，

00:04:11.404 --> 00:04:13.060
包括非結構化的數據，

00:04:13.084 --> 00:04:15.362
像我們數位生活產生的數據；

00:04:15.386 --> 00:04:18.116
系統翻撈這些數據來學習。

00:04:18.489 --> 00:04:20.015
至關重要的是，

00:04:20.039 --> 00:04:24.419
這些系統不在產生
單一答案的邏輯系統下運作；

00:04:24.443 --> 00:04:26.153
它們不會給出一個簡單的答案，

00:04:26.153 --> 00:04:28.056
而是以更接近機率的形式呈現：

00:04:28.056 --> 00:04:30.909
「這可能更接近你所要找的。」

00:04:31.843 --> 00:04:34.913
好處是：這方法強而有力。

00:04:34.937 --> 00:04:37.013
谷歌的人工智慧系統負責人稱之為：

00:04:37.037 --> 00:04:39.234
「不合理的數據有效性。」

00:04:39.611 --> 00:04:40.964
缺點是，

00:04:41.558 --> 00:04:44.629
我們未能真正明白
系統學到了什麼。

00:04:44.653 --> 00:04:46.240
事實上，這就是它的力量。

00:04:46.766 --> 00:04:50.564
這不像下指令給計算機；

00:04:51.020 --> 00:04:53.468
而更像是訓練

00:04:53.468 --> 00:04:57.479
我們未能真正了解
或無法控制的機器寵物狗。

00:04:58.182 --> 00:04:59.733
這是我們的問題。

00:05:00.247 --> 00:05:04.509
人工智慧系統出錯時會是個問題；

00:05:04.533 --> 00:05:08.073
即使它弄對了還是個問題，

00:05:08.097 --> 00:05:11.725
因碰到主觀問題時，
我們不知哪個是哪個。

00:05:11.749 --> 00:05:14.298
我們不知道系統在想什麼。

00:05:15.313 --> 00:05:18.996
就拿招募人員的演算法來說，

00:05:19.943 --> 00:05:24.254
亦即以機器學習來僱用人的系統，

00:05:24.872 --> 00:05:28.491
這樣的系統用
已有的員工數據來訓練機器，

00:05:28.491 --> 00:05:31.066
指示它尋找和僱用那些

00:05:31.090 --> 00:05:34.128
類似公司現有的高績效員工的人。

00:05:34.634 --> 00:05:35.787
聽起來不錯。

00:05:35.811 --> 00:05:37.810
我曾參加某會議，

00:05:37.834 --> 00:05:40.959
聚集人資經理和高階主管，

00:05:40.983 --> 00:05:42.189
高層人士，

00:05:42.213 --> 00:05:43.772
使用這種系統招聘。

00:05:43.796 --> 00:05:45.442
他們超級興奮，

00:05:45.466 --> 00:05:48.873
認為這種系統會使招聘更為客觀，

00:05:48.873 --> 00:05:50.143
較少偏見，

00:05:50.143 --> 00:05:53.143
有利於婦女和少數民族

00:05:53.167 --> 00:05:55.355
避開有偏見的管理人。

00:05:55.379 --> 00:05:58.222
看哪！靠人類僱用是有偏見的。

00:05:58.919 --> 00:06:00.104
我知道。

00:06:00.128 --> 00:06:03.133
我的意思是，
在早期某個編寫程式的工作，

00:06:03.157 --> 00:06:07.159
有時候我的直屬主管會在

00:06:07.159 --> 00:06:10.802
大清早或下午很晚時來到我身旁，

00:06:10.826 --> 00:06:13.888
說：「日娜，走，吃午飯！」

00:06:14.544 --> 00:06:16.711
我被奇怪的時間點所困惑。

00:06:16.735 --> 00:06:18.864
下午4點。吃午餐？

00:06:18.888 --> 00:06:20.328
我很窮，

00:06:20.328 --> 00:06:22.438
因是免費的午餐，所以總是會去。

00:06:22.438 --> 00:06:24.505
後來我明白到底是怎麼回事。

00:06:24.529 --> 00:06:29.075
我的直屬主管沒讓她的主管知道，

00:06:29.099 --> 00:06:32.079
他們僱來做重要職務的程式設計師，

00:06:32.079 --> 00:06:33.586
是個穿牛仔褲和運動鞋

00:06:33.586 --> 00:06:36.166
來上班的十幾歲女孩。

00:06:36.994 --> 00:06:39.236
我工作做得很好，
只是外表形象看起來不符，

00:06:39.236 --> 00:06:40.919
年齡和性別不對。

00:06:40.943 --> 00:06:44.289
因此，性別和種族
不列入考慮的僱用系統

00:06:44.313 --> 00:06:46.178
對我而言當然不錯。

00:06:46.851 --> 00:06:50.192
但使用這些系統會更複雜，原因是：

00:06:50.788 --> 00:06:52.963
目前的計算系統

00:06:52.963 --> 00:06:58.475
可從你零散的數位足跡
推斷出關於你的各種事物，

00:06:58.499 --> 00:07:00.832
即使你未曾披露過。

00:07:01.326 --> 00:07:04.253
他們能推斷你的性取向，

00:07:04.814 --> 00:07:06.120
個性的特質，

00:07:06.679 --> 00:07:08.052
政治的傾向。

00:07:08.650 --> 00:07:12.335
他們的預測能力相當精準。

00:07:13.182 --> 00:07:15.760
請記住：知道你未曾公開的事情

00:07:15.784 --> 00:07:17.375
是推理。

00:07:17.399 --> 00:07:20.660
我有個朋友開發這樣的計算系統：

00:07:20.684 --> 00:07:25.775
從社交媒體數據來預測
臨床或產後抑鬱症的可能性。

00:07:26.496 --> 00:07:27.923
結果非常優異。

00:07:28.312 --> 00:07:29.363
她的系統

00:07:29.363 --> 00:07:35.596
能在出現任何症狀的幾個月前
預測出抑鬱的可能性，

00:07:35.620 --> 00:07:36.993
是好幾個月前。

00:07:37.017 --> 00:07:39.263
雖沒有症狀，已預測出來。

00:07:39.287 --> 00:07:44.099
她希望它被用來早期干預處理。
很好！

00:07:44.731 --> 00:07:47.141
但是，設想若把這系統
用在僱人的情況下。

00:07:47.847 --> 00:07:50.893
在這人資經理會議中，

00:07:50.917 --> 00:07:55.560
我走向一間大公司的高階經理，

00:07:55.560 --> 00:07:56.380
對她說：

00:07:56.380 --> 00:08:00.228
「假設在你不知道的情形下，

00:08:00.252 --> 00:08:06.801
那個系統被用來排除
未來極有可能抑鬱的人呢？

00:08:07.581 --> 00:08:10.957
他們現在不抑鬱，
只是未來『比較有可能』抑鬱。

00:08:11.743 --> 00:08:15.149
如果它被用來排除
在未來一兩年比較有可能懷孕，

00:08:15.173 --> 00:08:17.829
但現在沒懷孕的婦女呢？

00:08:18.664 --> 00:08:24.300
如果它被用來招募激進性格者，
以符合你的職場文化呢？」

00:08:24.993 --> 00:08:27.684
透過性別比例無法看到這些問題，

00:08:27.708 --> 00:08:29.210
因比例可能是均衡的。

00:08:29.234 --> 00:08:32.791
而且由於這是機器學習，
不是傳統編碼，

00:08:32.815 --> 00:08:37.722
沒有標記為「更高抑鬱症風險」、

00:08:37.746 --> 00:08:39.579
「更高懷孕風險」、

00:08:39.603 --> 00:08:41.337
「侵略性格者」的變數；

00:08:41.815 --> 00:08:45.494
你不僅不知道系統在選什麼，

00:08:45.518 --> 00:08:47.841
甚至不知道要從何找起。

00:08:47.865 --> 00:08:49.111
它就是個黑盒子，

00:08:49.135 --> 00:08:51.942
具有預測能力，但你不了解它。

00:08:52.306 --> 00:08:55.385
我問：「你有什麼能確保

00:08:55.385 --> 00:08:58.632
你的黑盒子沒在暗地裡
做了什麼不可告人之事？

00:09:00.683 --> 00:09:04.561
她看著我，彷彿我剛踩了
十隻小狗的尾巴。

00:09:04.585 --> 00:09:05.833
（笑聲）

00:09:05.857 --> 00:09:07.898
她盯著我，說：

00:09:08.376 --> 00:09:12.709
「關於這事，我不想
再聽妳多說一個字。」

00:09:13.278 --> 00:09:15.312
然後她就轉身走開了。

00:09:15.884 --> 00:09:17.370
提醒你們，她不是粗魯。

00:09:17.394 --> 00:09:18.856
她的意思很明顯：

00:09:18.856 --> 00:09:21.726
我不知道的事不是我的問題。

00:09:21.726 --> 00:09:23.726
走開。惡狠狠盯著。

00:09:23.726 --> 00:09:24.972
（笑聲）

00:09:25.682 --> 00:09:31.581
這樣的系統可能比人類經理
在某些方面更沒有偏見，

00:09:31.672 --> 00:09:33.818
可能也省錢；

00:09:34.393 --> 00:09:39.089
但也可能在不知不覺中逐步導致

00:09:39.089 --> 00:09:43.262
抑鬱症風險較高的人
在就業市場裡吃到閉門羹。

00:09:43.573 --> 00:09:48.479
我們要在不自覺的情形下
建立這種社會嗎？

00:09:48.502 --> 00:09:52.466
僅僅因我們讓給
我們不完全理解的機器做決策？

00:09:53.085 --> 00:09:56.083
另一個問題是：這些系統通常由

00:09:56.083 --> 00:10:01.476
我們行動產生的數據，
即人類的印記所訓練。

00:10:02.008 --> 00:10:05.816
它們可能只是反映我們的偏見，

00:10:05.840 --> 00:10:09.433
學習了我們的偏見

00:10:09.457 --> 00:10:10.770
並且放大，

00:10:10.794 --> 00:10:12.212
然後回饋給我們；

00:10:12.236 --> 00:10:13.698
而我們卻告訴自己：

00:10:13.722 --> 00:10:16.839
「這樣做是客觀、不偏頗的計算。」

00:10:18.134 --> 00:10:20.811
研究人員在谷歌上發現，

00:10:21.954 --> 00:10:27.267
女性看到高薪工作招聘的廣告
少於男性。

00:10:28.283 --> 00:10:30.813
蒐索非裔美國人的名字

00:10:30.837 --> 00:10:35.543
比較可能帶出暗示犯罪史的廣告，

00:10:35.567 --> 00:10:37.384
即使那人並無犯罪史。

00:10:38.513 --> 00:10:42.062
這種隱藏偏見和黑箱的演算法，

00:10:42.086 --> 00:10:46.059
有時被研究人員發現了，
但有時我們毫無所知，

00:10:46.083 --> 00:10:48.944
很可能產生改變生命的後果。

00:10:49.778 --> 00:10:55.357
在威斯康辛州，某個被告
因逃避警察而被判處六年監禁。

00:10:56.644 --> 00:10:57.830
你可能不知道

00:10:57.854 --> 00:11:00.116
演算法越來越頻繁地被用在

00:11:00.116 --> 00:11:01.876
假釋和量刑的決定上。

00:11:01.876 --> 00:11:04.831
想知道分數如何計算出來的嗎？

00:11:05.615 --> 00:11:07.280
這是個商業的黑盒子，

00:11:07.304 --> 00:11:08.586
開發它的公司

00:11:08.586 --> 00:11:12.216
拒絕讓演算法在公開法庭上受盤問。

00:11:12.216 --> 00:11:17.748
但是 ProPublica 這家
非營利機構評估該演算法，

00:11:17.772 --> 00:11:19.788
使用找得到的公共數據，

00:11:19.812 --> 00:11:22.128
發現其結果偏頗，

00:11:22.152 --> 00:11:25.781
預測能力相當差，
僅比碰運氣稍強，

00:11:25.805 --> 00:11:31.695
並錯誤地標記
黑人被告成為未來罪犯的機率，

00:11:31.695 --> 00:11:34.450
是白人被告的兩倍。

00:11:35.711 --> 00:11:37.275
考慮這個情況：

00:11:37.923 --> 00:11:44.165
這女人因來不及去佛州布勞沃德郡的
學校接她的乾妹妹，

00:11:44.165 --> 00:11:46.774
而與朋友狂奔趕赴學校。

00:11:46.774 --> 00:11:51.056
他們看到門廊上有一輛未上鎖的
兒童腳踏車和一台滑板車，

00:11:51.080 --> 00:11:52.712
愚蠢地跳上去，

00:11:52.736 --> 00:11:54.289
當他們趕時間快速離去時，

00:11:54.289 --> 00:11:57.564
一個女人出來說：
「嘿！那是我孩子的腳踏車！」

00:11:57.588 --> 00:12:00.882
雖然他們留下車子走開，
但被逮捕了。

00:12:00.906 --> 00:12:04.543
她錯了，她很蠢，
但她只有十八歲。

00:12:04.567 --> 00:12:07.111
曾觸犯兩次少年輕罪。

00:12:07.628 --> 00:12:08.687
同時，

00:12:08.687 --> 00:12:14.536
那個男人因在家得寶商店
偷竊八十五美元的東西而被捕，

00:12:14.536 --> 00:12:16.586
類似的小罪，

00:12:16.586 --> 00:12:21.365
但他曾兩次因武裝搶劫而被定罪。

00:12:21.735 --> 00:12:24.576
演算法認定她有再犯的高風險，

00:12:24.576 --> 00:12:26.136
而他卻不然。

00:12:26.566 --> 00:12:30.440
兩年後，ProPublica
發現她未曾再犯；

00:12:30.464 --> 00:12:33.014
但因有過犯罪紀錄而難以找到工作。

00:12:33.038 --> 00:12:35.114
另一方面，他再犯了，

00:12:35.138 --> 00:12:38.974
現正因再犯之罪而入監服刑八年。

00:12:39.908 --> 00:12:43.277
很顯然，我們必需審核黑盒子，

00:12:43.301 --> 00:12:46.146
並且不賦予它們
這類未經檢查的權力。

00:12:46.250 --> 00:12:48.819
（掌聲）

00:12:49.907 --> 00:12:54.149
審核極其重要，
但不足以解決所有的問題。

00:12:54.173 --> 00:12:56.921
拿臉書強大的動態消息演算法來說，

00:12:56.945 --> 00:13:00.248
就是通過你的朋友圈
和瀏覽過的頁面，

00:13:00.248 --> 00:13:04.498
排序並決定推薦
什麼給你看的演算法。

00:13:04.718 --> 00:13:06.993
應該再讓你看一張嬰兒照片嗎？

00:13:07.017 --> 00:13:08.213
（笑聲）

00:13:08.237 --> 00:13:10.833
或者一個熟人的哀傷筆記？

00:13:11.269 --> 00:13:13.125
還是一則重要但艱澀的新聞？

00:13:13.149 --> 00:13:14.631
沒有正確的答案。

00:13:14.655 --> 00:13:17.314
臉書根據在網站上的參與度來優化：

00:13:17.338 --> 00:13:19.003
喜歡，分享，評論。

00:13:19.988 --> 00:13:22.684
2014年八月，

00:13:22.708 --> 00:13:25.370
在密蘇里州弗格森市
爆發了抗議遊行，

00:13:25.394 --> 00:13:31.421
抗議一位白人警察在不明的狀況下
殺害一個非裔美國少年，

00:13:31.794 --> 00:13:33.801
抗議的消息充斥在

00:13:33.825 --> 00:13:36.510
我未經演算法篩選過的推特頁面上，

00:13:36.534 --> 00:13:38.604
但我的臉書上卻一則也沒有。

00:13:39.002 --> 00:13:40.816
是我的臉書好友不關注這事嗎？

00:13:40.816 --> 00:13:42.792
我關閉了臉書的演算法，

00:13:43.292 --> 00:13:44.424
但很麻煩惱人，

00:13:44.424 --> 00:13:48.200
因為臉書不斷地
想讓你回到演算法的控制下，

00:13:48.224 --> 00:13:50.462
臉書的朋友有在談論弗格森這事，

00:13:50.486 --> 00:13:52.865
只是臉書的演算法沒有顯示給我看。

00:13:52.865 --> 00:13:55.587
研究後，我發現這問題普遍存在。

00:13:56.085 --> 00:13:59.898
弗格森一事和演算法不合，

00:13:59.922 --> 00:14:01.093
它不討喜；

00:14:01.117 --> 00:14:02.669
誰會點擊「讚」呢？

00:14:03.320 --> 00:14:05.526
它甚至不易被評論。

00:14:05.550 --> 00:14:06.921
越是沒有讚、沒評論，

00:14:06.945 --> 00:14:10.237
演算法就顯示給越少人看，

00:14:10.241 --> 00:14:11.963
所以我們看不到這則新聞。

00:14:12.766 --> 00:14:13.798
相反地，

00:14:13.798 --> 00:14:18.486
臉書的演算法在那星期特別突顯
為漸凍人募款的冰桶挑戰這事。

00:14:18.590 --> 00:14:22.332
崇高的目標；傾倒冰水，捐贈慈善，
有意義，很好；

00:14:22.356 --> 00:14:24.440
這事與演算法超級速配，

00:14:25.039 --> 00:14:27.652
機器已為我們決定了。

00:14:27.676 --> 00:14:32.823
非常重要但艱澀的
新聞事件可能被埋沒掉，

00:14:32.823 --> 00:14:35.562
倘若臉書是唯一的新聞渠道。

00:14:35.937 --> 00:14:38.488
最後，這些系統

00:14:38.488 --> 00:14:42.494
也可能以不像人類犯錯的方式出錯。

00:14:42.518 --> 00:14:45.440
大家可還記得 IBM 的
機器智慧系統華生

00:14:45.464 --> 00:14:48.722
在 Jeopardy 的
智力問答比賽中橫掃人類的對手？

00:14:48.951 --> 00:14:50.379
它是個厲害的選手。

00:14:50.403 --> 00:14:52.489
在 Final Jeopardy 中

00:14:52.489 --> 00:14:54.479
華生被問到：

00:14:54.479 --> 00:14:57.411
「它的最大機場以二戰英雄命名，

00:14:57.435 --> 00:14:59.687
第二大機場以二戰戰場為名。」

00:14:59.711 --> 00:15:01.089
（哼 Jeopardy 的音樂）

00:15:01.402 --> 00:15:02.584
「芝加哥」，

00:15:02.608 --> 00:15:03.978
兩個人類選手的答案正確；

00:15:04.517 --> 00:15:08.865
華生則回答「多倫多」。

00:15:08.889 --> 00:15:11.057
這是個猜「美國」城市的問題啊！

00:15:11.416 --> 00:15:14.317
這個厲害的系統也犯了

00:15:14.341 --> 00:15:16.603
人類永遠不會犯，

00:15:16.603 --> 00:15:18.643
即使二年級學生也不會犯的錯誤。

00:15:18.643 --> 00:15:21.752
我們的機器智慧可能敗在

00:15:21.776 --> 00:15:24.876
與人類犯錯模式迥異之處，

00:15:24.900 --> 00:15:27.850
在我們完全想不到、
沒準備的地方出錯。

00:15:27.874 --> 00:15:31.512
得不到一份可勝任的工作
確實很糟糕，

00:15:31.536 --> 00:15:36.733
但若起因是機器的子程式漫溢，
會是三倍的糟糕。

00:15:36.743 --> 00:15:38.322
（笑聲）

00:15:38.346 --> 00:15:41.132
2010年五月，

00:15:41.156 --> 00:15:48.030
華爾街「賣出」演算法的
回饋迴路觸發了股市的急速崩盤，

00:15:48.276 --> 00:15:52.460
數萬億美元的市值
在36分鐘內蒸發掉了。

00:15:53.542 --> 00:15:54.913
我甚至不敢想

00:15:54.913 --> 00:15:59.692
若「錯誤」發生在致命的
自動武器上會是何種情況。

00:16:01.714 --> 00:16:05.504
是啊，人類總是有偏見。

00:16:05.528 --> 00:16:07.704
決策者和守門人

00:16:07.728 --> 00:16:12.171
在法庭、新聞中、戰爭裡 … 
都會犯錯；

00:16:12.171 --> 00:16:14.283
但這正是我的觀點：

00:16:14.307 --> 00:16:17.828
我們不能逃避這些困難的問題。

00:16:18.416 --> 00:16:21.932
我們不能把責任外包給機器。

00:16:22.496 --> 00:16:26.704
（掌聲）

00:16:28.909 --> 00:16:33.356
人工智慧不會給我們
「倫理免責卡」。

00:16:34.562 --> 00:16:37.943
數據科學家費德·本森
稱之為「數學粉飾」。

00:16:37.967 --> 00:16:39.356
我們需要相反的東西。

00:16:39.380 --> 00:16:44.768
我們需要培養懷疑、審視
和調查演算法的能力。

00:16:45.200 --> 00:16:48.398
我們需確保演算法有人負責，

00:16:48.422 --> 00:16:50.867
能被審查，並且確實公開透明。

00:16:51.200 --> 00:16:52.538
我們必須體認，

00:16:52.538 --> 00:16:57.428
把數學和演算法帶入凌亂、
具價值觀的人類事務

00:16:57.452 --> 00:16:59.836
不能帶來客觀性；

00:16:59.860 --> 00:17:03.493
相反地，人類事務的複雜性
侵入演算法。

00:17:03.968 --> 00:17:07.455
是啊，我們可以、也應該用演算法

00:17:07.479 --> 00:17:09.493
來幫助我們做出更好的決定。

00:17:09.517 --> 00:17:14.849
但我們也需要在判斷中
加入道德義務，

00:17:14.873 --> 00:17:17.691
並在該框架內使用演算法，

00:17:17.715 --> 00:17:22.650
而不是像人與人間相互推卸那樣，

00:17:22.674 --> 00:17:25.128
就把責任轉移給機器。

00:17:25.627 --> 00:17:28.236
機器智慧已經到來，

00:17:28.260 --> 00:17:31.645
這意味著我們必須更堅守

00:17:31.645 --> 00:17:33.852
人類價值觀和人類倫理。

00:17:33.876 --> 00:17:35.030
謝謝。

00:17:35.054 --> 00:17:38.694
(掌聲）

