WEBVTT
Kind: captions
Language: en

00:00:12.739 --> 00:00:16.861
So, I started my first job
as a computer programmer

00:00:16.885 --> 00:00:18.841
in my very first year of college --

00:00:18.865 --> 00:00:20.372
basically, as a teenager.

00:00:20.889 --> 00:00:22.621
Soon after I started working,

00:00:22.645 --> 00:00:24.255
writing software in a company,

00:00:24.799 --> 00:00:28.434
a manager who worked at the company
came down to where I was,

00:00:28.458 --> 00:00:29.726
and he whispered to me,

00:00:30.229 --> 00:00:33.090
"Can he tell if I'm lying?"

00:00:33.806 --> 00:00:35.883
There was nobody else in the room.

00:00:37.032 --> 00:00:41.421
"Can who tell if you're lying?
And why are we whispering?"

00:00:42.266 --> 00:00:45.373
The manager pointed
at the computer in the room.

00:00:45.397 --> 00:00:48.493
"Can he tell if I'm lying?"

00:00:49.613 --> 00:00:53.975
Well, that manager was having
an affair with the receptionist.

00:00:53.999 --> 00:00:55.111
(Laughter)

00:00:55.135 --> 00:00:56.901
And I was still a teenager.

00:00:57.447 --> 00:00:59.466
So I whisper-shouted back to him,

00:00:59.490 --> 00:01:03.114
"Yes, the computer can tell
if you're lying."

00:01:03.138 --> 00:01:04.944
(Laughter)

00:01:04.968 --> 00:01:07.891
Well, I laughed, but actually,
the laugh's on me.

00:01:07.915 --> 00:01:11.183
Nowadays, there are computational systems

00:01:11.207 --> 00:01:14.755
that can suss out
emotional states and even lying

00:01:14.779 --> 00:01:16.823
from processing human faces.

00:01:17.248 --> 00:01:21.401
Advertisers and even governments
are very interested.

00:01:22.319 --> 00:01:24.181
I had become a computer programmer

00:01:24.205 --> 00:01:27.318
because I was one of those kids
crazy about math and science.

00:01:27.942 --> 00:01:31.050
But somewhere along the line
I'd learned about nuclear weapons,

00:01:31.074 --> 00:01:34.026
and I'd gotten really concerned
with the ethics of science.

00:01:34.050 --> 00:01:35.254
I was troubled.

00:01:35.278 --> 00:01:37.919
However, because of family circumstances,

00:01:37.943 --> 00:01:41.241
I also needed to start working
as soon as possible.

00:01:41.265 --> 00:01:44.564
So I thought to myself, hey,
let me pick a technical field

00:01:44.588 --> 00:01:46.384
where I can get a job easily

00:01:46.408 --> 00:01:50.426
and where I don't have to deal
with any troublesome questions of ethics.

00:01:51.022 --> 00:01:52.551
So I picked computers.

00:01:52.575 --> 00:01:53.679
(Laughter)

00:01:53.703 --> 00:01:57.113
Well, ha, ha, ha!
All the laughs are on me.

00:01:57.137 --> 00:01:59.891
Nowadays, computer scientists
are building platforms

00:01:59.915 --> 00:02:04.124
that control what a billion
people see every day.

00:02:05.052 --> 00:02:08.874
They're developing cars
that could decide who to run over.

00:02:09.707 --> 00:02:12.920
They're even building machines, weapons,

00:02:12.944 --> 00:02:15.229
that might kill human beings in war.

00:02:15.253 --> 00:02:18.024
It's ethics all the way down.

00:02:19.183 --> 00:02:21.241
Machine intelligence is here.

00:02:21.823 --> 00:02:25.297
We're now using computation
to make all sort of decisions,

00:02:25.321 --> 00:02:27.207
but also new kinds of decisions.

00:02:27.231 --> 00:02:32.403
We're asking questions to computation
that have no single right answers,

00:02:32.427 --> 00:02:33.629
that are subjective

00:02:33.653 --> 00:02:35.978
and open-ended and value-laden.

00:02:36.002 --> 00:02:37.760
We're asking questions like,

00:02:37.784 --> 00:02:39.434
"Who should the company hire?"

00:02:40.096 --> 00:02:42.855
"Which update from which friend
should you be shown?"

00:02:42.879 --> 00:02:45.145
"Which convict is more
likely to reoffend?"

00:02:45.514 --> 00:02:48.568
"Which news item or movie
should be recommended to people?"

00:02:48.592 --> 00:02:51.964
Look, yes, we've been using
computers for a while,

00:02:51.988 --> 00:02:53.505
but this is different.

00:02:53.529 --> 00:02:55.596
This is a historical twist,

00:02:55.620 --> 00:03:00.957
because we cannot anchor computation
for such subjective decisions

00:03:00.981 --> 00:03:06.401
the way we can anchor computation
for flying airplanes, building bridges,

00:03:06.425 --> 00:03:07.684
going to the moon.

00:03:08.449 --> 00:03:11.708
Are airplanes safer?
Did the bridge sway and fall?

00:03:11.732 --> 00:03:16.230
There, we have agreed-upon,
fairly clear benchmarks,

00:03:16.254 --> 00:03:18.493
and we have laws of nature to guide us.

00:03:18.517 --> 00:03:21.911
We have no such anchors and benchmarks

00:03:21.935 --> 00:03:25.898
for decisions in messy human affairs.

00:03:25.922 --> 00:03:30.159
To make things more complicated,
our software is getting more powerful,

00:03:30.183 --> 00:03:33.956
but it's also getting less
transparent and more complex.

00:03:34.542 --> 00:03:36.582
Recently, in the past decade,

00:03:36.606 --> 00:03:39.335
complex algorithms
have made great strides.

00:03:39.359 --> 00:03:41.349
They can recognize human faces.

00:03:41.985 --> 00:03:44.040
They can decipher handwriting.

00:03:44.436 --> 00:03:46.502
They can detect credit card fraud

00:03:46.526 --> 00:03:47.715
and block spam

00:03:47.739 --> 00:03:49.776
and they can translate between languages.

00:03:49.800 --> 00:03:52.374
They can detect tumors in medical imaging.

00:03:52.398 --> 00:03:54.603
They can beat humans in chess and Go.

00:03:55.264 --> 00:03:59.768
Much of this progress comes
from a method called "machine learning."

00:04:00.175 --> 00:04:03.362
Machine learning is different
than traditional programming,

00:04:03.386 --> 00:04:06.971
where you give the computer
detailed, exact, painstaking instructions.

00:04:07.378 --> 00:04:11.560
It's more like you take the system
and you feed it lots of data,

00:04:11.584 --> 00:04:13.240
including unstructured data,

00:04:13.264 --> 00:04:15.542
like the kind we generate
in our digital lives.

00:04:15.566 --> 00:04:18.296
And the system learns
by churning through this data.

00:04:18.669 --> 00:04:20.195
And also, crucially,

00:04:20.219 --> 00:04:24.599
these systems don't operate
under a single-answer logic.

00:04:24.623 --> 00:04:27.582
They don't produce a simple answer;
it's more probabilistic:

00:04:27.606 --> 00:04:31.089
"This one is probably more like
what you're looking for."

00:04:32.023 --> 00:04:35.093
Now, the upside is:
this method is really powerful.

00:04:35.117 --> 00:04:37.193
The head of Google's AI systems called it,

00:04:37.217 --> 00:04:39.414
"the unreasonable effectiveness of data."

00:04:39.791 --> 00:04:41.144
The downside is,

00:04:41.738 --> 00:04:44.809
we don't really understand
what the system learned.

00:04:44.833 --> 00:04:46.420
In fact, that's its power.

00:04:46.946 --> 00:04:50.744
This is less like giving
instructions to a computer;

00:04:51.200 --> 00:04:55.264
it's more like training
a puppy-machine-creature

00:04:55.288 --> 00:04:57.659
we don't really understand or control.

00:04:58.362 --> 00:04:59.913
So this is our problem.

00:05:00.427 --> 00:05:04.689
It's a problem when this artificial
intelligence system gets things wrong.

00:05:04.713 --> 00:05:08.253
It's also a problem
when it gets things right,

00:05:08.277 --> 00:05:11.905
because we don't even know which is which
when it's a subjective problem.

00:05:11.929 --> 00:05:14.268
We don't know what this thing is thinking.

00:05:15.493 --> 00:05:19.176
So, consider a hiring algorithm --

00:05:20.123 --> 00:05:24.434
a system used to hire people,
using machine-learning systems.

00:05:25.052 --> 00:05:28.631
Such a system would have been trained
on previous employees' data

00:05:28.655 --> 00:05:31.246
and instructed to find and hire

00:05:31.270 --> 00:05:34.308
people like the existing
high performers in the company.

00:05:34.814 --> 00:05:35.967
Sounds good.

00:05:35.991 --> 00:05:37.990
I once attended a conference

00:05:38.014 --> 00:05:41.139
that brought together
human resources managers and executives,

00:05:41.163 --> 00:05:42.369
high-level people,

00:05:42.393 --> 00:05:43.952
using such systems in hiring.

00:05:43.976 --> 00:05:45.622
They were super excited.

00:05:45.646 --> 00:05:50.299
They thought that this would make hiring
more objective, less biased,

00:05:50.323 --> 00:05:53.323
and give women
and minorities a better shot

00:05:53.347 --> 00:05:55.535
against biased human managers.

00:05:55.559 --> 00:05:58.402
And look -- human hiring is biased.

00:05:59.099 --> 00:06:00.284
I know.

00:06:00.308 --> 00:06:03.313
I mean, in one of my early jobs
as a programmer,

00:06:03.337 --> 00:06:07.205
my immediate manager would sometimes
come down to where I was

00:06:07.229 --> 00:06:10.982
really early in the morning
or really late in the afternoon,

00:06:11.006 --> 00:06:14.068
and she'd say, "Zeynep,
let's go to lunch!"

00:06:14.724 --> 00:06:16.891
I'd be puzzled by the weird timing.

00:06:16.915 --> 00:06:19.044
It's 4pm. Lunch?

00:06:19.068 --> 00:06:22.162
I was broke, so free lunch. I always went.

00:06:22.618 --> 00:06:24.685
I later realized what was happening.

00:06:24.709 --> 00:06:29.255
My immediate managers
had not confessed to their higher-ups

00:06:29.279 --> 00:06:32.392
that the programmer they hired
for a serious job was a teen girl

00:06:32.416 --> 00:06:36.346
who wore jeans and sneakers to work.

00:06:37.174 --> 00:06:39.376
I was doing a good job,
I just looked wrong

00:06:39.400 --> 00:06:41.099
and was the wrong age and gender.

00:06:41.123 --> 00:06:44.469
So hiring in a gender- and race-blind way

00:06:44.493 --> 00:06:46.358
certainly sounds good to me.

00:06:47.031 --> 00:06:50.372
But with these systems,
it is more complicated, and here's why:

00:06:50.968 --> 00:06:56.759
Currently, computational systems
can infer all sorts of things about you

00:06:56.783 --> 00:06:58.655
from your digital crumbs,

00:06:58.679 --> 00:07:01.012
even if you have not
disclosed those things.

00:07:01.506 --> 00:07:04.433
They can infer your sexual orientation,

00:07:04.994 --> 00:07:06.300
your personality traits,

00:07:06.859 --> 00:07:08.232
your political leanings.

00:07:08.830 --> 00:07:12.515
They have predictive power
with high levels of accuracy.

00:07:13.362 --> 00:07:15.940
Remember -- for things
you haven't even disclosed.

00:07:15.964 --> 00:07:17.555
This is inference.

00:07:17.579 --> 00:07:20.840
I have a friend who developed
such computational systems

00:07:20.864 --> 00:07:24.505
to predict the likelihood
of clinical or postpartum depression

00:07:24.529 --> 00:07:25.945
from social media data.

00:07:26.676 --> 00:07:28.103
The results are impressive.

00:07:28.492 --> 00:07:31.849
Her system can predict
the likelihood of depression

00:07:31.873 --> 00:07:35.776
months before the onset of any symptoms --

00:07:35.800 --> 00:07:37.173
months before.

00:07:37.197 --> 00:07:39.443
No symptoms, there's prediction.

00:07:39.467 --> 00:07:44.279
She hopes it will be used
for early intervention. Great!

00:07:44.911 --> 00:07:46.951
But now put this in the context of hiring.

00:07:48.027 --> 00:07:51.073
So at this human resources
managers conference,

00:07:51.097 --> 00:07:55.806
I approached a high-level manager
in a very large company,

00:07:55.830 --> 00:08:00.408
and I said to her, "Look,
what if, unbeknownst to you,

00:08:00.432 --> 00:08:06.981
your system is weeding out people
with high future likelihood of depression?

00:08:07.761 --> 00:08:11.137
They're not depressed now,
just maybe in the future, more likely.

00:08:11.923 --> 00:08:15.329
What if it's weeding out women
more likely to be pregnant

00:08:15.353 --> 00:08:17.939
in the next year or two
but aren't pregnant now?

00:08:18.844 --> 00:08:24.480
What if it's hiring aggressive people
because that's your workplace culture?"

00:08:25.173 --> 00:08:27.864
You can't tell this by looking
at gender breakdowns.

00:08:27.888 --> 00:08:29.390
Those may be balanced.

00:08:29.414 --> 00:08:32.971
And since this is machine learning,
not traditional coding,

00:08:32.995 --> 00:08:37.902
there is no variable there
labeled "higher risk of depression,"

00:08:37.926 --> 00:08:39.759
"higher risk of pregnancy,"

00:08:39.783 --> 00:08:41.517
"aggressive guy scale."

00:08:41.995 --> 00:08:45.674
Not only do you not know
what your system is selecting on,

00:08:45.698 --> 00:08:48.021
you don't even know
where to begin to look.

00:08:48.045 --> 00:08:49.291
It's a black box.

00:08:49.315 --> 00:08:52.122
It has predictive power,
but you don't understand it.

00:08:52.486 --> 00:08:54.855
"What safeguards," I asked, "do you have

00:08:54.879 --> 00:08:58.552
to make sure that your black box
isn't doing something shady?"

00:09:00.863 --> 00:09:04.741
She looked at me as if I had
just stepped on 10 puppy tails.

00:09:04.765 --> 00:09:06.013
(Laughter)

00:09:06.037 --> 00:09:08.078
She stared at me and she said,

00:09:08.556 --> 00:09:12.889
"I don't want to hear
another word about this."

00:09:13.458 --> 00:09:15.492
And she turned around and walked away.

00:09:16.064 --> 00:09:17.550
Mind you -- she wasn't rude.

00:09:17.574 --> 00:09:23.882
It was clearly: what I don't know
isn't my problem, go away, death stare.

00:09:23.906 --> 00:09:25.152
(Laughter)

00:09:25.862 --> 00:09:29.701
Look, such a system
may even be less biased

00:09:29.725 --> 00:09:31.828
than human managers in some ways.

00:09:31.852 --> 00:09:33.998
And it could make monetary sense.

00:09:34.573 --> 00:09:36.223
But it could also lead

00:09:36.247 --> 00:09:40.995
to a steady but stealthy
shutting out of the job market

00:09:41.019 --> 00:09:43.312
of people with higher risk of depression.

00:09:43.753 --> 00:09:46.349
Is this the kind of society
we want to build,

00:09:46.373 --> 00:09:48.658
without even knowing we've done this,

00:09:48.682 --> 00:09:52.646
because we turned decision-making
to machines we don't totally understand?

00:09:53.265 --> 00:09:54.723
Another problem is this:

00:09:55.314 --> 00:09:59.766
these systems are often trained
on data generated by our actions,

00:09:59.790 --> 00:10:01.606
human imprints.

00:10:02.188 --> 00:10:05.996
Well, they could just be
reflecting our biases,

00:10:06.020 --> 00:10:09.613
and these systems
could be picking up on our biases

00:10:09.637 --> 00:10:10.950
and amplifying them

00:10:10.974 --> 00:10:12.392
and showing them back to us,

00:10:12.416 --> 00:10:13.878
while we're telling ourselves,

00:10:13.902 --> 00:10:17.019
"We're just doing objective,
neutral computation."

00:10:18.314 --> 00:10:20.991
Researchers found that on Google,

00:10:22.134 --> 00:10:27.447
women are less likely than men
to be shown job ads for high-paying jobs.

00:10:28.463 --> 00:10:30.993
And searching for African-American names

00:10:31.017 --> 00:10:35.723
is more likely to bring up ads
suggesting criminal history,

00:10:35.747 --> 00:10:37.314
even when there is none.

00:10:38.693 --> 00:10:42.242
Such hidden biases
and black-box algorithms

00:10:42.266 --> 00:10:46.239
that researchers uncover sometimes
but sometimes we don't know,

00:10:46.263 --> 00:10:48.924
can have life-altering consequences.

00:10:49.958 --> 00:10:54.117
In Wisconsin, a defendant
was sentenced to six years in prison

00:10:54.141 --> 00:10:55.496
for evading the police.

00:10:56.824 --> 00:10:58.010
You may not know this,

00:10:58.034 --> 00:11:02.032
but algorithms are increasingly used
in parole and sentencing decisions.

00:11:02.056 --> 00:11:05.011
He wanted to know:
How is this score calculated?

00:11:05.795 --> 00:11:07.460
It's a commercial black box.

00:11:07.484 --> 00:11:11.689
The company refused to have its algorithm
be challenged in open court.

00:11:12.396 --> 00:11:17.928
But ProPublica, an investigative
nonprofit, audited that very algorithm

00:11:17.952 --> 00:11:19.968
with what public data they could find,

00:11:19.992 --> 00:11:22.308
and found that its outcomes were biased

00:11:22.332 --> 00:11:25.961
and its predictive power
was dismal, barely better than chance,

00:11:25.985 --> 00:11:30.401
and it was wrongly labeling
black defendants as future criminals

00:11:30.425 --> 00:11:34.320
at twice the rate of white defendants.

00:11:35.891 --> 00:11:37.455
So, consider this case:

00:11:38.103 --> 00:11:41.955
This woman was late
picking up her godsister

00:11:41.979 --> 00:11:44.054
from a school in Broward County, Florida,

00:11:44.757 --> 00:11:47.113
running down the street
with a friend of hers.

00:11:47.137 --> 00:11:51.236
They spotted an unlocked kid's bike
and a scooter on a porch

00:11:51.260 --> 00:11:52.892
and foolishly jumped on it.

00:11:52.916 --> 00:11:55.515
As they were speeding off,
a woman came out and said,

00:11:55.539 --> 00:11:57.744
"Hey! That's my kid's bike!"

00:11:57.768 --> 00:12:01.062
They dropped it, they walked away,
but they were arrested.

00:12:01.086 --> 00:12:04.723
She was wrong, she was foolish,
but she was also just 18.

00:12:04.747 --> 00:12:07.291
She had a couple of juvenile misdemeanors.

00:12:07.808 --> 00:12:12.993
Meanwhile, that man had been arrested
for shoplifting in Home Depot --

00:12:13.017 --> 00:12:15.941
85 dollars' worth of stuff,
a similar petty crime.

00:12:16.766 --> 00:12:21.325
But he had two prior
armed robbery convictions.

00:12:21.955 --> 00:12:25.437
But the algorithm scored her
as high risk, and not him.

00:12:26.746 --> 00:12:30.620
Two years later, ProPublica found
that she had not reoffended.

00:12:30.644 --> 00:12:33.194
It was just hard to get a job
for her with her record.

00:12:33.218 --> 00:12:35.294
He, on the other hand, did reoffend

00:12:35.318 --> 00:12:39.154
and is now serving an eight-year
prison term for a later crime.

00:12:40.088 --> 00:12:43.457
Clearly, we need to audit our black boxes

00:12:43.481 --> 00:12:46.096
and not have them have
this kind of unchecked power.

00:12:46.120 --> 00:12:48.999
(Applause)

00:12:50.087 --> 00:12:54.329
Audits are great and important,
but they don't solve all our problems.

00:12:54.353 --> 00:12:57.101
Take Facebook's powerful
news feed algorithm --

00:12:57.125 --> 00:13:01.968
you know, the one that ranks everything
and decides what to show you

00:13:01.992 --> 00:13:04.276
from all the friends and pages you follow.

00:13:04.898 --> 00:13:07.173
Should you be shown another baby picture?

00:13:07.197 --> 00:13:08.393
(Laughter)

00:13:08.417 --> 00:13:11.013
A sullen note from an acquaintance?

00:13:11.449 --> 00:13:13.305
An important but difficult news item?

00:13:13.329 --> 00:13:14.811
There's no right answer.

00:13:14.835 --> 00:13:17.494
Facebook optimizes
for engagement on the site:

00:13:17.518 --> 00:13:18.933
likes, shares, comments.

00:13:20.168 --> 00:13:22.864
In August of 2014,

00:13:22.888 --> 00:13:25.550
protests broke out in Ferguson, Missouri,

00:13:25.574 --> 00:13:29.991
after the killing of an African-American
teenager by a white police officer,

00:13:30.015 --> 00:13:31.585
under murky circumstances.

00:13:31.974 --> 00:13:33.981
The news of the protests was all over

00:13:34.005 --> 00:13:36.690
my algorithmically
unfiltered Twitter feed,

00:13:36.714 --> 00:13:38.664
but nowhere on my Facebook.

00:13:39.182 --> 00:13:40.916
Was it my Facebook friends?

00:13:40.940 --> 00:13:42.972
I disabled Facebook's algorithm,

00:13:43.472 --> 00:13:46.320
which is hard because Facebook
keeps wanting to make you

00:13:46.344 --> 00:13:48.380
come under the algorithm's control,

00:13:48.404 --> 00:13:50.642
and saw that my friends
were talking about it.

00:13:50.666 --> 00:13:53.175
It's just that the algorithm
wasn't showing it to me.

00:13:53.199 --> 00:13:56.241
I researched this and found
this was a widespread problem.

00:13:56.265 --> 00:14:00.078
The story of Ferguson
wasn't algorithm-friendly.

00:14:00.102 --> 00:14:01.273
It's not "likable."

00:14:01.297 --> 00:14:02.849
Who's going to click on "like?"

00:14:03.500 --> 00:14:05.706
It's not even easy to comment on.

00:14:05.730 --> 00:14:07.101
Without likes and comments,

00:14:07.125 --> 00:14:10.417
the algorithm was likely showing it
to even fewer people,

00:14:10.441 --> 00:14:11.983
so we didn't get to see this.

00:14:12.946 --> 00:14:14.174
Instead, that week,

00:14:14.198 --> 00:14:16.496
Facebook's algorithm highlighted this,

00:14:16.520 --> 00:14:18.746
which is the ALS Ice Bucket Challenge.

00:14:18.770 --> 00:14:22.512
Worthy cause; dump ice water,
donate to charity, fine.

00:14:22.536 --> 00:14:24.440
But it was super algorithm-friendly.

00:14:25.219 --> 00:14:27.832
The machine made this decision for us.

00:14:27.856 --> 00:14:31.353
A very important
but difficult conversation

00:14:31.377 --> 00:14:32.932
might have been smothered,

00:14:32.956 --> 00:14:35.652
had Facebook been the only channel.

00:14:36.117 --> 00:14:39.914
Now, finally, these systems
can also be wrong

00:14:39.938 --> 00:14:42.674
in ways that don't resemble human systems.

00:14:42.698 --> 00:14:45.620
Do you guys remember Watson,
IBM's machine-intelligence system

00:14:45.644 --> 00:14:48.772
that wiped the floor
with human contestants on Jeopardy?

00:14:49.131 --> 00:14:50.559
It was a great player.

00:14:50.583 --> 00:14:54.152
But then, for Final Jeopardy,
Watson was asked this question:

00:14:54.659 --> 00:14:57.591
"Its largest airport is named
for a World War II hero,

00:14:57.615 --> 00:14:59.867
its second-largest
for a World War II battle."

00:14:59.891 --> 00:15:01.269
(Hums Final Jeopardy music)

00:15:01.582 --> 00:15:02.764
Chicago.

00:15:02.788 --> 00:15:04.158
The two humans got it right.

00:15:04.697 --> 00:15:09.045
Watson, on the other hand,
answered "Toronto" --

00:15:09.069 --> 00:15:10.887
for a US city category!

00:15:11.596 --> 00:15:14.497
The impressive system also made an error

00:15:14.521 --> 00:15:18.172
that a human would never make,
a second-grader wouldn't make.

00:15:18.823 --> 00:15:21.932
Our machine intelligence can fail

00:15:21.956 --> 00:15:25.056
in ways that don't fit
error patterns of humans,

00:15:25.080 --> 00:15:28.030
in ways we won't expect
and be prepared for.

00:15:28.054 --> 00:15:31.692
It'd be lousy not to get a job
one is qualified for,

00:15:31.716 --> 00:15:35.443
but it would triple suck
if it was because of stack overflow

00:15:35.467 --> 00:15:36.899
in some subroutine.

00:15:36.923 --> 00:15:38.502
(Laughter)

00:15:38.526 --> 00:15:41.312
In May of 2010,

00:15:41.336 --> 00:15:45.380
a flash crash on Wall Street
fueled by a feedback loop

00:15:45.404 --> 00:15:48.432
in Wall Street's "sell" algorithm

00:15:48.456 --> 00:15:52.640
wiped a trillion dollars
of value in 36 minutes.

00:15:53.722 --> 00:15:55.909
I don't even want to think
what "error" means

00:15:55.933 --> 00:15:59.522
in the context of lethal
autonomous weapons.

00:16:01.894 --> 00:16:05.684
So yes, humans have always made biases.

00:16:05.708 --> 00:16:07.884
Decision makers and gatekeepers,

00:16:07.908 --> 00:16:11.401
in courts, in news, in war ...

00:16:11.425 --> 00:16:14.463
they make mistakes;
but that's exactly my point.

00:16:14.487 --> 00:16:18.008
We cannot escape
these difficult questions.

00:16:18.596 --> 00:16:22.112
We cannot outsource
our responsibilities to machines.

00:16:22.676 --> 00:16:26.884
(Applause)

00:16:29.089 --> 00:16:33.536
Artificial intelligence does not give us
a "Get out of ethics free" card.

00:16:34.742 --> 00:16:38.123
Data scientist Fred Benenson
calls this math-washing.

00:16:38.147 --> 00:16:39.536
We need the opposite.

00:16:39.560 --> 00:16:44.948
We need to cultivate algorithm suspicion,
scrutiny and investigation.

00:16:45.380 --> 00:16:48.578
We need to make sure we have
algorithmic accountability,

00:16:48.602 --> 00:16:51.047
auditing and meaningful transparency.

00:16:51.380 --> 00:16:54.614
We need to accept
that bringing math and computation

00:16:54.638 --> 00:16:57.608
to messy, value-laden human affairs

00:16:57.632 --> 00:17:00.016
does not bring objectivity;

00:17:00.040 --> 00:17:03.673
rather, the complexity of human affairs
invades the algorithms.

00:17:04.148 --> 00:17:07.635
Yes, we can and we should use computation

00:17:07.659 --> 00:17:09.673
to help us make better decisions.

00:17:09.697 --> 00:17:15.029
But we have to own up
to our moral responsibility to judgment,

00:17:15.053 --> 00:17:17.871
and use algorithms within that framework,

00:17:17.895 --> 00:17:22.830
not as a means to abdicate
and outsource our responsibilities

00:17:22.854 --> 00:17:25.308
to one another as human to human.

00:17:25.807 --> 00:17:28.416
Machine intelligence is here.

00:17:28.440 --> 00:17:31.861
That means we must hold on ever tighter

00:17:31.885 --> 00:17:34.032
to human values and human ethics.

00:17:34.056 --> 00:17:35.210
Thank you.

00:17:35.234 --> 00:17:40.254
(Applause)

