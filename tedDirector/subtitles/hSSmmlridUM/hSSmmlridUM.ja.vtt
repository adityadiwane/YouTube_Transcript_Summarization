WEBVTT
Kind: captions
Language: ja

00:00:00.000 --> 00:00:07.000
翻訳: Naoko Fujii
校正: Misaki Sato

00:00:12.559 --> 00:00:16.681
私はコンピュータ・プログラマー
としての最初の仕事を

00:00:16.705 --> 00:00:18.661
大学１年生で始めました

00:00:18.685 --> 00:00:20.192
まあ 10代だったんですね

00:00:20.709 --> 00:00:22.441
ある会社で ソフトウェアを

00:00:22.465 --> 00:00:24.075
書くという仕事を 始めてまもなく

00:00:24.619 --> 00:00:28.254
会社のマネージャーが
私のところに来て

00:00:28.278 --> 00:00:29.546
こうささやきました

00:00:30.049 --> 00:00:32.910
「僕の嘘 彼にばれてる？」

00:00:33.626 --> 00:00:35.703
部屋には他に誰もいません

00:00:36.852 --> 00:00:41.241
「誰にばれてるって言うんです？
それにどうしてひそひそ声で？」

00:00:42.086 --> 00:00:45.193
マネージャーは
室内のコンピュータを指さして

00:00:45.217 --> 00:00:48.313
「僕の嘘 彼にばれてる？」

00:00:49.433 --> 00:00:53.795
実はこのマネージャー 
受付係と浮気してたんです

00:00:53.819 --> 00:00:54.931
（笑）

00:00:54.955 --> 00:00:56.721
私はまだ10代でした

00:00:57.267 --> 00:00:59.286
だからささやき声で
彼に叫び返しました

00:00:59.310 --> 00:01:02.934
「ええ コンピュータには
お見通しですよ」って

00:01:02.958 --> 00:01:04.764
（笑）

00:01:04.788 --> 00:01:07.711
笑っちゃいましたが 実は
その笑いは自分に返ってきました

00:01:07.735 --> 00:01:11.003
今日 コンピュータ・システムは

00:01:11.027 --> 00:01:14.575
人間の顔画像を処理することによって

00:01:14.599 --> 00:01:16.643
感情や 嘘まで見抜けるんです

00:01:17.068 --> 00:01:21.221
広告主や 政府までもが
非常に関心を寄せています

00:01:22.139 --> 00:01:24.001
コンピュータプログラマーに
私がなったのは

00:01:24.025 --> 00:01:27.138
子どもの頃から数学と科学が
熱狂的に好きだったからです

00:01:27.762 --> 00:01:30.870
しかしその過程で
核兵器についても学び

00:01:30.894 --> 00:01:33.846
科学の倫理について
非常に懸念するようになりました

00:01:33.870 --> 00:01:35.074
悩みました

00:01:35.098 --> 00:01:37.739
しかし 家庭の事情で

00:01:37.763 --> 00:01:41.061
私はできるだけ早く
働き始めなければなりませんでした

00:01:41.085 --> 00:01:44.384
それでひそかに考えました
技術者として

00:01:44.408 --> 00:01:46.204
簡単に職が得られて

00:01:46.228 --> 00:01:50.246
倫理という厄介な問題を何も
考えなくていい分野の仕事はないかと

00:01:50.842 --> 00:01:52.371
それで選んだのがコンピュータです

00:01:52.395 --> 00:01:53.499
（笑）

00:01:53.523 --> 00:01:56.933
ハハ 笑っちゃう
自分のことを笑ってるんです

00:01:56.957 --> 00:01:59.711
近頃 コンピュータ科学者は

00:01:59.735 --> 00:02:03.944
10億人が毎日見ているものを制御する
プラットフォームを作っています

00:02:04.872 --> 00:02:08.694
誰をひき殺すか決定できる
車を開発しています

00:02:09.527 --> 00:02:12.740
戦争で人間を殺すかもしれないような

00:02:12.764 --> 00:02:15.049
機械や兵器さえも作っています

00:02:15.073 --> 00:02:17.844
全てにおいて重要になるのが倫理です

00:02:19.003 --> 00:02:21.061
機械知能は もう存在しています

00:02:21.643 --> 00:02:25.117
私たちは今や コンピュータを使って
あらゆる種類の決定を下し

00:02:25.141 --> 00:02:27.027
さらに新しい類の決定も下します

00:02:27.051 --> 00:02:32.223
私たちは 単一の正答がない問題の答えを
コンピュータに尋ねています

00:02:32.247 --> 00:02:33.449
その問題とは 主観的で

00:02:33.473 --> 00:02:35.798
オープンエンドで
価値観にかかわるものです

00:02:35.822 --> 00:02:37.580
私たちがする質問はこんなふうです

00:02:37.604 --> 00:02:39.254
「誰を社員に採用すべきか？」

00:02:39.916 --> 00:02:42.675
「どの友達からの新着情報を
表示すべきか？」

00:02:42.699 --> 00:02:44.965
「再犯する可能性の高い受刑者は誰か？」

00:02:45.334 --> 00:02:48.388
「人々に勧めるべき 
ニュースや映画はどれか？」

00:02:48.412 --> 00:02:51.784
確かに私たちは しばらくの間
コンピュータを使ってきました

00:02:51.808 --> 00:02:53.325
しかしこれは違います

00:02:53.349 --> 00:02:55.416
これは歴史的なひずみです

00:02:55.440 --> 00:03:00.777
なぜならそのような主観的な決定を
コンピュータには頼れないからです

00:03:00.801 --> 00:03:06.221
飛行機を飛ばしたり 建物を建てたり

00:03:06.245 --> 00:03:07.504
月に行く場合とは違うんです

00:03:08.269 --> 00:03:11.528
飛行機の方が安全か？
その橋は揺れたり落ちたりしたか？

00:03:11.552 --> 00:03:16.050
そこでは合意された
かなり明確な基準があり

00:03:16.074 --> 00:03:18.313
自然の法則が私たちを導いてくれます

00:03:18.337 --> 00:03:21.731
私たちがそのような
支えや基準を何も持っていないのが

00:03:21.755 --> 00:03:25.718
人間くさい事柄における
厄介な決定についてです

00:03:25.742 --> 00:03:29.979
もっと複雑なことに
ソフトウェアは強力になりつつあります

00:03:30.003 --> 00:03:33.776
その一方で 透明性を減らし
複雑さを増してもいるのです

00:03:34.362 --> 00:03:36.402
ここ10年のあいだ

00:03:36.426 --> 00:03:39.155
複雑なアルゴリズムは
大きく前進しました

00:03:39.179 --> 00:03:41.169
人間の顔を認識できます

00:03:41.805 --> 00:03:43.860
手書き文字を読み取れます

00:03:44.256 --> 00:03:46.322
クレジットカードの不正使用を探知し

00:03:46.346 --> 00:03:47.535
スパムをブロックし

00:03:47.559 --> 00:03:49.596
言語の翻訳もできます

00:03:49.620 --> 00:03:52.194
医用イメージングで
腫瘍を探しあてることもできます

00:03:52.218 --> 00:03:54.423
チェスや碁で人間を 
打ち負かすこともできます

00:03:55.084 --> 00:03:59.588
この進歩の多くは 「機械学習」と
呼ばれる方法から成り立っています

00:03:59.995 --> 00:04:03.182
機械学習は コンピュータに
詳細で正確、綿密な指示を与える―

00:04:03.206 --> 00:04:06.791
伝統的なプログラミングとは異なります

00:04:07.198 --> 00:04:11.380
機械学習は システムに 大量のデータを
しこたま詰め込むやり方です

00:04:11.404 --> 00:04:13.060
そこには非構造化データという

00:04:13.084 --> 00:04:15.362
人間がデジタルライフで
生成する類のものも含まれます

00:04:15.386 --> 00:04:18.116
そしてシステムはこのデータを
組み合わせながら学習します

00:04:18.489 --> 00:04:20.015
そしてまた重要なことに

00:04:20.039 --> 00:04:24.419
これらのシステムは
答が単一になる論理で動いてはいません

00:04:24.443 --> 00:04:27.402
単純に回答を与えるのではなく
もっと確率論的です

00:04:27.426 --> 00:04:30.909
「これはおそらくあなたが
探しているものにより近いでしょう」

00:04:31.843 --> 00:04:34.913
これの良い面は
この方法が非常に強力であることです

00:04:34.937 --> 00:04:37.013
GoogleのAIシステムのトップはこれを

00:04:37.037 --> 00:04:39.234
「データの理不尽なほどの強力さ」
と呼んでいます

00:04:39.611 --> 00:04:40.964
このシステムの悪い面は

00:04:41.558 --> 00:04:44.629
これが何を学習しているのか
私たちはそれほど理解していないことです

00:04:44.653 --> 00:04:46.240
実際 その強力さが問題なのです

00:04:46.766 --> 00:04:50.564
これはコンピュータに
指示を与えるというよりは

00:04:51.020 --> 00:04:55.084
むしろ子犬のような生き物として
訓練するようなものです

00:04:55.108 --> 00:04:57.479
その機械をそれほど
理解も制御もできていないのにです

00:04:58.182 --> 00:04:59.733
これは問題です

00:05:00.247 --> 00:05:04.509
この人工知能システムが
誤りを犯したときだけでなく

00:05:04.533 --> 00:05:08.073
正しいことをした場合にも
問題が生じます

00:05:08.097 --> 00:05:11.725
なぜなら主観的な問題の場合
私たちには正誤さえも分からないからです

00:05:11.749 --> 00:05:14.088
私たちはこの物体が
何を考えているか知りません

00:05:15.313 --> 00:05:18.996
ですから たとえば雇用アルゴリズムを
考えてみましょう

00:05:19.943 --> 00:05:24.254
社員を雇う際に使われるシステムで
機械学習システムを使っています

00:05:24.872 --> 00:05:28.451
そのようなシステムは過去の従業員の
データに基づいて訓練されています

00:05:28.475 --> 00:05:31.066
そしてそのシステムが指示するのは

00:05:31.090 --> 00:05:34.128
その会社に在籍する業績優秀者に似た
人材を探し雇うことです

00:05:34.634 --> 00:05:35.787
良さそうですね

00:05:35.811 --> 00:05:37.810
以前ある会議に
出席した折のことですが

00:05:37.834 --> 00:05:40.959
そこには人事部のマネージャーと
執行役が集まっていました

00:05:40.983 --> 00:05:42.189
高い職位の人たちで

00:05:42.213 --> 00:05:43.772
そのようなシステムを
雇用に活用しています

00:05:43.796 --> 00:05:45.442
彼らは非常にワクワクしていました

00:05:45.466 --> 00:05:50.119
彼らの考えでは このシステムは
より客観的で偏見の少ない雇用を行い

00:05:50.143 --> 00:05:53.143
マネージャーの偏見に対して
女性や少数派の人々に

00:05:53.167 --> 00:05:55.355
より良い機会を与えるものでした

00:05:55.379 --> 00:05:58.222
そうです　
雇用には偏見が混じるのです

00:05:58.919 --> 00:06:00.104
私は知っています

00:06:00.128 --> 00:06:03.133
ある職場で プログラマーとして
働きだした頃

00:06:03.157 --> 00:06:07.025
直属のマネージャーが
時々私のところに来ました

00:06:07.049 --> 00:06:10.802
それも早朝とか夕方にです

00:06:10.826 --> 00:06:13.888
そして彼女はこう言うんです
「ゼイナップ ランチ行きましょ」

00:06:14.544 --> 00:06:16.711
おかしなタイミングで
全く訳が分かりませんでした

00:06:16.735 --> 00:06:18.864
午後４時にランチ？

00:06:18.888 --> 00:06:21.982
私はお金がなかったので おごりでした
いつも行きました

00:06:22.438 --> 00:06:24.505
後で何が起こっていたのか悟りました

00:06:24.529 --> 00:06:29.075
直属のマネージャーは上層部に

00:06:29.099 --> 00:06:32.212
重要な仕事のために雇ったのが

00:06:32.236 --> 00:06:36.166
ジーンズとスニーカーで仕事をする
10代女子だと言ってなかったんです

00:06:36.994 --> 00:06:39.196
私は良い仕事ぶりだったのに
体裁が悪くて

00:06:39.220 --> 00:06:40.919
年齢や性別の点でも
良くなかったんです

00:06:40.943 --> 00:06:44.289
ですから性別や人種に
惑わされない形での雇用は

00:06:44.313 --> 00:06:46.178
非常に良いことだと
私には思えます

00:06:46.851 --> 00:06:50.192
でもこのシステムを用いると
事態はより複雑になります なぜなら

00:06:50.788 --> 00:06:56.579
現在コンピュータシステムは
あなたに関するあらゆる類のことを

00:06:56.603 --> 00:06:58.475
デジタル情報の断片から
推測できるからです

00:06:58.499 --> 00:07:00.832
自分が開示していなくてもです

00:07:01.326 --> 00:07:04.253
システムはあなたの性的志向や

00:07:04.814 --> 00:07:06.120
性格特徴や

00:07:06.679 --> 00:07:08.052
政治的傾向を推測できます

00:07:08.650 --> 00:07:12.335
システムは高水準の正確さで
予測する力を持っています

00:07:13.182 --> 00:07:15.760
思い出してください 
開示さえしていない事柄をですよ

00:07:15.784 --> 00:07:17.375
これが推測です

00:07:17.399 --> 00:07:20.660
ある友達は 
そのようなコンピュータシステムを

00:07:20.684 --> 00:07:24.325
病的な あるいは産後の 抑うつの可能性を
予測するために開発しています

00:07:24.349 --> 00:07:25.765
SNSのデータを用いるんです

00:07:26.496 --> 00:07:27.923
結果は素晴らしいです

00:07:28.312 --> 00:07:31.669
彼女のシステムは
うつ罹患の可能性を

00:07:31.693 --> 00:07:35.596
症状が現れる数か月前に
予測できるのです

00:07:35.620 --> 00:07:36.993
数か月も前ですよ

00:07:37.017 --> 00:07:39.263
症状が全くない段階での予測です

00:07:39.287 --> 00:07:44.099
彼女はこれを早期介入のために
活用したがっています 素晴らしい！

00:07:44.731 --> 00:07:46.771
でもこれを雇用の文脈で
考えてみましょう

00:07:47.847 --> 00:07:50.893
例の 人事マネージャーの会議では

00:07:50.917 --> 00:07:55.626
私はある非常に大きな企業の
高職位のマネージャーに近づき

00:07:55.650 --> 00:08:00.228
こう言いました
「まだご存じないこととは思いますが

00:08:00.252 --> 00:08:06.801
もしそのシステムが 将来うつになる可能性が
高い人を排除しているとしたらどうでしょう？

00:08:07.581 --> 00:08:10.957
今ではなく　
将来そうなる可能性が高い人です

00:08:11.743 --> 00:08:15.149
妊娠する可能性の
高い女性を排除しているとしたら？

00:08:15.173 --> 00:08:17.759
来年か再来年のことで 
今は妊娠していない場合ですよ？

00:08:18.664 --> 00:08:24.300
もし職場の文化に合っているからと
攻撃的な人が雇われたらどうします？」

00:08:24.993 --> 00:08:27.684
性別の構成からは
そのことを読み取れません

00:08:27.708 --> 00:08:29.210
構成比はバランスが取れています

00:08:29.234 --> 00:08:32.791
これは機械学習で 
伝統的なプログラムではないので

00:08:32.815 --> 00:08:37.722
たとえば「うつハイリスク」とか
「妊娠ハイリスク」

00:08:37.746 --> 00:08:39.579
「攻撃的な人物度」

00:08:39.603 --> 00:08:41.337
などの変数は登場しません

00:08:41.815 --> 00:08:45.494
システムが何に基づいて選択しているのか
分からないばかりか

00:08:45.518 --> 00:08:47.841
どうすれば分かるのかの
手がかりもありません

00:08:47.865 --> 00:08:49.111
ブラックボックスなんです

00:08:49.135 --> 00:08:51.942
システムには予測力がありますが
人間には理解できない代物です

00:08:52.306 --> 00:08:54.675
「どんな安全対策をしていますか？

00:08:54.699 --> 00:08:58.372
あなたのブラックボックスが
やましいことをしないようにです」

00:09:00.683 --> 00:09:04.561
彼女は子犬の尻尾を10匹分も踏みつけた
人でなしを見るかのような顔になりました

00:09:04.585 --> 00:09:05.833
（笑）

00:09:05.857 --> 00:09:07.898
彼女は私をじっと見て言いました

00:09:08.376 --> 00:09:12.709
「これについては
もう何も聞きたくない」

00:09:13.278 --> 00:09:15.312
そして彼女は踵を返して
行ってしまいました

00:09:15.884 --> 00:09:17.370
彼女が失礼なわけではありません

00:09:17.394 --> 00:09:23.702
明らかに 聞かなかったことにしたい
あっち行ってという憎悪の眼差しでした

00:09:23.726 --> 00:09:24.972
（笑）

00:09:25.682 --> 00:09:29.521
いいですか そのようなシステムは
ある意味 偏見の程度は

00:09:29.545 --> 00:09:31.648
人間のマネージャーよりは
少ないかもしれません

00:09:31.672 --> 00:09:33.818
費用の面でも
理にかなっているでしょう

00:09:34.393 --> 00:09:36.043
でもそれはまた

00:09:36.067 --> 00:09:40.815
ひそやかながら確実に
労働市場からの

00:09:40.839 --> 00:09:43.132
うつハイリスク者の締め出しに
つながりかねません

00:09:43.573 --> 00:09:46.169
これが私たちの築きたい
社会の姿でしょうか？

00:09:46.193 --> 00:09:48.478
こんなことをしていることさえ
私たちは知らないんです

00:09:48.502 --> 00:09:52.466
完全には理解していない機械に
意思決定をさせているんですからね

00:09:53.085 --> 00:09:54.543
もう１つの問題はこれです

00:09:55.134 --> 00:09:59.586
このようなシステムの訓練は往々にして
人間の行動データに基づいています

00:09:59.610 --> 00:10:01.426
人間らしさが刻み込まれています

00:10:02.008 --> 00:10:05.816
それらは私たちの偏見を
反映している可能性があり

00:10:05.840 --> 00:10:09.433
これらのシステムは
私たちの偏見を拾い上げ

00:10:09.457 --> 00:10:10.770
それを増幅して

00:10:10.794 --> 00:10:12.212
私たちに示し返しかねません

00:10:12.236 --> 00:10:13.698
私たちはこんな言いっぷりなのにですよ

00:10:13.722 --> 00:10:16.839
「私たちはまさしく客観的です
中立的なコンピューティングですから」

00:10:18.134 --> 00:10:20.811
研究者たちは Googleにおいて

00:10:21.954 --> 00:10:27.267
女性には 高給の求人広告が
表示されにくいことを見出しました

00:10:28.283 --> 00:10:30.813
また アフリカ系アメリカ人の
名前を検索すると

00:10:30.837 --> 00:10:35.543
犯罪歴をほのめかす広告が
高確率で表示されます

00:10:35.567 --> 00:10:37.134
犯罪歴がない人の場合でもそうです

00:10:38.513 --> 00:10:42.062
そのような隠れた偏見と
ブラックボックスのアルゴリズムを

00:10:42.086 --> 00:10:46.059
研究者が暴くこともありますが
知られない場合もあります

00:10:46.083 --> 00:10:48.744
それらは人生を
変える結果になりうるのです

00:10:49.778 --> 00:10:53.937
ウィスコンシンで ある被告が
刑期６年の判決を受けました

00:10:53.961 --> 00:10:55.316
警察官から逃げたためです

00:10:56.644 --> 00:10:57.830
ご存知ないかもしれませんが

00:10:57.854 --> 00:11:01.852
仮釈放や判決の決定においても
アルゴリズムの使用が増えています

00:11:01.876 --> 00:11:04.831
彼はこのスコアが計算される仕組みを
知りたいと思いました

00:11:05.615 --> 00:11:07.280
それは商用のブラックボックスです

00:11:07.304 --> 00:11:11.509
企業はアルゴリズムが
公開の法廷で検証されるのを拒みました

00:11:12.216 --> 00:11:17.748
でもProPublicaという非営利の調査団体が
そのアルゴリズムを監査しました

00:11:17.772 --> 00:11:19.788
入手可能だった
公開データを用いてです

00:11:19.812 --> 00:11:22.128
そして分かったのは
結果には偏見が影響しており

00:11:22.152 --> 00:11:25.781
予測力はひどいものでした
偶然よりわずかにましな程度です

00:11:25.805 --> 00:11:30.221
黒人の被告は 白人の被告に比べて
将来犯罪を起こす確率が

00:11:30.245 --> 00:11:34.140
２倍高いと
誤ってラベリングされていました

00:11:35.711 --> 00:11:37.275
ではこのケースを考えてみましょう

00:11:37.923 --> 00:11:41.775
女性のほうは予定より遅れて
親友を迎えに行くため

00:11:41.799 --> 00:11:43.874
フロリダ州ブロワード郡の
ある学校に向かって

00:11:44.577 --> 00:11:46.933
友達と一緒に道を走っていました

00:11:46.957 --> 00:11:51.056
ふたりはある家の玄関で 無施錠の
子ども用の自転車とキックスケーターを見つけ

00:11:51.080 --> 00:11:52.712
愚かにもそれに飛び乗りました

00:11:52.736 --> 00:11:55.335
走り去ろうとしたところ
女性が出てきて言いました

00:11:55.359 --> 00:11:57.564
「ちょっと！
それはうちの子の自転車よ！」

00:11:57.588 --> 00:12:00.882
ふたりは降りて 歩き去りましたが
逮捕されました

00:12:00.906 --> 00:12:04.543
彼女は間違っていたし愚かでした
でもまだ18歳です

00:12:04.567 --> 00:12:07.111
彼女は２回の非行歴がありました

00:12:07.628 --> 00:12:12.813
一方 男性のほうは Home Depoで
万引きをして捕まりました

00:12:12.837 --> 00:12:15.761
彼が万引きしたのは85ドル相当で
同じく軽犯罪ですが

00:12:16.586 --> 00:12:21.145
彼は強盗で前科２犯でした

00:12:21.775 --> 00:12:25.257
でもアルゴリズムは 男性ではなく
女性の方をハイリスクと評価しました

00:12:26.566 --> 00:12:30.440
その女性が２年後に再犯していないことを
ProPiblicaは明らかにしています

00:12:30.464 --> 00:12:33.014
犯罪記録をもつ彼女が
職を得るのは実に困難でした

00:12:33.038 --> 00:12:35.114
一方 男性の方は再犯し

00:12:35.138 --> 00:12:38.974
２つ目の犯罪のために
現在は８年間の収監中です

00:12:39.908 --> 00:12:43.277
ブラックボックスに対して
監査が必要なのは明白です

00:12:43.301 --> 00:12:45.916
チェックしないままこの種の権力を
与えてはいけないのです

00:12:45.940 --> 00:12:48.819
（拍手）

00:12:49.907 --> 00:12:54.149
監査は偉大で重要ですが
それで全ての問題を解決できはしません

00:12:54.173 --> 00:12:56.921
Facebookのニュース・フィードの
強力なアルゴリズムの場合

00:12:56.945 --> 00:13:01.788
全てをランク付けし
全ての友達やフォロー中のページのなかで

00:13:01.812 --> 00:13:04.096
何を見るべきか決定する仕組みですね

00:13:04.718 --> 00:13:06.993
赤ちゃんの写真をもう１枚見るべきか？

00:13:07.017 --> 00:13:08.213
（笑）

00:13:08.237 --> 00:13:10.833
知り合いからの ご機嫌斜めのコメントは？

00:13:11.269 --> 00:13:13.125
重要だけど難解なニュース記事は？

00:13:13.149 --> 00:13:14.631
正答はありません

00:13:14.655 --> 00:13:17.314
Facebookはサイト上での
やりとりに応じて最適化します

00:13:17.338 --> 00:13:18.753
「いいね」やシェア コメント
といったものです

00:13:19.988 --> 00:13:22.684
2014年8月

00:13:22.708 --> 00:13:25.370
ミズーリ州ファーガソンで
抗議運動が勃発しました

00:13:25.394 --> 00:13:29.811
アフリカ系アメリカ人の10代が
白人の警察官に殺され

00:13:29.835 --> 00:13:31.405
その状況が不審だったのです

00:13:31.794 --> 00:13:33.801
抗議運動のニュースは

00:13:33.825 --> 00:13:36.510
フィルタリングされない
Twitterフィードを埋め尽くしました

00:13:36.534 --> 00:13:38.484
でもFacebookには何ら
表示されませんでした

00:13:39.002 --> 00:13:40.736
Facebook上の友達との
関連でしょうか？

00:13:40.760 --> 00:13:42.792
私はFacebookのアルゴリズムを
無効にしました

00:13:43.292 --> 00:13:46.140
Facebookはアルゴリズムの
管理下に置きたがるので

00:13:46.164 --> 00:13:48.200
難しかったですけどね

00:13:48.224 --> 00:13:50.462
すると友達が 抗議運動のことを
話しているのが分かりました

00:13:50.486 --> 00:13:52.995
アルゴリズムが私に
見せなかっただけなんです

00:13:53.019 --> 00:13:56.061
調査して分かりましたが
これは広範囲にわたる問題でした

00:13:56.085 --> 00:13:59.898
ファーガソンの話題は
アルゴリズムに馴染まなかったんです

00:13:59.922 --> 00:14:01.093
「いいね」しにくいのです

00:14:01.117 --> 00:14:02.669
誰が「いいね」します？

00:14:03.320 --> 00:14:05.526
コメントをするのさえ
容易じゃありません

00:14:05.550 --> 00:14:06.921
「いいね」もコメントもないので

00:14:06.945 --> 00:14:10.237
アルゴリズムは少数の人にしか
それを表示しません

00:14:10.261 --> 00:14:11.803
だから目にすることがなかったんです

00:14:12.766 --> 00:14:13.994
そのかわり その週

00:14:14.018 --> 00:14:16.316
Facebookのアルゴリズムが
ハイライトしたのは

00:14:16.340 --> 00:14:18.566
ALSアイス・バケツ・チャレンジでした

00:14:18.590 --> 00:14:22.332
価値のある目的で氷水をかぶり
チャリティに寄付 良いですね

00:14:22.356 --> 00:14:24.260
でも極めてよく
アルゴリズムに馴染みます

00:14:25.039 --> 00:14:27.652
機械が私たちのために
これを決定したんです

00:14:27.676 --> 00:14:31.173
非常に重要だけれど難解な会話は

00:14:31.197 --> 00:14:32.752
Facebookが唯一の経路の場合

00:14:32.776 --> 00:14:35.472
抑え込まれてきたのかもしれません

00:14:35.937 --> 00:14:39.734
さて最後にこれらのシステムは

00:14:39.758 --> 00:14:42.494
人間のシステムとは似つかない誤りを
犯しうるのです

00:14:42.518 --> 00:14:45.440
皆さんはワトソンを覚えていますか
IBMの機械知能システムで

00:14:45.464 --> 00:14:48.592
クイズ番組『ジェパディ！』で
対戦相手の人間を打ち負かしました

00:14:48.951 --> 00:14:50.379
すごい選手だったんです

00:14:50.403 --> 00:14:53.972
しかし最終問題で
ワトソンは こんな質問をされました

00:14:54.479 --> 00:14:57.411
「その地域最大の空港の名は
第二次世界大戦の英雄に由来し

00:14:57.435 --> 00:14:59.687
２番目の空港の名の由来は
第二次世界大戦中の戦いです」

00:14:59.711 --> 00:15:01.089
（最終問題の音楽をハミング）

00:15:01.402 --> 00:15:02.584
「シカゴ」

00:15:02.608 --> 00:15:03.978
人間ふたりは正答でした

00:15:04.517 --> 00:15:08.865
一方ワトソンの答えは
「トロント」

00:15:08.889 --> 00:15:10.707
米国の都市についての
問題だったのに！

00:15:11.416 --> 00:15:14.317
この素晴らしいシステムも
エラーをするんです

00:15:14.341 --> 00:15:17.992
人間はしないようなエラーです
２年生の子どもでもしません

00:15:18.643 --> 00:15:21.752
機械知能は失敗を犯すこともあるんです

00:15:21.776 --> 00:15:24.876
人間のエラーパターンとは
異なります

00:15:24.900 --> 00:15:27.850
予想外であり 備えもできないような方法です

00:15:27.874 --> 00:15:31.512
資質のある人が仕事を得られないのも
ひどい話ですが

00:15:31.536 --> 00:15:35.263
もしそれがプログラムのサブルーチンに伴う
スタックオーバーフローが原因なら

00:15:35.287 --> 00:15:36.719
３倍ひどい話です

00:15:36.743 --> 00:15:38.322
（笑）

00:15:38.346 --> 00:15:41.132
2010年5月

00:15:41.156 --> 00:15:45.200
ウォールストリートの
「売り」アルゴリズムでの

00:15:45.224 --> 00:15:48.252
フィードバックループによって
瞬間暴落が起き

00:15:48.276 --> 00:15:52.460
36分間で１兆ドル相当の
損失が出ました

00:15:53.542 --> 00:15:55.729
「エラー」の意味を
考えたくもないのが

00:15:55.753 --> 00:15:59.342
無人攻撃機の場合です

00:16:01.714 --> 00:16:05.504
ええ人間には 偏見がつきものです

00:16:05.528 --> 00:16:07.704
意思決定者やゲートキーパー

00:16:07.728 --> 00:16:11.221
法廷、ニュース、戦争・・・

00:16:11.245 --> 00:16:14.283
そこではミスが生じますが
これこそ私の言いたいことです

00:16:14.307 --> 00:16:17.828
これらの難問から
私たちは逃れられません

00:16:18.416 --> 00:16:21.932
私たちは責任を
機械に外部委託することはできないのです

00:16:22.496 --> 00:16:26.704
（拍手）

00:16:28.909 --> 00:16:33.356
人工知能は「倫理問題からの解放」カードを
私たちにくれたりしません

00:16:34.562 --> 00:16:37.943
データ科学者のフレッド・ベネンソンは
これを数学による洗脳だと呼びました

00:16:37.967 --> 00:16:39.356
私たちに必要なのは逆のものです

00:16:39.380 --> 00:16:44.768
私たちはアルゴリズムを疑い 
精査するようにならねばなりません

00:16:45.200 --> 00:16:48.398
私たちは アルゴリズムについての
説明責任を持ち

00:16:48.422 --> 00:16:50.867
監査や意味のある透明化を
求めなければなりません

00:16:51.200 --> 00:16:54.434
私たちは厄介で価値観にかかわる
人間くさい事柄に対して

00:16:54.458 --> 00:16:57.428
数学や計算機は
客観性をもたらしえないことを

00:16:57.452 --> 00:16:59.836
受け入れなければなりません

00:17:00.714 --> 00:17:03.493
むしろ人間くささのもつ複雑さが
アルゴリズムを管理するのです

00:17:03.968 --> 00:17:07.455
確かに私たちは コンピュータを
良い決断を下す助けとして

00:17:07.479 --> 00:17:09.493
使いうるし そうすべきです

00:17:09.517 --> 00:17:14.849
でも私たちは判断を下すことへの
自分の道徳的な責任を認め

00:17:14.873 --> 00:17:17.691
そしてアルゴリズムを
その枠内で用いなければなりません

00:17:17.715 --> 00:17:22.650
自分の責任を放棄して
別の人間へ委ねることとは

00:17:22.674 --> 00:17:25.128
異なるのです

00:17:25.627 --> 00:17:28.236
機械知能はもう存在しています

00:17:28.260 --> 00:17:31.681
つまり私たちは
人間としての価値観や倫理感を

00:17:31.705 --> 00:17:33.852
よりしっかり持たねばなりません

00:17:33.876 --> 00:17:35.030
ありがとうございました

00:17:35.054 --> 00:17:40.074
（拍手）

