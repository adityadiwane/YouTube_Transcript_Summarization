WEBVTT
Kind: captions
Language: iw

00:00:00.000 --> 00:00:07.000
מתרגם: Gili Baltsan
מבקר: Ido Dekkers

00:00:12.739 --> 00:00:16.861
התחלתי את עבודתי הראשונה
כמתכנתת מחשבים

00:00:16.885 --> 00:00:18.841
בשנתי הראשונה בקולג' -

00:00:18.865 --> 00:00:20.372
כשהייתי נערה מתבגרת.

00:00:20.889 --> 00:00:22.621
זמן קצר לאחר שהתחלתי לעבוד,

00:00:22.645 --> 00:00:24.255
לכתוב תוכנות בחברה,

00:00:24.799 --> 00:00:28.434
מנהל שעבד בחברה
נכנס לחדר בו הייתי,

00:00:28.458 --> 00:00:29.726
ולחש אליי,

00:00:30.229 --> 00:00:33.090
"האם הוא יכול לדעת אם אני משקר?"

00:00:33.806 --> 00:00:35.883
לא היה אף אחד נוסף בחדר.

00:00:37.032 --> 00:00:41.421
"מי יכול לדעת אם אתה משקר?
ולמה אנחנו לוחשים?"

00:00:42.266 --> 00:00:45.373
המנהל הצביע על המחשב בחדר.

00:00:45.397 --> 00:00:48.493
"האם הוא יכול לדעת אם אני משקר?"

00:00:49.613 --> 00:00:53.975
טוב, המנהל הזה ניהל רומן עם פקידת הקבלה.

00:00:53.999 --> 00:00:55.111
(צחוק)

00:00:55.135 --> 00:00:56.901
ואני הייתי עדיין נערה מתבגרת.

00:00:57.447 --> 00:00:59.466
אז לחשתי-צעקתי אליו בחזרה,

00:00:59.490 --> 00:01:03.114
"כן, המחשב יכול לדעת אם אתה משקר."

00:01:03.138 --> 00:01:04.944
(צחוק)

00:01:04.968 --> 00:01:07.891
טוב, צחקתי, אבל בעצם,
הבדיחה היא על חשבוני.

00:01:07.915 --> 00:01:11.183
בימינו, קיימות מערכות ממוחשבות

00:01:11.207 --> 00:01:14.755
שיכולות לקלוט מצבים רגשיים ואפילו שקר

00:01:14.779 --> 00:01:16.823
באמצעות עיבוד פרצופים אנושיים.

00:01:17.248 --> 00:01:21.401
מפרסמים ואפילו ממשלות מאד מתעניינות בזה.

00:01:22.319 --> 00:01:24.181
אני נהייתי מתכנתת מחשבים

00:01:24.205 --> 00:01:27.318
כי הייתי מהילדים האלה 
שמשוגעים על מתמטיקה ומדעים.

00:01:27.942 --> 00:01:31.050
אבל איפשהו במהלך הדרך
למדתי על נשק גרעיני,

00:01:31.074 --> 00:01:34.026
והתחלתי להיות מאוד מודאגת
בעניין האתיקה של המדע.

00:01:34.050 --> 00:01:35.254
הייתי מוטרדת.

00:01:35.278 --> 00:01:37.919
עם זאת, בגלל נסיבות משפחתיות,

00:01:37.943 --> 00:01:41.241
נאלצתי להתחיל לעבוד
מוקדם ככל האפשר.

00:01:41.265 --> 00:01:44.564
אז חשבתי לעצמי,
אני אבחר בתחום טכני

00:01:44.588 --> 00:01:46.384
בו אמצא עבודה בקלות

00:01:46.408 --> 00:01:50.426
ולא אצטרך להתעסק
בשאלות אתיות מטרידות.

00:01:51.022 --> 00:01:52.551
אז בחרתי במחשבים.

00:01:52.575 --> 00:01:53.679
(צחוק)

00:01:53.703 --> 00:01:57.113
טוב, חה, חה, חה!
כל הצחוק הוא על חשבוני.

00:01:57.137 --> 00:01:59.891
בימינו, מדעני מחשבים בונים פלטפורמות

00:01:59.915 --> 00:02:04.124
אשר שולטות במה שמיליארד אנשים
יצפו בכל יום.

00:02:05.052 --> 00:02:08.874
הם מפתחים מכוניות
שיכולות להחליט את מי לדרוס.

00:02:09.707 --> 00:02:12.920
הם אפילו בונים מכונות, כלי נשק,

00:02:12.944 --> 00:02:15.229
שיכולים להרוג בני אדם במלחמה.

00:02:15.253 --> 00:02:18.024
מדובר באתיקה לאורך כל הדרך.

00:02:19.183 --> 00:02:21.241
הבינה המלאכותית כבר כאן.

00:02:21.823 --> 00:02:25.297
אנו משתמשים כעת במחשוב
כדי לקבל כל מיני החלטות.

00:02:25.321 --> 00:02:27.207
אבל גם החלטות מסוגים חדשים.

00:02:27.231 --> 00:02:32.403
אנחנו שואלים את המחשוב שאלות 
שיש להן יותר מתשובה נכונה אחת,

00:02:32.427 --> 00:02:33.629
שהן סוביקטיביות

00:02:33.653 --> 00:02:35.978
שאלות פתוחות ובעלות מטען ערכי.

00:02:36.002 --> 00:02:37.760
אנו שואלים שאלות כמו,

00:02:37.784 --> 00:02:39.434
"את מי צריכה החברה להעסיק?"

00:02:40.096 --> 00:02:42.855
"איזה עדכון מאיזה חבר
אנחנו צריכים להראות?"

00:02:42.879 --> 00:02:45.145
"מיהו האסיר שסביר יותר שיפשע שוב?"

00:02:45.514 --> 00:02:48.568
"על איזה אייטם חדשותי או סרט
כדאי להמליץ לאנשים?"

00:02:48.592 --> 00:02:51.964
תראו, כן, אנחנו משתמשים במחשבים 
כבר זמן רב,

00:02:51.988 --> 00:02:53.505
אבל זה שונה.

00:02:53.529 --> 00:02:55.596
זהו טוויסט היסטורי,

00:02:55.620 --> 00:03:00.957
כיוון שאנחנו לא יכולים להיעזר במחשוב 
בשאלות סובייקטיביות כאלו

00:03:00.981 --> 00:03:06.401
כפי שאנו נעזרים במחשוב 
להטסת מטוסים, לבניית גשרים,

00:03:06.425 --> 00:03:07.684
להגיע לירח.

00:03:08.449 --> 00:03:11.708
האם מטוסים בטוחים יותר?
האם הגשר התנדנד ונפל?

00:03:11.732 --> 00:03:16.230
בדברים האלו הסכמנו על
אמות מידה ברורות למדי,

00:03:16.254 --> 00:03:18.493
ויש לנו את חוקי הטבע שמדריכים אותנו.

00:03:18.517 --> 00:03:21.911
אין לנו עוגנים ואמות מידה כאלו

00:03:21.935 --> 00:03:25.898
עבור החלטות בעניינים אנושיים מסובכים.

00:03:25.922 --> 00:03:30.159
כדי לסבך את העניין עוד יותר,
התוכנה שלנו הולכת ונעשית חזקה יותר,

00:03:30.183 --> 00:03:33.956
אבל היא גם נעשית פחות שקופה
ויותר מורכבת.

00:03:34.542 --> 00:03:36.582
לאחרונה, בעשור החולף,

00:03:36.606 --> 00:03:39.335
אלגוריתמים מורכבים התקדמו מאד.

00:03:39.359 --> 00:03:41.349
הם יכולים לזהות פרצופים אנושיים.

00:03:41.985 --> 00:03:44.040
הם יכולים לפענח כתב יד.

00:03:44.436 --> 00:03:46.502
הם יכולים לזהות הונאת כרטיסי אשראי

00:03:46.526 --> 00:03:47.715
ולחסום דואר זבל

00:03:47.739 --> 00:03:49.776
והם יכולים לתרגם משפה לשפה.

00:03:49.800 --> 00:03:52.374
הם יכולים לזהות גידולים בהדמיה רפואית.

00:03:52.398 --> 00:03:54.603
הם יכולים לנצח בני אדם 
במשחקי שח-מט וגו.

00:03:55.264 --> 00:03:59.768
הרבה מההתקדמות הזו היא הודות לשיטה
שנקראת "לימוד מכונה".

00:04:00.175 --> 00:04:03.362
לימוד מכונה הוא שונה מתכנות מסורתי,

00:04:03.386 --> 00:04:06.971
בו נותנים למחשב הוראות מפורטות, 
מדויקות, מדוקדקות.

00:04:07.378 --> 00:04:11.560
זה יותר כמו שמכניסים למערכת הרבה נתונים,

00:04:11.584 --> 00:04:13.240
כולל נתונים לא מובנים,

00:04:13.264 --> 00:04:15.542
כמו אלה שאנו מייצרים בחיינו הדיגיטליים.

00:04:15.566 --> 00:04:18.296
והמערכת לומדת, 
על-ידי ערבול כל הנתונים הללו.

00:04:18.669 --> 00:04:20.195
כמו כן, באופן מכריע,

00:04:20.219 --> 00:04:24.599
המערכות הללו לא פועלות 
על פי ההיגיון של תשובה-אחת.

00:04:24.623 --> 00:04:27.582
הן לא מייצרות תשובה פשוטה;
זה יותר הסתברותי:

00:04:27.606 --> 00:04:31.089
"זו כנראה יותר מתאימה למה שאתה מחפש."

00:04:32.023 --> 00:04:35.093
היתרון הוא: 
השיטה הזו ממש חזקה.

00:04:35.117 --> 00:04:37.193
המנהל של מערכת AI של גוגל קרא לזה,

00:04:37.217 --> 00:04:39.414
"היעילות חסרת ההיגיון של נתונים."

00:04:39.791 --> 00:04:41.144
החיסרון הוא,

00:04:41.738 --> 00:04:44.809
שאנחנו לא באמת מבינים
מה המערכת למדה.

00:04:44.833 --> 00:04:46.420
בעצם, זה הכוח שלה.

00:04:46.946 --> 00:04:50.744
זה פחות כמו לתת הוראות למחשב;

00:04:51.200 --> 00:04:55.264
זה יותר כמו לאמן יצור שהוא גור-מכונה

00:04:55.288 --> 00:04:57.659
שאנחנו לא באמת מבינים ושולטים בו.

00:04:58.362 --> 00:04:59.913
אז זו הבעיה שלנו.

00:05:00.427 --> 00:05:04.689
זו בעיה כאשר הבינה המלאכותית הזו טועה.

00:05:04.713 --> 00:05:08.253
זו גם בעיה כאשר היא אינה טועה,

00:05:08.277 --> 00:05:11.905
מכיוון שאנו לא יודעים מהו מה
כאשר הבעיה היא סוביקטיבית.

00:05:11.929 --> 00:05:14.268
איננו יודעים מה הדבר הזה חושב.

00:05:15.493 --> 00:05:19.176
אז, חישבו על אלגוריתם של העסקת עובדים -

00:05:20.123 --> 00:05:24.434
מערכת שרגילה להעסיק אנשים,
משתמשת במערכות לימוד-מכונה.

00:05:25.052 --> 00:05:28.631
מערכת כזו הייתה מאומנת
על נתוני העסקה קודמים

00:05:28.655 --> 00:05:31.246
וניתנת לה הנחייה למצוא ולהעסיק

00:05:31.270 --> 00:05:34.308
אנשים בעלי ביצועים טובים
כמו אלה שכבר נמצאים בחברה.

00:05:34.814 --> 00:05:35.967
נשמע טוב.

00:05:35.991 --> 00:05:37.990
פעם השתתפתי בכנס

00:05:38.014 --> 00:05:41.139
אשר חיבר יחד 
מנהלי משאבי אנוש,

00:05:41.163 --> 00:05:42.363
אנשים ברמה גבוהה,

00:05:42.363 --> 00:05:44.092
המשתמשים במערכות כאלה להעסקת עובדים.

00:05:44.092 --> 00:05:45.622
הם מאד התלהבו.

00:05:45.646 --> 00:05:50.299
הם חשבו שזה יהפוך את תהליך ההעסקה 
לאובייקטיבי יותר, פחות מוטה,

00:05:50.323 --> 00:05:53.323
ויעניק לנשים ומיעוטים סיכוי גבוה יותר

00:05:53.347 --> 00:05:55.535
כנגד מנהלים אנושיים משוחדים.

00:05:55.559 --> 00:05:58.402
ותראו - העסקה על-ידי בני-אדם היא משוחדת.

00:05:59.099 --> 00:06:00.284
אני יודעת.

00:06:00.308 --> 00:06:03.313
באחד ממקומות העבודה המוקדמים שלי כמתכנתת,

00:06:03.337 --> 00:06:07.205
המנהלת הישירה שלי הייתה באה לפעמים 
למקום בו הייתי

00:06:07.229 --> 00:06:10.982
ממש מוקדם בבוקר
או ממש מאוחר אחר הצהריים,

00:06:11.006 --> 00:06:14.068
ואמרה לי, "זיינב,
בואי נלך לאכול ארוחת צהרים!"

00:06:14.724 --> 00:06:16.891
הייתי מבולבלת מהתזמון המוזר.

00:06:16.915 --> 00:06:19.044
עכשיו 4 אחר הצהריים. ארוחת צהריים?

00:06:19.068 --> 00:06:22.162
הייתי מרוששת, אז ארוחת חינם. תמיד הלכתי.

00:06:22.618 --> 00:06:24.685
מאוחר יותר הבנתי מה קרה.

00:06:24.709 --> 00:06:29.255
מנהליי הישירים לא הודו 
בפני הממונים עליהם

00:06:29.279 --> 00:06:32.392
שהמתכנתת שהם העסיקו בתפקיד רציני
היא נערה מתבגרת

00:06:32.416 --> 00:06:36.346
שבאה לעבודה בג'ינס וסניקרס.

00:06:37.174 --> 00:06:39.376
עשיתי עבודה טובה,
רק לא נראיתי נכון

00:06:39.400 --> 00:06:41.099
והייתי בגיל ובמגדר הלא נכונים.

00:06:41.123 --> 00:06:44.469
אז העסקת עובדים באופן עיוור למגדר וגזע

00:06:44.493 --> 00:06:46.358
בהחלט נשמע לי טוב.

00:06:47.031 --> 00:06:50.372
אבל עם המערכות האלו,
זה יותר מורכב, וזו הסיבה:

00:06:50.968 --> 00:06:56.759
כעת, מערכות ממוחשבות יכולות להסיק
כל מיני דברים עליכם

00:06:56.783 --> 00:06:58.655
מהפירורים הדיגיטליים שלכם,

00:06:58.679 --> 00:07:01.012
אפילו אם לא גילית את הדברים האלה.

00:07:01.506 --> 00:07:04.433
הם יכולות להסיק לגבי האוריינטציה
המינית שלכם,

00:07:04.994 --> 00:07:06.300
תכונות האופי שלכם,

00:07:06.859 --> 00:07:08.232
הנטייה הפוליטית שלכם.

00:07:08.830 --> 00:07:12.515
יש להן יכולת ניבוי עם רמות דיוק גבוהות.

00:07:13.362 --> 00:07:15.940
זכרו - אפילו לגבי דברים שלא גיליתם.

00:07:15.964 --> 00:07:17.555
זוהי הסקה.

00:07:17.579 --> 00:07:20.840
יש לי חברה שפיתחה
מערכות ממוחשבות שיכולות

00:07:20.864 --> 00:07:24.505
לנבא את הסיכון לדיכאון קליני
או דיכאון אחרי לידה

00:07:24.529 --> 00:07:25.945
מנתונים של מדיה חברתית.

00:07:26.676 --> 00:07:28.103
התוצאות הן מרשימות.

00:07:28.492 --> 00:07:31.849
המערכת שלה יכולה לנבא 
את הסיכון ללקות בדיכאון

00:07:31.873 --> 00:07:35.776
חודשים לפני הופעת סימפטומים כלשהם -

00:07:35.800 --> 00:07:37.173
חודשים לפני.

00:07:37.197 --> 00:07:39.443
אין סימפטומים, יש ניבוי.

00:07:39.467 --> 00:07:44.279
היא מקווה שישתמשו בזה 
להתערבות מוקדמת. נהדר!

00:07:44.911 --> 00:07:46.951
אבל עכשיו שימו את זה בהקשר של העסקה.

00:07:48.027 --> 00:07:51.073
אז בכנס הזה של מנהלי משאבי אנוש,

00:07:51.097 --> 00:07:55.806
פניתי אל מנהלת בכירה בחברה גדולה מאד,

00:07:55.830 --> 00:08:00.408
ואמרתי לה, "תראי,
מה אם, ללא ידיעתך,

00:08:00.432 --> 00:08:06.981
"המערכת שלך מנפה החוצה אנשים
עם סיכון עתידי גבוה ללקות בדיכאון?

00:08:07.761 --> 00:08:11.137
"הם לא מדוכאים כעת,
רק אולי בעתיד, בסיכון גבוה יותר.

00:08:11.923 --> 00:08:15.329
"מה אם היא מנפה החוצה נשים
שסיכוייהן גדולים יותר להרות

00:08:15.353 --> 00:08:17.939
"בשנה או שנתיים הקרובות
אך אינן בהריון כעת?

00:08:18.844 --> 00:08:24.480
"מה אם המערכת מעסיקה אנשים אגרסיביים
מכיוון שזו התרבות בסביבת העבודה שלך?"

00:08:25.173 --> 00:08:27.864
אתם לא יכולים לדעת את זה
על-ידי חלוקה למגדרים.

00:08:27.888 --> 00:08:29.390
אלו יכולים להיות מאוזנים.

00:08:29.414 --> 00:08:32.971
ומאחר וזאת למידת מכונה,
לא שיטת קידוד מסורתית,

00:08:32.995 --> 00:08:37.902
אין שם משתנה שמוגדר "סיכון מוגבר לדיכאון",

00:08:37.926 --> 00:08:39.759
"סיכון מוגבר להריון,"

00:08:39.783 --> 00:08:41.517
"סולם של אגרסיביות".

00:08:41.995 --> 00:08:45.674
לא רק שאינכם יודעים 
לפי מה המערכת שלכם בוחרת,

00:08:45.698 --> 00:08:48.021
אתם אפילו לא יודעים איפה להתחיל לחפש.

00:08:48.045 --> 00:08:49.291
זוהי קופסה שחורה.

00:08:49.315 --> 00:08:52.122
יש לה יכולת ניבוי,
אבל אתם לא מבינים אותה.

00:08:52.486 --> 00:08:54.855
"אילו אמצעי ביטחון," שאלתי, "יש לך

00:08:54.879 --> 00:08:58.552
"להבטיח שהקופסה השחורה שלך
לא עושה משהו מפוקפק?"

00:09:00.863 --> 00:09:04.741
היא הסתכלה עלי כאילו שדרכתי
על 10 זנבות של כלבלבים.

00:09:04.765 --> 00:09:06.013
(צחוק)

00:09:06.037 --> 00:09:08.078
היא נעצה בי מבט ואמרה,

00:09:08.556 --> 00:09:12.889
"אני לא רוצה לשמוע מילה נוספת על זה."

00:09:13.458 --> 00:09:15.492
והיא הסתובבה והלכה משם.

00:09:16.064 --> 00:09:17.550
שימו לב - היא לא הייתה גסת רוח.

00:09:17.574 --> 00:09:23.882
זה היה בבירור: מה שאני לא יודעת
הוא לא הבעיה שלי, לכי מכאן, מבט מקפיא.

00:09:23.906 --> 00:09:25.152
(צחוק)

00:09:25.862 --> 00:09:29.701
תראו, מערכת כזו
יכולה אפילו להיות פחות מוטה

00:09:29.725 --> 00:09:31.828
מאשר מנהלים אנושיים באופנים מסוימים.

00:09:31.852 --> 00:09:33.998
והיא יכולה להיות הגיונית מבחינה כלכלית.

00:09:34.573 --> 00:09:36.223
אבל היא גם יכולה להוביל

00:09:36.247 --> 00:09:40.995
לסגירה מתמשכת וחמקנית של שוק העבודה

00:09:41.019 --> 00:09:43.312
בפני אנשים עם סיכון גבוה ללקות בדיכאון.

00:09:43.753 --> 00:09:46.349
האם זו החברה שאנחנו רוצים לבנות,

00:09:46.373 --> 00:09:48.658
מבלי שנדע אפילו שאנחנו עושים זאת,

00:09:48.682 --> 00:09:52.646
בגלל שהשארנו את קבלת ההחלטות
בידי מכונות שאנחנו לא מבינים עד הסוף?

00:09:53.265 --> 00:09:54.723
בעיה נוספת היא זו:

00:09:55.314 --> 00:09:59.766
המערכות האלו לעיתים קרובות מכוונות 
לנתונים המיוצרים על ידי הפעולות שלנו,

00:09:59.790 --> 00:10:01.606
חותם אנושי.

00:10:02.188 --> 00:10:05.996
אם כך, הן יכולות פשוט לשקף 
את ההעדפות שלנו.

00:10:06.020 --> 00:10:09.613
והמערכות האלו יכולות להיטפל להעדפות שלנו

00:10:09.637 --> 00:10:10.950
ולהגביר אותן

00:10:10.974 --> 00:10:12.392
ולשקף לנו אותן בחזרה,

00:10:12.416 --> 00:10:13.878
בזמן שאנחנו אומרים לעצמנו,

00:10:13.902 --> 00:10:17.019
"אנחנו עוסקים במחשוב אובייקטיבי וניטרלי."

00:10:18.314 --> 00:10:20.991
חוקרים מצאו שבגוגל,

00:10:22.134 --> 00:10:27.447
לנשים יש סיכוי נמוך יותר מאשר לגברים 
לקבל מודעות דרושים לתפקידים עם שכר גבוה.

00:10:28.463 --> 00:10:30.993
וחיפוש של שמות אפריקנים-אמריקנים

00:10:31.017 --> 00:10:35.723
יעלה בסיכוי גבוה יותר פרסומות
המרמזות על עבר פלילי,

00:10:35.747 --> 00:10:37.314
אפילו כאשר אין כזה.

00:10:38.693 --> 00:10:42.242
להטיות חבויות כאלה
ואלגוריתמים של קופסא שחורה

00:10:42.266 --> 00:10:46.239
שחוקרים מגלים לפעמים
אבל לפעמים איננו יודעים עליהם,

00:10:46.263 --> 00:10:48.924
יכולות להיות השלכות משנות חיים.

00:10:49.958 --> 00:10:54.117
בוויסקונסין, נאשם
נשפט לשש שנים בכלא

00:10:54.141 --> 00:10:55.496
על התחמקות מהמשטרה.

00:10:56.674 --> 00:10:58.070
יכול להיות שאתם לא יודעים זאת,

00:10:58.070 --> 00:11:02.032
אך השימוש באלגוריתמים הולך וגובר
בהחלטות על ענישה וחנינה.

00:11:02.056 --> 00:11:05.011
הוא רצה לדעת:
איך המספר הזה חושב?

00:11:05.795 --> 00:11:07.460
זו היא קופסה שחורה מסחרית.

00:11:07.484 --> 00:11:11.689
החברה סירבה לאפשר לאתגר את האלגוריתם
שלה באולם בית המשפט

00:11:12.396 --> 00:11:17.928
אבל פרו-פבליקה, חברת חקירות 
ללא מטרות רווח, בדקה את האלגוריתם הזה

00:11:17.952 --> 00:11:19.968
עם כל הנתונים הציבוריים שיכלו למצוא,

00:11:19.992 --> 00:11:22.308
וגילו שהתוצאות היו מוטות

00:11:22.332 --> 00:11:25.961
וכוח הניבוי שלו היה מפחיד,
בקושי יותר טוב ממזל,

00:11:25.985 --> 00:11:30.401
והוא הגדיר באופן מוטעה
נאשם שחור כפושע עתידי

00:11:30.425 --> 00:11:34.320
בשיעור כפול מאשר נאשם לבן.

00:11:35.891 --> 00:11:37.455
אז, בחנו את המקרה הבא:

00:11:38.103 --> 00:11:41.955
האישה הזו איחרה לאסוף
את אחותה החורגת

00:11:41.979 --> 00:11:44.054
מבית ספר במחוז ברווארד, בפלורידה,

00:11:44.757 --> 00:11:47.113
והיא רצה ברחוב עם חברתה.

00:11:47.137 --> 00:11:51.236
הן הבחינו באופני ילדים ובקורקינט
שהיו לא קשורים במרפסת

00:11:51.260 --> 00:11:52.892
ובטיפשות קפצו עליהם.

00:11:52.916 --> 00:11:55.515
בעוד הן דוהרות, 
אישה הגיחה ואמרה,

00:11:55.549 --> 00:11:57.754
" הי, אלו האופניים של הילד שלי!"

00:11:57.768 --> 00:12:01.062
הן זרקו אותם, הן הלכו משם,
אבל הן נעצרו.

00:12:01.086 --> 00:12:04.723
היא טעתה, היא עשתה שטות,
אבל היא גם הייתה רק בת 18.

00:12:04.747 --> 00:12:07.291
היו לה שתי עברות נעורים קודמות.

00:12:07.808 --> 00:12:12.993
בינתיים, האיש הזה נעצר על גניבה מחנות
בהום דיפו -

00:12:13.017 --> 00:12:15.941
דברים בשווי של 85 דולר,
פשע חסר חשיבות דומה.

00:12:16.766 --> 00:12:21.325
אבל היו לו שתי הרשעות קודמות
על שוד מזויין.

00:12:21.955 --> 00:12:25.437
אבל האלגוריתם חישב אותה בסיכון גבוה,
ולא אותו.

00:12:26.746 --> 00:12:30.620
שנתיים לאחר מכן, פרו-פבליקה מצאה
שהיא לא פשעה שוב.

00:12:30.644 --> 00:12:33.194
רק היה לה קשה למצוא עבודה
עם העבר שלה.

00:12:33.218 --> 00:12:35.294
הוא, לעומת זאת, כן פשע שוב

00:12:35.318 --> 00:12:39.154
וכעת הוא מרצה עונש של שמונה שנות מאסר
בגלל פשע מאוחר יותר.

00:12:40.088 --> 00:12:43.457
בברור, עלינו לבקר 
את הקופסאות השחורות שלנו

00:12:43.481 --> 00:12:46.096
ולא לאפשר להן סוג כזה של כוח בלתי בדוק.

00:12:46.120 --> 00:12:48.999
(מחיאות כפיים)

00:12:50.087 --> 00:12:54.329
ביקורות הן נהדרות וחשובות
אך הן לא פותרות את כל בעיותינו.

00:12:54.353 --> 00:12:57.101
קחו למשל את האלגוריתם החזק
של הפיד החדשותי של פייסבוק -

00:12:57.125 --> 00:13:01.968
אתם יודעים, זה שמדרג כל דבר 
ומחליט מה להראות לכם

00:13:01.992 --> 00:13:04.276
מכל החברים והדפים שאחריהם אתם עוקבים.

00:13:04.898 --> 00:13:07.173
האם צריך להראות לכם 
תמונה אחרת של תינוק?

00:13:07.197 --> 00:13:08.393
(צחוק)

00:13:08.417 --> 00:13:11.013
הערה זועפת מאיזה מכר?

00:13:11.449 --> 00:13:13.305
פריט חדשותי חשוב אך קשה לצפייה?

00:13:13.329 --> 00:13:14.811
אין כאן תשובה נכונה.

00:13:14.835 --> 00:13:17.494
פייסבוק מְיַטֶב לצורך מעורבות באתר:

00:13:17.518 --> 00:13:18.933
לייקים, שיתופים, תגובות.

00:13:20.168 --> 00:13:22.864
באוגוסט 2014,

00:13:22.888 --> 00:13:25.550
פרצה מחאה בפרגוסון, מיזורי

00:13:25.574 --> 00:13:29.991
לאחר הריגתו של נער אפריקני-אמריקני
על-ידי שוטר לבן,

00:13:30.015 --> 00:13:31.585
בנסיבות חשודות.

00:13:31.974 --> 00:13:33.981
החדשות בנוגע למחאה הופיעו בגדול

00:13:34.005 --> 00:13:36.690
בדף הטוויטר שלי שהוא ללא סינון אלגוריתמי,

00:13:36.714 --> 00:13:38.664
אך לא הופיעו כלל בדף הפייסבוק שלי.

00:13:39.182 --> 00:13:40.916
האם היו אלה חבריי בפייסבוק?

00:13:40.940 --> 00:13:42.972
ניטרלתי את האלגוריתם של הפייסבוק,

00:13:43.472 --> 00:13:46.320
דבר שקשה לעשות כי פייסבוק רוצה שתהיו

00:13:46.344 --> 00:13:48.380
כל הזמן תחת שליטת האלגוריתם,

00:13:48.404 --> 00:13:50.642
וראיתי שחבריי דיברו על זה.

00:13:50.666 --> 00:13:53.175
רק שהאלגוריתם לא הראה לי את זה.

00:13:53.199 --> 00:13:56.241
בדקתי את זה ומצאתי
שזו הייתה בעיה נרחבת.

00:13:56.265 --> 00:14:00.078
הסיפור מפרגוסון לא היה ידידותי לאלגוריתם.

00:14:00.102 --> 00:14:01.273
הוא לא "אהוב".

00:14:01.297 --> 00:14:02.849
מי יסמן לזה "לייק"?

00:14:03.500 --> 00:14:05.706
אפילו לא קל להגיב עליו.

00:14:05.730 --> 00:14:07.101
ללא לייקים ותגובות,

00:14:07.125 --> 00:14:10.417
סביר שהאלגוריתם הראה אותו 
אפילו לפחות אנשים,

00:14:10.441 --> 00:14:11.983
אז לא זכינו לראות את זה.

00:14:12.946 --> 00:14:14.174
במקומו, באותו שבוע,

00:14:14.198 --> 00:14:16.496
האלגוריתם של פייסבוק הדגיש את זה,

00:14:16.520 --> 00:14:18.746
זהו אתגר דלי הקרח של ALS.

00:14:18.770 --> 00:14:22.512
מטרה טובה; שפוך דלי קרח,
תרום כסף, יופי.

00:14:22.536 --> 00:14:24.440
אבל הוא היה ידידותי ביותר לאלגוריתם.

00:14:25.219 --> 00:14:27.832
המכונה קיבלה את ההחלטה הזו עבורנו.

00:14:27.856 --> 00:14:31.353
שיחה חשובה מאד אך קשה

00:14:31.377 --> 00:14:32.932
אולי הייתה מושתקת,

00:14:32.956 --> 00:14:35.652
אם פייסבוק היה הערוץ היחיד.

00:14:36.117 --> 00:14:39.914
כעת, לבסוף, המערכות האלה 
גם יכולות לטעות

00:14:39.938 --> 00:14:42.674
בצורות שאינן דומות למערכות אנושיות.

00:14:42.698 --> 00:14:45.620
האם אתם זוכרים את ווטסון,
מערכת הבינה המלאכותית של IBM

00:14:45.644 --> 00:14:48.772
שטאטאה את הרצפה
עם מתחרה אנושי במשחק "סכנה"?

00:14:49.131 --> 00:14:50.559
הוא היה שחקן נהדר.

00:14:50.583 --> 00:14:54.152
אבל אז, במשימה האחרונה,
ווטסון נשאל את השאלה הבאה:

00:14:54.659 --> 00:14:57.591
"שמו של שדה התעופה הכי גדול שלו
נקרא ע"ש גיבור מלחמת העולם ה-2,

00:14:57.615 --> 00:14:59.867
השני הכי גדול 
על שם קרב במלחמת העולם ה-II."

00:14:59.891 --> 00:15:01.269
(מזמזת את הנעימה של השלב הסופי במשחק)

00:15:01.582 --> 00:15:02.764
שיקגו.

00:15:02.788 --> 00:15:04.158
שני האנשים ענו נכון.

00:15:04.697 --> 00:15:09.045
ווטסון, לעומת זאת, ענה "טורונטו" -

00:15:09.069 --> 00:15:10.887
בקטגוריה של ערים בארצות הברית!

00:15:11.596 --> 00:15:14.497
המערכת המרשימה גם עשתה טעות

00:15:14.521 --> 00:15:18.172
שבן אנוש לעולם לא היה עושה,
שתלמיד כיתה ב' לא היה עושה.

00:15:18.823 --> 00:15:21.932
הבינה המלאכותית שלנו יכולה להכשל

00:15:21.956 --> 00:15:25.056
בדרכים שאינן תואמות
לדפוסי טעויות אנושיות,

00:15:25.080 --> 00:15:28.030
בדרכים שאנו לא מצפים להן
ולא מוכנים להן.

00:15:28.054 --> 00:15:31.692
זה יהיה מחורבן לא לקבל עבודה
שאתה מוכשר אליה,

00:15:31.716 --> 00:15:35.443
אבל זה יעצבן פי שלוש
אם זה יהיה בגלל גלישת מחסנית

00:15:35.467 --> 00:15:36.899
באיזו פונקציה תכנותית.

00:15:36.923 --> 00:15:38.502
(צחוק)

00:15:38.526 --> 00:15:41.312
במאי 2010,

00:15:41.336 --> 00:15:45.380
התרסקות בזק בוול סטריט
שהתגברה עקב תגובה חוזרת ונשנית

00:15:45.404 --> 00:15:48.432
של אלגוריתם ה"מכירה" של וול סטריט

00:15:48.456 --> 00:15:52.640
מחקה ערך של טריליון דולר
ב- 36 דקות.

00:15:53.722 --> 00:15:55.909
אני אפילו לא רוצה לחשוב 
מה המשמעות של "טעות"

00:15:55.933 --> 00:15:59.522
בהקשר של נשק קטלני אוטונומי.

00:16:01.894 --> 00:16:05.684
אז כן, אנשים מאז ומתמיד עשו הטיות.

00:16:05.708 --> 00:16:07.884
מקבלי החלטות ושומרי הסף,

00:16:07.908 --> 00:16:11.401
בבתי משפט, בחדשות, במלחמה...

00:16:11.425 --> 00:16:14.463
הם עושים טעויות:
אבל זו בדיוק הטענה שלי.

00:16:14.487 --> 00:16:18.008
אנחנו לא יכולים לברוח
מהשאלות הקשות הללו.

00:16:18.596 --> 00:16:22.112
איננו יכולים להעביר 
את האחריות שלנו למכונות.

00:16:22.676 --> 00:16:26.884
(מחיאות כפים)

00:16:29.089 --> 00:16:33.536
הבינה המלאכותית לא נותנת לנו 
אישור להשתחרר מהאתיקה.

00:16:34.742 --> 00:16:38.123
מדען הנתונים פרד בנסון
מכנה זאת "שטיפה-מתמטית".

00:16:38.147 --> 00:16:39.536
אנחנו זקוקים לדבר ההפוך.

00:16:39.560 --> 00:16:44.948
עלינו לפתח אלגוריתם לחשדנות,
בחינה מדוקדקת וחקירה.

00:16:45.380 --> 00:16:48.578
עלינו להבטיח שיש לנו 
נטילת אחריות אלגוריתמית,

00:16:48.602 --> 00:16:51.047
ביקורות ושקיפות משמעותית.

00:16:51.380 --> 00:16:54.614
עלינו לקבל שהכנסת מתמטיקה ומחשוב

00:16:54.638 --> 00:16:57.608
אל עניינים אנושיים מסובכים, עמוסי ערך

00:16:57.632 --> 00:17:00.016
לא מביאה אוביקטיביות;

00:17:00.040 --> 00:17:03.673
אלא, המורכבות של עניינים אנושיים
חודרת אל האלגוריתמים.

00:17:04.148 --> 00:17:07.635
כן, אנו יכולים וצריכים להשתמש במחשוב

00:17:07.659 --> 00:17:09.673
כדי לעזור לנו לקבל החלטות טובות יותר.

00:17:09.697 --> 00:17:15.029
אך עלינו לקחת בעלות 
על האחריות המוסרית והשיפוטיות שלנו,

00:17:15.053 --> 00:17:17.871
ולהשתמש באלגוריתמים במסגרת הזו,

00:17:17.895 --> 00:17:22.830
לא כאמצעי להתפטר 
או להעביר את האחריות שלנו

00:17:22.854 --> 00:17:25.308
מאחד לשני כמו מאדם לאדם.

00:17:25.807 --> 00:17:28.416
הבינה המלאכותית כבר כאן.

00:17:28.440 --> 00:17:31.861
זה אומר שעלינו לשמור יותר מאי פעם

00:17:31.885 --> 00:17:34.032
על ערכים אנושיים ואתיקה אנושית.

00:17:34.056 --> 00:17:35.210
תודה רבה.

00:17:35.234 --> 00:17:40.254
(מחיאות כפיים)

