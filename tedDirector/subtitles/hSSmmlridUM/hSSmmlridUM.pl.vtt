WEBVTT
Kind: captions
Language: pl

00:00:00.000 --> 00:00:07.000
Tłumaczenie: Anna Kowalczyk
Korekta: Marta Rytwinska

00:00:12.650 --> 00:00:18.161
Zaczęłam pracować jako programistka
na pierwszym roku studiów,

00:00:18.351 --> 00:00:20.371
jeszcze jako nastolatka.

00:00:20.651 --> 00:00:24.161
Niedługo później zaczęłam pracować
dla firmy tworzącej oprogramowania.

00:00:24.565 --> 00:00:29.135
Pewnego razu jeden z managerów
podszedł do mnie i wyszeptał:

00:00:29.563 --> 00:00:32.833
"Czy on wie kiedy kłamię?".

00:00:33.330 --> 00:00:36.080
Byliśmy sami w pokoju.

00:00:36.368 --> 00:00:39.546
"Niby kto ma wiedzieć kiedy kłamiesz?

00:00:39.796 --> 00:00:41.546
I dlaczego szepczemy?"

00:00:42.326 --> 00:00:44.726
Manager wskazał na komputer.

00:00:45.088 --> 00:00:48.078
"Czy on wie kiedy kłamię?"

00:00:49.551 --> 00:00:53.601
Ten menadżer miał romans z recepcjonistką.

00:00:53.902 --> 00:00:54.932
(Śmiech)

00:00:55.247 --> 00:00:58.837
Byłam nastolatką,
więc odkrzyknęłam szeptem:

00:00:59.101 --> 00:01:02.861
"Tak, komputer wie kiedy kłamiesz!".

00:01:03.106 --> 00:01:04.476
(Śmiech)

00:01:04.898 --> 00:01:07.818
Wtedy się z tego śmiałam,
ale teraz można śmiać się ze mnie.

00:01:07.818 --> 00:01:11.596
Obecnie istnieją programy,
które potrafią ropoznawać

00:01:11.855 --> 00:01:16.125
stany emocjonalne i kłamstwo
na podstawie zdjęć twarzy.

00:01:16.617 --> 00:01:21.347
Spece od marketingu i organy rządowe
wyrażają zainteresowanie.

00:01:21.479 --> 00:01:26.629
Zostałam programistką, bo od dziecka
uwielbiałam matematykę i naukę.

00:01:26.807 --> 00:01:33.387
Dowiedziawszy się o broni jądrowej,
przejęłam się kwestią etyki w nauce.

00:01:33.580 --> 00:01:34.930
Byłam w kropce.

00:01:35.336 --> 00:01:41.006
Ze względu na sytuację rodzinną
musiałam jak najszybciej zacząć pracować.

00:01:41.494 --> 00:01:45.854
Pomyślałam, że wybiorę dziedzinę,
w której łatwo znajdę pracę

00:01:46.253 --> 00:01:49.893
i nie będę musiała myśleć
o zawiłych kwestiach etycznych.

00:01:50.637 --> 00:01:52.317
Wybrałam informatykę.

00:01:52.533 --> 00:01:53.633
(Śmiech)

00:01:53.925 --> 00:01:56.295
Ha, ha, ha. Można się ze mnie nabijać.

00:01:56.729 --> 00:02:01.649
Programiści tworzą platformy,
które kontrolują to,

00:02:01.916 --> 00:02:04.376
co każdego dnia widzą miliardy ludzi.

00:02:04.573 --> 00:02:08.783
Tworzą samochody, 
mogące zdecydować, kogo przejechać.

00:02:08.987 --> 00:02:14.477
Tworzą nawet broń mechaniczną,
która mogłaby być użyta w czasie wojny.

00:02:14.688 --> 00:02:17.778
To są od początku do końca
kwestie etyczne.

00:02:18.901 --> 00:02:21.211
Sztuczna inteligencja to rzeczywistość.

00:02:21.676 --> 00:02:24.916
Korzystamy z obliczeń komputerowych
do podejmowania różnych decyzji,

00:02:25.047 --> 00:02:26.801
również decyzji nowego rodzaju.

00:02:26.801 --> 00:02:29.731
Próbujemy z pomocą komputerów, 
uzyskać odpowiedzi na pytania,

00:02:29.731 --> 00:02:32.241
na które nie ma jednej dobrej odpowiedzi,

00:02:32.241 --> 00:02:35.340
które są subiektywne,
otwarte i wartościujące.

00:02:35.896 --> 00:02:38.636
Pytamy: "Kogo powinno się zatrudnić?

00:02:38.916 --> 00:02:42.046
Który i czyj post dana osoba
powinna zobaczyć?

00:02:42.250 --> 00:02:45.280
Który skazaniec prawdopodobnie
popełni kolejne przestępstwo?

00:02:45.280 --> 00:02:48.460
Jaka informacja lub film powinny być
zarekomendowane widowni?".

00:02:48.556 --> 00:02:52.886
Używamy komputerów od lat,
ale to są kwestie innego rodzaju.

00:02:53.097 --> 00:02:54.957
To jest historyczna zmiana,

00:02:55.292 --> 00:03:00.352
bo nie możemy powiązać obliczeń
z tak subiektywnymi kwestiami,

00:03:00.646 --> 00:03:03.351
w taki sam sposób gdy w grę wchodzi

00:03:03.461 --> 00:03:07.121
latanie samolotami,
budowa mostów i wyprawy na Księżyc.

00:03:07.421 --> 00:03:09.461
Czy samoloty są bezpieczniejsze?

00:03:09.811 --> 00:03:11.371
Czy mosty chwieją się i walą?

00:03:11.572 --> 00:03:17.852
Dla takich spraw mamy jasne standardy
i prawa natury, które wskazują drogę.

00:03:18.353 --> 00:03:25.273
Nie mamy takich standardów dla decyzji
w pogmatwanych ludzkich sprawach.

00:03:26.141 --> 00:03:29.511
Tymczasem, oprogramowanie
staje się coraz potężniejsze,

00:03:30.040 --> 00:03:33.700
ale również coraz mniej
przejrzyste i bardziej złożone.

00:03:33.936 --> 00:03:37.456
W ciągu ostatnich dziesięcioleci
skomplikowane algorytmy

00:03:37.575 --> 00:03:39.732
dokonały milowego kroku do przodu.

00:03:39.732 --> 00:03:41.712
Potrafią rozpoznawać twarze,

00:03:41.815 --> 00:03:43.795
odczytywać odręczne pismo,

00:03:43.958 --> 00:03:46.726
wykrywać kradzieże
na kartach kredytowych,

00:03:46.726 --> 00:03:49.056
blokować spam, tłumaczyć teksty,

00:03:49.256 --> 00:03:52.344
wykryć guz w obrazowaniu medycznym

00:03:52.344 --> 00:03:54.624
i pokonać człowieka w szachy.

00:03:55.320 --> 00:03:57.850
Większą część tego postępu
zawdzięczamy metodzie

00:03:57.850 --> 00:03:59.816
zwanej "samouczeniem maszyn".

00:04:00.290 --> 00:04:03.242
Samouczenie maszyn różni się
od klasycznego programowania,

00:04:03.242 --> 00:04:06.602
w którym wydaje się komputerowi
dokładną i szczegółową instrunkcję.

00:04:06.876 --> 00:04:11.065
Polega bardziej na wprowadzaniu
dużej ilości danych do systemu,

00:04:11.065 --> 00:04:14.785
również danych przypadkowych,
takich jakie generujemy w sieci.

00:04:14.974 --> 00:04:18.184
System uczy się,
przedzierając się przez te dane.

00:04:18.374 --> 00:04:21.467
Najistotniejszy jest fakt, że te systemy

00:04:21.657 --> 00:04:24.405
nie pracują zgodnie z logiką
jednoznacznych odpowiedzi.

00:04:24.405 --> 00:04:27.695
Nie dają prostej odpowiedzi,
tylko określają prawdopodobieństwo:

00:04:27.695 --> 00:04:30.932
"To jest być może bardziej podobne,
do tego czego szukasz".

00:04:31.932 --> 00:04:34.496
Plusem jest to, że ta metoda
ma bardzo duży potencjał.

00:04:34.496 --> 00:04:36.926
Szef systemów
sztucznej inteligencji Google

00:04:36.926 --> 00:04:39.881
nazwał tę metodę
"nieuzasadnioną skutecznością danych".

00:04:40.242 --> 00:04:44.192
Minusem jest to, że nie rozumiemy,
czego nauczył się system

00:04:44.348 --> 00:04:46.128
i to właśnie stanowi jego siłę.

00:04:46.327 --> 00:04:50.177
To nie przypomina
wydawania instrukcji komputerowi.

00:04:50.567 --> 00:04:54.802
Przypomina raczej trenowanie
mechanicznego szczeniaczka,

00:04:54.956 --> 00:04:57.946
którego nie rozumiemy
i nie potrafimy kontrolować.

00:04:58.449 --> 00:04:59.769
I to jest problem.

00:05:00.538 --> 00:05:03.758
Problem pojawia się,
gdy ta sztuczna inteligencja się myli.

00:05:03.914 --> 00:05:07.806
Problem pojawia się
również wtedy, gdy ma rację,

00:05:08.036 --> 00:05:11.336
ponieważ nie umiemy tego rozróżnić
w przypadku subiektywnej kwestii.

00:05:11.755 --> 00:05:14.105
Nie mamy pojęcia,
co ten system sobie myśli.

00:05:15.288 --> 00:05:19.568
Rozważmy algorytm odpowiedzialny
za rekrutację pracowników.

00:05:20.197 --> 00:05:24.167
Algorytm rekrutujący oparty
na metodzie samouczenia się maszyn.

00:05:25.017 --> 00:05:28.647
Taki system byłby oparty na danych
pracowników danej firmy

00:05:28.867 --> 00:05:33.785
i miałby zatrudniać ludzi o takim profilu
jak obecni najwydajniejsi pracownicy.

00:05:34.204 --> 00:05:35.534
Brzmi nieźle.

00:05:35.795 --> 00:05:39.323
Byłam raz na konferencji,
na której obecne były także osoby

00:05:39.453 --> 00:05:43.553
zajmujące się rekrutacją, managerowie
i dyrektorzy używający takich systemów.

00:05:43.553 --> 00:05:45.503
Wszyscy byli zachwyceni.

00:05:45.503 --> 00:05:48.762
Sądzili, że dzięki temu zatrudnianie
stanie się bardziej obiektywne,

00:05:48.762 --> 00:05:52.727
mniej tendencyjne i gwarantujące kobietom
i przedstawicielom mniejszości

00:05:52.727 --> 00:05:55.941
większe szanse zatrudnienia
wobec tendencyjnych decyzji managerów.

00:05:55.941 --> 00:05:59.146
Ludzkie decyzje dotyczące
zatrudnienia są stronnicze.

00:05:59.146 --> 00:06:04.056
W jednej z moich pierwszych prac,
moja bezpośrednia przełożona

00:06:04.179 --> 00:06:08.009
czasem przychodziła do mnie
bardzo wcześnie rano

00:06:08.130 --> 00:06:13.380
lub bardzo późnym popołudniem
i mówiła: "Chodźmy na lunch!".

00:06:13.759 --> 00:06:17.809
Byłam zaskoczona dziwną porą,

00:06:18.094 --> 00:06:22.344
ale byłam też spłukana,
więc zawsze się zgadzałam.

00:06:22.475 --> 00:06:25.135
Później zorientowałam się o co chodziło.

00:06:25.235 --> 00:06:28.775
Moi bezpośredni przełożeni
nie przyznali się swoim szefom,

00:06:28.954 --> 00:06:33.054
że do poważnej roboty
zatrudnili programistę-nastolatkę,

00:06:34.073 --> 00:06:36.643
noszącą jeansy i trampki do pracy.

00:06:36.700 --> 00:06:39.000
Robiłam dobrą robotę,
ale wyglądałam nie tak,

00:06:39.100 --> 00:06:41.600
byłam w nieodpowiednim wieku
i nieodpowiedniej płci.

00:06:41.600 --> 00:06:45.097
Popieram zatrudnianie 
bez uwzględniania płci czy rasy.

00:06:46.506 --> 00:06:49.866
Ale w przypadku tych systemów
to wszystko jest bardziej skomplikowane.

00:06:50.722 --> 00:06:54.482
Obecnie, te systemy mogą
wyciągnąć wiele wniosków

00:06:54.832 --> 00:06:58.502
na podstawie strzępów
elektronicznej informacji,

00:06:58.673 --> 00:07:01.093
nawet jeśli te informacje nie są jawne.

00:07:01.217 --> 00:07:04.537
Mogą wnioskować na temat
orientacji seksualnej,

00:07:05.076 --> 00:07:08.216
cech osobowości,
przekonań politycznych.

00:07:08.751 --> 00:07:12.131
Te przewidywania są niezwykle trafne.

00:07:13.309 --> 00:07:16.989
Pamiętajcie - te przewidywania dotyczą
spraw, których nawet nie ujawniliście.

00:07:17.359 --> 00:07:20.403
Moja przyjaciółka stworzyła system
tego rodzaju, aby przewidywać

00:07:20.403 --> 00:07:23.614
prawdopodobieństwo zapadnięcia na
depresję kliniczną lub poporodową

00:07:23.614 --> 00:07:26.624
na podstawie danych zaczerpniętych
z serwisów społecznościowych.

00:07:26.624 --> 00:07:28.074
Wyniki są imponujące.

00:07:28.074 --> 00:07:30.684
Jej system potrafi
przewidzieć prawdopodobieństwo

00:07:30.684 --> 00:07:35.204
wystąpienia depresji miesiące przed
wystąpieniem jakichkolwiek objawów.

00:07:35.511 --> 00:07:38.521
Miesiące przed.
Nie ma objawów, jest przewidywanie.

00:07:39.334 --> 00:07:43.004
Moja przyjaciółka ma nadzieję,
że system posłuży wczesnemu leczeniu.

00:07:43.309 --> 00:07:44.489
Wspaniale.

00:07:44.728 --> 00:07:47.238
Ale spójrzmy na to
w kontekście zatrudniania.

00:07:47.386 --> 00:07:53.456
Na tamtej konferencji podeszłam
do jednej wysokopostawionej managerki

00:07:53.567 --> 00:07:56.427
pracującej dla jednej z wielkich firm
i powiedziałam:

00:07:56.490 --> 00:08:01.850
"Co jeśli twój system
za twoimi plecami wyklucza osoby

00:08:02.062 --> 00:08:06.612
o większym prawdopodobieństwie
zachorowania na depresję?

00:08:06.797 --> 00:08:11.357
Nie mają depresji teraz,
ale być może w przyszłości.

00:08:11.618 --> 00:08:15.808
Co jeśli wyklucza kobiety, 
które prawdopodobnie za rok lub dwa

00:08:15.940 --> 00:08:18.530
mogą być w ciąży, ale teraz nie są?

00:08:19.405 --> 00:08:21.532
Co jeśli zatrudnia osoby agresywne,

00:08:21.532 --> 00:08:24.532
bo to odpowiada kulturze pracy 
w waszej firmie?".

00:08:24.532 --> 00:08:27.342
Nie można tego stwierdzić
patrząc na zestawienia pracowników

00:08:27.342 --> 00:08:29.515
pod względem płci;
te mogą się równoważyć.

00:08:29.515 --> 00:08:32.585
Ponieważ to jest uczenie maszynowe
a nie klasyczne programowanie,

00:08:32.585 --> 00:08:40.545
nie ma zmiennych podpisanych: "depresja"
"ciąża" lub "wysoki poziom agresji".

00:08:41.183 --> 00:08:44.896
Nie znasz kryteriów, na podstawie których
twój system dokonuje wyboru.

00:08:45.188 --> 00:08:47.598
Nie wiesz nawet gdzie zacząć ich szukać.

00:08:47.774 --> 00:08:49.774
To jest technologia czarnej skrzynki.

00:08:49.914 --> 00:08:52.334
Potrafi przewidywać,
ale nie rozumiesz jak działa.

00:08:52.497 --> 00:08:55.532
"Jakie masz zabezpieczenia,
żeby upewnić się,

00:08:55.532 --> 00:08:58.372
że twoja czarna skrzynka
nie robi czegoś podejrzanego?"

00:08:59.006 --> 00:09:04.036
Spojrzała na mnie jakbym
nadepnęła na ogon szczeniaczkowi.

00:09:04.160 --> 00:09:05.640
(Śmiech)

00:09:05.754 --> 00:09:12.474
Popatrzyła na mnie i powiedziała:
"Ani słowa więcej".

00:09:13.075 --> 00:09:15.515
Odwróciła się i odeszła.

00:09:15.515 --> 00:09:17.184
Uwaga - nie była niegrzeczna.

00:09:17.184 --> 00:09:18.994
Ewidentnie chciała powiedzieć:

00:09:18.994 --> 00:09:21.784
"Jeśli o czymś nie wiem,
to nie jest mój problem, odejdź".

00:09:21.784 --> 00:09:23.872
Na odchodne rzuciła mordercze spojrzenie.

00:09:25.071 --> 00:09:25.961
(Śmiech)

00:09:26.382 --> 00:09:30.642
Takie systemy mogą być mniej
stronnicze niż managerowie.

00:09:31.422 --> 00:09:34.352
Mogą być również opłacalne.

00:09:34.506 --> 00:09:39.006
Ale mogą też prowadzić do stopniowego
i niepostrzeżonego zamykania

00:09:39.119 --> 00:09:42.729
rynku pracy dla osób z większym
ryzykiem zachorowań na depresję.

00:09:43.129 --> 00:09:48.269
Czy takie społeczeństwo chcemy budować,
nie wiedząc nawet o tym, że to robimy,

00:09:48.269 --> 00:09:51.056
bo powierzamy proces decyzyjny systemom,

00:09:51.056 --> 00:09:53.226
których do końca nie rozumiemy?

00:09:53.320 --> 00:10:01.630
Kolejny problem: te systemy pracują
na danych opartych na ludzkich działaniach

00:10:02.192 --> 00:10:05.572
Mogą po prostu odzwierciedlać
nasze własne uprzedzenia,

00:10:05.687 --> 00:10:09.837
wzmacniać je i zwracać w postaci wyniku,

00:10:10.168 --> 00:10:15.598
który będziemy interpretować jako
obiektywne i neutralne obliczenia.

00:10:17.764 --> 00:10:20.964
Udowodniono, że wyszukiwarka Google

00:10:21.075 --> 00:10:27.675
rzadziej pokazuje kobietom
dobrze płatne oferty pracy.

00:10:27.941 --> 00:10:31.941
A wyszukiwanie afroamerykańskich nazwisk

00:10:32.071 --> 00:10:36.251
jest skorelowane z ogłoszeniami
sugerującymi kryminalną przeszłość,

00:10:36.412 --> 00:10:38.352
nawet jeśli takowej nie ma.

00:10:38.585 --> 00:10:41.685
Tego typu ukryte uprzedzenia
i algorytmy typu czarnej skrzynki,

00:10:41.685 --> 00:10:45.648
które czasem odkrywamy, a czasem nie,

00:10:45.889 --> 00:10:48.549
mogą diametralnie wpływać 
na ludzkie życie.

00:10:49.732 --> 00:10:56.072
Pewien pozwany został skazany na 6 lat
więzienia za ucieczkę przed policją.

00:10:56.640 --> 00:11:01.210
Algorytmy są coraz częściej stosowane
przy wydawaniu wyroków.

00:11:01.707 --> 00:11:04.717
Mężczyzna chciał wiedzieć,
jak ten wynik jest obliczany.

00:11:05.268 --> 00:11:07.697
Używano komercyjnej czarnej skrzynki.

00:11:07.697 --> 00:11:09.367
Firma, która ją sprzedaje,

00:11:09.367 --> 00:11:12.558
sprzeciwiła się zweryfikowaniu
algorytmu podczas otwartej rozprawy.

00:11:12.558 --> 00:11:17.238
Śledcza organizacja non-profit,
ProPublica, zweryfikowała ten algorytm

00:11:17.247 --> 00:11:20.485
na podstawie publicznie
dostępnych danych i dowiodła,

00:11:20.485 --> 00:11:23.453
że generowane przez niego wyniki
były stronnicze,

00:11:23.453 --> 00:11:26.503
a jego moc przewidywania niewielka,
niewiele lepsza od losowej,

00:11:26.503 --> 00:11:29.553
a algorytm bezzasadnie wskazywał
czarnoskórych oskarżonych

00:11:29.553 --> 00:11:34.347
jako możliwych przyszłych przestępców
dwukrotnie częściej niż białych.

00:11:35.175 --> 00:11:43.215
Kolejna sprawa: ta kobieta była spóźniona
po odbiór chrześnicy ze szkoły.

00:11:44.098 --> 00:11:48.108
Biegła ulicą w towarzystwie przyjaciela
i na jednym z ganków zauważyli

00:11:48.228 --> 00:11:52.173
niezapięty rowerek dziecięcy i hulajnogę,
na które bezmyślnie wskoczyli.

00:11:52.253 --> 00:11:56.523
Gdy odjeżdżali, z domu wybiegła kobieta
krzycząc: "To rowerek mojego dziecka!".

00:11:56.860 --> 00:12:00.543
Zostawili rower i hulajnogę i odeszli,
ale zostali aresztowani.

00:12:00.543 --> 00:12:04.103
Dziewczyna postąpiła głupio i źle,
ale miała tylko 18 lat.

00:12:04.321 --> 00:12:07.411
Miała już na koncie
kilka młodzieńczych wykroczeń.

00:12:08.976 --> 00:12:14.294
W tym samym czasie aresztowano mężczyznę
za kradzież towarów o wartości 85 dolarów,

00:12:14.294 --> 00:12:16.268
równie niewielkie wykroczenie.

00:12:16.268 --> 00:12:21.068
Ale on już wcześniej był
dwukrotnie skazany za napad z bronią.

00:12:21.627 --> 00:12:25.827
Algorytm przypisał jej większe ryzyko
ponownego popełnienia przestępstwa.

00:12:26.049 --> 00:12:29.805
Dwa lata później, ProPublica odkryła,
że kobieta nie popełniła więcej wykroczeń,

00:12:29.805 --> 00:12:33.065
miała tylko problemy ze znalezieniem
pracy z taką historią wykroczeń.

00:12:33.484 --> 00:12:36.404
Mężczyzna natomiast
odsiadywał ośmioletni wyrok

00:12:36.518 --> 00:12:39.548
za przestępstwo,
którego dopuścił się ponownie.

00:12:39.548 --> 00:12:43.533
Jest jasne, że musimy
dokonywać rewizji czarnych skrzynek

00:12:43.533 --> 00:12:46.373
i ograniczać ich niekontrolowaną władzę.

00:12:46.474 --> 00:12:49.224
(Brawa)

00:12:50.047 --> 00:12:53.437
Rewizje są dobre i potrzebne,
ale nie rozwiążą wszystkich problemów.

00:12:53.735 --> 00:12:57.987
Spójrzmy na algorytm Facebooka,
który nadaje wszystkiemu jakąś wagę

00:12:57.987 --> 00:13:02.107
i decyduje o tym, co powinniście zobaczyć

00:13:02.107 --> 00:13:04.677
spośród tego wszystkiego,
co publikują wasi znajomi.

00:13:04.677 --> 00:13:07.307
Czy trzeba zobaczyć
kolejne zdjęcie bobasa?

00:13:07.307 --> 00:13:08.257
(Śmiech)

00:13:08.257 --> 00:13:10.617
Ponury post jednego ze znajomych?

00:13:10.617 --> 00:13:12.907
Ważną i trudną informację?

00:13:12.907 --> 00:13:14.517
Nie ma jednej dobrej odpowiedzi.

00:13:14.517 --> 00:13:18.784
Facebook ocenia liczbę reakcji:
komentarze, udostępnienia, polubienia.

00:13:19.015 --> 00:13:23.547
W sierpniu 2014 roku w Missouri
wybuchł protest po śmierci

00:13:23.547 --> 00:13:29.935
afroamerykańskiego nastolatka
zabitego przez białego policjanta

00:13:29.935 --> 00:13:31.865
w niejasnych okolicznościach.

00:13:31.865 --> 00:13:35.635
Wiadomości na ten temat zalały
mój niefiltrowany profil na Tweeterze,

00:13:36.210 --> 00:13:38.650
ale nie było ich na moim Facebooku.

00:13:38.650 --> 00:13:41.092
Czyżby moi znajomi o tym nie rozmawiali?

00:13:41.092 --> 00:13:44.142
Zablokowałam algorytm Facebooka,
co jest trudne,

00:13:44.142 --> 00:13:47.612
bo Facebook dąży do tego,
żeby nas kontrolować

00:13:47.612 --> 00:13:50.672
i zobaczyłam, że przyjaciele
rozmawiali o tej sprawie.

00:13:50.672 --> 00:13:53.882
Po prostu ja tego nie widziałam,
bo Facebook mi tego nie pokazywał.

00:13:53.882 --> 00:13:56.392
Okazało się, że to rozległy problem.

00:13:56.522 --> 00:13:59.242
Ta historia nie była
wysoko oceniona przez algorytm.

00:13:59.642 --> 00:14:01.362
Niełatwo ją "polubić".

00:14:03.421 --> 00:14:05.581
Nie jest nawet łatwo ją skomentować.

00:14:05.581 --> 00:14:08.811
Bez lajków i komentarzy algorytm
prawdopodobnie pokazywał

00:14:08.811 --> 00:14:12.201
tę informację coraz mniejszej ilości osób
i dlatego jej nie widzieliśmy.

00:14:12.475 --> 00:14:17.835
Równocześnie algorytm Facebooka
wypromował Ice Bucket Challenge.

00:14:17.835 --> 00:14:22.205
Szlachetna sprawa: oblej się zimną wodą,
ofiaruj pieniądze, super.

00:14:22.404 --> 00:14:24.964
To było wysoko ocenione przez algorytm.

00:14:25.143 --> 00:14:27.553
Maszyna podjęła za nas decyzję.

00:14:27.884 --> 00:14:32.054
Bardzo ważna lecz trudna dyskusja
mogła zostać wyciszona,

00:14:32.455 --> 00:14:35.605
gdyby Facebook był jedynym medium.

00:14:35.753 --> 00:14:40.133
Te systemy mogą się też mylić w sposób,

00:14:40.304 --> 00:14:42.654
który nie przypomina ludzkich błędów.

00:14:42.883 --> 00:14:46.193
Pamiętacie Watsona,
system sztucznej inteligencji IBM,

00:14:46.467 --> 00:14:48.997
który zmiażdżył uczestników
konkursu wiedzy Jeopardy?

00:14:49.176 --> 00:14:50.536
To był świetny gracz.

00:14:50.676 --> 00:14:53.893
Ale w finałowym odcinku padło pytanie:

00:14:54.257 --> 00:14:57.255
"Największe lotnisko tego miasta
nosi nazwę bohatera

00:14:57.255 --> 00:15:00.595
drugiej co do wielkości bitwy
podczas II wojny światowej".

00:15:01.299 --> 00:15:02.359
Chicago.

00:15:02.359 --> 00:15:04.619
Dwóch uczestników-ludzi
odpowiedziało poprawnie.

00:15:04.619 --> 00:15:10.239
Watson odpowiedział "Toronto"
w kategorii dotyczącej miast w USA.

00:15:10.660 --> 00:15:14.200
Ten imponujący system zrobił błąd,

00:15:14.200 --> 00:15:17.600
którego żaden człowiek by nie popełnił,
nawet drugoklasista.

00:15:17.839 --> 00:15:21.609
Sztuczna inteligencja
może zawieść w sposób,

00:15:21.609 --> 00:15:24.939
który nie przypomina ludzkich pomyłek,

00:15:24.939 --> 00:15:28.179
którego się nie spodziewamy,
i na który nie będziemy gotowi.

00:15:28.179 --> 00:15:31.339
Byłoby głupio nie dostać pracy,
na którą się zasługuje,

00:15:31.339 --> 00:15:34.449
ale byłoby potrójnie beznadziejnie,
gdyby to się stało

00:15:34.449 --> 00:15:36.789
z powodu nadmiaru danych
w jakimś podprogramie.

00:15:36.789 --> 00:15:37.599
(Śmiech)

00:15:38.738 --> 00:15:42.548
W maju 2010 roku
krach giełdy na Wall Street

00:15:42.548 --> 00:15:47.838
wzmocniony sprzężeniem
zwrotnym algorytmu "sprzedaży"

00:15:47.838 --> 00:15:52.144
doprowadził do zniknięcia
trylionów dolarów w 36 minut.

00:15:53.654 --> 00:15:57.494
Nie chcę nawet myśleć,
co taka "pomyłka" oznaczałaby

00:15:57.494 --> 00:16:00.974
w przypadku automatycznej
broni śmiercionośnej.

00:16:02.282 --> 00:16:05.002
Tak, ludzie są stronniczy.

00:16:05.474 --> 00:16:11.874
Ludzie podejmujący decyzje w sądach,
mediach, na wojnie... popełniają błędy.

00:16:11.880 --> 00:16:13.650
Ale o tym właśnie mówię.

00:16:14.430 --> 00:16:17.603
Nie możemy uciec od tych trudnych kwestii.

00:16:17.603 --> 00:16:21.593
Nie możemy zrzucić naszej
odpowiedzialności na maszyny.

00:16:23.128 --> 00:16:28.828
(Brawa)

00:16:29.002 --> 00:16:33.532
Sztuczna inteligencja
nie zwalnia nas z myślenia etycznego.

00:16:33.956 --> 00:16:37.683
Fred Benenson, specjalista od danych,
nazywał to "matematycznym praniem mózgu".

00:16:37.683 --> 00:16:39.324
Potrzebujemy czegoś odwrotnego.

00:16:39.324 --> 00:16:42.084
Musimy rozwijać w sobie
sceptycyzm wobec algorytmów

00:16:42.084 --> 00:16:45.424
i daleko posuniętą ostrożność
oraz dokładność w ich badaniu.

00:16:45.424 --> 00:16:48.386
Musimy mieć pewność, że mamy
algorytmiczną odpowiedzialność,

00:16:48.386 --> 00:16:50.886
rewizję i sensowną przejrzystość.

00:16:51.061 --> 00:16:54.320
Musimy zaakceptować, 
że wprowadzenie matematyki i obliczeń

00:16:54.320 --> 00:16:57.770
do zawikłanych i wymagających
wartościowania ludzkich spraw

00:16:57.770 --> 00:17:00.150
nie wprowadza obiektywizacji.

00:17:00.150 --> 00:17:02.360
Przeciwnie: komplikacja ludzkich spraw

00:17:02.360 --> 00:17:04.440
najedzie algorytmy.

00:17:04.440 --> 00:17:07.200
Można i trzeba korzystać z obliczeń,

00:17:07.200 --> 00:17:09.370
aby móc podejmować lepsze decyzje.

00:17:09.473 --> 00:17:12.973
Ale trzeba uznać moralną odpowiedzialność

00:17:12.973 --> 00:17:18.098
za podejmowanie decyzji
i używać algorytmów w jej granicach,

00:17:18.098 --> 00:17:24.672
a nie jako środek do zrzeczenia się
i odrzucenia tej odpowiedzialności.

00:17:25.839 --> 00:17:29.059
Sztuczna inteligencja jest faktem.

00:17:29.059 --> 00:17:33.506
To oznacza, że tym mocniej musimy
trzymać się ludzkich wartości i etyki.

00:17:33.506 --> 00:17:34.648
Dziękuję

00:17:34.648 --> 00:17:39.288
(Brawa)

