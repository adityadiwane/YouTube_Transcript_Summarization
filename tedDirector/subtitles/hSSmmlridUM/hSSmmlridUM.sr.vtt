WEBVTT
Kind: captions
Language: sr

00:00:00.000 --> 00:00:07.000
Prevodilac: Milenka Okuka
Lektor: Mile Živković

00:00:12.739 --> 00:00:16.861
Dakle, počela sam da radim
kao kompjuterska programerka

00:00:16.885 --> 00:00:18.841
kad sam bila prva godina na fakultetu -

00:00:18.865 --> 00:00:20.372
u suštini kao tinejdžerka.

00:00:20.889 --> 00:00:22.621
Ubrzo nakon što sam počela da radim,

00:00:22.645 --> 00:00:24.255
da pišem softvere za firmu,

00:00:24.799 --> 00:00:28.434
menadžer koji je radio u firmi
se spustio do mene

00:00:28.458 --> 00:00:29.726
i prošaputao mi je:

00:00:30.229 --> 00:00:33.090
"Zna li da lažem?"

00:00:33.806 --> 00:00:35.883
Nije bilo više bilo koga u prostoriji.

00:00:37.032 --> 00:00:41.421
"Zna li ko da lažete? I zašto šapućemo?"

00:00:42.266 --> 00:00:45.373
Menadžer je pokazao
na kompjuter u prostoriji.

00:00:45.397 --> 00:00:48.493
"Zna li da lažem?"

00:00:49.613 --> 00:00:53.975
Pa, taj menadžer je imao aferu
sa recepcionerom.

00:00:53.999 --> 00:00:55.111
(Smeh)

00:00:55.135 --> 00:00:56.901
A ja sam još uvek bila tinejdžerka.

00:00:57.447 --> 00:00:59.466
Pa sam mu polušapatom odgovorila:

00:00:59.490 --> 00:01:03.114
"Da, kompjuter zna kad mu lažete."

00:01:03.138 --> 00:01:04.944
(Smeh)

00:01:04.968 --> 00:01:07.891
Pa, smejala sam se,
ali zapravo šala je na moj račun.

00:01:07.915 --> 00:01:11.183
Ovih dana imamo kompjuterske sisteme

00:01:11.207 --> 00:01:14.755
koji mogu da prozru
emocionalna stanja, čak i laganje,

00:01:14.779 --> 00:01:16.823
obradom ljudskih lica.

00:01:17.248 --> 00:01:21.401
Oglašivači, čak i vlade
su veoma zainteresovane za to.

00:01:22.319 --> 00:01:24.181
Postala sam kompjuterska programerka

00:01:24.205 --> 00:01:27.318
jer sam bila jedno od dece
koja su luda za matematikom i naukom.

00:01:27.942 --> 00:01:31.050
No, nekako sam usput
saznala za nuklearno oružje

00:01:31.074 --> 00:01:34.026
i postala sam zaista zabrinuta
zbog naučne etike.

00:01:34.050 --> 00:01:35.254
Mučilo me je to.

00:01:35.278 --> 00:01:37.919
Međutim, zbog porodičnih okolnosti,

00:01:37.943 --> 00:01:41.241
takođe je bilo potrebno
da počnem da radim što pre.

00:01:41.265 --> 00:01:44.564
Pa sam pomislila u sebi, hej,
hajde da izaberem tehničku oblast

00:01:44.588 --> 00:01:46.384
gde mogu lako da se zaposlim

00:01:46.408 --> 00:01:50.426
i gde ne moram da se bavim
bilo kakvim mučnim etičkim pitanjem.

00:01:51.022 --> 00:01:52.551
Pa sam odabrala kompjutere.

00:01:52.575 --> 00:01:53.679
(Smeh)

00:01:53.703 --> 00:01:57.113
Pa, ha, ha, ha!
Šala je skroz na moj račun.

00:01:57.137 --> 00:01:59.891
Ovih dana kompjuterski naučnici
prave platforme

00:01:59.915 --> 00:02:04.124
koje upravljaju onim
što milijarde ljudi gledaju svakodnevno.

00:02:05.052 --> 00:02:08.874
Razvijaju automobile
koji mogu da odluče koga da pregaze.

00:02:09.707 --> 00:02:12.920
Čak grade mašine, oružja,

00:02:12.944 --> 00:02:15.229
koja mogu da ubijaju ljude u ratu.

00:02:15.253 --> 00:02:18.024
U potpunosti se radi o etici.

00:02:19.183 --> 00:02:21.241
Mašinska inteligencija je tu.

00:02:21.823 --> 00:02:25.297
Trenutno koristimo kompjutere
da donesemo razne odluke,

00:02:25.321 --> 00:02:27.207
ali i nove tipove odluka.

00:02:27.231 --> 00:02:32.403
Postavljamo kompjuterima pitanja
koja nemaju jedan pravi odgovor,

00:02:32.427 --> 00:02:33.629
koja su subjektivna,

00:02:33.653 --> 00:02:35.978
otvorena i krcata vrednostima.

00:02:36.002 --> 00:02:37.760
Postavljamo ovakva pitanja:

00:02:37.784 --> 00:02:39.434
"Koga da firma zaposli?"

00:02:40.096 --> 00:02:42.855
"Koje ažuriranje od kog prijatelja
treba da bude vidljivo?"

00:02:42.879 --> 00:02:45.145
"Koji osuđenik je skloniji
novom prestupu?"

00:02:45.514 --> 00:02:48.568
"Koji novinski čalanak ili film
treba preporučiti ljudima?"

00:02:48.592 --> 00:02:51.964
Gledajte, da, već neko vreme
koristimo kompjutere,

00:02:51.988 --> 00:02:53.505
ali ovo je drugačije.

00:02:53.529 --> 00:02:55.596
Ovo je istorijski preokret

00:02:55.620 --> 00:03:00.957
jer ne možemo da usidrimo kompjutere
kod sličnih subjektivnih odluka

00:03:00.981 --> 00:03:06.401
na način na koji usidravamo kompjutere
koji upravljaju avionima, grade mostove,

00:03:06.425 --> 00:03:07.684
idu na mesec.

00:03:08.449 --> 00:03:11.708
Jesu li avioni bezbedniji?
Da li se most zaljuljao i pao?

00:03:11.732 --> 00:03:16.230
Tu imamo uspostavljene
prilično jasne repere

00:03:16.254 --> 00:03:18.493
i tu su zakoni prirode da nas vode.

00:03:18.517 --> 00:03:21.911
Nemamo slična težišta i repere

00:03:21.935 --> 00:03:25.898
za odluke koje se tiču
haotičnih ljudskih odnosa.

00:03:25.922 --> 00:03:30.159
Da bi stvari bile još složenije,
naši softveri postaju sve moćniji,

00:03:30.183 --> 00:03:33.956
ali takođe postaju manje
transparentni i složeniji.

00:03:34.542 --> 00:03:36.582
U skorije vreme, u poslednjoj deceniji,

00:03:36.606 --> 00:03:39.335
složeni algoritmi
su poprilično napredovali.

00:03:39.359 --> 00:03:41.349
Mogu da prepoznaju ljudska lica.

00:03:41.985 --> 00:03:44.040
Mogu da dešifruju rukopis.

00:03:44.436 --> 00:03:46.502
Mogu da zapaze prevaru
kod kreditnih kartica

00:03:46.526 --> 00:03:47.715
i da blokiraju spamove

00:03:47.739 --> 00:03:49.776
i da prevode s jezika na jezik.

00:03:49.800 --> 00:03:52.374
Mogu da zapaze tumore
na medicinskim snimcima.

00:03:52.398 --> 00:03:54.603
Mogu da pobede ljude u šahu i gou.

00:03:55.264 --> 00:03:59.768
Veliki deo ovog napretka potiče
od metoda nazvanog "mašinsko učenje".

00:04:00.175 --> 00:04:03.362
Mašinsko učenje se razlikuje
od tradicionalnog programiranja,

00:04:03.386 --> 00:04:06.971
gde dajete kompjuteru
detaljne, tačne, minuciozne instrukcije.

00:04:07.378 --> 00:04:11.494
Pre se radi o odabiru sistema
i pohranjivanju podataka u njega,

00:04:11.494 --> 00:04:13.240
uključujući nestrukturirane podatke,

00:04:13.264 --> 00:04:15.542
poput onih koje stvaramo
u digitalnim životima.

00:04:15.566 --> 00:04:18.296
A sistem uči, pretresajući podatke.

00:04:18.669 --> 00:04:20.195
Suštinsko je takođe

00:04:20.219 --> 00:04:24.599
da se ovi sistemi ne vode
logikom samo jednog odgovora.

00:04:24.623 --> 00:04:27.582
Ne proizvode jednostavne odgovore;
više se radi o verovatnoći:

00:04:27.606 --> 00:04:31.089
"Ovo je verovatno sličnije
onome što tražite."

00:04:32.023 --> 00:04:35.087
Sad, pozitivno je:
ovaj metod je zaista moćan.

00:04:35.087 --> 00:04:37.193
Glavni u Guglovom sistemu
za VI je to nazvao:

00:04:37.217 --> 00:04:39.414
"nerazumna efikasnost podataka."

00:04:39.791 --> 00:04:41.144
Negativno je:

00:04:41.738 --> 00:04:44.809
ne razumemo zaista šta je sistem naučio.

00:04:44.833 --> 00:04:46.420
Zapravo, to je njegova moć.

00:04:46.946 --> 00:04:50.744
Ovo manje liči na davanje
uputstava kompjuteru;

00:04:51.200 --> 00:04:55.264
više liči na dresiranje
bića - mehaničko kuče,

00:04:55.288 --> 00:04:57.659
koje zaista ne razumemo,
niti kontrolišemo.

00:04:58.362 --> 00:04:59.913
Dakle, to je naš problem.

00:05:00.427 --> 00:05:04.689
Problem je kad ovaj sistem veštačke
inteligencije nešto pogrešno shvati.

00:05:04.713 --> 00:05:08.253
Takođe je problem kad nešto dobro shvati.

00:05:08.277 --> 00:05:11.905
jer čak ni ne znamo šta je šta
kod subjektivnog problema.

00:05:11.929 --> 00:05:14.268
Ne znamo o čemu ova stvar razmišlja.

00:05:15.493 --> 00:05:19.176
Dakle, uzmite u obzir
algoritam za zapošljavanje -

00:05:20.123 --> 00:05:24.434
sistem koji se koristi pri zapošljavanju,
koji koristi sisteme mašinskog učenja.

00:05:25.052 --> 00:05:28.631
Sličan sistem je obučavan
na podacima prethodnih zaposlenih

00:05:28.655 --> 00:05:31.246
i naučen je da pronalazi i zapošljava

00:05:31.270 --> 00:05:34.308
ljude poput postojećih
najučinkovitijih u firmi.

00:05:34.814 --> 00:05:35.967
Zvuči dobro.

00:05:35.991 --> 00:05:37.990
Jednom sam bila na konferenciji

00:05:38.014 --> 00:05:41.123
koja je spojila menadžere
iz kadrovske službe i direktore,

00:05:41.123 --> 00:05:42.353
ljude s visokih pozicija,

00:05:42.353 --> 00:05:44.072
koristeći ove sisteme zapošljavanja.

00:05:44.072 --> 00:05:45.622
Bili su veoma uzbuđeni.

00:05:45.646 --> 00:05:50.299
Smatrali su da bi zbog ovoga zapošljavanje
bilo objektivnije, nepristrasnije,

00:05:50.323 --> 00:05:53.323
i da bi žene i manjine imale više šanse,

00:05:53.347 --> 00:05:55.535
nasuprot pristrasnim ljudskim menadžerima.

00:05:55.559 --> 00:05:58.402
I, gledajte -
zapošljavanje ljudi je pristrasno.

00:05:59.099 --> 00:06:00.284
Znam.

00:06:00.308 --> 00:06:03.313
Mislim, na jednom od mojih
prvih poslova kao programerke,

00:06:03.337 --> 00:06:07.205
moja nadređena menadžerka bi ponekad
prišla mestu na kom sam,

00:06:07.229 --> 00:06:10.982
veoma rano ujutru
ili veoma kasno poslepodne,

00:06:11.006 --> 00:06:14.068
i rekla bi: "Zejnep, pođimo na ručak!"

00:06:14.724 --> 00:06:16.891
Zbunilo bi me neobično vreme.

00:06:16.915 --> 00:06:19.044
Četiri je popodne. Ručak?

00:06:19.068 --> 00:06:22.162
Bila sam švorc, pa sam uvek išla
na besplatan ručak.

00:06:22.618 --> 00:06:24.685
Kasnije sam shvatila o čemu se radilo.

00:06:24.709 --> 00:06:29.255
Moji nadređeni menadžeri
nisu priznali svojim nadređenim

00:06:29.259 --> 00:06:33.202
da je programer kog su zaposlili
za ozbiljan posao bila tinejdžerka

00:06:33.622 --> 00:06:36.426
koja je nosila farmerke i patike na posao.

00:06:37.174 --> 00:06:39.376
Bila sam dobar radnik,
samo pogrešnog izgleda

00:06:39.400 --> 00:06:41.099
i bila sam pogrešnih godina i roda.

00:06:41.123 --> 00:06:44.469
Pa zapošljavanje
na rodno i rasno nepristrasan način

00:06:44.493 --> 00:06:46.358
izvesno da mi zvuči dobro.

00:06:47.031 --> 00:06:50.372
Ali uz ove sisteme,
složenije je, a evo zašto:

00:06:50.968 --> 00:06:56.759
trenutno kompjuterski sistemi
mogu da zaključe razne stvari o vama

00:06:56.783 --> 00:06:58.655
iz vaših digitalnih tragova,

00:06:58.679 --> 00:07:01.012
čak iako to niste obelodanili.

00:07:01.506 --> 00:07:04.433
Mogu da zaključe vašu
seksualnu orijentaciju,

00:07:04.994 --> 00:07:06.300
vaše lične osobine,

00:07:06.859 --> 00:07:08.232
vaša politička naginjanja.

00:07:08.830 --> 00:07:12.515
Imaju moć predviđanja
sa visokim stepenom tačnosti.

00:07:13.362 --> 00:07:15.940
Zapamtite - za ono što čak
niste ni obelodanili.

00:07:15.964 --> 00:07:17.555
To je zaključivanje.

00:07:17.579 --> 00:07:20.840
Imam prijateljicu koja je razvila
sličan kompjuterski sistem

00:07:20.864 --> 00:07:24.505
za predviđanje verovatnoće
kliničke ili postporođajne depresije

00:07:24.529 --> 00:07:26.075
iz podataka sa društvenih mreža.

00:07:26.676 --> 00:07:28.103
Rezultati su bili impresivni.

00:07:28.492 --> 00:07:31.849
Njeni sistemi mogu da predvide
verovatnoću depresije

00:07:31.873 --> 00:07:35.776
mesecima pre nastupa
bilo kakvih simptoma -

00:07:35.800 --> 00:07:37.173
mesecima ranije.

00:07:37.197 --> 00:07:39.443
Bez simptoma imamo predviđanje.

00:07:39.467 --> 00:07:44.279
Ona se nada da će biti korišćeni
za rane intervencije. Sjajno!

00:07:44.911 --> 00:07:46.951
Sad ovo stavite u kontekst zapošljavanja.

00:07:48.027 --> 00:07:51.073
Pa sam na ovoj konferenciji
menadžera iz kadrovske

00:07:51.097 --> 00:07:55.806
prišla visokoprofilnoj menadžerki
iz prilično velike firme,

00:07:55.830 --> 00:08:00.408
i rekla sam joj: "Pazi, šta ako bi,
bez tvog znanja,

00:08:00.432 --> 00:08:06.981
ovaj sistem iskorenjivao ljude sa velikim
izgledima za depresiju u budućnosti?

00:08:07.761 --> 00:08:11.237
Trenutno nisu depresivni, ali je veća
verovatnoća da će biti u budućnosti.

00:08:11.923 --> 00:08:15.329
Šta ako iskorenjuje žene
s većom verovatnoćom da zatrudne

00:08:15.353 --> 00:08:17.939
u narednih godinu ili dve,
ali trenutno nisu trudne?

00:08:18.844 --> 00:08:24.480
Šta ako zapošljava agresivne ljude
jer je to kultura na vašem radnom mestu?"

00:08:25.173 --> 00:08:27.864
Ovo ne možete da vidite,
posmatrajući rodnu nejednakost.

00:08:27.888 --> 00:08:29.460
Ona bi mogla da bude u ravnoteži.

00:08:29.460 --> 00:08:32.971
A kako se radi o mašinskom učenju,
a ne tradicionalnom programiranju,

00:08:32.995 --> 00:08:37.902
tu nemamo varijablu
s oznakom "veći rizik od depresije",

00:08:37.926 --> 00:08:39.759
"veći rizik za trudnoću",

00:08:39.783 --> 00:08:41.517
"skala agresivnih muškaraca".

00:08:41.995 --> 00:08:45.674
Ne samo da ne znate
na osnovu čega vaš sistem bira,

00:08:45.698 --> 00:08:48.021
čak ne znate ni gde da gledate.

00:08:48.045 --> 00:08:49.291
To je crna kutija.

00:08:49.315 --> 00:08:52.122
Ima moć predviđanja,
ali je vi ne razumete.

00:08:52.486 --> 00:08:54.855
"Koja vam je zaštita",
pitala sam, "koju imate

00:08:54.879 --> 00:08:58.552
kojom se starate da crna kutija
ne obavlja nešto sumnjivo?"

00:09:00.863 --> 00:09:04.741
Pogledala me je kao da sam
nagazila na 10 kučećih repića.

00:09:04.765 --> 00:09:06.013
(Smeh)

00:09:06.037 --> 00:09:08.078
Buljila je u mene i rekla:

00:09:08.556 --> 00:09:12.889
"Ne želim da čujem ni reč više o ovome."

00:09:13.458 --> 00:09:15.492
Okrenula se i otišla.

00:09:16.064 --> 00:09:17.550
Pazite - nije bila nepristojna.

00:09:17.574 --> 00:09:23.882
Jasno se radilo o ovome: ono što ne znam
nije moj problem, nestani, prazan pogeld.

00:09:23.906 --> 00:09:25.152
(Smeh)

00:09:25.862 --> 00:09:29.701
Vidite, sličan sistem bi mogao
čak da bude na neki način

00:09:29.725 --> 00:09:31.828
i manje pristrasan od ljudskih menadžera.

00:09:31.852 --> 00:09:33.998
I mogao bi da ima finansijsku prednost.

00:09:34.573 --> 00:09:36.223
Ali bi takođe mogao da dovede

00:09:36.247 --> 00:09:40.995
do stabilnog, ali prikrivenog
isključivanja sa tržišta rada

00:09:41.019 --> 00:09:43.312
ljudi s većim rizikom od depresije.

00:09:43.753 --> 00:09:46.349
Da li je ovo oblik društva
koji želimo da gradimo,

00:09:46.373 --> 00:09:48.658
a da pri tom ne znamo da smo to uradili

00:09:48.682 --> 00:09:52.646
jer smo prepustili donošenje odluka
mašinama koje u potpunosti ne razumemo?

00:09:53.265 --> 00:09:54.723
Drugi problem je sledeće:

00:09:55.314 --> 00:09:59.766
ovi sistemi su često obučavani
na podacima koje proizvode naša delanja,

00:09:59.790 --> 00:10:01.606
na ljudskom otisku.

00:10:02.188 --> 00:10:05.996
Pa, oni bi prosto mogli
da odražavaju naše predrasude,

00:10:06.020 --> 00:10:09.613
i ovi sistemi bi mogli
da pokupe naše predrasude

00:10:09.637 --> 00:10:10.950
i da ih naglase

00:10:10.974 --> 00:10:12.392
i potom da nam ih pokažu,

00:10:12.416 --> 00:10:13.878
dok mi govorimo sebi:

00:10:13.902 --> 00:10:17.019
"Samo izvodimo objektivne,
neutralne proračune."

00:10:18.314 --> 00:10:20.991
Istraživači su otkrili da na Guglu

00:10:22.134 --> 00:10:27.447
ženama mnogo ređe nego muškarcima
prikazuju oglase za dobro plaćene poslove.

00:10:28.463 --> 00:10:30.993
A pretraga afroameričkih imena

00:10:31.017 --> 00:10:35.723
često sa sobom povlači oglase
koji nagoveštavaju kriminalnu prošlost,

00:10:35.747 --> 00:10:37.314
čak i kad ona ne postoji.

00:10:38.693 --> 00:10:42.242
Slične prikrivene predrasude
i algoritmi nalik crnoj kutiji,

00:10:42.266 --> 00:10:46.239
koje istraživači povremeno otkrivaju,
ali ponekad to ne uspeju,

00:10:46.263 --> 00:10:48.924
mogu da imaju ozbiljne posledice.

00:10:49.958 --> 00:10:54.117
Okrivljeni iz Viskonsina
je osuđen na šest godina zatvora

00:10:54.141 --> 00:10:55.496
zbog izbegavanja policije.

00:10:56.824 --> 00:10:58.010
Možda ne znate za ovo,

00:10:58.034 --> 00:11:02.032
ali algoritme sve češće koriste
u odlučivanju o uslovnoj ili kazni.

00:11:02.056 --> 00:11:05.011
Želeo je da zna:
kako su izračunali ovaj rezultat?

00:11:05.795 --> 00:11:07.460
To je komercijalna crna kutija.

00:11:07.484 --> 00:11:11.689
Firma je odbila da njen algoritam
izazovu na javnom suđenju.

00:11:12.396 --> 00:11:17.928
No, ProPublica, istraživačka neprofitna
organizacija je proverila taj algoritam

00:11:17.952 --> 00:11:19.968
sa podacima koje su uspeli da nađu

00:11:19.992 --> 00:11:22.308
i otkrili su da su njihovi
rezultati pristrasni,

00:11:22.332 --> 00:11:25.961
a da je njihova moć predviđanja očajna,
jedva bolja od nagađanja

00:11:25.985 --> 00:11:30.401
i da su pogrešno označavali
okrivljene crnce kao buduće kriminalce,

00:11:30.425 --> 00:11:34.320
dvostruko češće nego okrivljene belce.

00:11:35.891 --> 00:11:37.455
Pa, razmotrite ovaj slučaj:

00:11:38.103 --> 00:11:41.955
ova žena je kasnila da pokupi svoje kumče

00:11:41.979 --> 00:11:44.054
iz okruga Brauard u Floridi,

00:11:44.757 --> 00:11:47.113
trčala je niz ulicu
sa svojom prijateljicom.

00:11:47.137 --> 00:11:51.236
Spazile su nezaključan dečji bicikl
i skuter na tremu

00:11:51.260 --> 00:11:52.892
i nesmotreno su sele na bicikl.

00:11:52.916 --> 00:11:55.515
Dok su jurile dalje,
žena je izašla i rekla:

00:11:55.539 --> 00:11:57.744
"Hej! To je bicikl mog deteta!"

00:11:57.768 --> 00:12:01.062
Ostavile su ga, odšetale, ali su uhapšene.

00:12:01.086 --> 00:12:04.723
Pogrešila je, bila je nesmotrena,
ali je takođe imala svega 18 godina.

00:12:04.747 --> 00:12:07.291
Imala je nekoliko maloletničkih prekršaja.

00:12:07.808 --> 00:12:12.993
U međuvremenu, ovaj čovek je uhapšen
zbog krađe u supermarketu -

00:12:13.017 --> 00:12:15.941
robe u vrednosti od 85 dolara,
sličan manji zločin.

00:12:16.766 --> 00:12:21.325
Ali je pre toga imao
dve osude zbog oružane pljačke.

00:12:21.955 --> 00:12:25.437
Ali je algoritam nju ocenio
kao visokorizičnu, a njega nije.

00:12:26.746 --> 00:12:30.620
Dve godine kasnije, ProPublica je otkrila
da ona nije imala novih prekršaja.

00:12:30.644 --> 00:12:33.194
Samo joj je sa dosijeom
bilo teško da nađe posao.

00:12:33.218 --> 00:12:35.294
On, s druge strane, ponovo je u zatvoru

00:12:35.318 --> 00:12:39.154
i ovaj put služi kaznu od osam godina
zbog kasnijeg zločina.

00:12:40.088 --> 00:12:43.457
Očigledno, moramo da proveravamo
naše crne kutije

00:12:43.481 --> 00:12:46.096
kako ne bi imale
sličnu nekontrolisanu moć.

00:12:46.120 --> 00:12:48.999
(Aplauz)

00:12:50.087 --> 00:12:54.329
Provere su sjajne i važne,
ali ne rešavaju sve naše probleme.

00:12:54.353 --> 00:12:57.101
Uzmite Fejsbukov moćan
algoritam za dostavu vesti -

00:12:57.125 --> 00:13:01.968
znate, onaj koji sve rangira
i odlučuje šta da vam pokaže

00:13:01.992 --> 00:13:04.276
od svih prijatelja
i stranica koje pratite.

00:13:04.898 --> 00:13:07.173
Da li da vam pokaže još jednu sliku bebe?

00:13:07.197 --> 00:13:08.393
(Smeh)

00:13:08.417 --> 00:13:11.013
Sumornu poruku od poznanika?

00:13:11.449 --> 00:13:13.305
Važnu, ali tešku vest?

00:13:13.329 --> 00:13:14.811
Nema pravog odgovora.

00:13:14.835 --> 00:13:17.494
Fejsbuk najviše ima koristi
od angažmana na sajtu:

00:13:17.518 --> 00:13:19.013
sviđanja, deljenja, komentara.

00:13:20.168 --> 00:13:22.864
Avgusta 2014,

00:13:22.888 --> 00:13:25.550
izbili su protesti u Fergusonu, Misuri,

00:13:25.574 --> 00:13:29.991
nakon ubistva afroameričkog tinejdžera
od strane policajca belca,

00:13:30.015 --> 00:13:31.585
pod nejasnim okolnostima.

00:13:31.974 --> 00:13:33.981
Vesti o protestima su bile svuda

00:13:34.005 --> 00:13:36.690
po mom algoritamski nefilterisanom
Tviter nalogu,

00:13:36.714 --> 00:13:38.664
ali nigde na mom Fejsbuku.

00:13:39.142 --> 00:13:40.956
Da li su krivi prijatelji na Fejsbuku?

00:13:40.956 --> 00:13:42.972
Onemogućila sam Fejsbukov algoritam,

00:13:43.472 --> 00:13:46.320
a to je teško jer Fejsbuk
nastoji da vas natera

00:13:46.344 --> 00:13:48.380
da budete pod kontrolom algoritma,

00:13:48.404 --> 00:13:50.642
i videla sam da moji prijatelji
govore o tome.

00:13:50.666 --> 00:13:53.175
Samo mi moj algoritam to nije pokazivao.

00:13:53.199 --> 00:13:56.241
Istražila sam ovo i otkrila
da je ovo raširen problem.

00:13:56.265 --> 00:14:00.072
Priča o Fergusonu
nije bila prihvatljiva za algoritam.

00:14:00.072 --> 00:14:01.273
Nije nešto za "sviđanje".

00:14:01.297 --> 00:14:02.849
Ko će da pritisne "sviđanje"?

00:14:03.500 --> 00:14:05.706
Nije je čak lako ni komentarisati.

00:14:05.730 --> 00:14:07.101
Bez sviđanja i komentara,

00:14:07.125 --> 00:14:10.417
algoritam je težio da je prikaže
čak i manjem broju ljudi,

00:14:10.441 --> 00:14:11.983
pa nismo mogli da je vidimo.

00:14:12.946 --> 00:14:14.174
Umesto toga, te sedmice,

00:14:14.198 --> 00:14:16.496
Fejsbukov algoritam je izdvojio ovo,

00:14:16.520 --> 00:14:18.746
a to je ALS ledeni izazov.

00:14:18.770 --> 00:14:22.512
Plemenit cilj; polij se ledenom vodom,
doniraj u dobrotvorne svrhe, fino.

00:14:22.536 --> 00:14:24.620
Ali bilo je veoma
algoritamski prihvatljivo.

00:14:25.219 --> 00:14:27.832
Mašina je donela ovu odluku u naše ime.

00:14:27.856 --> 00:14:31.353
Veoma važan, ali težak razgovor

00:14:31.377 --> 00:14:32.932
je mogao da bude ugušen,

00:14:32.956 --> 00:14:35.652
da je Fejsbuk bio jedini kanal.

00:14:36.117 --> 00:14:39.914
Sad, naposletku, ovi sistemi
takođe mogu da greše

00:14:39.938 --> 00:14:42.648
drugačije od ljudskih sistema.

00:14:42.648 --> 00:14:45.840
Ljudi, sećate li se Votsona,
IBM-ovog sistema mašinske inteligencije

00:14:45.840 --> 00:14:48.802
koji je obrisao pod
ljudskim takmičarima na kvizu?

00:14:49.131 --> 00:14:50.559
Bio je sjajan takmičar.

00:14:50.583 --> 00:14:54.152
Ali su ga onda u finalnom izazovu
upitali sledeće pitanje:

00:14:54.569 --> 00:14:57.611
"Njegov najveći aerodrom je nazvan
po heroju iz II svetskog rata,

00:14:57.615 --> 00:14:59.897
a drugi po veličini
po bici iz II svetskog rata."

00:14:59.897 --> 00:15:01.339
(Pevuši finalnu temu iz kviza)

00:15:01.582 --> 00:15:02.764
Čikago.

00:15:02.788 --> 00:15:04.238
Oba ljudska bića su pogodila.

00:15:04.697 --> 00:15:09.045
Votson, s druge strane,
odgovorio je: "Toronto" -

00:15:09.069 --> 00:15:10.887
za kategoriju gradova SAD-a.

00:15:11.596 --> 00:15:14.497
Impresivni sistem je napravio i grešku

00:15:14.521 --> 00:15:18.172
koju ljudsko biće nikad ne bi,
drugaš je nikad ne bi napravio.

00:15:18.823 --> 00:15:21.932
Naša mašinska inteligencija može da omane

00:15:21.956 --> 00:15:25.056
na načine koji se ne uklapaju
sa obrascima grešenja kod ljudi,

00:15:25.080 --> 00:15:28.030
na načine koji su neočekivani
i na koje nismo pripremljeni.

00:15:28.054 --> 00:15:31.692
Bilo bi loše ne dobiti posao
za koji ste kvalifikovani,

00:15:31.716 --> 00:15:35.437
ali bi bilo tri puta gore
ako bi to bilo zbog preopterećenja

00:15:35.437 --> 00:15:36.979
u nekakvoj sistemskoj podrutini.

00:15:36.979 --> 00:15:38.522
(Smeh)

00:15:38.526 --> 00:15:41.312
U maju 2010,

00:15:41.336 --> 00:15:45.380
munjevit krah na Vol Stritu
je pokrenut povratnom petljom

00:15:45.404 --> 00:15:48.432
u Vol Stritovom algoritmu "prodaja",

00:15:48.456 --> 00:15:52.640
izbrisao je vrednost
od trilion dolara za 36 minuta.

00:15:53.692 --> 00:15:55.929
Ne želim da razmišljam
o tome šta znači "greška"

00:15:55.933 --> 00:15:59.522
u kontekstu smrtonosnog
autonomnog oružja.

00:16:01.894 --> 00:16:05.684
Dakle, da, ljudi su oduvek
bili pristrasni.

00:16:05.708 --> 00:16:07.884
Donosioci odluka i čuvari informacija,

00:16:07.908 --> 00:16:11.401
na sudovima, vestima, u ratu...

00:16:11.425 --> 00:16:14.463
oni greše; ali u tome je poenta.

00:16:14.487 --> 00:16:18.008
Ne možemo da izbegnemo
ova teška pitanja.

00:16:18.596 --> 00:16:22.112
Ne možemo da delegiramo
naša zaduženja mašinama.

00:16:22.676 --> 00:16:26.884
(Aplauz)

00:16:29.089 --> 00:16:33.536
Veštačka inteligencija nam ne pruža
besplatnu kartu za "beg od etike".

00:16:34.742 --> 00:16:38.123
Naučnik za podatke Fred Benson
to naziva matematičkim ispiranjem.

00:16:38.147 --> 00:16:39.536
Potrebno nam je suprotno.

00:16:39.560 --> 00:16:44.948
Moramo da negujemo sumnju u algoritme,
nadzor i istraživanje.

00:16:45.380 --> 00:16:48.578
Moramo da se postaramo
da imamo algoritamsku odgovrnost,

00:16:48.602 --> 00:16:51.047
proveru i smislenu transparentnost.

00:16:51.380 --> 00:16:54.614
Moramo da prihvatimo
da uvođenje matematike i kompjutera

00:16:54.638 --> 00:16:57.608
u neuredne ljudske odnose
vođene vrednostima

00:16:57.632 --> 00:17:00.016
ne donosi objektivnost;

00:17:00.040 --> 00:17:03.673
već pre složenost ljudskih odnosa
osvaja algoritme.

00:17:04.148 --> 00:17:07.635
Da, možemo i treba da koristimo kompjutere

00:17:07.659 --> 00:17:09.673
kako bi donosili bolje odluke.

00:17:09.697 --> 00:17:15.029
Ali moramo da ovladamo
našom moralnom odgovornošću i rasuđivanjem

00:17:15.053 --> 00:17:17.871
i da koristimo algoritme
unutar tog okvira,

00:17:17.895 --> 00:17:22.830
ne kao sredstva da se odreknemo
i da delegiramo naše odgovornosti

00:17:22.854 --> 00:17:25.308
nekom drugom, kao čovek čoveku.

00:17:25.807 --> 00:17:28.416
Mašinska inteligencija je tu.

00:17:28.440 --> 00:17:31.861
To znači da se kao nikad pre
moramo čvrsto držati

00:17:31.885 --> 00:17:34.032
ljudskih vrednosti i ljudske etike.

00:17:34.056 --> 00:17:35.210
Hvala vam.

00:17:35.234 --> 00:17:39.764
(Aplauz)

