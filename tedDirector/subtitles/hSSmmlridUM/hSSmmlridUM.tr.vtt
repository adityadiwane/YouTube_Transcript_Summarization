WEBVTT
Kind: captions
Language: tr

00:00:00.000 --> 00:00:07.000
Çeviri: Selda Yener
Gözden geçirme: Cihan Ekmekçi

00:00:12.559 --> 00:00:16.681
Bilgisayar programcısı olarak 
ilk çalışmaya başladığımda

00:00:16.705 --> 00:00:18.661
üniversite birinci sınıftaydım,

00:00:18.685 --> 00:00:20.192
yani yeni yetme sayılırdım.

00:00:20.709 --> 00:00:22.441
Bir şirkette program yazma göreviyle

00:00:22.465 --> 00:00:24.075
işe girdikten kısa süre sonra

00:00:24.619 --> 00:00:28.254
şirkette çalışan bir müdür yanıma geldi

00:00:28.278 --> 00:00:29.546
ve fısıltıyla sordu:

00:00:30.049 --> 00:00:32.910
"Yalan söylediğimi anlayabilir mi?"

00:00:33.626 --> 00:00:35.703
Odada başka kimse yoktu.

00:00:36.852 --> 00:00:41.241
"Yalanı kim anlayabilir mi?
Ayrıca neden fısıldaşıyoruz?" dedim.

00:00:42.086 --> 00:00:45.193
Müdür, odada bulunan bilgisayarı gösterdi.

00:00:45.217 --> 00:00:48.313
"Yalan söyleyip söylemediğimi anlar mı?"

00:00:49.433 --> 00:00:53.795
Bu müdür danışmada çalışan 
biriyle ilişki yaşıyordu.

00:00:53.819 --> 00:00:54.931
(Gülüşmeler)

00:00:54.955 --> 00:00:56.721
Ben de yeni yetmeyim tabii.

00:00:57.267 --> 00:00:59.286
Fısıltıyla bağırır gibi cevap verdim:

00:00:59.310 --> 00:01:02.934
"Evet, yalan söylerseniz
bilgisayar anlar." dedim.

00:01:02.958 --> 00:01:04.764
(Gülüşmeler)

00:01:04.788 --> 00:01:07.711
Gülüyordum ama aslında
gülünmesi gereken bendim.

00:01:07.735 --> 00:01:11.003
Günümüzde insan yüzlerini

00:01:11.027 --> 00:01:14.575
işlemden geçirerek ruh hâlini 
ve hatta yalan söylediğini

00:01:14.599 --> 00:01:16.643
tespit eden bilgisayar programları var.

00:01:17.068 --> 00:01:21.221
Reklamcılar, hatta devletler de
çok ilgi duyuyor.

00:01:22.139 --> 00:01:24.001
Matematik ve fen bilgisini çok seven

00:01:24.025 --> 00:01:27.138
bir çocuk olduğum için
bilgisayar programcısı olmuştum.

00:01:27.762 --> 00:01:30.870
Fakat o sıralarda
nükleer silahlara dair bilgi edinmiş

00:01:30.894 --> 00:01:33.846
ve bilim etiğine de
çok ilgi duymaya başlamıştım.

00:01:33.870 --> 00:01:35.074
Sorunlarım vardı.

00:01:35.098 --> 00:01:37.739
Ne yazık ki ailevi durumlar yüzünden

00:01:37.763 --> 00:01:41.061
mümkün olduğu kadar çabuk
işe girmem gerekiyordu.

00:01:41.085 --> 00:01:44.384
Kendi kendime düşündüm;
kolaylıkla iş bulabileceğim

00:01:44.408 --> 00:01:46.204
teknik bir alan seçeyim ki

00:01:46.228 --> 00:01:50.246
sıkıcı etik problemlerle
uğraşmak zorunda kalmayayım dedim.

00:01:50.842 --> 00:01:52.371
Böylelikle bilgisayarı seçtim.

00:01:52.395 --> 00:01:53.499
(Gülüşmeler)

00:01:53.523 --> 00:01:56.933
Ha, ha, ha!
Herkes bana gülüyordur.

00:01:56.957 --> 00:01:59.711
Günümüzde bilgisayar uzmanları
her gün milyarlarca insanın

00:01:59.735 --> 00:02:03.944
göreceği şeyleri kontrol eden
programlar kuruyor.

00:02:04.872 --> 00:02:08.694
Kime çarpacağına karar verebilecek
arabalar geliştiriyorlar.

00:02:09.527 --> 00:02:12.740
Savaşta insanları öldürecek türden

00:02:12.764 --> 00:02:15.049
makineler, silahlar bile geliştiriyorlar.

00:02:15.073 --> 00:02:17.844
Etik tamamen ortadan kalkıyor.

00:02:19.003 --> 00:02:21.061
Makine zekâsı işin içinde.

00:02:21.643 --> 00:02:25.117
Artık herhangi bir karar verirken
bilgisayar kullanıyoruz,

00:02:25.141 --> 00:02:27.027
üstelik yeni kararlar alırken bile.

00:02:27.051 --> 00:02:32.223
Bilgisayara tek bir doğru 
cevabı olmayan, öznel,

00:02:32.247 --> 00:02:33.449
açık uçlu ve değer yüklü

00:02:33.473 --> 00:02:35.798
sorular soruyoruz.

00:02:35.822 --> 00:02:37.580
Mesela şu gibi sorular:

00:02:37.604 --> 00:02:39.254
"Şirket kimi işe almalı?"

00:02:39.916 --> 00:02:42.675
"Hangi arkadaştan
hangi güncellemeyi görmelisiniz?"

00:02:42.699 --> 00:02:44.965
"Hangi mahkûm tekrar suç işlemeye yatkın?"

00:02:45.334 --> 00:02:48.388
"İnsanlara hangi haber ya da film 
tavsiye edilmeli?"

00:02:48.412 --> 00:02:51.784
Bakın, evet bir süredir bilgisayar
kullanıyoruz,

00:02:51.808 --> 00:02:53.325
ama bu farklı bir durum.

00:02:53.349 --> 00:02:55.416
Bu tarihi bir hata,

00:02:55.440 --> 00:03:00.777
çünkü böyle öznel kararlarda
bilgisayara güvenemeyiz,

00:03:00.801 --> 00:03:06.181
bilgisayara uçak uçurmada, 
köprü inşa etmede, aya gitmede

00:03:06.251 --> 00:03:07.694
güvendiğimiz gibi güvenemeyiz.

00:03:08.269 --> 00:03:11.528
Uçaklar daha güvenli mi?
Köprü sallanıp çöker miydi?

00:03:11.552 --> 00:03:16.050
Burada üzerinde anlaşılan, açık ölçütler 
ve bize yol gösteren doğa kanunları

00:03:16.074 --> 00:03:18.313
olduğu konusunda hemfikiriz.

00:03:18.337 --> 00:03:21.731
Karmaşık insan ilişkilerinde
karar verirken

00:03:21.755 --> 00:03:25.718
bu tür dayanak ve ölçütler yok.

00:03:25.742 --> 00:03:29.979
İşleri daha çetrefilli hâle getirmek için
yazılımımız gittikçe güçleniyor,

00:03:30.003 --> 00:03:33.776
fakat aynı zamanda daha az şeffaflaşıp
daha karmaşık oluyor.

00:03:34.362 --> 00:03:36.402
Son on yıl içerisinde,

00:03:36.426 --> 00:03:39.155
kompleks algoritmalar
büyük aşamalar katetti.

00:03:39.179 --> 00:03:41.169
İnsan yüzlerini tanıyabiliyorlar.

00:03:41.805 --> 00:03:43.860
El yazılarını çözebiliyorlar.

00:03:44.256 --> 00:03:46.322
Kredi kartı dolandırıcılığını 
tespit edip

00:03:46.346 --> 00:03:47.535
spam postaları engelliyor

00:03:47.559 --> 00:03:49.596
ve diller arası çeviri yapabiliyorlar.

00:03:49.620 --> 00:03:52.194
Tıbbi görüntülemelerde tümörleri
teşhis edebiliyorlar.

00:03:52.218 --> 00:03:54.423
Go ve satrançta insanları yenebiliyorlar.

00:03:55.084 --> 00:03:59.588
Bu gelişmelerin çoğu "makine öğrenimi"
denilen bir yöntemden geliyor.

00:03:59.995 --> 00:04:03.182
Makine öğrenimi, bilgisayara 
detaylı, doğru ve itinalı

00:04:03.206 --> 00:04:06.791
talimatlar verdiğiniz
geleneksel programlamadan farklıdır.

00:04:07.198 --> 00:04:11.380
Daha çok sistemi kavrayıp onu
dijital yaşamlarımızda ürettiğimiz türden

00:04:11.404 --> 00:04:13.060
yapısal olmayan veri dahil

00:04:13.084 --> 00:04:15.362
bir sürü veriyle desteklemeniz gibidir.

00:04:15.386 --> 00:04:18.116
Sistem bu bilgileri baştan sona
karıştırarak öğrenir.

00:04:18.489 --> 00:04:20.015
Ayrıca en önemlisi

00:04:20.039 --> 00:04:24.419
bu sistemlerin tek cevaplı 
mantıkla çalışmadığıdır.

00:04:24.443 --> 00:04:27.402
Tek bir cevap üretmezler,
daha çok olasılık vardır:

00:04:27.426 --> 00:04:30.909
"Belki de aradığınız şey bu olabilir."

00:04:31.843 --> 00:04:34.913
Şimdi avantajı şu:
Bu yöntem gerçekten çok güçlüdür.

00:04:34.937 --> 00:04:37.013
Google'ın Yapay Zeka'sının dediği gibi

00:04:37.037 --> 00:04:39.234
"verinin akıl almaz etkinliği"dir.

00:04:39.611 --> 00:04:40.964
Dezavantajı ise,

00:04:41.558 --> 00:04:44.629
sistemin ne öğrendiğini
tam olarak anlamıyor olmamızdır.

00:04:44.653 --> 00:04:46.240
Aslında bu onun gücü.

00:04:46.766 --> 00:04:50.564
Bu, bir bilgisayara
talimat vermek gibi değildir;

00:04:51.020 --> 00:04:55.084
nasıl kontrol edeceğimizi bilmediğimiz
bir içecek sıkma makinesini

00:04:55.108 --> 00:04:57.479
çalıştırmak gibidir.

00:04:58.182 --> 00:04:59.733
Yani sorunumuz bu.

00:05:00.247 --> 00:05:04.509
Yapay zeka sistemi bir şeyleri
yanlış anladığında problem olur.

00:05:04.533 --> 00:05:08.073
Aynı zamanda doğru anladığında da
problem olur,

00:05:08.097 --> 00:05:11.725
çünkü öznel bir problem olduğunda
hangisinin hangisi olduğunu bilmiyoruz.

00:05:11.749 --> 00:05:14.088
Bu şeyin ne düşündüğünü bile bilmiyoruz.

00:05:15.313 --> 00:05:18.996
Şimdi, insanları işe almak için kullanılan

00:05:19.943 --> 00:05:24.254
makine öğrenimi sistemlerinden yararlanan
bir işe alım algoritma sistemi düşünün.

00:05:24.872 --> 00:05:28.451
Böyle bir sistem önceki 
çalışanların verilerine ayarlı

00:05:28.475 --> 00:05:31.066
ve şirketteki yüksek performansı olan

00:05:31.090 --> 00:05:34.128
çalışanlar gibi insanlar bulmaya ve 
işe almaya kurulmuş olmalı.

00:05:34.634 --> 00:05:35.787
İyi fikir.

00:05:35.811 --> 00:05:37.810
Bir keresinde bir konferansa katıldım,

00:05:37.834 --> 00:05:40.959
işe alımda bu sistemi kullanan
insan kaynakları uzmanları ile

00:05:40.983 --> 00:05:42.189
yöneticileri,

00:05:42.213 --> 00:05:43.902
üst düzey insanları buluşturuyordu.

00:05:43.902 --> 00:05:45.442
Çok heyecanlıydılar.

00:05:45.466 --> 00:05:50.119
Bu yöntemin işe alımı daha nesnel,
daha az ön yargılı yapacağını

00:05:50.143 --> 00:05:53.143
ve kadın ve azınlıkları ön yargılı 
insan kaynakları karşısında

00:05:53.167 --> 00:05:55.355
daha şanslı hâle getireceğini
düşünüyorlardı.

00:05:55.379 --> 00:05:58.222
İşe alımda ön yargılı davranılır.

00:05:58.919 --> 00:06:00.104
Biliyorum.

00:06:00.128 --> 00:06:03.133
Yani, programcı olarak çalıştığım ilk
işlerimden birinde

00:06:03.157 --> 00:06:07.025
ilk patronum bazen sabah çok erken ya da

00:06:07.049 --> 00:06:10.802
öğleden sonra çok geç saatlerde
yanıma gelir

00:06:10.826 --> 00:06:13.888
ve "Zeynep, hadi yemeğe çıkalım!" derdi.

00:06:14.544 --> 00:06:16.711
Garip zamanlamasına şaşırırdım.

00:06:16.735 --> 00:06:18.864
Saat dört. Öğle yemeği mi?

00:06:18.888 --> 00:06:21.982
Param yoktu, bedava öğle yemeği vardı.
Her zaman giderdim.

00:06:22.438 --> 00:06:24.505
Neler olduğunu daha sonraları anladım.

00:06:24.529 --> 00:06:29.075
Amirlerim kendi üst düzey müdürlerine
çok ciddi bir iş için

00:06:29.099 --> 00:06:32.212
işe aldıkları programcının 
kot pantolon ve spor ayakkabı giyen

00:06:32.236 --> 00:06:36.166
yeni yetme bir kız olduğunu
söylememişlerdi.

00:06:36.994 --> 00:06:39.196
İyi iş çıkarıyordum,
ama yanlış görünüyordum,

00:06:39.220 --> 00:06:40.919
yanlış yaşta ve cinsiyetteydim.

00:06:40.943 --> 00:06:44.289
Dolayısıyla cinsiyet ve ırk 
gözetilmeksizin işe alım

00:06:44.313 --> 00:06:46.178
bence kesinlikle iyi fikir.

00:06:46.851 --> 00:06:50.192
Ancak bu sistemlerle konu 
daha karmaşık oluyor, nedeni şu:

00:06:50.788 --> 00:06:56.579
Şimdilerde bilgisayar sistemleri
paylaşmadığınız şeyler bile olsa

00:06:56.603 --> 00:06:58.475
dijital kırıntılarınızdan

00:06:58.499 --> 00:07:00.832
hakkınızda birçok çıkarım yapabilir.

00:07:01.326 --> 00:07:04.253
Cinsel yöneliminizi,

00:07:04.814 --> 00:07:06.120
kişilik özelliklerinizi,

00:07:06.679 --> 00:07:08.212
siyasi görüşünüzü anlayabiliyor.

00:07:08.650 --> 00:07:12.335
Yüksek doğruluk oranlı öngörü güçleri var.

00:07:13.182 --> 00:07:15.760
Unutmayın - paylaşmadığınız
şeyleri bile.

00:07:15.784 --> 00:07:17.375
Bu, çıkarımda bulunmaktır.

00:07:17.399 --> 00:07:20.660
Sosyal medya verilerinden
klinik depresyon veya

00:07:20.684 --> 00:07:24.325
doğum sonrası depresyon ihtimalini
öngören sayısal sistemler geliştiren

00:07:24.349 --> 00:07:25.765
bir arkadaşım var.

00:07:26.496 --> 00:07:27.923
Sonuçlar etkileyici.

00:07:28.312 --> 00:07:31.669
Sistem depresyon ihtimalini
semptomların başlamasından

00:07:31.693 --> 00:07:35.596
aylar öncesinden öngörebiliyor,

00:07:35.620 --> 00:07:36.993
aylar öncesinden.

00:07:37.017 --> 00:07:39.263
Hiçbir semptom yok, ama öngörü var.

00:07:39.287 --> 00:07:44.099
Erken müdahalede
kullanılacağını umuyor. Muhteşem!

00:07:44.731 --> 00:07:46.771
Şimdi de bunu
işe alım kavramına uygulayın.

00:07:47.847 --> 00:07:50.893
Bu insan kaynakları
yöneticileri konferansında

00:07:50.917 --> 00:07:55.626
büyük bir şirketin 
üst düzey bir yöneticisiyle görüştüm.

00:07:55.650 --> 00:08:00.228
Ona, "Peki sisteminiz size bildirmeden

00:08:00.252 --> 00:08:06.801
gelecekte yüksek depresyon ihtimali olan
insanları ayıklıyorsa?

00:08:07.581 --> 00:08:10.957
Şu an psikolojik bozuklukları yok,
ancak belki ileride olabilir.

00:08:11.743 --> 00:08:15.149
Peki şu an hamile olmayan ancak
gelecek bir ya da iki yıl içinde

00:08:15.173 --> 00:08:17.759
hamile kalma ihtimali olan 
kadınları ayıklıyorsa?

00:08:18.664 --> 00:08:24.300
İş yeri kültürünüz öyle olduğu için
agresif insanları işe alıyorsa?" dedim.

00:08:24.993 --> 00:08:27.684
Bunu cinsiyet analizlerine bakarak 
anlayamazsınız.

00:08:27.708 --> 00:08:29.210
Dengelenmiş olabilirler.

00:08:29.234 --> 00:08:32.791
Ayrıca bu geleneksel kodlama değil
makine öğrenimi olduğu için

00:08:32.815 --> 00:08:37.722
"yüksek depresyon riski",
"yüksek hamilelik riski",

00:08:37.746 --> 00:08:39.579
"agresiflik ölçümü" gibi

00:08:39.603 --> 00:08:41.337
etiket taşıyan değişkenler yok.

00:08:41.815 --> 00:08:45.494
Sadece sisteminizin neyi seçtiğini değil,

00:08:45.518 --> 00:08:47.921
nereden bakmaya başlayacağınızı
bile bilmiyorsunuz.

00:08:47.921 --> 00:08:49.111
Bir kara kutu.

00:08:49.135 --> 00:08:51.942
Tahmin gücüne sahip ancak
onu anlamıyorsunuz.

00:08:52.306 --> 00:08:54.675
"Ne güvenlikleri var?" diye sordum,
"kara kutunuzun

00:08:54.699 --> 00:08:58.372
gizli bir şey yapmadığından
emin olmak için?"

00:09:00.683 --> 00:09:04.561
Kuyruğuna basmışım gibi bakakaldı.

00:09:04.585 --> 00:09:05.833
(Gülüşmeler)

00:09:05.857 --> 00:09:07.898
Dik dik baktı ve

00:09:08.376 --> 00:09:12.709
"buna dair başka bir şey 
duymak istemiyorum." dedi.

00:09:13.278 --> 00:09:15.312
Arkasını dönüp gitti.

00:09:15.884 --> 00:09:17.370
Gerçi kaba değildi.

00:09:17.394 --> 00:09:23.702
Açıklaması şu: Bilmediğim şey benim
sorunum değil, bas git, öldürücü bakış.

00:09:23.726 --> 00:09:24.972
(Gülüşmeler)

00:09:25.682 --> 00:09:29.521
Bakın böyle bir sistem bazı açılardan
insan yöneticilerden

00:09:29.545 --> 00:09:31.648
daha az ön yargılı olabilir.

00:09:31.672 --> 00:09:33.818
Parasal anlam taşıyabilir.

00:09:34.393 --> 00:09:36.043
Ancak depresyon riski

00:09:36.067 --> 00:09:40.815
yüksek olan insanların iş piyasasına
istikrarlı ve gizli olarak girmesini

00:09:40.839 --> 00:09:43.132
engellemeye sebep olabilir.

00:09:43.573 --> 00:09:46.169
Ne yapmış olduğumuzu bile bilmeden 

00:09:46.193 --> 00:09:48.478
kurmak istediğimiz toplum modeli bu mu?

00:09:48.502 --> 00:09:52.466
Çünkü karar verme işini tam olarak
anlamadığımız makinelere bırakıyoruz.

00:09:53.085 --> 00:09:54.543
Diğer bir problemse şu:

00:09:55.134 --> 00:09:59.586
Bu sistemler çoğunlukla bizim 
davranışlarımızla; insan izleri tarafından

00:09:59.610 --> 00:10:01.426
üretilen bilgilerle test edilir.

00:10:02.008 --> 00:10:05.816
Ön yargılarımızı yansıtıyor olabilirler.

00:10:05.840 --> 00:10:09.433
Bu sistemler ön yargılarımıza dönüp

00:10:09.457 --> 00:10:10.770
onları büyütüp

00:10:10.794 --> 00:10:12.342
bize tekrar gösteriyor olabilir,

00:10:12.342 --> 00:10:13.698
biz ise bu arada kendimize

00:10:13.722 --> 00:10:16.839
"Sadece nesnel, tarafsız hesap
yapıyoruz." diyoruz.

00:10:18.134 --> 00:10:20.811
Araştırmacılar Google'da
yüksek maaşlı iş ilanlarını

00:10:21.954 --> 00:10:27.267
kadınların görme ihtimalinin 
erkeklerden daha az olduğunu tespit etti.

00:10:28.283 --> 00:10:30.813
Afro-Amerikan isimleri araştırırken

00:10:30.837 --> 00:10:35.543
alakası olmasa bile 
sabıka geçmişi ile ilgili ilanları

00:10:35.567 --> 00:10:37.134
öne sürmesi daha muhtemeldir.

00:10:38.513 --> 00:10:42.062
Araştırmacıların bazen ortaya çıkardığı
ancak bazen bizim bilmediğimiz

00:10:42.086 --> 00:10:46.059
bu tür gizli ön yargıların ve
kara kutu algoritmalarının

00:10:46.083 --> 00:10:48.744
hayat değiştiren sonuçları var.

00:10:49.778 --> 00:10:53.937
Wisconsin'de bir sanık,
altı yıl hapse mahkûm edildi,

00:10:53.961 --> 00:10:55.316
polisten kaçtığı için.

00:10:56.644 --> 00:10:57.830
Belki bilmiyorsunuzdur,

00:10:57.854 --> 00:11:01.852
algoritmaların şartlı tahliye ve ceza
kararlarında kullanımı giderek artıyor.

00:11:01.876 --> 00:11:04.831
Şunu öğrenmek istiyordu:
Bu sonuç nasıl hesaplanmıştı?

00:11:05.615 --> 00:11:07.280
Ticari bir kara kutu.

00:11:07.304 --> 00:11:11.509
Şirket, halka açık duruşmada
işlemlerinin sorgulanmasını reddetmişti.

00:11:12.216 --> 00:11:17.748
Kâr amacı gütmeyen araştırmacı kurum 
ProPublica, bu algoritmayı

00:11:17.772 --> 00:11:19.788
bulabildiği kamusal verilerle inceledi

00:11:19.812 --> 00:11:22.128
ve sonuçlarının ön yargılı olduğunu,

00:11:22.152 --> 00:11:25.781
öngörü gücününse kötü, olasılıktan
biraz iyi olduğunu

00:11:25.805 --> 00:11:30.221
ve siyahi sanıkları haksız yere 
beyaz sanıklardan iki kat fazla bir oranla

00:11:30.245 --> 00:11:34.140
geleceğin suçluları olarak 
etiketlediğini bulguladı.

00:11:35.711 --> 00:11:37.275
Şu olayı göz önüne alın:

00:11:37.923 --> 00:11:41.775
Bu kadın, vaftiz kardeşini Florida'nın 
Broward bölgesindeki bir okuldan

00:11:41.799 --> 00:11:43.874
almaya geç kalmış,

00:11:44.577 --> 00:11:46.933
bir arkadaşıyla caddede ilerliyorlar.

00:11:46.957 --> 00:11:51.056
Verandada duran kilitsiz bir 
çocuk bisikleti ve bir kaydırak görüyor

00:11:51.080 --> 00:11:52.712
ve düşünmeden biniyorlar.

00:11:52.736 --> 00:11:55.335
Tam yola koyulacakken 
kadının biri çıkıyor ve

00:11:55.359 --> 00:11:57.564
"Hey! Bu benim çocuğumun 
bisikleti!" diyor.

00:11:57.588 --> 00:12:00.882
Bırakıp gidiyorlar, ancak tutuklanıyorlar.

00:12:00.906 --> 00:12:04.543
Haksızdı, aptalca davranmıştı,
ama henüz 18 yaşındaydı.

00:12:04.567 --> 00:12:07.111
Çocukken ciddi olmayan bir iki suç işlemişti.

00:12:07.628 --> 00:12:12.813
Bu arada, adam da Home Depot'da
hırsızlık yapmaktan tutuklanmıştı.

00:12:12.837 --> 00:12:15.761
85 dolar değerinde,
benzeri bir hafif suç.

00:12:16.586 --> 00:12:21.145
Ama öncesinde iki silahlı
soygun sabıkası vardı.

00:12:21.775 --> 00:12:25.257
Ancak algoritma, adamı değil
kadını yüksek riskli olarak işaretledi.

00:12:26.566 --> 00:12:30.440
İki yıl sonra ProPublica kadının
tekrar suç işlemediğini tespit etti.

00:12:30.464 --> 00:12:33.014
Sabıka puanıyla iş bulması çok zordu.

00:12:33.038 --> 00:12:35.114
Öte yandan adam tekrar suç işlemişti

00:12:35.138 --> 00:12:38.974
ve şimdi daha sonra işlediği bir suç
yüzünden sekiz yıllık hapis cezasında.

00:12:39.908 --> 00:12:43.277
Belli ki kara kutularımızı
kontrol etmemiz

00:12:43.301 --> 00:12:45.916
ve onlara böyle kontrolsüz güç
vermememiz gerekiyor.

00:12:45.940 --> 00:12:48.819
(Alkışlar)

00:12:49.907 --> 00:12:54.149
Kontroller önemli ve etkili,
ancak tüm sorunlarımızı çözmüyorlar.

00:12:54.173 --> 00:12:56.921
Facebook'un muhteşem
haber akışı algoritmasına bakın,

00:12:56.945 --> 00:13:01.788
yani takip ettiğiniz tüm arkadaşlarınız
ve sayfalardan her şeyi sıralayıp

00:13:01.812 --> 00:13:04.096
size ne göstereceğine 
karar veren algoritma.

00:13:04.718 --> 00:13:06.993
Başka bir bebek fotoğrafı görmeli misiniz?

00:13:07.017 --> 00:13:08.213
(Gülüşmeler)

00:13:08.237 --> 00:13:10.833
Bir tanıdıktan somurtkan bir not?

00:13:11.269 --> 00:13:13.125
Önemli ama üzücü haberler?

00:13:13.149 --> 00:13:14.631
Doğru bir cevap yok.

00:13:14.655 --> 00:13:17.314
Facebook meşgul olacaklarınızı
en uygun hâle getiriyor:

00:13:17.338 --> 00:13:18.933
Beğeniler, paylaşımlar, yorumlar.

00:13:19.988 --> 00:13:22.684
Ağustos 2014'te,

00:13:22.708 --> 00:13:25.370
Missouri, Ferguson'da
Afro-Amerikan bir gencin

00:13:25.394 --> 00:13:29.811
beyaz bir polis tarafından şüpheli 
bir şekilde öldürülmesi sonrası

00:13:29.835 --> 00:13:31.405
protestolar başladı.

00:13:31.794 --> 00:13:33.801
Protesto haberleri

00:13:33.825 --> 00:13:36.510
algoritmik olarak filtrelenmeyen
Twitter akışımda vardı

00:13:36.534 --> 00:13:38.484
ancak Facebook'ta hiçbir yerde yoktu.

00:13:39.002 --> 00:13:40.736
Facebook arkadaşlarım ne hâldeydi?

00:13:40.760 --> 00:13:42.962
Facebook'un algoritmasını 
devre dışı bıraktım,

00:13:43.292 --> 00:13:46.140
ki bu çok zordur, çünkü Facebook 
sizi algoritmanın

00:13:46.164 --> 00:13:48.200
kontrolü altında tutmak ister.

00:13:48.224 --> 00:13:50.462
Baktım ki arkadaşlarım da
bunu konuşuyor.

00:13:50.486 --> 00:13:52.995
Bunu bana göstermeyen
algoritmanın ta kendisiydi.

00:13:53.019 --> 00:13:56.061
Bunu araştırdım ve 
yaygın bir problem olduğunu gördüm.

00:13:56.085 --> 00:13:59.898
Ferguson haberi algoritma dostu değildi.

00:13:59.922 --> 00:14:01.093
"Beğenilebilir" değildi.

00:14:01.117 --> 00:14:02.669
Kim "beğen"e tıklayacaktı?

00:14:03.320 --> 00:14:05.526
Yorum yapılması bile kolay değildi.

00:14:05.550 --> 00:14:07.031
Beğeniler ve yorumlar olmayınca

00:14:07.041 --> 00:14:10.237
algoritma bunu daha az insana
gösteriyor olmalıydı,

00:14:10.261 --> 00:14:11.803
dolayısıyla görmüyorduk.

00:14:12.766 --> 00:14:13.994
Onun yerine, o hafta

00:14:14.018 --> 00:14:16.316
Facebook'un algoritması 
şunu ön plana çıkardı:

00:14:16.340 --> 00:14:18.566
ALS Buz Kovası Düellosu.

00:14:18.590 --> 00:14:22.332
İyi bir sebep; buzlu su dök,
bağış yap, tamam.

00:14:22.356 --> 00:14:24.260
Fakat süper algoritma dostuydu.

00:14:25.039 --> 00:14:27.652
Makine bu kararı bizim için almıştı.

00:14:27.676 --> 00:14:31.173
Facebook tek kanal olsaydı
çok önemli ancak

00:14:31.197 --> 00:14:32.752
etkili bir sohbet

00:14:32.776 --> 00:14:35.472
engellenmiş olabilirdi.

00:14:35.937 --> 00:14:39.734
Şimdi sonuç olarak bu sistemler
insan sistemlerine

00:14:39.758 --> 00:14:42.494
benzememesi bakımından da
yanlış olabilir.

00:14:42.518 --> 00:14:45.440
Watson'ı hatırlıyor musunuz?
IBM'in Riziko'da

00:14:45.464 --> 00:14:48.592
insan rakiplerini yenilgiye uğratan 
makine zekâsı sistemini?

00:14:48.951 --> 00:14:50.379
Harika bir oyuncuydu.

00:14:50.403 --> 00:14:53.972
Ancak o zaman, Riziko'nun finalinde
Watson'a şu soru soruldu:

00:14:54.479 --> 00:14:57.581
"En büyük havaalanı adını İkinci
Dünya Savaşı kahramanından alır,

00:14:57.581 --> 00:14:59.527
ikincisi İkinci Dünya Savaşı savaşından.

00:14:59.707 --> 00:15:01.089
(Final Riziko müziği çalar)

00:15:01.402 --> 00:15:02.584
Chicago.

00:15:02.608 --> 00:15:04.038
İki insan soruyu doğru anladı.

00:15:04.517 --> 00:15:08.865
Buna karşın Watson,
Birleşik Devletler şehri olarak

00:15:08.889 --> 00:15:10.707
"Toronto" cevabını verdi.

00:15:11.416 --> 00:15:14.317
Bu etkileyici sistem

00:15:14.341 --> 00:15:17.992
bir insanın, ikinci sınıfa giden birinin
asla yapmayacağı bir hata yapmıştı.

00:15:18.643 --> 00:15:21.752
Makine zekâmız

00:15:21.776 --> 00:15:24.876
insanların hata şekline
uymayan şekilde,

00:15:24.900 --> 00:15:27.850
beklemediğimiz ve hazırlıksız olduğumuz
şekilde hata yapabilir.

00:15:27.874 --> 00:15:31.512
Kalifiye biri için işe alınmamak
kötü olabilirdi,

00:15:31.536 --> 00:15:35.263
fakat bu, bazı alt programlarda 
bellek dolu dediği için oluyorsa

00:15:35.287 --> 00:15:36.719
üç kat daha kötü olurdu.

00:15:36.743 --> 00:15:38.322
(Gülüşmeler)

00:15:38.346 --> 00:15:41.132
Mayıs 2010'da

00:15:41.156 --> 00:15:45.200
Wall Steet'te olan Wall Street'in
"satış" algoritmalarını

00:15:45.224 --> 00:15:48.252
bir geri bildirim döngüsünün
körüklediği ani bir düşüş

00:15:48.276 --> 00:15:52.460
36 dakika içinde trilyon dolarları sildi.

00:15:53.542 --> 00:15:55.729
Ölümcül otonom silahlar bağlamında

00:15:55.753 --> 00:15:59.342
"hata yapmak" ne demek bunu
düşünmek bile istemiyorum.

00:16:01.714 --> 00:16:05.504
Evet, insanlar her zaman 
ön yargıda bulunur.

00:16:05.528 --> 00:16:07.704
Karar vericiler ve geçit deneticiler

00:16:07.728 --> 00:16:11.221
mahkemelerde, haberlerde, savaşlarda...

00:16:11.245 --> 00:16:14.283
Hata yaparlar; işte benim 
asıl dikkat çekmek istediğim bu.

00:16:14.307 --> 00:16:17.828
Bu zor sorulardan kaçamayız.

00:16:18.416 --> 00:16:21.932
Kendi sorumluluklarımızı 
makinelere yaptıramayız.

00:16:22.496 --> 00:16:26.704
(Alkışlar)

00:16:28.909 --> 00:16:33.356
Yapay zekâ bize "etikten kurtul geç"
kartı vermiyor.

00:16:34.562 --> 00:16:37.943
Veri uzmanı Fred Benenson 
buna matematiksel yıkama diyor.

00:16:37.967 --> 00:16:39.356
Tam tersine ihtiyacımız var.

00:16:39.380 --> 00:16:44.768
Algoritmayı şüphe, gözlem ve 
inceleme ile desteklemeliyiz.

00:16:45.200 --> 00:16:48.398
Algoritmik izlenebilirlik, denetim ve

00:16:48.422 --> 00:16:50.867
anlamlı şeffaflığımız olduğundan
emin olmamız gerek.

00:16:51.200 --> 00:16:54.434
Matematik ve programlamayı 
karmaşık, değer yüklü

00:16:54.458 --> 00:16:57.428
insani ilişkilere uygulamanın

00:16:57.452 --> 00:16:59.836
nesnellik getirmeyeceğini 
kabul etmemiz gerekiyor;

00:16:59.860 --> 00:17:03.493
aksine, insan ilişkilerinin karmaşıklığı
algoritmaları ele geçiriyor.

00:17:03.968 --> 00:17:07.455
Tabii ki daha iyi kararlar almamız için
bilgisayar kullanabiliriz,

00:17:07.479 --> 00:17:09.493
kullanmalıyız da.

00:17:09.517 --> 00:17:14.849
Ancak doğru karar vermek için, 
ahlaki sorumluluk alıp

00:17:14.873 --> 00:17:17.691
algoritmaları bu çerçevede 
kullanmak zorundayız,

00:17:17.715 --> 00:17:22.650
insan olarak birbirimize karşı olan
sorumluluklarımızı üstümüzden atıp

00:17:22.674 --> 00:17:25.348
dış kaynaktan temin etmenin 
bir yolu gibi görmemeliyiz.

00:17:25.627 --> 00:17:28.236
Makine zekâsı işte böyledir.

00:17:28.260 --> 00:17:31.681
Bu demektir ki, insani değerlere ve etiğe

00:17:31.705 --> 00:17:33.852
hiç olmadığı kadar
sıkı sarılmamız gerekiyor.

00:17:33.876 --> 00:17:35.030
Teşekkür ederim.

00:17:35.054 --> 00:17:40.074
(Alkışlar)

