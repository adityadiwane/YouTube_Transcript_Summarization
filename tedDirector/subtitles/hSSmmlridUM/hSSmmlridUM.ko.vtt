WEBVTT
Kind: captions
Language: ko

00:00:00.000 --> 00:00:07.000
번역: Jeongmin Kim
검토: JY Kang

00:00:12.739 --> 00:00:16.861
저는 제 첫 번째 직업인
컴퓨터 프로그래머 일을

00:00:16.885 --> 00:00:18.841
대학 1학년 때 시작했습니다.

00:00:18.865 --> 00:00:20.372
아직 십대였죠.

00:00:20.889 --> 00:00:22.621
일을 시작한 지 얼마 안 되어

00:00:22.645 --> 00:00:24.255
회사에서 프로그래밍을 하고 있는데

00:00:24.799 --> 00:00:28.434
회사의 한 관리자 제 자리로 와서는

00:00:28.458 --> 00:00:29.726
저한테 속삭였어요.

00:00:30.229 --> 00:00:33.090
"내가 거짓말하면 쟤가 알아챌까요?"

00:00:33.806 --> 00:00:35.883
하지만 방에는 둘밖에 없었어요.

00:00:37.032 --> 00:00:41.421
"누가 알아챈다는 거죠? 
아무도 없는데 왜 속삭이시는 거예요?"

00:00:42.266 --> 00:00:45.373
매니저는 방에 있는
컴퓨터를 가리켰어요.

00:00:45.397 --> 00:00:48.493
"저놈이 알아챌 수 있을까요?"

00:00:49.613 --> 00:00:53.975
그 매니저는 회사 접수계 
직원과 바람을 피우고 있었죠.

00:00:53.999 --> 00:00:55.111
(웃음)

00:00:55.135 --> 00:00:56.901
전 아직 십대였기에

00:00:57.447 --> 00:00:59.466
그 사람 귀에 대고 소리쳤죠.

00:00:59.490 --> 00:01:03.114
"네, 저 컴퓨터는 당신 부정을
알 수 있을 거예요."

00:01:03.138 --> 00:01:04.944
(웃음)

00:01:04.968 --> 00:01:07.891
전 웃고 말았지만, 
결국 제가 어리석었죠.

00:01:07.915 --> 00:01:11.183
요즘 컴퓨터 시스템은

00:01:11.207 --> 00:01:14.755
감정 상태나, 심지어 거짓말까지

00:01:14.779 --> 00:01:16.823
인간 표정으로 알아낼 수 있거든요.

00:01:17.248 --> 00:01:21.401
광고 업체와 정부까지도 
이 기술에 관심을 기울이고 있죠.

00:01:22.319 --> 00:01:24.671
저는 어릴 때 수학과 
과학을 매우 좋아해서

00:01:24.671 --> 00:01:27.318
자연스레 프로그래머가 되었습니다.

00:01:27.942 --> 00:01:31.050
그런데 언젠가 핵무기를 알게 되었을 때

00:01:31.074 --> 00:01:34.026
저는 과학 윤리에 대해
괌심이 많아졌습니다.

00:01:34.050 --> 00:01:35.254
고민을 많이 했죠.

00:01:35.278 --> 00:01:37.919
하지만 집안 사정 때문에

00:01:37.943 --> 00:01:41.241
최대한 빨리 일을 해야 했죠.

00:01:41.265 --> 00:01:44.564
그래서 과학기술 분야 중에서

00:01:44.588 --> 00:01:46.384
직업을 쉽게 가질 수 있으면서도

00:01:46.408 --> 00:01:50.426
복잡한 윤리적 고민을 할 필요가 
없는 일을 고르기로 했습니다.

00:01:51.022 --> 00:01:52.551
그래서 컴퓨터 관련된 일을 골랐죠.

00:01:52.575 --> 00:01:53.679
(웃음)

00:01:53.703 --> 00:01:57.113
하하하! 그런데 또
어리석은 생각이었네요.

00:01:57.137 --> 00:01:59.891
요즘 컴퓨터 과학자들이
만드는 플랫폼은

00:01:59.915 --> 00:02:04.124
십억 명이 매일 접하게 되는
시스템을 통제합니다.

00:02:05.052 --> 00:02:08.874
누굴 치게 될지도 모를 차를 개발하고

00:02:09.707 --> 00:02:12.920
전쟁에서 사람을 죽일 수 있는

00:02:12.944 --> 00:02:15.229
기계나 무기도 설계하고 있죠.

00:02:15.253 --> 00:02:18.024
모두 윤리에 관한 거죠.

00:02:19.183 --> 00:02:21.241
인공지능의 시대입니다.

00:02:21.823 --> 00:02:25.297
컴퓨터는 점점 다양한
의사결정에 사용되고 있고

00:02:25.321 --> 00:02:27.207
그 중엔 새로운 것도 있죠.

00:02:27.231 --> 00:02:29.727
주관적이고 가치 판단이 필요한

00:02:29.727 --> 00:02:35.979
정답이 없는 열린 질문까지도
컴퓨터에게 묻고 있습니다.

00:02:36.002 --> 00:02:37.760
이를테면 이런 질문들이죠.

00:02:37.784 --> 00:02:39.434
"누굴 채용해야 할까요?"

00:02:40.096 --> 00:02:42.855
"어느 친구의 어떤 소식을
업데이트해야 할까요?

00:02:42.879 --> 00:02:45.355
"어느 재소자가 재범 
가능성이 더 높을까요?"

00:02:45.514 --> 00:02:48.568
"어떤 뉴스 기사나 영화를
추천 목록에 넣을까요?"

00:02:48.592 --> 00:02:51.964
인간은 컴퓨터를 꽤 오래 사용했지만

00:02:51.988 --> 00:02:53.505
이건 좀 다른 문제죠.

00:02:53.529 --> 00:02:55.596
역사적 반전입니다.

00:02:55.620 --> 00:03:00.957
왜냐하면 그런 주관적인 결정까지 
컴퓨터에 의지할 수는 없거든요.

00:03:00.981 --> 00:03:07.665
비행기 조종이나 다리를 짓거나
달에 가는 것과 다르죠.

00:03:08.449 --> 00:03:11.708
비행기가 안전할 것인가.
다리가 흔들리고 무너질 것인가.

00:03:11.732 --> 00:03:16.230
이런 문제는 명확하고
모두가 동의할 만한 기준이 있고

00:03:16.254 --> 00:03:18.493
자연법칙에 따라 판단하면 되죠.

00:03:18.517 --> 00:03:21.911
하지만 인간 사회의 
일을 판단하는 데에는

00:03:21.935 --> 00:03:25.898
그런 기준이 존재하지 않습니다.

00:03:26.242 --> 00:03:30.159
더욱이 요즘 소프트웨어는
점점 더 강력해지고 있지만

00:03:30.183 --> 00:03:33.956
동시에 더욱 불투명해지고
이해하기 힘들어지고 있습니다.

00:03:34.542 --> 00:03:36.582
지난 십 년 동안

00:03:36.606 --> 00:03:39.335
복합 알고리즘에는
굉장한 진전이 있었죠.

00:03:39.359 --> 00:03:41.349
사람 얼굴을 인식할 수 있고

00:03:41.985 --> 00:03:44.040
손글씨도 읽어내며

00:03:44.436 --> 00:03:46.502
신용카드 사기를 간파하고

00:03:46.526 --> 00:03:47.715
스팸을 막고

00:03:47.739 --> 00:03:49.776
번역도 할 수 있어요.

00:03:49.800 --> 00:03:52.374
영상 의료 사진에서
종양을 식별할 수 있고

00:03:52.398 --> 00:03:54.863
인간과 체스나 바둑을 두어
이길 수 있습니다.

00:03:55.264 --> 00:03:59.768
이 진전에는 '기계 학습'이라는
기법의 공이 큽니다.

00:04:00.175 --> 00:04:03.362
기계 학습은 기존의 
프로그래밍과는 다릅니다. 

00:04:03.386 --> 00:04:06.971
기존에는 컴퓨터에게 명확하고
자세한 지시를 내려야 했죠.

00:04:07.378 --> 00:04:11.560
이제는 시스템을 만든 뒤에
대량의 데이터를 입력합니다.

00:04:11.584 --> 00:04:13.320
우리의 디지털 시대에 생성된

00:04:13.320 --> 00:04:15.542
구조화되지 않은 데이터들까지
포함해서 말이죠.

00:04:15.566 --> 00:04:18.296
시스템은 이 데이터를
헤쳐나가면서 학습합니다.

00:04:18.669 --> 00:04:20.195
또 중요한 사실이 있는데

00:04:20.219 --> 00:04:24.599
이 시스템은 정답을 단정짓는
논리로 작동하지 않습니다.

00:04:24.623 --> 00:04:27.582
확정적인 정답을 내기보다는
확률적 결론을 내리죠.

00:04:27.606 --> 00:04:31.089
"이게 당신이 찾던 결과일
가능성이 높습니다."라고요.

00:04:32.023 --> 00:04:35.093
장점은 이 방식이
정말 강력하다는 겁니다.

00:04:35.117 --> 00:04:36.977
구글의 인공지능 시스템
책임자는 이를 두고

00:04:36.977 --> 00:04:39.524
'정보의 과잉 효율성'이라고 표현했죠.

00:04:39.791 --> 00:04:41.144
단점은

00:04:41.738 --> 00:04:44.809
그 시스템이 무엇을 배웠는지
우리는 알 수 없다는 겁니다.

00:04:44.833 --> 00:04:46.420
사실 장점이기도 하죠.

00:04:46.946 --> 00:04:50.744
컴퓨터에 명령을 내린다기 보다

00:04:51.200 --> 00:04:55.264
우리가 이해하거나 통제하지 
못하는 강아지 같은 기계를

00:04:55.288 --> 00:04:57.659
훈련시키는 거라고 할 수 있죠.

00:04:58.362 --> 00:04:59.913
그런데 문제가 있어요.

00:05:00.427 --> 00:05:04.689
이 인공지능이 잘못된 것을
학습할 때 문제가 발생하죠.

00:05:04.713 --> 00:05:08.253
그리고 잘 학습했어도 문제가 됩니다.

00:05:08.277 --> 00:05:11.729
왜냐하면 주관적인 문제에서는
뭐가 뭔지도 모르니까요.

00:05:11.729 --> 00:05:14.508
무슨 생각으로 이런 판단을 
했는지 알 수가 없는 거죠.

00:05:15.493 --> 00:05:19.176
채용 알고리즘을 생각해 보세요.

00:05:20.123 --> 00:05:24.434
사람을 가려내는
기계 학습 시스템입니다.

00:05:25.052 --> 00:05:28.631
이런 시스템은 이전 직원들 데이터를
바탕으로 훈련되었을 것이고

00:05:28.655 --> 00:05:31.246
성과가 좋을 만한 직원들을

00:05:31.270 --> 00:05:34.308
미리 찾아서 고용하려고 하겠죠.

00:05:34.814 --> 00:05:35.967
괜찮아 보입니다.

00:05:35.991 --> 00:05:37.990
저는 인사부와 임원들

00:05:38.014 --> 00:05:41.139
회사 고위직들이 한데 모인
그런 채용 시스템 도입을

00:05:41.163 --> 00:05:43.989
주제로 한 컨퍼런스에 
참석한 적이 있습니다.

00:05:43.989 --> 00:05:45.622
그 분들은 정말 들떠 있었죠.

00:05:45.646 --> 00:05:50.299
그들은 이 시스템이 편파적이지 않고
객관적인 채용을 가능케 하고

00:05:50.323 --> 00:05:53.323
편견을 가진 인간 관리자보다
여성과 소수자에게

00:05:53.347 --> 00:05:55.535
더 많은 기회를 주리라 기대했습니다.

00:05:55.559 --> 00:05:58.402
사람에 의한 고용은 편향됐죠.

00:05:59.099 --> 00:06:00.284
저도 알아요.

00:06:00.308 --> 00:06:03.313
프로그래머로서의 제 초기
직장 중 하나에서

00:06:03.337 --> 00:06:07.205
제 직속 상사는 가끔
제가 아침 일찍부터

00:06:07.229 --> 00:06:10.982
밤 늦게까지 일하던 자리로 와서

00:06:11.006 --> 00:06:14.068
"제이넵, 점심 먹으러 갑시다"
라고 말했었어요.

00:06:14.724 --> 00:06:16.891
시계를 보고 의아해했죠.

00:06:16.915 --> 00:06:19.044
오후 4시에 점심이라니?

00:06:19.068 --> 00:06:22.162
전 돈이 없었으니
점심 사 준다니까 항상 갔죠.

00:06:22.618 --> 00:06:24.685
나중에 무슨 이유인지 알게 되었어요.

00:06:24.709 --> 00:06:29.255
제 직속 상사는
그녀가 고용한 프로그래머가

00:06:29.279 --> 00:06:33.472
청바지에 운동화를 신고 
일터에 오는 십대 학생이란 걸

00:06:33.472 --> 00:06:36.346
말하지 않았던 거죠.

00:06:37.174 --> 00:06:39.716
일은 잘 하고 있었지만
입은 옷과 나이와 성별이

00:06:39.716 --> 00:06:41.099
'적절치 않았던' 거죠.

00:06:41.123 --> 00:06:44.469
그래서 나이와 인종을
가리지 않은 채용은

00:06:44.493 --> 00:06:46.358
저에게는 좋아 보입니다.

00:06:47.031 --> 00:06:50.372
하지만 이 시스템이 양날의 
칼인 이유는 따로 있죠.

00:06:50.968 --> 00:06:56.759
현재 이런 시스템은 여러분이 
공개하지 않은 개인 정보도

00:06:56.783 --> 00:07:01.005
여러분이 남긴 정보 부스러기에서
추론할 수 있기 때문입니다.

00:07:01.506 --> 00:07:04.433
여러분의 성적 취향을 추측할 수 있고

00:07:04.994 --> 00:07:06.300
성격 특성과

00:07:06.859 --> 00:07:08.232
정치 성향까지 추측하죠.

00:07:08.830 --> 00:07:12.515
상당히 높은 수준의 적중률로요.

00:07:13.362 --> 00:07:15.940
공개하지도 않은 정보를 알아냅니다.

00:07:15.964 --> 00:07:17.555
추론한 것이죠.

00:07:17.579 --> 00:07:20.840
제 친구 하나가 SNS 자료를 통해

00:07:20.864 --> 00:07:25.875
질병이나 산후 우울증 가능성을 
예측하는 시스템을 개발했습니다.

00:07:26.676 --> 00:07:28.103
결과는 인상적이었습니다.

00:07:28.492 --> 00:07:31.849
그녀가 만든 시스템은
증상이 시작되기 몇 달 전에

00:07:31.873 --> 00:07:35.776
우울증 발생 가능성을
예측할 수 있었어요.

00:07:35.800 --> 00:07:37.173
몇 달 전에요.

00:07:37.197 --> 00:07:39.443
증상도 없이 예측한 것이죠.

00:07:39.467 --> 00:07:44.279
그녀는 이게 우울증 예방에 
사용될 거라 생각했어요. 좋은 일이죠?

00:07:44.911 --> 00:07:47.141
하지만 지금 그 기술은 
채용 시스템에 적용되었습니다.

00:07:48.027 --> 00:07:51.073
저는 인사 담당자들이 모인 
아까 그 학회에서

00:07:51.097 --> 00:07:56.816
대기업의 고위직 관리자에게 
다가가서 이렇게 말했습니다.

00:07:56.816 --> 00:08:00.408
"당신이 모르는 사이에 프로그램이

00:08:00.432 --> 00:08:06.981
우울증 발병 가능성이 높은 사람들을
가려내고 있다면 어떻게 될까요?

00:08:07.761 --> 00:08:11.137
지금은 우울증이 없지만,
미래에 위험 가능성이 있죠.

00:08:11.923 --> 00:08:13.983
지금은 임신하지 않았지만

00:08:13.983 --> 00:08:17.939
1,2년 내에 출산 휴가를 낼 만한
여성들을 미리 가려내고 있다면요?

00:08:18.844 --> 00:08:24.480
직장 문화에 적합하다는 이유로
공격적인 사람만을 고용한다면요?"

00:08:25.173 --> 00:08:27.864
이건 남녀 성비만으로는
판단할 수 없어요.

00:08:27.888 --> 00:08:29.390
성비는 이미 균형잡혀 있겠죠.

00:08:29.414 --> 00:08:32.971
그리고 전통적인 프로그래밍 
방식이 아닌 기계 학습이기 때문에

00:08:32.995 --> 00:08:37.902
'우울증 위험성 높음'이라는
변수명은 존재하지 않아요.

00:08:37.926 --> 00:08:39.759
'임신 가능성 높음'

00:08:39.783 --> 00:08:41.517
'남성 공격성 척도'도 없죠.

00:08:41.995 --> 00:08:45.674
시스템이 어떤 기준으로
선택하는지 모르는 것은 물론

00:08:45.698 --> 00:08:48.021
어디부터 봐야 할지조차 모르는 거죠.

00:08:48.045 --> 00:08:49.291
블랙박스입니다.

00:08:49.315 --> 00:08:52.122
그 예측 능력을 우리는 알지 못하죠.

00:08:52.486 --> 00:08:54.109
그래서 제가 물었죠.

00:08:54.109 --> 00:08:58.552
"블랙박스가 뭔가 이상한 짓을 못하도록
어떤 안전 장치를 마련하시겠어요?"

00:09:00.863 --> 00:09:04.741
제가 엄청난 사건을 일으킨 
것처럼 쳐다보더라고요.

00:09:04.765 --> 00:09:06.013
(웃음)

00:09:06.037 --> 00:09:08.078
저를 빤히 쳐다보곤 이렇게 말하더군요.

00:09:08.556 --> 00:09:12.889
"이것에 대해서는 더 이상
듣고 싶지 않네요."

00:09:13.458 --> 00:09:15.492
그리고 돌아서서 가 버렸어요.

00:09:16.064 --> 00:09:18.460
그렇다고 무례했던 건 아니에요.

00:09:18.460 --> 00:09:23.882
자기가 모르는 건 자기 문제가 아니니 
신경쓰게 하지 말라는 경고였죠.

00:09:23.906 --> 00:09:25.152
(웃음)

00:09:25.862 --> 00:09:31.831
시스템은 인간 관리자와 달리
편견이 없을 수도 있어요.

00:09:31.852 --> 00:09:33.998
인사 업무에 돈도 덜 써도 되겠죠.

00:09:34.573 --> 00:09:39.073
하지만 이 시스템은
지속적이고 암묵적으로

00:09:39.099 --> 00:09:43.312
우울증 위험이 높은 사람들의
고용 기회를 박탈할 수 있습니다.

00:09:43.753 --> 00:09:48.679
이해하지도 못하는 기계에게
의사결정을 맡기면서

00:09:48.682 --> 00:09:52.646
우리 모르게 기회를 박탈하는 게
바람직한 사회인가요?

00:09:53.265 --> 00:09:54.723
또 다른 문제도 있습니다.

00:09:55.314 --> 00:09:59.766
이 시스템은 우리 인간의 행동방식이
만들어 낸 정보들로 학습을 합니다.

00:09:59.790 --> 00:10:01.606
인간이 남긴 흔적들이죠.

00:10:02.188 --> 00:10:05.996
그러면 우리의 편견을 
그대로 반영하게 되고

00:10:06.020 --> 00:10:12.163
시스템은 그 편견을 익히고 확대시켜
우리에게 결과로 보여주게 됩니다. 

00:10:12.163 --> 00:10:13.546
우리는 그걸 합리화하죠.

00:10:13.546 --> 00:10:16.518
"지금 객관적이고 공정한 계산 
결과를 뽑는 중이야~" 라면서요.

00:10:18.314 --> 00:10:20.991
연구 결과에 따르면
구글 검색의 경우에는

00:10:22.134 --> 00:10:27.447
여성은 남성보다 고소득 구인 
광고에 덜 노출된다고 합니다.

00:10:28.463 --> 00:10:30.993
그리고 흑인계 이름으로 검색해 보면

00:10:31.017 --> 00:10:35.723
범죄 전과를 시사하는 광고가 
더 많이 나온다고 해요.

00:10:35.747 --> 00:10:37.314
전과가 없는데도 말이죠.

00:10:38.693 --> 00:10:42.242
이런 암묵적 편견과
블랙박스 속 알고리즘은

00:10:42.266 --> 00:10:46.239
연구자들이 모두 밝혀낼 수 없어
우리도 모르는 사이에

00:10:46.263 --> 00:10:48.924
개인의 삶에 영향을 미칠 수 있죠.

00:10:49.958 --> 00:10:54.117
위스콘신에서는 어느 피고인이
경찰 수사를 거부한 죄로

00:10:54.141 --> 00:10:55.496
6년형을 선고받았습니다.

00:10:56.824 --> 00:10:58.060
알고 계실지 모르겠지만

00:10:58.060 --> 00:11:02.032
알고리즘이 가석방과 형량 판결에
점점 더 많이 사용되고 있습니다.

00:11:02.056 --> 00:11:05.011
그는 대체 그런 결정이 
어떻게 나오는지 알고 싶었죠.

00:11:05.795 --> 00:11:07.460
그건 상업적인 블랙박스였고

00:11:07.484 --> 00:11:11.689
개발사는 알고리즘이 법정에서
심판받는 것을 거부했죠.

00:11:12.396 --> 00:11:16.562
하지만 ProPublica라는
비영리 수사 기구가

00:11:16.562 --> 00:11:19.968
각종 데이터로 그 알고리즘을 검사했고

00:11:19.992 --> 00:11:25.961
밝혀진 사실은 그 알고리즘 예측 성능이
우연과 별다르지 않은 수준이었으며

00:11:25.985 --> 00:11:30.401
흑인 피고를 잠재적 범죄자로
낙인찍는 확률이

00:11:30.425 --> 00:11:34.320
백인 피고에 비해 두 배나
높다는 것이었습니다.

00:11:35.891 --> 00:11:37.525
다른 경우도 살펴 볼까요.

00:11:38.103 --> 00:11:41.955
오른쪽 여성은 플로리다 브로워드 
카운티의 학교에 다니는 

00:11:41.979 --> 00:11:44.054
교회 동생을 데리러 갈
약속에 늦는 바람에

00:11:44.757 --> 00:11:47.113
친구와 함께 뛰어가고 있었죠.

00:11:47.137 --> 00:11:51.236
그러다 어느 집 현관에 있던
자전거와 스쿠터를 발견하고는

00:11:51.260 --> 00:11:52.746
어리석게도 그걸 집어 탔어요.

00:11:52.746 --> 00:11:55.515
그들이 속도를 내며 달아날 때
한 여자가 뛰어나와 소리쳤죠.

00:11:55.539 --> 00:11:57.744
"우리 애 자전거로 뭐 하는 거야!"

00:11:57.768 --> 00:12:01.062
그들은 자전거를 버리고 걸어서 
달아났지만 체포되었습니다.

00:12:01.086 --> 00:12:04.723
그들이 잘못했고, 어리석긴 했어요.
그런데 겨우 열여덟 살이었죠.

00:12:04.747 --> 00:12:07.291
그녀는 청소년 범죄 
전과가 몇 건 있었죠.

00:12:07.808 --> 00:12:14.763
한편, 이 남성은 마트에서 85달러어치
좀도둑질을 하다가 체포되었습니다.

00:12:14.807 --> 00:12:15.941
비슷한 범죄죠.

00:12:16.766 --> 00:12:21.325
하지만 그에게는 두 번의 
무장강도 전과가 있었어요.

00:12:21.955 --> 00:12:25.437
그런데 알고리즘은 남자가 아니라
여자를 고위험군으로 분류했죠.

00:12:26.746 --> 00:12:27.746
2년 뒤에

00:12:27.746 --> 00:12:30.644
ProPublica가 조사해보니
여성은 재범하지 않았습니다.

00:12:30.644 --> 00:12:33.194
전과가 있었기에 취직이 어렵긴 했죠.

00:12:33.218 --> 00:12:35.294
반면 남자는 재범하였고

00:12:35.318 --> 00:12:39.154
그 이후 저지른 범죄로
현재 8년을 복역 중입니다.

00:12:40.088 --> 00:12:43.457
우리는 블랙박스를 잘 검수해서

00:12:43.481 --> 00:12:46.096
엉뚱한 권한을 갖지 않도록
분명히 해야 합니다.

00:12:46.120 --> 00:12:48.999
(박수)

00:12:50.087 --> 00:12:54.329
검수는 중요하고 또 유효하지만
모든 문제를 해결하진 못합니다.

00:12:54.353 --> 00:12:57.101
페이스북의 강력한 뉴스 피드
알고리즘을 살펴 보죠.

00:12:57.125 --> 00:12:59.622
모든 것을 순위대로 나열하고

00:12:59.622 --> 00:13:04.276
팔로우하는 모든 친구와 페이지에서
무엇을 보여 줄지를 결정하죠.

00:13:04.898 --> 00:13:07.173
아기 사진을 또 봐야 할까요?

00:13:07.197 --> 00:13:08.393
(웃음)

00:13:08.417 --> 00:13:11.013
아니면 지인의 삐진 듯한 글?

00:13:11.449 --> 00:13:13.305
어렵지만 중요한 뉴스 기사?

00:13:13.329 --> 00:13:14.811
정답은 없습니다.

00:13:14.835 --> 00:13:17.494
알고리즘은 페이스북 활동에
최적화되어 있습니다.

00:13:17.518 --> 00:13:18.933
좋아요, 공유, 댓글이죠.

00:13:20.168 --> 00:13:22.864
2014년 8월에

00:13:22.888 --> 00:13:25.550
백인 경찰관이
범행이 불확실한 상황에서

00:13:25.574 --> 00:13:29.991
십대 흑인에게 발포하여 살해한
사건 후, 미주리 주 퍼거슨에서

00:13:30.015 --> 00:13:31.585
시위가 일어났습니다.

00:13:31.974 --> 00:13:33.981
시위 뉴스는 알고리즘 필터가 없는

00:13:34.005 --> 00:13:36.690
트위터 글목록에는 나타났지만

00:13:36.714 --> 00:13:38.664
페이스북에는 흔적이 없었습니다.

00:13:39.182 --> 00:13:42.906
페이스북 친구 때문인가 생각하고
알고리즘을 해제해 보았습니다.

00:13:43.472 --> 00:13:48.390
페이스북은 계속 알고리즘이 추천하는
글을 보여주려고 해서 좀 까다로웠죠.

00:13:48.404 --> 00:13:51.142
친구들이 시위 이야기를 
하지 않던 게 아니었습니다.

00:13:51.142 --> 00:13:53.175
알고리즘이 전달을
막고 있었던 겁니다.

00:13:53.199 --> 00:13:56.241
전 이걸 조사하고는
광범위한 문제임을 알았습니다.

00:13:56.265 --> 00:14:00.078
퍼거슨 사건은 알고리즘이
선호할 만한 게 아니죠.

00:14:00.102 --> 00:14:01.273
좋아요가 적습니다.

00:14:01.297 --> 00:14:02.849
댓글 남기기도 껄끄러운데

00:14:03.500 --> 00:14:05.706
누가 좋아요를 누르겠어요?

00:14:05.730 --> 00:14:07.101
좋아요와 댓글이 적어서

00:14:07.125 --> 00:14:10.417
알고리즘이 더 보여주고
싶지 않아했고

00:14:10.441 --> 00:14:12.223
결국 우리가 보지 못한 겁니다.

00:14:12.946 --> 00:14:16.524
대신 그 주에 페이스북 
알고리즘이 선호한 것은

00:14:16.524 --> 00:14:18.856
루게릭 병 모금을 위한
아이스 버킷 챌린지였습니다.

00:14:18.856 --> 00:14:22.512
얼음물 세례를 맞고 기부를
한다는 취지 자체는 괜찮죠.

00:14:22.536 --> 00:14:25.440
하지만 알고리즘이 과하게 
좋아할 만한 것이었습니다.

00:14:25.440 --> 00:14:27.832
기계가 결정을 내려 버린 거죠.

00:14:27.856 --> 00:14:31.353
페이스북이 유일한 소통 창구였다면

00:14:31.377 --> 00:14:35.792
중요하지만 까다로운 
쟁점이 묻힐 뻔했습니다.

00:14:36.117 --> 00:14:39.914
마지막으로, 이 시스템은
인간과는 다른 방식으로

00:14:39.938 --> 00:14:42.674
오류를 범할 수 있습니다.

00:14:42.698 --> 00:14:45.620
퀴즈 프로그램에서 인간 참가자를
누르고 우승을 차지한

00:14:45.644 --> 00:14:48.772
IBM의 인공지능 왓슨을 기억하시나요?

00:14:49.131 --> 00:14:50.559
대단한 실력이었죠.

00:14:50.583 --> 00:14:54.152
하지만 왓슨이 맞이한
마지막 문제를 보시면

00:14:54.659 --> 00:14:57.165
"이 도시 최대 공항 이름은
2차 대전 영웅을,

00:14:57.165 --> 00:15:00.067
두 번째로 큰 공항은
2차 대전 전투를 따서 지어졌다.

00:15:00.067 --> 00:15:01.529
(Jeopardy 대기 음악)

00:15:01.582 --> 00:15:02.764
답은 시카고죠.

00:15:02.788 --> 00:15:04.498
인간 참가자는 모두 맞췄습니다.

00:15:04.697 --> 00:15:09.045
하지만 왓슨은 '토론토'라고 썼죠.

00:15:09.069 --> 00:15:10.887
주제가 미국 도시였는데도 말이죠.

00:15:11.596 --> 00:15:14.497
또한 이 뛰어난 시스템은

00:15:14.521 --> 00:15:18.172
초등학교 2학년생도
하지 않을 실수를 저질렀죠.

00:15:18.823 --> 00:15:25.062
인공지능은 인간의 오류와는 
다른 방식으로 오작동할 수 있고

00:15:25.080 --> 00:15:28.030
그래서 우리가 예상하거나
대비하기 어렵습니다.

00:15:28.054 --> 00:15:31.692
능력에 맞는 직업을 갖지 
못한다면 기분 나쁠 거예요.

00:15:31.716 --> 00:15:35.443
그런데 그 이유가 프로그램 함수의
과부하 오류 때문이라면

00:15:35.467 --> 00:15:36.899
몇 배는 더 기분 나쁘겠죠.

00:15:36.923 --> 00:15:38.502
(웃음)

00:15:38.526 --> 00:15:41.312
2010년 5월에

00:15:41.336 --> 00:15:48.450
월가의 매도 알고리즘의 피드백 
반복문 오류로 주가가 폭락했고

00:15:48.456 --> 00:15:52.640
36분 만에 1조 달러어치의
가치가 사라진 일이 있었죠.

00:15:53.722 --> 00:15:57.659
살상 무기의 경우에 '오류'가
일어나면 어떻게 될지

00:15:57.659 --> 00:15:59.522
생각하고 싶지도 않습니다.

00:16:01.894 --> 00:16:05.684
인간의 결정에는 결함이 많죠.

00:16:05.708 --> 00:16:07.884
의사결정과 보안

00:16:07.908 --> 00:16:11.401
법정, 언론, 전쟁에서

00:16:11.425 --> 00:16:14.463
모두 실수가 일어나지만
저는 그래야 한다고 생각합니다.

00:16:14.487 --> 00:16:18.008
우리는 어려운 문제를
피할 수 없습니다.

00:16:18.596 --> 00:16:22.112
기계에게 책임을 떠넘겨서는 안 됩니다.

00:16:22.676 --> 00:16:26.884
(박수)

00:16:29.089 --> 00:16:33.536
인공지능이 '윤리적 문제의 
면죄부'를 주지는 않습니다.

00:16:34.742 --> 00:16:38.233
데이터 과학자인 프레드 베넨슨은 
이를 두고 '논리 세탁'이라 표현했죠.

00:16:38.233 --> 00:16:39.696
정반대 태도가 필요합니다.

00:16:39.696 --> 00:16:44.948
알고리즘을 의심하고 조사하고
검수하는 능력을 길러야 합니다.

00:16:45.380 --> 00:16:48.578
알고리즘에 대한 회계와 감사
그리고 투명성 제고 방법을

00:16:48.602 --> 00:16:51.047
구축해야 합니다.

00:16:51.380 --> 00:16:54.614
인간 사회의 가치 판단 문제에 
수학과 컴퓨터를 도입한다고 해서

00:16:54.638 --> 00:16:57.608
객관적인 일이 되지는 않는다는 걸

00:16:57.632 --> 00:17:00.016
받아들여야 합니다.

00:17:00.040 --> 00:17:03.673
오히려 인간 문제의 복잡성이
알고리즘을 주관적으로 만들죠.

00:17:04.148 --> 00:17:09.665
물론 더 나은 의사결정을 위해서라면
컴퓨터를 이용할 수도 있을 겁니다.

00:17:09.697 --> 00:17:15.029
하지만 우리 판단의 도덕적 책임은
우리 스스로가 짊어져야 합니다.

00:17:15.053 --> 00:17:17.871
알고리즘은 그 틀 안에서만 
이용되어야 할 뿐이고

00:17:17.895 --> 00:17:25.220
우리의 도덕적 책임을 다른 쪽에
전가하는 수단이 되어서는 안되죠.

00:17:25.807 --> 00:17:28.416
인공지능의 시대에는

00:17:28.440 --> 00:17:31.861
인간 가치와 윤리가

00:17:31.885 --> 00:17:34.032
더욱더 중요합니다.

00:17:34.056 --> 00:17:35.210
감사합니다.

00:17:35.234 --> 00:17:40.254
(박수)

