WEBVTT
Kind: captions
Language: it

00:00:00.000 --> 00:00:07.000
Traduttore: Federico MINELLE
Revisore: Rachael Gwinn

00:00:12.559 --> 00:00:16.681
Ho iniziato il mio primo lavoro 
come programmatrice di computer

00:00:16.705 --> 00:00:18.661
nel mio primo anno di college -

00:00:18.685 --> 00:00:20.192
sostanzialmete un'adolescente.

00:00:20.709 --> 00:00:24.101
Subito dopo ho cominciato a lavorare,
scrivendo software in una azienda,

00:00:24.619 --> 00:00:28.254
un manager che lavorava nell'azienda
mi venne a trovare dove stavo,

00:00:28.278 --> 00:00:29.546
e mi susssurrò,

00:00:30.049 --> 00:00:32.910
"Può lui dirmi se sto mentendo?"

00:00:33.626 --> 00:00:35.703
Non c'era nessun altro nella stanza.

00:00:36.852 --> 00:00:41.241
"Chi può dire se stai mentendo?
E perché sussurriamo?"

00:00:42.086 --> 00:00:45.193
Il manager indicò il computer 
nella stanza.

00:00:45.217 --> 00:00:48.313
"Può lui dirmi se sto mentendo?"

00:00:49.433 --> 00:00:53.795
Ebbene, quel manager aveva una relazione 
con la segretaria alla reception.

00:00:53.819 --> 00:00:54.931
(Risate)

00:00:54.955 --> 00:00:56.721
Ed ero ancora una adolescente.

00:00:57.267 --> 00:00:59.286
Così gli sussurrai-urlai,

00:00:59.310 --> 00:01:02.934
"Si, il computer può dire 
se stai mentendo."

00:01:02.958 --> 00:01:04.764
(Risate)

00:01:04.788 --> 00:01:07.711
Bene, io risi, ma in effetti,
la risata era nei mei confronti.

00:01:07.735 --> 00:01:11.003
Oggi, vi sono sistemi di calcolo

00:01:11.027 --> 00:01:14.575
che possono scoprire gli stati emotivi
e pure la menzogna

00:01:14.599 --> 00:01:16.643
tramite la elaborazione dei volti umani.

00:01:17.068 --> 00:01:21.221
I pubblicitari ed anche i governi
sono molto interessati.

00:01:21.959 --> 00:01:24.001
Sono diventata programmatrice di computer

00:01:24.025 --> 00:01:27.138
perchè ero uno di quei giovani
pazzi per matematica e scienze

00:01:27.762 --> 00:01:30.870
Ma lungo il mio percorso
ho imparato a conoscere le armi nucleari,

00:01:30.894 --> 00:01:33.846
e mi sono proprio preoccupata 
dell'etica della scienza.

00:01:33.870 --> 00:01:35.074
Ero preoccupata.

00:01:35.098 --> 00:01:37.739
Tuttavia, a causa della 
mia situazione familiare,

00:01:37.763 --> 00:01:41.061
dovevo iniziare a lavorare
al più presto.

00:01:41.085 --> 00:01:44.384
Così mi sono detta, hey,
scegliamo un settore tecnico

00:01:44.408 --> 00:01:46.204
dove posso trovare facilmente 
un lavoro

00:01:46.228 --> 00:01:50.246
e dove non devo aver a che fare 
con qualche fastidiosa questione etica.

00:01:50.842 --> 00:01:52.371
Così ho scelto i computer.

00:01:52.395 --> 00:01:53.499
(Risate)

00:01:53.523 --> 00:01:56.933
Bene, ah, ah!
Tutti ridono di me.

00:01:56.957 --> 00:01:59.711
Ora, gli scienziati del computer
costruicono piattaforme

00:01:59.735 --> 00:02:03.944
che controllano ciò che un miliardo 
di persone vedono ogni giorno.

00:02:04.872 --> 00:02:08.694
Stanno sviluppando auto
che potrebbero decidere chi investire.

00:02:09.527 --> 00:02:12.740
Stanno anche costruendo macchine, armi,

00:02:12.764 --> 00:02:15.049
che possono uccidere persone in guerra.

00:02:15.073 --> 00:02:17.844
C'è sempre l'etica alla fine.

00:02:19.003 --> 00:02:21.061
L'intelligenza delle macchine è qui.

00:02:21.643 --> 00:02:25.117
Ora utilizziamo il computer 
per prendere ogni tipo di decisioni,

00:02:25.141 --> 00:02:27.027
ma anche nuovi tipi di decisioni.

00:02:27.051 --> 00:02:32.223
Stiamo ponendo al computer domande
che non hanno una sola risposta giusta,

00:02:32.247 --> 00:02:33.449
che sono soggettive

00:02:33.473 --> 00:02:35.798
e sono <i>aperte</i> e cariche di valore.

00:02:35.822 --> 00:02:37.580
Domandiamo cose come,

00:02:37.604 --> 00:02:39.254
"Chi dovrebbe essere assunto?"

00:02:39.916 --> 00:02:42.675
"Quale update e di quale amico
dovrebbe essere mostrato?"

00:02:42.699 --> 00:02:45.205
"Quale pregiudicato sarà più
probabilmente recidivo?"

00:02:45.334 --> 00:02:48.388
"Quale notizia o film
dovrebbe essere consigliato alla gente?"

00:02:48.412 --> 00:02:51.784
Certo, stiamo usando 
il computer da un po',

00:02:51.808 --> 00:02:53.325
ma questo è diverso.

00:02:53.349 --> 00:02:55.416
Questa è un svolta storica,

00:02:55.440 --> 00:03:00.777
perchè non possiamo poggiarci sul computer
per queste decisioni soggettive

00:03:00.801 --> 00:03:06.221
allo stesso modo in cui si usa il computer
per far volare gli aerei, costruire ponti,

00:03:06.245 --> 00:03:07.504
andare sulla luna.

00:03:08.269 --> 00:03:11.528
Sono più sicuri gli aerei?
Vacillano e cadono i ponti?

00:03:11.552 --> 00:03:16.050
Qui abbiamo chiari livelli di riferimento,
su cui concordiamo,

00:03:16.074 --> 00:03:18.313
ed abbiamo le leggi naturali per guidarci.

00:03:18.337 --> 00:03:21.731
Non abbiamo tali riferimenti 
per decidere

00:03:21.755 --> 00:03:25.718
nelle complicate faccende umane.

00:03:25.742 --> 00:03:29.979
Per rendere le cose più complicate,
il software sta diventando più potente,

00:03:30.003 --> 00:03:33.776
ma anche meno trasparente e più complesso.

00:03:34.362 --> 00:03:36.402
Recentemente, nell'ultima decennio,

00:03:36.426 --> 00:03:39.155
algoritmi complessi hanno fatto
notevoli passi avanti.

00:03:39.179 --> 00:03:41.169
Possono riconoscere le facce umane.

00:03:41.805 --> 00:03:43.860
Possono decifrare la scrittura manuale.

00:03:44.136 --> 00:03:47.509
possono individuare le frodi 
sulle carte di credito e bloccare lo spam

00:03:47.509 --> 00:03:49.590
e possono tradurre le lingue.

00:03:49.590 --> 00:03:52.504
Possono individuare i tumori 
nelle diagnostica per immagini.

00:03:52.504 --> 00:03:54.753
Possono battere gli umani a scacchi
e nel <i>GO</i>

00:03:55.084 --> 00:03:59.588
Molti di questi progressi derivano
dal metodo di "apprendimento automatico".

00:03:59.995 --> 00:04:03.322
L'apprendimento automatico è diverso
dalla tradizionale programmazione,

00:04:03.322 --> 00:04:06.791
dove si danno al computer istruzioni
dettagliate, precise ed accurate.

00:04:07.198 --> 00:04:11.380
Sembra di più come se forniste
al sistema molti dati,

00:04:11.404 --> 00:04:13.054
Inclusi dati non strutturati,

00:04:13.054 --> 00:04:15.462
del tipo che generiamo
nella nostre vite digitali.

00:04:15.462 --> 00:04:18.116
E il sistema impara 
maneggiando quei dati.

00:04:18.489 --> 00:04:20.015
Ed anche, crucialmente,

00:04:20.039 --> 00:04:24.419
quei sistemi non lavorano 
con la logica di una sola risposta.

00:04:24.443 --> 00:04:27.522
Non forniscono una semplice risposta;
è piuttosto probabilistica:

00:04:27.522 --> 00:04:30.909
"Questo è probabilmente 
quello che state cercando."

00:04:31.843 --> 00:04:34.913
Ora, la parte positiva è: 
questo metodo è veramente potente.

00:04:34.937 --> 00:04:37.013
Il capo sistema AI 
di Google lo ha chiamato

00:04:37.037 --> 00:04:39.234
"la irragionevole efficacia dei dati."

00:04:39.611 --> 00:04:40.964
La parte negativa è,

00:04:41.558 --> 00:04:44.629
non capiamo veramente 
cosa il sistema ha imparato.

00:04:44.653 --> 00:04:46.240
Infatti, questa è la sua forza.

00:04:46.766 --> 00:04:50.564
Ciò è meno simile a dare
istruzioni ad un computer;

00:04:51.020 --> 00:04:55.084
è più simile all'istruire
un cucciolo-macchina

00:04:55.108 --> 00:04:57.479
che noi non capiamo 
o controlliamo veramente

00:04:58.182 --> 00:04:59.733
Questo è il nostro problema.

00:05:00.247 --> 00:05:04.509
Il problema è quando questi sistemi
di intelligenza artificiale lavorano male.

00:05:04.533 --> 00:05:08.073
Vi è anche il problema 
di quando lavorano bene,

00:05:08.097 --> 00:05:11.725
perchè non sappiamo chi è chi
quando il problema è soggettivo.

00:05:11.749 --> 00:05:14.088
Non sappiamo come il computer ragioni.

00:05:15.313 --> 00:05:18.996
Così, considerate un algoritmo 
per le assunzioni--

00:05:19.943 --> 00:05:24.254
un sistema per assumere la gente,
usando l'apprendimento automatico.

00:05:24.872 --> 00:05:28.451
Un tale sistema sarebbe stato formato
sui dati dei precedenti assunti

00:05:28.475 --> 00:05:31.066
e istruito per trovare ed assumere

00:05:31.090 --> 00:05:34.128
le persone simili a quelli
più performanti in azienda.

00:05:34.634 --> 00:05:37.534
Sembra giusto.
Una volta ho partecipato ad una conferenza

00:05:37.534 --> 00:05:41.089
che coinvolgeva i responsabili 
delle Risorse Umane e i manager aziendali,

00:05:41.089 --> 00:05:44.309
persone ad alto livello,
che usavano questo sistema per assumere.

00:05:44.309 --> 00:05:45.642
Erano super entusiasti.

00:05:45.642 --> 00:05:50.119
Pensavano che ciò avrebbe portato ad
assunzioni più oggettive, meno distorte,

00:05:50.143 --> 00:05:53.143
e dato alle donne e alle minoranze
una migliore opportunità

00:05:53.167 --> 00:05:55.355
rispetto a manager umani influenzabili.

00:05:55.379 --> 00:05:58.222
E guarda - le assunzioni fatte 
dagli uomini sono distorte.

00:05:58.919 --> 00:06:00.104
Lo so.

00:06:00.128 --> 00:06:03.133
Voglio dire, in uno dei miei primi lavori,
come programmatrice,

00:06:03.157 --> 00:06:07.025
la manager da cui dipendevo qualche volta
scendeva dove stavo

00:06:07.049 --> 00:06:10.802
molto presto alla mattina 
o molto tardi nel pomeriggio,

00:06:10.826 --> 00:06:13.888
e mi diceva, "Zeynep,
andiamo a pranzo!"

00:06:14.544 --> 00:06:16.711
Ero perplessa per la strana tempistica.

00:06:16.735 --> 00:06:18.864
Sono le 4 del pomeriggio. Pranzo?

00:06:18.888 --> 00:06:21.982
Ero al verde, ed è un pranzo gratis.
Quindi andavo sempre.

00:06:22.438 --> 00:06:24.505
Più tardi capii cosa stava accadendo.

00:06:24.529 --> 00:06:29.075
La manager da cui dipendevo
non aveva confessato ai livelli superiori

00:06:29.099 --> 00:06:32.212
che il programmatore da lei assunto 
per un importante lavoro era adolescente

00:06:32.236 --> 00:06:36.166
che indossava jeans e <i>sneaker</i> al lavoro.

00:06:36.994 --> 00:06:39.196
Lavoravo bene,
solo sembravo sbagliata,

00:06:39.220 --> 00:06:40.919
ed era l'età e genere sbagliato.

00:06:40.943 --> 00:06:44.289
Così asssumere senza considerare
genere e razza

00:06:44.313 --> 00:06:46.178
certamante mi sembra giusto.

00:06:46.851 --> 00:06:50.192
Ma con questi sistemi,
è più complicato, ed ecco perchè:

00:06:50.788 --> 00:06:56.579
Ad oggi, i sistemi di calcolo
possono desumere qualsiasi cosa su di voi

00:06:56.603 --> 00:06:58.475
dalle vostre <i>briciole</i> digitali,

00:06:58.499 --> 00:07:00.832
pur se non avete reso pubbliche
quelle cose.

00:07:01.326 --> 00:07:04.253
Possono desumere 
il vostro orientamento sessuale,

00:07:04.814 --> 00:07:06.120
caratteristiche personali,

00:07:06.679 --> 00:07:08.052
orientamento politico.

00:07:08.650 --> 00:07:12.335
Hanno potenza predittiva
con alto livello di accuratezza.

00:07:13.182 --> 00:07:15.760
Ricordate -- per cose che
non avete mai dichiarato.

00:07:15.784 --> 00:07:17.375
Questa è l'inferenza.

00:07:17.399 --> 00:07:20.660
Ho un'amica che ha sviluppato
un sistema di elaborazione

00:07:20.684 --> 00:07:24.325
per predire la probabilità
della depressione <i>postpartum</i>

00:07:24.349 --> 00:07:25.765
dai dati delle reti sociali.

00:07:26.496 --> 00:07:27.923
Risultati impressionanti!

00:07:28.312 --> 00:07:31.669
Il suo sistema può predire
la probabilità della depressione

00:07:31.693 --> 00:07:35.596
mesi prima dell'insorgere 
di un qualsiasi sintomo --

00:07:35.620 --> 00:07:36.993
mesi prima.

00:07:37.017 --> 00:07:39.263
Nessun sintomo, ma c'è predizione.

00:07:39.287 --> 00:07:44.099
Lei spera che sarà usato
per anticipare un intervento. Ottimo!

00:07:44.731 --> 00:07:46.771
Ma ora consideratelo in una assunzione.

00:07:47.847 --> 00:07:50.893
Così a quella conferenza dei manager
delle risorse umane,

00:07:50.917 --> 00:07:55.626
ho avvicinato un manager di alto livello
in una grande azienda,

00:07:55.650 --> 00:08:00.228
e le ho detto, "Guarda,
che succederebbe se, alla tua insaputa,

00:08:00.252 --> 00:08:06.801
il vostro sistema elimina le persone
con un'alta probabilità di depressione?

00:08:07.581 --> 00:08:10.957
Non sono depresse ora,
magari forse in futuro, più probabilmente.

00:08:11.743 --> 00:08:15.149
Che succederebbe se eliminasse le donne
più probabilmente incinte

00:08:15.173 --> 00:08:17.759
nei prossimi uno o due anni
ma che non lo sono ora?

00:08:18.664 --> 00:08:24.300
E se assumeste persone aggressive perchè
questa è la vostra cultura aziendale?"

00:08:24.993 --> 00:08:27.684
Non potete parlarne guardando
solo alle quote di genere.

00:08:27.708 --> 00:08:29.210
Queste si possono bilanciare.

00:08:29.234 --> 00:08:32.791
E dato che ciò è apprendimento automatico,
non programmazione tradizionale,

00:08:32.815 --> 00:08:37.722
non c'è alcuna variabile di codifica
etichettata "alto rischio di depressione",

00:08:37.746 --> 00:08:39.579
"maggior rischio di gravidanza,"

00:08:39.603 --> 00:08:41.337
"gradazione di maschio aggressivo."

00:08:41.815 --> 00:08:45.494
Non solo non sapete su cosa 
il vostro sistema basi la selezione,

00:08:45.518 --> 00:08:47.841
ma neppure sapete 
dove cominciare a guardare.

00:08:47.865 --> 00:08:49.111
E' una "scatola nera".

00:08:49.135 --> 00:08:51.942
Ha una potenza predittiva
ma non la capite.

00:08:52.306 --> 00:08:54.675
"Quali garanzie," chiesi, "avete

00:08:54.699 --> 00:08:58.372
per essere sicuri che la scatola nera
non sta facendo qualcosa losco?"

00:09:00.683 --> 00:09:04.561
Mi ha guardato come se avessi
schiacciato la coda a 10 cuccioli.

00:09:04.585 --> 00:09:05.833
(Risate)

00:09:05.857 --> 00:09:07.898
Mi fissò e disse,

00:09:08.376 --> 00:09:12.709
"Non voglio sentire altro su questo."

00:09:13.278 --> 00:09:15.312
E si girò andandosene via.

00:09:15.884 --> 00:09:17.480
Considerate - non era maleducata.

00:09:17.480 --> 00:09:23.702
Era chiaro: quello che non so non è un
mio problema. Va via, sguardo assassino.

00:09:23.726 --> 00:09:24.972
(Risate)

00:09:25.682 --> 00:09:29.521
Vedete, un tale sistema 
può anche essere meno influenzata

00:09:29.545 --> 00:09:31.648
in molti modi rispetto ai manager umani.

00:09:31.672 --> 00:09:33.818
E potrebbe avere senso economico.

00:09:34.393 --> 00:09:36.043
Ma può anche portare

00:09:36.067 --> 00:09:40.815
a una stabile, ma furtiva
chiusura dal mercato del lavoro

00:09:40.839 --> 00:09:43.132
di gente più a rischio di depressione.

00:09:43.573 --> 00:09:46.169
Questo è il tipo di società
che vogliamo costruire,

00:09:46.193 --> 00:09:48.478
senza neppure sapere che lo abbiamo fatto,

00:09:48.502 --> 00:09:52.466
poichè abbiamo lasciato la decisione
a macchine che non comprendiamo del tutto?

00:09:53.085 --> 00:09:54.543
Un altro problema è questo:

00:09:55.134 --> 00:09:59.586
quei sistemi sono spesso addestrati
con dati generati dalle nostre azioni,

00:09:59.610 --> 00:10:01.426
di impronta umana.

00:10:02.008 --> 00:10:05.816
Bene, possono proprio riflettere 
le nostre distorsioni o pregiudizi,

00:10:05.840 --> 00:10:09.433
e questi sistemi
si portano dietro i nostri pregiudizi

00:10:09.457 --> 00:10:10.770
e li amplificano

00:10:10.794 --> 00:10:12.212
e ce li rispecchiano,

00:10:12.236 --> 00:10:13.698
mentre ci diciamo,

00:10:13.722 --> 00:10:16.839
"Stiamo facendo solo elaborazioni
oggettive e neutrali."

00:10:18.134 --> 00:10:20.811
Ricerche effettuate su Google trovano,

00:10:21.954 --> 00:10:27.267
che è meno probabile vengano mostrati 
alle donne avvisi per lavori ben pagati.

00:10:28.283 --> 00:10:30.813
E cercando nomi di Afro-Americani

00:10:30.837 --> 00:10:35.543
è più probabile trovare avvisi
alludenti a storie criminali,

00:10:35.567 --> 00:10:37.134
anche quando non ce ne sono.

00:10:38.513 --> 00:10:42.062
Questi sono i pregiudizi nascosti
e gli algoritmi a scatola nera

00:10:42.086 --> 00:10:46.059
che i ricercatori talvolta scoprono
ma che altre volte non conosciamo,

00:10:46.083 --> 00:10:48.744
possono avere la conseguenza di 
alterare la vita.

00:10:49.778 --> 00:10:53.937
In Wisconsin, un accusato
è stato condannato a sei anni di prigione

00:10:53.961 --> 00:10:55.316
per aver eluso la polizia.

00:10:56.644 --> 00:10:57.830
Si può non saperlo,

00:10:57.854 --> 00:11:01.852
ma gli algoritmi sono sempre più usati 
per prendere decisioni giudiziarie.

00:11:01.876 --> 00:11:04.831
Egli voleva sapere:
come è stato calcolato il punteggio?

00:11:05.615 --> 00:11:07.280
Ma è una scatola nera 
sul mercato

00:11:07.304 --> 00:11:11.509
L'azienda si è rifiutata di far verificare
il suo algoritmo in una corte pubblica.

00:11:12.216 --> 00:11:17.748
Ma ProPublica, associazione investigativa
non-profit, ha verificato quell'algoritmo

00:11:17.772 --> 00:11:19.788
con i dati pubblici disponibili,

00:11:19.812 --> 00:11:22.128
trovando che i risultati 
erano influenzati

00:11:22.152 --> 00:11:25.781
e la sua potenza predittiva era
niente più di una possibilità,

00:11:25.805 --> 00:11:30.221
e che era sbagliato etichettare 
accusati neri come futuri criminali

00:11:30.245 --> 00:11:34.140
a livello doppio degli accusati bianchi.

00:11:35.711 --> 00:11:37.275
Così considerate questo caso:

00:11:37.923 --> 00:11:41.775
questa donna è in ritardo 
nel prelevare la sua figlioccia

00:11:41.799 --> 00:11:43.874
da scuola nella Contea di Broward, 
Florida,

00:11:44.577 --> 00:11:46.933
correndo per la strada
con un suo amico.

00:11:46.957 --> 00:11:51.056
Vedono una bici da ragazzo non bloccata
ed uno scooter in una veranda

00:11:51.080 --> 00:11:52.676
e stupidamente vi saltano su.

00:11:52.676 --> 00:11:55.395
Come stavano scappando via,
una donna uscì fuori e disse,

00:11:55.395 --> 00:11:57.564
"Ehi, quella bicicletta è mia!"

00:11:57.588 --> 00:12:00.882
Essi la lascarono, andarono via,
ma furono arrestati.

00:12:00.906 --> 00:12:04.543
Aveva sbagliato, era stata sciocca,
ma era appena diventata diciottenne.

00:12:04.567 --> 00:12:07.111
Fu condannata per un paio 
di crimini giovanili.

00:12:07.628 --> 00:12:12.813
Nel frattempo, quell'uomo fu arrestato
per furto di merce al Home Depot --

00:12:12.837 --> 00:12:15.761
merce per 85 dollari di valore,
un simile piccolo crimine.

00:12:16.586 --> 00:12:21.145
Ma lui aveva due precedenti
condanne per rapina a mano armata

00:12:21.775 --> 00:12:25.257
Ma l'algoritmo aveva valutato lei 
ad alto rischio, non lui.

00:12:26.566 --> 00:12:30.440
Due anni dopo, ProPublica trovò
che lei non era stata recidiva.

00:12:30.464 --> 00:12:33.014
Ma le fu duro trovare
lavoro, visti i precedenti.

00:12:33.038 --> 00:12:35.114
Lui, d'altra parte, fu recidivo

00:12:35.138 --> 00:12:38.974
ed ora è in prigione per 8 anni
a causa di un successivo crimine.

00:12:39.908 --> 00:12:43.277
Chiaramente, ci bisogna verificare 
le scatole nere

00:12:43.301 --> 00:12:45.916
per non dare loro questo 
incontrollato potere.

00:12:45.940 --> 00:12:48.819
(Applausi)

00:12:49.907 --> 00:12:54.149
Le verifiche sono importanti,
ma da sole non risolvono tutti i problemi.

00:12:54.173 --> 00:12:56.921
Prendete l'algoritmo di Facebook 
per caricare le <i>news</i>--

00:12:56.945 --> 00:13:01.788
sapete, quello che riordina tutto
e decide cosa mostrarvi

00:13:01.812 --> 00:13:04.096
da tutti gli amici e le pagine seguite.

00:13:04.718 --> 00:13:06.993
Dovrebbe mostrarvi un'altra foto di bimbo?

00:13:07.017 --> 00:13:08.213
(Risate)

00:13:08.237 --> 00:13:10.833
Una nota <i>imbronciata</i> da un conoscente?

00:13:11.269 --> 00:13:13.125
Una importante ma scabrosa notizia?

00:13:13.149 --> 00:13:14.631
Non c'è una risposta giusta.

00:13:14.655 --> 00:13:17.314
Facebook ottimizza per 
attraervi al loro sito:

00:13:17.338 --> 00:13:18.753
i <i>like</i>, <i>condividi</i>, commenti

00:13:19.988 --> 00:13:22.684
Nell'agosto 2014,

00:13:22.708 --> 00:13:26.480
proteste scoppiarono a Ferguson, Missouri,
dopo l'uccisione di

00:13:26.480 --> 00:13:29.811
un ragazzo Afro-Americano
da parte di un poliziotto bianco,

00:13:29.835 --> 00:13:31.405
in circostanze oscure.

00:13:31.794 --> 00:13:33.801
La notizia delle proteste era presente

00:13:33.825 --> 00:13:36.510
sul mio caricatore Twitter
non filtrato da algoritmi,

00:13:36.534 --> 00:13:38.484
ma nulla sul mio Facebook.

00:13:39.002 --> 00:13:40.896
Era a causa dei miei amici su Facebook?

00:13:40.896 --> 00:13:43.152
Ho disabilitato l'algoritmo di Facebook,

00:13:43.292 --> 00:13:46.140
che non è facile, poichè Facebook
vuole mantenervi

00:13:46.164 --> 00:13:48.200
sotto il controllo dell'algoritmo,

00:13:48.224 --> 00:13:50.462
e vidi che i miei amici
parlavano di questo.

00:13:50.486 --> 00:13:52.995
Era proprio l'algoritmo 
che non me lo mostrava.

00:13:53.019 --> 00:13:56.061
Ho approfondito ed ho trovato
che è un problema diffuso.

00:13:56.085 --> 00:13:59.898
La storia di Ferguson non è 
facile per l'algoritmo.

00:13:59.922 --> 00:14:01.093
Non è <i>likable</i>.

00:14:01.117 --> 00:14:02.669
Chi sta cliccando su "like?"

00:14:03.320 --> 00:14:05.526
Non è neppure facile da commentarsi.

00:14:05.550 --> 00:14:06.921
Senza <i>like</i> e commenti,

00:14:06.945 --> 00:14:10.237
l'algoritmo tende a mostrare la notizia
ad ancora meno persone,

00:14:10.261 --> 00:14:11.803
così non riusciamo a vederla.

00:14:12.766 --> 00:14:13.994
Invece, questa settimana,

00:14:14.018 --> 00:14:16.316
l'algoritmo di Facebook ha evidenziato

00:14:16.340 --> 00:14:18.566
questo: il <i>ALS Ice Bucket Challenge</i>.

00:14:18.590 --> 00:14:22.332
Causa benefica; svuota acqua ghiacciata,
dona alla beneficenza, bene.

00:14:22.356 --> 00:14:24.260
Ottimo per agevolare l'algoritmo.

00:14:25.039 --> 00:14:27.652
Una decisione automatica è stata
presa per noi

00:14:27.676 --> 00:14:31.173
Una conversazione molto importante
ma scabrosa

00:14:31.197 --> 00:14:32.752
può essere moderata,

00:14:32.776 --> 00:14:35.472
essendo Facebook l'unica via trasmissiva.

00:14:35.937 --> 00:14:39.734
Ora, infine, quei sistemi 
possono sbagliare

00:14:39.758 --> 00:14:42.494
in modi che non somigliano 
a sistemi umani.

00:14:42.518 --> 00:14:45.440
Vi ricordate Watson,
la macchina intelligente di IBM

00:14:45.464 --> 00:14:48.592
che ha spazzato via 
i contendenti umani a <i>Jeopardy</i>?

00:14:48.951 --> 00:14:50.379
Era un grande giocatore.

00:14:50.403 --> 00:14:54.059
Ma poi, alla finale di <i>Jeopardy</i>,
fu posta questa domanda a Watson:

00:14:54.059 --> 00:14:57.731
"Il suo più grande aeroporto è intitolato
a un eroe della II Guerra Mondiale,

00:14:57.731 --> 00:14:59.997
il suo secondo più grande 
a una sua battaglia."

00:14:59.997 --> 00:15:01.599
(Musica della finale di <i>Jeopardy</i>)

00:15:01.599 --> 00:15:02.584
Chicago.

00:15:02.608 --> 00:15:03.978
I due umani risposero bene.

00:15:04.517 --> 00:15:08.865
Watson, da altra parte,
rispose "Toronto" --

00:15:08.889 --> 00:15:10.707
per una città nella categoria USA!

00:15:11.416 --> 00:15:14.317
L'impressionante sistema aveva sbagliato

00:15:14.341 --> 00:15:17.992
come un umano non avrebbbe mai fatto,
neppure un alunno delle elementari.

00:15:18.643 --> 00:15:21.752
L'intelligenza automatica può sbagliare

00:15:21.776 --> 00:15:24.876
in modi non paragonabili 
con l'approccio dell'errore umano,

00:15:24.900 --> 00:15:27.850
in modi che non ci aspetteremmo
e per i quali siamo preparati.

00:15:27.874 --> 00:15:31.512
Sarebbe pessimo non ottenere un lavoro
per il quale si è qualificati,

00:15:31.536 --> 00:15:35.263
ma sarebbe tre volte peggio
se fosse causato da un errore software

00:15:35.287 --> 00:15:36.719
in qualche sua subroutine.

00:15:36.743 --> 00:15:38.322
(Risate)

00:15:38.346 --> 00:15:41.132
Nel maggio 2010,

00:15:41.156 --> 00:15:45.200
una crisi improvvisa a Wall Street,
alimentata da un erroneo ciclo di calcolo

00:15:45.224 --> 00:15:48.252
nell'algoritmo di "vendi" di Wall Street

00:15:48.276 --> 00:15:52.460
ha spazzato via un valore
di un trilione di dollari in 36 minuti.

00:15:53.542 --> 00:15:55.729
Non voglio pensare 
cosa significhi "errore"

00:15:55.753 --> 00:15:59.342
nel contesto di armi letali autonome.

00:16:01.714 --> 00:16:05.504
Si certo, gli umani sono sempre parziali.

00:16:05.528 --> 00:16:07.704
I decisori e controllori,

00:16:07.728 --> 00:16:11.221
nelle corti, nei notiziari, in guerra ...

00:16:11.245 --> 00:16:14.283
commettono errori;
ma questo è proprio il mio punto.

00:16:14.307 --> 00:16:17.828
non possiamo sfuggire 
a queste difficili domande.

00:16:18.416 --> 00:16:21.932
Non possiamo delegare 
le nostre responsabilità alle macchine.

00:16:22.496 --> 00:16:26.704
(Applausi)

00:16:28.909 --> 00:16:33.356
L'intelligenza artificiale non ci dà
un permesso di "uscire dall'etica" gratis.

00:16:34.562 --> 00:16:37.943
Lo scienziato sui dati Fred Benenson
lo chiama "pulizia matematica".

00:16:37.967 --> 00:16:39.356
Necessitiamo del contrario.

00:16:39.380 --> 00:16:44.768
Verso gli algoritmi occorre coltivare 
la diffidenza, verifica e indagine.

00:16:44.900 --> 00:16:48.398
Occorre essere sicuri che di sia
una responsabilità sugli algoritmi,

00:16:48.445 --> 00:16:50.890
metodi di verifica e 
una comprensibile trasparenza.

00:16:50.890 --> 00:16:54.330
È necessario accettare 
che portare matematica ed elaborazione

00:16:54.330 --> 00:16:57.298
in relazioni umane caotiche e di valore,

00:16:57.298 --> 00:16:59.228
non aggiunge obiettività;

00:16:59.228 --> 00:17:03.396
piuttosto, la complessità degli affari 
umani invade gli algoritmi.

00:17:03.396 --> 00:17:06.373
Si, possiamo e dovremmo usare il calcolo

00:17:06.373 --> 00:17:09.385
per aiutarci nel decidere meglio.

00:17:09.435 --> 00:17:14.293
Ma dobbiamo mantenere la 
responsabilità morale del giudizio,

00:17:14.293 --> 00:17:16.989
e usare algoritmi dentro quel contesto,

00:17:16.989 --> 00:17:21.561
non come gli strumenti per abdicare
e dare in outsource

00:17:21.561 --> 00:17:25.120
le nostre responsibilità
a qualcun altro come fra umani.

00:17:25.120 --> 00:17:27.838
La intelligenza delle macchine è qui.

00:17:27.838 --> 00:17:30.976
Significa che dobbiamo tenerci più forti

00:17:30.976 --> 00:17:33.861
ai valori ed etica umani.

00:17:33.861 --> 00:17:34.732
Grazie.

00:17:34.732 --> 00:17:35.930
(Applausi)

