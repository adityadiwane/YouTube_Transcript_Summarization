WEBVTT
Kind: captions
Language: hu

00:00:00.000 --> 00:00:07.000
Fordító: Peter Balla
Lektor: Péter Pallós

00:00:12.559 --> 00:00:16.681
Az első munkám számítógép-programozás volt

00:00:16.705 --> 00:00:18.661
a főiskola legelső évében,

00:00:18.685 --> 00:00:20.192
amit még tizenévesként kezdtem.

00:00:20.709 --> 00:00:22.441
Kezdésem után nem sokkal

00:00:22.465 --> 00:00:24.075
szoftvereket írtam egy cégnek,

00:00:24.619 --> 00:00:28.254
amikor a cég egyik főnök odajött hozzám,

00:00:28.278 --> 00:00:29.546
és suttogva megkérdezte:

00:00:30.049 --> 00:00:32.910
"Ő tudja-e, ha hazudok?"

00:00:33.626 --> 00:00:35.703
Senki más nem volt a szobában.

00:00:36.852 --> 00:00:41.241
"Kicsoda tudja-e, ha hazudsz?
És miért suttogunk?"

00:00:42.086 --> 00:00:45.193
A főnök a szobában lévő 
számítógépre mutatott.

00:00:45.217 --> 00:00:48.313
"Ő tudja-e, ha hazudok?"

00:00:49.433 --> 00:00:53.795
Hát, ennek a főnöknek
viszonya volt a recepcióssal.

00:00:53.819 --> 00:00:54.931
(Nevetés)

00:00:54.955 --> 00:00:56.721
És én még csak tizenéves voltam.

00:00:57.267 --> 00:00:59.286
Hangosan visszasuttogtam:

00:00:59.310 --> 00:01:02.934
"Igen, a számítógép tudja, mikor hazudsz."

00:01:02.958 --> 00:01:04.764
(Nevetés)

00:01:04.788 --> 00:01:07.711
Kinevettem, de igazából
engem kellene kinevetni.

00:01:07.735 --> 00:01:11.003
Manapság olyan számítógépes
rendszerek vannak,

00:01:11.027 --> 00:01:14.575
amelyek kiszimatolják az érzelmi 
állapotot, sőt a hazugságot is

00:01:14.599 --> 00:01:16.643
az emberi arc tanulmányozásából.

00:01:17.068 --> 00:01:21.221
A reklámozók és a hatóságok is
erősen érdeklődnek irántuk.

00:01:22.139 --> 00:01:24.001
Azért lettem számítógép-programozó,

00:01:24.025 --> 00:01:27.138
mert gyerekként megőrültem
a matekért és a tudományért.

00:01:27.762 --> 00:01:30.870
De valamikor később tudomást
szereztem a nukleáris fegyverekről,

00:01:30.894 --> 00:01:33.846
és komolyan aggódni kezdtem
a tudomány etikájáért.

00:01:33.870 --> 00:01:35.074
Zavarban voltam.

00:01:35.098 --> 00:01:37.739
Ám családi körülményeim miatt

00:01:37.763 --> 00:01:41.061
minél hamarabb munkába kellett állnom.

00:01:41.085 --> 00:01:44.384
Úgyhogy azt gondoltam magamban:
választok egy műszaki területet,

00:01:44.408 --> 00:01:46.204
ahol könnyen találok munkát,

00:01:46.228 --> 00:01:50.246
és ahol nem kell foglalkoznom
semmilyen zavaró etikai kérdéssel.

00:01:50.842 --> 00:01:52.371
A számítógépeket választottam.

00:01:52.395 --> 00:01:53.499
(Nevetés)

00:01:53.523 --> 00:01:56.933
Hát, ha-ha-ha! Ki lehet engem nevetni.

00:01:56.957 --> 00:01:59.711
Manapság az informatikusok
olyan felületeket építenek,

00:01:59.735 --> 00:02:03.944
amelyek azt irányítják,
amit egymilliárd ember naponta lát.

00:02:04.872 --> 00:02:08.694
Autókat fejlesztenek,
amelyek eldönthetik, kit ütnek el.

00:02:09.527 --> 00:02:12.740
Sőt, gépeket és fegyvereket gyártanak,

00:02:12.764 --> 00:02:15.049
amelyek háborúban 
embereket ölhetnek meg.

00:02:15.073 --> 00:02:17.844
Etika végestelen végig.

00:02:19.003 --> 00:02:21.061
A gépi intelligencia megérkezett.

00:02:21.643 --> 00:02:25.117
Ma a számítástechnikát
mindenféle döntésre használjuk,

00:02:25.141 --> 00:02:27.027
újfajta döntésekre is.

00:02:27.051 --> 00:02:32.223
Olyan kérdéseket teszünk fel a gépeknek,
amelyekre nincs egyetlen helyes válasz,

00:02:32.247 --> 00:02:33.449
szubjektív,

00:02:33.473 --> 00:02:35.798
nyitott és fontos kérdéseket.

00:02:35.822 --> 00:02:37.580
Ilyen kérdéseket teszünk fel:

00:02:37.604 --> 00:02:39.254
„Kit alkalmazzon a cég?”,

00:02:39.916 --> 00:02:42.675
„Melyik ismerős melyik frissítését
mutassuk meg neked?”,

00:02:42.699 --> 00:02:45.225
„Melyik elítéltről valószínűbb,
hogy újból bűnözik?”,

00:02:45.334 --> 00:02:48.388
„Melyik hírt vagy mozifilmet
ajánljuk az embereknek?”

00:02:48.412 --> 00:02:51.784
Már használjuk egy ideje a számítógépeket,

00:02:51.808 --> 00:02:53.325
de ez most más.

00:02:53.349 --> 00:02:55.100
Ez történelmi fordulat,

00:02:55.340 --> 00:03:00.777
mert a számítástechnikát nem tudjuk
úgy kötni e szubjektív döntésekhez,

00:03:00.801 --> 00:03:06.221
ahogy összekapcsoljuk a légi
közlekedéssel, a hídépítéssel,

00:03:06.245 --> 00:03:07.504
a holdra szállással.

00:03:08.269 --> 00:03:11.528
Biztonságosabbak a repülőgépek?
Kilengett és összedőlt a híd?

00:03:11.552 --> 00:03:16.050
Ott vannak egyeztetett,
elég világos viszonyítási alapjaink,

00:03:16.074 --> 00:03:18.313
és a természet törvényei irányítanak.

00:03:18.337 --> 00:03:21.731
A zűrös emberi viszonyokra
vonatkozó döntésekhez

00:03:21.755 --> 00:03:25.718
nincsenek ilyen kapcsolódási
és viszonyítási pontjaink.

00:03:26.272 --> 00:03:29.979
Hogy bonyolítsa a dolgokat,
szoftverünk egyre erősebb,

00:03:30.003 --> 00:03:33.776
ugyanakkor kevésbé átlátható
és egyre bonyolultabb lesz.

00:03:34.362 --> 00:03:36.402
Nemrég, az elmúlt évtizedben,

00:03:36.426 --> 00:03:39.155
a komplex algoritmusok
nagyot léptek előre.

00:03:39.179 --> 00:03:41.169
Emberi arcokat ismernek fel.

00:03:41.805 --> 00:03:43.860
Kézírást silabizálnak ki.

00:03:44.256 --> 00:03:46.322
Hitelkártyacsalásokat ismernek fel,

00:03:46.346 --> 00:03:47.535
spamet szűrnek ki,

00:03:47.559 --> 00:03:49.596
és egyik nyelvről a másikra fordítanak.

00:03:49.620 --> 00:03:52.194
Az orvosi képalkotásban
daganatokat ismernek fel.

00:03:52.218 --> 00:03:54.423
Megverik az embert sakkban és <i>gó</i>ban.

00:03:55.084 --> 00:03:59.588
A haladás zöme az ún. „gépi tanulás”
módszerének köszönhető.

00:03:59.995 --> 00:04:03.182
A gépi tanulás különbözik
a hagyományos programozástól,

00:04:03.206 --> 00:04:06.791
ahol a számítógépnek részletes,
pontos, precíz utasításokat adunk.

00:04:07.198 --> 00:04:11.380
Inkább olyan, hogy egy rendszerbe
beletöltünk egy csomó adatot,

00:04:11.404 --> 00:04:13.060
strukturálatlan adatokat is,

00:04:13.084 --> 00:04:15.362
amilyeneket digitális életünk hoz létre.

00:04:15.386 --> 00:04:18.116
A rendszer az adatok átfésüléséből tanul.

00:04:18.489 --> 00:04:20.015
Még egy alapvető dolog:

00:04:20.039 --> 00:04:24.419
e rendszerek nem az egyetlen 
válasz logikáját követik.

00:04:24.443 --> 00:04:27.402
Nem egy egyszerű választ
szűrnek le, inkább valószínűséget:

00:04:27.426 --> 00:04:30.909
„Ez inkább hasonlít arra, amit keresel.”

00:04:31.843 --> 00:04:34.913
A jó dolog ebben az,
hogy a módszer valóban hatásos.

00:04:34.937 --> 00:04:37.013
A Google MI-rendszer vezetője így nevezte:

00:04:37.037 --> 00:04:39.234
„az adatok észszerűtlen hatékonysága”.

00:04:39.611 --> 00:04:40.964
A hátránya,

00:04:41.558 --> 00:04:44.629
hogy nemigen értjük,
mit tanult a rendszer.

00:04:44.653 --> 00:04:46.240
Valójában ez az erőssége.

00:04:46.766 --> 00:04:50.564
Ez nem olyan, mint amikor utasításokat 
adunk egy számítógépnek,

00:04:51.020 --> 00:04:55.084
inkább olyan, mintha kiképeznénk
egy gép-kutyus-szerű lényt,

00:04:55.108 --> 00:04:57.479
akit nemigen értünk,
és nem tudunk irányítani.

00:04:58.182 --> 00:04:59.733
Ez a gondunk.

00:05:00.247 --> 00:05:04.509
Gond, ha ez az MI-rendszer valamit elszúr.

00:05:04.533 --> 00:05:08.073
Az is, ha nem szúrja el,

00:05:08.097 --> 00:05:11.725
mert nem is tudjuk, mi micsoda.
ha szubjektív kérdésről van szó.

00:05:11.749 --> 00:05:14.088
Nem tudjuk, mit gondol ez a dolog.

00:05:15.313 --> 00:05:18.996
Vegyünk egy felvételi algoritmust,

00:05:19.943 --> 00:05:24.254
azaz felvételre való rendszert,
amely gépi tanulási rendszert használ.

00:05:24.872 --> 00:05:28.451
A rendszert az eddigi alkalmazottak
adatain tanították be,

00:05:28.475 --> 00:05:31.066
és arra utasították, hogy a cég jelenlegi

00:05:31.090 --> 00:05:34.128
jól teljesítő embereihez hasonlókat
találjon és alkalmazzon.

00:05:34.634 --> 00:05:35.787
Jól hangzik.

00:05:35.811 --> 00:05:37.810
Részt vettem egyszer egy konferencián,

00:05:37.834 --> 00:05:40.959
ahol humánerőforrás-ügyintézők
és -vezetők gyűltek össze,

00:05:40.983 --> 00:05:42.133
magas rangú emberek,

00:05:42.133 --> 00:05:43.852
ők ilyeneket használtak felvételhez.

00:05:43.852 --> 00:05:45.442
Rendkívül izgatottak voltak.

00:05:45.466 --> 00:05:50.119
Azt gondolták, hogy ez objektívabbá,
pártatlanabbá teszi a felvételt,

00:05:50.143 --> 00:05:53.143
és jobb esélyt ad nőknek
és a kisebbségeknek

00:05:53.167 --> 00:05:55.355
a részrehajló ügyintézőkkel szemben.

00:05:55.379 --> 00:05:58.222
De az emberek felvétele részrehajló.

00:05:58.919 --> 00:06:00.104
Én tudom.

00:06:00.128 --> 00:06:03.133
Az egyik korai munkahelyemen
programozóként dolgoztam,

00:06:03.157 --> 00:06:07.025
és a kisfőnököm néha odajött hozzám

00:06:07.049 --> 00:06:10.802
nagyon korán reggel
vagy nagyon késő délután,

00:06:10.826 --> 00:06:13.888
és azt mondta: „Zeynep, menjünk ebédelni!”

00:06:14.544 --> 00:06:16.711
Meglepett a fura időzítés.

00:06:16.735 --> 00:06:18.864
Délután négykor ebédeljünk?

00:06:18.888 --> 00:06:21.982
Le voltam égve, ingyenes az ebéd...
Mindig vele mentem.

00:06:22.438 --> 00:06:24.505
Később rájöttem, mi történt.

00:06:24.529 --> 00:06:29.075
Kisfőnökeim nem vallották
be feletteseiknek,

00:06:29.099 --> 00:06:32.212
hogy a komoly munkára
felvett programozó egy tinilány,

00:06:32.236 --> 00:06:36.166
aki farmerben és surranóban jár munkába.

00:06:36.994 --> 00:06:39.196
Jól dolgoztam, de nem néztem ki elég jól,

00:06:39.220 --> 00:06:40.919
nem felelt meg a korom és a nemem.

00:06:40.943 --> 00:06:44.289
Ezért egy nemtől és rassztól
független felvétel

00:06:44.313 --> 00:06:46.178
jól hangzik nekem.

00:06:46.851 --> 00:06:50.192
De elmondom, hogy e rendszerekkel
a helyzet ennél miért bonyolultabb,

00:06:50.788 --> 00:06:56.579
Ma már a számítógép-rendszerek mindenfélét
ki tudnak következtetni rólunk

00:06:56.603 --> 00:06:58.475
a digitális morzsáinkból,

00:06:58.499 --> 00:07:00.832
még akkor is, ha nem
hoztuk nyilvánosságra őket.

00:07:01.326 --> 00:07:04.253
Ki tudják következtetni
nemi orientációnkat,

00:07:04.814 --> 00:07:06.120
személyiségjegyeinket,

00:07:06.679 --> 00:07:08.052
politikai szimpátiáinkat.

00:07:08.650 --> 00:07:12.335
Előrejelző erejük nagy fokú
pontossággal párosul.

00:07:13.182 --> 00:07:15.760
Figyelem: olyan dolgokról,
amelyeket nem is közöltünk.

00:07:15.784 --> 00:07:17.375
Ez a kikövetkeztetés.

00:07:17.399 --> 00:07:20.660
Egyik barátnőm olyan számítógépes
rendszereket fejlesztett,

00:07:20.684 --> 00:07:24.325
amelyek előrejelzik a szülés utáni
vagy a súlyos depresszió valószínűségét

00:07:24.349 --> 00:07:25.765
a közösségi média adataiból.

00:07:26.496 --> 00:07:27.923
Az eredmények lenyűgözőek.

00:07:28.312 --> 00:07:31.669
Rendszere előrejelzi
a depresszió valószínűségét

00:07:31.693 --> 00:07:35.596
hónapokkal a tünetek megjelenése előtt.

00:07:35.620 --> 00:07:36.993
Hónapokkal előtte!

00:07:37.017 --> 00:07:39.263
Nincs még tünet, de előrejelzés már van.

00:07:39.287 --> 00:07:44.099
Barátnőm reméli, hogy felhasználják 
majd a korai beavatkozáshoz. Nagyszerű!

00:07:44.731 --> 00:07:46.771
De nézzük ezt a felvételi szempontjából!

00:07:47.847 --> 00:07:50.893
Az említett humánerőforrás-konferencián

00:07:50.917 --> 00:07:55.626
odamentem az egyik óriási cég
magas rangú vezetőjéhez,

00:07:55.650 --> 00:08:00.228
és megkérdeztem tőle:
„M van, ha az ön tudta nélkül

00:08:00.252 --> 00:08:04.581
rendszerük kiszűri azokat,

00:08:04.581 --> 00:08:10.991
akiknél a depresszió jövőbeni
valószínűsége nagy?

00:08:11.473 --> 00:08:13.959
Mi van, ha kiszűri azokat,
akik egy-két éven belül

00:08:13.959 --> 00:08:17.759
valószínűleg teherbe esnek,
de most nem terhesek?

00:08:18.664 --> 00:08:24.300
Mi van, ha agresszív embereket vesz fel,
mert a munkahelyi kultúrába beleillenek?"

00:08:24.993 --> 00:08:27.774
Ez nem derül ki a nemek 
szerinti bontásból.

00:08:27.774 --> 00:08:29.100
Az lehet, hogy rendben van.

00:08:29.234 --> 00:08:32.791
Mivel ez gépi tanulás,
nem hagyományos kódolás,

00:08:32.815 --> 00:08:37.722
nincs „magas depressziókockázat”,

00:08:37.746 --> 00:08:39.579
„magas terhességi kockázat”,

00:08:39.603 --> 00:08:41.357
„agresszív pasi skála” nevű változó.

00:08:41.815 --> 00:08:45.494
Nemcsak azt nem tudjuk,
mi alapján választ a rendszer,

00:08:45.518 --> 00:08:47.841
hanem azt sem, hol kezd el keresni.

00:08:47.865 --> 00:08:49.111
Ez egy fekete doboz.

00:08:49.135 --> 00:08:51.942
Előrejelző ereje van, de nem értjük.

00:08:52.306 --> 00:08:54.675
„Mi a biztosíték rá – kérdeztem,

00:08:54.699 --> 00:08:58.372
hogy a fekete doboz nem 
csinál valami kétes dolgot?”

00:09:00.683 --> 00:09:04.561
Úgy nézett rám, mintha megöltem
volna a tanácselnököt.

00:09:04.585 --> 00:09:05.833
(Nevetés)

00:09:05.857 --> 00:09:07.898
Bámult rám, majd azt mondta:

00:09:08.376 --> 00:09:12.709
„Nem akarok erről hallani
többet egy szót sem”.

00:09:13.278 --> 00:09:15.312
Sarkon fordult, és elment.

00:09:15.884 --> 00:09:17.370
Megjegyezném: nem volt durva.

00:09:17.394 --> 00:09:23.702
Világos volt: amiről nem tudok, az nem
az én problémám, lelépek, bután nézek.

00:09:23.726 --> 00:09:24.972
(Nevetés)

00:09:25.682 --> 00:09:29.521
Egy ilyen rendszer, lehet,
hogy kevésbé részrehajló,

00:09:29.545 --> 00:09:31.648
mint bizonyos esetekben az ügyintézők.

00:09:31.672 --> 00:09:33.818
Pénzügyileg megérheti.

00:09:34.393 --> 00:09:36.043
De ahhoz vezethet,

00:09:36.067 --> 00:09:40.815
hogy folyamatosan és alattomosan
kizárja a munkaerőpiacról

00:09:40.839 --> 00:09:43.132
a magas depressziókockázatúakat.

00:09:43.573 --> 00:09:46.169
Ilyenfajta társadalmat akarunk építeni,

00:09:46.193 --> 00:09:48.478
még ha tudtunkon kívül is,

00:09:48.502 --> 00:09:52.466
csak mert olyan gépeknek adtuk át
a döntéshozatalt, amelyeket nem is értünk?

00:09:53.085 --> 00:09:54.543
Egy másik nehézség:

00:09:55.134 --> 00:09:59.586
a rendszerek tanítása gyakran 
tetteinkből létrehozott adatokkal,

00:09:59.610 --> 00:10:01.426
emberi lenyomatokkal történik.

00:10:02.008 --> 00:10:05.816
Lehet, hogy ezek
tükrözik részrehajlásunkat,

00:10:05.840 --> 00:10:09.433
és a rendszerek eltanulják őket,

00:10:09.457 --> 00:10:10.770
felerősítik,

00:10:10.794 --> 00:10:12.212
és visszatükrözik nekünk,

00:10:12.236 --> 00:10:13.698
miközben azt mondjuk magunknak

00:10:13.722 --> 00:10:16.519
„Mi csak objektív, semleges
számítástechnikát gyakorlunk.”

00:10:18.134 --> 00:10:21.431
Kutatók kimutatták, hogy a Google
nőknek kisebb valószínűséggel mutat

00:10:21.954 --> 00:10:27.203
jól fizetett állásokról szóló
hirdetéseket, mint férfiaknak.

00:10:28.283 --> 00:10:30.813
Ha afroamerikai nevekre keresünk,

00:10:30.837 --> 00:10:35.543
gyakrabban hoz fel büntetett
előéletet sejtető reklámokat,

00:10:35.567 --> 00:10:37.134
akkor is, ha nincs ilyenről szó.

00:10:38.513 --> 00:10:42.062
Az ilyen rejtett részrehajlásokat
és „fekete doboz” algoritmusokat

00:10:42.086 --> 00:10:46.059
olykor felfedik a kutatók,
de néha nem tudunk róluk,

00:10:46.083 --> 00:10:48.744
pedig következményeik
megváltoztathatják az életet.

00:10:49.778 --> 00:10:53.937
Wisconsinban egy vádlottat
hat év börtönre ítéltek,

00:10:53.961 --> 00:10:55.466
mert kijátszotta a rendőröket.

00:10:56.644 --> 00:10:58.490
Tán nem tudják: szabadlábra helyezési

00:10:58.500 --> 00:11:01.852
és büntetési döntésekhez egyre
gyakrabban használnak algoritmust.

00:11:01.876 --> 00:11:04.831
Ez az ember meg akarta tudni,
hogyan számítják a pontszámot.

00:11:05.615 --> 00:11:07.280
Ez egy kereskedelmi fekete doboz.

00:11:07.304 --> 00:11:11.509
A cég visszautasította, hogy nyilvános
bíróság elé vigyék az algoritmusát.

00:11:12.216 --> 00:11:17.748
De a ProPublica nonprofit
nyomozó szervezet auditálta az algoritmust

00:11:17.772 --> 00:11:19.788
a hozzáférhető nyilvános adatokkal,

00:11:19.812 --> 00:11:22.128
és az eredményeket részrehajlónak találta,

00:11:22.152 --> 00:11:25.781
az előrejelző képességét pedig
pocséknak, alig jobbnak a véletlennél,

00:11:25.805 --> 00:11:30.221
kétszer olyan gyakran jelölte hibásan
leendő bűnözőnek a fekete,

00:11:30.245 --> 00:11:34.140
mint a fehér elítélteket.

00:11:35.711 --> 00:11:37.275
Vegyünk egy másik esetet:

00:11:37.923 --> 00:11:41.775
egy nőnek el kellett hoznia keresztlányát

00:11:41.799 --> 00:11:43.874
az iskolából a floridai Broward megyében.

00:11:44.577 --> 00:11:46.933
Késésben volt, rohant
az utcán a barátnőjével.

00:11:46.957 --> 00:11:51.056
Egy tornácon lezáratlan
gyerekbiciklit és rollert láttak meg,

00:11:51.080 --> 00:11:52.712
meggondolatlanul felugrottak rá.

00:11:52.736 --> 00:11:55.335
Ahogy elhajtottak,
kijött egy nő, s azt kiáltotta:

00:11:55.359 --> 00:11:57.564
„Hé! Az a gyerek biciklije!”

00:11:57.588 --> 00:12:00.882
Eldobták, továbbmentek,
de letartóztatták őket.

00:12:00.906 --> 00:12:04.543
Rosszat tett, őrültség volt,
de csak 18 éves volt.

00:12:04.567 --> 00:12:07.111
Volt néhány fiatalkori kihágása.

00:12:07.628 --> 00:12:12.813
Ugyanakkor letartóztattak egy férfit
85 dollár értékű áruházi lopásért,

00:12:12.837 --> 00:12:15.761
ez egy hasonló kisebb vétség.

00:12:16.586 --> 00:12:21.145
Volt viszont két korábbi
ítélete fegyveres rablásért.

00:12:21.775 --> 00:12:25.257
Az algoritmus mégis a nőt értékelte
magas kockázatúnak, nem a férfit.

00:12:26.566 --> 00:12:30.440
Két évvel később a ProPublica azt találta,
hogy a nő nem követett el új vétséget.

00:12:30.464 --> 00:12:33.014
Csak nehéz volt munkát találnia
a priusza miatt.

00:12:33.038 --> 00:12:35.114
A férfi viszont visszaeső volt,

00:12:35.138 --> 00:12:38.974
és most nyolcéves büntetését tölti
egy későbbi bűncselekmény miatt.

00:12:39.908 --> 00:12:43.277
Világos, hogy auditálnunk kell
a fekete dobozainkat,

00:12:43.301 --> 00:12:45.916
s nem engedhetjük, hogy ilyen
korlátlan hatalmuk legyen.

00:12:45.940 --> 00:12:48.819
(Taps)

00:12:49.907 --> 00:12:54.149
Az auditok nagyszerűek és fontosak,
de nem oldják meg minden gondunkat.

00:12:54.173 --> 00:12:56.921
Vegyük a Facebook hatásos
hírválogató algoritmusát,

00:12:56.945 --> 00:13:01.788
azt, amelyik mindent sorba rak,
és eldönti, mit mutasson meg nekünk

00:13:01.812 --> 00:13:04.096
az összes követett ismerős és oldal közül.

00:13:04.718 --> 00:13:06.993
Mutasson még egy kisbabás képet?

00:13:07.017 --> 00:13:08.213
(Nevetés)

00:13:08.237 --> 00:13:10.833
Egy ismerős morcos megjegyzését?

00:13:11.269 --> 00:13:13.125
Egy fontos, de fajsúlyos hírt?

00:13:13.149 --> 00:13:14.631
Nincs helyes válasz.

00:13:14.655 --> 00:13:17.314
A Facebook az oldalon zajló
tevékenységre optimalizál:

00:13:17.338 --> 00:13:18.753
lájk, megosztás, komment.

00:13:19.988 --> 00:13:22.684
2014 augusztusában

00:13:22.708 --> 00:13:25.370
tüntetések törtek ki a Missouribeli
Fergusonban,

00:13:25.394 --> 00:13:29.811
miután egy fehér rendőr lelőtt 
egy afroamerikai tinédzsert,

00:13:29.835 --> 00:13:31.405
gyanús körülmények között.

00:13:31.794 --> 00:13:33.801
Tele volt a tüntetések híreivel

00:13:33.825 --> 00:13:36.510
az algoritmussal nem szűrt Twitter-fiókom,

00:13:36.534 --> 00:13:38.484
de a Facebookon nem volt semmi.

00:13:39.002 --> 00:13:40.736
A Facebook-ismerőseim az oka?

00:13:40.760 --> 00:13:42.792
Kikapcsoltam a Facebook algoritmusát,

00:13:43.292 --> 00:13:46.140
ami nehéz, mert a Facebook
azt akarja,

00:13:46.164 --> 00:13:48.200
hogy az algoritmus irányítson minket.

00:13:48.224 --> 00:13:50.462
Láttam, hogy az ismerőseim
beszélgetnek róla.

00:13:50.486 --> 00:13:52.995
Csakhogy az algoritmus azt
nem mutatta meg nekem.

00:13:53.019 --> 00:13:56.061
Kutattam utána, és azt találtam,
hogy ez egy elterjedt probléma.

00:13:56.085 --> 00:13:59.898
A fergusoni sztori nem volt
szimpatikus az algoritmusnak.

00:13:59.922 --> 00:14:01.093
Nem lájkolható.

00:14:01.117 --> 00:14:02.669
Ki fog a lájkra kattintani?

00:14:03.320 --> 00:14:05.526
Még kommentelni sem könnyű.

00:14:05.550 --> 00:14:06.921
Lájkok és kommentek nélkül

00:14:06.945 --> 00:14:10.437
az algoritmus egyre kisebb valószínűséggel
mutatta egyre kevesebbeknek,

00:14:10.457 --> 00:14:11.803
így nem láthattuk meg.

00:14:12.766 --> 00:14:13.994
Ehelyett azon a héten

00:14:14.018 --> 00:14:16.316
a Facebook algoritmusa előtérbe helyezte

00:14:16.340 --> 00:14:18.566
az ALS jeges vödör kihívást.

00:14:18.590 --> 00:14:22.332
Nemes cél: önts magadra jeges vizet,
és adakozz; rendben.

00:14:22.356 --> 00:14:24.260
Ez szimpatikus volt az algoritmusnak.

00:14:25.039 --> 00:14:27.652
A gép ezt helyettünk döntötte el.

00:14:27.676 --> 00:14:31.173
Egy igen fontos, de nehéz beszélgetést

00:14:31.197 --> 00:14:32.752
fojtott volna el,

00:14:32.776 --> 00:14:35.472
ha a Facebook lett volna
az egyetlen csatorna.

00:14:35.937 --> 00:14:39.734
S végül, e rendszerek úgy is hibázhatnak,

00:14:39.758 --> 00:14:42.494
ami nem hasonlít az emberi rendszerekre.

00:14:42.518 --> 00:14:45.404
Emlékeznek a Watsonra,
az IBM gépi intelligencia rendszerére,

00:14:45.404 --> 00:14:48.692
amelyik felmosta a padlót a "Mindent
vagy semmit" kvíz versenyzőivel?

00:14:48.951 --> 00:14:50.379
Nagyszerű játékos volt.

00:14:50.403 --> 00:14:53.972
De az utolsó fordulóban a következő
kérdést tették fel a Watsonnak:

00:14:54.479 --> 00:14:57.411
„Legnagyobb repterét egy
II. világháborús hősről nevezték el,

00:14:57.435 --> 00:14:59.687
a másodikat egy II. 
világháborús csatáról.”

00:14:59.711 --> 00:15:01.089
(Dúdolja a kvíz zenéjét)

00:15:01.402 --> 00:15:02.584
Chicago.

00:15:02.608 --> 00:15:03.978
A két ember eltalálta.

00:15:04.517 --> 00:15:08.865
A Watson válasza viszont Toronto volt –

00:15:08.889 --> 00:15:10.707
az USA-város kategóriában!

00:15:11.416 --> 00:15:14.317
A lenyűgöző rendszer olyan hibát ejtett,

00:15:14.341 --> 00:15:17.992
amilyet ember sosem tenne,
egy másodikos gyerek sem.

00:15:18.643 --> 00:15:21.752
A gépi intelligenciánk

00:15:21.776 --> 00:15:24.876
olyan módon hibázhat, ami nem 
hasonlít az emberi hibamintákra,

00:15:24.900 --> 00:15:27.850
olyan módon, amire nem számítunk,
nem vagyunk rá felkészülve.

00:15:27.874 --> 00:15:31.512
Ronda dolog lenne lemaradni egy munkáról,
amire megvan a képesítésünk,

00:15:31.536 --> 00:15:34.267
de háromszoros szívás lenne,

00:15:34.297 --> 00:15:36.719
ha ezt egy szubrutinban
túlcsordult verem okozná.

00:15:36.743 --> 00:15:38.322
(Nevetés)

00:15:38.346 --> 00:15:41.132
2010 májusában

00:15:41.156 --> 00:15:45.200
a Wall Street villámkrachja,
amit a tőzsdei „eladási” algoritmus

00:15:45.224 --> 00:15:48.252
visszacsatolási hurka okozott,

00:15:48.276 --> 00:15:52.460
egybillió dollárnyi értéket
tett semmivé 36 perc alatt.

00:15:53.542 --> 00:15:55.729
Bele sem akarok gondolni,
mit jelent a „hiba”

00:15:55.753 --> 00:15:59.342
az autonóm halálos fegyverekkel
összefüggésben.

00:16:01.714 --> 00:16:05.504
Igen, az emberek mindig
részrehajlók voltak.

00:16:05.528 --> 00:16:07.704
Döntéshozók és kapuőrök

00:16:07.728 --> 00:16:11.221
a bíróságokon, hírekben, háborúban...

00:16:11.245 --> 00:16:14.283
hibáznak; de éppen erről beszélek.

00:16:14.307 --> 00:16:17.828
Nem szabadulhatunk meg
e fogós kérdésektől.

00:16:18.416 --> 00:16:21.932
Nem szervezhetjük ki
felelősségünket gépekbe.

00:16:22.496 --> 00:16:26.704
(Taps)

00:16:28.909 --> 00:16:33.356
A mesterséges intelligencia nem
ment föl az etikus gondolkodás alól.

00:16:34.562 --> 00:16:37.943
Fred Benenson adatkutató
„math-washing”-nak nevezi ezt.

00:16:37.967 --> 00:16:39.356
Az ellenkezője szükséges.

00:16:39.380 --> 00:16:44.768
Gyanakodnunk kell az algoritmusokra,
és vizsgálnunk kell őket.

00:16:45.200 --> 00:16:48.398
Gondoskodnunk kell róla, 
hogy az algoritmusok számon kérhetők,

00:16:48.422 --> 00:16:50.867
auditálhatók és észszerűen
átláthatók legyenek.

00:16:51.200 --> 00:16:54.434
El kell fogadnunk, hogy a matek 
és az informatika bevonása

00:16:54.458 --> 00:16:57.428
a zavaros, értékeket hordozó emberi
viszonyokba

00:16:57.452 --> 00:16:59.836
nem eredményez objektivitást;

00:16:59.860 --> 00:17:03.493
ehelyett az emberi viszonyok
komplexitása átitatja az algoritmust.

00:17:03.968 --> 00:17:07.455
Igen, lehet és kell használnunk
az informatikát,

00:17:07.479 --> 00:17:09.493
hogy segítsen jobb döntéseket hozni.

00:17:09.517 --> 00:17:14.849
De ítéleteinkért az erkölcsi felelősséget
nekünk kell viselnünk,

00:17:14.873 --> 00:17:17.691
és az algoritmusokat azon
keretek között kell használnunk,

00:17:17.715 --> 00:17:22.650
nem pedig arra, hogy lemondjunk
az egymás iránti felelősségünkről,

00:17:22.674 --> 00:17:25.128
és azt kiszervezzük.

00:17:25.627 --> 00:17:28.236
A gépi intelligencia megérkezett.

00:17:28.260 --> 00:17:31.681
Ez azt jelenti, hogy még szigorúbban
kell ragaszkodnunk

00:17:31.705 --> 00:17:33.852
az emberi értékekhez és etikához.

00:17:33.876 --> 00:17:35.030
Köszönöm.

00:17:35.054 --> 00:17:40.074
(Taps) (Üdvrivalgás)

