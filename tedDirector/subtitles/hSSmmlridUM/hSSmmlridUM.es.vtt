WEBVTT
Kind: captions
Language: es

00:00:00.000 --> 00:00:07.000
Traductor: Lidia Cámara de la Fuente
Revisor: Sebastian Betti

00:00:12.739 --> 00:00:16.861
Empecé mi primer trabajo 
como programadora informática

00:00:16.885 --> 00:00:18.705
en mi primer año de universidad,

00:00:18.705 --> 00:00:20.632
básicamente, siendo aún adolescente.

00:00:20.889 --> 00:00:22.575
Poco después de empezar a trabajar,

00:00:22.575 --> 00:00:24.375
programando software en una empresa,

00:00:24.799 --> 00:00:28.434
un gerente que trabajaba en la compañía 
vino allí donde estaba yo,

00:00:28.458 --> 00:00:29.726
y me dijo al oído:

00:00:30.229 --> 00:00:33.090
"¿Puede decir ella si estoy mintiendo?"

00:00:33.806 --> 00:00:35.883
No había nadie más en la habitación.

00:00:37.032 --> 00:00:41.421
"¿Puede "quién" decir si está mintiendo? 
¿Y por qué estamos susurrando?"

00:00:42.266 --> 00:00:45.373
El gerente señaló la computadora 
de la habitación.

00:00:45.397 --> 00:00:48.493
"¿Puede ella decir si estoy mintiendo?"

00:00:49.613 --> 00:00:53.975
Bueno, el gerente tenía 
una aventura con la recepcionista.

00:00:53.999 --> 00:00:55.111
(Risas)

00:00:55.135 --> 00:00:56.901
Y yo todavía era adolescente.

00:00:57.447 --> 00:00:59.466
Por lo tanto, le susurro yo a él:

00:00:59.490 --> 00:01:03.114
"Sí, la computadora puede determinar 
si Ud. está mintiendo".

00:01:03.138 --> 00:01:04.944
(Risas)

00:01:04.968 --> 00:01:07.891
Bueno, me reí, pero, 
en realidad, me reía de mí.

00:01:07.915 --> 00:01:11.183
Hoy en día, existen sistemas informáticos

00:01:11.207 --> 00:01:14.669
que pueden detectar 
estados emocionales e incluso mentir

00:01:14.669 --> 00:01:17.081
a partir del procesamiento 
de rostros humanos.

00:01:17.248 --> 00:01:21.401
Los anunciantes, e incluso 
hay gobiernos muy interesados.

00:01:21.859 --> 00:01:24.231
Me había convertido en 
programadora informática

00:01:24.231 --> 00:01:27.812
porque yo era una de esas chicas 
locas por las matemáticas y la ciencia.

00:01:27.812 --> 00:01:31.220
Pero también me había 
interesado por las armas nucleares,

00:01:31.220 --> 00:01:34.646
y había empezado a realmente a 
preocuparme por la ética de la ciencia.

00:01:34.646 --> 00:01:36.234
Yo estaba preocupada.

00:01:36.234 --> 00:01:38.479
Sin embargo, 
por circunstancias familiares,

00:01:38.479 --> 00:01:41.241
también debía empezar 
a trabajar lo antes posible.

00:01:41.265 --> 00:01:44.478
Así que me dije, bueno, 
vamos a elegir un campo técnico

00:01:44.478 --> 00:01:46.384
donde poder conseguir un trabajo fácil

00:01:46.408 --> 00:01:50.426
y donde no tenga que lidiar 
con preguntas molestas sobre ética.

00:01:51.022 --> 00:01:52.551
Así que elegí las computadoras.

00:01:52.575 --> 00:01:53.679
(Risas)

00:01:53.703 --> 00:01:57.113
Bueno, ¡ja, ja, ja! 
Todas las risas a mi costa.

00:01:57.137 --> 00:01:59.891
Hoy en día, los informáticos 
construyen plataformas

00:01:59.915 --> 00:02:04.124
que controlan lo que millones 
de personas ven todos los días.

00:02:05.052 --> 00:02:08.874
Están desarrollando automóviles que 
podrían decidir a quién atropellar.

00:02:09.707 --> 00:02:12.920
Es más, están construyendo 
máquinas, armas,

00:02:12.944 --> 00:02:15.229
que podrían matar 
a seres humanos en la guerra.

00:02:15.253 --> 00:02:18.024
Esto es ética a fondo.

00:02:19.183 --> 00:02:21.241
La inteligencia artificial está aquí.

00:02:21.823 --> 00:02:24.951
Estamos usando la computación 
para tomar todo tipo de decisiones,

00:02:24.951 --> 00:02:27.101
además de nuevos tipos de decisiones.

00:02:27.101 --> 00:02:31.563
Planteamos preguntas a las computadoras 
que no tienen respuestas

00:02:31.563 --> 00:02:33.629
correctas individuales, 
por ser subjetivas

00:02:33.653 --> 00:02:35.978
e indefinidas y cargadas de valores.

00:02:36.002 --> 00:02:37.760
Planteamos preguntas como:

00:02:37.784 --> 00:02:39.784
"¿A quién debe contratar la empresa?"

00:02:40.096 --> 00:02:42.855
"¿Qué actualización de qué amigo 
debe mostrarse?"

00:02:42.879 --> 00:02:45.535
"¿Qué convicto tiene 
más probabilidades de reincidir?"

00:02:45.535 --> 00:02:48.968
"¿Qué artículo de noticias o película 
se deben recomendar a la gente?"

00:02:48.968 --> 00:02:51.964
Miren, sí, hemos venido usando 
computadoras hace tiempo,

00:02:51.988 --> 00:02:53.505
pero esto es diferente.

00:02:53.529 --> 00:02:55.596
Se trata de un giro histórico,

00:02:55.620 --> 00:03:00.957
porque no podemos anclar el cálculo 
para este tipo de decisiones subjetivas

00:03:00.981 --> 00:03:06.401
como anclamos el cálculo para 
pilotar aviones, construir puentes

00:03:06.425 --> 00:03:07.684
o ir a la luna.

00:03:08.449 --> 00:03:11.708
¿Son los aviones más seguros? 
¿Se balanceó el puente y cayó?

00:03:11.732 --> 00:03:16.230
Ahí, hemos acordado puntos 
de referencia bastante claros,

00:03:16.254 --> 00:03:18.493
y tenemos leyes de 
la naturaleza que nos guían.

00:03:18.517 --> 00:03:21.911
Nosotros no tenemos tales anclas 
y puntos de referencia

00:03:21.935 --> 00:03:25.898
para las decisiones sobre cuestiones
humanas desordenadas.

00:03:25.922 --> 00:03:30.159
Para complicar más las cosas, 
nuestro software es cada vez más potente,

00:03:30.183 --> 00:03:33.956
pero también es cada vez 
menos transparente y más complejo.

00:03:34.542 --> 00:03:36.582
Recientemente, en la última década,

00:03:36.606 --> 00:03:39.335
algunos algoritmos complejos 
han hecho grandes progresos.

00:03:39.359 --> 00:03:41.349
Pueden reconocer rostros humanos.

00:03:41.985 --> 00:03:44.040
Pueden descifrar la letra.

00:03:44.116 --> 00:03:46.502
Pueden detectar 
el fraude de tarjetas de crédito

00:03:46.526 --> 00:03:47.715
y bloquear el spam

00:03:47.739 --> 00:03:49.776
y pueden traducir a otros idiomas.

00:03:49.800 --> 00:03:52.374
Pueden detectar tumores 
en imágenes médicas.

00:03:52.374 --> 00:03:54.999
Puede vencer a los humanos 
en el ajedrez y en el Go.

00:03:55.264 --> 00:03:59.768
Gran parte de este progreso viene de un 
método llamado "aprendizaje automático".

00:03:59.905 --> 00:04:03.362
El aprendizaje automático es 
diferente a la programación tradicional,

00:04:03.386 --> 00:04:06.971
donde se da al equipo instrucciones 
exactas, detalladas y meticulosas.

00:04:07.378 --> 00:04:11.344
Es como si uno alimentara el sistema 
con una gran cantidad de datos,

00:04:11.344 --> 00:04:13.240
incluyendo los datos no estructurados,

00:04:13.264 --> 00:04:15.782
como los que generamos 
en nuestras vidas digitales.

00:04:15.782 --> 00:04:18.296
Y el sistema aprende de esos datos.

00:04:18.669 --> 00:04:20.195
Y también, de manera crucial,

00:04:20.219 --> 00:04:24.599
estos sistemas no funcionan 
bajo una lógica de una sola respuesta.

00:04:24.623 --> 00:04:27.582
No producen una respuesta sencilla; 
es más probabilístico:

00:04:27.606 --> 00:04:31.089
"Esto es probablemente parecido 
a lo que estás buscando".

00:04:31.833 --> 00:04:34.437
La ventaja es que 
este método es muy potente.

00:04:34.437 --> 00:04:37.663
El jefe de sistemas de inteligencia 
artificial de Google lo llama:

00:04:37.663 --> 00:04:39.534
"la eficacia irrazonable de los datos".

00:04:39.791 --> 00:04:41.144
La desventaja es que

00:04:41.738 --> 00:04:44.809
realmente no entendemos 
lo que aprendió el sistema.

00:04:44.833 --> 00:04:46.420
De hecho, ese es su poder.

00:04:46.946 --> 00:04:50.744
Esto no se parece a dar instrucciones 
a una computadora;

00:04:51.200 --> 00:04:55.264
se parece más a la formación 
de una criatura cachorro máquina

00:04:55.288 --> 00:04:57.659
que realmente no entendemos o controlamos.

00:04:58.052 --> 00:05:01.674
Así que este es nuestro problema;
un problema cuando el sistema

00:05:01.746 --> 00:05:04.689
de inteligencia artificial 
hace cosas erróneas.

00:05:04.713 --> 00:05:08.037
Es también un problema 
cuando hace bien las cosas,

00:05:08.037 --> 00:05:11.905
porque ni siquiera sabemos qué es qué 
cuando se trata de un problema subjetivo.

00:05:11.929 --> 00:05:14.268
No sabemos qué está pensando esta cosa.

00:05:15.493 --> 00:05:19.176
Por lo tanto, piensen en 
un algoritmo de contratación,

00:05:20.123 --> 00:05:24.434
un sistema usado para contratar, 
usa sistemas de aprendizaje automático.

00:05:25.052 --> 00:05:28.631
un sistema así habría sido entrenado 
con anteriores datos de empleados

00:05:28.655 --> 00:05:31.246
y tiene la instrucción 
de encontrar y contratar

00:05:31.270 --> 00:05:34.308
personas como las de alto rendimiento 
existentes en la empresa.

00:05:34.814 --> 00:05:35.967
Suena bien.

00:05:35.991 --> 00:05:37.884
Una vez asistí a una conferencia

00:05:37.884 --> 00:05:40.923
que reunió a los responsables 
de recursos humanos y ejecutivos,

00:05:40.923 --> 00:05:42.303
las personas de alto nivel,

00:05:42.303 --> 00:05:44.452
que usaban estos sistemas 
en la contratación.

00:05:44.452 --> 00:05:45.622
Estaban muy emocionados.

00:05:45.646 --> 00:05:50.299
Pensaban que esto haría la contratación 
más objetiva, menos tendenciosa,

00:05:50.323 --> 00:05:53.323
para dar a las mujeres y a las minorías 
mejores oportunidades

00:05:53.347 --> 00:05:55.715
contra los administradores 
humanos tendenciosos.

00:05:55.715 --> 00:05:58.402
La contratación humana es tendenciosa.

00:05:59.099 --> 00:06:00.284
Lo sé.

00:06:00.308 --> 00:06:03.313
Es decir, en uno de mis primeros 
trabajos como programadora,

00:06:03.337 --> 00:06:07.205
mi jefa a veces venía 
allí donde yo estaba

00:06:07.229 --> 00:06:10.982
muy temprano en la mañana 
o muy tarde por la tarde,

00:06:11.006 --> 00:06:14.068
y decía: "Zeynep, ¡vayamos a comer!"

00:06:14.174 --> 00:06:17.151
Me dejaba perpleja por el 
momento extraño de preguntar.

00:06:17.151 --> 00:06:19.044
Son las 16. ¿Almuerzo?

00:06:19.068 --> 00:06:22.332
Estaba en la ruina, así que, 
ante un almuerzo gratis, siempre fui.

00:06:22.428 --> 00:06:24.985
Más tarde me di cuenta 
de lo que estaba ocurriendo.

00:06:24.985 --> 00:06:28.969
Mis jefes inmediatos no habían 
confesado a sus altos mandos

00:06:28.969 --> 00:06:32.392
que el programador contratado para 
un trabajo serio era una adolescente

00:06:32.416 --> 00:06:36.346
que llevaba pantalones vaqueros 
y zapatillas de deporte en el trabajo.

00:06:36.624 --> 00:06:39.230
Yo hacía un buen trabajo, 
solo que no encajaba

00:06:39.230 --> 00:06:41.099
por la edad y por el sexo equivocado.

00:06:41.123 --> 00:06:44.469
Así que contratar a ciegas 
independiente del género y de la raza

00:06:44.493 --> 00:06:46.358
ciertamente me parece bien.

00:06:47.031 --> 00:06:50.372
Sin embargo, con estos sistemas, 
es más complicado, y he aquí por qué:

00:06:50.968 --> 00:06:56.759
Hoy los sistemas informáticos pueden 
deducir todo tipo de cosas sobre Uds.

00:06:56.783 --> 00:06:58.655
a partir de sus pistas digitales,

00:06:58.679 --> 00:07:01.012
incluso si no las han dado a conocer.

00:07:01.506 --> 00:07:04.433
Pueden inferir su orientación sexual,

00:07:04.994 --> 00:07:06.300
sus rasgos de personalidad,

00:07:06.859 --> 00:07:08.232
sus inclinaciones políticas.

00:07:08.830 --> 00:07:12.515
Tienen poder predictivo 
con altos niveles de precisión.

00:07:13.082 --> 00:07:15.940
Recuerden, por cosas que 
ni siquiera han dado a conocer.

00:07:15.964 --> 00:07:17.555
Esta es la inferencia.

00:07:17.579 --> 00:07:20.840
Tengo una amiga que desarrolló 
este tipo de sistemas informáticos

00:07:20.864 --> 00:07:24.505
para predecir la probabilidad 
de depresión clínica o posparto

00:07:24.529 --> 00:07:26.435
a partir de datos de medios sociales.

00:07:26.676 --> 00:07:28.363
Los resultados son impresionantes.

00:07:28.492 --> 00:07:31.849
Su sistema puede predecir 
la probabilidad de depresión

00:07:31.873 --> 00:07:35.776
meses antes de la aparición 
de cualquier síntoma,

00:07:35.800 --> 00:07:37.173
meses antes.

00:07:37.197 --> 00:07:39.443
No hay síntomas, sí hay predicción.

00:07:39.467 --> 00:07:44.279
Ella espera que se use para 
la intervención temprana. ¡Estupendo!

00:07:44.721 --> 00:07:47.431
Pero ahora pongan esto en el 
contexto de la contratación.

00:07:48.027 --> 00:07:51.073
Así que en esa conferencia 
de recursos humanos,

00:07:51.097 --> 00:07:55.806
me acerqué a una gerenta de alto nivel 
de una empresa muy grande,

00:07:55.830 --> 00:08:00.408
y le dije: "Mira, ¿qué pasaría si, 
sin su conocimiento,

00:08:00.432 --> 00:08:06.981
el sistema elimina a las personas con 
alta probabilidad futura de la depresión?

00:08:07.761 --> 00:08:11.137
No están deprimidos ahora, solo 
quizá en el futuro, sea probable.

00:08:11.853 --> 00:08:15.293
¿Y si elimina a las mujeres con más 
probabilidades de estar embarazadas

00:08:15.293 --> 00:08:18.179
en el próximo año o dos, 
pero no está embarazada ahora?

00:08:18.844 --> 00:08:24.480
¿Y si contratamos a personas agresivas, 
porque esa es su cultura de trabajo?"

00:08:25.173 --> 00:08:27.798
No se puede saber esto 
mirando un desglose por sexos.

00:08:27.798 --> 00:08:29.284
Estos pueden ser equilibrados.

00:08:29.284 --> 00:08:33.521
Y como esto es aprendizaje automático, 
no la programación tradicional,

00:08:33.521 --> 00:08:37.902
no hay una variable etiquetada 
como "mayor riesgo de depresión",

00:08:37.926 --> 00:08:39.759
"mayor riesgo de embarazo",

00:08:39.783 --> 00:08:41.517
"escala de chico agresivo".

00:08:41.995 --> 00:08:45.674
Ud. no solo no sabe lo que 
su sistema selecciona,

00:08:45.698 --> 00:08:48.020
sino que ni siquiera sabe 
por dónde empezar a buscar.

00:08:48.044 --> 00:08:49.291
Es una caja negra.

00:08:49.315 --> 00:08:52.121
Tiene capacidad de predicción, 
pero uno no lo entiende.

00:08:52.486 --> 00:08:54.855
"¿Qué salvaguardia", pregunté,

00:08:54.879 --> 00:08:58.552
"puede asegurar que la caja negra 
no hace algo perjudicial?"

00:09:00.863 --> 00:09:04.741
Ella me miró como si acabara 
de romper algo valioso.

00:09:04.765 --> 00:09:06.013
(Risas)

00:09:06.037 --> 00:09:08.078
Me miró y dijo:

00:09:08.556 --> 00:09:12.889
"No quiero oír ni una palabra de esto".

00:09:13.458 --> 00:09:15.492
Dio la vuelta y se alejó.

00:09:16.064 --> 00:09:17.550
Eso sí, ella no fue grosera.

00:09:17.574 --> 00:09:23.882
Era claramente: lo que no sé, no es 
mi problema, vete, encara la muerte.

00:09:23.906 --> 00:09:25.152
(Risas)

00:09:25.652 --> 00:09:29.325
Un sistema de este tipo 
puede ser incluso menos sesgado

00:09:29.325 --> 00:09:31.888
que los administradores humanos 
en algunos aspectos.

00:09:31.888 --> 00:09:33.998
Y podría tener sentido monetario.

00:09:34.573 --> 00:09:36.223
Pero también podría llevar

00:09:36.247 --> 00:09:40.995
a un cierre constante pero sigiloso 
del mercado de trabajo

00:09:41.019 --> 00:09:43.312
a las personas 
con mayor riesgo de depresión.

00:09:43.553 --> 00:09:46.349
¿Es este el tipo de sociedad 
la que queremos construir,

00:09:46.373 --> 00:09:48.658
sin siquiera saber que lo hemos hecho,

00:09:48.682 --> 00:09:52.646
porque nos movemos en torno a decisiones 
de máquinas que no entendemos totalmente?

00:09:53.265 --> 00:09:54.723
Otro problema es el siguiente:

00:09:55.314 --> 00:09:59.560
estos sistemas son a menudo 
entrenados con datos generados

00:09:59.560 --> 00:10:01.876
por nuestras acciones, 
por huellas humanas.

00:10:02.188 --> 00:10:05.996
Podrían pues estar reflejando 
nuestros prejuicios,

00:10:06.020 --> 00:10:09.613
y estos sistemas podrían dar cuenta 
de nuestros prejuicios

00:10:09.637 --> 00:10:10.950
y la amplificación de ellos

00:10:10.974 --> 00:10:12.392
volviendo a nosotros,

00:10:12.416 --> 00:10:13.878
mientras que decimos:

00:10:13.902 --> 00:10:17.019
"Somos objetivos, es el cómputo neutral".

00:10:18.314 --> 00:10:21.541
Los investigadores encontraron 
que en Google las mujeres tienen

00:10:22.134 --> 00:10:25.463
menos probabilidades que los hombres

00:10:25.463 --> 00:10:28.463
de que les aparezcan anuncios 
de trabajo bien remunerados.

00:10:28.463 --> 00:10:30.993
Y buscando nombres afroestadounidenses

00:10:31.017 --> 00:10:35.723
es más probable que aparezcan anuncios 
que sugieren antecedentes penales,

00:10:35.747 --> 00:10:37.314
incluso cuando no existan.

00:10:38.693 --> 00:10:42.242
Estos sesgos ocultos 
y algoritmos de la caja negra

00:10:42.266 --> 00:10:46.239
que descubren los investigadores 
a veces, pero a veces no,

00:10:46.263 --> 00:10:48.924
pueden tener consecuencias 
que cambian la vida.

00:10:49.958 --> 00:10:54.117
En Wisconsin, un acusado 
fue condenado a seis años de prisión

00:10:54.141 --> 00:10:55.496
por escaparse de la policía.

00:10:55.904 --> 00:10:58.970
Quizá no lo sepan, pero los algoritmos 
se usan cada vez más

00:10:58.970 --> 00:11:02.032
en las decisiones de 
libertad condicional y de sentencia.

00:11:02.056 --> 00:11:05.011
El acusado quiso saber: 
¿Cómo se calcula la puntuación?

00:11:05.795 --> 00:11:07.460
Es una caja negra comercial.

00:11:07.484 --> 00:11:11.689
La empresa se negó a que se cuestionara 
su algoritmo en audiencia pública.

00:11:12.206 --> 00:11:16.992
Pero ProPublica, organización 
no lucrativa de investigación,

00:11:16.992 --> 00:11:20.178
auditó precisamente ese algoritmo
con los datos públicos que encontró,

00:11:20.178 --> 00:11:22.598
y descubrió que sus resultados 
estaban sesgados

00:11:22.598 --> 00:11:25.961
y su capacidad de predicción era pésima, 
apenas mejor que el azar,

00:11:25.985 --> 00:11:30.401
y se etiquetaban erróneamente 
acusados ​​negros como futuros criminales

00:11:30.425 --> 00:11:34.320
con una tasa del doble 
que a los acusados ​​blancos.

00:11:35.641 --> 00:11:37.565
Piensen en este caso:

00:11:38.103 --> 00:11:41.955
Esta mujer llegó tarde a 
recoger a la hija de su madrina

00:11:41.979 --> 00:11:44.434
de una escuela en 
el condado de Broward, Florida,

00:11:44.757 --> 00:11:47.113
iba corriendo por la calle con una amiga.

00:11:47.137 --> 00:11:51.236
Vieron la bicicleta de un niño sin candado
y una moto en un porche

00:11:51.260 --> 00:11:52.892
y tontamente saltó sobre ella.

00:11:52.916 --> 00:11:55.515
A medida que aceleraban, 
una mujer salió y dijo,

00:11:55.539 --> 00:11:57.744
"¡Eh, esa es la bicicleta de mi hijo!"

00:11:57.768 --> 00:12:01.062
Se bajaron, se alejaron, 
pero fueron detenidas.

00:12:01.086 --> 00:12:04.723
Estaba equivocada, fue una tontería, 
pero también tenía solo 18 años.

00:12:04.747 --> 00:12:07.291
Tenía un par de faltas menores.

00:12:07.808 --> 00:12:12.993
Mientras tanto, detenían al hombre 
por hurto en Home Depot,

00:12:13.017 --> 00:12:15.941
por un valor de USD 85, 
un delito menor similar.

00:12:16.766 --> 00:12:21.325
Pero él tenía dos condenas anteriores
por robo a mano armada.

00:12:21.955 --> 00:12:25.437
Sin embargo, el algoritmo la anotó 
a ella como de alto riesgo, y no a él.

00:12:26.386 --> 00:12:29.864
Dos años más tarde, ProPublica descubrió 
que ella no había vuelto a delinquir.

00:12:29.864 --> 00:12:33.534
Pero le era difícil conseguir un trabajo 
con sus antecedentes registrados.

00:12:33.534 --> 00:12:35.464
Él, por el contrario, era reincidente

00:12:35.464 --> 00:12:39.414
y ahora cumple una pena de ocho años 
de prisión por un delito posterior.

00:12:40.088 --> 00:12:43.457
Es evidente que necesitamos 
auditar nuestras cajas negras

00:12:43.481 --> 00:12:46.096
para no tener 
este tipo de poder sin control.

00:12:46.120 --> 00:12:48.999
(Aplausos)

00:12:50.087 --> 00:12:54.329
Las auditorías son grandes e importantes, 
pero no resuelven todos los problemas.

00:12:54.353 --> 00:12:57.101
Tomemos el potente algoritmo 
de noticias de Facebook,

00:12:57.125 --> 00:13:01.968
ese que sabe todo y decide qué mostrarles

00:13:01.992 --> 00:13:04.276
de las páginas de los amigos que siguen.

00:13:04.898 --> 00:13:07.173
¿Debería mostrarles otra imagen de bebé?

00:13:07.197 --> 00:13:08.393
(Risas)

00:13:08.417 --> 00:13:11.013
¿Una nota deprimente de un conocido?

00:13:11.449 --> 00:13:13.305
¿Una noticia importante pero difícil?

00:13:13.329 --> 00:13:14.811
No hay una respuesta correcta.

00:13:14.835 --> 00:13:17.494
Facebook optimiza para 
que se participe en el sitio:

00:13:17.518 --> 00:13:18.933
con Me gusta, Compartir 
y con Comentarios.

00:13:20.168 --> 00:13:22.864
En agosto de 2014,

00:13:22.888 --> 00:13:25.550
estallaron protestas 
en Ferguson, Missouri,

00:13:25.574 --> 00:13:29.991
tras la muerte de un adolescente 
afroestadounidense por un policía blanco,

00:13:30.015 --> 00:13:31.585
en circunstancias turbias.

00:13:31.974 --> 00:13:33.981
La noticia de las protestas llegaron

00:13:34.005 --> 00:13:36.690
en mi cuenta de Twitter 
algorítmicamente sin filtrar

00:13:36.714 --> 00:13:38.664
pero en ninguna parte en mi Facebook.

00:13:38.782 --> 00:13:40.916
¿Y qué pasaba con mis amigos de Facebook?

00:13:40.940 --> 00:13:42.972
Desactivé el algoritmo de Facebook,

00:13:43.132 --> 00:13:46.320
lo cual es difícil ya que Facebook 
quiere seguir manteniéndonos

00:13:46.344 --> 00:13:48.380
bajo el control del algoritmo,

00:13:48.404 --> 00:13:50.642
y vi que mis amigos 
estaban hablando de ello.

00:13:50.666 --> 00:13:53.175
Pero el algoritmo no me lo mostraba.

00:13:53.199 --> 00:13:56.241
He investigado esto y encontré 
que era un problema generalizado.

00:13:56.265 --> 00:14:00.078
La historia de Ferguson no era 
compatible con el algoritmo.

00:14:00.102 --> 00:14:01.273
No es "gustable".

00:14:01.297 --> 00:14:03.249
¿Quién va a hacer clic en "Me gusta"?

00:14:03.500 --> 00:14:05.706
Ni siquiera es fácil de comentar.

00:14:05.730 --> 00:14:07.401
Sin Me gusta y sin comentarios,

00:14:07.401 --> 00:14:10.331
el algoritmo era probable de 
mostrarse a aún menos personas,

00:14:10.331 --> 00:14:12.393
así que no tuvimos 
oportunidad de ver esto.

00:14:12.946 --> 00:14:14.174
En cambio, esa semana,

00:14:14.198 --> 00:14:16.496
el algoritmo de Facebook destacó esto,

00:14:16.520 --> 00:14:18.746
el ALS que era 
el desafío del cubo de hielo.

00:14:18.770 --> 00:14:22.512
Noble causa; verter agua con hielo, 
donar a la caridad, bien.

00:14:22.536 --> 00:14:24.830
Esa causa era súper compatible 
con el algoritmo.

00:14:25.219 --> 00:14:27.832
La máquina tomó 
esta decisión por nosotros.

00:14:27.856 --> 00:14:31.353
Una conversación 
muy importante pero difícil

00:14:31.377 --> 00:14:32.932
podría haber sido silenciada

00:14:32.956 --> 00:14:35.652
si Facebook hubiese sido el único canal.

00:14:36.117 --> 00:14:39.914
Ahora, por fin, estos sistemas 
pueden también equivocarse

00:14:39.938 --> 00:14:42.398
de formas que no se parecen a los humanos.

00:14:42.398 --> 00:14:45.780
¿Se acuerdan de Watson, el sistema 
de inteligencia artificial de IBM

00:14:45.780 --> 00:14:48.772
que arrasó con los concursantes 
humanos en Jeopardy?

00:14:49.131 --> 00:14:50.413
Fue un gran jugador.

00:14:50.413 --> 00:14:54.079
Pero entonces, para la final de Jeopardy, 
a Watson se le hizo esta pregunta:

00:14:54.079 --> 00:14:57.591
"Su mayor aeropuerto lleva el nombre 
de un héroe de la 2ª Guerra Mundial,

00:14:57.615 --> 00:15:00.167
la 2ª batalla más grande 
de la 2ª Guerra Mundial".

00:15:00.167 --> 00:15:01.539
(Música final de Jeopardy)

00:15:01.582 --> 00:15:02.764
Chicago.

00:15:02.788 --> 00:15:04.568
Los dos humanos lo hicieron bien.

00:15:04.697 --> 00:15:09.045
Watson, por otra parte, 
respondió "Toronto"

00:15:09.069 --> 00:15:11.357
para una categoría de ciudad de EE.UU.

00:15:11.596 --> 00:15:14.497
El impresionante sistema 
también cometió un error

00:15:14.521 --> 00:15:18.172
que un humano nunca cometería, que
un estudiante de segundo grado tampoco.

00:15:18.823 --> 00:15:21.636
La inteligencia artificial puede fallar

00:15:21.636 --> 00:15:25.056
en formas que no se ajustan a 
los patrones de error de los humanos,

00:15:25.080 --> 00:15:28.030
de maneras que no esperamos
y para las que no estamos preparados.

00:15:28.054 --> 00:15:31.692
Sería pésimo no conseguir trabajo, 
una vez que uno se cualifica para ello,

00:15:31.716 --> 00:15:35.443
pero sería el triple de pésimo 
si fue por un desbordamiento de pila

00:15:35.467 --> 00:15:36.899
en algunas subrutinas.

00:15:36.923 --> 00:15:38.502
(Risas)

00:15:38.526 --> 00:15:41.312
En mayo del 2010

00:15:41.336 --> 00:15:45.380
un flash crash de Wall Street alimentado 
por un circuito de retroalimentación

00:15:45.404 --> 00:15:48.432
por el algoritmo de "venta" de Wall Street

00:15:48.456 --> 00:15:52.640
borró un billón de dólares en 36 minutos.

00:15:53.412 --> 00:15:55.909
Yo no quiero ni pensar 
lo que significa "error"

00:15:55.933 --> 00:16:00.082
en el contexto de 
las armas autónomas letales.

00:16:01.894 --> 00:16:05.684
Los humanos siempre 
hemos tenido prejuicios.

00:16:05.708 --> 00:16:07.884
Los que toman decisiones y los guardias,

00:16:07.908 --> 00:16:11.401
en los tribunales, 
en la actualidad, en la guerra...

00:16:11.425 --> 00:16:14.463
cometen errores; pero ese 
es exactamente mi tema.

00:16:14.487 --> 00:16:18.008
No podemos escapar 
a estas preguntas difíciles.

00:16:18.596 --> 00:16:22.112
No podemos delegar nuestra
responsabilidad a las máquinas.

00:16:22.676 --> 00:16:26.884
(Aplausos)

00:16:29.089 --> 00:16:33.536
La inteligencia artificial 
no nos da una tarjeta libre de ética.

00:16:34.102 --> 00:16:38.123
El experto en datos Fred Benenson lo 
llama "mathwashing" o lavado matemático.

00:16:38.147 --> 00:16:39.536
Necesitamos lo contrario.

00:16:39.560 --> 00:16:44.948
Necesitamos fomentar un algoritmo 
de sospecha, escrutinio e investigación.

00:16:45.380 --> 00:16:48.578
Tenemos que asegurarnos de tener 
responsabilidad algorítmica,

00:16:48.602 --> 00:16:51.047
auditoría y transparencia significativa.

00:16:51.380 --> 00:16:54.614
Tenemos que aceptar que llevar 
las matemáticas y la computación

00:16:54.638 --> 00:16:57.608
a los asuntos humanos, 
desordenados y cargados de valores

00:16:57.632 --> 00:17:00.016
no conlleva a la objetividad;

00:17:00.040 --> 00:17:03.673
más bien, la complejidad de los asuntos 
humanos invaden los algoritmos.

00:17:04.148 --> 00:17:07.635
Sí, podemos y debemos 
usar la computación

00:17:07.659 --> 00:17:09.673
para ayudar a tomar mejores decisiones.

00:17:09.697 --> 00:17:15.029
Pero tenemos que apropiarnos de 
nuestra responsabilidad moral de juicio,

00:17:15.053 --> 00:17:17.871
y usar algoritmos dentro de ese marco,

00:17:17.895 --> 00:17:22.829
no como un medio para abdicar 
y delegar nuestras responsabilidades

00:17:22.854 --> 00:17:25.308
el uno al otro, como de humano a humano.

00:17:25.807 --> 00:17:28.415
La inteligencia artificial está aquí.

00:17:28.440 --> 00:17:31.861
Eso significa que hay que 
ajustarla cada vez más

00:17:31.885 --> 00:17:34.031
a los valores humanos y a la ética humana.

00:17:34.056 --> 00:17:35.210
Gracias.

00:17:35.233 --> 00:17:40.252
(Aplausos)

