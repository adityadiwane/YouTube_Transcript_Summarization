WEBVTT
Kind: captions
Language: fr

00:00:00.000 --> 00:00:07.000
Traducteur: Morgane Quilfen
Relecteur: Claire Ghyselen

00:00:12.739 --> 00:00:16.635
Mon premier travail
était programmeuse informatique

00:00:16.635 --> 00:00:18.881
durant ma première année à l'université --

00:00:18.881 --> 00:00:20.372
quand j'étais adolescente.

00:00:20.889 --> 00:00:24.291
Peu après avoir commencé
à écrire des programmes en entreprise,

00:00:24.799 --> 00:00:28.434
un responsable de l'entreprise
est venu me voir

00:00:28.458 --> 00:00:29.726
et m'a murmuré :

00:00:30.229 --> 00:00:33.090
« Peut-il dire si je mens ? »

00:00:33.806 --> 00:00:35.883
Il n'y avait personne d'autre
dans la pièce.

00:00:37.032 --> 00:00:41.421
« Qui peut dire si vous mentez ?
Et pourquoi chuchotez-vous ? »

00:00:42.266 --> 00:00:45.373
Le responsable a pointé du doigt
l'ordinateur dans la pièce.

00:00:45.397 --> 00:00:48.493
« Peut-il dire si je mens ? »

00:00:49.613 --> 00:00:53.975
Ce responsable avait une aventure
avec la réceptionniste.

00:00:53.999 --> 00:00:55.111
(Rires)

00:00:55.135 --> 00:00:56.901
J'étais toujours adolescente.

00:00:57.447 --> 00:00:59.466
J'ai lui ai murmuré-crié :

00:00:59.490 --> 00:01:03.114
« Oui, l'ordinateur peut dire
si vous mentez. »

00:01:03.138 --> 00:01:04.944
(Rires)

00:01:04.968 --> 00:01:07.891
J'ai rigolé, mais c'est de moi
qu'on peut se moquer.

00:01:07.915 --> 00:01:11.183
Il y a aujourd'hui
des systèmes informatiques

00:01:11.207 --> 00:01:14.755
qui peuvent repérer
les états émotionnels et les mensonges

00:01:14.779 --> 00:01:16.983
en traitant les informations
du visage humain.

00:01:17.248 --> 00:01:21.401
Les publicitaires et les gouvernements
sont très intéressés.

00:01:22.319 --> 00:01:24.321
J'étais devenue programmeuse informatique

00:01:24.321 --> 00:01:27.318
car j'étais l'une de ces gamines
folles de maths et de sciences.

00:01:27.942 --> 00:01:31.050
Mais, en chemin, j'avais découvert
les armes nucléaires

00:01:31.074 --> 00:01:34.026
et je me sentais très concernée
par l'éthique de la science.

00:01:34.050 --> 00:01:35.254
J'étais troublée.

00:01:35.278 --> 00:01:37.919
Cependant, du fait
de circonstances familiales,

00:01:37.943 --> 00:01:41.241
je devais aussi commencer à travailler
aussi vite que possible.

00:01:41.265 --> 00:01:44.564
Je me suis dit :
« Choisis un domaine technique

00:01:44.588 --> 00:01:46.384
où tu peux avoir un emploi facilement

00:01:46.408 --> 00:01:50.426
et où je n'ai pas à gérer
des questions d'éthique difficiles. »

00:01:51.022 --> 00:01:52.551
J'ai donc choisi l'informatique.

00:01:52.575 --> 00:01:53.679
(Rires)

00:01:53.703 --> 00:01:57.113
Eh bien, ah ah ah !
On peut se moquer de moi.

00:01:57.137 --> 00:01:59.951
Aujourd'hui, les informaticiens
construisent des plateformes

00:01:59.951 --> 00:02:04.124
qui contrôlent chaque jour
ce que voient un milliard de personnes.

00:02:05.052 --> 00:02:08.874
Ils développent des voitures
pouvant décider qui écraser.

00:02:09.707 --> 00:02:12.904
Ils construisent même
des machines, des armes

00:02:12.904 --> 00:02:15.449
qui pourraient tuer
des êtres humains dans une guerre.

00:02:15.449 --> 00:02:18.024
Il y a de l'éthique partout.

00:02:19.183 --> 00:02:21.241
L'intelligence artificielle est arrivée.

00:02:21.823 --> 00:02:25.297
Nous utilisons l'informatique
pour prendre toutes sortes de décisions,

00:02:25.321 --> 00:02:27.207
y compris de nouvelles décisions.

00:02:27.231 --> 00:02:29.427
Nous posons à l'informatique

00:02:29.427 --> 00:02:32.427
des questions auxquelles
il n'y a pas d'unique bonne réponse,

00:02:32.427 --> 00:02:33.629
qui sont subjectives,

00:02:33.653 --> 00:02:35.978
ouvertes et reposent sur des valeurs.

00:02:36.002 --> 00:02:37.760
Nous posons des questions comme :

00:02:37.784 --> 00:02:39.434
« Qui devrait-on embaucher ? »

00:02:40.096 --> 00:02:42.855
« Quelles nouvelles de quel ami
devrait-on vous montrer ? »

00:02:42.879 --> 00:02:45.145
« Quel prisonnier
va probablement récidiver ? »

00:02:45.514 --> 00:02:48.568
« Quel nouvel objet ou film
devrait être recommandé aux gens ? »

00:02:48.592 --> 00:02:51.964
Cela fait un certain temps
que nous utilisons des ordinateurs

00:02:51.988 --> 00:02:53.505
mais c'est différent.

00:02:53.529 --> 00:02:55.596
C'est un changement historique :

00:02:55.620 --> 00:03:00.957
car on ne peut pas utiliser l'informatique
pour des décisions si subjectives

00:03:00.981 --> 00:03:06.401
comme on utilise l'informatique
pour piloter un avion, construire un pont,

00:03:06.425 --> 00:03:07.684
aller sur la Lune.

00:03:08.449 --> 00:03:11.708
Les avions sont-ils plus sûrs ?
Un pont a-t-il bougé et est tombé ?

00:03:11.732 --> 00:03:16.230
Là, nous nous accordons
sur des repères assez clairs

00:03:16.254 --> 00:03:18.493
et les lois de la nature nous guident.

00:03:18.517 --> 00:03:21.911
Nous n'avons pas de tels ancres et repères

00:03:21.935 --> 00:03:25.898
pour les décisions
des affaires complexes humaines.

00:03:25.922 --> 00:03:30.159
Pour compliquer encore les choses,
nos logiciels gagnent en puissance

00:03:30.183 --> 00:03:33.956
mais sont aussi moins transparents
et plus complexes.

00:03:34.542 --> 00:03:36.582
Récemment, les dix dernières années,

00:03:36.606 --> 00:03:39.335
les algorithmes complexes
ont fait de grandes avancées.

00:03:39.359 --> 00:03:41.479
Ils peuvent reconnaître
les visages humains,

00:03:41.985 --> 00:03:44.040
déchiffrer l'écriture,

00:03:44.436 --> 00:03:46.502
détecter la fraude à la carte bancaire,

00:03:46.526 --> 00:03:47.715
bloquer le spam,

00:03:47.739 --> 00:03:49.776
traduire d'une langue à une autre,

00:03:49.800 --> 00:03:52.374
détecter les tumeurs en imagerie médicale,

00:03:52.398 --> 00:03:54.603
battre les humains aux échecs et au go.

00:03:55.264 --> 00:03:58.315
Beaucoup de ces progrès
découlent d'une méthode :

00:03:58.315 --> 00:04:00.175
« l'apprentissage de la machine ».

00:04:00.175 --> 00:04:03.362
Cette méthode est différente
de la programmation traditionnelle

00:04:03.386 --> 00:04:07.191
où l'on donne des instructions détaillées,
exactes, méticuleuses à l'ordinateur.

00:04:07.378 --> 00:04:11.560
Cela ressemble plus
à un système nourri de données,

00:04:11.584 --> 00:04:13.240
dont des données non structurées,

00:04:13.264 --> 00:04:15.542
comme celles générées
par notre vie numérique.

00:04:15.566 --> 00:04:18.296
Le système apprend
en parcourant ces données.

00:04:18.669 --> 00:04:20.195
Et aussi, c'est crucial,

00:04:20.219 --> 00:04:24.599
ces systèmes n'utilisent pas la logique
de la réponse unique.

00:04:24.623 --> 00:04:27.712
Ils ne produisent pas une seule réponse,
c'est plus probabiliste :

00:04:27.712 --> 00:04:31.089
« Celle-ci est probablement
plus proche de ce que vous cherchez. »

00:04:32.023 --> 00:04:35.093
L'avantage est que cette méthode
est très puissante.

00:04:35.117 --> 00:04:37.193
Le chef de l'IA chez Google l'a appelée :

00:04:37.217 --> 00:04:39.414
« l'efficacité irraisonnable
des données ».

00:04:39.791 --> 00:04:41.144
L'inconvénient est :

00:04:41.738 --> 00:04:44.809
nous ne comprenons pas vraiment
ce que le système a appris.

00:04:44.833 --> 00:04:46.420
En fait, c'est sa force.

00:04:46.946 --> 00:04:50.744
C'est moins comme donner
des instructions à un ordinateur ;

00:04:51.200 --> 00:04:55.264
plus comme entraîner
une machine-chiot-créature

00:04:55.288 --> 00:04:57.809
que nous ne comprenons
ni ne contrôlons vraiment.

00:04:58.362 --> 00:04:59.913
Voilà notre problème.

00:05:00.427 --> 00:05:04.689
C'est un problème quand cette intelligence
artificielle comprend mal les choses.

00:05:04.713 --> 00:05:08.253
C'est aussi un problème
quand elle comprend les choses

00:05:08.277 --> 00:05:11.905
car on ne sait pas différencier
ces situations pour un problème subjectif.

00:05:11.929 --> 00:05:14.268
Nous ignorons ce que pense cette chose.

00:05:15.493 --> 00:05:19.176
Considérez un algorithme d'embauche --

00:05:20.123 --> 00:05:24.434
un système utilisé pour embaucher des gens
en utilisant l'apprentissage des machines.

00:05:25.052 --> 00:05:28.631
Un tel système aurait été entraîné
sur les données des employés

00:05:28.655 --> 00:05:31.246
et chargé de trouver et embaucher

00:05:31.270 --> 00:05:34.308
des gens similaires à ceux
les plus performants de l'entreprise.

00:05:34.814 --> 00:05:35.967
Cela semble bien.

00:05:35.991 --> 00:05:37.924
Une fois, j'ai assisté à une conférence

00:05:37.924 --> 00:05:41.209
qui réunissait responsables
des ressources humaines et des dirigeants,

00:05:41.209 --> 00:05:42.369
des gens de haut niveau,

00:05:42.393 --> 00:05:43.982
avec de tels systèmes d'embauche.

00:05:43.982 --> 00:05:45.622
Ils étaient très excités.

00:05:45.646 --> 00:05:50.299
Ils pensaient que cela rendrait l'embauche
plus objective, moins biaisée

00:05:50.323 --> 00:05:53.323
et donnerait plus de chances
aux femmes et minorités

00:05:53.347 --> 00:05:55.535
face à des responsables RH partiaux.

00:05:55.559 --> 00:05:58.402
L'embauche humaine est partiale.

00:05:59.099 --> 00:06:00.284
Je sais.

00:06:00.308 --> 00:06:03.313
Dans l'un de mes premiers postes
en tant que programmeuse,

00:06:03.337 --> 00:06:07.205
ma responsable directe
venait parfois me voir

00:06:07.229 --> 00:06:10.982
très tôt le matin
ou très tard l'après-midi

00:06:11.006 --> 00:06:14.068
et elle disait : « Zeinep,
allons déjeuner ! »

00:06:14.724 --> 00:06:16.891
L'heure étrange me laissait perplexe.

00:06:16.915 --> 00:06:19.044
Il est 16h, déjeuner ?

00:06:19.068 --> 00:06:22.292
J'étais fauchée, le déjeuner était gratuit
donc j'y allais toujours.

00:06:22.618 --> 00:06:24.685
Plus tard, j'ai réalisé
ce qu'il se passait.

00:06:24.709 --> 00:06:29.255
Mes responsables directs
n'avaient pas dit à leurs responsables

00:06:29.279 --> 00:06:32.392
qu'ils avaient embauché
pour un travail sérieux une adolescente

00:06:32.416 --> 00:06:36.346
qui portait un jeans
et des baskets au travail.

00:06:37.174 --> 00:06:39.606
Je faisais du bon travail
mais mon allure clochait,

00:06:39.606 --> 00:06:41.139
j'avais les mauvais âge et sexe.

00:06:41.139 --> 00:06:44.469
Embaucher d'une manière
aveugle à la couleur et au sexe

00:06:44.493 --> 00:06:46.358
me semble très bien.

00:06:47.031 --> 00:06:50.372
Mais avec ces systèmes,
c'est plus compliqué, voici pourquoi :

00:06:50.968 --> 00:06:56.759
actuellement, les systèmes informatiques
peuvent déduire beaucoup vous concernant

00:06:56.783 --> 00:06:58.655
grâce à vos miettes numériques,

00:06:58.679 --> 00:07:01.012
même si vous n'avez rien révélé.

00:07:01.506 --> 00:07:04.433
Ils peuvent déduire
votre orientation sexuelle,

00:07:04.994 --> 00:07:06.300
vos traits de personnalité,

00:07:06.859 --> 00:07:08.232
vos tendances politiques.

00:07:08.830 --> 00:07:12.515
Ils ont des pouvoirs prédictifs
ayant une exactitude élevée.

00:07:13.362 --> 00:07:15.940
Pour des choses
que vous n'avez pas révélées.

00:07:15.964 --> 00:07:17.555
C'est de la déduction.

00:07:17.579 --> 00:07:20.840
J'ai une amie qui a développé
de tels systèmes informatiques

00:07:20.864 --> 00:07:24.505
pour prévoir la probabilité
d'une dépression clinique ou post-partum

00:07:24.529 --> 00:07:25.945
grâce à vos médias sociaux.

00:07:26.676 --> 00:07:28.373
Les résultats sont impressionnants.

00:07:28.492 --> 00:07:31.849
Son système peut prévoir
les risques de dépression

00:07:31.873 --> 00:07:35.776
des mois avant l'apparition
de tout symptôme --

00:07:35.800 --> 00:07:37.173
des mois avant.

00:07:37.197 --> 00:07:39.443
Aucun symptôme mais une prédiction.

00:07:39.467 --> 00:07:44.279
Elle espère que cela sera utilisé
pour des interventions précoces, super !

00:07:44.911 --> 00:07:47.261
Mais mettez cela
dans le contexte de l'embauche.

00:07:48.027 --> 00:07:51.073
Lors de cette conférence
de responsables des ressources humaines,

00:07:51.097 --> 00:07:55.806
j'ai approché une responsable
d'une très grande entreprise

00:07:55.830 --> 00:08:00.408
et lui ai dit : « Et si, à votre insu,

00:08:00.432 --> 00:08:06.981
votre système éliminait les gens
avec de forts risques de dépression ?

00:08:07.761 --> 00:08:11.137
Ils ne sont pas en dépression
mais ont plus de risques pour l'avenir.

00:08:11.923 --> 00:08:15.329
Et s'il éliminait les femmes
ayant plus de chances d'être enceintes

00:08:15.353 --> 00:08:17.939
dans un ou deux ans
mais ne le sont pas actuellement ?

00:08:18.844 --> 00:08:24.480
Et s'il embauchait des gens agressifs
car c'est la culture de l'entreprise ? »

00:08:25.173 --> 00:08:27.994
On ne peut pas le dire en regardant
la répartition par sexe.

00:08:27.994 --> 00:08:29.390
Cela peut être équilibré.

00:08:29.414 --> 00:08:32.971
Puisque c'est de l'apprentissage
de la machine, non du code traditionnel,

00:08:32.995 --> 00:08:37.902
il n'y a pas de variables appelées
« plus de risques de dépression »,

00:08:37.926 --> 00:08:39.759
« plus de risques d'être enceinte »,

00:08:39.783 --> 00:08:41.517
« échelle d'agressivité d'un mec ».

00:08:41.995 --> 00:08:45.674
Non seulement vous ignorez
ce que votre système utilise pour choisir,

00:08:45.698 --> 00:08:48.021
mais vous ignorez où chercher.

00:08:48.045 --> 00:08:49.291
C'est une boîte noire.

00:08:49.315 --> 00:08:52.122
Elle a un pouvoir prédictif
mais vous ne le comprenez pas.

00:08:52.486 --> 00:08:54.855
J'ai demandé : « Quelle garantie avez-vous

00:08:54.879 --> 00:08:58.552
pour vous assurer que votre boîte noire
ne fait rien de louche ? »

00:09:00.863 --> 00:09:04.741
Elle m'a regardée comme
si je venais de l'insulter.

00:09:04.765 --> 00:09:06.013
(Rires)

00:09:06.037 --> 00:09:08.078
Elle m'a fixée et m'a dit :

00:09:08.556 --> 00:09:12.889
« Je ne veux rien entendre de plus. »

00:09:13.458 --> 00:09:15.492
Puis elle s'est tournée et est partie.

00:09:16.064 --> 00:09:17.550
Elle n'était pas impolie.

00:09:17.574 --> 00:09:18.906
C'était clairement du :

00:09:18.906 --> 00:09:23.946
« ce que j'ignore n'est pas mon problème,
allez-vous en, regard meurtrier ».

00:09:23.946 --> 00:09:25.152
(Rires)

00:09:25.862 --> 00:09:29.701
Un tel système pourrait être moins biaisé

00:09:29.725 --> 00:09:31.828
que les responsables humains.

00:09:31.852 --> 00:09:33.998
Et il pourrait être monétairement censé.

00:09:34.573 --> 00:09:36.223
Mais il pourrait aussi mener

00:09:36.247 --> 00:09:40.995
à une fermeture du marché du travail
stable mais dissimulée

00:09:41.019 --> 00:09:43.312
pour les gens avec
plus de risques de dépression.

00:09:43.753 --> 00:09:46.349
Est-ce le genre de société
que nous voulons bâtir,

00:09:46.373 --> 00:09:48.658
sans même savoir que nous l'avons fait,

00:09:48.682 --> 00:09:51.405
car nous avons confié
la prise de décisions à des machines

00:09:51.405 --> 00:09:53.265
que nous ne comprenons pas vraiment ?

00:09:53.265 --> 00:09:54.723
Un autre problème :

00:09:55.314 --> 00:09:59.766
ces systèmes sont souvent entraînés
sur des données générées par nos actions,

00:09:59.790 --> 00:10:01.606
des empreintes humaines.

00:10:02.188 --> 00:10:05.996
Elles pourraient refléter nos préjugés

00:10:06.020 --> 00:10:09.613
et ces systèmes pourraient
apprendre nos préjugés,

00:10:09.637 --> 00:10:10.950
les amplifier

00:10:10.974 --> 00:10:12.392
et nous les retourner

00:10:12.416 --> 00:10:13.878
alors que nous nous disons :

00:10:13.902 --> 00:10:17.019
« Nous ne faisons que de l'informatique
neutre et objective. »

00:10:18.314 --> 00:10:20.991
Des chercheurs chez Google ont découvert

00:10:22.134 --> 00:10:25.463
qu'on a moins de chances de montrer
aux femmes plutôt qu'aux hommes

00:10:25.463 --> 00:10:28.463
des offres d'emploi avec un salaire élevé.

00:10:28.463 --> 00:10:30.993
Et chercher des noms afro-américains

00:10:31.017 --> 00:10:32.747
a plus de chances de retourner

00:10:32.747 --> 00:10:35.747
des publicités suggérant
un historique criminel,

00:10:35.747 --> 00:10:37.314
même quand il n'y en a pas.

00:10:38.693 --> 00:10:42.242
De tels préjugés cachés
et des algorithmes boîte noire

00:10:42.266 --> 00:10:46.239
qui sont parfois découverts
par les chercheurs, parfois non,

00:10:46.263 --> 00:10:48.924
peuvent avoir des conséquences
qui changent la vie.

00:10:49.958 --> 00:10:54.117
Dans le Wisconsin, un prévenu
a été condamné à 6 ans de prison

00:10:54.141 --> 00:10:55.736
pour avoir échappé à la police.

00:10:56.824 --> 00:10:58.010
Vous l'ignorez peut-être

00:10:58.034 --> 00:11:02.032
mais des algorithmes sont utilisés
pour les probations et les condamnations.

00:11:02.056 --> 00:11:05.011
Nous voulions savoir
comment ce score était calculé.

00:11:05.795 --> 00:11:07.460
C'est une boîte noire commerciale.

00:11:07.484 --> 00:11:11.689
L'entreprise a refusé que l'on conteste
son algorithme en audience publique.

00:11:12.396 --> 00:11:17.928
Mais ProPublica, une organisation
d'enquête, a audité cet algorithme

00:11:17.952 --> 00:11:19.968
avec des données publiques

00:11:19.992 --> 00:11:22.308
et a découvert
que les résultats étaient biaisés,

00:11:22.332 --> 00:11:25.961
que son pouvoir prédictif était mauvais,
à peine meilleur que la chance,

00:11:25.985 --> 00:11:30.401
et qu'il étiquetait les prévenus noirs
comme de futurs criminels

00:11:30.425 --> 00:11:34.320
avec un taux deux fois plus élevé
que pour les prévenus blancs.

00:11:35.891 --> 00:11:37.455
Considérez ce cas :

00:11:38.103 --> 00:11:41.955
cette femme était en retard
pour récupérer sa filleule

00:11:41.979 --> 00:11:44.084
à une école du comté de Broward,
en Floride,

00:11:44.757 --> 00:11:47.113
elle courait dans la rue
avec une amie à elle.

00:11:47.137 --> 00:11:51.236
Elles ont repéré une bécane et un vélo
non attachés sur un porche

00:11:51.260 --> 00:11:52.892
et ont bêtement sauté dessus.

00:11:52.916 --> 00:11:55.585
Alors qu'elles partaient,
une femme est sortie et a dit :

00:11:55.585 --> 00:11:57.744
« Hey ! C'est la bécane de mon fils ! »

00:11:57.768 --> 00:12:01.062
Elles l'ont lâchée, sont parties
mais ont été arrêtées.

00:12:01.086 --> 00:12:04.723
Elle avait tort, elle a été idiote
mais elle n'avait que 18 ans.

00:12:04.747 --> 00:12:07.291
Adolescente, elle avait commis
quelques méfaits.

00:12:07.808 --> 00:12:12.993
Pendant ce temps, cet homme a été arrêté
pour vol chez Home Depot --

00:12:13.017 --> 00:12:15.941
pour une valeur de 85$,
un crime mineur similaire.

00:12:16.766 --> 00:12:21.325
Mais il avait deux condamnations
pour vol à main armée.

00:12:21.955 --> 00:12:25.577
L'algorithme l'a considérée elle,
comme étant un risque important, pas lui.

00:12:26.746 --> 00:12:30.584
Deux ans plus tard, ProPublica a découvert
qu'elle n'avait pas récidivé.

00:12:30.584 --> 00:12:33.234
Son casier judiciaire compliquait
sa recherche d'emploi.

00:12:33.234 --> 00:12:35.294
Lui, d'un autre côté, avait récidivé

00:12:35.318 --> 00:12:39.154
et avait été condamné à 8 ans
pour un autre crime.

00:12:40.088 --> 00:12:43.457
Clairement, nous devons
auditer nos boîtes noires

00:12:43.481 --> 00:12:46.096
et ne pas leur laisser
ce genre de pouvoir incontrôlé.

00:12:46.120 --> 00:12:48.999
(Applaudissements)

00:12:50.087 --> 00:12:51.383
Les audits sont importants,

00:12:51.383 --> 00:12:54.353
mais ils ne résolvent pas
tous nos problèmes.

00:12:54.353 --> 00:12:57.151
Prenez le puissant algorithme
du fil d'actualités Facebook,

00:12:57.151 --> 00:13:01.968
celui qui classe tout
et décide quoi vous montrer

00:13:01.992 --> 00:13:04.276
des amis et des pages que vous suivez.

00:13:04.898 --> 00:13:07.243
Devrait-on vous montrer
une autre photo de bébé ?

00:13:07.243 --> 00:13:08.393
(Rires)

00:13:08.417 --> 00:13:11.013
Une note maussade d'une connaissance ?

00:13:11.449 --> 00:13:13.305
Une actualité importante mais dure ?

00:13:13.329 --> 00:13:14.811
Il n'y a pas de bonne réponse.

00:13:14.835 --> 00:13:17.494
Facebook optimise
pour vous engager envers le site :

00:13:17.518 --> 00:13:19.243
les j'aime, partages, commentaires.

00:13:20.168 --> 00:13:22.818
En août 2014,

00:13:22.818 --> 00:13:25.600
des manifestations ont éclaté
à Ferguson, dans le Missouri,

00:13:25.600 --> 00:13:29.991
après qu'un adolescent afro-américain
a été tué par un officier de police blanc

00:13:30.015 --> 00:13:31.585
dans des circonstances douteuses.

00:13:31.974 --> 00:13:36.691
La nouvelle des manifestations remplissait
mon fil d'actualité Twitter non filtré

00:13:36.714 --> 00:13:38.664
mais n'était pas sur mon Facebook.

00:13:39.182 --> 00:13:40.916
Était-ce mes amis Facebook ?

00:13:40.940 --> 00:13:42.972
J'ai désactivé l'algorithme Facebook,

00:13:43.472 --> 00:13:46.320
ce qui est difficile car Facebook
veut vous faire passer

00:13:46.344 --> 00:13:48.380
sous le contrôle de l'algorithme,

00:13:48.404 --> 00:13:50.642
et j'ai vu que mes amis en parlaient.

00:13:50.666 --> 00:13:53.175
C'est juste que l'algorithme
ne me le montrait pas.

00:13:53.199 --> 00:13:56.351
Après des recherches, j'ai découvert
que le problème est répandu.

00:13:56.351 --> 00:14:00.078
L'histoire de Ferguson
ne plaisait pas à l'algorithme.

00:14:00.102 --> 00:14:03.203
Ce n'était pas « aimable »,
qui allait cliquer sur « j'aime » ?

00:14:03.500 --> 00:14:05.706
Ce n'est même pas facile à commenter.

00:14:05.730 --> 00:14:07.101
Sans j'aime et commentaires,

00:14:07.125 --> 00:14:10.417
l'algorithme allait le montrer
à un nombre décroissant de gens,

00:14:10.441 --> 00:14:11.983
donc nous ne pouvions le voir.

00:14:12.946 --> 00:14:14.174
Cette semaine-là,

00:14:14.198 --> 00:14:16.496
Facebook a plutôt souligné ceci,

00:14:16.520 --> 00:14:18.746
le Ice Bucket Challenge.

00:14:18.770 --> 00:14:22.512
Cause méritante, lâcher d'eau glacée,
donner à une charité, très bien.

00:14:22.536 --> 00:14:24.440
Cela plaisait beaucoup à l'algorithme.

00:14:25.219 --> 00:14:27.832
La machine a pris
cette décision pour nous.

00:14:27.856 --> 00:14:31.353
Une conversation
très importante mais difficile

00:14:31.377 --> 00:14:32.932
aurait pu être étouffée

00:14:32.956 --> 00:14:35.652
si Facebook avait été le seul canal.

00:14:36.117 --> 00:14:39.914
Finalement, ces systèmes
peuvent aussi avoir tort

00:14:39.938 --> 00:14:42.674
de façons qui ne ressemblent pas
aux systèmes humains.

00:14:42.698 --> 00:14:45.620
Vous souvenez-vous de Watson,
le système d'IA d'IBM

00:14:45.644 --> 00:14:48.772
qui a éliminé les participants humains
dans Jeopardy ?

00:14:49.131 --> 00:14:50.559
C'était un super joueur.

00:14:50.583 --> 00:14:54.152
Mais, pour la finale de Jeopardy,
on a posé cette question à Watson :

00:14:54.659 --> 00:14:57.565
« Le plus grand aéroport
ayant le nom d'un héros de 39-45,

00:14:57.565 --> 00:14:59.897
le second plus grand
pour une bataille de 39-45. »

00:14:59.897 --> 00:15:01.269
(Signal sonore de fin)

00:15:01.582 --> 00:15:02.764
Chicago.

00:15:02.788 --> 00:15:04.438
Les deux humains avaient raison.

00:15:04.697 --> 00:15:09.045
Watson, par contre,
a répondu « Toronto » --

00:15:09.069 --> 00:15:11.557
à une question
sur les villes des États-Unis !

00:15:11.597 --> 00:15:14.497
L'impressionnant système
a aussi fait une erreur

00:15:14.521 --> 00:15:18.172
qu'un humain ne ferait jamais,
qu'un CE1 ne ferait jamais.

00:15:18.823 --> 00:15:21.932
Notre intelligence artificielle
peut échouer

00:15:21.956 --> 00:15:25.056
de façons ne correspondant pas
aux schémas d'erreurs humaines,

00:15:25.080 --> 00:15:28.030
de façons inattendues et imprévues.

00:15:28.054 --> 00:15:31.692
Il serait lamentable de ne pas obtenir
un emploi pour lequel on est qualifié

00:15:31.716 --> 00:15:35.443
mais ce serait pire si c'était à cause
d'un dépassement de pile

00:15:35.467 --> 00:15:36.899
dans une sous-routine.

00:15:36.923 --> 00:15:38.502
(Rires)

00:15:38.526 --> 00:15:41.312
En mai 2010,

00:15:41.336 --> 00:15:45.380
un crash éclair sur Wall Street
alimenté par une boucle de rétroaction

00:15:45.404 --> 00:15:48.432
dans un algorithme de vente de Wall Street

00:15:48.456 --> 00:15:52.640
a fait perdre mille milliards de dollars
en 36 minutes.

00:15:53.722 --> 00:15:55.909
Je refuse de penser
au sens du mot « erreur »

00:15:55.933 --> 00:15:59.522
dans le contexte des armes
mortelles autonomes.

00:16:01.894 --> 00:16:05.684
Oui, les humains
ont toujours été partiaux.

00:16:05.708 --> 00:16:07.884
Les preneurs de décision et gardiens,

00:16:07.908 --> 00:16:11.401
dans les tribunaux,
les actualités, en guerre...

00:16:11.425 --> 00:16:14.463
Ils font des erreurs ;
mais c'est de cela dont je parle.

00:16:14.487 --> 00:16:18.008
Nous ne pouvons pas échapper
à ces questions difficiles.

00:16:18.596 --> 00:16:22.112
Nous ne pouvons pas sous-traiter
nos responsabilités aux machines.

00:16:22.676 --> 00:16:26.884
(Applaudissements)

00:16:29.089 --> 00:16:33.536
L'intelligence artificielle n'offre pas
une carte « sortie de l'éthique ».

00:16:34.742 --> 00:16:38.373
Le scientifique des données Fred Benenson
qualifie cela de lavage des maths.

00:16:38.373 --> 00:16:39.536
Il nous faut l'opposé.

00:16:39.560 --> 00:16:44.948
Nous devons cultiver la suspicion,
le contrôle et l'enquête de l'algorithme.

00:16:45.380 --> 00:16:48.578
Nous devons nous assurer
de la responsabilité des algorithmes,

00:16:48.602 --> 00:16:51.047
les auditer et avoir
une transparence significative.

00:16:51.380 --> 00:16:54.614
Nous devons accepter
qu'apporter les maths et l'informatique

00:16:54.638 --> 00:16:57.688
dans les affaires humaines
désordonnées et basées sur des valeurs

00:16:57.688 --> 00:17:00.016
n'apporte pas l'objectivité

00:17:00.040 --> 00:17:02.448
mais plutôt que la complexité
des affaires humaines

00:17:02.448 --> 00:17:04.148
envahit les algorithmes.

00:17:04.148 --> 00:17:07.635
Nous devrions utiliser l'informatique

00:17:07.659 --> 00:17:09.673
pour prendre de meilleures décisions.

00:17:09.697 --> 00:17:15.029
Mais nous devons assumer
notre responsabilité morale de jugement

00:17:15.053 --> 00:17:17.871
et utiliser les algorithmes dans ce cadre,

00:17:17.895 --> 00:17:22.830
pas comme un moyen d'abdiquer
et sous-traiter nos responsabilités

00:17:22.854 --> 00:17:25.308
d'un humain à un autre.

00:17:25.807 --> 00:17:28.416
L'intelligence artificielle est arrivée.

00:17:28.440 --> 00:17:31.861
Cela signifie que nous devons
nous accrocher encore plus

00:17:31.885 --> 00:17:34.032
aux valeurs et éthiques humaines.

00:17:34.056 --> 00:17:35.210
Merci.

00:17:35.234 --> 00:17:40.254
(Applaudissements)

