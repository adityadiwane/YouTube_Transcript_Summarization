WEBVTT
Kind: captions
Language: pt

00:00:00.000 --> 00:00:07.000
Tradutor: Raissa Mendes
Revisor: Ruy Lopes Pereira

00:00:12.739 --> 00:00:16.701
Meu primeiro emprego na vida foi
como programadora de computador

00:00:16.745 --> 00:00:18.841
já no primeiro ano de faculdade,

00:00:18.865 --> 00:00:20.372
praticamente uma adolescente.

00:00:20.889 --> 00:00:22.621
Logo que comecei a trabalhar,

00:00:22.645 --> 00:00:24.255
escrevendo software numa empresa,

00:00:24.799 --> 00:00:28.434
um gerente que trabalhava lá
se aproximou de mim

00:00:28.458 --> 00:00:29.926
e começou a sussurrar:

00:00:30.229 --> 00:00:33.090
"Ele consegue dizer se estou mentindo?"

00:00:33.806 --> 00:00:35.946
Não havia mais ninguém na sala.

00:00:35.946 --> 00:00:37.032
(Risos)

00:00:37.032 --> 00:00:41.421
"Quem consegue dizer se você está
mentindo? E por que estamos sussurrando?"

00:00:42.266 --> 00:00:45.373
O gerente apontou
para o computador na sala.

00:00:45.397 --> 00:00:48.493
"Ele consegue dizer se estou mentindo?"

00:00:49.613 --> 00:00:53.975
Bem, aquele gerente estava tendo 
um caso com a recepcionista.

00:00:53.999 --> 00:00:55.111
(Risos)

00:00:55.135 --> 00:00:57.281
E eu ainda era uma adolescente.

00:00:57.447 --> 00:00:59.466
Assim, gritei sussurrando de volta:

00:00:59.490 --> 00:01:03.114
"Sim, o computador sabe
se você está mentindo".

00:01:03.138 --> 00:01:04.944
(Risos)

00:01:04.968 --> 00:01:07.891
Eu ri, mas, na verdade,
estava rindo de mim mesma.

00:01:07.915 --> 00:01:11.183
Atualmente, há sistemas computacionais

00:01:11.207 --> 00:01:14.755
que conseguem perceber
estados emocionais e até mentiras

00:01:14.779 --> 00:01:16.823
ao processar rostos humanos.

00:01:17.248 --> 00:01:21.591
Anunciantes e até governos
estão muito interessados nisso.

00:01:22.319 --> 00:01:24.181
Tornei-me programadora de computador,

00:01:24.205 --> 00:01:27.318
pois era uma dessas crianças
loucas por matemática e ciências.

00:01:27.942 --> 00:01:31.050
Mas, a uma certa altura,
descobri as armas nucleares

00:01:31.074 --> 00:01:34.026
e passei a me preocupar
com a ética da ciência.

00:01:34.050 --> 00:01:35.254
Aquilo me perturbou.

00:01:35.278 --> 00:01:37.919
No entanto, devido
a circunstâncias familiares,

00:01:37.943 --> 00:01:41.241
eu também precisava começar
a trabalhar o mais rápido possível.

00:01:41.265 --> 00:01:44.318
Assim, pensei comigo mesma:
"Ei, vou escolher a área técnica,

00:01:44.318 --> 00:01:46.384
onde posso conseguir
um emprego facilmente

00:01:46.408 --> 00:01:50.426
e não tenho de lidar com quaisquer
questões éticas perturbadoras".

00:01:51.022 --> 00:01:52.551
Então escolhi os computadores.

00:01:52.575 --> 00:01:53.679
(Risos)

00:01:53.703 --> 00:01:57.113
Bem, ha, ha, ha! Eu virei a piada.

00:01:57.137 --> 00:01:59.891
Hoje, os cientistas da computação
constroem plataformas

00:01:59.915 --> 00:02:04.454
que controlam o que um bilhão
de pessoas veem todos os dias.

00:02:05.052 --> 00:02:08.827
Eles estão desenvolvendo carros
que poderiam decidir quem atropelar.

00:02:08.827 --> 00:02:09.707
(Risos)

00:02:09.707 --> 00:02:12.920
Estão construindo até mesmo
máquinas e armas

00:02:12.944 --> 00:02:15.229
que podem matar seres humanos na guerra.

00:02:15.253 --> 00:02:18.304
É ética o tempo todo.

00:02:19.183 --> 00:02:21.241
A inteligência de máquina chegou.

00:02:21.823 --> 00:02:25.297
Hoje em dia usamos a computação
para tomar todo tipo de decisão,

00:02:25.321 --> 00:02:27.207
mas também novos tipos de decisão.

00:02:27.231 --> 00:02:32.403
Perguntamos aos computadores coisas
que não têm apenas uma resposta correta,

00:02:32.427 --> 00:02:35.929
que são subjetivas, abertas
e que envolvem julgamento de valor.

00:02:36.002 --> 00:02:39.240
Perguntamos coisas do tipo:
"Quem a empresa deve contratar?"

00:02:40.096 --> 00:02:42.855
"Que atualização de qual amigo
deve ser mostrada?"

00:02:42.879 --> 00:02:45.535
"Qual condenado tem mais chance
de reincidir num crime?"

00:02:45.535 --> 00:02:48.568
"Quais notícias ou filmes devem
ser recomendados às pessoas?"

00:02:48.592 --> 00:02:51.964
Sim, temos usado os computadores
já faz um bom tempo,

00:02:51.988 --> 00:02:53.505
mas isso é diferente.

00:02:53.529 --> 00:02:55.596
Essa é uma virada histórica,

00:02:55.620 --> 00:03:00.957
pois não podemos confiar na computação
para essas decisões subjetivas

00:03:00.981 --> 00:03:05.285
da mesma forma que podemos confiar
na computação para pilotar aviões,

00:03:05.285 --> 00:03:07.684
construir pontes, ir à Lua.

00:03:08.129 --> 00:03:11.708
Os aviões estão mais seguros?
A ponte balançou e caiu?

00:03:11.732 --> 00:03:16.230
Nesses casos, concordamos de forma
bem clara com os parâmetros,

00:03:16.254 --> 00:03:18.493
e temos as leis da natureza como baliza.

00:03:18.517 --> 00:03:21.911
Nós não temos tais âncoras ou marcos

00:03:21.935 --> 00:03:25.898
para a tomada de decisões
em negócios humanos confusos.

00:03:26.262 --> 00:03:30.159
Para complicar ainda mais as coisas,
nosso software está ficando mais poderoso,

00:03:30.183 --> 00:03:33.956
mas também menos transparente
e mais complexo.

00:03:34.542 --> 00:03:36.582
Recentemente, na década passada,

00:03:36.606 --> 00:03:39.335
algoritmos complexos deram passos enormes.

00:03:39.359 --> 00:03:41.659
Eles conseguem reconhecer rostos humanos.

00:03:41.985 --> 00:03:44.040
Eles conseguem decifrar caligrafia.

00:03:44.436 --> 00:03:47.722
Eles conseguem detectar fraude
de cartão de crédito, bloquear spam,

00:03:47.739 --> 00:03:49.776
conseguem traduzir línguas

00:03:49.800 --> 00:03:52.374
e detectar tumores em exames de imagem.

00:03:52.398 --> 00:03:55.163
Conseguem vencer humanos
em jogos de xadrez e Go.

00:03:55.264 --> 00:03:59.768
Muito desse progresso vem de um método
chamado "aprendizado de máquina".

00:04:00.175 --> 00:04:03.362
O aprendizado de máquina é diferente
da programação tradicional

00:04:03.386 --> 00:04:06.971
em que instruções detalhadas, exatas
e meticulosas são dadas ao computador.

00:04:07.378 --> 00:04:11.560
É mais como pegar um sistema
e alimentá-lo com montes de dados,

00:04:11.584 --> 00:04:13.240
incluindo dados não estruturados,

00:04:13.264 --> 00:04:15.542
como os que geramos
em nossas vidas digitais.

00:04:15.566 --> 00:04:18.296
E o sistema aprende revirando esses dados.

00:04:18.669 --> 00:04:20.195
Além disso, fundamentalmente,

00:04:20.219 --> 00:04:24.599
esses sistemas não operam
sob a lógica de uma resposta única.

00:04:24.623 --> 00:04:27.582
Eles não produzem uma resposta
simples; é mais probabilidade:

00:04:27.606 --> 00:04:31.379
"É provável que isto aqui seja
o que você está procurando".

00:04:32.023 --> 00:04:35.093
Bem, a vantagem é que esse método
é realmente poderoso.

00:04:35.117 --> 00:04:37.193
O responsável pela IA do Google o chamou

00:04:37.217 --> 00:04:39.414
de "a eficácia irracional dos dados".

00:04:39.791 --> 00:04:41.254
O lado negativo é

00:04:41.738 --> 00:04:44.703
não sabermos realmente
o que o sistema aprendeu.

00:04:44.703 --> 00:04:47.010
Na realidade, é aí que está seu poder.

00:04:47.010 --> 00:04:50.744
Isso tem pouco a ver com dar
instruções a um computador;

00:04:51.200 --> 00:04:55.264
é mais como adestrar uma máquina-filhote

00:04:55.288 --> 00:04:57.608
que não controlamos
ou conhecemos de verdade.

00:04:57.608 --> 00:04:58.362
(Risos)

00:04:58.362 --> 00:05:00.122
Então, este é o nosso problema.

00:05:00.432 --> 00:05:01.427
É um problema

00:05:01.427 --> 00:05:04.809
quando esse sistema de inteligência
artificial entende as coisas errado.

00:05:04.809 --> 00:05:08.253
Também é um problema
quando ele entende certo,

00:05:08.277 --> 00:05:11.905
pois não sabemos distinguir os dois
quando se trata de um problema subjetivo.

00:05:11.929 --> 00:05:14.268
Não sabemos o que esta coisa
está pensando.

00:05:15.493 --> 00:05:19.456
Por exemplo, vamos imaginar
um algoritmo de contratação,

00:05:20.123 --> 00:05:22.542
um sistema usado para contratar pessoas,

00:05:22.542 --> 00:05:25.052
usando o aprendizado de máquina.

00:05:25.052 --> 00:05:28.631
Tal sistema teria sido treinado
em dados prévios dos empregados

00:05:28.655 --> 00:05:31.246
e treinado para encontrar e contratar

00:05:31.270 --> 00:05:34.498
pessoas como os atuais empregados
com alto desempenho na empresa.

00:05:34.814 --> 00:05:35.967
Parece uma coisa boa.

00:05:35.991 --> 00:05:37.990
Uma vez, fui a um seminário

00:05:38.014 --> 00:05:41.139
que reuniu gerentes e executivos
da área de recursos humanos,

00:05:41.163 --> 00:05:44.059
pessoas de alto nível,
usando tais sistemas para contratar.

00:05:44.059 --> 00:05:45.622
Eles estavam muito empolgados.

00:05:45.646 --> 00:05:50.299
Achavam que isso tornaria a contratação
mais objetiva, menos parcial,

00:05:50.323 --> 00:05:53.323
e daria a mulheres e minorias
uma melhor oportunidade

00:05:53.347 --> 00:05:55.535
versus gerentes humanos tendenciosos.

00:05:55.559 --> 00:05:58.402
Pois vejam: a contratação
humana é tendenciosa.

00:05:59.099 --> 00:06:00.284
Sei bem disso.

00:06:00.308 --> 00:06:03.313
Quero dizer, num dos meus primeiros
empregos como programadora,

00:06:03.337 --> 00:06:07.205
minha gerente imediata
às vezes vinha ao meu setor

00:06:07.229 --> 00:06:10.982
bem cedinho pela manhã
ou bem no final da tarde,

00:06:11.006 --> 00:06:14.068
e me chamava: "Zeynep, vamos almoçar!"

00:06:14.724 --> 00:06:16.891
Eu ficava atônita com o horário estranho.

00:06:16.915 --> 00:06:19.044
Eram quatro da tarde. Almoço?

00:06:19.068 --> 00:06:22.162
Eu não tinha grana, então,
almoço grátis, eu sempre ia.

00:06:22.358 --> 00:06:24.685
Só mais tarde percebi
o que estava acontecendo.

00:06:24.709 --> 00:06:29.255
Meus gerentes imediatos não tinham
confessado a seus superiores

00:06:29.279 --> 00:06:33.402
que a programadora que tinham contratado
para um emprego sério era uma adolescente

00:06:33.932 --> 00:06:36.776
que vinha trabalhar de jeans e tênis.

00:06:37.074 --> 00:06:39.376
Eu trabalhava direito, só parecia errada,

00:06:39.400 --> 00:06:41.099
e tinha a idade e o gênero errados.

00:06:41.123 --> 00:06:44.469
Assim, contratar independente
da raça e do gênero

00:06:44.493 --> 00:06:46.818
certamente me parece uma coisa boa.

00:06:47.031 --> 00:06:50.622
Mas, com esses sistemas,
é mais complicado, e eis a razão:

00:06:50.968 --> 00:06:56.759
hoje, sistemas de computador podem
inferir todo tipo de coisas sobre nós

00:06:56.783 --> 00:06:58.655
por meio de nossas pegadas digitais,

00:06:58.679 --> 00:07:01.012
mesmo coisas que não tivermos revelado.

00:07:01.506 --> 00:07:04.433
Eles podem inferir
nossa orientação sexual,

00:07:04.994 --> 00:07:06.700
os traços de nossa personalidade,

00:07:06.859 --> 00:07:08.362
nossas tendências políticas.

00:07:08.830 --> 00:07:12.515
Eles têm poder preditivo
com altos níveis de precisão.

00:07:13.362 --> 00:07:17.500
Vejam bem, para coisas
que não revelamos: isso é inferência.

00:07:17.579 --> 00:07:20.840
Tenho uma amiga que desenvolveu
um sistema de computador

00:07:20.864 --> 00:07:24.505
para predizer a probabilidade
de depressão clínica pós-parto

00:07:24.529 --> 00:07:26.205
com base em dados de mídia social.

00:07:26.676 --> 00:07:28.423
Os resultados são impressionantes.

00:07:28.492 --> 00:07:31.849
O sistema dela prevê
a probabilidade de depressão

00:07:31.873 --> 00:07:35.776
meses antes do surgimento
de quaisquer sintomas,

00:07:35.800 --> 00:07:37.173
meses antes.

00:07:37.197 --> 00:07:39.443
Mesmo sem sintomas, ele prevê.

00:07:39.467 --> 00:07:43.491
Ela espera que isso seja usado
para intervenção precoce.

00:07:43.491 --> 00:07:44.481
Ótimo!

00:07:44.911 --> 00:07:47.321
Mas coloque isso
no contexto da contratação.

00:07:48.027 --> 00:07:51.073
Então, nesse seminário
de gerentes de recursos humanos,

00:07:51.097 --> 00:07:55.470
eu me aproximei de uma gerente
com alto cargo numa grande empresa,

00:07:55.830 --> 00:08:00.408
e perguntei a ela: "Olhe,
e se, sem seu conhecimento,

00:08:00.432 --> 00:08:06.981
seu sistema estiver cortando pessoas
com probabilidade de depressão futura?

00:08:07.761 --> 00:08:11.587
Elas não estão deprimidas agora,
mas, no futuro, bem provavelmente.

00:08:11.923 --> 00:08:15.329
E se estiver cortando mulheres
com maior probabilidade de engravidar

00:08:15.353 --> 00:08:18.329
dentro de um ou dois anos,
mas que não estejam grávidas agora?

00:08:18.844 --> 00:08:24.480
E se contratar pessoas agressivas
por causa da cultura da empresa?

00:08:25.173 --> 00:08:27.848
Não se pode saber isso
com análises de gênero.

00:08:27.848 --> 00:08:29.390
Isso pode estar equacionado.

00:08:29.414 --> 00:08:32.971
E, como isso é aprendizado de máquina, 
não é codificação tradicional,

00:08:32.995 --> 00:08:37.902
não há uma variável chamada
'alto risco de depressão',

00:08:37.926 --> 00:08:39.759
'maior risco de gravidez',

00:08:39.783 --> 00:08:41.517
'pessoa altamente agressiva'.

00:08:41.995 --> 00:08:45.674
Não só você não sabe o que
seu sistema está selecionando,

00:08:45.698 --> 00:08:48.021
como também não sabe onde começar a olhar.

00:08:48.045 --> 00:08:49.291
É uma caixa-preta.

00:08:49.315 --> 00:08:52.122
Ele tem poder preditivo,
mas você não o entende.

00:08:52.486 --> 00:08:54.855
Que salvaguardas", perguntei, "você teria

00:08:54.879 --> 00:08:58.552
para se assegurar de que sua caixa-preta
não está fazendo algo suspeito?"

00:09:00.863 --> 00:09:04.741
Ela olhou para mim como se eu tivesse
pisado no rabo de dez cachorrinhos.

00:09:04.765 --> 00:09:06.013
(Risos)

00:09:06.037 --> 00:09:08.308
Ela olhou bem pra mim e falou:

00:09:08.556 --> 00:09:13.069
"Eu não quero ouvir
nem mais uma palavra sobre isso".

00:09:13.458 --> 00:09:15.752
E virou as costas e foi embora.

00:09:16.064 --> 00:09:17.550
Vejam bem, ela não foi rude.

00:09:17.574 --> 00:09:18.796
Era claramente:

00:09:18.966 --> 00:09:23.906
"O que eu não sei não é problema meu,
vai embora, agourenta".

00:09:23.906 --> 00:09:25.482
(Risos)

00:09:25.862 --> 00:09:29.701
Vejam, tal sistema pode até
ser menos tendencioso

00:09:29.725 --> 00:09:31.828
do que gerentes humanos de alguma forma.

00:09:31.852 --> 00:09:34.398
E poderia fazer sentido financeiramente.

00:09:34.573 --> 00:09:36.223
Mas também poderia levar

00:09:36.247 --> 00:09:40.995
a um constante mas sorrateiro
fechamento do mercado de trabalho

00:09:41.019 --> 00:09:43.312
para pessoas com alto risco de depressão.

00:09:43.753 --> 00:09:46.349
É esse tipo de sociedade
que queremos construir,

00:09:46.373 --> 00:09:48.658
sem nem sequer saber que fizemos isso,

00:09:48.682 --> 00:09:52.646
por termos dado às máquinas um poder
de decisão que não entendemos totalmente?

00:09:53.265 --> 00:09:54.893
Outro problema é o seguinte:

00:09:55.314 --> 00:09:59.536
esses sistemas normalmente são treinados
com dados gerados pelas nossas ações,

00:09:59.790 --> 00:10:01.566
vestígios humanos.

00:10:02.188 --> 00:10:05.796
Bem, eles poderiam estar apenas
refletindo nossas tendências,

00:10:06.020 --> 00:10:09.613
e esses sistemas poderiam
estar pegando nossas tendências,

00:10:09.637 --> 00:10:13.560
amplificando-as e devolvendo-as para nós,
enquanto dizemos a nós mesmos:

00:10:13.560 --> 00:10:16.829
"Estamos fazendo apenas
computação objetiva e neutra".

00:10:18.314 --> 00:10:21.121
Pesquisadores descobriram que, no Google,

00:10:22.134 --> 00:10:27.447
são mostrados menos anúncios de empregos
bem pagos às mulheres do que aos homens.

00:10:28.463 --> 00:10:30.993
E, numa pesquisa de nomes afro-americanos,

00:10:31.017 --> 00:10:35.723
é provável que nos sejam mostrados
anúncios sugerindo história criminal,

00:10:35.747 --> 00:10:37.624
mesmo quando não há nenhuma.

00:10:38.693 --> 00:10:42.242
Tais tendências escondidas
e algoritmos caixas-pretas,

00:10:42.266 --> 00:10:46.239
que os pesquisadores às vezes revelam,
mas às vezes desconhecemos,

00:10:46.263 --> 00:10:48.924
podem afetar a vida das pessoas.

00:10:49.958 --> 00:10:54.117
Em Wisconsin, um réu foi sentenciado
a seis anos de prisão

00:10:54.141 --> 00:10:56.026
por fugir da polícia.

00:10:56.734 --> 00:10:58.010
Talvez não saibam disso,

00:10:58.034 --> 00:11:02.032
mas os algoritmos estão sendo cada vez
mais usados em decisões judiciais.

00:11:02.056 --> 00:11:05.011
E o réu quis saber:
como esse número foi calculado?

00:11:05.795 --> 00:11:07.460
É uma caixa-preta comercial.

00:11:07.484 --> 00:11:12.049
A empresa se recusou a ter
seu algoritmo exposto no tribunal.

00:11:12.396 --> 00:11:16.452
Mas a ProPublica, uma organização
investigativa sem fins lucrativos,

00:11:16.452 --> 00:11:19.968
auditou esse algoritmo com os dados
públicos que conseguiu encontrar

00:11:19.992 --> 00:11:22.308
e descobriu que os resultados
eram tendenciosos,

00:11:22.332 --> 00:11:25.961
e seu poder preditivo era deplorável,
apenas um pouco melhor do que o acaso,

00:11:25.985 --> 00:11:30.401
e estava rotulando erroneamente
réus negros como futuros criminosos

00:11:30.425 --> 00:11:34.480
duas vezes mais do que os réus brancos.

00:11:35.891 --> 00:11:37.585
Vejam por exemplo este caso:

00:11:38.103 --> 00:11:41.955
esta mulher se atrasou
para buscar sua parente

00:11:41.979 --> 00:11:44.324
numa escola no Condado
de Broward, na Flórida.

00:11:44.757 --> 00:11:47.113
Correndo pela rua com uma amiga,

00:11:47.137 --> 00:11:51.236
elas viram uma bicicleta sem cadeado
e uma lambreta numa varanda

00:11:51.260 --> 00:11:52.892
e, impensadamente, pularam nela.

00:11:52.916 --> 00:11:55.389
Quando estavam indo embora,
uma mulher saiu e falou:

00:11:55.389 --> 00:11:57.744
"Ei! Esta é a bicicleta do meu filho!"

00:11:57.768 --> 00:12:01.062
Elas largaram a bicicleta,
fugiram, mas foram presas.

00:12:01.086 --> 00:12:04.723
Ela errou, foi infantil,
mas tinha apenas 18 anos.

00:12:04.747 --> 00:12:07.501
Ela tinha algumas contravenções juvenis.

00:12:07.808 --> 00:12:12.993
Enquanto isso, aquele homem
tinha sido preso por furtar na Home Depot

00:12:13.017 --> 00:12:16.171
bens no valor de US$ 85,
um crime pequeno similar.

00:12:16.766 --> 00:12:21.325
Mas ele tinha duas condenações
prévias por roubo à mão armada.

00:12:21.715 --> 00:12:25.647
No entanto, o algoritmo a classificou
como sendo de alto risco, e ele não.

00:12:26.586 --> 00:12:30.484
Dois anos depois, a ProPublica descobriu
que ela não tinha reincidido em crime.

00:12:30.484 --> 00:12:33.364
E foi muito difícil conseguir
um emprego com esse histórico.

00:12:33.364 --> 00:12:35.294
Ele, por outro lado, reincidiu no crime

00:12:35.318 --> 00:12:39.454
e agora está cumprindo oito anos
de prisão por um crime posterior.

00:12:40.088 --> 00:12:43.457
Claramente, precisamos
auditar nossas caixas-pretas

00:12:43.481 --> 00:12:46.096
e não deixá-las ter esse tipo
de poder sem controle.

00:12:46.120 --> 00:12:48.999
(Aplausos)

00:12:50.087 --> 00:12:54.329
Auditorias são ótimas e importantes,
mas não resolvem todos os problemas.

00:12:54.353 --> 00:12:57.101
Peguem o poderoso algoritmo do Facebook.

00:12:57.125 --> 00:13:01.968
Sabe aquele que escolhe o que nos mostrar

00:13:01.992 --> 00:13:04.276
entre todos os amigos
e páginas que seguimos?

00:13:04.678 --> 00:13:07.173
Será que deveriam lhe mostrar
uma outra foto de bebê?

00:13:07.197 --> 00:13:08.393
(Risos)

00:13:08.417 --> 00:13:11.013
Um comentário estranho de um conhecido?

00:13:11.449 --> 00:13:13.305
Uma notícia importante, mas difícil?

00:13:13.329 --> 00:13:14.811
Não existe resposta certa.

00:13:14.835 --> 00:13:17.494
O Facebook o aperfeiçoa pelo uso do site:

00:13:17.518 --> 00:13:19.633
curtidas, compartilhamentos, comentários.

00:13:20.168 --> 00:13:22.864
Em agosto de 2014,

00:13:22.888 --> 00:13:25.550
houve uma onda de protestos
em Ferguson, no Missouri,

00:13:25.574 --> 00:13:29.991
depois do assassinato de um adolescente
afro-americano por um policial branco,

00:13:30.015 --> 00:13:31.835
sob circunstâncias nebulosas.

00:13:31.835 --> 00:13:34.201
Notícias sobre os protestos
estavam por toda parte

00:13:34.221 --> 00:13:36.690
no "feed" do meu Twitter
sem algoritmo de filtragem,

00:13:36.714 --> 00:13:38.914
mas em nenhum lugar no meu Facebook.

00:13:39.182 --> 00:13:40.916
Seriam meus amigos no Facebook?

00:13:40.940 --> 00:13:42.972
Eu desabilitei o algoritmo do Facebook,

00:13:43.472 --> 00:13:46.320
o que é difícil,
pois o Facebook espera que nós

00:13:46.344 --> 00:13:48.380
fiquemos sob o controle do algoritmo,

00:13:48.404 --> 00:13:50.876
e vi que meus amigos
estavam falando sobre o assunto.

00:13:50.876 --> 00:13:53.025
Mas o algoritmo
simplesmente não mostrava.

00:13:53.025 --> 00:13:56.241
Fui pesquisar e descobri
que era um problema geral.

00:13:56.265 --> 00:14:00.078
A história de Ferguson não
era compatível com o algoritmo.

00:14:00.102 --> 00:14:02.813
Não era "curtível";
quem ia "curtir" aquilo?

00:14:03.500 --> 00:14:05.806
Não era fácil nem mesmo
comentar sobre o assunto.

00:14:05.806 --> 00:14:07.101
Sem curtidas e comentários,

00:14:07.125 --> 00:14:10.557
era provável que o algoritmo mostrasse
isso para cada vez menos pessoas,

00:14:10.557 --> 00:14:12.643
assim, não pudemos ver isto.

00:14:12.946 --> 00:14:16.190
Em seu lugar, naquela semana,
o algoritmo do Facebook priorizou isto:

00:14:16.190 --> 00:14:18.886
o desafio do balde da ELA,
esclerose lateral amiotrófica.

00:14:18.886 --> 00:14:22.512
Causa importante: jogar água gelada,
doar para caridade, tudo bem.

00:14:22.536 --> 00:14:24.650
Mas era supercompatível com o algoritmo.

00:14:25.119 --> 00:14:27.832
A máquina tomou essa decisão por nós.

00:14:27.856 --> 00:14:31.353
Uma conversa muito importante,
mas muito difícil,

00:14:31.377 --> 00:14:32.932
teria sido atenuada,

00:14:32.956 --> 00:14:35.652
caso o Facebook fosse o único canal.

00:14:36.117 --> 00:14:39.914
Finalmente, esses sistemas
também podem errar

00:14:39.938 --> 00:14:42.488
de maneiras diferentes
dos sistemas humanos.

00:14:42.488 --> 00:14:45.620
Lembram-se do Watson, o sistema
de inteligência de máquina da IBM

00:14:45.644 --> 00:14:48.772
que sempre ganhava, competindo
com seres humanos num show de TV?

00:14:49.131 --> 00:14:50.559
Ele era um ótimo jogador.

00:14:50.583 --> 00:14:54.152
Mas então, na final, foi feita
a seguinte pergunta a Watson:

00:14:54.389 --> 00:14:57.591
"Seu maior aeroporto possui o nome
de um herói da Segunda Guerra,

00:14:57.615 --> 00:14:59.867
a maior grande batalha da Segunda Guerra".

00:14:59.891 --> 00:15:01.269
(Cantarola música do show)

00:15:01.582 --> 00:15:02.764
"Chicago."

00:15:02.788 --> 00:15:04.668
Os dois seres humanos acertaram.

00:15:04.697 --> 00:15:08.735
Watson, por sua vez, respondeu "Toronto"

00:15:08.939 --> 00:15:10.887
na categoria "cidade dos EUA"!

00:15:11.596 --> 00:15:14.497
O incrível sistema também cometeu um erro

00:15:14.521 --> 00:15:18.572
que um humano jamais faria,
uma criança não cometeria.

00:15:18.693 --> 00:15:21.932
Nossa inteligência artifical pode falhar

00:15:21.956 --> 00:15:24.820
de formas que não se encaixam
nos padrões de erros humanos,

00:15:24.820 --> 00:15:28.030
de formas que não esperamos
e para as quais não estamos preparados.

00:15:28.054 --> 00:15:31.692
Seria péssimo não conseguir um emprego
para o qual se está qualificado,

00:15:31.716 --> 00:15:35.443
mas seria triplamente péssimo
se fosse por causa de um "stack overflow"

00:15:35.467 --> 00:15:36.899
em alguma sub-rotina.

00:15:36.923 --> 00:15:38.502
(Risos)

00:15:38.526 --> 00:15:41.312
Em maio de 2010,

00:15:41.336 --> 00:15:45.380
uma baixa repentina em Wall Street,
alimentada pelo sistema de autoajuste

00:15:45.404 --> 00:15:48.432
do algoritmo de "venda",

00:15:48.456 --> 00:15:52.640
varreu US$ 1 trilhão em 36 minutos.

00:15:53.722 --> 00:15:55.909
Não quero nem pensar
no que significaria "erro"

00:15:55.933 --> 00:15:59.522
no contexto de armas letais autônomas.

00:16:01.894 --> 00:16:05.684
Então, sim, os seres humanos
sempre foram tendenciosos.

00:16:05.708 --> 00:16:07.884
Tomadores de decisão e controladores,

00:16:07.908 --> 00:16:11.401
em tribunais, na mídia, na guerra...

00:16:11.425 --> 00:16:14.463
eles cometem erros;
mas esse é exatamente meu ponto.

00:16:14.487 --> 00:16:18.008
Não podemos fugir
dessas questões difíceis.

00:16:18.596 --> 00:16:22.112
Não podemos terceirizar
nossas responsabilidades para as máquinas.

00:16:22.676 --> 00:16:25.554
(Aplausos) (Vivas)

00:16:29.089 --> 00:16:33.536
A inteligência artificial não nos dá
um passe para a "zona livre de ética".

00:16:34.742 --> 00:16:38.123
O cientista de dados Fred Benenson
chama isso de "mathwashing".

00:16:38.147 --> 00:16:39.536
Precisamos fazer o contrário.

00:16:39.560 --> 00:16:44.948
Precisamos cultivar o escrutínio,
a suspeita e investigação dos algoritmos.

00:16:45.380 --> 00:16:48.578
Precisamos nos assegurar de que temos
responsabilidade algorítmica,

00:16:48.602 --> 00:16:51.047
auditoria e transparência relevante.

00:16:51.380 --> 00:16:54.614
Precisamos aceitar que trazer
a matemática e a computação

00:16:54.638 --> 00:16:57.608
para negócios humanos confusos,
envolvendo julgamento de valor,

00:16:57.632 --> 00:17:00.016
não traz objetividade;

00:17:00.040 --> 00:17:03.903
mas que, ao contrário, a complexidade
dos negócios humanos invade os algoritmos.

00:17:04.148 --> 00:17:07.635
Sim, podemos e devemos usar a computação

00:17:07.659 --> 00:17:09.673
para nos ajudar a tomar decisões melhores.

00:17:09.697 --> 00:17:15.029
Mas temos de reconhecer
nossa responsabilidade moral para julgar,

00:17:15.053 --> 00:17:17.871
e usar os algoritmos
dentro desse espectro,

00:17:17.895 --> 00:17:22.830
não como uma forma de abdicar de nossas
responsabilidades ou terceirizá-las,

00:17:22.854 --> 00:17:25.558
como se fosse de um ser humano para outro.

00:17:25.807 --> 00:17:28.416
A inteligência artificial está aí.

00:17:28.440 --> 00:17:31.861
Isso significa que vamos
ter de nos agarrar firmemente

00:17:31.885 --> 00:17:34.032
aos valores e à ética humanos.

00:17:34.056 --> 00:17:35.210
Obrigada.

00:17:35.234 --> 00:17:38.324
(Aplausos) (Vivas)

