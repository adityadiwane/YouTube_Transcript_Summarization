WEBVTT
Kind: captions
Language: nl

00:00:00.000 --> 00:00:07.000
Vertaald door: Rik Delaet
Nagekeken door: Peter van de Ven

00:00:12.739 --> 00:00:16.861
Ik begon mijn eerste baan 
als computerprogrammeur

00:00:16.885 --> 00:00:18.841
in mijn eerste jaar aan de universiteit.

00:00:18.865 --> 00:00:20.372
Ik was nog een tiener.

00:00:20.689 --> 00:00:22.445
Spoedig nadat ik er begon

00:00:22.445 --> 00:00:24.825
met het schrijven 
van software voor een bedrijf,

00:00:24.825 --> 00:00:29.724
kwam een manager van het bedrijf 
naar me toe en fluisterde:

00:00:30.229 --> 00:00:33.090
"Weet hij of ik lieg?"

00:00:33.806 --> 00:00:35.883
Er was niemand anders in de kamer.

00:00:37.032 --> 00:00:41.421
Ik: "Weet wie dat je liegt? 
En waarom fluisteren we?"

00:00:42.266 --> 00:00:45.373
De manager wees 
naar de computer in de kamer.

00:00:45.397 --> 00:00:48.493
"Weet hij of ik lieg?"

00:00:49.613 --> 00:00:53.975
Nu had die manager 
een affaire met de receptioniste.

00:00:53.999 --> 00:00:55.111
(Gelach)

00:00:55.135 --> 00:00:56.901
En ik was nog een tiener.

00:00:57.447 --> 00:00:59.466
Dus fluister-schreeuwde ik naar hem terug:

00:00:59.490 --> 00:01:03.114
"Ja, de computer weet of je liegt."

00:01:03.138 --> 00:01:04.944
(Gelach)

00:01:04.968 --> 00:01:08.221
Nou, ik lachte, maar op dit moment 
zou ik moeten worden uitgelachen.

00:01:08.221 --> 00:01:11.183
Tegenwoordig zijn er computersystemen

00:01:11.207 --> 00:01:14.755
die emotionele toestanden 
en zelfs liegen herkennen

00:01:14.779 --> 00:01:17.033
door het interpreteren
van menselijke gezichten.

00:01:17.248 --> 00:01:21.401
Adverteerders en zelfs regeringen 
zijn zeer geïnteresseerd.

00:01:22.319 --> 00:01:24.181
Ik was computerprogrammeur geworden

00:01:24.205 --> 00:01:27.828
omdat ik een van die kinderen was 
die gek zijn op wiskunde en wetenschap.

00:01:27.942 --> 00:01:31.050
Maar ik leerde wat over kernwapens

00:01:31.074 --> 00:01:34.026
en werd echt bezorgd 
over de ethiek van de wetenschap.

00:01:34.050 --> 00:01:35.254
Ik was in de war.

00:01:35.278 --> 00:01:37.919
Door familieomstandigheden

00:01:37.943 --> 00:01:41.241
moest ik zo snel mogelijk 
aan werk geraken.

00:01:41.265 --> 00:01:44.564
Ik dacht, nou, laat me 
een technisch gebied uitkiezen

00:01:44.588 --> 00:01:46.744
waarin ik gemakkelijk een baan kan vinden

00:01:46.744 --> 00:01:50.426
en waar ik niet hoef in te gaan 
op eventuele lastige vragen over ethiek.

00:01:51.022 --> 00:01:52.551
Dus koos ik voor computers.

00:01:52.575 --> 00:01:53.679
(Gelach)

00:01:53.703 --> 00:01:57.113
Nou ja, ha, ha, ha! Lach maar.

00:01:57.137 --> 00:01:59.891
Tegenwoordig bouwen 
computerwetenschappers systemen

00:01:59.915 --> 00:02:04.124
die elke dag een miljard
mensen controleren.

00:02:05.052 --> 00:02:08.874
Ze ontwikkelen auto's die zouden 
kunnen beslissen wie ze overrijden.

00:02:09.707 --> 00:02:12.920
Ze bouwen zelfs machines, wapens,

00:02:12.944 --> 00:02:15.229
die in de oorlog mensen 
zouden kunnen doden.

00:02:15.253 --> 00:02:18.024
Het is ethiek, al wat de klok slaat.

00:02:19.183 --> 00:02:21.241
Machine-intelligentie is hier.

00:02:21.823 --> 00:02:25.297
We nemen allerlei beslissingen 
aan de hand van berekeningen,

00:02:25.321 --> 00:02:27.527
maar ook nieuwe typen beslissingen.

00:02:27.527 --> 00:02:32.403
We stellen vragen aan de computer
waar niet één enkel antwoord op is,

00:02:32.427 --> 00:02:33.629
die subjectief zijn

00:02:33.653 --> 00:02:35.978
met een open einde en waardegeladen.

00:02:36.002 --> 00:02:37.760
We stellen vragen als:

00:02:37.784 --> 00:02:39.434
"Wie moet het bedrijf inhuren?"

00:02:40.096 --> 00:02:42.855
"Welke update van welke vriend 
moet je te zien krijgen?"

00:02:42.879 --> 00:02:45.925
"Welke gevangene heeft 
meer kans om te recidiveren?"

00:02:45.925 --> 00:02:48.568
"Welke nieuwsbericht of film 
moeten we aanbevelen?"

00:02:48.592 --> 00:02:51.964
Ik weet dat we al een tijdje
computers gebruiken,

00:02:51.988 --> 00:02:53.505
maar dit is anders.

00:02:53.529 --> 00:02:55.596
Dit is een historisch keerpunt,

00:02:55.620 --> 00:02:58.701
omdat we voor dergelijke 
subjectieve beslissingen

00:02:58.701 --> 00:03:00.981
niet op berekeningen kunnen vertrouwen,

00:03:00.981 --> 00:03:06.401
zoals we dat doen voor het vliegen 
van vliegtuigen, het bouwen van bruggen

00:03:06.425 --> 00:03:07.684
of naar de maan gaan.

00:03:08.449 --> 00:03:11.708
Zijn vliegtuigen veiliger? 
Is de brug gaan zwaaien en ingestort?

00:03:11.732 --> 00:03:16.230
Daar hebben we vrij duidelijke
normen voor afgesproken

00:03:16.254 --> 00:03:18.493
en we hebben natuurwetten 
om ons te leiden.

00:03:18.517 --> 00:03:21.911
Dergelijke ijkpunten
en normen hebben we niet

00:03:21.935 --> 00:03:25.898
voor besluiten in rommelige 
menselijke aangelegenheden.

00:03:25.922 --> 00:03:30.159
Om de zaken nog ingewikkelder te maken, 
wordt onze software steeds krachtiger,

00:03:30.183 --> 00:03:33.956
maar ook steeds 
minder transparant en complexer.

00:03:34.542 --> 00:03:36.582
Recentelijk, in het afgelopen decennium,

00:03:36.606 --> 00:03:39.335
hebben complexe algoritmen 
grote vooruitgang geboekt.

00:03:39.359 --> 00:03:41.349
Ze kunnen menselijke gezichten herkennen.

00:03:41.985 --> 00:03:44.040
Ze kunnen handschrift ontcijferen.

00:03:44.436 --> 00:03:46.502
Ze kunnen creditcardfraude detecteren,

00:03:46.526 --> 00:03:47.715
spam blokkeren

00:03:47.739 --> 00:03:49.776
en vertalingen maken.

00:03:49.800 --> 00:03:52.654
Ze kunnen tumoren detecteren 
bij medische beeldvorming.

00:03:52.654 --> 00:03:54.993
Ze kunnen mensen 
met schaken en go verslaan.

00:03:55.264 --> 00:03:57.175
Een groot deel van deze vooruitgang

00:03:57.175 --> 00:04:00.175
komt van een methode 
'machine learning' genaamd.

00:04:00.175 --> 00:04:03.362
Machine learning is anders 
dan het traditionele programmeren,

00:04:03.386 --> 00:04:06.971
waar je de computer gedetailleerde, 
exacte, nauwgezette instructies geeft.

00:04:07.378 --> 00:04:11.560
Het is meer alsof je 
het systeem veel data voert,

00:04:11.584 --> 00:04:13.240
ook ongestructureerde data,

00:04:13.264 --> 00:04:15.542
zoals we ze genereren 
in ons digitale leven.

00:04:15.566 --> 00:04:18.296
En het systeem leert 
door op deze gegeven te broeden.

00:04:18.669 --> 00:04:20.195
Ook van cruciaal belang is

00:04:20.219 --> 00:04:24.599
dat deze systemen niet werken 
met een één-antwoord logica.

00:04:24.623 --> 00:04:27.582
Ze geven geen simpel antwoord; 
het is meer probabilistisch:

00:04:27.606 --> 00:04:31.089
"Dit is waarschijnlijk meer wat je zoekt."

00:04:32.023 --> 00:04:35.093
Het voordeel is dat deze methode
echt krachtig is.

00:04:35.117 --> 00:04:37.503
Het hoofd van Google's 
AI-systemen noemde het:

00:04:37.503 --> 00:04:39.624
"De onredelijke effectiviteit van data."

00:04:39.791 --> 00:04:41.144
Het nadeel is

00:04:41.738 --> 00:04:44.809
dat we niet echt begrijpen 
wat het systeem leerde.

00:04:44.833 --> 00:04:46.420
Dat is in feite zijn kracht.

00:04:46.946 --> 00:04:50.744
Dit lijkt minder op het geven 
van instructies aan een ​​computer

00:04:51.200 --> 00:04:55.264
dan op het trainen 
van een puppy-machine-schepsel

00:04:55.288 --> 00:04:57.659
dat we niet echt begrijpen of controleren.

00:04:58.362 --> 00:04:59.913
Dus dit is ons probleem.

00:05:00.427 --> 00:05:04.383
Als het kunstmatige-intelligentie systeem 
in de fout gaat, hebben we een probleem.

00:05:04.713 --> 00:05:08.253
Het is ook een probleem 
wanneer het goed werkt,

00:05:08.277 --> 00:05:09.627
omdat we niet eens weten

00:05:09.627 --> 00:05:12.459
wanneer het in de fout gaat 
bij een subjectief probleem.

00:05:12.459 --> 00:05:14.608
We weten niet wat dit ding denkt.

00:05:15.493 --> 00:05:19.176
Denk eens aan een algoritme 
voor aanwerving --

00:05:20.123 --> 00:05:23.032
een systeem dat wordt gebruikt 
om mensen aan te nemen

00:05:23.032 --> 00:05:25.052
met behulp van machine learning-systemen.

00:05:25.052 --> 00:05:28.631
Een dergelijk systeem werd getraind 
met data van vroegere werknemers

00:05:28.655 --> 00:05:31.246
en geïnstrueerd voor het vinden en inhuren

00:05:31.270 --> 00:05:34.308
van mensen zoals de beste
presteerders in het bedrijf.

00:05:34.814 --> 00:05:35.967
Klinkt goed.

00:05:35.991 --> 00:05:37.990
Ik heb eens een conferentie bijgewoond

00:05:38.014 --> 00:05:40.983
met human-resourcesmanagers
en leidinggevenden,

00:05:40.983 --> 00:05:41.973
hoge pieten,

00:05:41.973 --> 00:05:44.402
die dergelijke systemen 
voor het inhuren gebruiken.

00:05:44.402 --> 00:05:45.642
Ze waren superenthousiast.

00:05:45.646 --> 00:05:47.323
Ze dachten dat dit het inhuren

00:05:47.323 --> 00:05:50.323
objectiever en minder
bevooroordeeld zou maken

00:05:50.323 --> 00:05:53.323
en vrouwen en minderheden 
betere kansen zouden geven

00:05:53.347 --> 00:05:55.905
ten opzichte van vooringenomen 
menselijke managers.

00:05:55.905 --> 00:05:58.672
En ja -- het inhuren
door mensen is bevooroordeeld.

00:05:59.099 --> 00:06:00.284
Ik weet het.

00:06:00.308 --> 00:06:03.313
In een van mijn eerste banen
als programmeur

00:06:03.337 --> 00:06:07.205
kwam mijn directe manager 
soms naar me toe,

00:06:07.229 --> 00:06:10.982
heel vroeg in de ochtend 
of erg laat in de middag,

00:06:11.006 --> 00:06:14.068
en zei: "Zeynep, laten we gaan lunchen!"

00:06:14.724 --> 00:06:16.891
Ik was verbaasd over de vreemde timing.

00:06:16.915 --> 00:06:19.044
Het is vier uur in de namiddag. Lunch?

00:06:19.068 --> 00:06:22.162
Ik was blut. Dus gratis lunch?
Ik ging altijd mee.

00:06:22.618 --> 00:06:24.685
Ik besefte pas later wat er loos was.

00:06:24.709 --> 00:06:28.989
Mijn directe managers 
hadden hun oversten niet verteld

00:06:28.989 --> 00:06:32.089
dat de programmeur die ze 
voor een serieuze baan hadden ingehuurd

00:06:32.089 --> 00:06:33.656
een tienermeisje was

00:06:33.656 --> 00:06:36.346
dat op het werk een spijkerbroek 
en sportschoenen droeg.

00:06:36.844 --> 00:06:39.376
Ik deed mijn werk goed, 
ik zag er alleen niet uit

00:06:39.400 --> 00:06:41.669
en had de verkeerde leeftijd en geslacht.

00:06:41.669 --> 00:06:44.469
Dus inhuren zonder oog 
voor geslacht of ras,

00:06:44.493 --> 00:06:46.358
daar kan ik alleen maar blij mee zijn.

00:06:47.031 --> 00:06:50.372
Maar met deze systemen 
ligt het ingewikkelder en wel hierom:

00:06:50.968 --> 00:06:56.759
momenteel kunnen computersystemen 
allerlei dingen over je afleiden

00:06:56.783 --> 00:06:58.655
uit je digitale kruimels,

00:06:58.679 --> 00:07:01.012
zelfs als je die dingen niet hebt verteld.

00:07:01.506 --> 00:07:04.433
Ze kunnen je seksuele geaardheid afleiden,

00:07:04.994 --> 00:07:06.710
je persoonlijkheidskenmerken,

00:07:06.859 --> 00:07:08.232
je politieke kleur.

00:07:08.830 --> 00:07:12.515
Ze kunnen met grote 
nauwkeurigheid voorspellen.

00:07:13.212 --> 00:07:16.470
Nogmaals -- ook voor dingen 
die je nog niet eens hebt bekendgemaakt.

00:07:16.470 --> 00:07:17.555
Dat is inferentie.

00:07:17.579 --> 00:07:20.840
Ik heb een vriend die dergelijke 
computersystemen heeft ontwikkeld

00:07:20.864 --> 00:07:24.505
om de waarschijnlijkheid van klinische 
of postpartumdepressie te voorspellen

00:07:24.529 --> 00:07:26.295
op basis van social-mediagegevens.

00:07:26.676 --> 00:07:28.543
De resultaten zijn indrukwekkend.

00:07:28.543 --> 00:07:31.849
Haar systeem kan de kans 
op depressie voorspellen

00:07:31.873 --> 00:07:35.776
maanden vóór het begin van de symptomen --

00:07:35.800 --> 00:07:37.173
maanden eerder.

00:07:37.197 --> 00:07:39.443
Geen symptomen, wel een voorspelling.

00:07:39.467 --> 00:07:44.279
Ze hoopt dat ​​het zal worden gebruikt 
voor vroege interventie. Geweldig!

00:07:44.911 --> 00:07:47.891
Maar bekijk dat nu eens
in het kader van aanwerven.

00:07:48.027 --> 00:07:51.073
Op deze conferentie 
voor human-resourcesmanagers

00:07:51.097 --> 00:07:55.806
benaderde ik een vooraanstaande manager 
van een zeer groot bedrijf

00:07:55.830 --> 00:08:00.408
en zei tegen haar: 
"Kijk, wat als zonder dat je het weet,

00:08:00.432 --> 00:08:06.981
je systeem mensen met een hoge 
toekomstige kans op depressie uitwiedt?

00:08:07.761 --> 00:08:11.137
Ze zijn nu niet depressief, 
maar in de toekomst waarschijnlijk wel.

00:08:11.923 --> 00:08:15.329
Wat als het vrouwen uitwiedt 
omdat ze kans hebben om zwanger te worden

00:08:15.353 --> 00:08:17.939
in de volgende paar jaar, 
maar nu niet zwanger zijn?

00:08:18.844 --> 00:08:24.480
Wat als het agressieve mensen inhuurt, 
omdat dat jullie werkplekcultuur is?"

00:08:24.923 --> 00:08:27.934
Je weet dit niet door te kijken 
naar analyses voor geslacht.

00:08:27.934 --> 00:08:29.390
Die kunnen in evenwicht zijn.

00:08:29.414 --> 00:08:32.971
En aangezien dit machine learning is 
en geen traditioneel programmeren,

00:08:32.995 --> 00:08:37.902
is er geen variabele 
gelabeld 'groter risico op depressie',

00:08:37.926 --> 00:08:39.759
'hoger risico op zwangerschap'

00:08:39.783 --> 00:08:41.517
of een 'agressieve-man-schaal'.

00:08:41.995 --> 00:08:45.674
Niet alleen weet je niet 
waarop jouw systeem selecteert,

00:08:45.698 --> 00:08:48.021
je weet niet eens 
waar je moet gaan zoeken.

00:08:48.045 --> 00:08:49.291
Het is een zwarte doos.

00:08:49.315 --> 00:08:52.122
Het heeft voorspellende kracht, 
maar je begrijpt het niet.

00:08:52.486 --> 00:08:54.855
"Welke garanties", vroeg ik, "heb je

00:08:54.879 --> 00:08:58.552
om ervoor te zorgen dat jouw zwarte doos 
geen rare dingen gaat doen?"

00:09:00.863 --> 00:09:04.741
Ze keek me aan alsof ik 
op tien puppystaarten tegelijk trapte.

00:09:04.765 --> 00:09:06.013
(Gelach)

00:09:06.037 --> 00:09:08.078
Ze keek me aan en zei:

00:09:08.556 --> 00:09:12.889
"Hier wil ik geen woord meer over horen."

00:09:13.458 --> 00:09:15.492
En ze draaide zich om en liep weg.

00:09:16.064 --> 00:09:17.890
Let wel -- ze was niet onbeleefd.

00:09:17.890 --> 00:09:23.882
Het was duidelijk: wat ik niet weet, 
is mijn probleem niet, ga weg, lege blik.

00:09:23.906 --> 00:09:25.152
(Gelach)

00:09:25.862 --> 00:09:29.545
Kijk, een dergelijk systeem 
kan misschien minder bevooroordeeld zijn

00:09:29.545 --> 00:09:32.108
dan menselijke managers,
in sommige opzichten.

00:09:32.108 --> 00:09:33.998
Het kan misschien wel geld besparen.

00:09:34.573 --> 00:09:36.223
Maar het kan ook leiden

00:09:36.247 --> 00:09:40.995
tot een gestage, maar sluipende 
uitsluiting uit de arbeidsmarkt

00:09:41.019 --> 00:09:43.312
van mensen met een 
hoger risico op depressie.

00:09:43.753 --> 00:09:45.933
Is dit het soort
samenleving dat we willen,

00:09:45.933 --> 00:09:48.658
zonder zelfs maar te weten 
dat we dat hebben gedaan,

00:09:48.682 --> 00:09:51.605
omdat we besluitvorming 
delegeerden naar machines

00:09:51.605 --> 00:09:53.265
die we niet helemaal begrijpen?

00:09:53.265 --> 00:09:54.723
Een ander probleem is dit:

00:09:55.314 --> 00:09:59.766
deze systemen worden vaak getraind 
met data gegenereerd door onze acties,

00:09:59.790 --> 00:10:01.606
menselijke indrukken.

00:10:02.188 --> 00:10:05.996
Nou, die zouden wel eens
onze vooroordelen kunnen weerspiegelen,

00:10:06.020 --> 00:10:09.613
welke die systemen vervolgens
vlekkeloos overnemen,

00:10:09.637 --> 00:10:10.950
ze versterken

00:10:10.974 --> 00:10:12.392
en weer aan ons terugkaatsen,

00:10:12.416 --> 00:10:13.878
terwijl we onszelf wijsmaken:

00:10:13.902 --> 00:10:17.019
"We maken objectieve, 
neutrale berekeningen."

00:10:18.314 --> 00:10:20.991
Onderzoekers ontdekten dat op Google

00:10:22.134 --> 00:10:24.963
vrouwen minder kans hebben dan mannen

00:10:24.963 --> 00:10:28.463
om vacatures voor goedbetaalde banen 
te zien te krijgen.

00:10:28.463 --> 00:10:30.993
Het zoeken naar Afro-Amerikaanse namen

00:10:31.017 --> 00:10:33.297
geeft een grotere kans op advertenties

00:10:33.297 --> 00:10:35.747
die een criminele geschiedenis suggereren,

00:10:35.747 --> 00:10:37.314
zelfs wanneer er geen is.

00:10:38.693 --> 00:10:42.242
Dergelijke verborgen vooroordelen 
en zwarte-doosalgoritmen,

00:10:42.266 --> 00:10:46.239
die onderzoekers soms ontdekken
maar soms ook niet,

00:10:46.263 --> 00:10:48.924
kunnen iemands leven 
diepgaand beïnvloeden.

00:10:49.958 --> 00:10:54.117
In Wisconsin werd een verdachte 
veroordeeld tot zes jaar gevangenis

00:10:54.141 --> 00:10:56.256
door te gaan lopen voor de politie.

00:10:56.484 --> 00:10:58.010
Misschien weten jullie dit niet,

00:10:58.034 --> 00:11:00.102
maar algoritmen 
worden steeds meer gebruikt

00:11:00.102 --> 00:11:02.772
bij voorwaardelijke
invrijheidstelling en straftoemeting.

00:11:02.772 --> 00:11:05.626
Hij wilde weten hoe dat 
deze score berekend wordt.

00:11:05.795 --> 00:11:07.460
Het is een commerciële zwarte doos.

00:11:07.484 --> 00:11:09.016
Het bedrijf weigerde

00:11:09.016 --> 00:11:12.396
zijn algoritme in een openbare
rechtszitting te laten betwisten.

00:11:12.396 --> 00:11:17.632
Maar ProPublica, een onderzoekende 
non-profit, testte dat algoritme

00:11:17.632 --> 00:11:19.702
met openbare gegevens 
die ze konden vinden,

00:11:19.702 --> 00:11:22.308
en vond dat de resultaten ervan 
bevooroordeeld waren

00:11:22.332 --> 00:11:26.202
en dat de voorspellende kracht onduidelijk
en nauwelijks beter dan het toeval was.

00:11:26.205 --> 00:11:30.401
Ze labelde zwarte verdachten 
onterecht als toekomstige criminelen

00:11:30.425 --> 00:11:34.320
en wel twee keer zo vaak als blanken.

00:11:35.891 --> 00:11:37.455
Bekijk dit geval:

00:11:38.103 --> 00:11:41.955
deze vrouw was wat laat
om een meisje op te pikken

00:11:41.979 --> 00:11:44.724
op een school in Broward County, Florida.

00:11:44.757 --> 00:11:47.113
Ze liep op straat met een vriendin.

00:11:47.137 --> 00:11:51.236
Op een veranda zagen ze 
een kinderfiets en een scooter.

00:11:51.260 --> 00:11:52.892
Zonder nadenken sprongen ze erop.

00:11:52.916 --> 00:11:55.545
Terwijl ze wegreden, 
kwam een vrouw naar buiten en zei:

00:11:55.545 --> 00:11:57.744
"Hé! Dat is de fiets van mijn kind!"

00:11:57.768 --> 00:12:01.062
Ze lieten hem vallen, liepen door, 
maar werden gearresteerd.

00:12:01.086 --> 00:12:04.723
Ze ging in de fout, ze was dom, 
maar ze was ook pas 18.

00:12:04.747 --> 00:12:07.291
Ze had een paar
jeugdmisdrijven op haar naam.

00:12:07.808 --> 00:12:12.993
Ondertussen werd deze man gearresteerd
voor winkeldiefstal in Home Depot --

00:12:13.017 --> 00:12:15.941
85 dollar aan spullen, 
soortgelijke kleine criminaliteit.

00:12:16.766 --> 00:12:21.325
Maar hij had twee eerdere veroordelingen 
voor gewapende overval.

00:12:21.955 --> 00:12:25.437
Het algoritme scoorde haar 
als hoog risico en hem niet.

00:12:26.746 --> 00:12:30.620
Twee jaar later vond ProPublica 
dat ze niet had gerecidiveerd.

00:12:30.644 --> 00:12:33.324
Maar ze vond wel moeilijk 
een ​​baan met haar strafblad.

00:12:33.324 --> 00:12:35.294
Hij daarentegen recidiveerde

00:12:35.318 --> 00:12:39.154
en zit nu voor acht jaar in de gevangenis 
voor een latere misdaad.

00:12:40.088 --> 00:12:43.457
Het is duidelijk dat we 
onze zwarte dozen moeten controleren

00:12:43.481 --> 00:12:46.166
en ze niet dit soort ongecontroleerde
macht moeten geven.

00:12:46.166 --> 00:12:48.999
(Applaus)

00:12:50.087 --> 00:12:54.329
Audits zijn geweldig en belangrijk, 
maar ze lossen niet al onze problemen op.

00:12:54.353 --> 00:12:57.101
Neem Facebook's krachtige 
nieuwsfeed-algoritme --

00:12:57.125 --> 00:13:01.968
je weet wel, hetgeen dat alles sorteert 
en beslist wat jij te zien krijgt

00:13:01.992 --> 00:13:04.276
van alle vrienden
en pagina's die je volgt.

00:13:04.628 --> 00:13:07.173
Moet je nog een babyfoto 
voorgeschoteld krijgen?

00:13:07.197 --> 00:13:08.393
(Gelach)

00:13:08.417 --> 00:13:11.013
Een sombere opmerking van een kennis?

00:13:11.449 --> 00:13:13.585
Een belangrijk
maar moeilijk nieuwsbericht?

00:13:13.585 --> 00:13:14.951
Er is geen juist antwoord.

00:13:14.951 --> 00:13:17.494
Facebook optimaliseert 
voor betrokkenheid op de site:

00:13:17.518 --> 00:13:19.673
wat je leuk vindt, deelt 
of becommentarieert.

00:13:20.168 --> 00:13:22.864
In augustus 2014

00:13:22.888 --> 00:13:25.550
braken protesten uit 
in Ferguson, Missouri,

00:13:25.574 --> 00:13:29.991
na het doden van een Afro-Amerikaanse 
tiener door een blanke politieagent

00:13:30.015 --> 00:13:31.585
onder duistere omstandigheden.

00:13:31.974 --> 00:13:34.411
Het nieuws van de protesten
was prominent aanwezig

00:13:34.411 --> 00:13:36.690
op mijn algoritmisch 
ongefilterde Twitter-feed,

00:13:36.714 --> 00:13:38.664
maar nergens op mijn Facebook.

00:13:39.182 --> 00:13:40.916
Waren het mijn Facebookvrienden?

00:13:40.940 --> 00:13:42.972
Ik schakelde het Facebookalgoritme uit,

00:13:43.472 --> 00:13:46.320
wat moeilijk is,
want Facebook houdt je liever

00:13:46.344 --> 00:13:48.620
onder de controle van het algoritme.

00:13:48.620 --> 00:13:50.642
Ik zag dat mijn vrienden erover praatten.

00:13:50.666 --> 00:13:53.175
Maar het algoritme liet het me niet zien.

00:13:53.199 --> 00:13:56.241
Ik onderzocht dit en vond dat dit 
een wijdverbreid probleem was.

00:13:56.265 --> 00:13:59.782
Het verhaal van Ferguson 
was niet 'algoritmevriendelijk'.

00:13:59.782 --> 00:14:01.147
Het was niet 'sympathiek'.

00:14:01.147 --> 00:14:03.320
Wie gaat er een 'vind ik leuk' aan geven?

00:14:03.320 --> 00:14:05.706
Het is zelfs niet makkelijk 
te becommentariëren.

00:14:05.730 --> 00:14:07.611
Zonder vind-ik-leuk's en commentaar

00:14:07.611 --> 00:14:10.817
toonde het algoritme het waarschijnlijk 
aan nog minder mensen,

00:14:10.817 --> 00:14:12.623
zodat we het niet te zien kregen.

00:14:12.946 --> 00:14:16.124
In plaats daarvan benadrukte 
Facebook's algoritme die week dit:

00:14:16.520 --> 00:14:18.746
de ALS Ice Bucket Challenge.

00:14:18.770 --> 00:14:22.512
Giet ijswater, doneer 
aan een goed doel, allemaal fijn.

00:14:22.536 --> 00:14:24.960
Maar het was uiterst 
algoritmevriendelijk.

00:14:25.219 --> 00:14:27.832
De machine nam die beslissing voor ons.

00:14:27.856 --> 00:14:31.353
Een zeer belangrijk maar moeilijk gesprek

00:14:31.377 --> 00:14:32.932
zou zijn gesmoord,

00:14:32.956 --> 00:14:35.652
als Facebook het enige kanaal was geweest.

00:14:36.117 --> 00:14:39.914
Nu kunnen deze systemen ook verkeerd zijn

00:14:39.938 --> 00:14:42.548
op een manier die niet 
op menselijke systemen lijkt.

00:14:42.548 --> 00:14:45.790
Herinneren jullie zich Watson, 
IBM's machine-intelligentie-systeem

00:14:45.790 --> 00:14:49.172
dat op Jeopardy de vloer aanveegde 
met de menselijke deelnemers?

00:14:49.172 --> 00:14:50.559
Het was een geweldige speler.

00:14:50.583 --> 00:14:54.152
Maar in de finale kreeg Watson deze vraag:

00:14:54.539 --> 00:14:57.641
"Zijn grootste luchthaven is vernoemd 
naar een held uit WO II,

00:14:57.641 --> 00:14:59.957
zijn tweede grootste 
naar een slag uit WO II."

00:14:59.957 --> 00:15:01.559
(Neuriet Final Jeopardy-muziek)

00:15:01.582 --> 00:15:02.764
Chicago.

00:15:02.788 --> 00:15:04.638
De twee mensen hadden het goed.

00:15:04.697 --> 00:15:09.045
Maar Watson antwoordde "Toronto" --

00:15:09.069 --> 00:15:10.887
voor een stad van de VS!

00:15:11.596 --> 00:15:14.497
Het indrukwekkende systeem 
maakte ook een fout

00:15:14.521 --> 00:15:18.172
die een mens nooit zou maken, 
die een zevenjarige niet zou maken.

00:15:18.823 --> 00:15:21.932
Onze machine-intelligentie kan mislukken

00:15:21.956 --> 00:15:25.056
op een manier die niet past 
in foutpatronen van mensen,

00:15:25.080 --> 00:15:28.030
op een onverwachte manier
waarop we niet zijn voorbereid.

00:15:28.054 --> 00:15:30.154
Het zou stom zijn 
om een ​​baan mis te lopen

00:15:30.154 --> 00:15:31.716
waarvoor je gekwalificeerd bent,

00:15:31.716 --> 00:15:35.443
maar het zou driedubbel stom zijn 
als het was vanwege een stack overflow

00:15:35.467 --> 00:15:37.169
in de een of andere subroutine.

00:15:37.169 --> 00:15:38.502
(Gelach)

00:15:38.526 --> 00:15:41.312
In mei 2010

00:15:41.336 --> 00:15:45.380
ontstond een flash crash op Wall Street,
aangewakkerd door een terugkoppeling

00:15:45.404 --> 00:15:48.432
in Wall Streets 'verkopen'-algoritme.

00:15:48.456 --> 00:15:52.640
Het wiste een biljoen dollar weg 
in 36 minuten.

00:15:53.722 --> 00:15:56.419
Ik wil er niet eens aan denken 
wat 'fout' betekent

00:15:56.419 --> 00:15:59.522
als het over dodelijke 
autonome wapens gaat.

00:16:01.894 --> 00:16:05.684
Dus ja, mensen hebben altijd vooroordelen.

00:16:05.708 --> 00:16:07.884
Beslissers en bewakers,

00:16:07.908 --> 00:16:11.401
in rechtbanken, in het nieuws, 
in de oorlog...

00:16:11.425 --> 00:16:14.463
maken ze fouten;
maar dat is precies wat ik bedoel.

00:16:14.487 --> 00:16:18.008
We kunnen niet ontsnappen 
aan deze moeilijke vragen.

00:16:18.596 --> 00:16:22.112
We kunnen onze verantwoordelijkheden 
niet uitbesteden aan machines.

00:16:22.676 --> 00:16:26.884
(Applaus)

00:16:29.089 --> 00:16:33.536
Kunstmatige intelligentie stelt ons
niet vrij van de ethische vraagstukken.

00:16:34.742 --> 00:16:38.123
Datawetenschapper Fred Benenson 
noemt dit 'wiskunde-wassen'.

00:16:38.147 --> 00:16:39.926
We moeten het tegenovergestelde doen.

00:16:39.926 --> 00:16:44.948
Algoritmes verdienen een cultuur
van achterdocht, controle en onderzoek.

00:16:45.380 --> 00:16:48.578
We moeten zorgen 
voor algoritmische verantwoording,

00:16:48.602 --> 00:16:51.047
auditing en betekenisvolle transparantie.

00:16:51.380 --> 00:16:54.614
We moeten accepteren dat het toepassen
van wiskunde en berekening

00:16:54.638 --> 00:16:57.608
op rommelige, waardegeladen 
menselijke aangelegenheden

00:16:57.632 --> 00:17:00.016
geen objectiviteit met zich meebrengt,

00:17:00.040 --> 00:17:04.063
maar dat de complexiteit van menselijke 
aangelegenheden in de algoritmen sluipt.

00:17:04.148 --> 00:17:07.635
Ja, we kunnen en we moeten 
berekening gebruiken

00:17:07.659 --> 00:17:10.013
om ons te helpen 
om betere beslissingen te nemen.

00:17:10.013 --> 00:17:15.029
Maar het uiteindelijke oordeel
blijft onze morele verantwoordelijkheid,

00:17:15.053 --> 00:17:17.871
en binnen dat kader kunnen 
we algoritmen gebruiken,

00:17:17.895 --> 00:17:22.830
niet als een middel om 
onze verantwoordelijkheden

00:17:22.854 --> 00:17:25.308
van mens tot mens te ontlopen.

00:17:25.807 --> 00:17:28.416
Machine-intelligentie is hier.

00:17:28.440 --> 00:17:31.861
Dat betekent dat we 
steeds strakker moeten vasthouden

00:17:31.885 --> 00:17:34.032
aan menselijke waarden 
en menselijke ethiek.

00:17:34.056 --> 00:17:35.210
Dank je.

00:17:35.234 --> 00:17:40.254
(Applaus)

