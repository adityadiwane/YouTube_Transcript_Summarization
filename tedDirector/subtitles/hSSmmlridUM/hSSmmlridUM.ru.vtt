WEBVTT
Kind: captions
Language: ru

00:00:00.000 --> 00:00:07.000
Переводчик: Olenka Rasskazova 
Редактор: Marina Lee

00:00:12.559 --> 00:00:16.451
Я начала работать программистом

00:00:16.545 --> 00:00:18.661
в первый год учёбы в колледже,

00:00:18.685 --> 00:00:20.192
практически подростком.

00:00:20.519 --> 00:00:22.591
Вскоре после того, как я начала работать —

00:00:22.591 --> 00:00:24.075
писать программное обеспечение,

00:00:24.619 --> 00:00:28.254
один из работающих в компании 
менеджеров подошёл ко мне

00:00:28.278 --> 00:00:29.546
и прошептал:

00:00:30.049 --> 00:00:32.910
«Он понимает, когда я вру?»

00:00:33.626 --> 00:00:35.703
В комнате никого больше не было.

00:00:36.852 --> 00:00:41.241
«Кто он? И почему ты шепчешь?»

00:00:42.086 --> 00:00:45.193
Менеджер указал на компьютер.

00:00:45.217 --> 00:00:48.313
«Он может понять, когда я вру?»

00:00:49.433 --> 00:00:53.795
Кстати, у этого менеджера
был роман с секретаршей.

00:00:53.819 --> 00:00:54.931
(Смех)

00:00:54.955 --> 00:00:57.011
А а была ещё подростком.

00:00:57.267 --> 00:00:59.286
Поэтому я шёпотом крикнула ему:

00:00:59.310 --> 00:01:02.934
«Да, компьютер может понять,
когда вы лжёте».

00:01:02.958 --> 00:01:04.764
(Смех)

00:01:04.788 --> 00:01:07.711
Я пошутила, но оказалось,
что это была не шутка.

00:01:07.735 --> 00:01:11.003
Теперь существуют вычислительные системы,

00:01:11.027 --> 00:01:14.575
которые могут определить ваше
эмоциональное состояние и даже ложь,

00:01:14.599 --> 00:01:16.643
обрабатывая выражения человеческих лиц.

00:01:17.068 --> 00:01:21.221
Рекламодатели и правительства 
очень заинтересованы.

00:01:22.139 --> 00:01:24.001
Я стала программистом,

00:01:24.025 --> 00:01:27.648
потому что я была одержимым
математикой и наукой ребёнком.

00:01:27.762 --> 00:01:30.870
Но в какой-то момент я узнала 
о существовании ядерного оружия

00:01:30.894 --> 00:01:33.846
и всерьёз задумалась о научной этике.

00:01:33.870 --> 00:01:35.074
Меня это беспокоило.

00:01:35.098 --> 00:01:37.739
Однако из-за семейных обстоятельств

00:01:37.763 --> 00:01:41.061
мне пришлось начать 
работать как можно скорее.

00:01:41.085 --> 00:01:44.384
Так что я подумала: «Эй, мне просто 
надо выбрать техническую область,

00:01:44.408 --> 00:01:46.204
где я смогу легко получить работу,

00:01:46.228 --> 00:01:50.376
и где мне не придётся иметь 
дело со сложными этическими вопросами».

00:01:50.842 --> 00:01:52.371
Так что я выбрала компьютеры.

00:01:52.395 --> 00:01:53.499
(Смех)

00:01:53.523 --> 00:01:56.933
Ха, ха, ха! Все смеются надо мной.

00:01:56.957 --> 00:01:59.711
В наши дни компьютерные 
учёные создают платформы,

00:01:59.735 --> 00:02:03.944
которые контролируют то, 
что миллиард человек видит каждый день.

00:02:04.872 --> 00:02:08.694
Они разрабатывают автомобили, 
которые могли бы решить, кого задавить.

00:02:09.527 --> 00:02:12.740
Они даже разрабатывают машины и оружие,

00:02:12.764 --> 00:02:15.049
которые могут убивать людей на войне.

00:02:15.073 --> 00:02:17.844
Здесь вопросы этики повсюду.

00:02:19.003 --> 00:02:21.061
Искусственный интеллект уже здесь.

00:02:21.643 --> 00:02:25.117
Мы уже используем вычислительную
технику для принятия каких угодно решений,

00:02:25.141 --> 00:02:27.027
и даже для создания новых решений.

00:02:27.051 --> 00:02:32.223
Мы задаём компьютерам вопросы, 
на которые нет единого правильного ответа:

00:02:32.247 --> 00:02:33.449
субъективные вопросы,

00:02:33.473 --> 00:02:35.798
открытые и вопросы оценочного характера.

00:02:35.822 --> 00:02:37.580
Мы задавали такие вопросы, как:

00:02:37.604 --> 00:02:39.374
«Кого стоит нанять в компанию?»

00:02:39.916 --> 00:02:42.675
«Какое обновление и от какого 
друга мы должны видеть?»

00:02:42.699 --> 00:02:45.295
«Кто из осуждённых 
скорее всего станет рецидивистом?»

00:02:45.334 --> 00:02:48.388
«Какие новости или фильмы 
рекомендовать людям?»

00:02:48.412 --> 00:02:51.784
Да, мы используем компьютеры 
уже продолжительное время,

00:02:51.808 --> 00:02:53.325
но это совсем другое.

00:02:53.349 --> 00:02:55.416
Это исторический поворот,

00:02:55.440 --> 00:03:00.777
потому что базис для вычисления 
принятия субъективных решений,

00:03:00.801 --> 00:03:06.221
отличается от того, что используется для 
сборки самолётов, строительства мостов

00:03:06.245 --> 00:03:07.504
или полётов на Луну.

00:03:08.269 --> 00:03:11.528
Самолёты стали безопаснее?
Мосты больше не падают?

00:03:11.552 --> 00:03:15.344
Здесь у нас есть 
достаточно чёткие критерии

00:03:15.344 --> 00:03:18.313
и законы природы, 
на которые мы можем положиться.

00:03:18.337 --> 00:03:21.731
Но нет чётких критериев

00:03:21.755 --> 00:03:25.718
для принятия решений 
в запутанных людских делах.

00:03:25.742 --> 00:03:29.979
Ещё больше усложняет задачу 
программное обеспечение,

00:03:30.003 --> 00:03:33.776
становящееся менее прозрачным 
и более сложным и мощным.

00:03:34.362 --> 00:03:36.402
За последнее десятилетие

00:03:36.426 --> 00:03:39.155
развитие сложных алгоритмов
достигло больших успехов.

00:03:39.179 --> 00:03:41.169
Они могут распознавать человеческие лица.

00:03:41.805 --> 00:03:43.860
Они могут расшифровывать почерк.

00:03:44.086 --> 00:03:46.322
Выявить мошенничество
с кредитными картами

00:03:46.346 --> 00:03:47.535
или блокировать спам,

00:03:47.559 --> 00:03:49.596
они могут переводить с других языков.

00:03:49.620 --> 00:03:52.194
Они могут выявлять опухоли
в рентгенографии.

00:03:52.218 --> 00:03:54.873
Они могут обыгрывать нас в шахматы и в Го.

00:03:55.084 --> 00:03:59.588
Большáя часть этого прогресса достигнута
с помощью «машинного обучения».

00:03:59.995 --> 00:04:03.182
Машинное обучение отличается
от традиционного программирования,

00:04:03.206 --> 00:04:06.791
где вы даёте компьютеру подробные, 
точные, чёткие инструкции.

00:04:07.198 --> 00:04:11.380
Это больше похоже, как будто мы 
скармливаем компьютеру много данных,

00:04:11.404 --> 00:04:13.060
в том числе бессистемных данных,

00:04:13.084 --> 00:04:15.362
как те, что мы создаём 
в нашей цифровой жизни.

00:04:15.386 --> 00:04:18.116
И система сама учится
систематизировать эти данные.

00:04:18.489 --> 00:04:20.015
Особенно важно то,

00:04:20.039 --> 00:04:24.419
что эти системы не работают 
по логике поиска единого ответа.

00:04:24.443 --> 00:04:27.402
Они не дают однозначного ответа,
они основаны на вероятности:

00:04:27.426 --> 00:04:30.909
«Этот ответ, вероятно, 
похож на то, что вы ищете».

00:04:31.843 --> 00:04:34.913
Плюс этого метода в том,
что он очень перспективный.

00:04:34.937 --> 00:04:37.013
Глава систем ИИ Google назвал его

00:04:37.037 --> 00:04:39.234
«нерационально высокая
эффективность данных».

00:04:39.611 --> 00:04:40.964
Минус в том,

00:04:41.558 --> 00:04:44.629
что мы не знаем, 
что именно система выучила.

00:04:44.653 --> 00:04:46.240
В этом мощь системы.

00:04:46.766 --> 00:04:50.564
Это не похоже на то,
как давать указания компьютеру;

00:04:51.020 --> 00:04:55.084
это больше похоже 
на обучение машины-щенка,

00:04:55.108 --> 00:04:57.479
которого мы не понимаем и не контролируем.

00:04:58.182 --> 00:04:59.733
В этом наша проблема.

00:05:00.247 --> 00:05:04.509
Плохо, когда система искусственного
интеллекта понимает что-то неправильно.

00:05:04.533 --> 00:05:08.073
И также плохо, когда система
понимает что-то правильно,

00:05:08.097 --> 00:05:11.875
потому что мы не знаем, что есть что,
когда дело касается субъективой проблемы.

00:05:11.875 --> 00:05:14.088
Мы не знаем, о чём эта штука думает.

00:05:15.313 --> 00:05:18.996
Рассмотрим алгоритм приёма на работу —

00:05:19.943 --> 00:05:24.254
система для найма людей
с использованием машинного обучения.

00:05:24.872 --> 00:05:28.451
Такая система будет обучаться 
по данным о предыдущих сотрудниках

00:05:28.475 --> 00:05:31.066
и будет искать и нанимать людей,

00:05:31.090 --> 00:05:34.128
похожих на нынешних самых эффективных 
сотрудников компании.

00:05:34.634 --> 00:05:35.787
Звучит хорошо.

00:05:35.811 --> 00:05:37.810
Однажды я была на конференции

00:05:37.834 --> 00:05:41.179
для руководителей, менеджеров по персоналу

00:05:41.179 --> 00:05:42.189
и топ-менеджеров,

00:05:42.213 --> 00:05:43.812
использующих такую систему найма.

00:05:43.812 --> 00:05:45.442
Все были очень воодушевлены.

00:05:45.466 --> 00:05:50.119
Они думали, что это сделает процесс
найма более объективным, менее предвзятым,

00:05:50.143 --> 00:05:53.143
даст женщинам и меньшинствам 
больше шансов

00:05:53.167 --> 00:05:55.355
в отличие от предвзято
настроенных менеджеров.

00:05:55.379 --> 00:05:58.222
Найм сотрудников построен на предвзятости.

00:05:58.919 --> 00:06:00.104
Я знаю.

00:06:00.128 --> 00:06:03.133
На одной из моих первых работ
в качестве программиста

00:06:03.157 --> 00:06:07.025
моя непосредственная начальница
иногда подходила ко мне

00:06:07.049 --> 00:06:10.802
очень рано утром или очень поздно днём

00:06:10.826 --> 00:06:13.888
и говорила: «Зейнеп, пойдём обедать!»

00:06:14.544 --> 00:06:16.711
Я была озадачена странным выбором времени.

00:06:16.735 --> 00:06:18.864
Обед в 4 часа дня?

00:06:18.888 --> 00:06:21.982
Бесплатный ланч; денег у меня нет. 
Я всегда ходила.

00:06:22.438 --> 00:06:24.505
Позже я поняла, что происходит.

00:06:24.529 --> 00:06:29.075
Мои непосредственные руководители 
не признались вышестоящему руководству,

00:06:29.099 --> 00:06:32.802
что программист, которого они наняли
на серьёзный проект — девушка подросток,

00:06:32.802 --> 00:06:36.166
которая ходит на работу
в джинсах и кроссовках.

00:06:36.604 --> 00:06:39.360
Я хорошо выполняла работу,
я просто выглядела неподобающе,

00:06:39.360 --> 00:06:41.059
была неправильного возраста и пола.

00:06:41.059 --> 00:06:44.289
Так что найм без учёта пола и расы,

00:06:44.313 --> 00:06:46.438
конечно, звучит для меня
как хорошая идея.

00:06:46.851 --> 00:06:50.192
Но с этими системами 
всё сложнее, и вот почему:

00:06:50.788 --> 00:06:56.533
Сейчас вычислительные системы 
могут узнать всю информацию о вас

00:06:56.533 --> 00:06:58.725
по крошкам, что вы оставляете 
в цифровом виде,

00:06:58.725 --> 00:07:00.832
даже если вы не разглашаете
такую информацию.

00:07:01.326 --> 00:07:04.253
Они могут вычислить 
вашу сексуальную ориентацию,

00:07:04.814 --> 00:07:06.120
ваши черты характера,

00:07:06.679 --> 00:07:08.312
ваши политические пристрастия.

00:07:08.650 --> 00:07:12.525
Они могут составлять прогнозы
с высоким уровнем точности.

00:07:13.062 --> 00:07:15.990
Помните, даже для информации,
которую вы даже не разглашаете.

00:07:15.990 --> 00:07:17.375
Это предположения.

00:07:17.399 --> 00:07:20.660
У меня есть подруга, которая 
разрабатывает такие системы

00:07:20.684 --> 00:07:24.325
для прогнозирования вероятности 
клинической или послеродовой депрессии

00:07:24.349 --> 00:07:26.095
по данным из социальных сетей.

00:07:26.496 --> 00:07:27.923
Результаты впечатляют.

00:07:28.312 --> 00:07:31.669
Её система может предсказать
вероятность депрессии

00:07:31.693 --> 00:07:35.596
до появления каких-либо симптомов —

00:07:35.620 --> 00:07:36.993
за несколько месяцев.

00:07:37.017 --> 00:07:39.263
Симптомов нет, а прогноз есть.

00:07:39.287 --> 00:07:44.099
Она надеется, что программа будет 
использоваться для профилактики. Отлично!

00:07:44.731 --> 00:07:46.771
Теперь представьте это в контексте найма.

00:07:47.697 --> 00:07:51.033
На той конференции
для управляющих персоналом

00:07:51.033 --> 00:07:55.626
я подошла к менеджеру высокого уровня 
в очень крупной компании,

00:07:55.650 --> 00:08:00.228
и спросила её: 
«Что, если система без вашего ведома,

00:08:00.252 --> 00:08:06.801
начнёт отсеивать людей 
с высокой вероятностью будущей депрессии?

00:08:07.581 --> 00:08:10.957
Сейчас у них нет депрессии, 
но в будущем вероятность высока.

00:08:11.493 --> 00:08:15.409
Что, если система начнёт отсеивать женщин,
чья вероятность забеременеть

00:08:15.409 --> 00:08:17.939
через год или два выше,
но они не беременны сейчас?

00:08:18.664 --> 00:08:24.300
Если начнёт нанимать агрессивных людей, 
потому что это норма для вашей компании?

00:08:24.883 --> 00:08:27.684
Этого не определить, глядя
на процентное соотношение полов.

00:08:27.708 --> 00:08:29.330
Эти показатели могут быть в норме.

00:08:29.330 --> 00:08:32.791
Так как это машинное обучение, 
а не традиционное программирование,

00:08:32.815 --> 00:08:37.722
тут нет переменной
«более высокий риск депрессии»,

00:08:37.746 --> 00:08:39.579
«высокий риск беременности»,

00:08:39.603 --> 00:08:41.337
или «агрессивный парень».

00:08:41.815 --> 00:08:45.494
Мало того, что вы не знаете, 
как ваша система делает выводы,

00:08:45.518 --> 00:08:47.841
вы даже не знаете, откуда что берётся.

00:08:47.865 --> 00:08:49.111
Это чёрный ящик.

00:08:49.135 --> 00:08:52.322
Он может прогнозировать,
но мы не понимаем принцип его работы.

00:08:52.322 --> 00:08:54.675
Я спросила: 
«Какие у вас меры предосторожности,

00:08:54.699 --> 00:08:58.372
чтобы убедиться, что чёрный ящик 
не делает ничего сомнительного?»

00:09:00.683 --> 00:09:04.561
Она посмотрела на меня, как будто я 
только что отдавила хвосты 10 щенкам.

00:09:04.585 --> 00:09:05.833
(Смех)

00:09:05.857 --> 00:09:07.898
Она посмотрела на меня и сказала: 

00:09:08.376 --> 00:09:12.709
«Я не хочу слышать ни слова об этом».

00:09:13.278 --> 00:09:15.312
Она повернулась и пошла прочь.

00:09:15.614 --> 00:09:17.420
Имейте в виду — она ​​не грубила мне.

00:09:17.420 --> 00:09:23.702
Позиция очевидна: то, что я не знаю — 
не моя проблема, отвяжись, стрелы из глаз.

00:09:23.726 --> 00:09:24.972
(Смех)

00:09:25.682 --> 00:09:29.521
Такая система 
может быть менее предвзятой,

00:09:29.545 --> 00:09:31.648
чем сами менеджеры в каком-то смысле.

00:09:31.672 --> 00:09:33.818
В этом может быть финансовая выгода.

00:09:34.393 --> 00:09:36.043
Но это также может привести

00:09:36.067 --> 00:09:40.815
к неуклонному и скрытому
выдавливанию с рынка труда

00:09:40.839 --> 00:09:43.362
людей с более высоким 
риском развития депрессии.

00:09:43.573 --> 00:09:46.169
Мы хотим построить такое общество,

00:09:46.193 --> 00:09:48.478
даже не осознавая, что мы делаем,

00:09:48.502 --> 00:09:52.466
потому что отдали право принятия решений
машинам, которых до конца не понимаем?

00:09:53.085 --> 00:09:54.933
Следующая проблема:

00:09:55.134 --> 00:09:59.586
эти системы часто обучаются на данных,
произведённых нашими действиями —

00:09:59.610 --> 00:10:01.426
человеческим поведением.

00:10:02.008 --> 00:10:05.816
Возможно, они просто 
отражают наши предубеждения,

00:10:05.840 --> 00:10:09.433
и эти системы могут 
собирать наши пристрастия

00:10:09.457 --> 00:10:10.770
и усиливать их,

00:10:10.794 --> 00:10:12.212
показывая нам их вновь,

00:10:12.236 --> 00:10:13.698
а мы говорим себе:

00:10:13.722 --> 00:10:16.839
«Мы просто проводим объективные, 
непредвзятые вычисления».

00:10:18.134 --> 00:10:20.811
Исследователи обнаружили что в Google

00:10:21.954 --> 00:10:27.267
женщины реже, чем мужчины, видят 
объявления о высокооплачиваемой работе.

00:10:28.283 --> 00:10:30.813
Набирая в поисковике
афро-американские имена,

00:10:30.837 --> 00:10:35.543
вероятность увидеть объявления 
криминального характера будет выше,

00:10:35.567 --> 00:10:37.134
даже там, где криминала нет.

00:10:38.513 --> 00:10:42.062
Скрытая необъективность
и алгоритмы чёрного ящика,

00:10:42.086 --> 00:10:46.059
которые исследователи 
иногда выявляют, а иногда нет,

00:10:46.083 --> 00:10:48.744
могут иметь далеко идущие последствия.

00:10:49.778 --> 00:10:53.937
В Висконсине подсудимый был приговорён 
к шести годам лишения свободы

00:10:53.961 --> 00:10:55.316
за уклонение от полиции.

00:10:56.644 --> 00:10:57.830
Может, вы не знаете,

00:10:57.854 --> 00:11:01.852
но эти алгоритмы всё чаще используются 
в вынесении приговоров.

00:11:01.876 --> 00:11:05.081
Он хотел узнать,
как всё это рассчитывается?

00:11:05.615 --> 00:11:07.280
Это коммерческий чёрный ящик.

00:11:07.304 --> 00:11:11.509
Компания отказалась обсуждать
свой алгоритм на открытом заседании суда.

00:11:12.216 --> 00:11:17.748
Но следственная некоммерческая организация
ProPublica проверила алгоритм,

00:11:17.772 --> 00:11:19.788
используя данные из Интернета,

00:11:19.812 --> 00:11:22.128
и обнаружила, 
что результаты необъективны,

00:11:22.152 --> 00:11:25.781
способность прогнозирования ужасная, 
немного лучше, чем случайность.

00:11:25.805 --> 00:11:30.221
Система классифицирует чернокожих 
обвиняемых как будущих преступников

00:11:30.245 --> 00:11:34.140
в два раза чаще, чем белых обвиняемых.

00:11:35.711 --> 00:11:37.275
Рассмотрим следующий случай:

00:11:37.923 --> 00:11:41.775
Эта девушка опаздывала, чтобы забрать
свою крёстную сестру

00:11:41.799 --> 00:11:43.874
из школы в округе Броуард, штат Флорида.

00:11:44.577 --> 00:11:46.933
Они с подругой бежали по улице.

00:11:46.957 --> 00:11:51.056
Тут они заметили незапертые 
велосипед и скутер на крыльце

00:11:51.080 --> 00:11:52.712
и по глупости взяли их.

00:11:52.736 --> 00:11:55.335
Когда они отъезжали, 
вышла женщина и крикнула:

00:11:55.359 --> 00:11:57.564
«Эй! Это велосипед моего ребёнка!»

00:11:57.588 --> 00:12:00.882
Они его бросили и ушли, 
но их арестовали.

00:12:00.906 --> 00:12:04.543
Она была не права, она сглупила, 
но ей было всего 18 лет.

00:12:04.567 --> 00:12:07.111
У неё была пара 
малолетних правонарушений.

00:12:07.628 --> 00:12:12.813
В то же время этот мужчина был 
арестован за кражу в магазине Home Depot,

00:12:12.837 --> 00:12:15.761
примерно на сумму 85 долларов —
такое же мелкое преступление.

00:12:16.586 --> 00:12:21.145
Но у него за спиной было 
две судимости за вооружённый грабеж.

00:12:21.775 --> 00:12:25.257
Алгоритм посчитал, 
что её показатель риска выше, чем его.

00:12:26.566 --> 00:12:30.440
Спустя пару лет ProPublica выяснили, 
что она больше не совершала преступлений.

00:12:30.464 --> 00:12:33.014
Но зато ей было сложно найти 
работу, имея судимость.

00:12:33.038 --> 00:12:35.114
Тогда как этот мужчина стал рецидивистом,

00:12:35.138 --> 00:12:38.974
и в настоящее время отбывает восьмилетний 
срок за своё последнее преступление.

00:12:39.908 --> 00:12:43.277
Очевидно, мы должны проверять
наши чёрные ящики,

00:12:43.301 --> 00:12:45.916
чтобы они не получили 
бесконтрольную власть.

00:12:45.940 --> 00:12:48.819
(Аплодисменты)

00:12:49.907 --> 00:12:54.149
Проверка и контроль важны, 
но они не решают всех проблем.

00:12:54.173 --> 00:12:56.921
Вспомните мощный алгоритм
ленты новостей на Facebook —

00:12:56.945 --> 00:13:01.788
знаете, тот, который оценивает всё 
и решает, что именно вам показывать

00:13:01.812 --> 00:13:04.426
от ваших друзей и до страниц,
на которые вы подписаны.

00:13:04.718 --> 00:13:06.993
Показать вам ещё одну картинку младенца?

00:13:07.017 --> 00:13:08.213
(Смех)

00:13:08.237 --> 00:13:10.833
Грустный комментарий от знакомого?

00:13:11.269 --> 00:13:13.125
Важную, но непростую новость?

00:13:13.149 --> 00:13:14.631
Тут нет единого ответа.

00:13:14.655 --> 00:13:17.314
Facebook оптимизирует 
вашу деятельность на сайте:

00:13:17.338 --> 00:13:18.753
лайки, ссылки, комментарии.

00:13:19.988 --> 00:13:22.684
В августе 2014 года

00:13:22.708 --> 00:13:25.370
в Фергюсоне, штат Миссури
вспыхнули протесты

00:13:25.394 --> 00:13:29.811
после того, как белый полицейский
убил афро-американского подростка

00:13:29.835 --> 00:13:31.405
при невыясненных обстоятельствах.

00:13:31.794 --> 00:13:33.801
Новости о протестах заполонили

00:13:33.825 --> 00:13:36.510
мой алгоритмически
нефильтрованный Twitter,

00:13:36.534 --> 00:13:38.484
но в моём Facebook их не было.

00:13:38.732 --> 00:13:40.736
Может, это из-за моих друзей в Facebook?

00:13:40.760 --> 00:13:42.792
Я отключила алгоритм Facebook,

00:13:43.292 --> 00:13:46.140
что было сложно, так как Facebook хочет,

00:13:46.164 --> 00:13:48.200
чтобы вы были под контролем алгоритма.

00:13:48.224 --> 00:13:50.462
Я увидела, что мои друзья
обсуждали эту тему.

00:13:50.486 --> 00:13:52.749
Просто алгоритм не показывал это мне.

00:13:52.749 --> 00:13:56.081
Я изучила этот вопрос и выяснила,
что это распространённая проблема.

00:13:56.081 --> 00:13:59.632
Новость про Фергюсон 
была неудобна для алгоритма.

00:13:59.632 --> 00:14:01.203
Эта новость не наберёт лайки.

00:14:01.203 --> 00:14:02.669
Кто будет лайкать это?

00:14:03.320 --> 00:14:05.526
Это даже сложно комментировать.

00:14:05.550 --> 00:14:06.921
Без лайков и комментариев

00:14:06.945 --> 00:14:10.237
алгоритм, вероятно, показывал 
новость ещё меньшему кругу людей,

00:14:10.261 --> 00:14:11.803
поэтому мы не видели это.

00:14:12.616 --> 00:14:13.994
Вместо этого на той же неделе

00:14:14.018 --> 00:14:16.316
алгоритм Facebook выделил это —

00:14:16.340 --> 00:14:18.550
кампания «испытание ведром ледяной воды».

00:14:18.550 --> 00:14:22.462
Важное дело: выливаем ведро со льдом, 
жертвуем на благотворительность — супер.

00:14:22.462 --> 00:14:24.630
Это было очень удобно для алгоритма.

00:14:25.039 --> 00:14:27.652
Машина решила за нас.

00:14:27.676 --> 00:14:31.173
Очень важный, но трудный разговор,

00:14:31.197 --> 00:14:32.752
возможно, был бы замят,

00:14:32.776 --> 00:14:35.472
будь Facebook единственным каналом.

00:14:35.937 --> 00:14:39.734
Наконец, эти системы могут делать ошибки,

00:14:39.758 --> 00:14:42.494
которые не похожи на ошибки людей.

00:14:42.518 --> 00:14:45.440
Помните Уотсона,
искусственный интеллект IBM,

00:14:45.464 --> 00:14:48.592
который разгромил соперников-людей
на телевикторине Jeopardy?

00:14:48.951 --> 00:14:50.379
Он был отличным игроком.

00:14:50.403 --> 00:14:53.972
Тогда, во время финала игры
Уотсону задали вопрос:

00:14:54.209 --> 00:14:57.411
«Его крупнейший аэропорт назван 
в честь героя Второй мировой войны,

00:14:57.435 --> 00:14:59.887
а второй — в честь битвы
Второй мировой войны».

00:14:59.887 --> 00:15:01.259
(Музыка Final Jeopardy)

00:15:01.402 --> 00:15:02.584
Чикаго.

00:15:02.608 --> 00:15:04.358
Два человека ответили правильно.

00:15:04.517 --> 00:15:08.865
Уотсон ответил «Торонто» —

00:15:08.889 --> 00:15:10.707
в категории городов США!

00:15:11.416 --> 00:15:14.317
Мощнейшая система сделала ошибку,

00:15:14.341 --> 00:15:17.992
которую человек никогда бы не сделал, 
даже второклассник бы не ошибся.

00:15:18.643 --> 00:15:21.752
Искусственный интеллект 
может ошибиться там,

00:15:21.776 --> 00:15:24.876
где человек не допустит ошибку,

00:15:24.900 --> 00:15:27.850
там, где мы не ожидаем
ошибку и не готовы к ней.

00:15:27.874 --> 00:15:31.512
Жалко не получить работу тому, 
кто для неё подходит,

00:15:31.536 --> 00:15:35.483
но ещё хуже, если это произошло
из-за переполнения стека

00:15:35.483 --> 00:15:36.719
в какой-то подпрограмме.

00:15:36.743 --> 00:15:38.322
(Смех)

00:15:38.346 --> 00:15:41.132
В мае 2010 года

00:15:41.156 --> 00:15:45.200
произошёл обвал рынка Уолл-стрит
по вине метода передачи данных

00:15:45.224 --> 00:15:48.252
в алгоритме Уолл-стрит «сбыт»,

00:15:48.276 --> 00:15:52.460
что снизило стоимость бумаг 
на триллион долларов на 36 минут.

00:15:53.172 --> 00:15:56.499
Даже подумать страшно, какие
последствия может иметь «ошибка»

00:15:56.499 --> 00:16:00.142
в контексте автономного летального оружия.

00:16:01.714 --> 00:16:05.504
У людей всегда предвзятый взгляд на вещи.

00:16:05.528 --> 00:16:07.704
Лица, принимающие решения, и контролёры;

00:16:07.728 --> 00:16:11.221
в судах, в новостях, на войне ...

00:16:11.245 --> 00:16:14.283
люди совершают ошибки —
именно это я и имею в виду.

00:16:14.307 --> 00:16:17.828
Мы не можем избежать сложных вопросов.

00:16:18.416 --> 00:16:21.932
Мы не можем переложить 
свои обязанности на машины.

00:16:22.496 --> 00:16:26.704
(Аплодисменты)

00:16:28.909 --> 00:16:33.356
Искусственный интеллект не даёт нам
права переложить вопросы этики на машину.

00:16:34.562 --> 00:16:37.943
Эксперт по данным Фред Бененсон
называет это «математической чисткой».

00:16:37.967 --> 00:16:39.356
Нам нужно совсем другое.

00:16:39.380 --> 00:16:44.768
Нам необходимы пристальное внимание, 
контроль и оценка алгоритмов.

00:16:45.200 --> 00:16:48.398
У нас должна быть
алгоритмическая отчётность,

00:16:48.422 --> 00:16:50.867
проверка и достаточная прозрачность.

00:16:51.200 --> 00:16:54.434
Мы должны признать, что, добавив
математику и вычисления

00:16:54.458 --> 00:16:57.428
к запутанным человеческим делам,

00:16:57.452 --> 00:16:59.836
мы не получим объективности;

00:16:59.860 --> 00:17:03.493
скорее, сложность человеческих 
отношений вторгнется в алгоритмы.

00:17:03.968 --> 00:17:07.455
Да, мы можем, и мы должны 
использовать вычисления

00:17:07.479 --> 00:17:09.493
для поиска лучших решений.

00:17:09.517 --> 00:17:14.849
Мы также должны нести моральную
ответственность и принимать решения,

00:17:14.873 --> 00:17:17.691
успользуя алгоритмы в этих рамках,

00:17:17.715 --> 00:17:22.650
а не как средство отказа от обязательств, 
чтобы передать наши обязанности

00:17:22.674 --> 00:17:25.128
друг другу, как один человек другому.

00:17:25.627 --> 00:17:28.236
Искусственный интеллект уже здесь.

00:17:28.260 --> 00:17:31.681
Это значит, что мы должны 
ещё больше придерживаться

00:17:31.705 --> 00:17:33.852
человеческих ценностей и этики.

00:17:33.876 --> 00:17:35.030
Спасибо.

00:17:35.054 --> 00:17:40.074
(Аплодисменты)

