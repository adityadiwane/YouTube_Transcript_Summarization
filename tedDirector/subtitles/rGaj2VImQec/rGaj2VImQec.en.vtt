WEBVTT
Kind: captions
Language: en

00:00:15.260 --> 00:00:18.260
I didn't always love unintended consequences,

00:00:18.260 --> 00:00:20.260
but I've really learned to appreciate them.

00:00:20.260 --> 00:00:22.260
I've learned that they're really the essence

00:00:22.260 --> 00:00:24.260
of what makes for progress,

00:00:24.260 --> 00:00:27.260
even when they seem to be terrible.

00:00:27.260 --> 00:00:29.260
And I'd like to review

00:00:29.260 --> 00:00:32.260
just how unintended consequences

00:00:32.260 --> 00:00:35.260
play the part that they do.

00:00:35.260 --> 00:00:40.260
Let's go to 40,000 years before the present,

00:00:40.260 --> 00:00:44.260
to the time of the cultural explosion,

00:00:44.260 --> 00:00:49.260
when music, art, technology,

00:00:49.260 --> 00:00:51.260
so many of the things that we're enjoying today,

00:00:51.260 --> 00:00:54.260
so many of the things that are being demonstrated at TED

00:00:54.260 --> 00:00:56.260
were born.

00:00:56.260 --> 00:00:59.260
And the anthropologist Randall White

00:00:59.260 --> 00:01:02.260
has made a very interesting observation:

00:01:02.260 --> 00:01:04.260
that if our ancestors

00:01:04.260 --> 00:01:06.260
40,000 years ago

00:01:06.260 --> 00:01:09.260
had been able to see

00:01:09.260 --> 00:01:11.260
what they had done,

00:01:11.260 --> 00:01:13.260
they wouldn't have really understood it.

00:01:13.260 --> 00:01:15.260
They were responding

00:01:15.260 --> 00:01:18.260
to immediate concerns.

00:01:18.260 --> 00:01:20.260
They were making it possible for us

00:01:20.260 --> 00:01:22.260
to do what they do,

00:01:22.260 --> 00:01:24.260
and yet, they didn't really understand

00:01:24.260 --> 00:01:26.260
how they did it.

00:01:26.260 --> 00:01:31.260
Now let's advance to 10,000 years before the present.

00:01:31.260 --> 00:01:33.260
And this is when it really gets interesting.

00:01:33.260 --> 00:01:36.260
What about the domestication of grains?

00:01:36.260 --> 00:01:39.260
What about the origins of agriculture?

00:01:39.260 --> 00:01:42.260
What would our ancestors 10,000 years ago

00:01:42.260 --> 00:01:44.260
have said

00:01:44.260 --> 00:01:46.260
if they really had technology assessment?

00:01:46.260 --> 00:01:48.260
And I could just imagine the committees

00:01:48.260 --> 00:01:50.260
reporting back to them

00:01:50.260 --> 00:01:53.260
on where agriculture was going to take humanity,

00:01:53.260 --> 00:01:56.260
at least in the next few hundred years.

00:01:56.260 --> 00:01:58.260
It was really bad news.

00:01:58.260 --> 00:02:00.260
First of all, worse nutrition,

00:02:00.260 --> 00:02:02.260
maybe shorter life spans.

00:02:02.260 --> 00:02:04.260
It was simply awful for women.

00:02:04.260 --> 00:02:06.260
The skeletal remains from that period

00:02:06.260 --> 00:02:11.260
have shown that they were grinding grain morning, noon and night.

00:02:11.260 --> 00:02:14.260
And politically, it was awful.

00:02:14.260 --> 00:02:17.260
It was the beginning of a much higher degree

00:02:17.260 --> 00:02:20.260
of inequality among people.

00:02:20.260 --> 00:02:23.260
If there had been rational technology assessment then,

00:02:23.260 --> 00:02:25.260
I think they very well might have said,

00:02:25.260 --> 00:02:28.260
"Let's call the whole thing off."

00:02:28.260 --> 00:02:32.260
Even now, our choices are having unintended effects.

00:02:32.260 --> 00:02:34.260
Historically, for example,

00:02:34.260 --> 00:02:37.260
chopsticks -- according to one Japanese anthropologist

00:02:37.260 --> 00:02:39.260
who wrote a dissertation about it

00:02:39.260 --> 00:02:41.260
at the University of Michigan --

00:02:41.260 --> 00:02:44.260
resulted in long-term changes

00:02:44.260 --> 00:02:46.260
in the dentition, in the teeth,

00:02:46.260 --> 00:02:48.260
of the Japanese public.

00:02:48.260 --> 00:02:51.260
And we are also changing our teeth right now.

00:02:51.260 --> 00:02:53.260
There is evidence

00:02:53.260 --> 00:02:55.260
that the human mouth and teeth

00:02:55.260 --> 00:02:57.260
are growing smaller all the time.

00:02:57.260 --> 00:03:00.260
That's not necessarily a bad unintended consequence.

00:03:00.260 --> 00:03:02.260
But I think from the point of view of a Neanderthal,

00:03:02.260 --> 00:03:04.260
there would have been a lot of disapproval

00:03:04.260 --> 00:03:07.260
of the wimpish choppers that we now have.

00:03:07.260 --> 00:03:10.260
So these things are kind of relative

00:03:10.260 --> 00:03:14.260
to where you or your ancestors happen to stand.

00:03:14.260 --> 00:03:16.260
In the ancient world

00:03:16.260 --> 00:03:19.260
there was a lot of respect for unintended consequences,

00:03:19.260 --> 00:03:22.260
and there was a very healthy sense of caution,

00:03:22.260 --> 00:03:24.260
reflected in the Tree of Knowledge,

00:03:24.260 --> 00:03:26.260
in Pandora's Box,

00:03:26.260 --> 00:03:28.260
and especially in the myth of Prometheus

00:03:28.260 --> 00:03:30.260
that's been so important

00:03:30.260 --> 00:03:32.260
in recent metaphors about technology.

00:03:32.260 --> 00:03:35.260
And that's all very true.

00:03:35.260 --> 00:03:37.260
The physicians of the ancient world --

00:03:37.260 --> 00:03:39.260
especially the Egyptians,

00:03:39.260 --> 00:03:41.260
who started medicine as we know it --

00:03:41.260 --> 00:03:43.260
were very conscious

00:03:43.260 --> 00:03:45.260
of what they could and couldn't treat.

00:03:45.260 --> 00:03:50.260
And the translations of the surviving texts say,

00:03:50.260 --> 00:03:52.260
"This I will not treat. This I cannot treat."

00:03:52.260 --> 00:03:54.260
They were very conscious.

00:03:54.260 --> 00:03:56.260
So were the followers of Hippocrates.

00:03:56.260 --> 00:03:58.260
The Hippocratic manuscripts also --

00:03:58.260 --> 00:04:01.260
repeatedly, according to recent studies --

00:04:01.260 --> 00:04:04.260
show how important it is not to do harm.

00:04:04.260 --> 00:04:06.260
More recently,

00:04:06.260 --> 00:04:08.260
Harvey Cushing,

00:04:08.260 --> 00:04:10.260
who really developed neurosurgery as we know it,

00:04:10.260 --> 00:04:13.260
who changed it from a field of medicine

00:04:13.260 --> 00:04:17.260
that had a majority of deaths resulting from surgery

00:04:17.260 --> 00:04:20.260
to one in which there was a hopeful outlook,

00:04:20.260 --> 00:04:22.260
he was very conscious

00:04:22.260 --> 00:04:25.260
that he was not always going to do the right thing.

00:04:25.260 --> 00:04:27.260
But he did his best,

00:04:27.260 --> 00:04:29.260
and he kept meticulous records

00:04:29.260 --> 00:04:32.260
that let him transform that branch of medicine.

00:04:32.260 --> 00:04:35.260
Now if we look forward a bit

00:04:35.260 --> 00:04:37.260
to the 19th century,

00:04:37.260 --> 00:04:39.260
we find a new style of technology.

00:04:39.260 --> 00:04:41.260
What we find is,

00:04:41.260 --> 00:04:44.260
no longer simple tools,

00:04:44.260 --> 00:04:46.260
but systems.

00:04:46.260 --> 00:04:48.260
We find more and more

00:04:48.260 --> 00:04:50.260
complex arrangements of machines

00:04:50.260 --> 00:04:52.260
that make it harder and harder

00:04:52.260 --> 00:04:54.260
to diagnose what's going on.

00:04:54.260 --> 00:04:56.260
And the first people who saw that

00:04:56.260 --> 00:04:59.260
were the telegraphers of the mid-19th century,

00:04:59.260 --> 00:05:01.260
who were the original hackers.

00:05:01.260 --> 00:05:04.260
Thomas Edison would have been very, very comfortable

00:05:04.260 --> 00:05:07.260
in the atmosphere of a software firm today.

00:05:07.260 --> 00:05:10.260
And these hackers had a word

00:05:10.260 --> 00:05:13.260
for those mysterious bugs in telegraph systems

00:05:13.260 --> 00:05:15.260
that they called bugs.

00:05:15.260 --> 00:05:19.260
That was the origin of the word "bug."

00:05:19.260 --> 00:05:21.260
This consciousness, though,

00:05:21.260 --> 00:05:24.260
was a little slow to seep through the general population,

00:05:24.260 --> 00:05:27.260
even people who were very, very well informed.

00:05:27.260 --> 00:05:29.260
Samuel Clemens, Mark Twain,

00:05:29.260 --> 00:05:31.260
was a big investor

00:05:31.260 --> 00:05:34.260
in the most complex machine of all times --

00:05:34.260 --> 00:05:36.260
at least until 1918 --

00:05:36.260 --> 00:05:38.260
registered with the U.S. Patent Office.

00:05:38.260 --> 00:05:40.260
That was the Paige typesetter.

00:05:40.260 --> 00:05:42.260
The Paige typesetter

00:05:42.260 --> 00:05:44.260
had 18,000 parts.

00:05:44.260 --> 00:05:47.260
The patent had 64 pages of text

00:05:47.260 --> 00:05:51.260
and 271 figures.

00:05:51.260 --> 00:05:53.260
It was such a beautiful machine

00:05:53.260 --> 00:05:56.260
because it did everything that a human being did

00:05:56.260 --> 00:05:58.260
in setting type --

00:05:58.260 --> 00:06:00.260
including returning the type to its place,

00:06:00.260 --> 00:06:02.260
which was a very difficult thing.

00:06:02.260 --> 00:06:04.260
And Mark Twain, who knew all about typesetting,

00:06:04.260 --> 00:06:07.260
really was smitten by this machine.

00:06:07.260 --> 00:06:10.260
Unfortunately, he was smitten in more ways than one,

00:06:10.260 --> 00:06:12.260
because it made him bankrupt,

00:06:12.260 --> 00:06:14.260
and he had to tour the world speaking

00:06:14.260 --> 00:06:17.260
to recoup his money.

00:06:17.260 --> 00:06:19.260
And this was an important thing

00:06:19.260 --> 00:06:21.260
about 19th century technology,

00:06:21.260 --> 00:06:23.260
that all these relationships among parts

00:06:23.260 --> 00:06:27.260
could make the most brilliant idea fall apart,

00:06:27.260 --> 00:06:29.260
even when judged by the most expert people.

00:06:29.260 --> 00:06:32.260
Now there is something else, though, in the early 20th century

00:06:32.260 --> 00:06:35.260
that made things even more complicated.

00:06:35.260 --> 00:06:38.260
And that was that safety technology itself

00:06:38.260 --> 00:06:40.260
could be a source of danger.

00:06:40.260 --> 00:06:43.260
The lesson of the Titanic, for a lot of the contemporaries,

00:06:43.260 --> 00:06:45.260
was that you must have enough lifeboats

00:06:45.260 --> 00:06:47.260
for everyone on the ship.

00:06:47.260 --> 00:06:50.260
And this was the result

00:06:50.260 --> 00:06:52.260
of the tragic loss of lives

00:06:52.260 --> 00:06:54.260
of people who could not get into them.

00:06:54.260 --> 00:06:57.260
However, there was another case, the Eastland,

00:06:57.260 --> 00:07:01.260
a ship that capsized in Chicago Harbor in 1915,

00:07:01.260 --> 00:07:04.260
and it killed 841 people --

00:07:04.260 --> 00:07:06.260
that was 14 more

00:07:06.260 --> 00:07:09.260
than the passenger toll of the Titanic.

00:07:09.260 --> 00:07:11.260
The reason for it, in part, was

00:07:11.260 --> 00:07:14.260
the extra life boats that were added

00:07:14.260 --> 00:07:17.260
that made this already unstable ship

00:07:17.260 --> 00:07:19.260
even more unstable.

00:07:19.260 --> 00:07:21.260
And that again proves

00:07:21.260 --> 00:07:24.260
that when you're talking about unintended consequences,

00:07:24.260 --> 00:07:26.260
it's not that easy to know

00:07:26.260 --> 00:07:28.260
the right lessons to draw.

00:07:28.260 --> 00:07:31.260
It's really a question of the system, how the ship was loaded,

00:07:31.260 --> 00:07:34.260
the ballast and many other things.

00:07:35.260 --> 00:07:38.260
So the 20th century, then,

00:07:38.260 --> 00:07:40.260
saw how much more complex reality was,

00:07:40.260 --> 00:07:43.260
but it also saw a positive side.

00:07:43.260 --> 00:07:46.260
It saw that invention

00:07:46.260 --> 00:07:48.260
could actually benefit from emergencies.

00:07:48.260 --> 00:07:50.260
It could benefit

00:07:50.260 --> 00:07:53.260
from tragedies.

00:07:53.260 --> 00:07:55.260
And my favorite example of that --

00:07:55.260 --> 00:07:57.260
which is not really widely known

00:07:57.260 --> 00:07:59.260
as a technological miracle,

00:07:59.260 --> 00:08:02.260
but it may be one of the greatest of all times,

00:08:02.260 --> 00:08:06.260
was the scaling up of penicillin in the Second World War.

00:08:06.260 --> 00:08:09.260
Penicillin was discovered in 1928,

00:08:09.260 --> 00:08:11.260
but even by 1940,

00:08:11.260 --> 00:08:14.260
no commercially and medically useful quantities of it

00:08:14.260 --> 00:08:16.260
were being produced.

00:08:16.260 --> 00:08:19.260
A number of pharmaceutical companies were working on it.

00:08:19.260 --> 00:08:21.260
They were working on it independently,

00:08:21.260 --> 00:08:23.260
and they weren't getting anywhere.

00:08:23.260 --> 00:08:25.260
And the Government Research Bureau

00:08:25.260 --> 00:08:27.260
brought representatives together

00:08:27.260 --> 00:08:29.260
and told them that this is something

00:08:29.260 --> 00:08:31.260
that has to be done.

00:08:31.260 --> 00:08:33.260
And not only did they do it,

00:08:33.260 --> 00:08:35.260
but within two years,

00:08:35.260 --> 00:08:37.260
they scaled up penicillin

00:08:37.260 --> 00:08:40.260
from preparation in one-liter flasks

00:08:40.260 --> 00:08:44.260
to 10,000-gallon vats.

00:08:44.260 --> 00:08:48.260
That was how quickly penicillin was produced

00:08:48.260 --> 00:08:52.260
and became one of the greatest medical advances of all time.

00:08:52.260 --> 00:08:54.260
In the Second World War, too,

00:08:54.260 --> 00:08:56.260
the existence

00:08:56.260 --> 00:08:58.260
of solar radiation

00:08:58.260 --> 00:09:01.260
was demonstrated by studies of interference

00:09:01.260 --> 00:09:05.260
that was detected by the radar stations of Great Britain.

00:09:05.260 --> 00:09:08.260
So there were benefits in calamities --

00:09:08.260 --> 00:09:10.260
benefits to pure science,

00:09:10.260 --> 00:09:12.260
as well as to applied science

00:09:12.260 --> 00:09:15.260
and medicine.

00:09:15.260 --> 00:09:18.260
Now when we come to the period after the Second World War,

00:09:18.260 --> 00:09:22.260
unintended consequences get even more interesting.

00:09:22.260 --> 00:09:24.260
And my favorite example of that

00:09:24.260 --> 00:09:27.260
occurred beginning in 1976,

00:09:27.260 --> 00:09:29.260
when it was discovered

00:09:29.260 --> 00:09:32.260
that the bacteria causing Legionnaires disease

00:09:32.260 --> 00:09:35.260
had always been present in natural waters,

00:09:35.260 --> 00:09:39.260
but it was the precise temperature of the water

00:09:39.260 --> 00:09:42.260
in heating, ventilating and air conditioning systems

00:09:42.260 --> 00:09:46.260
that raised the right temperature

00:09:46.260 --> 00:09:49.260
for the maximum reproduction

00:09:49.260 --> 00:09:51.260
of Legionella bacillus.

00:09:51.260 --> 00:09:53.260
Well, technology to the rescue.

00:09:53.260 --> 00:09:55.260
So chemists got to work,

00:09:55.260 --> 00:09:57.260
and they developed a bactericide

00:09:57.260 --> 00:10:00.260
that became widely used in those systems.

00:10:00.260 --> 00:10:04.260
But something else happened in the early 1980s,

00:10:04.260 --> 00:10:06.260
and that was that there was a mysterious epidemic

00:10:06.260 --> 00:10:09.260
of failures of tape drives

00:10:09.260 --> 00:10:11.260
all over the United States.

00:10:11.260 --> 00:10:14.260
And IBM, which made them,

00:10:14.260 --> 00:10:17.260
just didn't know what to do.

00:10:17.260 --> 00:10:20.260
They commissioned a group of their best scientists

00:10:20.260 --> 00:10:22.260
to investigate,

00:10:22.260 --> 00:10:24.260
and what they found was

00:10:24.260 --> 00:10:26.260
that all these tape drives

00:10:26.260 --> 00:10:29.260
were located near ventilation ducts.

00:10:29.260 --> 00:10:32.260
What happened was the bactericide was formulated

00:10:32.260 --> 00:10:34.260
with minute traces of tin.

00:10:34.260 --> 00:10:37.260
And these tin particles were deposited on the tape heads

00:10:37.260 --> 00:10:40.260
and were crashing the tape heads.

00:10:40.260 --> 00:10:43.260
So they reformulated the bactericide.

00:10:43.260 --> 00:10:45.260
But what's interesting to me

00:10:45.260 --> 00:10:47.260
is that this was the first case

00:10:47.260 --> 00:10:49.260
of a mechanical device

00:10:49.260 --> 00:10:52.260
suffering, at least indirectly, from a human disease.

00:10:52.260 --> 00:10:55.260
So it shows that we're really all in this together.

00:10:55.260 --> 00:10:57.260
(Laughter)

00:10:57.260 --> 00:11:00.260
In fact, it also shows something interesting,

00:11:00.260 --> 00:11:03.260
that although our capabilities and technology

00:11:03.260 --> 00:11:05.260
have been expanding geometrically,

00:11:05.260 --> 00:11:08.260
unfortunately, our ability to model their long-term behavior,

00:11:08.260 --> 00:11:10.260
which has also been increasing,

00:11:10.260 --> 00:11:13.260
has been increasing only arithmetically.

00:11:13.260 --> 00:11:16.260
So one of the characteristic problems of our time

00:11:16.260 --> 00:11:18.260
is how to close this gap

00:11:18.260 --> 00:11:21.260
between capabilities and foresight.

00:11:21.260 --> 00:11:24.260
One other very positive consequence

00:11:24.260 --> 00:11:27.260
of 20th century technology, though,

00:11:27.260 --> 00:11:31.260
was the way in which other kinds of calamities

00:11:31.260 --> 00:11:34.260
could lead to positive advances.

00:11:34.260 --> 00:11:37.260
There are two historians of business

00:11:37.260 --> 00:11:39.260
at the University of Maryland,

00:11:39.260 --> 00:11:41.260
Brent Goldfarb and David Kirsch,

00:11:41.260 --> 00:11:43.260
who have done some extremely interesting work,

00:11:43.260 --> 00:11:46.260
much of it still unpublished,

00:11:46.260 --> 00:11:48.260
on the history of major innovations.

00:11:48.260 --> 00:11:51.260
They have combined the list of major innovations,

00:11:51.260 --> 00:11:54.260
and they've discovered that the greatest number, the greatest decade,

00:11:54.260 --> 00:11:56.260
for fundamental innovations,

00:11:56.260 --> 00:12:00.260
as reflected in all of the lists that others have made --

00:12:00.260 --> 00:12:02.260
a number of lists that they have merged --

00:12:02.260 --> 00:12:05.260
was the Great Depression.

00:12:05.260 --> 00:12:08.260
And nobody knows just why this was so,

00:12:08.260 --> 00:12:11.260
but one story can reflect something of it.

00:12:11.260 --> 00:12:14.260
It was the origin of the Xerox copier,

00:12:14.260 --> 00:12:17.260
which celebrated its 50th anniversary

00:12:17.260 --> 00:12:19.260
last year.

00:12:19.260 --> 00:12:24.260
And Chester Carlson, the inventor,

00:12:24.260 --> 00:12:27.260
was a patent attorney.

00:12:27.260 --> 00:12:30.260
He really was not intending

00:12:30.260 --> 00:12:32.260
to work in patent research,

00:12:32.260 --> 00:12:36.260
but he couldn't really find an alternative technical job.

00:12:36.260 --> 00:12:38.260
So this was the best job he could get.

00:12:38.260 --> 00:12:42.260
He was upset by the low quality and high cost

00:12:42.260 --> 00:12:45.260
of existing patent reproductions,

00:12:45.260 --> 00:12:48.260
and so he started to develop

00:12:48.260 --> 00:12:51.260
a system of dry photocopying,

00:12:51.260 --> 00:12:54.260
which he patented in the late 1930s --

00:12:54.260 --> 00:12:58.260
and which became the first dry photocopier

00:12:58.260 --> 00:13:00.260
that was commercially practical

00:13:00.260 --> 00:13:02.260
in 1960.

00:13:02.260 --> 00:13:04.260
So we see that sometimes,

00:13:04.260 --> 00:13:06.260
as a result of these dislocations,

00:13:06.260 --> 00:13:08.260
as a result of people

00:13:08.260 --> 00:13:11.260
leaving their original intended career

00:13:11.260 --> 00:13:13.260
and going into something else

00:13:13.260 --> 00:13:15.260
where their creativity could make a difference,

00:13:15.260 --> 00:13:17.260
that depressions

00:13:17.260 --> 00:13:20.260
and all kinds of other unfortunate events

00:13:20.260 --> 00:13:23.260
can have a paradoxically stimulating effect

00:13:23.260 --> 00:13:25.260
on creativity.

00:13:25.260 --> 00:13:27.260
What does this mean?

00:13:27.260 --> 00:13:29.260
It means, I think,

00:13:29.260 --> 00:13:31.260
that we're living in a time of unexpected possibilities.

00:13:31.260 --> 00:13:34.260
Think of the financial world, for example.

00:13:34.260 --> 00:13:37.260
The mentor of Warren Buffett, Benjamin Graham,

00:13:37.260 --> 00:13:42.260
developed his system of value investing

00:13:42.260 --> 00:13:44.260
as a result of his own losses

00:13:44.260 --> 00:13:46.260
in the 1929 crash.

00:13:46.260 --> 00:13:48.260
And he published that book

00:13:48.260 --> 00:13:51.260
in the early 1930s,

00:13:51.260 --> 00:13:53.260
and the book still exists in further editions

00:13:53.260 --> 00:13:55.260
and is still a fundamental textbook.

00:13:55.260 --> 00:13:59.260
So many important creative things can happen

00:13:59.260 --> 00:14:02.260
when people learn from disasters.

00:14:02.260 --> 00:14:06.260
Now think of the large and small plagues that we have now --

00:14:06.260 --> 00:14:11.260
bed bugs, killer bees, spam --

00:14:11.260 --> 00:14:14.260
and it's very possible that the solutions to those

00:14:14.260 --> 00:14:17.260
will really extend well beyond the immediate question.

00:14:17.260 --> 00:14:20.260
If we think, for example, of Louis Pasteur,

00:14:20.260 --> 00:14:22.260
who in the 1860s

00:14:22.260 --> 00:14:24.260
was asked to study

00:14:24.260 --> 00:14:28.260
the diseases of silk worms for the silk industry,

00:14:28.260 --> 00:14:31.260
and his discoveries were really the beginning

00:14:31.260 --> 00:14:33.260
of the germ theory of disease.

00:14:33.260 --> 00:14:36.260
So very often, some kind of disaster --

00:14:36.260 --> 00:14:39.260
sometimes the consequence, for example,

00:14:39.260 --> 00:14:42.260
of over-cultivation of silk worms,

00:14:42.260 --> 00:14:44.260
which was a problem in Europe at the time --

00:14:44.260 --> 00:14:46.260
can be the key to something much bigger.

00:14:46.260 --> 00:14:48.260
So this means

00:14:48.260 --> 00:14:50.260
that we need to take a different view

00:14:50.260 --> 00:14:52.260
of unintended consequences.

00:14:52.260 --> 00:14:55.260
We need to take a really positive view.

00:14:55.260 --> 00:14:58.260
We need to see what they can do for us.

00:14:58.260 --> 00:15:00.260
We need to learn

00:15:00.260 --> 00:15:02.260
from those figures that I mentioned.

00:15:02.260 --> 00:15:05.260
We need to learn, for example, from Dr. Cushing,

00:15:05.260 --> 00:15:07.260
who killed patients

00:15:07.260 --> 00:15:09.260
in the course of his early operations.

00:15:09.260 --> 00:15:12.260
He had to have some errors. He had to have some mistakes.

00:15:12.260 --> 00:15:15.260
And he learned meticulously from his mistakes.

00:15:15.260 --> 00:15:17.260
And as a result,

00:15:17.260 --> 00:15:20.260
when we say, "This isn't brain surgery,"

00:15:20.260 --> 00:15:23.260
that pays tribute to how difficult it was

00:15:23.260 --> 00:15:25.260
for anyone to learn from their mistakes

00:15:25.260 --> 00:15:27.260
in a field of medicine

00:15:27.260 --> 00:15:30.260
that was considered so discouraging in its prospects.

00:15:30.260 --> 00:15:33.260
And we can also remember

00:15:33.260 --> 00:15:35.260
how the pharmaceutical companies

00:15:35.260 --> 00:15:37.260
were willing to pool their knowledge,

00:15:37.260 --> 00:15:39.260
to share their knowledge,

00:15:39.260 --> 00:15:41.260
in the face of an emergency,

00:15:41.260 --> 00:15:44.260
which they hadn't really been for years and years.

00:15:44.260 --> 00:15:47.260
They might have been able to do it earlier.

00:15:47.260 --> 00:15:50.260
The message, then, for me,

00:15:50.260 --> 00:15:52.260
about unintended consequences

00:15:52.260 --> 00:15:55.260
is chaos happens;

00:15:55.260 --> 00:15:57.260
let's make better use of it.

00:15:57.260 --> 00:15:59.260
Thank you very much.

00:15:59.260 --> 00:16:03.260
(Applause)

