WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:07.000
Translator: Joseph Geni
Reviewer: Joanna Pietrulewicz

00:00:12.580 --> 00:00:15.636
So you probably have the sense,
as most people do,

00:00:15.660 --> 00:00:19.316
that polarization
is getting worse in our country,

00:00:19.340 --> 00:00:22.796
that the divide
between the left and the right

00:00:22.820 --> 00:00:26.356
is as bad as it's been
in really any of our lifetimes.

00:00:26.380 --> 00:00:31.660
But you might also reasonably wonder
if research backs up your intuition.

00:00:32.380 --> 00:00:37.060
And in a nutshell,
the answer is sadly yes.

00:00:38.740 --> 00:00:40.756
In study after study, we find

00:00:40.780 --> 00:00:44.460
that liberals and conservatives
have grown further apart.

00:00:45.260 --> 00:00:50.036
They increasingly wall themselves off
in these ideological silos,

00:00:50.060 --> 00:00:54.196
consuming different news,
talking only to like-minded others

00:00:54.220 --> 00:00:57.460
and more and more choosing
to live in different parts of the country.

00:00:58.540 --> 00:01:01.756
And I think that
most alarming of all of it

00:01:01.780 --> 00:01:05.580
is seeing this rising
animosity on both sides.

00:01:06.260 --> 00:01:07.916
Liberals and conservatives,

00:01:07.940 --> 00:01:09.836
Democrats and Republicans,

00:01:09.860 --> 00:01:12.980
more and more they just
don't like one another.

00:01:14.140 --> 00:01:16.156
You see it in many different ways.

00:01:16.180 --> 00:01:19.836
They don't want to befriend one another.
They don't want to date one another.

00:01:19.860 --> 00:01:23.156
If they do, if they find out,
they find each other less attractive,

00:01:23.180 --> 00:01:26.276
and they more and more don't want
their children to marry someone

00:01:26.300 --> 00:01:27.996
who supports the other party,

00:01:28.020 --> 00:01:29.780
a particularly shocking statistic.

00:01:31.460 --> 00:01:34.276
You know, in my lab,
the students that I work with,

00:01:34.300 --> 00:01:37.756
we're talking about
some sort of social pattern --

00:01:37.780 --> 00:01:41.316
I'm a movie buff, and so I'm often like,

00:01:41.340 --> 00:01:44.300
what kind of movie are we in here
with this pattern?

00:01:44.900 --> 00:01:48.180
So what kind of movie are we in
with political polarization?

00:01:48.900 --> 00:01:51.620
Well, it could be a disaster movie.

00:01:52.700 --> 00:01:54.380
It certainly seems like a disaster.

00:01:54.740 --> 00:01:56.740
Could be a war movie.

00:01:57.460 --> 00:01:58.660
Also fits.

00:01:59.300 --> 00:02:03.116
But what I keep thinking is that
we're in a zombie apocalypse movie.

00:02:03.140 --> 00:02:04.596
(Laughter)

00:02:04.620 --> 00:02:06.916
Right? You know the kind.

00:02:06.940 --> 00:02:09.356
There's people wandering around in packs,

00:02:09.380 --> 00:02:11.156
not thinking for themselves,

00:02:11.180 --> 00:02:12.796
seized by this mob mentality

00:02:12.820 --> 00:02:16.060
trying to spread their disease
and destroy society.

00:02:17.300 --> 00:02:19.636
And you probably think, as I do,

00:02:19.660 --> 00:02:23.116
that you're the good guy
in the zombie apocalypse movie,

00:02:23.140 --> 00:02:26.836
and all this hate and polarization,
it's being propagated by the other people,

00:02:26.860 --> 00:02:28.740
because we're Brad Pitt, right?

00:02:29.580 --> 00:02:32.476
Free-thinking, righteous,

00:02:32.500 --> 00:02:34.796
just trying to hold on
to what we hold dear,

00:02:34.820 --> 00:02:38.396
you know, not foot soldiers
in the army of the undead.

00:02:38.420 --> 00:02:39.876
Not that.

00:02:39.900 --> 00:02:41.100
Never that.

00:02:41.900 --> 00:02:43.396
But here's the thing:

00:02:43.420 --> 00:02:46.140
what movie do you suppose
they think they're in?

00:02:47.300 --> 00:02:48.516
Right?

00:02:48.540 --> 00:02:51.076
Well, they absolutely think
that they're the good guys

00:02:51.100 --> 00:02:52.956
in the zombie apocalypse movie. Right?

00:02:52.980 --> 00:02:55.956
And you'd better believe
that they think that they're Brad Pitt

00:02:55.980 --> 00:02:58.100
and that we, we are the zombies.

00:03:00.940 --> 00:03:03.300
And who's to say that they're wrong?

00:03:04.260 --> 00:03:07.380
I think that the truth is
that we're all a part of this.

00:03:08.060 --> 00:03:11.220
And the good side of that
is that we can be a part of the solution.

00:03:12.100 --> 00:03:14.100
So what are we going to do?

00:03:15.140 --> 00:03:19.396
What can we do to chip away
at polarization in everyday life?

00:03:19.420 --> 00:03:23.236
What could we do to connect with
and communicate with

00:03:23.260 --> 00:03:24.980
our political counterparts?

00:03:25.540 --> 00:03:29.676
Well, these were exactly the questions
that I and my colleague, Matt Feinberg,

00:03:29.700 --> 00:03:31.558
became fascinated with a few years ago,

00:03:31.582 --> 00:03:33.782
and we started
doing research on this topic.

00:03:34.740 --> 00:03:37.716
And one of the first things
that we discovered

00:03:37.740 --> 00:03:41.196
that I think is really helpful
for understanding polarization

00:03:41.220 --> 00:03:42.436
is to understand

00:03:42.460 --> 00:03:46.876
that the political divide in our country
is undergirded by a deeper moral divide.

00:03:46.900 --> 00:03:51.676
So one of the most robust findings
in the history of political psychology

00:03:51.700 --> 00:03:55.396
is this pattern identified
by Jon Haidt and Jesse Graham,

00:03:55.420 --> 00:03:56.636
psychologists,

00:03:56.660 --> 00:04:00.676
that liberals and conservatives
tend to endorse different values

00:04:00.700 --> 00:04:01.900
to different degrees.

00:04:02.420 --> 00:04:07.916
So for example, we find that liberals
tend to endorse values like equality

00:04:07.940 --> 00:04:11.596
and fairness and care
and protection from harm

00:04:11.620 --> 00:04:13.756
more than conservatives do.

00:04:13.780 --> 00:04:19.036
And conservatives tend to endorse
values like loyalty, patriotism,

00:04:19.060 --> 00:04:22.516
respect for authority and moral purity

00:04:22.540 --> 00:04:24.620
more than liberals do.

00:04:25.740 --> 00:04:29.796
And Matt and I were thinking
that maybe this moral divide

00:04:29.820 --> 00:04:32.916
might be helpful
for understanding how it is

00:04:32.940 --> 00:04:35.356
that liberals and conservatives
talk to one another

00:04:35.380 --> 00:04:37.796
and why they so often
seem to talk past one another

00:04:37.820 --> 00:04:39.036
when they do.

00:04:39.060 --> 00:04:41.036
So we conducted a study

00:04:41.060 --> 00:04:44.156
where we recruited liberals to a study

00:04:44.180 --> 00:04:46.636
where they were supposed
to write a persuasive essay

00:04:46.660 --> 00:04:51.100
that would be compelling to a conservative
in support of same-sex marriage.

00:04:51.620 --> 00:04:54.876
And what we found was that liberals
tended to make arguments

00:04:54.900 --> 00:04:59.076
in terms of the liberal moral values
of equality and fairness.

00:04:59.100 --> 00:05:00.836
So they said things like,

00:05:00.860 --> 00:05:04.236
"Everyone should have the right
to love whoever they choose,"

00:05:04.260 --> 00:05:06.836
and, "They" -- they being gay Americans --

00:05:06.860 --> 00:05:09.620
"deserve the same equal rights
as other Americans."

00:05:10.180 --> 00:05:13.396
Overall, we found
that 69 percent of liberals

00:05:13.420 --> 00:05:18.836
invoked one of the more liberal
moral values in constructing their essay,

00:05:18.860 --> 00:05:22.556
and only nine percent invoked
one of the more conservative moral values,

00:05:22.580 --> 00:05:25.996
even though they were supposed
to be trying to persuade conservatives.

00:05:26.020 --> 00:05:30.316
And when we studied conservatives
and had them make persuasive arguments

00:05:30.340 --> 00:05:33.236
in support of making English
the official language of the US,

00:05:33.260 --> 00:05:35.796
a classically conservative
political position,

00:05:35.820 --> 00:05:38.036
we found that they weren't
much better at this.

00:05:38.060 --> 00:05:39.676
59 percent of them made arguments

00:05:39.700 --> 00:05:42.396
in terms of one of the more
conservative moral values,

00:05:42.420 --> 00:05:44.916
and just eight percent
invoked a liberal moral value,

00:05:44.940 --> 00:05:48.300
even though they were supposed
to be targeting liberals for persuasion.

00:05:49.300 --> 00:05:53.340
Now, you can see right away
why we're in trouble here. Right?

00:05:54.100 --> 00:05:57.596
People's moral values,
they're their most deeply held beliefs.

00:05:57.620 --> 00:06:01.020
People are willing
to fight and die for their values.

00:06:01.540 --> 00:06:04.236
Why are they going to give that up
just to agree with you

00:06:04.260 --> 00:06:07.796
on something that they don't particularly
want to agree with you on anyway?

00:06:07.820 --> 00:06:11.076
If that persuasive appeal that
you're making to your Republican uncle

00:06:11.100 --> 00:06:13.516
means that he doesn't
just have to change his view,

00:06:13.540 --> 00:06:15.706
he's got to change
his underlying values, too,

00:06:15.730 --> 00:06:17.290
that's not going to go very far.

00:06:17.900 --> 00:06:19.220
So what would work better?

00:06:20.020 --> 00:06:24.316
Well, we believe it's a technique
that we call moral reframing,

00:06:24.340 --> 00:06:26.956
and we've studied it
in a series of experiments.

00:06:26.980 --> 00:06:28.476
In one of these experiments,

00:06:28.500 --> 00:06:31.636
we recruited liberals
and conservatives to a study

00:06:31.660 --> 00:06:33.956
where they read one of three essays

00:06:33.980 --> 00:06:37.020
before having their environmental
attitudes surveyed.

00:06:37.460 --> 00:06:38.956
And the first of these essays

00:06:38.980 --> 00:06:42.356
was a relatively conventional
pro-environmental essay

00:06:42.380 --> 00:06:46.396
that invoked the liberal values
of care and protection from harm.

00:06:46.420 --> 00:06:48.956
It said things like,
"In many important ways

00:06:48.980 --> 00:06:51.796
we are causing real harm
to the places we live in,"

00:06:51.820 --> 00:06:54.636
and, "It is essential
that we take steps now

00:06:54.660 --> 00:06:57.580
to prevent further destruction
from being done to our Earth."

00:06:58.940 --> 00:07:00.356
Another group of participants

00:07:00.380 --> 00:07:02.596
were assigned to read
a really different essay

00:07:02.620 --> 00:07:07.060
that was designed to tap into
the conservative value of moral purity.

00:07:08.010 --> 00:07:09.996
It was a pro-environmental essay as well,

00:07:10.020 --> 00:07:11.516
and it said things like,

00:07:11.540 --> 00:07:15.780
"Keeping our forests, drinking water,
and skies pure is of vital importance."

00:07:16.820 --> 00:07:18.316
"We should regard the pollution

00:07:18.340 --> 00:07:20.380
of the places we live in
to be disgusting."

00:07:20.980 --> 00:07:23.076
And, "Reducing pollution
can help us preserve

00:07:23.100 --> 00:07:26.260
what is pure and beautiful
about the places we live."

00:07:27.700 --> 00:07:29.116
And then we had a third group

00:07:29.140 --> 00:07:31.636
that were assigned
to read just a nonpolitical essay.

00:07:31.660 --> 00:07:34.396
It was just a comparison group
so we could get a baseline.

00:07:34.420 --> 00:07:36.373
And what we found when we surveyed people

00:07:36.397 --> 00:07:38.596
about their environmental
attitudes afterwards,

00:07:38.620 --> 00:07:41.556
we found that liberals,
it didn't matter what essay they read.

00:07:41.580 --> 00:07:44.676
They tended to have highly
pro-environmental attitudes regardless.

00:07:44.700 --> 00:07:47.116
Liberals are on board
for environmental protection.

00:07:47.140 --> 00:07:48.356
Conservatives, however,

00:07:48.380 --> 00:07:52.796
were significantly more supportive
of progressive environmental policies

00:07:52.820 --> 00:07:54.356
and environmental protection

00:07:54.380 --> 00:07:56.436
if they had read the moral purity essay

00:07:56.460 --> 00:07:58.860
than if they read
one of the other two essays.

00:07:59.980 --> 00:08:03.076
We even found that conservatives
who read the moral purity essay

00:08:03.100 --> 00:08:06.596
were significantly more likely to say
that they believed in global warming

00:08:06.620 --> 00:08:08.525
and were concerned about global warming,

00:08:08.549 --> 00:08:11.276
even though this essay
didn't even mention global warming.

00:08:11.300 --> 00:08:13.756
That's just a related environmental issue.

00:08:13.780 --> 00:08:16.860
But that's how robust
this moral reframing effect was.

00:08:17.780 --> 00:08:21.516
And we've studied this on a whole slew
of different political issues.

00:08:21.540 --> 00:08:25.276
So if you want to move conservatives

00:08:25.300 --> 00:08:28.396
on issues like same-sex marriage
or national health insurance,

00:08:28.420 --> 00:08:31.876
it helps to tie these liberal
political issues to conservative values

00:08:31.900 --> 00:08:34.700
like patriotism and moral purity.

00:08:35.620 --> 00:08:37.716
And we studied it the other way, too.

00:08:37.740 --> 00:08:41.556
If you want to move liberals
to the right on conservative policy issues

00:08:41.580 --> 00:08:46.196
like military spending and making English
the official language of the US,

00:08:46.220 --> 00:08:47.876
you're going to be more persuasive

00:08:47.900 --> 00:08:51.236
if you tie those conservative
policy issues to liberal moral values

00:08:51.260 --> 00:08:53.140
like equality and fairness.

00:08:54.460 --> 00:08:57.316
All these studies
have the same clear message:

00:08:57.340 --> 00:09:00.276
if you want to persuade
someone on some policy,

00:09:00.300 --> 00:09:04.140
it's helpful to connect that policy
to their underlying moral values.

00:09:05.340 --> 00:09:07.516
And when you say it like that

00:09:07.540 --> 00:09:09.036
it seems really obvious. Right?

00:09:09.060 --> 00:09:10.836
Like, why did we come here tonight?

00:09:10.860 --> 00:09:12.076
Why --

00:09:12.100 --> 00:09:13.636
(Laughter)

00:09:13.660 --> 00:09:15.700
It's incredibly intuitive.

00:09:17.220 --> 00:09:20.516
And even though it is,
it's something we really struggle to do.

00:09:20.540 --> 00:09:24.396
You know, it turns out that when we go
to persuade somebody on a political issue,

00:09:24.420 --> 00:09:27.156
we talk like we're speaking into a mirror.

00:09:27.180 --> 00:09:31.556
We don't persuade so much
as we rehearse our own reasons

00:09:31.580 --> 00:09:34.460
for why we believe
some sort of political position.

00:09:35.220 --> 00:09:39.636
We kept saying when we were designing
these reframed moral arguments,

00:09:39.660 --> 00:09:42.300
"Empathy and respect,
empathy and respect."

00:09:42.860 --> 00:09:44.316
If you can tap into that,

00:09:44.340 --> 00:09:45.996
you can connect

00:09:46.020 --> 00:09:48.820
and you might be able to persuade
somebody in this country.

00:09:49.380 --> 00:09:51.796
So thinking again

00:09:51.820 --> 00:09:54.100
about what movie we're in,

00:09:55.020 --> 00:09:56.596
maybe I got carried away before.

00:09:56.620 --> 00:09:58.580
Maybe it's not a zombie apocalypse movie.

00:09:59.340 --> 00:10:01.260
Maybe instead it's a buddy cop movie.

00:10:01.860 --> 00:10:03.876
(Laughter)

00:10:03.900 --> 00:10:05.916
Just roll with it, just go with it please.

00:10:05.940 --> 00:10:07.380
(Laughter)

00:10:08.300 --> 00:10:10.996
You know the kind:
there's a white cop and a black cop,

00:10:11.020 --> 00:10:13.156
or maybe a messy cop and an organized cop.

00:10:13.180 --> 00:10:15.236
Whatever it is, they don't get along

00:10:15.260 --> 00:10:16.546
because of this difference.

00:10:17.340 --> 00:10:20.556
But in the end, when they have
to come together and they cooperate,

00:10:20.580 --> 00:10:22.516
the solidarity that they feel,

00:10:22.540 --> 00:10:26.180
it's greater because of that gulf
that they had to cross. Right?

00:10:27.100 --> 00:10:29.076
And remember that in these movies,

00:10:29.100 --> 00:10:31.996
it's usually worst in the second act

00:10:32.020 --> 00:10:34.420
when our leads are further apart
than ever before.

00:10:35.260 --> 00:10:37.596
And so maybe that's
where we are in this country,

00:10:37.620 --> 00:10:39.796
late in the second act
of a buddy cop movie --

00:10:39.820 --> 00:10:42.396
(Laughter)

00:10:42.420 --> 00:10:45.500
torn apart but about
to come back together.

00:10:47.220 --> 00:10:48.876
It sounds good,

00:10:48.900 --> 00:10:50.756
but if we want it to happen,

00:10:50.780 --> 00:10:53.500
I think the responsibility
is going to start with us.

00:10:54.340 --> 00:10:56.500
So this is my call to you:

00:10:57.300 --> 00:10:59.300
let's put this country back together.

00:11:00.900 --> 00:11:03.956
Let's do it despite the politicians

00:11:03.980 --> 00:11:06.836
and the media and Facebook and Twitter

00:11:06.860 --> 00:11:08.396
and Congressional redistricting

00:11:08.420 --> 00:11:11.140
and all of it,
all the things that divide us.

00:11:12.180 --> 00:11:14.420
Let's do it because it's right.

00:11:15.740 --> 00:11:20.156
And let's do it
because this hate and contempt

00:11:20.180 --> 00:11:22.340
that flows through all of us every day

00:11:23.220 --> 00:11:26.396
makes us ugly and it corrupts us,

00:11:26.420 --> 00:11:29.740
and it threatens
the very fabric of our society.

00:11:31.780 --> 00:11:34.436
We owe it to one another and our country

00:11:34.460 --> 00:11:36.620
to reach out and try to connect.

00:11:37.820 --> 00:11:40.980
We can't afford to hate them any longer,

00:11:42.020 --> 00:11:44.220
and we can't afford
to let them hate us either.

00:11:45.700 --> 00:11:47.060
Empathy and respect.

00:11:47.700 --> 00:11:48.940
Empathy and respect.

00:11:49.740 --> 00:11:53.540
If you think about it, it's the very least
that we owe our fellow citizens.

00:11:54.220 --> 00:11:55.436
Thank you.

00:11:55.460 --> 00:12:00.145
(Applause)

