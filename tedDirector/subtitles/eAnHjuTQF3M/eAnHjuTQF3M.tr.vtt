WEBVTT
Kind: captions
Language: tr

00:00:00.000 --> 00:00:07.000
Çeviri: Cihan Senel
Gözden geçirme: osman oguz ahsen

00:00:15.260 --> 00:00:18.260
Çocukluğumda

00:00:18.260 --> 00:00:20.260
"Star Wars"u ilk defa gördüğümden beri,

00:00:20.260 --> 00:00:22.260
kişisel robotlar fikrinden

00:00:22.260 --> 00:00:24.260
hep etkilenmişimdir.

00:00:24.260 --> 00:00:26.260
Ve küçük bir kızken,

00:00:26.260 --> 00:00:28.260
insanlarla yardımsever, güvenilir bir dost gibi

00:00:28.260 --> 00:00:31.260
etkileşim içinde olan robot fikrini hep sevmişimdir --

00:00:31.260 --> 00:00:33.260
bizi mutlu eden, hayatımızı zenginleştiren

00:00:33.260 --> 00:00:36.260
ve bize birkaç galaksiyi kurtarmamızda yardımcı olan birşey.

00:00:37.260 --> 00:00:40.260
Böyle robotların olmadığını biliyordum,

00:00:40.260 --> 00:00:42.260
ama onları yapmak istediğimi de.

00:00:42.260 --> 00:00:44.260
20 yıl geçti --

00:00:44.260 --> 00:00:46.260
Şu an MIT'de öğrenciyim

00:00:46.260 --> 00:00:48.260
ve yapay zeka üzerine çalışıyorum,

00:00:48.260 --> 00:00:50.260
yıllardan 1997,

00:00:50.260 --> 00:00:53.260
ve NASA Mars'a ilk robotu indirdi.

00:00:53.260 --> 00:00:56.260
Fakat işe bakın ki robotlar hala evlerimize girmediler.

00:00:56.260 --> 00:00:58.260
Ve ben bunun neden böyle olduğuyla ilgili

00:00:58.260 --> 00:01:00.260
sebepleri düşündüğümü hatırlıyorum.

00:01:00.260 --> 00:01:02.260
Bunlardan biri beni çok etkiledi.

00:01:02.260 --> 00:01:05.260
Robotik hep birşeylerle etkileşim içinde olmakla ilgili omuştur,

00:01:05.260 --> 00:01:07.260
insanlar dışında --

00:01:07.260 --> 00:01:09.260
tabii ki bize normal gelen sosyal bir şekilde değil

00:01:09.260 --> 00:01:11.260
ki böyle birşey insanların robotların günlük hayatımıza girmesini

00:01:11.260 --> 00:01:13.260
kabul etmesine yardımcı olurdu.

00:01:13.260 --> 00:01:16.260
Benim için bu, robotların henüz eksik olduğu bir noktaydı.

00:01:16.260 --> 00:01:19.260
O sene bir robot yapmaya başladım, Kısmet,

00:01:19.260 --> 00:01:22.260
Dünya'nın ilk sosyal robotu.

00:01:22.260 --> 00:01:24.260
Üç yıl sonra --

00:01:24.260 --> 00:01:26.260
çok uzun süren programlama,

00:01:26.260 --> 00:01:28.260
diğer öğrencilerle laboratuvarda yapılan çalışmalar --

00:01:28.260 --> 00:01:30.260
Kismet insanlarla etkileşime hazırdı.

00:01:30.260 --> 00:01:32.260
(Vidyo) Biliminsanı: Size birşey göstermek istiyorum.

00:01:32.260 --> 00:01:34.260
Kismet: (Mantıksız).

00:01:34.260 --> 00:01:37.260
Biliminsanı: Bu benim kız arkadaşımın bana verdiği bir saat.

00:01:37.260 --> 00:01:39.260
Kismet: (Mantıksız).

00:01:39.260 --> 00:01:41.260
Biliminsanı: Evet, bak, içinde birde mavi bir ışık var.

00:01:41.260 --> 00:01:44.260
Bu hafta onu nerdeyse kaybediyordum.

00:01:44.260 --> 00:01:47.260
Cynthia Breazeal: Kismet insanlarla bu şekilde

00:01:47.260 --> 00:01:50.260
konuşmayı bilmeyen yada yeni öğrenen bir çocuk gibi iletişim kurdu,

00:01:50.260 --> 00:01:53.260
bu ona çok uygundu çünkü o türünün gerçekten de ilk örneğiydi.

00:01:53.260 --> 00:01:55.260
Bir dili konuşmuyordu ama bu önemli değildi.

00:01:55.260 --> 00:01:57.260
Bu küçük robot bir şekilde

00:01:57.260 --> 00:02:00.260
içimizdeki o derin sosyal duyguya dokunmayı başarıyordu.

00:02:00.260 --> 00:02:02.260
Ve bu şekilde robotlarla iletişim kurmanın

00:02:02.260 --> 00:02:04.260
yepyeni bir yolu vaad ediliyordu.

00:02:04.260 --> 00:02:06.260
Geçtiğimiz birkaç yıl içerisinde

00:02:06.260 --> 00:02:08.260
robotlarla insanlar arasındaki bu ilişkiyi inceledim,

00:02:08.260 --> 00:02:10.260
ve şimdi medya laboratuvarında

00:02:10.260 --> 00:02:12.260
kendi takımımdaki inanılmaz yetenekli öğrencilerle birlikte çalışmaya devam ediyorum.

00:02:12.260 --> 00:02:15.260
Favori robotlarımdan biri Leonardo.

00:02:15.260 --> 00:02:18.260
Leonardoyu Stan Winston stüdyosuyla birlikte geliştirdik.

00:02:18.260 --> 00:02:21.260
Ve şimdi size Leo'nun benim için özel bir anını göstermek istiyorum.

00:02:21.260 --> 00:02:23.260
Leo ile iletişim içinde olan Matt Berlin,

00:02:23.260 --> 00:02:25.260
Leo'ya yeni bir nesneyi tanıtıyor.

00:02:25.260 --> 00:02:28.260
Ve yeni olduğu için Leo bu nesneyle ne yapacağını bilemiyor.

00:02:28.260 --> 00:02:30.260
Ama bizim gibi, Matt'in tepkisini izleyerek

00:02:30.260 --> 00:02:33.260
hakkında birşeyler öğrenebilir.

00:02:33.260 --> 00:02:35.260
(Video) Matt Berlin: Merhaba, Leo.

00:02:38.260 --> 00:02:41.260
Leo, bu kurabiye canavarı.

00:02:44.260 --> 00:02:47.260
Kurabiye Canavarı'nı bulabilirmisin?

00:02:52.260 --> 00:02:55.260
Leo, Kurabiye Canavarı çok kötü.

00:02:56.260 --> 00:02:58.260
O çok kötü, Leo.

00:03:00.260 --> 00:03:03.260
Kurabiye Canavarı çok çok kötü.

00:03:07.260 --> 00:03:09.260
O korkunç bir canavar.

00:03:09.260 --> 00:03:11.260
Senin kurabiyelerini almak istiyor.

00:03:12.260 --> 00:03:14.260
(Gülüşmeler)

00:03:14.260 --> 00:03:17.260
CB: Tamam, Leo ve Kurabiye

00:03:17.260 --> 00:03:19.260
zor bi başlangıç yapmış olabilirler,

00:03:19.260 --> 00:03:22.260
ama şimdi araları çok iyi.

00:03:22.260 --> 00:03:24.260
Bu sistemleri oluştururken

00:03:24.260 --> 00:03:26.260
öğrendim ki

00:03:26.260 --> 00:03:28.260
robotlar aslında

00:03:28.260 --> 00:03:30.260
çok etkileyici soyal bir teknolojiymişler.

00:03:30.260 --> 00:03:32.260
Aslında onların

00:03:32.260 --> 00:03:34.260
bizim sosyal yanımıza dokunabilme

00:03:34.260 --> 00:03:36.260
ve bizimle bir arkadaş gibi ilişki kurabilme yetenekleri

00:03:36.260 --> 00:03:39.260
onların temel işlevlerinden biri.

00:03:39.260 --> 00:03:41.260
Ve bu farklı düşünce şekliyle şimdi robotlar için

00:03:41.260 --> 00:03:44.260
yeni sorular, yeni olasılıklar üzerinde

00:03:44.260 --> 00:03:47.260
düşünmeye başlayabiliriz.

00:03:47.260 --> 00:03:49.260
Peki ben "sosyal yanımıza dokunuyor" derken ne söylemek istiyorum?

00:03:49.260 --> 00:03:51.260
Öğrendiğimiz şeylerden bir tanesi,

00:03:51.260 --> 00:03:53.260
eğer bu robotları bizimle iletişim kurmaları için dizayn edersek,

00:03:53.260 --> 00:03:55.260
insanların kullandığı aynı vücut dilini,

00:03:55.260 --> 00:03:57.260
ve aynı şekildeki sözsüz ifadeleri kullanarak --

00:03:57.260 --> 00:04:00.260
bizim insancıl robotumuz Nexi'nin yaptığı gibi --

00:04:00.260 --> 00:04:02.260
bulduğumuz şey insanların robotlara, insanlara verdikleri tepkiye

00:04:02.260 --> 00:04:04.260
çok benzer bir şekilde tepki vermeleriydi.

00:04:04.260 --> 00:04:07.260
İnsanlar bu tepkileri kullanarak birisinin ne kadar ikna edici,

00:04:07.260 --> 00:04:09.260
ne kadar sevilebilir, ne kadar çekici,

00:04:09.260 --> 00:04:11.260
ne kadar güvenilir olduğuna karar verirler.

00:04:11.260 --> 00:04:13.260
Aynı şey robotlar içinde geçerli.

00:04:13.260 --> 00:04:15.260
Şu anlaşılıyorki

00:04:15.260 --> 00:04:18.260
robotlar insan davranışını anlamak için çok ilginç bilimsel bir araç

00:04:18.260 --> 00:04:20.260
haline geliyorlar.

00:04:20.260 --> 00:04:23.260
Nasıl oluyorda kısa bir karşılaşmadan sonra bir insanın ne kadar

00:04:23.260 --> 00:04:26.260
güvenilir olduğunu tahmin edebiliyoruz gibi soruları yanıtlamak için.

00:04:26.260 --> 00:04:29.260
Mimicry bir rol yaptığına inandı, ama nasıl?

00:04:29.260 --> 00:04:32.260
Önemli olan belirli davranışların taklit edilmesi miydi?

00:04:32.260 --> 00:04:34.260
İnsanları izleyerek bunu anlamak yada bunu

00:04:34.260 --> 00:04:36.260
öğrenmek çok zor çünkü biz insanlarla iletişim kurarken

00:04:36.260 --> 00:04:39.260
herşeyi otomatik olarak yaparız.

00:04:39.260 --> 00:04:41.260
Yaptıklarımızı dikkatlice kontrol edemeyiz çünkü onlar bilinç altından gelir.

00:04:41.260 --> 00:04:43.260
Ama robotlarla bunu yapabiliriz.

00:04:43.260 --> 00:04:45.260
Burda gördüğünüz vidyodaki gibi --

00:04:45.260 --> 00:04:48.260
bu vidyo Northeastern üniversitesi'nde David DeSteno'nun laboratuvarında çekildi.

00:04:48.260 --> 00:04:50.260
David DeSteno birlikte çalıştığımız bir psikolog.

00:04:50.260 --> 00:04:53.260
Aradığımız cevaba ulaşmak için

00:04:53.260 --> 00:04:56.260
Nexi'nin hareketlerini çok dikkatlice inceleyen bir biliminsanı var.

00:04:56.260 --> 00:04:58.260
Sonuç olarak -- bu yöntemin işe yaramasının sebebi --

00:04:58.260 --> 00:05:00.260
insanlar robotlarla iletişim kurarken yine

00:05:00.260 --> 00:05:03.260
insan gibi davranıyorlar.

00:05:03.260 --> 00:05:05.260
Bu anahtar fikir ele alındığında

00:05:05.260 --> 00:05:07.260
robotlar için yeni uygulama alanları bulmayı

00:05:07.260 --> 00:05:10.260
hayal etmeye başlayabiliriz.

00:05:10.260 --> 00:05:13.260
Örneğin, eğer robotlar bizim söylevsel olmayan davranışlarımıza

00:05:13.260 --> 00:05:17.260
tepki verebilselerdi, bu çok ilginç bir iletişim teknolojisi olabilirdi.

00:05:17.260 --> 00:05:19.260
Şunu hayal edin:

00:05:19.260 --> 00:05:21.260
Cep telefonunuz için bir robot aksesuvarına ne dersiniz?

00:05:21.260 --> 00:05:23.260
Arkadaşını arıyorsun, o cep telefonunu bir robotun içine koyuyor,

00:05:23.260 --> 00:05:25.260
ve, bem!, şimdi bir MeBot'sun (BenRobot) --

00:05:25.260 --> 00:05:28.260
arkadaşlarınla göz kontağı kurabilir, onlarla konuşabilirsin,

00:05:28.260 --> 00:05:30.260
etrafta dolanabilir ve tepkini gösterebilirsin --

00:05:30.260 --> 00:05:33.260
orda gerçekten olmaktan sonra en iyi şey, değil mi?

00:05:33.260 --> 00:05:35.260
Bu soruyu incelemek için

00:05:35.260 --> 00:05:38.260
öğrencim Siggy Adalgeirsson bir araştırma yaptı.

00:05:38.260 --> 00:05:41.260
Bu araştırmada insanları laboratuvarımıza davet ettik

00:05:41.260 --> 00:05:43.260
ve onlardan başka bir yerdeki insanlarla

00:05:43.260 --> 00:05:45.260
ortaklaşa bir ödevi gerçekleştirmelerini istedik.

00:05:45.260 --> 00:05:47.260
Bu ödevin içerdikleri

00:05:47.260 --> 00:05:49.260
masa üstünde duran bir kaç nesneye bakıp

00:05:49.260 --> 00:05:52.260
bunların belli bir ödevi yerine getirmek için ne kadar alakalı ve önemli olduğunu tartışmak --

00:05:52.260 --> 00:05:54.260
bu bir hayatta kalma görevine dönmüştü --

00:05:54.260 --> 00:05:56.260
ve bunları önemine göre

00:05:56.260 --> 00:05:58.260
sıralamaktı.

00:05:58.260 --> 00:06:01.260
Uzaktaki işbirlikçimiz bizim grubumuzdan birisiydi

00:06:01.260 --> 00:06:03.260
ve katılımcılarla iletişime geçmek için

00:06:03.260 --> 00:06:05.260
üç farklı teknolojiyi kullandılar.

00:06:05.260 --> 00:06:07.260
Bunlardan ilki bir ekrandı.

00:06:07.260 --> 00:06:10.260
Günümüzdeki vidyo konferansın aynısı.

00:06:10.260 --> 00:06:13.260
İkincisi ekranı hareketli bir plaform üstüne yerleştirip, hareket yeteneği sağlamaktı.

00:06:13.260 --> 00:06:16.260
Eğer bugünkü teleprezens robotlarını biliyorsanız,

00:06:16.260 --> 00:06:19.260
burda yaptığımız bunun bir örneği.

00:06:19.260 --> 00:06:21.260
Ve son olarak MeBot.

00:06:21.260 --> 00:06:23.260
Bu deneyden sonra

00:06:23.260 --> 00:06:26.260
insanlara uzaktaki arkadaşımızla

00:06:26.260 --> 00:06:28.260
teknolojiyi farklı şekillerde kullanarak

00:06:28.260 --> 00:06:31.260
gerçekleştirdikleri iletişimin kalitesini sorduk.

00:06:31.260 --> 00:06:33.260
Psikolojik katılımı inceledik --

00:06:33.260 --> 00:06:35.260
diğer insan için ne kadar empati hissetiniz?

00:06:35.260 --> 00:06:37.260
Toplamda gösterilen çabayı inceledik.

00:06:37.260 --> 00:06:39.260
İnsanların işbirliği yapma isteğini inceledik.

00:06:39.260 --> 00:06:42.260
Ve burda gördüğümüz, insanlar sadece ekranı kullandığında ortaya çıkan sonuç.

00:06:42.260 --> 00:06:45.260
Eğer hareket kabiliyeti eklersen -- masanın etrafında dönme yeteneği --

00:06:45.260 --> 00:06:47.260
sonuçlarda hafif bir artma görülüyor.

00:06:47.260 --> 00:06:50.260
Ve MeBot kullanıldığında, çok daha fazla bir artış elde ediliyor.

00:06:50.260 --> 00:06:52.260
Ve öyle görülüyorki bu fiziki sosyal ifade biçimi

00:06:52.260 --> 00:06:54.260
gerçekten bir fark yaratıyor.

00:06:54.260 --> 00:06:57.260
Şimdi bunu bir kontekste oturtalım.

00:06:57.260 --> 00:07:00.260
Günümüzde aileler birbirinden çok uzaklarda yaşıyorlar,

00:07:00.260 --> 00:07:02.260
ve bu aile ilişkilerine ve birbirinden

00:07:02.260 --> 00:07:04.260
uzak aile bağlarına zarar veriyor.

00:07:04.260 --> 00:07:06.260
Bana gelince, benim üç oğlum var

00:07:06.260 --> 00:07:08.260
ve ben onların büyükanne ve büyükbabalarıyla iyi

00:07:08.260 --> 00:07:10.260
ilişkileri olmasını istiyorum.

00:07:10.260 --> 00:07:12.260
Ama annem ve babam bizden binlerce mil uzakta yaşıyor

00:07:12.260 --> 00:07:14.260
ve bu yüzden birbirlerini çok sık göremiyorlar.

00:07:14.260 --> 00:07:16.260
Skype'ı deniyoruz, telefonla konuşmayı deniyoruz,

00:07:16.260 --> 00:07:18.260
ama benim çocuklarım daha küçük -- konuşmak o kadar hoşlarına gitmiyor,

00:07:18.260 --> 00:07:20.260
oynamak istiyorlar.

00:07:20.260 --> 00:07:22.260
Robotları yeni bir uzaktan oynama teknolojisi

00:07:22.260 --> 00:07:25.260
olarak düşünmeyi seviyorlar.

00:07:25.260 --> 00:07:28.260
Şu andan çok uzak olmayan bir gelecek düşünüyorum --

00:07:28.260 --> 00:07:30.260
annem bilgisayarına gidiyor,

00:07:30.260 --> 00:07:32.260
internet tarayıcısını açıyor ve küçük bir robota bağlanıyor.

00:07:32.260 --> 00:07:35.260
Ve grandma-bot(Büyükanne-bot) olarak,

00:07:35.260 --> 00:07:37.260
o şimdi oynayabilir, ama gerçekten oynayabilir,

00:07:37.260 --> 00:07:39.260
benim çocuklarımla, onun torunlarıyla,

00:07:39.260 --> 00:07:42.260
gerçek Dünya'da gerçek oyuncaklarla.

00:07:42.260 --> 00:07:44.260
Büyükannelerin torunlarıyla, arakadaşlarıyla sosyal oyunlar

00:07:44.260 --> 00:07:46.260
oynayabildiğini ve evdeki başka hertürlü aktiviteyi,

00:07:46.260 --> 00:07:48.260
uyumadan önce anlatılan bir hikaye gibi,

00:07:48.260 --> 00:07:50.260
paylaşabildiğini hayal edebiliyorum.

00:07:50.260 --> 00:07:52.260
Ve bu teknoloji sayesinde,

00:07:52.260 --> 00:07:54.260
torunlarının hayatının

00:07:54.260 --> 00:07:56.260
aktiv bir parçası olabilecekler,

00:07:56.260 --> 00:07:58.260
bugün mümkün olmayan bir yöntemle.

00:07:58.260 --> 00:08:00.260
Başka alanları ele alalım,

00:08:00.260 --> 00:08:02.260
sağlık mesela.

00:08:02.260 --> 00:08:04.260
Bugün Amerika'da,

00:08:04.260 --> 00:08:07.260
insanların yüzde 65'i ya aşırı kilolu yada obez,

00:08:07.260 --> 00:08:09.260
ve bu şimdi çocuklarımız içinde büyük bir problem.

00:08:09.260 --> 00:08:11.260
Ve biliyoruzki, gençlikte obezlik

00:08:11.260 --> 00:08:14.260
ilerleyen yaşlarda kronik hastalıklara sebep olabiliyor,

00:08:14.260 --> 00:08:16.260
ki bu sadece hayat kalitemizi düşürmekle kalmıyor,

00:08:16.260 --> 00:08:19.260
aynı zamanda sağlık sistemimiz için de son derece ağır bir yük.

00:08:19.260 --> 00:08:21.260
Eğer robotlar insanlarda ilgi uyandırabilirlerse,

00:08:21.260 --> 00:08:23.260
eğer biz robotlarla işbirliği yapmak istersek,

00:08:23.260 --> 00:08:25.260
eğer robotlar ikna edici olabilirse,

00:08:25.260 --> 00:08:27.260
belki bir robot sana diyet yada

00:08:27.260 --> 00:08:29.260
egzersiz programına bağlı kalmanda,

00:08:29.260 --> 00:08:32.260
belkide kilonu korumanda yardımcı olabilir.

00:08:32.260 --> 00:08:34.260
Bir çeşit dijital Jiminiy --

00:08:34.260 --> 00:08:36.260
o çok ünlü peri masalındaki gibi,

00:08:36.260 --> 00:08:38.260
her zaman yanınızda olan sizi destekleyen bir arakadaş,

00:08:38.260 --> 00:08:40.260
size doğru kararı vermenizde yardımcı olan,

00:08:40.260 --> 00:08:42.260
doğru şekilde ve doğru zamanda,

00:08:42.260 --> 00:08:44.260
size doğru alışkanlıklar kazanmanıza yardımcı olan.

00:08:44.260 --> 00:08:46.260
Bu fikri aslında laboratuvarlarımızda keşfettik.

00:08:46.260 --> 00:08:48.260
Bu bir robot, Autom.

00:08:48.260 --> 00:08:51.260
Cory Kidd bu robotu doktora çalışması için geliştirdi.

00:08:51.260 --> 00:08:54.260
Ve o bir egzersiz ve diyet koçu olarak dizayn edildi.

00:08:54.260 --> 00:08:56.260
Yapabildiği söylevsel olmayan birkaç yeteneği vardı.

00:08:56.260 --> 00:08:58.260
Sizinle göz kontağı kurabilirdi.

00:08:58.260 --> 00:09:00.260
Bir ekrana bakarak bilgiyi paylaşabilirdi.

00:09:00.260 --> 00:09:02.260
Bir ekranı kullanarak bugün kaç kalori yediğiniz,

00:09:02.260 --> 00:09:04.260
ne kadar egzersiz yaptığınız

00:09:04.260 --> 00:09:06.260
bilgisini girebilirdiniz.

00:09:06.260 --> 00:09:08.260
Ve o bunu takip etmenizde size yardımcı olurdu.

00:09:08.260 --> 00:09:10.260
Robot sentetik bir sesle konuştu,

00:09:10.260 --> 00:09:12.260
sizi antrenör ve hastalar örnek alınarak

00:09:12.260 --> 00:09:14.260
hazırlanan bir diyaloğa

00:09:14.260 --> 00:09:16.260
dahil etmek için.

00:09:16.260 --> 00:09:18.260
O sizinle bu diyalog yoluyla bir işbirliği

00:09:18.260 --> 00:09:20.260
kurmaya çalışırdı.

00:09:20.260 --> 00:09:22.260
Size hedefler koymanızda ve ilerleyişinizi takip etmenizde

00:09:22.260 --> 00:09:24.260
yardımcı olabilir, sizi motive edebilirdi.

00:09:24.260 --> 00:09:26.260
Şöyle bir soru akla geliyor,

00:09:26.260 --> 00:09:29.260
sosyal kimlik gerçekten önemli mi? Onun bir robot olması birşeyi değiştirir mi?

00:09:29.260 --> 00:09:32.260
Önemli olan sadece verilen bilgi ve tavsiyenin kalitesi mi?

00:09:32.260 --> 00:09:34.260
Bu soruyu yanıtlamak için,

00:09:34.260 --> 00:09:36.260
Boston civarında bir araştırma yaptık,

00:09:36.260 --> 00:09:39.260
birkaç haftalığına insanların evlerine üç faklı

00:09:39.260 --> 00:09:41.260
müdahilden birini yerleştirdik.

00:09:41.260 --> 00:09:44.260
Bunlardan ilki şurda gördüğünüz robottu.

00:09:44.260 --> 00:09:47.260
Diğeri ise aynı dokunmatik ekranı ve aynı diyalogları kullanan

00:09:47.260 --> 00:09:49.260
bir bilgisayardı.

00:09:49.260 --> 00:09:51.260
Verilen tavsiyelerin kalitesi aynıydı.

00:09:51.260 --> 00:09:53.260
Ve üçüncüsü sadece bir kalem ve kağıttan ibaretti

00:09:53.260 --> 00:09:55.260
çünkü bir diyet yada egzersiz programına başladığında,

00:09:55.260 --> 00:09:58.260
normalde takip için kullanacağın yöntem budur.

00:09:58.260 --> 00:10:01.260
Sonuç olarak görmek istediğimiz şeylerden bir tanesi

00:10:01.260 --> 00:10:04.260
insanların ne kadar kilo kaybettiği değil

00:10:04.260 --> 00:10:07.260
robotlarla ne kadar etkileşim içinde olduklarıydı.

00:10:07.260 --> 00:10:10.260
Aslında mesele kilo vermek değil, mesele kilo almamak.

00:10:10.260 --> 00:10:13.260
Ve ne kadar uzun süre bu kontrol yöntemlerinden birini kullanırsan

00:10:13.260 --> 00:10:16.260
potansiyel olarak daha uzun vadeli bir başarıya ulaşılabilir.

00:10:16.260 --> 00:10:18.260
İlk olarak bakmak istediğim şey,

00:10:18.260 --> 00:10:20.260
insanların bu sistemlerle ne kadar uzun süre etkileşim içinde olduğu.

00:10:20.260 --> 00:10:22.260
İnsanlar robotlarla çok daha fazla

00:10:22.260 --> 00:10:24.260
etkileşim içindeler,

00:10:24.260 --> 00:10:27.260
yapılan tavsiyelerin kalitesinin bilgisayarlarla aynı olmasına rağmen.

00:10:28.260 --> 00:10:31.260
İnsanlardan farklı sistemlerle yapılan bu işbirliğinin kalitesinin değerlendirilmesi istenildiğinde,

00:10:31.260 --> 00:10:33.260
insanlar robotlara daha fazla değer verdiler

00:10:33.260 --> 00:10:35.260
ve robotlara daha fazla güvendiler.

00:10:35.260 --> 00:10:37.260
(Gülüşmeler)

00:10:37.260 --> 00:10:39.260
Ve kurulan duygusal bağı incelersek,

00:10:39.260 --> 00:10:41.260
bu çok farklıydı.

00:10:41.260 --> 00:10:43.260
İnsanlar robotlara isimler verdiler.

00:10:43.260 --> 00:10:45.260
Robotları giydirdiler.

00:10:45.260 --> 00:10:47.260
(Gülüşmeler)

00:10:47.260 --> 00:10:50.260
Ve hatta bu araştırmanın sonunda robotları almaya geldiğimizde,

00:10:50.260 --> 00:10:52.260
insanlar arabanın yanına kadar gelip robotlarla vedalaştılar.

00:10:52.260 --> 00:10:54.260
Bunu bir bilgisayarla yapmadılar.

00:10:54.260 --> 00:10:56.260
Bu gün hakkında konuşmak istediğim son şey,

00:10:56.260 --> 00:10:58.260
çocuk medyasının geleceği.

00:10:58.260 --> 00:11:01.260
Bildiğimiz gibi çocuklar ekranın karşısında çok fazla zaman geçiriyorlar,

00:11:01.260 --> 00:11:04.260
bu televizyon, bilgisayar oyunları yada herhangi başka birşey olabilir.

00:11:04.260 --> 00:11:07.260
Çocuklarım ekranı çok seviyorlar.

00:11:07.260 --> 00:11:10.260
Ama ben onların oynamalarını istiyorum; bir anne olarak onların

00:11:10.260 --> 00:11:12.260
gerçek Dünyada'ki oyunları oynamalarını istiyorum.

00:11:12.260 --> 00:11:15.260
Bugün size grubumda başlattığım yeni bir projemi sunmak istiyorum,

00:11:15.260 --> 00:11:17.260
adı Playtime Computing olan bu proje

00:11:17.260 --> 00:11:19.260
dijital medyanın neden bu kadar çekici

00:11:19.260 --> 00:11:21.260
olduğu konusunda düşünmeye çalışıyor

00:11:21.260 --> 00:11:23.260
ve kelimenin tam anlamıyla bunu ekranın dışına taşımaya çalışıyor,

00:11:23.260 --> 00:11:25.260
çocukların gerçek Dünya'sına,

00:11:25.260 --> 00:11:28.260
gerçek Dünya'daki oyunların birçok özelliğini alabileceği yere.

00:11:29.260 --> 00:11:33.260
Bu fikrin ilk araştırması şu oldu,

00:11:33.260 --> 00:11:36.260
karakterler gerçek yada sanal olabilir,

00:11:36.260 --> 00:11:38.260
ve dijital içerik

00:11:38.260 --> 00:11:40.260
ekranın dışına taşınabilir,

00:11:40.260 --> 00:11:42.260
gerçek Dünya'ya ve tekrar sanal ortama.

00:11:42.260 --> 00:11:44.260
Ben bunu karmaşık

00:11:44.260 --> 00:11:46.260
bir Atari oyunu gibi

00:11:46.260 --> 00:11:48.260
düşünmeyi seviyorum.

00:11:48.260 --> 00:11:50.260
Bu fikri daha da geliştirbiliriz.

00:11:50.260 --> 00:11:52.260
Eğer --

00:11:52.260 --> 00:11:55.260
(Oyun) Nathan: İşte o geliyor. Yay!

00:11:55.260 --> 00:11:58.260
CB: -- karakterin kendisi sizin Dünya'nıza gelebiliseydi?

00:11:58.260 --> 00:12:00.260
Görülüyorki karakterlerin gerçek olup

00:12:00.260 --> 00:12:03.260
onların Dünya'sına girmesi çocukların hoşuna gidiyor.

00:12:03.260 --> 00:12:05.260
Ve onların Dünya'sındayken,

00:12:05.260 --> 00:12:07.260
çocuklar onlarla ekranda olduğundan çok farklı bir

00:12:07.260 --> 00:12:09.260
şekilde oynayabilir ve onlarla bir ilişki kurabilir.

00:12:09.260 --> 00:12:11.260
Önemli başka bir nokta ise

00:12:11.260 --> 00:12:14.260
karakterin sanal ve gerçek Dünya arasındaki devamlılığı.

00:12:14.260 --> 00:12:16.260
Çocukların gerçek Dünya'da yaptığı değişiklikler

00:12:16.260 --> 00:12:18.260
sanal Dünya'ya da yansıtılmalı.

00:12:18.260 --> 00:12:21.260
Burda Nathan A harfini 2 sayısına çevirdi.

00:12:21.260 --> 00:12:23.260
Bu sembollerin karakterlere

00:12:23.260 --> 00:12:26.260
sanal Dünya'da farklı güçler verdiğini düşünebilirsiniz.

00:12:26.260 --> 00:12:29.260
Şimdi karakteri gerçek Dünya'ya gönderiyorlar.

00:12:29.260 --> 00:12:32.260
Ve şimdi karakterin sayı gücü var.

00:12:32.260 --> 00:12:34.260
Sonuç olarak, burda yapmaya çalıştığım

00:12:34.260 --> 00:12:37.260
çocuklar için tamamen gerçekci bir deneyim yaratmak,

00:12:37.260 --> 00:12:40.260
onların bu hikayenin bir parçası olduklarını hissettikleri

00:12:40.260 --> 00:12:42.260
bir deneyim.

00:12:42.260 --> 00:12:44.260
Çocukların hayal güçlerini harekete geçirmek istiyorum,

00:12:44.260 --> 00:12:47.260
ben küçük bir kızken "Star Wars" izlediğimde olduğu gibi.

00:12:47.260 --> 00:12:49.260
Ama ben bundan fazlasını yapmak istiyorum.

00:12:49.260 --> 00:12:52.260
Ben onların kendilerinin bu denyimi yaratmalarını istiyorum.

00:12:52.260 --> 00:12:54.260
Onların hayal ettiklerini bu deneyimlere yansıtmalarını

00:12:54.260 --> 00:12:56.260
ve bu deneyimleri kendilerinin yapmalarını istiyorum.

00:12:56.260 --> 00:12:58.260
Teleprezens ve karışık realiteyle ilgili farklı

00:12:58.260 --> 00:13:00.260
fikirleri ele alarak çocukların düşüncelerini bu

00:13:00.260 --> 00:13:03.260
alana nasıl yansıtacağını ve diğer çocukların bu fikirlerle

00:13:03.260 --> 00:13:05.260
etkileşim içinde olup bu fikirleri nasıl

00:13:05.260 --> 00:13:07.260
geliştireceğini araştırdık.

00:13:07.260 --> 00:13:10.260
Çocuk medyasında çocukların yaratıcılıklarını, öğrenmelerini

00:13:10.260 --> 00:13:13.260
ve yeni fikirleri teşvik edecek yeni yöntemler bulmak istiyorum.

00:13:13.260 --> 00:13:16.260
Bunun çok çok önemli olduğunu düşünüyorum.

00:13:16.260 --> 00:13:18.260
Bu yeni bir proje.

00:13:18.260 --> 00:13:20.260
Çocukları buraya davet ettik,

00:13:20.260 --> 00:13:23.260
onlar bunu mükemmel buldu.

00:13:23.260 --> 00:13:25.260
Şunu söyleyebilirim, çocukların en çok sevdiği şey

00:13:25.260 --> 00:13:27.260
robot.

00:13:27.260 --> 00:13:30.260
Önem verdikleri şey robot.

00:13:30.260 --> 00:13:33.260
Robotlar içimizdeki derin insani duyguya dokunuyorlar.

00:13:33.260 --> 00:13:35.260
İster bize yaratıcı ve yenilikçi olmamızda

00:13:35.260 --> 00:13:37.260
yardımcı olmaları,

00:13:37.260 --> 00:13:39.260
ister bize uzaklara rağmen derinden

00:13:39.260 --> 00:13:41.260
bağlılık hissetmemizde yardım etmeleri,

00:13:41.260 --> 00:13:43.260
yada bizim güvenilir destekçimiz olarak

00:13:43.260 --> 00:13:45.260
bize olabileceğimizin en iyisi olmak için koyduğumuz

00:13:45.260 --> 00:13:47.260
hedeflerimize ulaşmamızda yardımcı olmaları,

00:13:47.260 --> 00:13:50.260
benim için, robotlar tamamen insanlarla ilgili.

00:13:50.260 --> 00:13:52.260
Teşekkür ederim.

00:13:52.260 --> 00:13:57.260
(Alkışlar)

