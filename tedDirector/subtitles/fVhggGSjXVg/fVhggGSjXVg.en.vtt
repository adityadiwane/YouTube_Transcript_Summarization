WEBVTT
Kind: captions
Language: en

00:00:16.260 --> 00:00:18.260
Up until now, our communication with machines

00:00:18.260 --> 00:00:20.260
has always been limited

00:00:20.260 --> 00:00:22.260
to conscious and direct forms.

00:00:22.260 --> 00:00:24.260
Whether it's something simple

00:00:24.260 --> 00:00:26.260
like turning on the lights with a switch,

00:00:26.260 --> 00:00:29.260
or even as complex as programming robotics,

00:00:29.260 --> 00:00:32.260
we have always had to give a command to a machine,

00:00:32.260 --> 00:00:34.260
or even a series of commands,

00:00:34.260 --> 00:00:37.260
in order for it to do something for us.

00:00:37.260 --> 00:00:39.260
Communication between people, on the other hand,

00:00:39.260 --> 00:00:42.260
is far more complex and a lot more interesting

00:00:42.260 --> 00:00:44.260
because we take into account

00:00:44.260 --> 00:00:47.260
so much more than what is explicitly expressed.

00:00:47.260 --> 00:00:50.260
We observe facial expressions, body language,

00:00:50.260 --> 00:00:52.260
and we can intuit feelings and emotions

00:00:52.260 --> 00:00:55.260
from our dialogue with one another.

00:00:55.260 --> 00:00:57.260
This actually forms a large part

00:00:57.260 --> 00:00:59.260
of our decision-making process.

00:00:59.260 --> 00:01:01.260
Our vision is to introduce

00:01:01.260 --> 00:01:04.260
this whole new realm of human interaction

00:01:04.260 --> 00:01:06.260
into human-computer interaction

00:01:06.260 --> 00:01:08.260
so that computers can understand

00:01:08.260 --> 00:01:10.260
not only what you direct it to do,

00:01:10.260 --> 00:01:12.260
but it can also respond

00:01:12.260 --> 00:01:14.260
to your facial expressions

00:01:14.260 --> 00:01:16.260
and emotional experiences.

00:01:16.260 --> 00:01:18.260
And what better way to do this

00:01:18.260 --> 00:01:20.260
than by interpreting the signals

00:01:20.260 --> 00:01:22.260
naturally produced by our brain,

00:01:22.260 --> 00:01:25.260
our center for control and experience.

00:01:25.260 --> 00:01:27.260
Well, it sounds like a pretty good idea,

00:01:27.260 --> 00:01:29.260
but this task, as Bruno mentioned,

00:01:29.260 --> 00:01:32.260
isn't an easy one for two main reasons:

00:01:32.260 --> 00:01:35.260
First, the detection algorithms.

00:01:35.260 --> 00:01:37.260
Our brain is made up of

00:01:37.260 --> 00:01:39.260
billions of active neurons,

00:01:39.260 --> 00:01:42.260
around 170,000 km

00:01:42.260 --> 00:01:44.260
of combined axon length.

00:01:44.260 --> 00:01:46.260
When these neurons interact,

00:01:46.260 --> 00:01:48.260
the chemical reaction emits an electrical impulse,

00:01:48.260 --> 00:01:50.260
which can be measured.

00:01:50.260 --> 00:01:53.260
The majority of our functional brain

00:01:53.260 --> 00:01:55.260
is distributed over

00:01:55.260 --> 00:01:57.260
the outer surface layer of the brain,

00:01:57.260 --> 00:02:00.260
and to increase the area that's available for mental capacity,

00:02:00.260 --> 00:02:03.260
the brain surface is highly folded.

00:02:03.260 --> 00:02:05.260
Now this cortical folding

00:02:05.260 --> 00:02:07.260
presents a significant challenge

00:02:07.260 --> 00:02:10.260
for interpreting surface electrical impulses.

00:02:10.260 --> 00:02:12.260
Each individual's cortex

00:02:12.260 --> 00:02:14.260
is folded differently,

00:02:14.260 --> 00:02:16.260
very much like a fingerprint.

00:02:16.260 --> 00:02:18.260
So even though a signal

00:02:18.260 --> 00:02:21.260
may come from the same functional part of the brain,

00:02:21.260 --> 00:02:23.260
by the time the structure has been folded,

00:02:23.260 --> 00:02:25.260
its physical location

00:02:25.260 --> 00:02:27.260
is very different between individuals,

00:02:27.260 --> 00:02:30.260
even identical twins.

00:02:30.260 --> 00:02:32.260
There is no longer any consistency

00:02:32.260 --> 00:02:34.260
in the surface signals.

00:02:34.260 --> 00:02:36.260
Our breakthrough was to create an algorithm

00:02:36.260 --> 00:02:38.260
that unfolds the cortex,

00:02:38.260 --> 00:02:40.260
so that we can map the signals

00:02:40.260 --> 00:02:42.260
closer to its source,

00:02:42.260 --> 00:02:45.260
and therefore making it capable of working across a mass population.

00:02:46.260 --> 00:02:48.260
The second challenge

00:02:48.260 --> 00:02:51.260
is the actual device for observing brainwaves.

00:02:51.260 --> 00:02:53.260
EEG measurements typically involve

00:02:53.260 --> 00:02:56.260
a hairnet with an array of sensors,

00:02:56.260 --> 00:02:59.260
like the one that you can see here in the photo.

00:02:59.260 --> 00:03:01.260
A technician will put the electrodes

00:03:01.260 --> 00:03:03.260
onto the scalp

00:03:03.260 --> 00:03:05.260
using a conductive gel or paste

00:03:05.260 --> 00:03:08.260
and usually after a procedure of preparing the scalp

00:03:08.260 --> 00:03:10.260
by light abrasion.

00:03:10.260 --> 00:03:12.260
Now this is quite time consuming

00:03:12.260 --> 00:03:14.260
and isn't the most comfortable process.

00:03:14.260 --> 00:03:16.260
And on top of that, these systems

00:03:16.260 --> 00:03:19.260
actually cost in the tens of thousands of dollars.

00:03:20.260 --> 00:03:23.260
So with that, I'd like to invite onstage

00:03:23.260 --> 00:03:25.260
Evan Grant, who is one of last year's speakers,

00:03:25.260 --> 00:03:27.260
who's kindly agreed

00:03:27.260 --> 00:03:29.260
to help me to demonstrate

00:03:29.260 --> 00:03:31.260
what we've been able to develop.

00:03:31.260 --> 00:03:37.260
(Applause)

00:03:37.260 --> 00:03:39.260
So the device that you see

00:03:39.260 --> 00:03:41.260
is a 14-channel, high-fidelity

00:03:41.260 --> 00:03:43.260
EEG acquisition system.

00:03:43.260 --> 00:03:46.260
It doesn't require any scalp preparation,

00:03:46.260 --> 00:03:48.260
no conductive gel or paste.

00:03:48.260 --> 00:03:51.260
It only takes a few minutes to put on

00:03:51.260 --> 00:03:53.260
and for the signals to settle.

00:03:53.260 --> 00:03:55.260
It's also wireless,

00:03:55.260 --> 00:03:58.260
so it gives you the freedom to move around.

00:03:58.260 --> 00:04:01.260
And compared to the tens of thousands of dollars

00:04:01.260 --> 00:04:04.260
for a traditional EEG system,

00:04:04.260 --> 00:04:06.260
this headset only costs

00:04:06.260 --> 00:04:08.260
a few hundred dollars.

00:04:08.260 --> 00:04:11.260
Now on to the detection algorithms.

00:04:11.260 --> 00:04:13.260
So facial expressions --

00:04:13.260 --> 00:04:15.260
as I mentioned before in emotional experiences --

00:04:15.260 --> 00:04:17.260
are actually designed to work out of the box

00:04:17.260 --> 00:04:19.260
with some sensitivity adjustments

00:04:19.260 --> 00:04:22.260
available for personalization.

00:04:22.260 --> 00:04:24.260
But with the limited time we have available,

00:04:24.260 --> 00:04:26.260
I'd like to show you the cognitive suite,

00:04:26.260 --> 00:04:28.260
which is the ability for you

00:04:28.260 --> 00:04:31.260
to basically move virtual objects with your mind.

00:04:32.260 --> 00:04:34.260
Now, Evan is new to this system,

00:04:34.260 --> 00:04:36.260
so what we have to do first

00:04:36.260 --> 00:04:38.260
is create a new profile for him.

00:04:38.260 --> 00:04:41.260
He's obviously not Joanne -- so we'll "add user."

00:04:41.260 --> 00:04:43.260
Evan. Okay.

00:04:43.260 --> 00:04:46.260
So the first thing we need to do with the cognitive suite

00:04:46.260 --> 00:04:48.260
is to start with training

00:04:48.260 --> 00:04:50.260
a neutral signal.

00:04:50.260 --> 00:04:52.260
With neutral, there's nothing in particular

00:04:52.260 --> 00:04:54.260
that Evan needs to do.

00:04:54.260 --> 00:04:56.260
He just hangs out. He's relaxed.

00:04:56.260 --> 00:04:58.260
And the idea is to establish a baseline

00:04:58.260 --> 00:05:00.260
or normal state for his brain,

00:05:00.260 --> 00:05:02.260
because every brain is different.

00:05:02.260 --> 00:05:04.260
It takes eight seconds to do this,

00:05:04.260 --> 00:05:06.260
and now that that's done,

00:05:06.260 --> 00:05:08.260
we can choose a movement-based action.

00:05:08.260 --> 00:05:10.260
So Evan, choose something

00:05:10.260 --> 00:05:12.260
that you can visualize clearly in your mind.

00:05:12.260 --> 00:05:14.260
Evan Grant: Let's do "pull."

00:05:14.260 --> 00:05:16.260
Tan Le: Okay, so let's choose "pull."

00:05:16.260 --> 00:05:18.260
So the idea here now

00:05:18.260 --> 00:05:20.260
is that Evan needs to

00:05:20.260 --> 00:05:22.260
imagine the object coming forward

00:05:22.260 --> 00:05:24.260
into the screen,

00:05:24.260 --> 00:05:27.260
and there's a progress bar that will scroll across the screen

00:05:27.260 --> 00:05:29.260
while he's doing that.

00:05:29.260 --> 00:05:31.260
The first time, nothing will happen,

00:05:31.260 --> 00:05:34.260
because the system has no idea how he thinks about "pull."

00:05:34.260 --> 00:05:36.260
But maintain that thought

00:05:36.260 --> 00:05:38.260
for the entire duration of the eight seconds.

00:05:38.260 --> 00:05:41.260
So: one, two, three, go.

00:05:49.260 --> 00:05:51.260
Okay.

00:05:51.260 --> 00:05:53.260
So once we accept this,

00:05:53.260 --> 00:05:55.260
the cube is live.

00:05:55.260 --> 00:05:57.260
So let's see if Evan

00:05:57.260 --> 00:06:00.260
can actually try and imagine pulling.

00:06:00.260 --> 00:06:02.260
Ah, good job!

00:06:02.260 --> 00:06:05.260
(Applause)

00:06:05.260 --> 00:06:07.260
That's really amazing.

00:06:07.260 --> 00:06:11.260
(Applause)

00:06:11.260 --> 00:06:13.260
So we have a little bit of time available,

00:06:13.260 --> 00:06:15.260
so I'm going to ask Evan

00:06:15.260 --> 00:06:17.260
to do a really difficult task.

00:06:17.260 --> 00:06:19.260
And this one is difficult

00:06:19.260 --> 00:06:22.260
because it's all about being able to visualize something

00:06:22.260 --> 00:06:24.260
that doesn't exist in our physical world.

00:06:24.260 --> 00:06:26.260
This is "disappear."

00:06:26.260 --> 00:06:28.260
So what you want to do -- at least with movement-based actions,

00:06:28.260 --> 00:06:31.260
we do that all the time, so you can visualize it.

00:06:31.260 --> 00:06:33.260
But with "disappear," there's really no analogies --

00:06:33.260 --> 00:06:35.260
so Evan, what you want to do here

00:06:35.260 --> 00:06:38.260
is to imagine the cube slowly fading out, okay.

00:06:38.260 --> 00:06:41.260
Same sort of drill. So: one, two, three, go.

00:06:50.260 --> 00:06:53.260
Okay. Let's try that.

00:06:53.260 --> 00:06:56.260
Oh, my goodness. He's just too good.

00:06:57.260 --> 00:06:59.260
Let's try that again.

00:07:04.260 --> 00:07:06.260
EG: Losing concentration.

00:07:06.260 --> 00:07:08.260
(Laughter)

00:07:08.260 --> 00:07:10.260
TL: But we can see that it actually works,

00:07:10.260 --> 00:07:12.260
even though you can only hold it

00:07:12.260 --> 00:07:14.260
for a little bit of time.

00:07:14.260 --> 00:07:17.260
As I said, it's a very difficult process

00:07:17.260 --> 00:07:19.260
to imagine this.

00:07:19.260 --> 00:07:21.260
And the great thing about it is that

00:07:21.260 --> 00:07:23.260
we've only given the software one instance

00:07:23.260 --> 00:07:26.260
of how he thinks about "disappear."

00:07:26.260 --> 00:07:29.260
As there is a machine learning algorithm in this --

00:07:29.260 --> 00:07:33.260
(Applause)

00:07:33.260 --> 00:07:35.260
Thank you.

00:07:35.260 --> 00:07:38.260
Good job. Good job.

00:07:38.260 --> 00:07:40.260
(Applause)

00:07:40.260 --> 00:07:43.260
Thank you, Evan, you're a wonderful, wonderful

00:07:43.260 --> 00:07:46.260
example of the technology.

00:07:46.260 --> 00:07:48.260
So, as you can see, before,

00:07:48.260 --> 00:07:51.260
there is a leveling system built into this software

00:07:51.260 --> 00:07:53.260
so that as Evan, or any user,

00:07:53.260 --> 00:07:55.260
becomes more familiar with the system,

00:07:55.260 --> 00:07:58.260
they can continue to add more and more detections,

00:07:58.260 --> 00:08:00.260
so that the system begins to differentiate

00:08:00.260 --> 00:08:03.260
between different distinct thoughts.

00:08:04.260 --> 00:08:06.260
And once you've trained up the detections,

00:08:06.260 --> 00:08:08.260
these thoughts can be assigned or mapped

00:08:08.260 --> 00:08:10.260
to any computing platform,

00:08:10.260 --> 00:08:12.260
application or device.

00:08:12.260 --> 00:08:14.260
So I'd like to show you a few examples,

00:08:14.260 --> 00:08:16.260
because there are many possible applications

00:08:16.260 --> 00:08:18.260
for this new interface.

00:08:19.260 --> 00:08:21.260
In games and virtual worlds, for example,

00:08:21.260 --> 00:08:23.260
your facial expressions

00:08:23.260 --> 00:08:25.260
can naturally and intuitively be used

00:08:25.260 --> 00:08:28.260
to control an avatar or virtual character.

00:08:29.260 --> 00:08:31.260
Obviously, you can experience the fantasy of magic

00:08:31.260 --> 00:08:34.260
and control the world with your mind.

00:08:36.260 --> 00:08:39.260
And also, colors, lighting,

00:08:39.260 --> 00:08:41.260
sound and effects

00:08:41.260 --> 00:08:43.260
can dynamically respond to your emotional state

00:08:43.260 --> 00:08:46.260
to heighten the experience that you're having, in real time.

00:08:47.260 --> 00:08:49.260
And moving on to some applications

00:08:49.260 --> 00:08:52.260
developed by developers and researchers around the world,

00:08:52.260 --> 00:08:55.260
with robots and simple machines, for example --

00:08:55.260 --> 00:08:57.260
in this case, flying a toy helicopter

00:08:57.260 --> 00:09:00.260
simply by thinking "lift" with your mind.

00:09:00.260 --> 00:09:02.260
The technology can also be applied

00:09:02.260 --> 00:09:04.260
to real world applications --

00:09:04.260 --> 00:09:06.260
in this example, a smart home.

00:09:06.260 --> 00:09:09.260
You know, from the user interface of the control system

00:09:09.260 --> 00:09:11.260
to opening curtains

00:09:11.260 --> 00:09:14.260
or closing curtains.

00:09:22.260 --> 00:09:25.260
And of course, also to the lighting --

00:09:25.260 --> 00:09:28.260
turning them on

00:09:28.260 --> 00:09:30.260
or off.

00:09:30.260 --> 00:09:32.260
And finally,

00:09:32.260 --> 00:09:34.260
to real life-changing applications,

00:09:34.260 --> 00:09:37.260
such as being able to control an electric wheelchair.

00:09:37.260 --> 00:09:39.260
In this example,

00:09:39.260 --> 00:09:42.260
facial expressions are mapped to the movement commands.

00:09:42.260 --> 00:09:45.260
Man: Now blink right to go right.

00:09:50.260 --> 00:09:53.260
Now blink left to turn back left.

00:10:02.260 --> 00:10:05.260
Now smile to go straight.

00:10:08.260 --> 00:10:10.260
TL: We really -- Thank you.

00:10:10.260 --> 00:10:15.260
(Applause)

00:10:15.260 --> 00:10:18.260
We are really only scratching the surface of what is possible today,

00:10:18.260 --> 00:10:20.260
and with the community's input,

00:10:20.260 --> 00:10:22.260
and also with the involvement of developers

00:10:22.260 --> 00:10:25.260
and researchers from around the world,

00:10:25.260 --> 00:10:27.260
we hope that you can help us to shape

00:10:27.260 --> 00:10:30.260
where the technology goes from here. Thank you so much.

