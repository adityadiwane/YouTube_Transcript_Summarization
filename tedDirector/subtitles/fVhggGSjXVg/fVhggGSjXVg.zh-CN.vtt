WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:07.000
翻译人员: Halei Liu
校对人员: Xu Jiang

00:00:16.260 --> 00:00:18.260
直到现在，我们和机器的沟通的方式

00:00:18.260 --> 00:00:20.260
都被限制在

00:00:20.260 --> 00:00:22.260
一种有意识和直接的形式上。

00:00:22.260 --> 00:00:24.260
无论是简单得像

00:00:24.260 --> 00:00:26.260
打开灯的开关那样，

00:00:26.260 --> 00:00:29.260
还是复杂得像编写一个机器人那样，

00:00:29.260 --> 00:00:32.260
我们必须给机器一段指令，

00:00:32.260 --> 00:00:34.260
或一系列的指令，

00:00:34.260 --> 00:00:37.260
才能让机器为我们做我们想做的事情。

00:00:37.260 --> 00:00:39.260
而人和人之间的交流

00:00:39.260 --> 00:00:42.260
就远远要复杂和有趣些，

00:00:42.260 --> 00:00:44.260
因为我们接受的

00:00:44.260 --> 00:00:47.260
远比外露的表现要多。

00:00:47.260 --> 00:00:50.260
我们观察脸部表情，肢体语言，

00:00:50.260 --> 00:00:52.260
我们可以通过对话

00:00:52.260 --> 00:00:55.260
感受到感觉和情感。

00:00:55.260 --> 00:00:57.260
这些其实都是我们做决定的过程中的

00:00:57.260 --> 00:00:59.260
一大部分。

00:00:59.260 --> 00:01:01.260
我们的视野是向人机互动科技

00:01:01.260 --> 00:01:04.260
介绍一个崭新的人类互动的

00:01:04.260 --> 00:01:06.260
新领域，

00:01:06.260 --> 00:01:08.260
这样的话，计算机就不仅仅

00:01:08.260 --> 00:01:10.260
按照你的指令工作，

00:01:10.260 --> 00:01:12.260
也能够根据你脸部表情

00:01:12.260 --> 00:01:14.260
和感情

00:01:14.260 --> 00:01:16.260
做出反应。

00:01:16.260 --> 00:01:18.260
如果要这样做的话

00:01:18.260 --> 00:01:20.260
还有什么能比得上

00:01:20.260 --> 00:01:22.260
去理解我们脑部所发出的电波，

00:01:22.260 --> 00:01:25.260
我们的控制和体验中心。

00:01:25.260 --> 00:01:27.260
好，这听起来是一个十分好的主意，

00:01:27.260 --> 00:01:29.260
但是这项任务，就像Burno提过的，

00:01:29.260 --> 00:01:32.260
并不是那么简单，这主要有两个原因：

00:01:32.260 --> 00:01:35.260
第一，检测的算法。

00:01:35.260 --> 00:01:37.260
我们的大脑是由上亿个

00:01:37.260 --> 00:01:39.260
活跃的脑神经所组成，

00:01:39.260 --> 00:01:42.260
轴突的长度总共有

00:01:42.260 --> 00:01:44.260
170,000 千米。

00:01:44.260 --> 00:01:46.260
当这些脑神经互动时，

00:01:46.260 --> 00:01:48.260
所产生的化学反应放出的电脉冲

00:01:48.260 --> 00:01:50.260
是可以被测量的。

00:01:50.260 --> 00:01:53.260
我们大脑的主要功能

00:01:53.260 --> 00:01:55.260
是分布在大脑

00:01:55.260 --> 00:01:57.260
的外表面层。

00:01:57.260 --> 00:02:00.260
从心理的能力来说要去增加这个区域是可能的，

00:02:00.260 --> 00:02:03.260
大脑表面充满了褶皱。

00:02:03.260 --> 00:02:05.260
皮质折叠

00:02:05.260 --> 00:02:07.260
对于解析表面电脉冲来说

00:02:07.260 --> 00:02:10.260
是一个重大挑战。

00:02:10.260 --> 00:02:12.260
每一个单独的皮层

00:02:12.260 --> 00:02:14.260
其折叠的结构都是有区别的，

00:02:14.260 --> 00:02:16.260
就像我们的指纹一样。

00:02:16.260 --> 00:02:18.260
就算一个信号

00:02:18.260 --> 00:02:21.260
可能来自于大脑同一功能的部分，

00:02:21.260 --> 00:02:23.260
就在这个结构被折叠的时间里，

00:02:23.260 --> 00:02:25.260
它的实际位置

00:02:25.260 --> 00:02:27.260
是十分不同的，

00:02:27.260 --> 00:02:30.260
就算是双胞胎也一样。

00:02:30.260 --> 00:02:32.260
表层的信号

00:02:32.260 --> 00:02:34.260
没有什么持续性。

00:02:34.260 --> 00:02:36.260
我们的突破是创造一种计算方法

00:02:36.260 --> 00:02:38.260
可以展开皮层，

00:02:38.260 --> 00:02:40.260
这样我们可以在更靠近源头的地方

00:02:40.260 --> 00:02:42.260
来接受信号，

00:02:42.260 --> 00:02:45.260
从而就可以在更广泛的人群中使用。

00:02:46.260 --> 00:02:48.260
第二个挑战是

00:02:48.260 --> 00:02:51.260
观测脑部的实际装置。

00:02:51.260 --> 00:02:53.260
脑电图一般是

00:02:53.260 --> 00:02:56.260
一系列传感器的阵列，

00:02:56.260 --> 00:02:59.260
就和你们在照片上所看到的一样。

00:02:59.260 --> 00:03:01.260
技术人员使用导电胶或粘贴

00:03:01.260 --> 00:03:03.260
将电极

00:03:03.260 --> 00:03:05.260
放到头皮上

00:03:05.260 --> 00:03:08.260
这通常会有一个光磨损的过程

00:03:08.260 --> 00:03:10.260
来准备头皮。

00:03:10.260 --> 00:03:12.260
这十分耗时

00:03:12.260 --> 00:03:14.260
而且过程也不舒适。

00:03:14.260 --> 00:03:16.260
加上，这个系统

00:03:16.260 --> 00:03:19.260
要花费上百万美元。

00:03:20.260 --> 00:03:23.260
我现在想邀请我们去年的

00:03:23.260 --> 00:03:25.260
一位演讲者Evan Grant，上台来。

00:03:25.260 --> 00:03:27.260
他很客气的同意了

00:03:27.260 --> 00:03:29.260
来帮助我们来展示

00:03:29.260 --> 00:03:31.260
我们的研究发展。

00:03:31.260 --> 00:03:37.260
（掌声）

00:03:37.260 --> 00:03:39.260
这个装置如你们所看见的

00:03:39.260 --> 00:03:41.260
是一个14个通道，高保真

00:03:41.260 --> 00:03:43.260
脑电采集系统。

00:03:43.260 --> 00:03:46.260
不需要任何的头皮处理过程，

00:03:46.260 --> 00:03:48.260
不用导流胶或导流膏。

00:03:48.260 --> 00:03:51.260
只需要几分钟来固定好

00:03:51.260 --> 00:03:53.260
和稳定信号。

00:03:53.260 --> 00:03:55.260
这也是无线的，

00:03:55.260 --> 00:03:58.260
所以这样就可以自由的移动。

00:03:58.260 --> 00:04:01.260
和上百万美元的

00:04:01.260 --> 00:04:04.260
传统脑电图系统比

00:04:04.260 --> 00:04:06.260
这个装置只用

00:04:06.260 --> 00:04:08.260
几百美元。

00:04:08.260 --> 00:04:11.260
现在说说检测的算法。

00:04:11.260 --> 00:04:13.260
所以脸部的表情 －－

00:04:13.260 --> 00:04:15.260
就像我先前提到的感情表达一样 －－

00:04:15.260 --> 00:04:17.260
都是意想不到的

00:04:17.260 --> 00:04:19.260
通过一些敏感性的调整

00:04:19.260 --> 00:04:22.260
使之个性化。

00:04:22.260 --> 00:04:24.260
但是由于时间原因，

00:04:24.260 --> 00:04:26.260
我向你们介绍一套认知系统，

00:04:26.260 --> 00:04:28.260
系统所做的是

00:04:28.260 --> 00:04:31.260
让你用你的意念来移动物体。

00:04:32.260 --> 00:04:34.260
现在，Evan是第一次接触这个系统，

00:04:34.260 --> 00:04:36.260
所以我们要先为他

00:04:36.260 --> 00:04:38.260
创建新的个人信息。

00:04:38.260 --> 00:04:41.260
他显然不是Joanne －－所以我们选择“添加用户。”

00:04:41.260 --> 00:04:43.260
Evan。 搞定。

00:04:43.260 --> 00:04:46.260
我们先要做的是

00:04:46.260 --> 00:04:48.260
开始训练一个

00:04:48.260 --> 00:04:50.260
中和的信号。

00:04:50.260 --> 00:04:52.260
所谓中和，就是Evan不用

00:04:52.260 --> 00:04:54.260
做任何事情。

00:04:54.260 --> 00:04:56.260
他只是放松。

00:04:56.260 --> 00:04:58.260
这个过程会给他建立一个地基

00:04:58.260 --> 00:05:00.260
或是他大脑的普通模式，

00:05:00.260 --> 00:05:02.260
因为每一人的大脑都是不一样的。

00:05:02.260 --> 00:05:04.260
这大概需要8秒的时间。

00:05:04.260 --> 00:05:06.260
好，现在完成了，

00:05:06.260 --> 00:05:08.260
我们可以选择一个以移动为主的动作。

00:05:08.260 --> 00:05:10.260
所以Evan选中一个

00:05:10.260 --> 00:05:12.260
他可以在他大脑中现形的物体。

00:05:12.260 --> 00:05:14.260
Evan：让我们来作“拉近。”

00:05:14.260 --> 00:05:16.260
Tan：好，让我们选中“拉近。”

00:05:16.260 --> 00:05:18.260
现在的目标是

00:05:18.260 --> 00:05:20.260
Evan要去想象

00:05:20.260 --> 00:05:22.260
这个物体会向

00:05:22.260 --> 00:05:24.260
屏幕靠近。

00:05:24.260 --> 00:05:27.260
在他做的同时，屏幕上会显示

00:05:27.260 --> 00:05:29.260
一个进度条。

00:05:29.260 --> 00:05:31.260
第一次，什么都没有。

00:05:31.260 --> 00:05:34.260
因为系统不知到他所想的“拉近”是什么。

00:05:34.260 --> 00:05:36.260
但是持续这个想象

00:05:36.260 --> 00:05:38.260
程序8秒钟。

00:05:38.260 --> 00:05:41.260
所以，1，2，3，开始。

00:05:49.260 --> 00:05:51.260
好。

00:05:51.260 --> 00:05:53.260
一旦我们接受这个，

00:05:53.260 --> 00:05:55.260
方块就活起来了。

00:05:55.260 --> 00:05:57.260
现在我们看Evan

00:05:57.260 --> 00:06:00.260
能不能想象一下“拉近。”

00:06:00.260 --> 00:06:02.260
哦，干的好！

00:06:02.260 --> 00:06:05.260
（掌声）

00:06:05.260 --> 00:06:07.260
十分令人惊叹。

00:06:07.260 --> 00:06:11.260
（掌声）

00:06:11.260 --> 00:06:13.260
这样我们还有一点时间，

00:06:13.260 --> 00:06:15.260
所以我们让Evan

00:06:15.260 --> 00:06:17.260
做个难一点的任务。

00:06:17.260 --> 00:06:19.260
这个比较难

00:06:19.260 --> 00:06:22.260
因为这个是要想象

00:06:22.260 --> 00:06:24.260
一个不存在我们现实世界里的物体。

00:06:24.260 --> 00:06:26.260
这是“消失。”

00:06:26.260 --> 00:06:28.260
所以，你要做的 －－ 先做一个运动为主的动作，

00:06:28.260 --> 00:06:31.260
我们在现实中一直在做这个动作，所以我们可以看到这个动作。

00:06:31.260 --> 00:06:33.260
但是“消失”，从没有过。

00:06:33.260 --> 00:06:35.260
所以Even，你现在要做的是

00:06:35.260 --> 00:06:38.260
想象这个正方体会慢慢的消失掉，好吗。

00:06:38.260 --> 00:06:41.260
跟刚才一样。所以，1，2，3，开始。

00:06:50.260 --> 00:06:53.260
好，让我们试试。

00:06:53.260 --> 00:06:56.260
哦，天哪。他太棒了。

00:06:57.260 --> 00:06:59.260
我们在试一次。

00:07:04.260 --> 00:07:06.260
Even：分心了。

00:07:06.260 --> 00:07:08.260
（笑）

00:07:08.260 --> 00:07:10.260
Tan：但是我们可以看到这是可行的，

00:07:10.260 --> 00:07:12.260
就算你只花了一点点的时间

00:07:12.260 --> 00:07:14.260
在这个上面。

00:07:14.260 --> 00:07:17.260
就像我说的，去想象“消失”

00:07:17.260 --> 00:07:19.260
是一个很难的过程。

00:07:19.260 --> 00:07:21.260
了不起的事情是

00:07:21.260 --> 00:07:23.260
我们只给了系统一个他如何想象“消失”

00:07:23.260 --> 00:07:26.260
的例子。

00:07:26.260 --> 00:07:29.260
因为这里有一个机器解析的过程 －－

00:07:29.260 --> 00:07:33.260
（掌声）

00:07:33.260 --> 00:07:35.260
谢谢。

00:07:35.260 --> 00:07:38.260
做的好，做的好。

00:07:38.260 --> 00:07:40.260
（掌声）

00:07:40.260 --> 00:07:43.260
谢谢你，Even，你是这项技术的

00:07:43.260 --> 00:07:46.260
完美代表。

00:07:46.260 --> 00:07:48.260
所以就像你之前看到的，

00:07:48.260 --> 00:07:51.260
这个系统是被建入这个软件中

00:07:51.260 --> 00:07:53.260
这样就算是Even，或者其他的用户，

00:07:53.260 --> 00:07:55.260
都能更熟悉这个系统，

00:07:55.260 --> 00:07:58.260
他们可以不停的加入更多的探测方式，

00:07:58.260 --> 00:08:00.260
而系统也会在不同

00:08:00.260 --> 00:08:03.260
的想法中区分不同的差别。

00:08:04.260 --> 00:08:06.260
而且，一旦你训练好了探测功能，

00:08:06.260 --> 00:08:08.260
这个功能会可以被分享到

00:08:08.260 --> 00:08:10.260
任何一种计算器平台，

00:08:10.260 --> 00:08:12.260
应用程序或装置中。

00:08:12.260 --> 00:08:14.260
所以我想向你们展示一些例子，

00:08:14.260 --> 00:08:16.260
因为这个新界面有很多

00:08:16.260 --> 00:08:18.260
潜在的应用程序。

00:08:19.260 --> 00:08:21.260
比如说在游戏和虚拟世界中，

00:08:21.260 --> 00:08:23.260
你的脸部表情

00:08:23.260 --> 00:08:25.260
可以直观的被用来

00:08:25.260 --> 00:08:28.260
控制虚拟替身或人物。

00:08:29.260 --> 00:08:31.260
显然的，你能体验到神奇的魔法

00:08:31.260 --> 00:08:34.260
和用你的意念来控制世界。

00:08:36.260 --> 00:08:39.260
颜色，和灯光，

00:08:39.260 --> 00:08:41.260
声音和特效，

00:08:41.260 --> 00:08:43.260
也会根据你的感情模式做出相应的反应

00:08:43.260 --> 00:08:46.260
以此来提高你在现实中的体验。

00:08:47.260 --> 00:08:49.260
现在看看世界各地的开发者和研究家们

00:08:49.260 --> 00:08:52.260
所开发的应用程序，

00:08:52.260 --> 00:08:55.260
用机器人和简单机械，比如说 －－

00:08:55.260 --> 00:08:57.260
在这个例子里，通过想象提升来

00:08:57.260 --> 00:09:00.260
简单的驾驶一个玩具直升机。

00:09:00.260 --> 00:09:02.260
这个科技也可以被应用到

00:09:02.260 --> 00:09:04.260
现实生活中 －－

00:09:04.260 --> 00:09:06.260
比如，智能家庭。

00:09:06.260 --> 00:09:09.260
你知道的，通过人机界面中的控制系统

00:09:09.260 --> 00:09:11.260
来打开窗帘

00:09:11.260 --> 00:09:14.260
或关闭窗帘。

00:09:22.260 --> 00:09:25.260
当然还有照明 －－

00:09:25.260 --> 00:09:28.260
开灯

00:09:28.260 --> 00:09:30.260
或者关灯。

00:09:30.260 --> 00:09:32.260
以及最后的，

00:09:32.260 --> 00:09:34.260
可以改变生活的应用程序

00:09:34.260 --> 00:09:37.260
就比如说控制电子轮椅。

00:09:37.260 --> 00:09:39.260
在这个例子里，

00:09:39.260 --> 00:09:42.260
脸部表情被用来控制移动命令。

00:09:42.260 --> 00:09:45.260
男子：现在眨右眼往右。

00:09:50.260 --> 00:09:53.260
现在眨左眼往左。

00:10:02.260 --> 00:10:05.260
现在微笑往前。

00:10:08.260 --> 00:10:10.260
Tan：我们十分感谢你 －－ 谢谢。

00:10:10.260 --> 00:10:15.260
（掌声）

00:10:15.260 --> 00:10:18.260
我们今天仅仅大致地揭开了这个系统潜力的一角。

00:10:18.260 --> 00:10:20.260
随着用户群体的投入，

00:10:20.260 --> 00:10:22.260
开发者

00:10:22.260 --> 00:10:25.260
以及世界各地的研究员的加盟，

00:10:25.260 --> 00:10:27.260
我们希望你们可以帮助我们来

00:10:27.260 --> 00:10:30.260
探寻这项技术将何去何从。十分谢谢你们。

