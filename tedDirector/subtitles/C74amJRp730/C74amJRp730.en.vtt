WEBVTT
Kind: captions
Language: en

00:00:12.780 --> 00:00:13.980
I want you to imagine

00:00:14.820 --> 00:00:16.020
walking into a room,

00:00:17.300 --> 00:00:19.436
a control room with a bunch of people,

00:00:19.460 --> 00:00:22.300
a hundred people, hunched
over a desk with little dials,

00:00:23.100 --> 00:00:24.620
and that that control room

00:00:25.500 --> 00:00:29.196
will shape the thoughts and feelings

00:00:29.220 --> 00:00:30.460
of a billion people.

00:00:32.380 --> 00:00:34.260
This might sound like science fiction,

00:00:35.140 --> 00:00:37.356
but this actually exists

00:00:37.380 --> 00:00:38.580
right now, today.

00:00:39.860 --> 00:00:43.220
I know because I used to be
in one of those control rooms.

00:00:43.979 --> 00:00:46.276
I was a design ethicist at Google,

00:00:46.300 --> 00:00:49.660
where I studied how do you ethically
steer people's thoughts?

00:00:50.380 --> 00:00:53.276
Because what we don't talk about
is how the handful of people

00:00:53.300 --> 00:00:55.836
working at a handful
of technology companies

00:00:55.860 --> 00:01:00.900
through their choices will steer
what a billion people are thinking today.

00:01:02.220 --> 00:01:03.956
Because when you pull out your phone

00:01:03.980 --> 00:01:07.076
and they design how this works
or what's on the feed,

00:01:07.100 --> 00:01:10.316
it's scheduling little blocks
of time in our minds.

00:01:10.340 --> 00:01:13.476
If you see a notification,
it schedules you to have thoughts

00:01:13.500 --> 00:01:15.540
that maybe you didn't intend to have.

00:01:16.220 --> 00:01:18.836
If you swipe over that notification,

00:01:18.860 --> 00:01:21.241
it schedules you into spending
a little bit of time

00:01:21.265 --> 00:01:22.646
getting sucked into something

00:01:22.670 --> 00:01:25.625
that maybe you didn't intend
to get sucked into.

00:01:27.140 --> 00:01:28.660
When we talk about technology,

00:01:29.860 --> 00:01:32.556
we tend to talk about it
as this blue sky opportunity.

00:01:32.580 --> 00:01:34.060
It could go any direction.

00:01:35.220 --> 00:01:37.076
And I want to get serious for a moment

00:01:37.100 --> 00:01:39.780
and tell you why it's going
in a very specific direction.

00:01:40.660 --> 00:01:42.860
Because it's not evolving randomly.

00:01:43.820 --> 00:01:45.836
There's a hidden goal
driving the direction

00:01:45.860 --> 00:01:47.996
of all of the technology we make,

00:01:48.020 --> 00:01:50.940
and that goal is the race
for our attention.

00:01:52.660 --> 00:01:55.396
Because every new site --

00:01:55.420 --> 00:01:58.156
TED, elections, politicians,

00:01:58.180 --> 00:02:00.156
games, even meditation apps --

00:02:00.180 --> 00:02:02.140
have to compete for one thing,

00:02:02.980 --> 00:02:04.716
which is our attention,

00:02:04.740 --> 00:02:06.340
and there's only so much of it.

00:02:08.260 --> 00:02:10.676
And the best way to get people's attention

00:02:10.700 --> 00:02:13.140
is to know how someone's mind works.

00:02:13.620 --> 00:02:15.956
And there's a whole bunch
of persuasive techniques

00:02:15.980 --> 00:02:19.476
that I learned in college at a lab
called the Persuasive Technology Lab

00:02:19.500 --> 00:02:21.100
to get people's attention.

00:02:21.700 --> 00:02:23.180
A simple example is YouTube.

00:02:23.820 --> 00:02:26.756
YouTube wants to maximize
how much time you spend.

00:02:26.780 --> 00:02:27.980
And so what do they do?

00:02:28.660 --> 00:02:30.940
They autoplay the next video.

00:02:31.580 --> 00:02:33.396
And let's say that works really well.

00:02:33.420 --> 00:02:35.836
They're getting a little bit
more of people's time.

00:02:35.860 --> 00:02:38.236
Well, if you're Netflix,
you look at that and say,

00:02:38.260 --> 00:02:40.118
well, that's shrinking my market share,

00:02:40.142 --> 00:02:42.142
so I'm going to autoplay the next episode.

00:02:43.140 --> 00:02:44.516
But then if you're Facebook,

00:02:44.540 --> 00:02:46.876
you say, that's shrinking
all of my market share,

00:02:46.900 --> 00:02:49.556
so now I have to autoplay
all the videos in the newsfeed

00:02:49.580 --> 00:02:51.342
before waiting for you to click play.

00:02:52.140 --> 00:02:55.300
So the internet is not evolving at random.

00:02:56.140 --> 00:03:00.556
The reason it feels
like it's sucking us in the way it is

00:03:00.580 --> 00:03:02.956
is because of this race for attention.

00:03:02.980 --> 00:03:04.396
We know where this is going.

00:03:04.420 --> 00:03:05.940
Technology is not neutral,

00:03:07.140 --> 00:03:10.556
and it becomes this race
to the bottom of the brain stem

00:03:10.580 --> 00:03:12.780
of who can go lower to get it.

00:03:13.740 --> 00:03:16.076
Let me give you an example of Snapchat.

00:03:16.100 --> 00:03:19.796
If you didn't know,
Snapchat is the number one way

00:03:19.820 --> 00:03:22.076
that teenagers in
the United States communicate.

00:03:22.100 --> 00:03:26.276
So if you're like me, and you use
text messages to communicate,

00:03:26.300 --> 00:03:28.076
Snapchat is that for teenagers,

00:03:28.100 --> 00:03:30.796
and there's, like,
a hundred million of them that use it.

00:03:30.820 --> 00:03:33.036
And they invented
a feature called Snapstreaks,

00:03:33.060 --> 00:03:34.956
which shows the number of days in a row

00:03:34.980 --> 00:03:37.596
that two people have
communicated with each other.

00:03:37.620 --> 00:03:39.476
In other words, what they just did

00:03:39.500 --> 00:03:42.460
is they gave two people
something they don't want to lose.

00:03:43.820 --> 00:03:47.276
Because if you're a teenager,
and you have 150 days in a row,

00:03:47.300 --> 00:03:49.276
you don't want that to go away.

00:03:49.300 --> 00:03:53.460
And so think of the little blocks of time
that that schedules in kids' minds.

00:03:53.980 --> 00:03:56.316
This isn't theoretical:
when kids go on vacation,

00:03:56.340 --> 00:03:59.596
it's been shown they give their passwords
to up to five other friends

00:03:59.620 --> 00:04:01.836
to keep their Snapstreaks going,

00:04:01.860 --> 00:04:03.876
even when they can't do it.

00:04:03.900 --> 00:04:05.836
And they have, like, 30 of these things,

00:04:05.860 --> 00:04:09.236
and so they have to get through
taking photos of just pictures or walls

00:04:09.260 --> 00:04:11.740
or ceilings just to get through their day.

00:04:13.020 --> 00:04:15.716
So it's not even like
they're having real conversations.

00:04:15.740 --> 00:04:17.676
We have a temptation to think about this

00:04:17.700 --> 00:04:20.396
as, oh, they're just using Snapchat

00:04:20.420 --> 00:04:22.436
the way we used to
gossip on the telephone.

00:04:22.460 --> 00:04:23.660
It's probably OK.

00:04:24.300 --> 00:04:26.556
Well, what this misses
is that in the 1970s,

00:04:26.580 --> 00:04:29.195
when you were just
gossiping on the telephone,

00:04:29.219 --> 00:04:32.236
there wasn't a hundred engineers
on the other side of the screen

00:04:32.260 --> 00:04:34.316
who knew exactly
how your psychology worked

00:04:34.340 --> 00:04:36.980
and orchestrated you
into a double bind with each other.

00:04:38.260 --> 00:04:41.660
Now, if this is making you
feel a little bit of outrage,

00:04:42.500 --> 00:04:45.076
notice that that thought
just comes over you.

00:04:45.100 --> 00:04:48.420
Outrage is a really good way also
of getting your attention,

00:04:49.700 --> 00:04:51.276
because we don't choose outrage.

00:04:51.300 --> 00:04:52.716
It happens to us.

00:04:52.740 --> 00:04:54.596
And if you're the Facebook newsfeed,

00:04:54.620 --> 00:04:56.036
whether you'd want to or not,

00:04:56.060 --> 00:04:58.796
you actually benefit when there's outrage.

00:04:58.820 --> 00:05:01.756
Because outrage
doesn't just schedule a reaction

00:05:01.780 --> 00:05:04.660
in emotional time, space, for you.

00:05:05.260 --> 00:05:07.676
We want to share that outrage
with other people.

00:05:07.700 --> 00:05:09.276
So we want to hit share and say,

00:05:09.300 --> 00:05:11.340
"Can you believe the thing
that they said?"

00:05:12.340 --> 00:05:15.716
And so outrage works really well
at getting attention,

00:05:15.740 --> 00:05:19.636
such that if Facebook had a choice
between showing you the outrage feed

00:05:19.660 --> 00:05:20.980
and a calm newsfeed,

00:05:21.940 --> 00:05:24.076
they would want
to show you the outrage feed,

00:05:24.100 --> 00:05:26.156
not because someone
consciously chose that,

00:05:26.180 --> 00:05:28.860
but because that worked better
at getting your attention.

00:05:30.940 --> 00:05:36.420
And the newsfeed control room
is not accountable to us.

00:05:36.860 --> 00:05:39.156
It's only accountable
to maximizing attention.

00:05:39.180 --> 00:05:40.396
It's also accountable,

00:05:40.420 --> 00:05:42.796
because of the business model
of advertising,

00:05:42.820 --> 00:05:46.156
for anybody who can pay the most
to actually walk into the control room

00:05:46.180 --> 00:05:47.756
and say, "That group over there,

00:05:47.780 --> 00:05:50.420
I want to schedule these thoughts
into their minds."

00:05:51.580 --> 00:05:52.780
So you can target,

00:05:53.860 --> 00:05:55.796
you can precisely target a lie

00:05:55.820 --> 00:05:58.740
directly to the people
who are most susceptible.

00:05:59.900 --> 00:06:02.780
And because this is profitable,
it's only going to get worse.

00:06:04.860 --> 00:06:06.660
So I'm here today

00:06:07.980 --> 00:06:09.980
because the costs are so obvious.

00:06:12.100 --> 00:06:14.236
I don't know a more urgent
problem than this,

00:06:14.260 --> 00:06:17.380
because this problem
is underneath all other problems.

00:06:18.540 --> 00:06:21.716
It's not just taking away our agency

00:06:21.740 --> 00:06:24.340
to spend our attention
and live the lives that we want,

00:06:25.540 --> 00:06:29.076
it's changing the way
that we have our conversations,

00:06:29.100 --> 00:06:30.836
it's changing our democracy,

00:06:30.860 --> 00:06:33.476
and it's changing our ability
to have the conversations

00:06:33.500 --> 00:06:35.500
and relationships we want with each other.

00:06:36.980 --> 00:06:38.756
And it affects everyone,

00:06:38.780 --> 00:06:42.140
because a billion people
have one of these in their pocket.

00:06:45.180 --> 00:06:46.580
So how do we fix this?

00:06:48.900 --> 00:06:51.836
We need to make three radical changes

00:06:51.860 --> 00:06:53.660
to technology and to our society.

00:06:55.540 --> 00:06:59.340
The first is we need to acknowledge
that we are persuadable.

00:07:00.660 --> 00:07:02.036
Once you start understanding

00:07:02.060 --> 00:07:04.836
that your mind can be scheduled
into having little thoughts

00:07:04.860 --> 00:07:07.436
or little blocks of time
that you didn't choose,

00:07:07.460 --> 00:07:09.516
wouldn't we want to use that understanding

00:07:09.540 --> 00:07:11.700
and protect against the way
that that happens?

00:07:12.420 --> 00:07:15.716
I think we need to see ourselves
fundamentally in a new way.

00:07:15.740 --> 00:07:17.956
It's almost like a new period
of human history,

00:07:17.980 --> 00:07:19.196
like the Enlightenment,

00:07:19.220 --> 00:07:21.436
but almost a kind of
self-aware Enlightenment,

00:07:21.460 --> 00:07:23.540
that we can be persuaded,

00:07:24.140 --> 00:07:26.380
and there might be something
we want to protect.

00:07:27.220 --> 00:07:31.796
The second is we need new models
and accountability systems

00:07:31.820 --> 00:07:35.316
so that as the world gets better
and more and more persuasive over time --

00:07:35.340 --> 00:07:37.676
because it's only going
to get more persuasive --

00:07:37.700 --> 00:07:39.556
that the people in those control rooms

00:07:39.580 --> 00:07:42.036
are accountable and transparent
to what we want.

00:07:42.060 --> 00:07:44.756
The only form of ethical
persuasion that exists

00:07:44.780 --> 00:07:46.716
is when the goals of the persuader

00:07:46.740 --> 00:07:48.940
are aligned with the goals
of the persuadee.

00:07:49.460 --> 00:07:53.300
And that involves questioning big things,
like the business model of advertising.

00:07:54.540 --> 00:07:56.116
Lastly,

00:07:56.140 --> 00:07:57.820
we need a design renaissance,

00:07:58.900 --> 00:08:01.956
because once you have
this view of human nature,

00:08:01.980 --> 00:08:04.956
that you can steer the timelines
of a billion people --

00:08:04.980 --> 00:08:07.716
just imagine, there's people
who have some desire

00:08:07.740 --> 00:08:10.596
about what they want to do
and what they want to be thinking

00:08:10.620 --> 00:08:13.756
and what they want to be feeling
and how they want to be informed,

00:08:13.780 --> 00:08:16.316
and we're all just tugged
into these other directions.

00:08:16.340 --> 00:08:20.036
And you have a billion people just tugged
into all these different directions.

00:08:20.060 --> 00:08:22.116
Well, imagine an entire design renaissance

00:08:22.140 --> 00:08:25.236
that tried to orchestrate
the exact and most empowering

00:08:25.260 --> 00:08:28.396
time-well-spent way
for those timelines to happen.

00:08:28.420 --> 00:08:30.076
And that would involve two things:

00:08:30.100 --> 00:08:32.236
one would be protecting
against the timelines

00:08:32.260 --> 00:08:34.116
that we don't want to be experiencing,

00:08:34.140 --> 00:08:36.556
the thoughts that we
wouldn't want to be happening,

00:08:36.580 --> 00:08:39.916
so that when that ding happens,
not having the ding that sends us away;

00:08:39.940 --> 00:08:43.556
and the second would be empowering us
to live out the timeline that we want.

00:08:43.580 --> 00:08:45.460
So let me give you a concrete example.

00:08:46.100 --> 00:08:48.556
Today, let's say your friend
cancels dinner on you,

00:08:48.580 --> 00:08:52.355
and you are feeling a little bit lonely.

00:08:52.379 --> 00:08:54.196
And so what do you do in that moment?

00:08:54.220 --> 00:08:55.499
You open up Facebook.

00:08:56.780 --> 00:08:58.476
And in that moment,

00:08:58.500 --> 00:09:01.876
the designers in the control room
want to schedule exactly one thing,

00:09:01.900 --> 00:09:04.940
which is to maximize how much time
you spend on the screen.

00:09:06.460 --> 00:09:10.356
Now, instead, imagine if those designers
created a different timeline

00:09:10.380 --> 00:09:13.876
that was the easiest way,
using all of their data,

00:09:13.900 --> 00:09:16.996
to actually help you get out
with the people that you care about?

00:09:17.020 --> 00:09:22.436
Just think, alleviating
all loneliness in society,

00:09:22.460 --> 00:09:25.956
if that was the timeline that Facebook
wanted to make possible for people.

00:09:25.980 --> 00:09:27.695
Or imagine a different conversation.

00:09:27.719 --> 00:09:31.036
Let's say you wanted to post
something supercontroversial on Facebook,

00:09:31.060 --> 00:09:33.476
which is a really important
thing to be able to do,

00:09:33.500 --> 00:09:35.196
to talk about controversial topics.

00:09:35.220 --> 00:09:37.556
And right now, when there's
that big comment box,

00:09:37.580 --> 00:09:40.956
it's almost asking you,
what key do you want to type?

00:09:40.980 --> 00:09:43.796
In other words, it's scheduling
a little timeline of things

00:09:43.820 --> 00:09:45.956
you're going to continue
to do on the screen.

00:09:45.980 --> 00:09:48.956
And imagine instead that there was
another button there saying,

00:09:48.980 --> 00:09:51.036
what would be most
time well spent for you?

00:09:51.060 --> 00:09:52.636
And you click "host a dinner."

00:09:52.660 --> 00:09:54.756
And right there
underneath the item it said,

00:09:54.780 --> 00:09:56.476
"Who wants to RSVP for the dinner?"

00:09:56.500 --> 00:09:59.756
And so you'd still have a conversation
about something controversial,

00:09:59.780 --> 00:10:03.516
but you'd be having it in the most
empowering place on your timeline,

00:10:03.540 --> 00:10:06.556
which would be at home that night
with a bunch of a friends over

00:10:06.580 --> 00:10:07.780
to talk about it.

00:10:08.820 --> 00:10:11.980
So imagine we're running, like,
a find and replace

00:10:12.820 --> 00:10:15.396
on all of the timelines
that are currently steering us

00:10:15.420 --> 00:10:17.980
towards more and more
screen time persuasively

00:10:18.900 --> 00:10:21.436
and replacing all of those timelines

00:10:21.460 --> 00:10:23.100
with what do we want in our lives.

00:10:26.780 --> 00:10:28.260
It doesn't have to be this way.

00:10:30.180 --> 00:10:32.436
Instead of handicapping our attention,

00:10:32.460 --> 00:10:35.276
imagine if we used all of this data
and all of this power

00:10:35.300 --> 00:10:36.916
and this new view of human nature

00:10:36.940 --> 00:10:39.796
to give us a superhuman ability to focus

00:10:39.820 --> 00:10:43.956
and a superhuman ability to put
our attention to what we cared about

00:10:43.980 --> 00:10:46.596
and a superhuman ability
to have the conversations

00:10:46.620 --> 00:10:48.620
that we need to have for democracy.

00:10:51.420 --> 00:10:54.100
The most complex challenges in the world

00:10:56.100 --> 00:10:59.220
require not just us
to use our attention individually.

00:11:00.260 --> 00:11:03.580
They require us to use our attention
and coordinate it together.

00:11:04.260 --> 00:11:07.076
Climate change is going to require
that a lot of people

00:11:07.100 --> 00:11:09.196
are being able
to coordinate their attention

00:11:09.220 --> 00:11:11.116
in the most empowering way together.

00:11:11.140 --> 00:11:14.220
And imagine creating
a superhuman ability to do that.

00:11:18.820 --> 00:11:22.980
Sometimes the world's
most pressing and important problems

00:11:23.860 --> 00:11:27.700
are not these hypothetical future things
that we could create in the future.

00:11:28.380 --> 00:11:30.116
Sometimes the most pressing problems

00:11:30.140 --> 00:11:32.476
are the ones that are
right underneath our noses,

00:11:32.500 --> 00:11:35.620
the things that are already directing
a billion people's thoughts.

00:11:36.420 --> 00:11:39.796
And maybe instead of getting excited
about the new augmented reality

00:11:39.820 --> 00:11:43.116
and virtual reality
and these cool things that could happen,

00:11:43.140 --> 00:11:46.436
which are going to be susceptible
to the same race for attention,

00:11:46.460 --> 00:11:48.636
if we could fix the race for attention

00:11:48.660 --> 00:11:51.380
on the thing that's already
in a billion people's pockets.

00:11:51.860 --> 00:11:53.436
Maybe instead of getting excited

00:11:53.460 --> 00:11:57.636
about the most exciting
new cool fancy education apps,

00:11:57.660 --> 00:12:00.556
we could fix the way
kids' minds are getting manipulated

00:12:00.580 --> 00:12:03.060
into sending empty messages
back and forth.

00:12:03.860 --> 00:12:08.156
(Applause)

00:12:08.180 --> 00:12:09.436
Maybe instead of worrying

00:12:09.460 --> 00:12:13.236
about hypothetical future
runaway artificial intelligences

00:12:13.260 --> 00:12:15.140
that are maximizing for one goal,

00:12:16.500 --> 00:12:19.156
we could solve the runaway
artificial intelligence

00:12:19.180 --> 00:12:21.236
that already exists right now,

00:12:21.260 --> 00:12:24.180
which are these newsfeeds
maximizing for one thing.

00:12:25.900 --> 00:12:29.716
It's almost like instead of running away
to colonize new planets,

00:12:29.740 --> 00:12:31.796
we could fix the one
that we're already on.

00:12:31.820 --> 00:12:35.940
(Applause)

00:12:39.860 --> 00:12:41.636
Solving this problem

00:12:41.660 --> 00:12:45.460
is critical infrastructure
for solving every other problem.

00:12:46.420 --> 00:12:50.436
There's nothing in your life
or in our collective problems

00:12:50.460 --> 00:12:54.020
that does not require our ability
to put our attention where we care about.

00:12:55.620 --> 00:12:56.860
At the end of our lives,

00:12:58.060 --> 00:13:00.700
all we have is our attention and our time.

00:13:01.620 --> 00:13:03.516
What will be time well spent for ours?

00:13:03.540 --> 00:13:04.756
Thank you.

00:13:04.780 --> 00:13:07.900
(Applause)

00:13:17.580 --> 00:13:20.516
Chris Anderson: Tristan, thank you.
Hey, stay up here a sec.

00:13:20.540 --> 00:13:21.876
First of all, thank you.

00:13:21.900 --> 00:13:24.676
I know we asked you to do this talk
on pretty short notice,

00:13:24.700 --> 00:13:26.916
and you've had quite a stressful week

00:13:26.940 --> 00:13:29.380
getting this thing together, so thank you.

00:13:30.500 --> 00:13:34.476
Some people listening might say,
what you complain about is addiction,

00:13:34.500 --> 00:13:37.996
and all these people doing this stuff,
for them it's actually interesting.

00:13:38.020 --> 00:13:39.276
All these design decisions

00:13:39.300 --> 00:13:42.396
have built user content
that is fantastically interesting.

00:13:42.420 --> 00:13:44.836
The world's more interesting
than it ever has been.

00:13:44.860 --> 00:13:46.116
What's wrong with that?

00:13:46.140 --> 00:13:48.396
Tristan Harris:
I think it's really interesting.

00:13:48.420 --> 00:13:52.436
One way to see this
is if you're just YouTube, for example,

00:13:52.460 --> 00:13:55.116
you want to always show
the more interesting next video.

00:13:55.140 --> 00:13:58.156
You want to get better and better
at suggesting that next video,

00:13:58.180 --> 00:14:00.636
but even if you could propose
the perfect next video

00:14:00.660 --> 00:14:02.316
that everyone would want to watch,

00:14:02.340 --> 00:14:05.676
it would just be better and better
at keeping you hooked on the screen.

00:14:05.700 --> 00:14:07.356
So what's missing in that equation

00:14:07.380 --> 00:14:09.516
is figuring out what
our boundaries would be.

00:14:09.540 --> 00:14:12.756
You would want YouTube to know
something about, say, falling asleep.

00:14:12.780 --> 00:14:14.396
The CEO of Netflix recently said,

00:14:14.420 --> 00:14:17.156
"our biggest competitors
are Facebook, YouTube and sleep."

00:14:17.180 --> 00:14:21.636
And so what we need to recognize
is that the human architecture is limited

00:14:21.660 --> 00:14:24.636
and that we have certain boundaries
or dimensions of our lives

00:14:24.660 --> 00:14:26.636
that we want to be honored and respected,

00:14:26.660 --> 00:14:28.476
and technology could help do that.

00:14:28.500 --> 00:14:31.116
(Applause)

00:14:31.140 --> 00:14:32.836
CA: I mean, could you make the case

00:14:32.860 --> 00:14:38.916
that part of the problem here is that
we've got a naïve model of human nature?

00:14:38.940 --> 00:14:41.676
So much of this is justified
in terms of human preference,

00:14:41.700 --> 00:14:44.316
where we've got these algorithms
that do an amazing job

00:14:44.340 --> 00:14:46.036
of optimizing for human preference,

00:14:46.060 --> 00:14:47.396
but which preference?

00:14:47.420 --> 00:14:50.916
There's the preferences
of things that we really care about

00:14:50.940 --> 00:14:52.316
when we think about them

00:14:52.340 --> 00:14:55.396
versus the preferences
of what we just instinctively click on.

00:14:55.420 --> 00:15:00.076
If we could implant that more nuanced
view of human nature in every design,

00:15:00.100 --> 00:15:01.556
would that be a step forward?

00:15:01.580 --> 00:15:03.556
TH: Absolutely. I mean, I think right now

00:15:03.580 --> 00:15:07.076
it's as if all of our technology
is basically only asking our lizard brain

00:15:07.100 --> 00:15:09.596
what's the best way
to just impulsively get you to do

00:15:09.620 --> 00:15:11.756
the next tiniest thing with your time,

00:15:11.780 --> 00:15:13.436
instead of asking you in your life

00:15:13.460 --> 00:15:15.636
what we would be most
time well spent for you?

00:15:15.660 --> 00:15:18.956
What would be the perfect timeline
that might include something later,

00:15:18.980 --> 00:15:22.156
would be time well spent for you
here at TED in your last day here?

00:15:22.180 --> 00:15:25.156
CA: So if Facebook and Google
and everyone said to us first up,

00:15:25.180 --> 00:15:28.076
"Hey, would you like us
to optimize for your reflective brain

00:15:28.100 --> 00:15:29.756
or your lizard brain? You choose."

00:15:29.780 --> 00:15:31.860
TH: Right. That would be one way. Yes.

00:15:34.178 --> 00:15:37.036
CA: You said persuadability,
that's an interesting word to me

00:15:37.060 --> 00:15:39.916
because to me there's
two different types of persuadability.

00:15:39.940 --> 00:15:42.476
There's the persuadability
that we're trying right now

00:15:42.500 --> 00:15:44.676
of reason and thinking
and making an argument,

00:15:44.700 --> 00:15:47.396
but I think you're almost
talking about a different kind,

00:15:47.410 --> 00:15:49.316
a more visceral type of persuadability,

00:15:49.340 --> 00:15:52.236
of being persuaded without
even knowing that you're thinking.

00:15:52.260 --> 00:15:55.116
TH: Exactly. The reason
I care about this problem so much is

00:15:55.140 --> 00:15:58.316
I studied at a lab called
the Persuasive Technology Lab at Stanford

00:15:58.340 --> 00:16:00.886
that taught [students how to recognize]
exactly these techniques.

00:16:00.926 --> 00:16:03.916
There's conferences and workshops
that teach people all these covert ways

00:16:03.940 --> 00:16:06.916
of getting people's attention
and orchestrating people's lives.

00:16:06.940 --> 00:16:09.596
And it's because most people
don't know that that exists

00:16:09.620 --> 00:16:11.516
that this conversation is so important.

00:16:11.540 --> 00:16:15.316
CA: Tristan, you and I, we both know
so many people from all these companies.

00:16:15.340 --> 00:16:17.316
There are actually many here in the room,

00:16:17.340 --> 00:16:19.817
and I don't know about you,
but my experience of them

00:16:19.841 --> 00:16:21.916
is that there is
no shortage of good intent.

00:16:21.940 --> 00:16:24.116
People want a better world.

00:16:24.140 --> 00:16:27.660
They are actually -- they really want it.

00:16:28.140 --> 00:16:32.316
And I don't think anything you're saying
is that these are evil people.

00:16:32.340 --> 00:16:36.036
It's a system where there's
these unintended consequences

00:16:36.060 --> 00:16:37.916
that have really got out of control --

00:16:37.940 --> 00:16:39.436
TH: Of this race for attention.

00:16:39.460 --> 00:16:42.636
It's the classic race to the bottom
when you have to get attention,

00:16:42.660 --> 00:16:43.876
and it's so tense.

00:16:43.900 --> 00:16:46.636
The only way to get more
is to go lower on the brain stem,

00:16:46.660 --> 00:16:49.076
to go lower into outrage,
to go lower into emotion,

00:16:49.100 --> 00:16:50.796
to go lower into the lizard brain.

00:16:50.820 --> 00:16:54.636
CA: Well, thank you so much for helping us
all get a little bit wiser about this.

00:16:54.660 --> 00:16:57.076
Tristan Harris, thank you.
TH: Thank you very much.

00:16:57.100 --> 00:16:59.340
(Applause)

