WEBVTT
Kind: captions
Language: en

00:00:13.160 --> 00:00:15.160
I thought I'd begin with a scene of war.

00:00:15.160 --> 00:00:17.160
There was little to warn of the danger ahead.

00:00:17.160 --> 00:00:19.160
The Iraqi insurgent had placed the IED,

00:00:19.160 --> 00:00:22.160
an Improvised Explosive Device,

00:00:22.160 --> 00:00:24.160
along the side of the road with great care.

00:00:24.160 --> 00:00:28.160
By 2006, there were more than 2,500

00:00:28.160 --> 00:00:31.160
of these attacks every single month,

00:00:31.160 --> 00:00:33.160
and they were the leading cause of

00:00:33.160 --> 00:00:35.160
casualties among American soldiers

00:00:35.160 --> 00:00:37.160
and Iraqi civilians.

00:00:37.160 --> 00:00:39.160
The team that was hunting for this IED

00:00:39.160 --> 00:00:41.160
is called an EOD team—

00:00:41.160 --> 00:00:43.160
Explosives Ordinance Disposal—and

00:00:43.160 --> 00:00:45.160
they're the pointy end of the spear in the

00:00:45.160 --> 00:00:48.160
American effort to suppress these roadside bombs.

00:00:48.160 --> 00:00:50.160
Each EOD team goes out on about

00:00:50.160 --> 00:00:52.160
600 of these bomb calls every year,

00:00:52.160 --> 00:00:55.160
defusing about two bombs a day.

00:00:55.160 --> 00:00:57.160
Perhaps the best sign of how valuable they

00:00:57.160 --> 00:00:59.160
are to the war effort, is that

00:00:59.160 --> 00:01:01.160
the Iraqi insurgents put a $50,000 bounty

00:01:01.160 --> 00:01:04.160
on the head of a single EOD soldier.

00:01:04.160 --> 00:01:06.160
Unfortunately, this particular call

00:01:06.160 --> 00:01:08.160
would not end well.

00:01:08.160 --> 00:01:10.160
By the time the soldier advanced close

00:01:10.160 --> 00:01:12.160
enough to see the telltale wires

00:01:12.160 --> 00:01:15.160
of the bomb, it exploded in a wave of flame.

00:01:15.160 --> 00:01:17.160
Now, depending how close you are

00:01:17.160 --> 00:01:19.160
and how much explosive has been packed

00:01:19.160 --> 00:01:21.160
into that bomb, it can cause death

00:01:21.160 --> 00:01:23.160
or injury. You have to be as far as

00:01:23.160 --> 00:01:25.160
50 yards away to escape that.

00:01:25.160 --> 00:01:27.160
The blast is so strong it can even break

00:01:27.160 --> 00:01:29.160
your limbs, even if you're not hit.

00:01:29.160 --> 00:01:31.160
That soldier had been on top of the bomb.

00:01:31.160 --> 00:01:34.160
And so when the rest of the team advanced

00:01:34.160 --> 00:01:36.160
they found little left. And that night the unit's

00:01:36.160 --> 00:01:38.160
commander did a sad duty, and he wrote

00:01:38.160 --> 00:01:40.160
a condolence letter back to the United

00:01:40.160 --> 00:01:42.160
States, and he talked about how hard the

00:01:42.160 --> 00:01:44.160
loss had been on his unit, about the fact

00:01:44.160 --> 00:01:46.160
that they had lost their bravest soldier,

00:01:46.160 --> 00:01:48.160
a soldier who had saved their lives

00:01:48.160 --> 00:01:50.160
many a time.

00:01:50.160 --> 00:01:52.160
And he apologized

00:01:52.160 --> 00:01:54.160
for not being able to bring them home.

00:01:54.160 --> 00:01:56.160
But then he talked up the silver lining

00:01:56.160 --> 00:01:58.160
that he took away from the loss.

00:01:58.160 --> 00:02:00.160
"At least," as he wrote, "when a robot dies,

00:02:00.160 --> 00:02:02.160
you don't have to write a letter

00:02:02.160 --> 00:02:04.160
to its mother."

00:02:04.160 --> 00:02:06.160
That scene sounds like science fiction,

00:02:06.160 --> 00:02:08.160
but is battlefield reality already.

00:02:08.160 --> 00:02:11.160
The soldier in that case

00:02:11.160 --> 00:02:14.160
was a 42-pound robot called a PackBot.

00:02:14.160 --> 00:02:17.160
The chief's letter went, not to some

00:02:17.160 --> 00:02:19.160
farmhouse in Iowa like you see

00:02:19.160 --> 00:02:22.160
in the old war movies, but went to

00:02:22.160 --> 00:02:24.160
the iRobot Company, which is

00:02:24.160 --> 00:02:27.160
named after the Asimov novel

00:02:27.160 --> 00:02:29.160
and the not-so-great Will Smith movie,

00:02:29.160 --> 00:02:31.160
and... um... (Laughter)...

00:02:31.160 --> 00:02:33.160
if you remember that

00:02:33.160 --> 00:02:35.160
in that fictional world, robots started out

00:02:35.160 --> 00:02:37.160
carrying out mundane chores, and then

00:02:37.160 --> 00:02:39.160
they started taking on life-and-death decisions.

00:02:39.160 --> 00:02:41.160
That's a reality we face today.

00:02:41.160 --> 00:02:43.160
What we're going to do is actually just

00:02:43.160 --> 00:02:45.160
flash a series of photos behind me that

00:02:45.160 --> 00:02:48.160
show you the reality of robots used in war

00:02:48.160 --> 00:02:50.160
right now or already at the prototype stage.

00:02:50.160 --> 00:02:53.160
It's just to give you a taste.

00:02:53.160 --> 00:02:55.160
Another way of putting it is you're not

00:02:55.160 --> 00:02:57.160
going to see anything that's powered

00:02:57.160 --> 00:02:59.160
by Vulcan technology, or teenage

00:02:59.160 --> 00:03:01.160
wizard hormones or anything like that.

00:03:01.160 --> 00:03:03.160
This is all real. So why don't we

00:03:03.160 --> 00:03:05.160
go ahead and start those pictures.

00:03:05.160 --> 00:03:07.160
Something big is going on in war today,

00:03:07.160 --> 00:03:09.160
and maybe even the history of humanity

00:03:09.160 --> 00:03:12.160
itself. The U.S. military went into Iraq with

00:03:12.160 --> 00:03:14.160
a handful of drones in the air.

00:03:14.160 --> 00:03:17.160
We now have 5,300.

00:03:17.160 --> 00:03:19.160
We went in with zero unmanned ground

00:03:19.160 --> 00:03:23.160
systems. We now have 12,000.

00:03:23.160 --> 00:03:25.160
And the tech term "killer application"

00:03:25.160 --> 00:03:28.160
takes on new meaning in this space.

00:03:28.160 --> 00:03:30.160
And we need to remember that we're

00:03:30.160 --> 00:03:32.160
talking about the Model T Fords,

00:03:32.160 --> 00:03:34.160
the Wright Flyers, compared

00:03:34.160 --> 00:03:36.160
to what's coming soon.

00:03:36.160 --> 00:03:38.160
That's where we're at right now.

00:03:38.160 --> 00:03:40.160
One of the people that I recently met with

00:03:40.160 --> 00:03:42.160
was an Air Force three-star general, and he

00:03:42.160 --> 00:03:44.160
said basically, where we're headed very

00:03:44.160 --> 00:03:46.160
soon is tens of thousands of robots

00:03:46.160 --> 00:03:48.160
operating in our conflicts, and these

00:03:48.160 --> 00:03:50.160
numbers matter, because we're not just

00:03:50.160 --> 00:03:52.160
talking about tens of thousands of today's

00:03:52.160 --> 00:03:54.160
robots, but tens of thousands of these

00:03:54.160 --> 00:03:56.160
prototypes and tomorrow's robots, because

00:03:56.160 --> 00:03:59.160
of course, one of the things that's operating

00:03:59.160 --> 00:04:01.160
in technology is Moore's Law,

00:04:01.160 --> 00:04:03.160
that you can pack in more and more

00:04:03.160 --> 00:04:05.160
computing power into those robots, and so

00:04:05.160 --> 00:04:07.160
flash forward around 25 years,

00:04:07.160 --> 00:04:09.160
if Moore's Law holds true,

00:04:09.160 --> 00:04:12.160
those robots will be close to a billion times

00:04:12.160 --> 00:04:15.160
more powerful in their computing than today.

00:04:15.160 --> 00:04:17.160
And so what that means is the kind of

00:04:17.160 --> 00:04:19.160
things that we used to only talk about at

00:04:19.160 --> 00:04:21.160
science fiction conventions like Comic-Con

00:04:21.160 --> 00:04:23.160
have to be talked about in the halls

00:04:23.160 --> 00:04:25.160
of power and places like the Pentagon.

00:04:25.160 --> 00:04:28.160
A robots revolution is upon us.

00:04:28.160 --> 00:04:30.160
Now, I need to be clear here.

00:04:30.160 --> 00:04:32.160
I'm not talking about a revolution where you

00:04:32.160 --> 00:04:34.160
have to worry about the Governor of

00:04:34.160 --> 00:04:36.160
California showing up at your door,

00:04:36.160 --> 00:04:38.160
a la the Terminator. (Laughter)

00:04:38.160 --> 00:04:40.160
When historians look at this period, they're

00:04:40.160 --> 00:04:42.160
going to conclude that we're in a different

00:04:42.160 --> 00:04:44.160
type of revolution: a revolution in war,

00:04:44.160 --> 00:04:46.160
like the invention of the atomic bomb.

00:04:46.160 --> 00:04:48.160
But it may be even bigger than that,

00:04:48.160 --> 00:04:50.160
because our unmanned systems don't just

00:04:50.160 --> 00:04:52.160
affect the "how" of war-fighting,

00:04:52.160 --> 00:04:54.160
they affect the "who" of fighting

00:04:54.160 --> 00:04:56.160
at its most fundamental level.

00:04:56.160 --> 00:04:58.160
That is, every previous revolution in war, be

00:04:58.160 --> 00:05:00.160
it the machine gun, be it the atomic bomb,

00:05:00.160 --> 00:05:03.160
was about a system that either shot faster,

00:05:03.160 --> 00:05:06.160
went further, had a bigger boom.

00:05:06.160 --> 00:05:09.160
That's certainly the case with robotics, but

00:05:09.160 --> 00:05:12.160
they also change the experience of the warrior

00:05:12.160 --> 00:05:15.160
and even the very identity of the warrior.

00:05:15.160 --> 00:05:18.160
Another way of putting this is that

00:05:18.160 --> 00:05:20.160
mankind's 5,000-year-old monopoly

00:05:20.160 --> 00:05:23.160
on the fighting of war is breaking down

00:05:23.160 --> 00:05:25.160
in our very lifetime. I've spent

00:05:25.160 --> 00:05:27.160
the last several years going around

00:05:27.160 --> 00:05:29.160
meeting with all the players in this field,

00:05:29.160 --> 00:05:31.160
from the robot scientists to the science

00:05:31.160 --> 00:05:33.160
fiction authors who inspired them to the

00:05:33.160 --> 00:05:35.160
19-year-old drone pilots who are fighting

00:05:35.160 --> 00:05:37.160
from Nevada, to the four-star generals

00:05:37.160 --> 00:05:39.160
who command them, to even the Iraqi

00:05:39.160 --> 00:05:41.160
insurgents who they are targeting and what

00:05:41.160 --> 00:05:43.160
they think about our systems, and

00:05:43.160 --> 00:05:45.160
what I found interesting is not just

00:05:45.160 --> 00:05:47.160
their stories, but how their experiences

00:05:47.160 --> 00:05:49.160
point to these ripple effects that are going

00:05:49.160 --> 00:05:51.160
outwards in our society, in our law

00:05:51.160 --> 00:05:53.160
and our ethics, etc. And so what I'd like

00:05:53.160 --> 00:05:55.160
to do with my remaining time is basically

00:05:55.160 --> 00:05:57.160
flesh out a couple of these.

00:05:57.160 --> 00:05:59.160
So the first is that the future of war,

00:05:59.160 --> 00:06:01.160
even a robotics one, is not going to be

00:06:01.160 --> 00:06:03.160
purely an American one.

00:06:03.160 --> 00:06:05.160
The U.S. is currently ahead in military

00:06:05.160 --> 00:06:07.160
robotics right now, but we know that in

00:06:07.160 --> 00:06:09.160
technology there's no such thing as

00:06:09.160 --> 00:06:12.160
a permanent first move or advantage.

00:06:12.160 --> 00:06:14.160
In a quick show of hands, how many

00:06:14.160 --> 00:06:16.160
people in this room still use

00:06:16.160 --> 00:06:18.160
Wang Computers? (Laughter)

00:06:18.160 --> 00:06:20.160
It's the same thing in war. The British and

00:06:20.160 --> 00:06:23.160
the French invented the tank.

00:06:23.160 --> 00:06:25.160
The Germans figured out how

00:06:25.160 --> 00:06:27.160
to use it right, and so what we have to

00:06:27.160 --> 00:06:29.160
think about for the U.S. is that we are

00:06:29.160 --> 00:06:31.160
ahead right now, but you have

00:06:31.160 --> 00:06:33.160
43 other countries out there

00:06:33.160 --> 00:06:35.160
working on military robotics, and they

00:06:35.160 --> 00:06:37.160
include all the interesting countries like

00:06:37.160 --> 00:06:40.160
Russia, China, Pakistan, Iran.

00:06:40.160 --> 00:06:43.160
And this raises a bigger worry for me.

00:06:43.160 --> 00:06:45.160
How do we move forward in this revolution

00:06:45.160 --> 00:06:47.160
given the state of our manufacturing

00:06:47.160 --> 00:06:49.160
and the state of our science and

00:06:49.160 --> 00:06:51.160
mathematics training in our schools?

00:06:51.160 --> 00:06:53.160
Or another way of thinking about this is,

00:06:53.160 --> 00:06:55.160
what does it mean to go to war increasingly

00:06:55.160 --> 00:06:58.160
with soldiers whose hardware is made

00:06:58.160 --> 00:07:03.160
in China and software is written in India?

00:07:03.160 --> 00:07:06.160
But just as software has gone open-source,

00:07:06.160 --> 00:07:08.160
so has warfare.

00:07:08.160 --> 00:07:11.160
Unlike an aircraft carrier or an atomic bomb,

00:07:11.160 --> 00:07:13.160
you don't need a massive manufacturing

00:07:13.160 --> 00:07:15.160
system to build robotics. A lot of it is

00:07:15.160 --> 00:07:17.160
off the shelf. A lot of it's even do-it-yourself.

00:07:17.160 --> 00:07:19.160
One of those things you just saw flashed

00:07:19.160 --> 00:07:21.160
before you was a raven drone, the handheld

00:07:21.160 --> 00:07:23.160
tossed one. For about a thousand dollars,

00:07:23.160 --> 00:07:25.160
you can build one yourself, equivalent to

00:07:25.160 --> 00:07:27.160
what the soldiers use in Iraq.

00:07:27.160 --> 00:07:29.160
That raises another wrinkle when it comes

00:07:29.160 --> 00:07:31.160
to war and conflict. Good guys might play

00:07:31.160 --> 00:07:33.160
around and work on these as hobby kits,

00:07:33.160 --> 00:07:35.160
but so might bad guys.

00:07:35.160 --> 00:07:37.160
This cross between robotics and things like

00:07:37.160 --> 00:07:39.160
terrorism is going to be fascinating

00:07:39.160 --> 00:07:41.160
and even disturbing,

00:07:41.160 --> 00:07:43.160
and we've already seen it start.

00:07:43.160 --> 00:07:45.160
During the war between Israel, a state,

00:07:45.160 --> 00:07:48.160
and Hezbollah, a non-state actor,

00:07:48.160 --> 00:07:50.160
the non-state actor flew

00:07:50.160 --> 00:07:52.160
four different drones against Israel.

00:07:52.160 --> 00:07:54.160
There's already a jihadi website

00:07:54.160 --> 00:07:56.160
that you can go on and remotely

00:07:56.160 --> 00:07:58.160
detonate an IED in Iraq while sitting

00:07:58.160 --> 00:08:00.160
at your home computer.

00:08:00.160 --> 00:08:02.160
And so I think what we're going to see is

00:08:02.160 --> 00:08:04.160
two trends take place with this.

00:08:04.160 --> 00:08:06.160
First is, you're going to reinforce the power

00:08:06.160 --> 00:08:10.160
of individuals against governments,

00:08:10.160 --> 00:08:12.160
but then the second is that

00:08:12.160 --> 00:08:14.160
we are going to see an expansion

00:08:14.160 --> 00:08:16.160
in the realm of terrorism.

00:08:16.160 --> 00:08:18.160
The future of it may be a cross between

00:08:18.160 --> 00:08:20.160
al Qaeda 2.0 and the

00:08:20.160 --> 00:08:22.160
next generation of the Unabomber.

00:08:22.160 --> 00:08:24.160
And another way of thinking about this

00:08:24.160 --> 00:08:26.160
is the fact that, remember, you don't have

00:08:26.160 --> 00:08:28.160
to convince a robot that they're gonna

00:08:28.160 --> 00:08:31.160
receive 72 virgins after they die

00:08:31.160 --> 00:08:34.160
to convince them to blow themselves up.

00:08:34.160 --> 00:08:36.160
But the ripple effects of this are going to go

00:08:36.160 --> 00:08:38.160
out into our politics. One of the people that

00:08:38.160 --> 00:08:40.160
I met with was a former Assistant Secretary of

00:08:40.160 --> 00:08:42.160
Defense for Ronald Reagan, and he put it

00:08:42.160 --> 00:08:44.160
this way: "I like these systems because

00:08:44.160 --> 00:08:46.160
they save American lives, but I worry about

00:08:46.160 --> 00:08:48.160
more marketization of wars,

00:08:48.160 --> 00:08:51.160
more shock-and-awe talk,

00:08:51.160 --> 00:08:53.160
to defray discussion of the costs.

00:08:53.160 --> 00:08:55.160
People are more likely to support the use

00:08:55.160 --> 00:08:58.160
of force if they view it as costless."

00:08:58.160 --> 00:09:00.160
Robots for me take certain trends

00:09:00.160 --> 00:09:03.160
that are already in play in our body politic,

00:09:03.160 --> 00:09:05.160
and maybe take them to

00:09:05.160 --> 00:09:07.160
their logical ending point.

00:09:07.160 --> 00:09:09.160
We don't have a draft. We don't

00:09:09.160 --> 00:09:12.160
have declarations of war anymore.

00:09:12.160 --> 00:09:14.160
We don't buy war bonds anymore.

00:09:14.160 --> 00:09:16.160
And now we have the fact that we're

00:09:16.160 --> 00:09:18.160
converting more and more of our American

00:09:18.160 --> 00:09:20.160
soldiers that we would send into harm's

00:09:20.160 --> 00:09:23.160
way into machines, and so we may take

00:09:23.160 --> 00:09:26.160
those already lowering bars to war

00:09:26.160 --> 00:09:29.160
and drop them to the ground.

00:09:29.160 --> 00:09:31.160
But the future of war is also going to be

00:09:31.160 --> 00:09:33.160
a YouTube war.

00:09:33.160 --> 00:09:35.160
That is, our new technologies don't merely

00:09:35.160 --> 00:09:37.160
remove humans from risk.

00:09:37.160 --> 00:09:40.160
They also record everything that they see.

00:09:40.160 --> 00:09:43.160
So they don't just delink the public:

00:09:43.160 --> 00:09:46.160
they reshape its relationship with war.

00:09:46.160 --> 00:09:48.160
There's already several thousand

00:09:48.160 --> 00:09:50.160
video clips of combat footage from Iraq

00:09:50.160 --> 00:09:52.160
on YouTube right now,

00:09:52.160 --> 00:09:54.160
most of it gathered by drones.

00:09:54.160 --> 00:09:56.160
Now, this could be a good thing.

00:09:56.160 --> 00:09:58.160
It could be building connections between

00:09:58.160 --> 00:10:00.160
the home front and the war front

00:10:00.160 --> 00:10:02.160
as never before.

00:10:02.160 --> 00:10:04.160
But remember, this is taking place

00:10:04.160 --> 00:10:07.160
in our strange, weird world, and so

00:10:07.160 --> 00:10:09.160
inevitably the ability to download these

00:10:09.160 --> 00:10:11.160
video clips to, you know, your iPod

00:10:11.160 --> 00:10:14.160
or your Zune gives you

00:10:14.160 --> 00:10:18.160
the ability to turn it into entertainment.

00:10:18.160 --> 00:10:20.160
Soldiers have a name for these clips.

00:10:20.160 --> 00:10:22.160
They call it war porn.

00:10:22.160 --> 00:10:24.160
The typical one that I was sent was

00:10:24.160 --> 00:10:26.160
an email that had an attachment of

00:10:26.160 --> 00:10:28.160
video of a Predator strike taking out

00:10:28.160 --> 00:10:30.160
an enemy site. Missile hits,

00:10:30.160 --> 00:10:33.160
bodies burst into the air with the explosion.

00:10:33.160 --> 00:10:35.160
It was set to music.

00:10:35.160 --> 00:10:37.160
It was set to the pop song

00:10:37.160 --> 00:10:40.160
"I Just Want To Fly" by Sugar Ray.

00:10:40.160 --> 00:10:43.160
This ability to watch more

00:10:43.160 --> 00:10:46.160
but experience less creates a wrinkle

00:10:46.160 --> 00:10:48.160
in the public's relationship with war.

00:10:48.160 --> 00:10:50.160
I think about this with a sports parallel.

00:10:50.160 --> 00:10:53.160
It's like the difference between

00:10:53.160 --> 00:10:56.160
watching an NBA game, a professional

00:10:56.160 --> 00:10:59.160
basketball game on TV, where the athletes

00:10:59.160 --> 00:11:01.160
are tiny figures on the screen, and

00:11:01.160 --> 00:11:04.160
being at that basketball game in person

00:11:04.160 --> 00:11:06.160
and realizing what someone seven feet

00:11:06.160 --> 00:11:08.160
really does look like.

00:11:08.160 --> 00:11:10.160
But we have to remember,

00:11:10.160 --> 00:11:12.160
these are just the clips.

00:11:12.160 --> 00:11:14.160
These are just the ESPN SportsCenter

00:11:14.160 --> 00:11:16.160
version of the game. They lose the context.

00:11:16.160 --> 00:11:18.160
They lose the strategy.

00:11:18.160 --> 00:11:20.160
They lose the humanity. War just

00:11:20.160 --> 00:11:23.160
becomes slam dunks and smart bombs.

00:11:23.160 --> 00:11:26.160
Now the irony of all this is that

00:11:26.160 --> 00:11:28.160
while the future of war may involve

00:11:28.160 --> 00:11:30.160
more and more machines,

00:11:30.160 --> 00:11:32.160
it's our human psychology that's driving

00:11:32.160 --> 00:11:34.160
all of this, it's our human failings

00:11:34.160 --> 00:11:36.160
that are leading to these wars.

00:11:36.160 --> 00:11:38.160
So one example of this that has

00:11:38.160 --> 00:11:40.160
big resonance in the policy realm is

00:11:40.160 --> 00:11:42.160
how this plays out on our very real

00:11:42.160 --> 00:11:44.160
war of ideas that we're fighting

00:11:44.160 --> 00:11:46.160
against radical groups.

00:11:46.160 --> 00:11:48.160
What is the message that we think we are

00:11:48.160 --> 00:11:50.160
sending with these machines versus what

00:11:50.160 --> 00:11:53.160
is being received in terms of the message.

00:11:53.160 --> 00:11:55.160
So one of the people that I met was

00:11:55.160 --> 00:11:57.160
a senior Bush Administration official,

00:11:57.160 --> 00:11:59.160
who had this to say about

00:11:59.160 --> 00:12:01.160
our unmanning of war:

00:12:01.160 --> 00:12:03.160
"It plays to our strength. The thing that

00:12:03.160 --> 00:12:05.160
scares people is our technology."

00:12:05.160 --> 00:12:07.160
But when you go out and meet with people,

00:12:07.160 --> 00:12:09.160
for example in Lebanon, it's a very

00:12:09.160 --> 00:12:11.160
different story. One of the people

00:12:11.160 --> 00:12:13.160
I met with there was a news editor, and

00:12:13.160 --> 00:12:15.160
we're talking as a drone is flying above him,

00:12:15.160 --> 00:12:17.162
and this is what he had to say.

00:12:17.162 --> 00:12:19.160
"This is just another sign of the coldhearted

00:12:19.160 --> 00:12:22.166
cruel Israelis and Americans,

00:12:22.166 --> 00:12:24.158
who are cowards because

00:12:24.158 --> 00:12:26.164
they send out machines to fight us.

00:12:26.164 --> 00:12:28.156
They don't want to fight us like real men,

00:12:28.156 --> 00:12:30.163
but they're afraid to fight,

00:12:30.163 --> 00:12:32.154
so we just have to kill a few of their soldiers

00:12:32.154 --> 00:12:35.159
to defeat them."

00:12:35.159 --> 00:12:37.166
The future of war also is featuring

00:12:37.166 --> 00:12:39.143
a new type of warrior,

00:12:39.159 --> 00:12:42.165
and it's actually redefining the experience

00:12:42.165 --> 00:12:44.163
of going to war.

00:12:44.163 --> 00:12:46.139
You can call this a cubicle warrior.

00:12:46.154 --> 00:12:48.161
This is what one Predator drone pilot

00:12:48.161 --> 00:12:50.168
described of his experience fighting

00:12:50.168 --> 00:12:53.157
in the Iraq War while never leaving Nevada.

00:12:53.157 --> 00:12:55.162
"You're going to war for 12 hours,

00:12:55.162 --> 00:12:57.153
shooting weapons at targets,

00:12:57.168 --> 00:13:00.159
directing kills on enemy combatants,

00:13:00.159 --> 00:13:02.165
and then you get in the car

00:13:02.165 --> 00:13:04.147
and you drive home and within 20 minutes,

00:13:04.163 --> 00:13:06.154
you're sitting at the dinner table

00:13:06.169 --> 00:13:08.162
talking to your kids about their homework."

00:13:08.162 --> 00:13:10.153
Now, the psychological balancing

00:13:10.168 --> 00:13:12.161
of those experiences is incredibly tough,

00:13:12.161 --> 00:13:15.153
and in fact those drone pilots have

00:13:15.153 --> 00:13:17.161
higher rates of PTSD than many

00:13:17.161 --> 00:13:20.166
of the units physically in Iraq.

00:13:20.166 --> 00:13:22.158
But some have worries that this

00:13:22.158 --> 00:13:24.152
disconnection will lead to something else,

00:13:24.167 --> 00:13:26.165
that it might make the contemplation of war

00:13:26.165 --> 00:13:28.170
crimes a lot easier when you have

00:13:28.170 --> 00:13:30.169
this distance. "It's like a video game,"

00:13:30.169 --> 00:13:32.168
is what one young pilot described to me

00:13:32.168 --> 00:13:34.128
of taking out enemy troops from afar.

00:13:34.158 --> 00:13:37.153
As anyone who's played Grand Theft Auto

00:13:37.168 --> 00:13:40.168
knows, we do things in the video world

00:13:40.168 --> 00:13:43.164
that we wouldn't do face to face.

00:13:43.164 --> 00:13:45.162
So much of what you're hearing from me

00:13:45.162 --> 00:13:47.155
is that there's another side

00:13:47.155 --> 00:13:49.163
to technologic revolutions,

00:13:49.163 --> 00:13:51.157
and that it's shaping our present

00:13:51.157 --> 00:13:54.158
and maybe will shape our future of war.

00:13:54.158 --> 00:13:56.159
Moore's Law is operative,

00:13:56.159 --> 00:13:58.157
but so's Murphy's Law.

00:13:58.157 --> 00:14:00.162
The fog of war isn't being lifted.

00:14:00.162 --> 00:14:02.172
The enemy has a vote.

00:14:02.172 --> 00:14:04.161
We're gaining incredible new capabilities,

00:14:04.161 --> 00:14:06.167
but we're also seeing and experiencing

00:14:06.167 --> 00:14:08.167
new human dilemmas. Now,

00:14:08.167 --> 00:14:10.166
sometimes these are just "oops" moments,

00:14:10.166 --> 00:14:12.158
which is what the head of a robotics

00:14:12.158 --> 00:14:14.169
company described it, you just have

00:14:14.169 --> 00:14:16.169
"oops" moments. Well, what are

00:14:16.169 --> 00:14:18.165
"oops" moments with robots in war?

00:14:18.165 --> 00:14:20.166
Well, sometimes they're funny. Sometimes,

00:14:20.166 --> 00:14:22.168
they're like that scene from the

00:14:22.168 --> 00:14:24.169
Eddie Murphy movie "Best Defense,"

00:14:24.169 --> 00:14:26.170
playing out in reality, where they tested out

00:14:26.170 --> 00:14:28.158
a machine gun-armed robot, and during

00:14:28.158 --> 00:14:30.159
the demonstration it started spinning

00:14:30.159 --> 00:14:33.168
in a circle and pointed its machine gun

00:14:33.168 --> 00:14:36.163
at the reviewing stand of VIPs.

00:14:36.163 --> 00:14:38.156
Fortunately the weapon wasn't loaded

00:14:38.156 --> 00:14:40.166
and no one was hurt, but other times

00:14:40.166 --> 00:14:42.158
"oops" moments are tragic,

00:14:42.158 --> 00:14:44.160
such as last year in South Africa, where

00:14:44.160 --> 00:14:47.159
an anti-aircraft cannon had a

00:14:47.159 --> 00:14:50.158
"software glitch," and actually did turn on

00:14:50.158 --> 00:14:53.162
and fired, and nine soldiers were killed.

00:14:53.162 --> 00:14:56.159
We have new wrinkles in the laws of war

00:14:56.159 --> 00:14:58.156
and accountability. What do we do

00:14:58.156 --> 00:15:00.166
with things like unmanned slaughter?

00:15:00.166 --> 00:15:02.164
What is unmanned slaughter?

00:15:02.164 --> 00:15:04.164
We've already had three instances of

00:15:04.164 --> 00:15:06.164
Predator drone strikes where we thought

00:15:06.164 --> 00:15:08.158
we got bin Laden, and it turned out

00:15:08.158 --> 00:15:10.160
not to be the case.

00:15:10.160 --> 00:15:12.168
And this is where we're at right now.

00:15:12.168 --> 00:15:14.168
This is not even talking about armed,

00:15:14.168 --> 00:15:16.157
autonomous systems

00:15:16.157 --> 00:15:18.170
with full authority to use force.

00:15:18.170 --> 00:15:20.164
And do not believe that that isn't coming.

00:15:20.164 --> 00:15:22.171
During my research I came across

00:15:22.171 --> 00:15:24.156
four different Pentagon projects

00:15:24.156 --> 00:15:26.168
on different aspects of that.

00:15:26.168 --> 00:15:28.170
And so you have this question:

00:15:28.170 --> 00:15:30.163
what does this lead to issues like

00:15:30.163 --> 00:15:32.155
war crimes? Robots are emotionless, so

00:15:32.155 --> 00:15:35.167
they don't get upset if their buddy is killed.

00:15:35.167 --> 00:15:37.161
They don't commit crimes of rage

00:15:37.161 --> 00:15:39.168
and revenge.

00:15:39.168 --> 00:15:42.165
But robots are emotionless.

00:15:42.165 --> 00:15:44.164
They see an 80-year-old grandmother

00:15:44.164 --> 00:15:46.164
in a wheelchair the same way they see

00:15:46.164 --> 00:15:49.166
a T-80 tank: they're both

00:15:49.166 --> 00:15:52.159
just a series of zeroes and ones.

00:15:52.159 --> 00:15:55.162
And so we have this question to figure out:

00:15:55.162 --> 00:15:57.165
How do we catch up our 20th century

00:15:57.165 --> 00:15:59.154
laws of war, that are so old right now

00:15:59.154 --> 00:16:02.162
that they could qualify for Medicare,

00:16:02.162 --> 00:16:05.170
to these 21st century technologies?

00:16:05.170 --> 00:16:08.157
And so, in conclusion, I've talked about

00:16:08.157 --> 00:16:11.168
what seems the future of war,

00:16:11.168 --> 00:16:13.158
but notice that I've only used

00:16:13.158 --> 00:16:15.170
real world examples and you've only seen

00:16:15.170 --> 00:16:17.171
real world pictures and videos.

00:16:17.171 --> 00:16:19.171
And so this sets a great challenge for

00:16:19.171 --> 00:16:21.157
all of us that we have to worry about well

00:16:21.157 --> 00:16:23.166
before you have to worry about your

00:16:23.166 --> 00:16:25.167
Roomba sucking the life away from you.

00:16:25.167 --> 00:16:27.166
Are we going to let the fact that what's

00:16:27.166 --> 00:16:30.166
unveiling itself right now in war

00:16:30.166 --> 00:16:33.157
sounds like science fiction and therefore

00:16:33.157 --> 00:16:35.162
keeps us in denial?

00:16:35.162 --> 00:16:37.166
Are we going to face the reality

00:16:37.166 --> 00:16:39.159
of 21st century war?

00:16:39.159 --> 00:16:41.160
Is our generation going to make the same

00:16:41.160 --> 00:16:43.169
mistake that a past generation did

00:16:43.169 --> 00:16:45.167
with atomic weaponry, and not deal with

00:16:45.167 --> 00:16:47.162
the issues that surround it until

00:16:47.162 --> 00:16:49.169
Pandora's box is already opened up?

00:16:49.169 --> 00:16:51.167
Now, I could be wrong on this, and

00:16:51.167 --> 00:16:53.171
one Pentagon robot scientist told me

00:16:53.171 --> 00:16:55.165
that I was. He said, "There's no real

00:16:55.165 --> 00:16:57.160
social, ethical, moral issues when it comes

00:16:57.160 --> 00:16:59.158
to robots.

00:16:59.158 --> 00:17:01.159
That is," he added, "unless the machine

00:17:01.159 --> 00:17:04.169
kills the wrong people repeatedly.

00:17:04.169 --> 00:17:07.158
Then it's just a product recall issue."

00:17:07.158 --> 00:17:10.169
And so the ending point for this is

00:17:10.169 --> 00:17:15.171
that actually, we can turn to Hollywood.

00:17:15.171 --> 00:17:17.167
A few years ago, Hollywood gathered

00:17:17.167 --> 00:17:20.173
all the top characters and created

00:17:20.173 --> 00:17:22.170
a list of the top 100 heroes and

00:17:22.170 --> 00:17:25.160
top 100 villains of all of Hollywood history,

00:17:25.160 --> 00:17:27.166
the characters that represented the best

00:17:27.166 --> 00:17:29.158
and worst of humanity.

00:17:29.158 --> 00:17:33.173
Only one character made it onto both lists:

00:17:33.173 --> 00:17:36.169
The Terminator, a robot killing machine.

00:17:36.169 --> 00:17:38.157
And so that points to the fact that

00:17:38.157 --> 00:17:40.170
our machines can be used

00:17:40.170 --> 00:17:42.169
for both good and evil, but for me

00:17:42.169 --> 00:17:44.165
it points to the fact that there's a duality

00:17:44.165 --> 00:17:47.165
of humans as well.

00:17:47.165 --> 00:17:49.163
This week is a celebration

00:17:49.163 --> 00:17:51.162
of our creativity. Our creativity

00:17:51.162 --> 00:17:53.160
has taken our species to the stars.

00:17:53.160 --> 00:17:55.167
Our creativity has created works of arts

00:17:55.167 --> 00:17:58.163
and literature to express our love.

00:17:58.163 --> 00:18:00.161
And now, we're using our creativity

00:18:00.161 --> 00:18:02.154
in a certain direction, to build fantastic

00:18:02.169 --> 00:18:05.156
machines with incredible capabilities,

00:18:05.156 --> 00:18:07.165
maybe even one day

00:18:07.165 --> 00:18:10.162
an entirely new species.

00:18:10.162 --> 00:18:12.167
But one of the main reasons that we're

00:18:12.167 --> 00:18:14.165
doing that is because of our drive

00:18:14.165 --> 00:18:17.165
to destroy each other, and so the question

00:18:17.165 --> 00:18:19.159
we all should ask:

00:18:19.159 --> 00:18:21.159
is it our machines, or is it us

00:18:21.159 --> 00:18:23.166
that's wired for war?

00:18:23.166 --> 00:18:25.264
Thank you. (Applause)

