WEBVTT
Kind: captions
Language: en

00:00:15.260 --> 00:00:17.236
We grew up

00:00:17.260 --> 00:00:20.236
interacting with the physical
objects around us.

00:00:20.260 --> 00:00:23.660
There are an enormous number of them
that we use every day.

00:00:24.553 --> 00:00:27.236
Unlike most of our computing devices,

00:00:27.260 --> 00:00:29.513
these objects are much more fun to use.

00:00:31.180 --> 00:00:33.236
When you talk about objects,

00:00:33.260 --> 00:00:36.236
one other thing automatically
comes attached to that thing,

00:00:36.260 --> 00:00:38.236
and that is gestures:

00:00:38.260 --> 00:00:40.236
how we manipulate these objects,

00:00:40.260 --> 00:00:43.236
how we use these objects in everyday life.

00:00:43.260 --> 00:00:46.236
We use gestures not only to interact
with these objects,

00:00:46.260 --> 00:00:48.546
but we also use them
to interact with each other.

00:00:48.570 --> 00:00:52.236
A gesture of "Namaste!",
maybe, to respect someone, or maybe,

00:00:52.260 --> 00:00:54.689
in India I don't need to teach
a kid that this means

00:00:54.713 --> 00:00:56.236
"four runs" in cricket.

00:00:56.260 --> 00:00:58.783
It comes as a part
of our everyday learning.

00:00:59.716 --> 00:01:03.236
So, I am very interested,
from the beginning,

00:01:03.260 --> 00:01:07.236
how our knowledge
about everyday objects and gestures,

00:01:07.260 --> 00:01:09.236
and how we use these objects,

00:01:09.260 --> 00:01:12.236
can be leveraged to our interactions
with the digital world.

00:01:12.260 --> 00:01:15.236
Rather than using a keyboard and mouse,

00:01:15.260 --> 00:01:18.236
why can I not use my computer

00:01:18.260 --> 00:01:21.236
in the same way that I interact
in the physical world?

00:01:21.260 --> 00:01:24.236
So, I started this exploration
around eight years back,

00:01:24.260 --> 00:01:27.236
and it literally started
with a mouse on my desk.

00:01:27.260 --> 00:01:33.236
Rather than using it for my computer,
I actually opened it.

00:01:33.260 --> 00:01:35.451
Most of you might be aware
that, in those days,

00:01:35.475 --> 00:01:37.475
the mouse used to come with a ball inside,

00:01:37.499 --> 00:01:39.236
and there were two rollers

00:01:39.260 --> 00:01:42.236
that actually guide the computer
where the ball is moving,

00:01:42.260 --> 00:01:44.356
and, accordingly,
where the mouse is moving.

00:01:44.380 --> 00:01:47.236
So, I was interested in these two rollers,

00:01:47.260 --> 00:01:50.641
and I actually wanted more, so I borrowed
another mouse from a friend --

00:01:50.665 --> 00:01:52.236
never returned to him --

00:01:52.260 --> 00:01:54.236
and I now had four rollers.

00:01:54.260 --> 00:01:57.236
Interestingly, what I did
with these rollers is,

00:01:57.260 --> 00:02:02.236
basically, I took them off of these mouses
and then put them in one line.

00:02:02.260 --> 00:02:05.236
It had some strings
and pulleys and some springs.

00:02:05.260 --> 00:02:08.236
What I got is basically
a gesture-interface device

00:02:08.260 --> 00:02:12.236
that actually acts
as a motion-sensing device

00:02:12.260 --> 00:02:14.236
made for two dollars.

00:02:14.260 --> 00:02:17.236
So, here, whatever movement
I do in my physical world

00:02:17.260 --> 00:02:20.236
is actually replicated
inside the digital world

00:02:20.260 --> 00:02:23.356
just using this small device
that I made, around eight years back,

00:02:23.380 --> 00:02:25.236
in 2000.

00:02:25.260 --> 00:02:27.927
Because I was interested
in integrating these two worlds,

00:02:27.951 --> 00:02:29.236
I thought of sticky notes.

00:02:29.260 --> 00:02:32.236
I thought, "Why can I not connect

00:02:32.260 --> 00:02:34.403
the normal interface
of a physical sticky note

00:02:34.427 --> 00:02:36.236
to the digital world?"

00:02:36.260 --> 00:02:38.408
A message written
on a sticky note to my mom,

00:02:38.432 --> 00:02:39.636
on paper,

00:02:39.660 --> 00:02:41.236
can come to an SMS,

00:02:41.260 --> 00:02:43.236
or maybe a meeting reminder

00:02:43.260 --> 00:02:45.451
automatically syncs
with my digital calendar --

00:02:45.475 --> 00:02:48.236
a to-do list that automatically
syncs with you.

00:02:48.260 --> 00:02:51.236
But you can also search
in the digital world,

00:02:51.260 --> 00:02:53.236
or maybe you can write a query, saying,

00:02:53.260 --> 00:02:55.236
"What is Dr. Smith's address?"

00:02:55.260 --> 00:02:57.451
and this small system
actually prints it out --

00:02:57.475 --> 00:02:59.952
so it actually acts like a paper
input-output system,

00:02:59.976 --> 00:03:02.761
just made out of paper.

00:03:05.260 --> 00:03:07.236
In another exploration,

00:03:07.260 --> 00:03:10.236
I thought of making a pen
that can draw in three dimensions.

00:03:10.260 --> 00:03:14.236
So, I implemented this pen
that can help designers and architects

00:03:14.260 --> 00:03:16.236
not only think in three dimensions,

00:03:16.260 --> 00:03:18.236
but they can actually draw,

00:03:18.260 --> 00:03:20.308
so that it's more intuitive
to use that way.

00:03:20.332 --> 00:03:22.380
Then I thought,
"Why not make a Google Map,

00:03:22.404 --> 00:03:24.236
but in the physical world?"

00:03:24.260 --> 00:03:27.236
Rather than typing a keyword
to find something,

00:03:27.260 --> 00:03:29.236
I put my objects on top of it.

00:03:29.260 --> 00:03:32.451
If I put a boarding pass, it will show me
where the flight gate is.

00:03:32.475 --> 00:03:35.236
A coffee cup will show
where you can find more coffee,

00:03:35.260 --> 00:03:37.236
or where you can trash the cup.

00:03:37.260 --> 00:03:40.236
So, these were some
of the earlier explorations I did

00:03:40.260 --> 00:03:43.260
because the goal was to connect
these two worlds seamlessly.

00:03:44.260 --> 00:03:46.236
Among all these experiments,

00:03:46.260 --> 00:03:48.236
there was one thing in common:

00:03:48.260 --> 00:03:51.765
I was trying to bring
a part of the physical world

00:03:51.789 --> 00:03:53.287
to the digital world.

00:03:53.311 --> 00:03:55.236
I was taking some part of the objects,

00:03:55.260 --> 00:03:58.237
or any of the intuitiveness of real life,

00:03:58.261 --> 00:04:00.450
and bringing them to the digital world,

00:04:00.474 --> 00:04:04.499
because the goal was to make
our computing interfaces more intuitive.

00:04:04.523 --> 00:04:09.236
But then I realized that we humans
are not actually interested in computing.

00:04:09.260 --> 00:04:12.236
What we are interested in is information.

00:04:12.260 --> 00:04:14.236
We want to know about things.

00:04:14.260 --> 00:04:16.641
We want to know about
dynamic things going around.

00:04:16.665 --> 00:04:21.236
So I thought, around last year --
in the beginning of the last year --

00:04:21.260 --> 00:04:24.737
I started thinking, "Why can I not take
this approach in the reverse way?"

00:04:25.379 --> 00:04:27.436
Maybe, "How about I take my digital world

00:04:27.460 --> 00:04:32.236
and paint the physical world
with that digital information?"

00:04:33.414 --> 00:04:37.036
Because pixels are actually, right now,
confined in these rectangular devices

00:04:37.060 --> 00:04:38.807
that fit in our pockets.

00:04:38.831 --> 00:04:41.236
Why can I not remove this confine

00:04:41.260 --> 00:04:44.236
and take that to my everyday
objects, everyday life

00:04:44.260 --> 00:04:46.403
so that I don't need
to learn the new language

00:04:46.427 --> 00:04:48.224
for interacting with those pixels?

00:04:49.474 --> 00:04:52.236
So, in order to realize this dream,

00:04:52.260 --> 00:04:55.236
I actually thought of putting
a big-size projector on my head.

00:04:55.260 --> 00:04:58.499
I think that's why this is called
a head-mounted projector, isn't it?

00:04:58.523 --> 00:05:00.236
I took it very literally,

00:05:00.260 --> 00:05:02.236
and took my bike helmet,

00:05:02.260 --> 00:05:05.641
put a little cut over there so that
the projector actually fits nicely.

00:05:05.665 --> 00:05:07.236
So now, what I can do --

00:05:07.260 --> 00:05:11.065
I can augment the world around me
with this digital information.

00:05:11.918 --> 00:05:13.136
But later,

00:05:13.160 --> 00:05:15.319
I realized that I actually
wanted to interact

00:05:15.343 --> 00:05:16.936
with those digital pixels, also.

00:05:16.960 --> 00:05:20.236
So I put a small camera over there
that acts as a digital eye.

00:05:20.260 --> 00:05:22.236
Later, we moved to a much better,

00:05:22.260 --> 00:05:24.260
consumer-oriented pendant version of that,

00:05:24.284 --> 00:05:27.236
that many of you now know
as the SixthSense device.

00:05:27.260 --> 00:05:30.236
But the most interesting thing
about this particular technology

00:05:30.260 --> 00:05:34.236
is that you can carry
your digital world with you

00:05:34.260 --> 00:05:36.236
wherever you go.

00:05:36.260 --> 00:05:39.236
You can start using any surface,
any wall around you,

00:05:39.260 --> 00:05:41.236
as an interface.

00:05:41.260 --> 00:05:44.236
The camera is actually tracking
all your gestures.

00:05:44.260 --> 00:05:46.236
Whatever you're doing with your hands,

00:05:46.260 --> 00:05:48.236
it's understanding that gesture.

00:05:48.260 --> 00:05:50.836
And, actually, if you see,
there are some color markers

00:05:50.860 --> 00:05:53.336
that in the beginning version
we are using with it.

00:05:53.360 --> 00:05:55.236
You can start painting on any wall.

00:05:55.260 --> 00:05:58.236
You stop by a wall,
and start painting on that wall.

00:05:58.260 --> 00:06:00.403
But we are not only tracking
one finger, here.

00:06:00.427 --> 00:06:04.236
We are giving you the freedom
of using all of both of your hands,

00:06:04.260 --> 00:06:07.403
so you can actually use both of your hands
to zoom into or zoom out

00:06:07.427 --> 00:06:09.403
of a map just by pinching all present.

00:06:09.427 --> 00:06:13.236
The camera is actually doing --
just, getting all the images --

00:06:13.260 --> 00:06:16.236
is doing the edge recognition
and also the color recognition

00:06:16.260 --> 00:06:19.236
and so many other small algorithms
are going on inside.

00:06:19.260 --> 00:06:21.260
So, technically,
it's a little bit complex,

00:06:21.284 --> 00:06:24.760
but it gives you an output which is more
intuitive to use, in some sense.

00:06:24.784 --> 00:06:27.636
But I'm more excited that you can
actually take it outside.

00:06:27.660 --> 00:06:30.236
Rather than getting your camera
out of your pocket,

00:06:30.260 --> 00:06:33.236
you can just do the gesture
of taking a photo,

00:06:33.260 --> 00:06:35.236
and it takes a photo for you.

00:06:35.260 --> 00:06:39.236
(Applause)

00:06:39.260 --> 00:06:40.260
Thank you.

00:06:40.859 --> 00:06:43.236
And later I can find a wall, anywhere,

00:06:43.260 --> 00:06:45.236
and start browsing those photos

00:06:45.260 --> 00:06:47.936
or maybe, "OK, I want to modify
this photo a little bit

00:06:47.960 --> 00:06:49.946
and send it as an email to a friend."

00:06:49.970 --> 00:06:52.236
So, we are looking for an era

00:06:52.260 --> 00:06:55.236
where computing will actually merge
with the physical world.

00:06:55.260 --> 00:06:58.236
And, of course,
if you don't have any surface,

00:06:58.260 --> 00:07:01.236
you can start using your palm
for simple operations.

00:07:01.260 --> 00:07:03.737
Here, I'm dialing a phone number
just using my hand.

00:07:07.140 --> 00:07:10.236
The camera is actually not
only understanding your hand movements,

00:07:10.260 --> 00:07:11.436
but, interestingly,

00:07:11.460 --> 00:07:14.699
is also able to understand what objects
you are holding in your hand.

00:07:15.269 --> 00:07:19.236
For example, in this case,

00:07:19.260 --> 00:07:21.236
the book cover is matched

00:07:21.260 --> 00:07:24.236
with so many thousands,
or maybe millions of books online,

00:07:24.260 --> 00:07:26.236
and checking out which book it is.

00:07:26.260 --> 00:07:27.736
Once it has that information,

00:07:27.760 --> 00:07:29.636
it finds out more reviews about that,

00:07:29.660 --> 00:07:32.236
or maybe New York Times
has a sound overview on that,

00:07:32.260 --> 00:07:34.356
so you can actually hear,
on a physical book,

00:07:34.380 --> 00:07:36.236
a review as sound.

00:07:36.260 --> 00:07:38.436
(Video) Famous talk
at Harvard University --

00:07:38.460 --> 00:07:42.236
This was Obama's visit last week to MIT.

00:07:42.260 --> 00:07:45.725
(Video) And particularly I want
to thank two outstanding MIT --

00:07:45.749 --> 00:07:48.783
Pranav Mistry: So, I was seeing
the live [video] of his talk,

00:07:48.807 --> 00:07:50.749
outside, on just a newspaper.

00:07:51.260 --> 00:07:54.236
Your newspaper will show you
live weather information

00:07:54.260 --> 00:07:56.866
rather than having it updated.

00:07:56.890 --> 00:07:59.737
You have to check your computer
in order to do that, right?

00:07:59.761 --> 00:08:04.236
(Applause)

00:08:04.260 --> 00:08:07.236
When I'm going back,
I can just use my boarding pass

00:08:07.260 --> 00:08:09.356
to check how much my flight
has been delayed,

00:08:09.380 --> 00:08:11.236
because at that particular time,

00:08:11.260 --> 00:08:13.236
I'm not feeling like opening my iPhone,

00:08:13.260 --> 00:08:15.236
and checking out a particular icon.

00:08:15.260 --> 00:08:18.394
And I think this technology
will not only change the way --

00:08:18.418 --> 00:08:19.394
(Laughter)

00:08:19.418 --> 00:08:20.436
Yes.

00:08:20.460 --> 00:08:22.938
It will change the way
we interact with people, also,

00:08:22.962 --> 00:08:24.477
not only the physical world.

00:08:24.501 --> 00:08:27.236
The fun part is, I'm going
to the Boston metro,

00:08:27.260 --> 00:08:32.236
and playing a pong game inside the train
on the ground, right?

00:08:32.260 --> 00:08:33.336
(Laughter)

00:08:33.360 --> 00:08:35.456
And I think the imagination
is the only limit

00:08:35.480 --> 00:08:37.236
of what you can think of

00:08:37.260 --> 00:08:39.736
when this kind of technology
merges with real life.

00:08:39.760 --> 00:08:41.636
But many of you argue, actually,

00:08:41.660 --> 00:08:44.336
that all of our work is not
only about physical objects.

00:08:44.360 --> 00:08:47.336
We actually do lots
of accounting and paper editing

00:08:47.360 --> 00:08:49.651
and all those kinds of things;
what about that?

00:08:49.675 --> 00:08:53.236
And many of you are excited
about the next-generation tablet computers

00:08:53.260 --> 00:08:55.236
to come out in the market.

00:08:55.260 --> 00:08:57.236
So, rather than waiting for that,

00:08:57.260 --> 00:09:00.236
I actually made my own,
just using a piece of paper.

00:09:00.260 --> 00:09:02.260
So, what I did here
is remove the camera --

00:09:02.284 --> 00:09:06.236
All the webcam cameras have
a microphone inside the camera.

00:09:06.260 --> 00:09:09.236
I removed the microphone from that,

00:09:09.260 --> 00:09:11.236
and then just pinched that --

00:09:11.260 --> 00:09:14.236
like I just made a clip
out of the microphone --

00:09:14.260 --> 00:09:18.236
and clipped that to a piece of paper,
any paper that you found around.

00:09:18.260 --> 00:09:21.236
So now the sound of the touch

00:09:21.260 --> 00:09:24.236
is getting me when exactly
I'm touching the paper.

00:09:24.260 --> 00:09:28.236
But the camera is actually tracking
where my fingers are moving.

00:09:28.260 --> 00:09:31.236
You can of course watch movies.

00:09:31.260 --> 00:09:34.236
(Video) Good afternoon.
My name is Russell,

00:09:34.260 --> 00:09:37.236
and I am a Wilderness
Explorer in Tribe 54."

00:09:37.260 --> 00:09:40.236
PM: And you can of course play games.

00:09:40.260 --> 00:09:43.236
(Car engine)

00:09:43.260 --> 00:09:46.594
Here, the camera is actually understanding
how you're holding the paper

00:09:46.618 --> 00:09:48.236
and playing a car-racing game.

00:09:48.260 --> 00:09:51.260
(Applause)

00:09:52.656 --> 00:09:55.434
Many of you already must have
thought, OK, you can browse.

00:09:55.458 --> 00:09:57.736
Yeah. Of course you can
browse to any websites

00:09:57.760 --> 00:10:00.436
or you can do all sorts
of computing on a piece of paper

00:10:00.460 --> 00:10:01.636
wherever you need it.

00:10:01.660 --> 00:10:04.236
So, more interestingly,

00:10:04.260 --> 00:10:07.236
I'm interested in how we can
take that in a more dynamic way.

00:10:07.260 --> 00:10:10.236
When I come back to my desk,
I can just pinch that information

00:10:10.260 --> 00:10:12.236
back to my desktop

00:10:12.260 --> 00:10:15.236
so I can use my full-size computer.

00:10:15.260 --> 00:10:17.236
(Applause)

00:10:17.260 --> 00:10:20.236
And why only computers?
We can just play with papers.

00:10:20.260 --> 00:10:23.236
Paper world is interesting to play with.

00:10:23.260 --> 00:10:25.236
Here, I'm taking a part of a document,

00:10:25.260 --> 00:10:29.236
and putting over here a second part
from a second place,

00:10:29.260 --> 00:10:34.236
and I'm actually modifying the information
that I have over there.

00:10:34.260 --> 00:10:39.236
Yeah. And I say, "OK, this looks nice,
let me print it out, that thing."

00:10:39.260 --> 00:10:41.641
So I now have a print-out of that thing.

00:10:41.665 --> 00:10:43.989
So the workflow is more intuitive,

00:10:44.013 --> 00:10:47.236
the way we used to do it
maybe 20 years back,

00:10:47.260 --> 00:10:50.236
rather than now switching
between these two worlds.

00:10:50.260 --> 00:10:53.236
So, as a last thought,

00:10:53.260 --> 00:10:57.636
I think that integrating
information to everyday objects

00:10:57.660 --> 00:11:01.236
will not only help us to get rid
of the digital divide,

00:11:01.260 --> 00:11:03.236
the gap between these two worlds,

00:11:03.260 --> 00:11:05.236
but will also help us, in some way,

00:11:05.260 --> 00:11:07.236
to stay human,

00:11:07.260 --> 00:11:10.260
to be more connected
to our physical world.

00:11:13.668 --> 00:11:16.236
And it will actually help us
not end up being machines

00:11:16.260 --> 00:11:17.978
sitting in front of other machines.

00:11:18.767 --> 00:11:21.236
That's all. Thank you.

00:11:21.260 --> 00:11:35.236
(Applause)

00:11:35.260 --> 00:11:36.436
Thank you.

00:11:36.460 --> 00:11:39.236
(Applause)

00:11:39.260 --> 00:11:43.236
Chris Anderson: So, Pranav,
first of all, you're a genius.

00:11:43.260 --> 00:11:46.236
This is incredible, really.

00:11:46.260 --> 00:11:49.360
What are you doing with this?
Is there a company being planned?

00:11:49.384 --> 00:11:51.236
Or is this research forever, or what?

00:11:51.260 --> 00:11:53.536
Pranav Mistry: So, there are
lots of companies,

00:11:53.560 --> 00:11:56.556
sponsor companies of Media Lab
interested in taking this ahead

00:11:56.580 --> 00:11:57.766
in one or another way.

00:11:57.790 --> 00:11:59.763
Companies like mobile-phone operators

00:11:59.787 --> 00:12:02.661
want to take this in a different way
than the NGOs in India,

00:12:02.685 --> 00:12:04.861
thinking, "Why can we only
have 'Sixth Sense'?

00:12:04.885 --> 00:12:08.336
We should have a 'Fifth Sense'
for missing-sense people who cannot speak.

00:12:08.360 --> 00:12:11.651
This technology can be used for them
to speak out in a different way

00:12:11.675 --> 00:12:12.951
maybe a speaker system."

00:12:12.975 --> 00:12:15.436
CA: What are your own plans?
Are you staying at MIT,

00:12:15.460 --> 00:12:17.536
or are you going to do
something with this?

00:12:17.560 --> 00:12:20.089
PM: I'm trying to make this
more available to people

00:12:20.113 --> 00:12:22.789
so that anyone can develop
their own SixthSense device,

00:12:22.813 --> 00:12:26.236
because the hardware is actually
not that hard to manufacture

00:12:26.260 --> 00:12:28.236
or hard to make your own.

00:12:28.260 --> 00:12:30.832
We will provide all the open source
software for them,

00:12:30.856 --> 00:12:32.236
maybe starting next month.

00:12:32.260 --> 00:12:34.236
CA: Open source? Wow.

00:12:34.260 --> 00:12:39.236
(Applause)

00:12:39.260 --> 00:12:42.689
CA: Are you going to come back to India
with some of this, at some point?

00:12:42.713 --> 00:12:44.236
PM: Yeah. Yes, yes, of course.

00:12:44.260 --> 00:12:46.236
CA: What are your plans? MIT? India?

00:12:46.260 --> 00:12:48.736
How are you going to split
your time going forward?

00:12:48.760 --> 00:12:51.236
PM: There is a lot of energy here.
Lots of learning.

00:12:51.260 --> 00:12:55.236
All of this work that you have seen
is all about my learning in India.

00:12:55.260 --> 00:12:58.236
And now, if you see, it's more about
the cost-effectiveness:

00:12:58.260 --> 00:13:00.236
this system costs you $300

00:13:00.260 --> 00:13:03.236
compared to the $20,000 surface tables,
or anything like that.

00:13:03.260 --> 00:13:09.236
Or maybe even the $2 mouse gesture system
at that time was costing around $5,000?

00:13:09.260 --> 00:13:15.236
I showed that, at a conference,
to President Abdul Kalam, at that time,

00:13:15.260 --> 00:13:18.784
and then he said, "OK, we should use this
in Bhabha Atomic Research Centre

00:13:18.808 --> 00:13:20.236
for some use of that."

00:13:20.260 --> 00:13:23.356
So I'm excited about how I can bring
the technology to the masses

00:13:23.380 --> 00:13:26.380
rather than just keeping that technology
in the lab environment.

00:13:26.404 --> 00:13:30.236
(Applause)

00:13:30.260 --> 00:13:32.378
CA: Based on the people we've seen at TED,

00:13:32.402 --> 00:13:34.736
I would say you're truly
one of the two or three

00:13:34.760 --> 00:13:36.636
best inventors in the world right now.

00:13:36.660 --> 00:13:38.236
It's an honor to have you at TED.

00:13:38.260 --> 00:13:40.236
Thank you so much.

00:13:40.260 --> 00:13:41.436
That's fantastic.

00:13:41.460 --> 00:13:45.260
(Applause)

