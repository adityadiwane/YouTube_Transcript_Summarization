WEBVTT
Kind: captions
Language: en

00:00:12.705 --> 00:00:14.250
In ancient Greece,

00:00:15.256 --> 00:00:19.199
when anyone from slaves to soldiers,
poets and politicians,

00:00:19.223 --> 00:00:23.227
needed to make a big decision
on life's most important questions,

00:00:23.251 --> 00:00:24.642
like, "Should I get married?"

00:00:24.666 --> 00:00:26.523
or "Should we embark on this voyage?"

00:00:26.547 --> 00:00:29.475
or "Should our army
advance into this territory?"

00:00:29.499 --> 00:00:32.078
they all consulted the oracle.

00:00:32.840 --> 00:00:34.280
So this is how it worked:

00:00:34.304 --> 00:00:37.416
you would bring her a question
and you would get on your knees,

00:00:37.440 --> 00:00:39.311
and then she would go into this trance.

00:00:39.335 --> 00:00:40.884
It would take a couple of days,

00:00:40.908 --> 00:00:43.071
and then eventually
she would come out of it,

00:00:43.095 --> 00:00:45.631
giving you her predictions as your answer.

00:00:46.730 --> 00:00:49.296
From the oracle bones of ancient China

00:00:49.320 --> 00:00:51.665
to ancient Greece to Mayan calendars,

00:00:51.689 --> 00:00:53.985
people have craved for prophecy

00:00:54.009 --> 00:00:57.146
in order to find out
what's going to happen next.

00:00:58.336 --> 00:01:01.575
And that's because we all want
to make the right decision.

00:01:01.599 --> 00:01:03.144
We don't want to miss something.

00:01:03.712 --> 00:01:05.455
The future is scary,

00:01:05.479 --> 00:01:08.196
so it's much nicer
knowing that we can make a decision

00:01:08.220 --> 00:01:10.202
with some assurance of the outcome.

00:01:10.899 --> 00:01:12.510
Well, we have a new oracle,

00:01:12.534 --> 00:01:14.679
and it's name is big data,

00:01:14.703 --> 00:01:18.642
or we call it "Watson"
or "deep learning" or "neural net."

00:01:19.160 --> 00:01:23.172
And these are the kinds of questions
we ask of our oracle now,

00:01:23.196 --> 00:01:27.118
like, "What's the most efficient way
to ship these phones

00:01:27.142 --> 00:01:28.965
from China to Sweden?"

00:01:28.989 --> 00:01:30.789
Or, "What are the odds

00:01:30.813 --> 00:01:34.176
of my child being born
with a genetic disorder?"

00:01:34.772 --> 00:01:38.016
Or, "What are the sales volume
we can predict for this product?"

00:01:39.928 --> 00:01:43.975
I have a dog. Her name is Elle,
and she hates the rain.

00:01:43.999 --> 00:01:47.305
And I have tried everything
to untrain her.

00:01:47.329 --> 00:01:50.100
But because I have failed at this,

00:01:50.124 --> 00:01:53.410
I also have to consult
an oracle, called Dark Sky,

00:01:53.434 --> 00:01:55.069
every time before we go on a walk,

00:01:55.093 --> 00:01:58.670
for very accurate weather predictions
in the next 10 minutes.

00:02:01.355 --> 00:02:02.658
She's so sweet.

00:02:03.647 --> 00:02:09.354
So because of all of this,
our oracle is a $122 billion industry.

00:02:09.826 --> 00:02:13.202
Now, despite the size of this industry,

00:02:13.226 --> 00:02:15.682
the returns are surprisingly low.

00:02:16.162 --> 00:02:18.656
Investing in big data is easy,

00:02:18.680 --> 00:02:20.613
but using it is hard.

00:02:21.801 --> 00:02:25.841
Over 73 percent of big data projects
aren't even profitable,

00:02:25.865 --> 00:02:28.296
and I have executives
coming up to me saying,

00:02:28.320 --> 00:02:30.109
"We're experiencing the same thing.

00:02:30.133 --> 00:02:31.886
We invested in some big data system,

00:02:31.910 --> 00:02:34.878
and our employees aren't making
better decisions.

00:02:34.902 --> 00:02:38.064
And they're certainly not coming up
with more breakthrough ideas."

00:02:38.734 --> 00:02:41.918
So this is all really interesting to me,

00:02:41.942 --> 00:02:43.952
because I'm a technology ethnographer.

00:02:44.450 --> 00:02:47.014
I study and I advise companies

00:02:47.038 --> 00:02:49.521
on the patterns
of how people use technology,

00:02:49.545 --> 00:02:52.223
and one of my interest areas is data.

00:02:52.247 --> 00:02:57.440
So why is having more data
not helping us make better decisions,

00:02:57.464 --> 00:03:00.247
especially for companies
who have all these resources

00:03:00.271 --> 00:03:02.007
to invest in these big data systems?

00:03:02.031 --> 00:03:04.429
Why isn't it getting any easier for them?

00:03:05.810 --> 00:03:08.444
So, I've witnessed the struggle firsthand.

00:03:09.194 --> 00:03:12.678
In 2009, I started
a research position with Nokia.

00:03:13.052 --> 00:03:14.210
And at the time,

00:03:14.234 --> 00:03:17.392
Nokia was one of the largest
cell phone companies in the world,

00:03:17.416 --> 00:03:20.618
dominating emerging markets
like China, Mexico and India --

00:03:20.642 --> 00:03:23.144
all places where I had done
a lot of research

00:03:23.168 --> 00:03:25.844
on how low-income people use technology.

00:03:25.868 --> 00:03:28.198
And I spent a lot of extra time in China

00:03:28.222 --> 00:03:30.814
getting to know the informal economy.

00:03:30.838 --> 00:03:33.239
So I did things like working
as a street vendor

00:03:33.263 --> 00:03:35.837
selling dumplings to construction workers.

00:03:35.861 --> 00:03:37.219
Or I did fieldwork,

00:03:37.243 --> 00:03:40.201
spending nights and days
in internet cafÃ©s,

00:03:40.225 --> 00:03:42.771
hanging out with Chinese youth,
so I could understand

00:03:42.795 --> 00:03:45.079
how they were using
games and mobile phones

00:03:45.103 --> 00:03:48.473
and using it between moving
from the rural areas to the cities.

00:03:50.155 --> 00:03:54.082
Through all of this qualitative evidence
that I was gathering,

00:03:54.106 --> 00:03:56.930
I was starting to see so clearly

00:03:56.954 --> 00:04:01.426
that a big change was about to happen
among low-income Chinese people.

00:04:02.840 --> 00:04:07.207
Even though they were surrounded
by advertisements for luxury products

00:04:07.231 --> 00:04:10.726
like fancy toilets --
who wouldn't want one? --

00:04:10.750 --> 00:04:13.640
and apartments and cars,

00:04:13.664 --> 00:04:15.484
through my conversations with them,

00:04:15.508 --> 00:04:19.349
I found out that the ads
the actually enticed them the most

00:04:19.373 --> 00:04:21.369
were the ones for iPhones,

00:04:21.393 --> 00:04:24.445
promising them this entry
into this high-tech life.

00:04:25.289 --> 00:04:28.452
And even when I was living with them
in urban slums like this one,

00:04:28.476 --> 00:04:31.472
I saw people investing
over half of their monthly income

00:04:31.496 --> 00:04:33.119
into buying a phone,

00:04:33.143 --> 00:04:35.445
and increasingly, they were "shanzhai,"

00:04:35.469 --> 00:04:38.857
which are affordable knock-offs
of iPhones and other brands.

00:04:40.123 --> 00:04:41.748
They're very usable.

00:04:42.710 --> 00:04:44.032
Does the job.

00:04:44.570 --> 00:04:50.359
And after years of living
with migrants and working with them

00:04:50.383 --> 00:04:53.817
and just really doing everything
that they were doing,

00:04:53.841 --> 00:04:57.438
I started piecing
all these data points together --

00:04:57.462 --> 00:05:00.585
from the things that seem random,
like me selling dumplings,

00:05:00.609 --> 00:05:02.413
to the things that were more obvious,

00:05:02.437 --> 00:05:05.669
like tracking how much they were spending
on their cell phone bills.

00:05:05.693 --> 00:05:08.332
And I was able to create
this much more holistic picture

00:05:08.356 --> 00:05:09.512
of what was happening.

00:05:09.536 --> 00:05:11.258
And that's when I started to realize

00:05:11.282 --> 00:05:14.791
that even the poorest in China
would want a smartphone,

00:05:14.815 --> 00:05:19.800
and that they would do almost anything
to get their hands on one.

00:05:20.893 --> 00:05:23.297
You have to keep in mind,

00:05:23.321 --> 00:05:26.405
iPhones had just come out, it was 2009,

00:05:26.429 --> 00:05:28.314
so this was, like, eight years ago,

00:05:28.338 --> 00:05:30.775
and Androids had just started
looking like iPhones.

00:05:30.799 --> 00:05:33.306
And a lot of very smart
and realistic people said,

00:05:33.330 --> 00:05:35.537
"Those smartphones -- that's just a fad.

00:05:36.063 --> 00:05:39.059
Who wants to carry around
these heavy things

00:05:39.083 --> 00:05:42.570
where batteries drain quickly
and they break every time you drop them?"

00:05:44.613 --> 00:05:45.814
But I had a lot of data,

00:05:45.838 --> 00:05:48.098
and I was very confident
about my insights,

00:05:48.122 --> 00:05:50.951
so I was very excited
to share them with Nokia.

00:05:53.152 --> 00:05:55.669
But Nokia was not convinced,

00:05:55.693 --> 00:05:58.028
because it wasn't big data.

00:05:58.842 --> 00:06:01.246
They said, "We have
millions of data points,

00:06:01.270 --> 00:06:05.517
and we don't see any indicators
of anyone wanting to buy a smartphone,

00:06:05.541 --> 00:06:09.929
and your data set of 100,
as diverse as it is, is too weak

00:06:09.953 --> 00:06:11.667
for us to even take seriously."

00:06:12.728 --> 00:06:14.333
And I said, "Nokia, you're right.

00:06:14.357 --> 00:06:15.917
Of course you wouldn't see this,

00:06:15.941 --> 00:06:19.312
because you're sending out surveys
assuming that people don't know

00:06:19.336 --> 00:06:20.495
what a smartphone is,

00:06:20.519 --> 00:06:22.885
so of course you're not going
to get any data back

00:06:22.909 --> 00:06:25.481
about people wanting to buy
a smartphone in two years.

00:06:25.505 --> 00:06:27.623
Your surveys, your methods
have been designed

00:06:27.647 --> 00:06:29.669
to optimize an existing business model,

00:06:29.693 --> 00:06:32.301
and I'm looking
at these emergent human dynamics

00:06:32.325 --> 00:06:33.679
that haven't happened yet.

00:06:33.703 --> 00:06:36.141
We're looking outside of market dynamics

00:06:36.165 --> 00:06:37.796
so that we can get ahead of it."

00:06:39.193 --> 00:06:41.437
Well, you know what happened to Nokia?

00:06:41.461 --> 00:06:43.826
Their business fell off a cliff.

00:06:44.611 --> 00:06:48.338
This -- this is the cost
of missing something.

00:06:48.983 --> 00:06:50.982
It was unfathomable.

00:06:51.823 --> 00:06:53.474
But Nokia's not alone.

00:06:54.078 --> 00:06:56.659
I see organizations
throwing out data all the time

00:06:56.683 --> 00:06:59.244
because it didn't come from a quant model

00:06:59.268 --> 00:07:01.036
or it doesn't fit in one.

00:07:02.039 --> 00:07:04.087
But it's not big data's fault.

00:07:04.762 --> 00:07:08.669
It's the way we use big data;
it's our responsibility.

00:07:09.550 --> 00:07:11.461
Big data's reputation for success

00:07:11.485 --> 00:07:15.244
comes from quantifying
very specific environments,

00:07:15.268 --> 00:07:20.181
like electricity power grids
or delivery logistics or genetic code,

00:07:20.205 --> 00:07:24.523
when we're quantifying in systems
that are more or less contained.

00:07:24.547 --> 00:07:27.516
But not all systems
are as neatly contained.

00:07:27.540 --> 00:07:30.798
When you're quantifying
and systems are more dynamic,

00:07:30.822 --> 00:07:34.621
especially systems
that involve human beings,

00:07:34.645 --> 00:07:37.071
forces are complex and unpredictable,

00:07:37.095 --> 00:07:40.581
and these are things
that we don't know how to model so well.

00:07:41.024 --> 00:07:43.837
Once you predict something
about human behavior,

00:07:43.861 --> 00:07:45.716
new factors emerge,

00:07:45.740 --> 00:07:48.105
because conditions
are constantly changing.

00:07:48.129 --> 00:07:49.932
That's why it's a never-ending cycle.

00:07:49.956 --> 00:07:51.420
You think you know something,

00:07:51.444 --> 00:07:53.686
and then something unknown
enters the picture.

00:07:53.710 --> 00:07:57.032
And that's why just relying
on big data alone

00:07:57.056 --> 00:07:59.905
increases the chance
that we'll miss something,

00:07:59.929 --> 00:08:03.706
while giving us this illusion
that we already know everything.

00:08:04.226 --> 00:08:08.082
And what makes it really hard
to see this paradox

00:08:08.106 --> 00:08:10.765
and even wrap our brains around it

00:08:10.789 --> 00:08:14.480
is that we have this thing
that I call the quantification bias,

00:08:14.504 --> 00:08:18.426
which is the unconscious belief
of valuing the measurable

00:08:18.450 --> 00:08:20.044
over the immeasurable.

00:08:21.042 --> 00:08:24.326
And we often experience this at our work.

00:08:24.350 --> 00:08:27.000
Maybe we work alongside
colleagues who are like this,

00:08:27.024 --> 00:08:29.452
or even our whole entire
company may be like this,

00:08:29.476 --> 00:08:32.022
where people become
so fixated on that number,

00:08:32.046 --> 00:08:34.113
that they can't see anything
outside of it,

00:08:34.137 --> 00:08:38.085
even when you present them evidence
right in front of their face.

00:08:38.943 --> 00:08:42.314
And this is a very appealing message,

00:08:42.338 --> 00:08:44.681
because there's nothing
wrong with quantifying;

00:08:44.705 --> 00:08:46.135
it's actually very satisfying.

00:08:46.159 --> 00:08:50.521
I get a great sense of comfort
from looking at an Excel spreadsheet,

00:08:50.545 --> 00:08:51.946
even very simple ones.

00:08:51.970 --> 00:08:52.984
(Laughter)

00:08:53.008 --> 00:08:54.160
It's just kind of like,

00:08:54.184 --> 00:08:57.688
"Yes! The formula worked. It's all OK.
Everything is under control."

00:08:58.612 --> 00:09:01.002
But the problem is

00:09:01.026 --> 00:09:03.687
that quantifying is addictive.

00:09:03.711 --> 00:09:05.093
And when we forget that

00:09:05.117 --> 00:09:08.155
and when we don't have something
to kind of keep that in check,

00:09:08.179 --> 00:09:10.297
it's very easy to just throw out data

00:09:10.321 --> 00:09:13.039
because it can't be expressed
as a numerical value.

00:09:13.063 --> 00:09:15.984
It's very easy just to slip
into silver-bullet thinking,

00:09:16.008 --> 00:09:18.587
as if some simple solution existed.

00:09:19.420 --> 00:09:23.482
Because this is a great moment of danger
for any organization,

00:09:23.506 --> 00:09:26.140
because oftentimes,
the future we need to predict --

00:09:26.164 --> 00:09:28.330
it isn't in that haystack,

00:09:28.354 --> 00:09:30.892
but it's that tornado
that's bearing down on us

00:09:30.916 --> 00:09:32.404
outside of the barn.

00:09:34.780 --> 00:09:37.106
There is no greater risk

00:09:37.130 --> 00:09:38.796
than being blind to the unknown.

00:09:38.820 --> 00:09:40.969
It can cause you to make
the wrong decisions.

00:09:40.993 --> 00:09:42.967
It can cause you to miss something big.

00:09:43.554 --> 00:09:46.655
But we don't have to go down this path.

00:09:47.273 --> 00:09:50.468
It turns out that the oracle
of ancient Greece

00:09:50.492 --> 00:09:54.458
holds the secret key
that shows us the path forward.

00:09:55.474 --> 00:09:58.069
Now, recent geological research has shown

00:09:58.093 --> 00:10:01.657
that the Temple of Apollo,
where the most famous oracle sat,

00:10:01.681 --> 00:10:04.765
was actually built
over two earthquake faults.

00:10:04.789 --> 00:10:07.675
And these faults would release
these petrochemical fumes

00:10:07.699 --> 00:10:09.384
from underneath the Earth's crust,

00:10:09.408 --> 00:10:13.274
and the oracle literally sat
right above these faults,

00:10:13.298 --> 00:10:16.886
inhaling enormous amounts
of ethylene gas, these fissures.

00:10:16.910 --> 00:10:17.918
(Laughter)

00:10:17.942 --> 00:10:19.115
It's true.

00:10:19.139 --> 00:10:20.156
(Laughter)

00:10:20.180 --> 00:10:23.689
It's all true, and that's what made her
babble and hallucinate

00:10:23.713 --> 00:10:25.437
and go into this trance-like state.

00:10:25.461 --> 00:10:27.231
She was high as a kite!

00:10:27.255 --> 00:10:31.716
(Laughter)

00:10:31.740 --> 00:10:34.519
So how did anyone --

00:10:34.543 --> 00:10:37.573
How did anyone get
any useful advice out of her

00:10:37.597 --> 00:10:38.787
in this state?

00:10:39.317 --> 00:10:41.698
Well, you see those people
surrounding the oracle?

00:10:41.722 --> 00:10:43.601
You see those people holding her up,

00:10:43.625 --> 00:10:45.342
because she's, like, a little woozy?

00:10:45.366 --> 00:10:47.674
And you see that guy
on your left-hand side

00:10:47.698 --> 00:10:49.296
holding the orange notebook?

00:10:49.925 --> 00:10:51.655
Well, those were the temple guides,

00:10:51.679 --> 00:10:54.695
and they worked hand in hand
with the oracle.

00:10:55.904 --> 00:10:58.420
When inquisitors would come
and get on their knees,

00:10:58.444 --> 00:11:00.784
that's when the temple guides
would get to work,

00:11:00.808 --> 00:11:02.672
because after they asked her questions,

00:11:02.696 --> 00:11:04.697
they would observe their emotional state,

00:11:04.721 --> 00:11:07.045
and then they would ask them
follow-up questions,

00:11:07.069 --> 00:11:09.903
like, "Why do you want to know
this prophecy? Who are you?

00:11:09.927 --> 00:11:12.191
What are you going to do
with this information?"

00:11:12.215 --> 00:11:15.397
And then the temple guides would take
this more ethnographic,

00:11:15.421 --> 00:11:17.577
this more qualitative information,

00:11:17.601 --> 00:11:19.676
and interpret the oracle's babblings.

00:11:21.248 --> 00:11:23.540
So the oracle didn't stand alone,

00:11:23.564 --> 00:11:25.712
and neither should our big data systems.

00:11:26.450 --> 00:11:27.611
Now to be clear,

00:11:27.635 --> 00:11:31.094
I'm not saying that big data systems
are huffing ethylene gas,

00:11:31.118 --> 00:11:33.471
or that they're even giving
invalid predictions.

00:11:33.495 --> 00:11:34.656
The total opposite.

00:11:34.680 --> 00:11:36.748
But what I am saying

00:11:36.772 --> 00:11:40.604
is that in the same way
that the oracle needed her temple guides,

00:11:40.628 --> 00:11:42.916
our big data systems need them, too.

00:11:42.940 --> 00:11:47.049
They need people like ethnographers
and user researchers

00:11:47.073 --> 00:11:49.579
who can gather what I call thick data.

00:11:50.322 --> 00:11:53.313
This is precious data from humans,

00:11:53.337 --> 00:11:57.439
like stories, emotions and interactions
that cannot be quantified.

00:11:57.463 --> 00:11:59.785
It's the kind of data
that I collected for Nokia

00:11:59.809 --> 00:12:02.478
that comes in in the form
of a very small sample size,

00:12:02.502 --> 00:12:05.457
but delivers incredible depth of meaning.

00:12:05.481 --> 00:12:09.161
And what makes it so thick and meaty

00:12:10.265 --> 00:12:14.294
is the experience of understanding
the human narrative.

00:12:14.318 --> 00:12:17.957
And that's what helps to see
what's missing in our models.

00:12:18.671 --> 00:12:22.716
Thick data grounds our business questions
in human questions,

00:12:22.740 --> 00:12:26.302
and that's why integrating
big and thick data

00:12:26.326 --> 00:12:28.015
forms a more complete picture.

00:12:28.592 --> 00:12:31.473
Big data is able to offer
insights at scale

00:12:31.497 --> 00:12:34.144
and leverage the best
of machine intelligence,

00:12:34.168 --> 00:12:37.740
whereas thick data can help us
rescue the context loss

00:12:37.764 --> 00:12:39.862
that comes from making big data usable,

00:12:39.886 --> 00:12:42.067
and leverage the best
of human intelligence.

00:12:42.091 --> 00:12:45.643
And when you actually integrate the two,
that's when things get really fun,

00:12:45.667 --> 00:12:48.103
because then you're no longer
just working with data

00:12:48.127 --> 00:12:49.323
you've already collected.

00:12:49.347 --> 00:12:52.084
You get to also work with data
that hasn't been collected.

00:12:52.108 --> 00:12:53.827
You get to ask questions about why:

00:12:53.851 --> 00:12:55.168
Why is this happening?

00:12:55.598 --> 00:12:56.977
Now, when Netflix did this,

00:12:57.001 --> 00:13:00.036
they unlocked a whole new way
to transform their business.

00:13:01.226 --> 00:13:05.182
Netflix is known for their really great
recommendation algorithm,

00:13:05.206 --> 00:13:10.003
and they had this $1 million prize
for anyone who could improve it.

00:13:10.027 --> 00:13:11.341
And there were winners.

00:13:12.075 --> 00:13:16.398
But Netflix discovered
the improvements were only incremental.

00:13:17.224 --> 00:13:19.188
So to really find out what was going on,

00:13:19.212 --> 00:13:22.953
they hired an ethnographer,
Grant McCracken,

00:13:22.977 --> 00:13:24.523
to gather thick data insights.

00:13:24.547 --> 00:13:28.471
And what he discovered was something
that they hadn't seen initially

00:13:28.495 --> 00:13:29.850
in the quantitative data.

00:13:30.892 --> 00:13:33.620
He discovered that people loved
to binge-watch.

00:13:33.644 --> 00:13:35.997
In fact, people didn't even
feel guilty about it.

00:13:36.021 --> 00:13:37.276
They enjoyed it.

00:13:37.300 --> 00:13:38.326
(Laughter)

00:13:38.350 --> 00:13:40.706
So Netflix was like,
"Oh. This is a new insight."

00:13:40.730 --> 00:13:42.668
So they went to their data science team,

00:13:42.692 --> 00:13:45.010
and they were able to scale
this big data insight

00:13:45.034 --> 00:13:47.621
in with their quantitative data.

00:13:47.645 --> 00:13:50.815
And once they verified it
and validated it,

00:13:50.839 --> 00:13:55.600
Netflix decided to do something
very simple but impactful.

00:13:56.654 --> 00:14:03.146
They said, instead of offering
the same show from different genres

00:14:03.170 --> 00:14:07.058
or more of the different shows
from similar users,

00:14:07.082 --> 00:14:09.636
we'll just offer more of the same show.

00:14:09.660 --> 00:14:11.765
We'll make it easier
for you to binge-watch.

00:14:11.789 --> 00:14:13.275
And they didn't stop there.

00:14:13.299 --> 00:14:14.773
They did all these things

00:14:14.797 --> 00:14:17.756
to redesign their entire
viewer experience,

00:14:17.780 --> 00:14:19.538
to really encourage binge-watching.

00:14:20.050 --> 00:14:23.291
It's why people and friends disappear
for whole weekends at a time,

00:14:23.315 --> 00:14:25.658
catching up on shows
like "Master of None."

00:14:25.682 --> 00:14:29.855
By integrating big data and thick data,
they not only improved their business,

00:14:29.879 --> 00:14:32.691
but they transformed how we consume media.

00:14:32.715 --> 00:14:37.267
And now their stocks are projected
to double in the next few years.

00:14:38.100 --> 00:14:41.930
But this isn't just about
watching more videos

00:14:41.954 --> 00:14:43.574
or selling more smartphones.

00:14:43.963 --> 00:14:48.013
For some, integrating thick data
insights into the algorithm

00:14:48.037 --> 00:14:50.300
could mean life or death,

00:14:50.324 --> 00:14:52.470
especially for the marginalized.

00:14:53.558 --> 00:14:56.992
All around the country,
police departments are using big data

00:14:57.016 --> 00:14:58.979
for predictive policing,

00:14:59.003 --> 00:15:02.087
to set bond amounts
and sentencing recommendations

00:15:02.111 --> 00:15:05.258
in ways that reinforce existing biases.

00:15:06.116 --> 00:15:08.539
NSA's Skynet machine learning algorithm

00:15:08.563 --> 00:15:14.007
has possibly aided in the deaths
of thousands of civilians in Pakistan

00:15:14.031 --> 00:15:16.752
from misreading cellular device metadata.

00:15:18.951 --> 00:15:22.354
As all of our lives become more automated,

00:15:22.378 --> 00:15:25.458
from automobiles to health insurance
or to employment,

00:15:25.482 --> 00:15:27.832
it is likely that all of us

00:15:27.856 --> 00:15:30.845
will be impacted
by the quantification bias.

00:15:32.792 --> 00:15:35.413
Now, the good news
is that we've come a long way

00:15:35.437 --> 00:15:37.887
from huffing ethylene gas
to make predictions.

00:15:37.911 --> 00:15:40.981
We have better tools,
so let's just use them better.

00:15:41.005 --> 00:15:43.328
Let's integrate the big data
with the thick data.

00:15:43.352 --> 00:15:45.613
Let's bring our temple guides
with the oracles,

00:15:45.637 --> 00:15:49.013
and whether this work happens
in companies or nonprofits

00:15:49.037 --> 00:15:51.506
or government or even in the software,

00:15:51.530 --> 00:15:53.322
all of it matters,

00:15:53.346 --> 00:15:56.369
because that means
we're collectively committed

00:15:56.393 --> 00:15:58.584
to making better data,

00:15:58.608 --> 00:16:00.444
better algorithms, better outputs

00:16:00.468 --> 00:16:02.111
and better decisions.

00:16:02.135 --> 00:16:05.693
This is how we'll avoid
missing that something.

00:16:07.042 --> 00:16:10.990
(Applause)

