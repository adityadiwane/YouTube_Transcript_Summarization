WEBVTT
Kind: captions
Language: en

00:00:12.653 --> 00:00:16.051
So in 2011, I altered my name

00:00:16.075 --> 00:00:20.051
so that I could participate
in Far Right youth camp in Hungary.

00:00:20.614 --> 00:00:24.980
I was doing a PhD looking at
youth political socialization --

00:00:25.004 --> 00:00:28.075
why young people were developing
political ideologies

00:00:28.099 --> 00:00:30.109
in a post-communist setting,

00:00:30.133 --> 00:00:33.366
and I saw that a lot
of young people I was talking to

00:00:33.390 --> 00:00:34.990
were joining the Far Right,

00:00:35.014 --> 00:00:37.170
and this was astounding to me.

00:00:37.194 --> 00:00:39.489
So I wanted to enroll in this youth camp

00:00:39.513 --> 00:00:42.649
to get a better understanding
of why people were joining.

00:00:42.673 --> 00:00:44.230
So a colleague enrolled me,

00:00:44.254 --> 00:00:47.182
and my last name sounds
a little bit too Jewish.

00:00:47.500 --> 00:00:50.245
So Erin got turned into Iréna,

00:00:50.269 --> 00:00:52.469
and Saltman got turned into Sós,

00:00:52.493 --> 00:00:55.414
which means "salty" in Hungarian.

00:00:55.438 --> 00:00:57.748
And in Hungarian,
your last name goes first,

00:00:57.772 --> 00:01:02.186
so my James Bond name
turned into "Salty Irena,"

00:01:02.210 --> 00:01:05.693
which is not something
I would have naturally chosen for myself.

00:01:06.100 --> 00:01:08.007
But going to this camp,

00:01:08.031 --> 00:01:12.554
I was further shocked to realize
that it was actually really fun.

00:01:13.000 --> 00:01:15.215
They talked very little about politics.

00:01:15.239 --> 00:01:18.252
It was mostly learning how to ride horses,

00:01:18.276 --> 00:01:20.144
shooting a bow and arrow,

00:01:20.168 --> 00:01:21.855
live music at night,

00:01:21.879 --> 00:01:23.848
free food and alcohol,

00:01:23.872 --> 00:01:26.738
also some air-gun target practice

00:01:26.762 --> 00:01:30.416
using mainstream politicians'
faces as targets.

00:01:30.440 --> 00:01:34.167
And this seemed like a very,
actually, friendly, inclusive group

00:01:34.191 --> 00:01:39.604
until you started talking or mentioning
anything to do with the Roma population,

00:01:39.628 --> 00:01:41.890
Jewish people or immigrants,

00:01:41.914 --> 00:01:46.064
and then the discourse would become
very hate-based very quickly.

00:01:46.663 --> 00:01:49.473
So it led me into my work now,

00:01:49.497 --> 00:01:51.878
where we pose the question,

00:01:51.902 --> 00:01:54.942
"Why do people join
violent extremist movements,

00:01:54.966 --> 00:01:57.997
and how do we effectively
counter these processes?"

00:01:58.393 --> 00:02:01.684
In the aftermath of horrible
atrocities and attacks

00:02:01.708 --> 00:02:05.071
in places like Belgium, France,
but all over the world,

00:02:05.095 --> 00:02:06.928
sometimes it's easier for us to think,

00:02:06.952 --> 00:02:08.897
"Well, these must be sociopaths,

00:02:08.921 --> 00:02:11.985
these must be naturally
violent individuals.

00:02:12.009 --> 00:02:14.605
They must have something wrong
with their upbringing."

00:02:14.629 --> 00:02:16.716
And what's really tragic

00:02:16.740 --> 00:02:18.931
is that oftentimes there's no one profile.

00:02:18.955 --> 00:02:22.209
Many people come
from educated backgrounds,

00:02:22.233 --> 00:02:24.329
different socioeconomic backgrounds,

00:02:24.353 --> 00:02:27.201
men and women, different ages,

00:02:27.225 --> 00:02:29.503
some with families, some single.

00:02:29.527 --> 00:02:32.182
So why? What is this allure?

00:02:32.206 --> 00:02:34.255
And this is what
I want to talk you through,

00:02:34.279 --> 00:02:37.166
as well as how do we
challenge this in a modern era?

00:02:38.531 --> 00:02:40.014
We do know, through research,

00:02:40.038 --> 00:02:42.394
that there are quite a number
of different things

00:02:42.418 --> 00:02:45.769
that affect somebody's
process of radicalization,

00:02:45.793 --> 00:02:48.563
and we categorize these
into push and pull factors.

00:02:48.587 --> 00:02:52.000
And these are pretty much similar
for Far Right, neo-Nazi groups

00:02:52.024 --> 00:02:54.928
all the way to Islamist extremist
and terrorist groups.

00:02:55.483 --> 00:02:59.341
And push factors are basically
what makes you vulnerable

00:02:59.365 --> 00:03:01.223
to a process of radicalization,

00:03:01.247 --> 00:03:03.453
to joining a violent extremist group.

00:03:03.477 --> 00:03:05.603
And these can be
a lot of different things,

00:03:05.627 --> 00:03:09.540
but roughly, a sense of alienation,
a sense of isolation,

00:03:09.564 --> 00:03:11.715
questioning your own identity,

00:03:11.739 --> 00:03:14.565
but also feeling that your in-group
is under attack,

00:03:14.589 --> 00:03:18.382
and your in group might be based
on a nationality or an ethnicity

00:03:18.406 --> 00:03:19.732
or a religion,

00:03:19.756 --> 00:03:23.367
and feeling that larger powers around you
are doing nothing to help.

00:03:23.895 --> 00:03:27.316
Now, push factors alone
do not make you a violent extremist,

00:03:27.340 --> 00:03:28.770
because if that were the fact,

00:03:28.794 --> 00:03:32.064
those same factors would go
towards a group like the Roma population,

00:03:32.088 --> 00:03:34.869
and they're not
a violently mobilized group.

00:03:34.893 --> 00:03:37.180
So we have to look at the pull factors.

00:03:37.204 --> 00:03:40.514
What are these violent
extremist organizations offering

00:03:40.538 --> 00:03:42.483
that other groups are not offering?

00:03:42.507 --> 00:03:45.070
And actually, this is usually
very positive things,

00:03:45.094 --> 00:03:47.111
very seemingly empowering things,

00:03:47.135 --> 00:03:49.598
such as brotherhood and sisterhood

00:03:49.622 --> 00:03:50.956
and a sense of belonging,

00:03:50.980 --> 00:03:53.854
as well as giving somebody
a spiritual purpose,

00:03:53.878 --> 00:03:57.593
a divine purpose
to build a utopian society

00:03:57.617 --> 00:03:59.538
if their goals can be met,

00:03:59.562 --> 00:04:02.313
but also a sense of empowerment
and adventure.

00:04:02.337 --> 00:04:04.380
When we look
at foreign terrorist fighters,

00:04:04.404 --> 00:04:07.095
we see young men
with the wind in their hair

00:04:07.119 --> 00:04:09.665
out in the desert
and women going to join them

00:04:09.689 --> 00:04:12.330
to have nuptials out in the sunset.

00:04:12.354 --> 00:04:16.174
It's very romantic, and you become a hero.

00:04:16.198 --> 00:04:19.086
For both men and women,
that's the propaganda being given.

00:04:19.487 --> 00:04:22.129
So what extremist groups are very good at

00:04:22.153 --> 00:04:26.979
is taking a very complicated,
confusing, nuanced world

00:04:27.003 --> 00:04:30.246
and simplifying that world
into black and white,

00:04:30.270 --> 00:04:31.480
good and evil.

00:04:31.504 --> 00:04:33.385
And you become what is good,

00:04:33.409 --> 00:04:35.264
challenging what is evil.

00:04:36.361 --> 00:04:40.225
So I want to talk a little bit
about ISIS, Daesh,

00:04:40.249 --> 00:04:44.627
because they have been a game changer
in how we look at these processes,

00:04:44.651 --> 00:04:47.857
and through a lot of the material
and their tactics.

00:04:47.881 --> 00:04:50.429
They're very much a modern movement.

00:04:50.745 --> 00:04:55.230
One of the aspects is the internet
and the usage of social media,

00:04:55.254 --> 00:04:59.636
as we've all seen in headlines
tweeting and videos of beheadings.

00:04:59.660 --> 00:05:02.135
But the internet alone
does not radicalize you.

00:05:02.159 --> 00:05:03.366
The internet is a tool.

00:05:03.390 --> 00:05:05.246
You don't go online shopping for shoes

00:05:05.270 --> 00:05:07.068
and accidentally become a jihadist.

00:05:07.613 --> 00:05:11.002
However, what the Internet
does do is it is a catalyst.

00:05:11.026 --> 00:05:15.145
It provides tools and scale and rapidity

00:05:15.169 --> 00:05:16.677
that doesn't exist elsewhere.

00:05:16.701 --> 00:05:19.162
And with ISIS, all of a sudden,

00:05:19.186 --> 00:05:24.504
this idea of a cloaked, dark figure
of a jihadist changed for us.

00:05:24.528 --> 00:05:26.583
All of a sudden,
we were in their kitchens.

00:05:26.607 --> 00:05:28.606
We saw what they were eating for dinner.

00:05:28.630 --> 00:05:29.781
They were tweeting.

00:05:29.805 --> 00:05:32.963
We had foreign terrorist fighters
tweeting in their own languages.

00:05:32.987 --> 00:05:35.939
We had women going out there
talking about their wedding day,

00:05:35.963 --> 00:05:37.710
about the births of their children.

00:05:37.734 --> 00:05:39.631
We had gaming culture, all of a sudden,

00:05:39.655 --> 00:05:42.821
and references
to Grand Theft Auto being made.

00:05:43.291 --> 00:05:45.752
So all of a sudden, they were homey.

00:05:45.776 --> 00:05:46.927
They became human.

00:05:46.951 --> 00:05:49.165
And the problem
is that trying to counter it,

00:05:49.189 --> 00:05:51.499
lots of governments
and social media companies

00:05:51.523 --> 00:05:52.674
just tried to censor.

00:05:52.698 --> 00:05:54.689
How do we get rid of terrorist content?

00:05:54.713 --> 00:05:56.368
And it became a cat-and-mouse game

00:05:56.392 --> 00:05:59.596
where we would see accounts taken down
and they'd just come back up,

00:05:59.620 --> 00:06:02.733
and an arrogance around somebody
having a 25th account

00:06:02.757 --> 00:06:05.851
and material that was
disseminated everywhere.

00:06:05.875 --> 00:06:07.896
But we also saw a dangerous trend --

00:06:07.920 --> 00:06:12.928
violent extremists know the rules
and regulations of social media, too.

00:06:12.952 --> 00:06:16.952
So we would see a banal
conversation with a recruiter

00:06:16.976 --> 00:06:18.949
start on a mainstream platform,

00:06:18.973 --> 00:06:21.054
and at the point
at which that conversation

00:06:21.078 --> 00:06:22.418
was going to become illegal,

00:06:22.442 --> 00:06:24.943
they would jump to a smaller,
less regulated,

00:06:24.967 --> 00:06:26.590
more encrypted platform.

00:06:26.614 --> 00:06:30.147
So all of a sudden, we couldn't
track where that conversation went.

00:06:30.171 --> 00:06:32.033
So this is a problem with censorship,

00:06:32.057 --> 00:06:35.289
which is why we need to develop
alternatives to censorship.

00:06:35.855 --> 00:06:39.205
ISIS is also a game-changer
because it's state-building.

00:06:39.229 --> 00:06:41.341
It's not just recruiting combatants;

00:06:41.365 --> 00:06:43.227
it's trying to build a state.

00:06:43.251 --> 00:06:45.191
And what that means is all of a sudden,

00:06:45.215 --> 00:06:47.215
your recruitment model is much more broad.

00:06:47.239 --> 00:06:49.288
You're not just trying to get fighters --

00:06:49.312 --> 00:06:53.578
now you need architects, engineers,
accountants, hackers and women.

00:06:53.602 --> 00:06:55.992
We've actually seen
a huge increase of women going

00:06:56.016 --> 00:06:59.515
in the last 24, but especially 12 months.

00:06:59.539 --> 00:07:02.428
Some countries, one in four
of the people going over to join

00:07:02.452 --> 00:07:03.691
are now women.

00:07:03.715 --> 00:07:05.083
And so, this really changes

00:07:05.107 --> 00:07:07.889
who we're trying to counter
this process with.

00:07:08.499 --> 00:07:10.150
Now, not all doom and gloom.

00:07:10.174 --> 00:07:13.134
So the rest I'd like to talk about
some of the positive things

00:07:13.158 --> 00:07:17.002
and the new innovation in trying
to prevent and counter violent extremism.

00:07:17.026 --> 00:07:19.299
Preventing is very different
than countering,

00:07:19.323 --> 00:07:21.879
and actually, you can think of it
in medical terms.

00:07:21.903 --> 00:07:24.125
So preventative medicine is,

00:07:24.149 --> 00:07:27.323
how do we make it
so you are naturally resilient

00:07:27.347 --> 00:07:29.847
to this process of radicalization,

00:07:29.871 --> 00:07:31.733
whereas that is going to be different

00:07:31.757 --> 00:07:34.416
if somebody is already showing
a symptom or a sign

00:07:34.440 --> 00:07:37.321
of belonging to a violent
extremist ideology.

00:07:37.345 --> 00:07:38.892
And so in preventative measures,

00:07:38.916 --> 00:07:41.607
we're talking more
about really broad groups of people

00:07:41.631 --> 00:07:43.448
and exposure to ideas

00:07:43.472 --> 00:07:45.239
to make them resilient.

00:07:45.263 --> 00:07:46.779
Whereas it's very different

00:07:46.803 --> 00:07:50.628
if somebody is starting to question
and agree with certain things online,

00:07:50.652 --> 00:07:54.501
and it's also very different
if somebody already has a swastika tattoo

00:07:54.525 --> 00:07:56.573
and is very much embedded within a group.

00:07:56.597 --> 00:07:58.031
How do you reach them?

00:07:58.605 --> 00:08:02.287
So I'd like to go through three examples
of each one of those levels

00:08:02.311 --> 00:08:03.526
and talk you through

00:08:03.550 --> 00:08:06.866
what some of the new ways
of engaging with people are becoming.

00:08:07.194 --> 00:08:08.607
One is "Extreme Dialogue,"

00:08:08.631 --> 00:08:11.711
and it's an educational program
that we helped develop.

00:08:11.735 --> 00:08:14.116
This one is from Canada,

00:08:14.140 --> 00:08:18.235
and it's meant to create dialogues
within a classroom setting,

00:08:18.259 --> 00:08:19.791
using storytelling,

00:08:19.815 --> 00:08:22.966
because violent extremism
can be very hard to try to explain,

00:08:22.990 --> 00:08:24.689
especially to younger individuals.

00:08:25.125 --> 00:08:29.038
So we have a network of former extremists
and survivors of extremism

00:08:29.062 --> 00:08:32.999
that tell their stories through video
and create question-giving to classrooms,

00:08:33.023 --> 00:08:35.326
to start a conversation about the topic.

00:08:35.350 --> 00:08:37.882
These two examples show Christianne,

00:08:37.906 --> 00:08:39.057
who lost her son,

00:08:39.081 --> 00:08:41.574
who radicalized and died
fighting for ISIS,

00:08:41.598 --> 00:08:43.265
and Daniel is a former neo-Nazi

00:08:43.289 --> 00:08:45.647
who was an extremely violent neo-Nazi,

00:08:45.671 --> 00:08:49.829
and they pose questions about their lives
and where they're at and regret,

00:08:49.853 --> 00:08:52.503
and force a classroom
to have a dialogue around it.

00:08:52.995 --> 00:08:55.980
Now, looking at that middle range
of individuals,

00:08:56.004 --> 00:08:58.703
actually, we need a lot
of civil society voices.

00:08:58.727 --> 00:09:02.172
How do you interact with people
that are looking for information online,

00:09:02.196 --> 00:09:04.538
that are starting to toy with an ideology,

00:09:04.562 --> 00:09:07.626
that are doing those searching
identity questions?

00:09:07.650 --> 00:09:09.792
How do we provide alternatives for that?

00:09:09.816 --> 00:09:13.206
And that's when we combine
large groups of civil society voices

00:09:13.230 --> 00:09:17.761
with creatives, techies,
app developers, artists, comedians,

00:09:17.785 --> 00:09:20.468
and we can create really specified content

00:09:20.492 --> 00:09:24.786
and actually, online, disseminate it
to very strategic audiences.

00:09:24.810 --> 00:09:27.613
So one example would be
creating a satirical video

00:09:27.637 --> 00:09:30.136
which makes fun of Islamophobia,

00:09:30.160 --> 00:09:34.096
and targeting it
to 15- to 20-year-olds online

00:09:34.120 --> 00:09:36.367
that have an interest in white power music

00:09:36.391 --> 00:09:38.790
and live specifically in Manchester.

00:09:38.814 --> 00:09:41.845
We can use these marketing tools
to be very specific,

00:09:41.869 --> 00:09:44.592
so that we know
when somebody's viewing, watching

00:09:44.616 --> 00:09:46.105
and engaging with that content,

00:09:46.129 --> 00:09:48.759
it's not just the average person,
it's not me or you --

00:09:48.783 --> 00:09:51.890
it's a very specific audience
that we are looking to engage with.

00:09:52.524 --> 00:09:56.223
Even more downstream, we developed
a pilot program called "One to One,"

00:09:56.247 --> 00:09:57.796
where we took former extremists

00:09:57.820 --> 00:10:02.684
and we had them reach out directly
to a group of labeled neofascists

00:10:02.708 --> 00:10:04.332
as well as Islamist extremists,

00:10:04.356 --> 00:10:08.171
and put direct messages through Facebook
Messenger into their inbox, saying,

00:10:08.195 --> 00:10:10.481
"Hey, I see where you're going.
I've been there.

00:10:10.505 --> 00:10:12.047
If you want to talk, I'm here."

00:10:12.071 --> 00:10:15.325
Now, we kind of expected death threats
from this sort of interaction.

00:10:15.349 --> 00:10:19.769
It's a little alarming to have
a former neo-Nazi say, "Hey, how are you?"

00:10:19.793 --> 00:10:22.000
But actually, we found
that around 60 percent

00:10:22.024 --> 00:10:24.578
of the people reached out to responded,

00:10:24.602 --> 00:10:28.687
and of that, around another 60 percent
had sustained engagement,

00:10:28.711 --> 00:10:30.767
meaning that they were
having conversations

00:10:30.791 --> 00:10:34.007
with the hardest people to reach
about what they were going through,

00:10:34.031 --> 00:10:35.182
planting seeds of doubt

00:10:35.206 --> 00:10:38.198
and giving them alternatives
for talking about these subjects,

00:10:38.222 --> 00:10:39.572
and that's really important.

00:10:40.881 --> 00:10:43.104
So what we're trying to do

00:10:43.128 --> 00:10:46.033
is actually bring
unlikely sectors to the table.

00:10:46.057 --> 00:10:48.383
We have amazing activists
all over the world,

00:10:48.407 --> 00:10:50.773
but oftentimes,
their messages are not strategic

00:10:50.797 --> 00:10:53.703
or they don't actually reach
the audiences they want to reach.

00:10:53.727 --> 00:10:55.966
So we work with networks
of former extremists.

00:10:55.990 --> 00:10:59.419
We work with networks of young people
in different parts of the world.

00:10:59.443 --> 00:11:02.213
And we work with them
to bring the tech sector to the table

00:11:02.237 --> 00:11:05.055
with artists and creatives
and marketing expertise

00:11:05.079 --> 00:11:10.080
so that we can actually have
a more robust and challenging of extremism

00:11:10.104 --> 00:11:11.404
that works together.

00:11:11.894 --> 00:11:14.474
So I would say
that if you are in the audience

00:11:14.498 --> 00:11:17.197
and you happen to be a graphic designer,

00:11:17.221 --> 00:11:19.403
a poet, a marketing expert,

00:11:19.427 --> 00:11:21.336
somebody that works in PR,

00:11:21.360 --> 00:11:22.713
a comedian --

00:11:22.737 --> 00:11:24.888
you might not think
that this is your sector,

00:11:24.912 --> 00:11:27.651
but actually, the skills
that you have right now

00:11:27.675 --> 00:11:29.678
might be exactly what is needed

00:11:29.702 --> 00:11:32.011
to help challenge extremism effectively.

00:11:32.035 --> 00:11:33.186
Thank you.

00:11:33.210 --> 00:11:37.423
(Applause)

