WEBVTT
Kind: captions
Language: en

00:00:25.000 --> 00:00:28.000
I want to talk today about --

00:00:28.000 --> 00:00:34.000
I've been asked to take the long view, and I'm going to tell you what

00:00:34.000 --> 00:00:38.000
I think are the three biggest problems for humanity

00:00:38.000 --> 00:00:41.000
from this long point of view.

00:00:41.000 --> 00:00:44.000
Some of these have already been touched upon by other speakers,

00:00:44.000 --> 00:00:46.000
which is encouraging.

00:00:46.000 --> 00:00:48.000
It seems that there's not just one person

00:00:48.000 --> 00:00:50.000
who thinks that these problems are important.

00:00:50.000 --> 00:00:54.000
The first is -- death is a big problem.

00:00:54.000 --> 00:00:57.000
If you look at the statistics,

00:00:57.000 --> 00:00:59.000
the odds are not very favorable to us.

00:00:59.000 --> 00:01:03.000
So far, most people who have lived have also died.

00:01:03.000 --> 00:01:07.000
Roughly 90 percent of everybody who has been alive has died by now.

00:01:07.000 --> 00:01:13.000
So the annual death rate adds up to 150,000 --

00:01:13.000 --> 00:01:16.000
sorry, the daily death rate -- 150,000 people per day,

00:01:16.000 --> 00:01:19.000
which is a huge number by any standard.

00:01:19.000 --> 00:01:24.000
The annual death rate, then, becomes 56 million.

00:01:24.000 --> 00:01:29.000
If we just look at the single, biggest cause of death -- aging --

00:01:30.000 --> 00:01:35.000
it accounts for roughly two-thirds of all human people who die.

00:01:35.000 --> 00:01:38.000
That adds up to an annual death toll

00:01:38.000 --> 00:01:40.000
of greater than the population of Canada.

00:01:40.000 --> 00:01:42.000
Sometimes, we don't see a problem

00:01:42.000 --> 00:01:46.000
because either it's too familiar or it's too big.

00:01:46.000 --> 00:01:48.000
Can't see it because it's too big.

00:01:48.000 --> 00:01:51.000
I think death might be both too familiar and too big

00:01:51.000 --> 00:01:54.000
for most people to see it as a problem.

00:01:54.000 --> 00:01:56.000
Once you think about it, you see this is not statistical points;

00:01:56.000 --> 00:01:58.000
these are -- let's see, how far have I talked?

00:01:58.000 --> 00:02:01.000
I've talked for three minutes.

00:02:01.000 --> 00:02:08.000
So that would be, roughly, 324 people have died since I've begun speaking.

00:02:08.000 --> 00:02:12.000
People like -- it's roughly the population in this room has just died.

00:02:13.000 --> 00:02:15.000
Now, the human cost of that is obvious,

00:02:15.000 --> 00:02:18.000
once you start to think about it -- the suffering, the loss --

00:02:18.000 --> 00:02:21.000
it's also, economically, enormously wasteful.

00:02:21.000 --> 00:02:24.000
I just look at the information, and knowledge, and experience

00:02:24.000 --> 00:02:27.000
that is lost due to natural causes of death in general,

00:02:27.000 --> 00:02:29.000
and aging, in particular.

00:02:29.000 --> 00:02:32.000
Suppose we approximated one person with one book?

00:02:32.000 --> 00:02:34.000
Now, of course, this is an underestimation.

00:02:34.000 --> 00:02:40.000
A person's lifetime of learning and experience

00:02:40.000 --> 00:02:42.000
is a lot more than you could put into a single book.

00:02:42.000 --> 00:02:44.000
But let's suppose we did this.

00:02:45.000 --> 00:02:50.000
52 million people die of natural causes each year

00:02:50.000 --> 00:02:54.000
corresponds, then, to 52 million volumes destroyed.

00:02:54.000 --> 00:02:57.000
Library of Congress holds 18 million volumes.

00:02:58.000 --> 00:03:01.000
We are upset about the burning of the Library of Alexandria.

00:03:01.000 --> 00:03:03.000
It's one of the great cultural tragedies

00:03:03.000 --> 00:03:06.000
that we remember, even today.

00:03:07.000 --> 00:03:09.000
But this is the equivalent of three Libraries of Congress --

00:03:09.000 --> 00:03:12.000
burnt down, forever lost -- each year.

00:03:12.000 --> 00:03:14.000
So that's the first big problem.

00:03:14.000 --> 00:03:17.000
And I wish Godspeed to Aubrey de Grey,

00:03:17.000 --> 00:03:19.000
and other people like him,

00:03:19.000 --> 00:03:22.000
to try to do something about this as soon as possible.

00:03:23.000 --> 00:03:26.000
Existential risk -- the second big problem.

00:03:26.000 --> 00:03:33.000
Existential risk is a threat to human survival, or to the long-term potential of our species.

00:03:33.000 --> 00:03:35.000
Now, why do I say that this is a big problem?

00:03:35.000 --> 00:03:39.000
Well, let's first look at the probability --

00:03:39.000 --> 00:03:42.000
and this is very, very difficult to estimate --

00:03:42.000 --> 00:03:45.000
but there have been only four studies on this in recent years,

00:03:45.000 --> 00:03:47.000
which is surprising.

00:03:47.000 --> 00:03:50.000
You would think that it would be of some interest

00:03:50.000 --> 00:03:54.000
to try to find out more about this given that the stakes are so big,

00:03:54.000 --> 00:03:56.000
but it's a very neglected area.

00:03:56.000 --> 00:03:58.000
But there have been four studies --

00:03:58.000 --> 00:04:00.000
one by John Lesley, wrote a book on this.

00:04:00.000 --> 00:04:02.000
He estimated a probability that we will fail

00:04:02.000 --> 00:04:05.000
to survive the current century: 50 percent.

00:04:05.000 --> 00:04:10.000
Similarly, the Astronomer Royal, whom we heard speak yesterday,

00:04:10.000 --> 00:04:13.000
also has a 50 percent probability estimate.

00:04:13.000 --> 00:04:16.000
Another author doesn't give any numerical estimate,

00:04:16.000 --> 00:04:19.000
but says the probability is significant that it will fail.

00:04:19.000 --> 00:04:22.000
I wrote a long paper on this.

00:04:22.000 --> 00:04:26.000
I said assigning a less than 20 percent probability would be a mistake

00:04:26.000 --> 00:04:29.000
in light of the current evidence we have.

00:04:29.000 --> 00:04:31.000
Now, the exact figures here,

00:04:31.000 --> 00:04:33.000
we should take with a big grain of salt,

00:04:33.000 --> 00:04:36.000
but there seems to be a consensus that the risk is substantial.

00:04:36.000 --> 00:04:39.000
Everybody who has looked at this and studied it agrees.

00:04:39.000 --> 00:04:41.000
Now, if we think about what just reducing

00:04:41.000 --> 00:04:46.000
the probability of human extinction by just one percentage point --

00:04:46.000 --> 00:04:51.000
not very much -- so that's equivalent to 60 million lives saved,

00:04:51.000 --> 00:04:55.000
if we just count the currently living people, the current generation.

00:04:55.000 --> 00:04:59.000
Now one percent of six billion people is equivalent to 60 million.

00:04:59.000 --> 00:05:01.000
So that's a large number.

00:05:01.000 --> 00:05:04.000
If we were to take into account future generations

00:05:04.000 --> 00:05:09.000
that will never come into existence if we blow ourselves up,

00:05:09.000 --> 00:05:12.000
then the figure becomes astronomical.

00:05:12.000 --> 00:05:15.000
If we could eventually colonize a chunk of the universe --

00:05:15.000 --> 00:05:17.000
the Virgo supercluster --

00:05:17.000 --> 00:05:19.000
maybe it will take us 100 million years to get there,

00:05:19.000 --> 00:05:22.000
but if we go extinct we never will.

00:05:22.000 --> 00:05:25.000
Then, even a one percentage point reduction

00:05:25.000 --> 00:05:29.000
in the extinction risk could be equivalent

00:05:29.000 --> 00:05:32.000
to this astronomical number -- 10 to the power of 32.

00:05:32.000 --> 00:05:36.000
So if you take into account future generations as much as our own,

00:05:36.000 --> 00:05:41.000
every other moral imperative of philanthropic cost just becomes irrelevant.

00:05:41.000 --> 00:05:43.000
The only thing you should focus on

00:05:43.000 --> 00:05:45.000
would be to reduce existential risk

00:05:45.000 --> 00:05:49.000
because even the tiniest decrease in existential risk

00:05:49.000 --> 00:05:53.000
would just overwhelm any other benefit you could hope to achieve.

00:05:53.000 --> 00:05:55.000
And even if you just look at the current people,

00:05:55.000 --> 00:06:00.000
and ignore the potential that would be lost if we went extinct,

00:06:00.000 --> 00:06:02.000
it should still have a high priority.

00:06:02.000 --> 00:06:07.000
Now, let me spend the rest of my time on the third big problem,

00:06:07.000 --> 00:06:12.000
because it's more subtle and perhaps difficult to grasp.

00:06:13.000 --> 00:06:17.000
Think about some time in your life --

00:06:17.000 --> 00:06:20.000
some people might never have experienced it -- but some people,

00:06:20.000 --> 00:06:23.000
there are just those moments that you have experienced

00:06:23.000 --> 00:06:25.000
where life was fantastic.

00:06:25.000 --> 00:06:32.000
It might have been at the moment of some great, creative inspiration

00:06:32.000 --> 00:06:34.000
you might have had when you just entered this flow stage.

00:06:34.000 --> 00:06:36.000
Or when you understood something you had never done before.

00:06:36.000 --> 00:06:40.000
Or perhaps in the ecstasy of romantic love.

00:06:40.000 --> 00:06:45.000
Or an aesthetic experience -- a sunset or a great piece of art.

00:06:45.000 --> 00:06:47.000
Every once in a while we have these moments,

00:06:47.000 --> 00:06:51.000
and we realize just how good life can be when it's at its best.

00:06:51.000 --> 00:06:56.000
And you wonder, why can't it be like that all the time?

00:06:56.000 --> 00:06:58.000
You just want to cling onto this.

00:06:58.000 --> 00:07:02.000
And then, of course, it drifts back into ordinary life and the memory fades.

00:07:02.000 --> 00:07:06.000
And it's really difficult to recall, in a normal frame of mind,

00:07:06.000 --> 00:07:09.000
just how good life can be at its best.

00:07:09.000 --> 00:07:12.000
Or how bad it can be at its worst.

00:07:12.000 --> 00:07:15.000
The third big problem is that life isn't usually

00:07:15.000 --> 00:07:17.000
as wonderful as it could be.

00:07:17.000 --> 00:07:21.000
I think that's a big, big problem.

00:07:21.000 --> 00:07:23.000
It's easy to say what we don't want.

00:07:24.000 --> 00:07:27.000
Here are a number of things that we don't want --

00:07:27.000 --> 00:07:30.000
illness, involuntary death, unnecessary suffering, cruelty,

00:07:30.000 --> 00:07:35.000
stunted growth, memory loss, ignorance, absence of creativity.

00:07:36.000 --> 00:07:39.000
Suppose we fixed these things -- we did something about all of these.

00:07:39.000 --> 00:07:41.000
We were very successful.

00:07:41.000 --> 00:07:43.000
We got rid of all of these things.

00:07:43.000 --> 00:07:46.000
We might end up with something like this,

00:07:46.000 --> 00:07:50.000
which is -- I mean, it's a heck of a lot better than that.

00:07:50.000 --> 00:07:55.000
But is this really the best we can dream of?

00:07:55.000 --> 00:07:57.000
Is this the best we can do?

00:07:57.000 --> 00:08:03.000
Or is it possible to find something a little bit more inspiring to work towards?

00:08:03.000 --> 00:08:05.000
And if we think about this,

00:08:05.000 --> 00:08:09.000
I think it's very clear that there are ways

00:08:09.000 --> 00:08:12.000
in which we could change things, not just by eliminating negatives,

00:08:12.000 --> 00:08:14.000
but adding positives.

00:08:14.000 --> 00:08:16.000
On my wish list, at least, would be:

00:08:16.000 --> 00:08:21.000
much longer, healthier lives, greater subjective well-being,

00:08:21.000 --> 00:08:26.000
enhanced cognitive capacities, more knowledge and understanding,

00:08:26.000 --> 00:08:28.000
unlimited opportunity for personal growth

00:08:28.000 --> 00:08:32.000
beyond our current biological limits, better relationships,

00:08:32.000 --> 00:08:34.000
an unbounded potential for spiritual, moral

00:08:34.000 --> 00:08:36.000
and intellectual development.

00:08:36.000 --> 00:08:44.000
If we want to achieve this, what, in the world, would have to change?

00:08:44.000 --> 00:08:49.000
And this is the answer -- we would have to change.

00:08:49.000 --> 00:08:52.000
Not just the world around us, but we, ourselves.

00:08:52.000 --> 00:08:56.000
Not just the way we think about the world, but the way we are -- our very biology.

00:08:56.000 --> 00:08:58.000
Human nature would have to change.

00:08:58.000 --> 00:09:00.000
Now, when we think about changing human nature,

00:09:00.000 --> 00:09:02.000
the first thing that comes to mind

00:09:02.000 --> 00:09:06.000
are these human modification technologies --

00:09:06.000 --> 00:09:08.000
growth hormone therapy, cosmetic surgery,

00:09:08.000 --> 00:09:11.000
stimulants like Ritalin, Adderall, anti-depressants,

00:09:11.000 --> 00:09:13.000
anabolic steroids, artificial hearts.

00:09:13.000 --> 00:09:16.000
It's a pretty pathetic list.

00:09:16.000 --> 00:09:18.000
They do great things for a few people

00:09:18.000 --> 00:09:20.000
who suffer from some specific condition,

00:09:20.000 --> 00:09:25.000
but for most people, they don't really transform

00:09:25.000 --> 00:09:27.000
what it is to be human.

00:09:27.000 --> 00:09:29.000
And they also all seem a little bit --

00:09:29.000 --> 00:09:32.000
most people have this instinct that, well, sure,

00:09:32.000 --> 00:09:34.000
there needs to be anti-depressants for the really depressed people.

00:09:34.000 --> 00:09:36.000
But there's a kind of queasiness

00:09:36.000 --> 00:09:39.000
that these are unnatural in some way.

00:09:39.000 --> 00:09:41.000
It's worth recalling that there are a lot of other

00:09:41.000 --> 00:09:44.000
modification technologies and enhancement technologies that we use.

00:09:44.000 --> 00:09:48.000
We have skin enhancements, clothing.

00:09:48.000 --> 00:09:52.000
As far as I can see, all of you are users of this

00:09:52.000 --> 00:09:57.000
enhancement technology in this room, so that's a great thing.

00:09:57.000 --> 00:10:00.000
Mood modifiers have been used from time immemorial --

00:10:00.000 --> 00:10:05.000
caffeine, alcohol, nicotine, immune system enhancement,

00:10:05.000 --> 00:10:07.000
vision enhancement, anesthetics --

00:10:07.000 --> 00:10:09.000
we take that very much for granted,

00:10:09.000 --> 00:10:13.000
but just think about how great progress that is --

00:10:13.000 --> 00:10:17.000
like, having an operation before anesthetics was not fun.

00:10:17.000 --> 00:10:23.000
Contraceptives, cosmetics and brain reprogramming techniques --

00:10:23.000 --> 00:10:25.000
that sounds ominous,

00:10:25.000 --> 00:10:29.000
but the distinction between what is a technology --

00:10:29.000 --> 00:10:31.000
a gadget would be the archetype --

00:10:31.000 --> 00:10:35.000
and other ways of changing and rewriting human nature is quite subtle.

00:10:35.000 --> 00:10:39.000
So if you think about what it means to learn arithmetic or to learn to read,

00:10:39.000 --> 00:10:42.000
you're actually, literally rewriting your own brain.

00:10:42.000 --> 00:10:45.000
You're changing the microstructure of your brain as you go along.

00:10:46.000 --> 00:10:49.000
So in a broad sense, we don't need to think about technology

00:10:49.000 --> 00:10:51.000
as only little gadgets, like these things here,

00:10:51.000 --> 00:10:55.000
but even institutions and techniques,

00:10:55.000 --> 00:10:57.000
psychological methods and so forth.

00:10:57.000 --> 00:11:02.000
Forms of organization can have a profound impact on human nature.

00:11:02.000 --> 00:11:04.000
Looking ahead, there is a range of technologies

00:11:04.000 --> 00:11:07.000
that are almost certain to be developed sooner or later.

00:11:07.000 --> 00:11:11.000
We are very ignorant about what the time scale for these things are,

00:11:11.000 --> 00:11:13.000
but they all are consistent with everything we know

00:11:13.000 --> 00:11:17.000
about physical laws, laws of chemistry, etc.

00:11:17.000 --> 00:11:19.000
It's possible to assume,

00:11:19.000 --> 00:11:22.000
setting aside a possibility of catastrophe,

00:11:22.000 --> 00:11:25.000
that sooner or later we will develop all of these.

00:11:25.000 --> 00:11:28.000
And even just a couple of these would be enough

00:11:28.000 --> 00:11:30.000
to transform the human condition.

00:11:30.000 --> 00:11:35.000
So let's look at some of the dimensions of human nature

00:11:35.000 --> 00:11:38.000
that seem to leave room for improvement.

00:11:38.000 --> 00:11:40.000
Health span is a big and urgent thing,

00:11:40.000 --> 00:11:42.000
because if you're not alive,

00:11:42.000 --> 00:11:45.000
then all the other things will be to little avail.

00:11:45.000 --> 00:11:47.000
Intellectual capacity -- let's take that box,

00:11:47.000 --> 00:11:52.000
which falls into a lot of different sub-categories:

00:11:52.000 --> 00:11:55.000
memory, concentration, mental energy, intelligence, empathy.

00:11:55.000 --> 00:11:57.000
These are really great things.

00:11:57.000 --> 00:11:59.000
Part of the reason why we value these traits

00:11:59.000 --> 00:12:03.000
is that they make us better at competing with other people --

00:12:03.000 --> 00:12:05.000
they're positional goods.

00:12:05.000 --> 00:12:07.000
But part of the reason --

00:12:07.000 --> 00:12:11.000
and that's the reason why we have ethical ground for pursuing these --

00:12:11.000 --> 00:12:14.000
is that they're also intrinsically valuable.

00:12:14.000 --> 00:12:18.000
It's just better to be able to understand more of the world around you

00:12:18.000 --> 00:12:20.000
and the people that you are communicating with,

00:12:20.000 --> 00:12:24.000
and to remember what you have learned.

00:12:24.000 --> 00:12:26.000
Modalities and special faculties.

00:12:26.000 --> 00:12:31.000
Now, the human mind is not a single unitary information processor,

00:12:31.000 --> 00:12:35.000
but it has a lot of different, special, evolved modules

00:12:35.000 --> 00:12:37.000
that do specific things for us.

00:12:37.000 --> 00:12:41.000
If you think about what we normally take as giving life a lot of its meaning --

00:12:41.000 --> 00:12:45.000
music, humor, eroticism, spirituality, aesthetics,

00:12:45.000 --> 00:12:50.000
nurturing and caring, gossip, chatting with people --

00:12:50.000 --> 00:12:54.000
all of these, very likely, are enabled by a special circuitry

00:12:54.000 --> 00:12:56.000
that we humans have,

00:12:56.000 --> 00:12:59.000
but that you could have another intelligent life form that lacks these.

00:12:59.000 --> 00:13:02.000
We're just lucky that we have the requisite neural machinery

00:13:02.000 --> 00:13:06.000
to process music and to appreciate it and enjoy it.

00:13:06.000 --> 00:13:09.000
All of these would enable, in principle -- be amenable to enhancement.

00:13:09.000 --> 00:13:11.000
Some people have a better musical ability

00:13:11.000 --> 00:13:13.000
and ability to appreciate music than others have.

00:13:13.000 --> 00:13:16.000
It's also interesting to think about what other things are --

00:13:16.000 --> 00:13:19.000
so if these all enabled great values,

00:13:20.000 --> 00:13:23.000
why should we think that evolution has happened to provide us

00:13:23.000 --> 00:13:26.000
with all the modalities we would need to engage

00:13:26.000 --> 00:13:28.000
with other values that there might be?

00:13:28.000 --> 00:13:30.000
Imagine a species

00:13:30.000 --> 00:13:34.000
that just didn't have this neural machinery for processing music.

00:13:34.000 --> 00:13:37.000
And they would just stare at us with bafflement

00:13:37.000 --> 00:13:41.000
when we spend time listening to a beautiful performance,

00:13:41.000 --> 00:13:43.000
like the one we just heard -- because of people making stupid movements,

00:13:43.000 --> 00:13:46.000
and they would be really irritated and wouldn't see what we were up to.

00:13:46.000 --> 00:13:49.000
But maybe they have another faculty, something else

00:13:49.000 --> 00:13:52.000
that would seem equally irrational to us,

00:13:52.000 --> 00:13:55.000
but they actually tap into some great possible value there.

00:13:55.000 --> 00:13:59.000
But we are just literally deaf to that kind of value.

00:13:59.000 --> 00:14:01.000
So we could think of adding on different,

00:14:01.000 --> 00:14:05.000
new sensory capacities and mental faculties.

00:14:05.000 --> 00:14:10.000
Bodily functionality and morphology and affective self-control.

00:14:10.000 --> 00:14:12.000
Greater subjective well-being.

00:14:12.000 --> 00:14:15.000
Be able to switch between relaxation and activity --

00:14:15.000 --> 00:14:19.000
being able to go slow when you need to do that, and to speed up.

00:14:19.000 --> 00:14:21.000
Able to switch back and forth more easily

00:14:21.000 --> 00:14:23.000
would be a neat thing to be able to do --

00:14:23.000 --> 00:14:25.000
easier to achieve the flow state,

00:14:25.000 --> 00:14:29.000
when you're totally immersed in something you are doing.

00:14:29.000 --> 00:14:31.000
Conscientiousness and sympathy.

00:14:31.000 --> 00:14:34.000
The ability to -- it's another interesting application

00:14:34.000 --> 00:14:37.000
that would have large social ramification, perhaps.

00:14:37.000 --> 00:14:43.000
If you could actually choose to preserve your romantic attachments to one person,

00:14:43.000 --> 00:14:45.000
undiminished through time,

00:14:45.000 --> 00:14:48.000
so that wouldn't have to -- love would never have to fade if you didn't want it to.

00:14:50.000 --> 00:14:53.000
That's probably not all that difficult.

00:14:53.000 --> 00:14:56.000
It might just be a simple hormone or something that could do this.

00:14:58.000 --> 00:15:00.000
It's been done in voles.

00:15:02.000 --> 00:15:05.000
You can engineer a prairie vole to become monogamous

00:15:05.000 --> 00:15:07.000
when it's naturally polygamous.

00:15:07.000 --> 00:15:09.000
It's just a single gene.

00:15:09.000 --> 00:15:11.000
Might be more complicated in humans, but perhaps not that much.

00:15:11.000 --> 00:15:13.000
This is the last picture that I want to --

00:15:14.000 --> 00:15:16.000
now we've got to use the laser pointer.

00:15:17.000 --> 00:15:20.000
A possible mode of being here would be a way of life --

00:15:20.000 --> 00:15:24.000
a way of being, experiencing, thinking, seeing,

00:15:24.000 --> 00:15:26.000
interacting with the world.

00:15:26.000 --> 00:15:31.000
Down here in this little corner, here, we have the little sub-space

00:15:31.000 --> 00:15:35.000
of this larger space that is accessible to human beings --

00:15:35.000 --> 00:15:38.000
beings with our biological capacities.

00:15:38.000 --> 00:15:41.000
It's a part of the space that's accessible to animals;

00:15:41.000 --> 00:15:44.000
since we are animals, we are a subset of that.

00:15:44.000 --> 00:15:48.000
And then you can imagine some enhancements of human capacities.

00:15:48.000 --> 00:15:51.000
There would be different modes of being you could experience

00:15:51.000 --> 00:15:54.000
if you were able to stay alive for, say, 200 years.

00:15:54.000 --> 00:15:58.000
Then you could live sorts of lives and accumulate wisdoms

00:15:58.000 --> 00:16:01.000
that are just not possible for humans as we currently are.

00:16:01.000 --> 00:16:05.000
So then, you move off to this larger sphere of "human +,"

00:16:05.000 --> 00:16:08.000
and you could continue that process and eventually

00:16:08.000 --> 00:16:12.000
explore a lot of this larger space of possible modes of being.

00:16:12.000 --> 00:16:14.000
Now, why is that a good thing to do?

00:16:14.000 --> 00:16:18.000
Well, we know already that in this little human circle there,

00:16:18.000 --> 00:16:22.000
there are these enormously wonderful and worthwhile modes of being --

00:16:22.000 --> 00:16:25.000
human life at its best is wonderful.

00:16:25.000 --> 00:16:30.000
We have no reason to believe that within this much, much larger space

00:16:30.000 --> 00:16:34.000
there would not also be extremely worthwhile modes of being,

00:16:34.000 --> 00:16:40.000
perhaps ones that would be way beyond our wildest ability

00:16:40.000 --> 00:16:42.000
even to imagine or dream about.

00:16:42.000 --> 00:16:44.000
And so, to fix this third problem,

00:16:44.000 --> 00:16:50.000
I think we need -- slowly, carefully, with ethical wisdom and constraint --

00:16:50.000 --> 00:16:55.000
develop the means that enable us to go out in this larger space and explore it

00:16:55.000 --> 00:16:57.000
and find the great values that might hide there.

00:16:57.000 --> 00:16:59.000
Thanks.

