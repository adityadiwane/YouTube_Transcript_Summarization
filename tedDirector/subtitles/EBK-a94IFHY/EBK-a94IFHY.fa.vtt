WEBVTT
Kind: captions
Language: fa

00:00:00.000 --> 00:00:07.000
Translator: Iman Mirzadeh
Reviewer: Leila Ataei

00:00:12.532 --> 00:00:14.084
این لی سادل هست.

00:00:14.108 --> 00:00:18.105
لی سادل یکی از بزرگترین
بازیکن های Go (یک بازی فکری)در جهان هست.

00:00:18.129 --> 00:00:21.014
و در حال تجربه لحظه‌ای هست
که دوستان من در سیلیکون ولی

00:00:21.038 --> 00:00:22.548
بهش میگن لحظه "یا پیغمبر!"

00:00:22.572 --> 00:00:23.645
(خنده حاضرین)

00:00:23.669 --> 00:00:25.857
لحظه‌ای که م‌ف‌همیم

00:00:25.881 --> 00:00:29.177
هوش مصنوعی واقعا داره سریعتر از چیزی که
انتظارشو داشتیم پیشرفت میکنه.

00:00:29.974 --> 00:00:33.021
پس، انسان‌ها روی تخته Go باخته‌‌اند.
در دنیای واقعی چطور؟

00:00:33.045 --> 00:00:35.145
باید بگم که دنیای واقعی خیلی بزرگتر

00:00:35.169 --> 00:00:37.418
و بسیار بسیار پیچیده‌تر
از تخته بازی Go هست.

00:00:37.442 --> 00:00:39.261
(دنیای واقعی) خیلی نامرئی‌تر هست،

00:00:39.285 --> 00:00:41.323
ولی همچنان مساله تصمیم گیری هست.

00:00:42.768 --> 00:00:45.089
و اگر درباره برخی از تکنولوژی‌ها فکر کنیم

00:00:45.113 --> 00:00:46.862
که در حال ظهور هستند...

00:00:47.558 --> 00:00:51.893
[Noriko [Arai اشاره کرده است که
توانایی خواندن هنوز در ماشین‌ها وجود ندارد،

00:00:51.917 --> 00:00:53.417
حداقل همراه با فهمیدن نیست.

00:00:53.441 --> 00:00:54.977
ولی این اتفاق خواهد افتاد.

00:00:55.001 --> 00:00:56.772
و وقتی که به وقوع بپیوندد،

00:00:56.796 --> 00:00:57.983
پس از آن، خیلی زود

00:00:58.007 --> 00:01:02.579
ماشین‌ها تمام آن‌چه را که
بشر نوشته است، خواهند خواند.

00:01:03.670 --> 00:01:05.700
و این ماشین‌ها را قادر می‌سازد

00:01:05.724 --> 00:01:08.644
که فراتر از انسان‌ها به آینده نگاه کنند،

00:01:08.668 --> 00:01:10.348
همانطور که قبلاً در ‌Go دیده‌ایم،

00:01:10.372 --> 00:01:12.536
اگر ماشین‌ها به اطلاعات بیشتری
دسترسی داشته باشند،

00:01:12.560 --> 00:01:16.828
می‌توانند تصمیمات بهتری
در جهان واقعی نسبت به ما بگیرند.

00:01:18.612 --> 00:01:20.218
آیا این یک اتفاق خوب است؟

00:01:21.718 --> 00:01:23.950
خب، امیدوارم که باشه.

00:01:26.514 --> 00:01:29.769
تمام تمدن ما،
هر چیزی که برایش ارزش قائل هستیم،

00:01:29.793 --> 00:01:31.861
بر پایه هوشمندی ما است.

00:01:31.885 --> 00:01:35.579
و اگر ما هوش بیشتری در اختیار داشتیم،

00:01:35.603 --> 00:01:38.905
آن‌ وقت هیچ حد و مرزی برای کارهایی
که انسان می‌تواند بکند وجود نداشت.

00:01:40.485 --> 00:01:43.810
و من فکر می‌کنم که،
همانطور که برخی توصیف کرد‌اند،

00:01:43.834 --> 00:01:45.850
این می‌تواند
بزرگترین رویداد تاریخ بشریت باشد.

00:01:48.485 --> 00:01:51.314
پس چرا بعضی‌ها حرفهایی میزنند،

00:01:51.338 --> 00:01:54.214
مثلا اینکه هوش مصنوعی می‌تواند
خاتمه دهنده نسل بشر باشد؟

00:01:55.258 --> 00:01:56.917
آیا این یک پدیده جدید است؟

00:01:56.941 --> 00:02:01.051
آیا فقط ایلان ماسک و بیل گیتس
و استیون هاوکینگ هستند؟

00:02:01.773 --> 00:02:05.035
راستش، نه.
این ایده خیلی وقته که وجود دارد.

00:02:05.059 --> 00:02:07.021
یه نقل قول میگه:

00:02:07.045 --> 00:02:11.395
«حتی اگر میتونستیم ماشین‌ها رو
فرمانبردار نگه داریم

00:02:11.419 --> 00:02:14.403
مثلا با خاموش کردنشان در لحظات استراتژیک»

00:02:14.427 --> 00:02:17.664
-- و من بعداً
به ایده «خاموش کردن»برمی‌گردم--

00:02:17.688 --> 00:02:20.492
«ما به عنوان یک گونه،
باید خیلی احساس پستی کنیم»

00:02:21.997 --> 00:02:25.445
کی این رو گفته؟
آلن تورینگ در سال ۱۹۵۱

00:02:26.120 --> 00:02:28.883
آلن تورینگ، همانطور که می‌دانید
پدر علم کامپیوتر هست.

00:02:28.907 --> 00:02:31.955
و از خیلی از جهات،
پدر علم هوش مصنوعی هم هست.

00:02:33.059 --> 00:02:34.941
پس اگر درباره این مساله فکر کنیم،

00:02:34.965 --> 00:02:38.752
مساله ساختن چیزی هوشمندتر از
گونه خودمان،

00:02:38.776 --> 00:02:41.398
شاید این رو «مساله گوریل» بنامیم.

00:02:42.165 --> 00:02:45.915
چون اجداد گوریل‌ها این کار رو
چند میلیون‌ سال قبل انجام داده اند،

00:02:45.939 --> 00:02:47.684
و الان می‌توانیم از گوریل‌ها بپرسیم که:

00:02:48.572 --> 00:02:49.732
آیا این کار ایده‌ خوبی بود؟

00:02:49.756 --> 00:02:53.286
اینم از گوریل‌هایی که در جلسه‌ای،
درباره اینکه آیا ایده خوبی بود بحث میکنند

00:02:53.310 --> 00:02:56.656
و بعد از مدت کوتاهی،
به این نتیجه میرسن که، نه

00:02:56.680 --> 00:02:58.025
یک ایده افتضاح بود

00:02:58.049 --> 00:02:59.831
گونه ما،‌ در تنگنای بدی قرار دارد

00:03:00.358 --> 00:03:04.621
در واقع، شما می‌توانید غم عالم رو
در چشمانشان ببینید

00:03:04.645 --> 00:03:06.285
(خنده حاضرین)

00:03:06.309 --> 00:03:11.149
پس شاید این احساس ناراحتی از به وجود آوردن
چیزی هوشمندتر از گونه خود

00:03:11.173 --> 00:03:13.538
ایده خوبی نباشد

00:03:14.308 --> 00:03:15.799
ما چه کاری از دستمان برمی‌آید؟

00:03:15.823 --> 00:03:20.590
درواقع، هیچی به جز متوقف کردن هوش مصنوعی،

00:03:20.614 --> 00:03:23.124
و به دلیل تمام فوایدی که گفتم

00:03:23.148 --> 00:03:24.864
و به دلیل اینکه من یک 
محقق هوش مصنوعی هستم

00:03:24.888 --> 00:03:26.679
من این مورد رو قبول ندارم.

00:03:27.103 --> 00:03:29.571
من میخوام که بتوانم همچنان
روی هوش مصنوعی کار کنم.

00:03:30.435 --> 00:03:33.113
پس باید این مساله رو بیشتر واکاوی کنیم.

00:03:33.137 --> 00:03:34.508
مشکل واقعا چی هست؟

00:03:34.532 --> 00:03:37.778
چرا هوش مصنوعی بهتر
منجر به فاجعه می‌شود؟

00:03:39.218 --> 00:03:40.716
اینم یک نقل قول دیگه:

00:03:41.755 --> 00:03:45.090
«بهتره که مطمئن باشیم
هدفی که در ماشین‌ قرار‌ میدهیم

00:03:45.114 --> 00:03:47.412
همان هدفی است که واقعا میخواهیم.»

00:03:48.102 --> 00:03:51.600
که توسط نوربرت وینر در ۱۹۶۰ گفته شده،

00:03:51.624 --> 00:03:55.626
بلافاصله بعد از اینکه وی دید
یکی از سیستم‌های یادگیرنده اولیه

00:03:55.650 --> 00:03:58.233
بازی چکرز رو
بهتر از سازندگانش بازی می‌کند.

00:04:00.422 --> 00:04:03.105
ولی این جمله می‌توانست توسط

00:04:03.129 --> 00:04:04.296
شاه میداس هم گفته شده باشد.

00:04:04.903 --> 00:04:08.037
شاه میداس گفت: «من میخواهم
هرچه را که لمس میکنم تبدیل به طلا شود،»

00:04:08.061 --> 00:04:10.534
و او دقیقاً چیزی را که خواسته بود گرفت.

00:04:10.558 --> 00:04:13.309
و آن هدفی بود که وی در ماشین قرار داد.

00:04:13.333 --> 00:04:14.783
اینطور که میگن،

00:04:14.807 --> 00:04:18.251
و بعدش غذا، نوشیدنی و اقوامش
تبدیل به طلا شدند.

00:04:18.275 --> 00:04:20.556
و از بدبختی و گشنگی مرد.

00:04:22.264 --> 00:04:24.605
پس ما به این مشکل میگوییم:
«مشکل شاه میداس»

00:04:24.629 --> 00:04:27.934
که در آن هدف را چیزی قرار می‌دهیم،

00:04:27.958 --> 00:04:30.371
که واقعاً هم جهت با
چیزی که ما مي‌خواهیم نیست.

00:04:30.395 --> 00:04:33.648
به بیان جدیدتر، به این مشکل میگیم:
«مشکل هم جهت سازی ارزش»

00:04:36.867 --> 00:04:40.352
هدف گذاری اشتباه تنها بخش مشکل نیست.

00:04:40.376 --> 00:04:41.528
بخش دیگری هم هست

00:04:41.980 --> 00:04:43.923
اگر شما یک هدف برای ماشین قرار دهید

00:04:43.947 --> 00:04:46.395
حتی به سادگیج «آوردن قهوه»

00:04:47.728 --> 00:04:49.569
ماشین به خودش میگه

00:04:50.553 --> 00:04:53.176
«چطوری ممکنه که من
نتونم قهوه رو بیارم؟

00:04:53.200 --> 00:04:54.780
یکی ممکنه منو خاموش کنه..

00:04:55.465 --> 00:04:57.852
خب پس من باید کاری کنم
که جلوی این کار رو بگیرم.

00:04:57.876 --> 00:04:59.782
من دکمه «خاموش» خودمو غیرفعال می‌کنم.

00:05:00.354 --> 00:05:03.313
من هرکاری میکنم تا
از خودم در برابر موانعی که

00:05:03.337 --> 00:05:05.966
سد راه هدفی که به من داده شده می‌شوند،
دفاع کنم.»

00:05:05.990 --> 00:05:08.002
بنابراین، این دنبال کردن تک-ذهنه

00:05:09.033 --> 00:05:11.978
در یک حالت خیلی دفاعی از هدف،

00:05:12.002 --> 00:05:14.816
در واقع، هم جهت با اهداف گونه انسان نیست.

00:05:15.942 --> 00:05:17.804
این مشکلی هست 
که باهاش مواجه هستیم

00:05:18.827 --> 00:05:23.594
و در واقع، این
نکته با ارزش این سخنرانی هست.

00:05:23.618 --> 00:05:25.673
اگر میخواهید 
فقط یک چیز را به خاطرتون بسپرید،

00:05:25.697 --> 00:05:28.372
اون اینه که اگر شما بمیرین،
دیگه نمی‌تونین قهوه رو بیارین

00:05:28.396 --> 00:05:29.457
(خنده حاضرین)

00:05:29.481 --> 00:05:33.310
خیلی سادست. فقط اینو یادتون باشه.
روزی سه بار با خودتون تکرار کنین.

00:05:33.334 --> 00:05:35.155
(خنده حضار)

00:05:35.179 --> 00:05:37.933
و در واقع، این نکته

00:05:37.957 --> 00:05:40.605
دقیقا پلات فیلم 2001 [A space Odyssey] است.

00:05:41.046 --> 00:05:43.136
HAL یک هدف داره، یک ماموریت

00:05:43.160 --> 00:05:46.892
که هم‌جهت با اهداف انسان‌ها نیست

00:05:46.916 --> 00:05:48.726
و این باعث بروز مشکلاتی میشه

00:05:49.314 --> 00:05:52.283
البته خوشبختانه، HAL خیلی باهوش نیست

00:05:52.307 --> 00:05:55.894
اون نسبتا باهوش هست، ولی در نهایت
Dave گولش میزنه.

00:05:55.918 --> 00:05:57.767
و میتونه خاموشش کنه.

00:06:01.648 --> 00:06:03.267
ولی شاید ما خیلی خوش‌ شانس نباشیم.

00:06:08.013 --> 00:06:09.605
پس باید چیکار کنیم؟

00:06:12.191 --> 00:06:14.792
من سعی میکنم هوش مصنوعی رو باز تعریف کنم.

00:06:14.816 --> 00:06:16.877
تا از این تصور سنتی بیرون بیایم.

00:06:16.901 --> 00:06:21.468
که طبق اون، ماشین‌هایی هستند 
که به صورت هوشمند اهداف رو دنبال میکنن.

00:06:22.532 --> 00:06:24.330
۳ تا اصل رو باید در نظر گرفت.

00:06:24.354 --> 00:06:27.643
اولین اصل، نوع دوستی هست.
اگر شما دوست داشته باشین

00:06:27.667 --> 00:06:30.929
تنها هدف ربات

00:06:30.953 --> 00:06:35.199
این باشه که اهداف انسان رو واقعیت ببخشه.

00:06:35.223 --> 00:06:36.613
و ارزش‌های انسانی رو.

00:06:36.637 --> 00:06:39.967
و منظورم از ارزش‌ها،
ارزش‌های احساسی یا خیرخواهانه نیست.

00:06:43.802 --> 00:06:45.145
زندگیش اون شکلی باشه.

00:06:47.184 --> 00:06:49.493
و این در واقع قانون آسیموف رو نقض میکنه.

00:06:49.517 --> 00:06:51.846
که ربات باید از حیات خودش محافظت کنه.

00:06:51.870 --> 00:06:55.593
ربات هیچ علاقه‌ای
به مراقبت از حیات خودش نداره.

00:06:57.240 --> 00:07:01.008
قانون دوم، قانون فروتنی هست.
البته اگه خوشتون بیاد.

00:07:01.794 --> 00:07:05.537
و این قانون نقش مهمی
در امن کردن ربات‌ها داره.

00:07:05.561 --> 00:07:08.703
طبق این قانون، ربات نمیدونه که

00:07:08.727 --> 00:07:10.755
ارزش‌های انسانی چه چیز‌هایی هستند

00:07:10.779 --> 00:07:13.957
باید در راستای محقق شدنشون تلاش کنه،
ولی نمیدونه چه چیزهایی هستند.

00:07:15.074 --> 00:07:17.700
و این از مشکل دنبال کردن تک-ذهنی هدف

00:07:17.724 --> 00:07:18.936
جلوگیری می‌کنه.

00:07:18.960 --> 00:07:21.132
این عدم قطعیت بسیار مهم هست.

00:07:21.546 --> 00:07:23.185
حالا، برای اینکه ربات برای ما مفید باشه

00:07:23.209 --> 00:07:25.940
باید یک ایده‌ای
ازچیزی که میخوایم داشته باشه.

00:07:27.043 --> 00:07:32.470
و این اطلاعات رو در درجه اول
از مشاهده انتخاب‌های انسان به دست می‌آره.

00:07:32.494 --> 00:07:35.295
پس، انتخاب‌های خود ما هستند
که اطلاعات رو آشکار میکنن

00:07:35.319 --> 00:07:38.619
درباره چیزی که ما ترجیح میدیم
زندگیمون شبیه به اون باشه.

00:07:40.452 --> 00:07:42.135
پس این ۳ اصل بیان شد.

00:07:42.159 --> 00:07:44.477
حالا بیاین ببینیم که این اصول،
چگونه روی این سوال عمل میکند:

00:07:44.501 --> 00:07:47.290
«آیا میتونی ماشین رو خاموش کنی؟»
همانطور که تورینگ پیشنهاد داد.

00:07:48.893 --> 00:07:51.013
این یک ربات مدل PR2 هستش.

00:07:51.037 --> 00:07:52.858
که ما یک نمونه از آن در آزمایشگاهمون داریم

00:07:52.882 --> 00:07:55.785
و یک دکمه بزرگ قرمز
برای «خاموش» کردن در پشتش داره.

00:07:56.361 --> 00:07:58.976
سوال اینه که آیا
این ربات بهتون اجازه میده که خاموشش کنین؟

00:07:59.000 --> 00:08:00.465
اگر ما از راه سنتی عمل کنیم،

00:08:00.489 --> 00:08:03.971
بهش هدف این هدف رو میدیم:
«قهوه روبیار، من باید قهوه رو بیارم

00:08:03.995 --> 00:08:06.575
من نمیتونم قهوه بیارم اگر مرده باشم»

00:08:06.599 --> 00:08:09.940
به وضوح، PR2 به سخنرانی من گوش کرده،

00:08:09.964 --> 00:08:13.717
پس میگه
من باید دکمه «خاموش» رو غیرفعال کنم.

00:08:14.796 --> 00:08:17.490
و احتمالاً با دستگاه شوک،
به تمام مردم داخل استارباکس شلیک کنم!

00:08:17.514 --> 00:08:19.074
کسانی که ممکنه سد راه من باشن.

00:08:19.098 --> 00:08:21.160
(خنده حاضرین)

00:08:21.184 --> 00:08:23.337
پس این مساله
به نظر اجتناب ناپذیر میاد، درسته؟

00:08:23.361 --> 00:08:25.759
این حالت شکست به نظر اجتناب ناپذیر هست،

00:08:25.783 --> 00:08:29.326
و از داشتن یک هدف دقیق و محکم نشأت میگیره.

00:08:30.632 --> 00:08:33.776
حالا چه اتفاقی می‌افته
اگه ماشین درباره هدف مطمئن نباشه؟

00:08:33.800 --> 00:08:35.927
اینطوری، یک جور دیگه استدلال میکنه.

00:08:35.951 --> 00:08:38.375
میگه: «باشه، انسان ممکنه منو خاموش کنه

00:08:38.964 --> 00:08:40.830
ولی تنها در صورتی این کارو میکنه
که من کار اشتباهی بکنم.

00:08:41.567 --> 00:08:44.042
من نمیدونم چه کاری اشتباهه

00:08:44.066 --> 00:08:46.110
ولی میدونم که نمیخوام انجامش بدم.»

00:08:46.134 --> 00:08:49.144
این اصل اول و دوم گفته شده بود.

00:08:49.168 --> 00:08:52.527
«پس باید بذارم که که انسان منو خاموش کنه.»

00:08:53.541 --> 00:08:57.497
و درواقع، شما میتونین انگیزه ای که ربات
برای خاموش کردنش

00:08:57.521 --> 00:09:00.014
توسط انسان دارد رو محاسبه کنید،

00:09:00.038 --> 00:09:01.952
و این مستقیماً مرتبطه با درجه عدم قطعیت

00:09:01.976 --> 00:09:04.722
درباره اهداف اصولی دارد.

00:09:05.797 --> 00:09:08.746
و وقتی که ماشین خاموش بشه

00:09:08.770 --> 00:09:10.575
اصل سوم وارد عمل میشه.

00:09:10.599 --> 00:09:13.661
ربات یه چیزی درباره اهداف یاد میگیره،
باید پیگیر باشه،

00:09:13.685 --> 00:09:16.218
چون‌ یاد میگیره کاری که کرده درست نبوده.

00:09:16.242 --> 00:09:19.812
در واقع، ما میتونیم
با استفاده درست از نماد‌های یونانی

00:09:19.836 --> 00:09:21.967
همانطور که معمولاً
ریاضیدانان این کار رو میکنن

00:09:21.991 --> 00:09:23.975
میتونیم یک قضیه رو ثابت کنیم

00:09:23.999 --> 00:09:27.552
که میگه،‌ این چنین رباتی
قطعا برای انسان مفید است.

00:09:27.576 --> 00:09:31.379
قطعا وجود ماشینی که اینطوری طراحی شده باشه

00:09:31.403 --> 00:09:32.649
از نبودنش بهتره.

00:09:33.057 --> 00:09:35.963
این یک مثال ساده است،
ولی قدم اول راه ماست.

00:09:35.987 --> 00:09:39.890
راه استفاده از هوش مصنوعی سازگار با انسان.

00:09:42.477 --> 00:09:45.734
حالا، اصل سوم

00:09:45.758 --> 00:09:48.870
فکر کنم چیزی باشه
که احتمالا درکش براتون سخت باشه.

00:09:48.894 --> 00:09:52.133
احتمالا شما فکر میکنین که
«راستش، من بد رفتار میکنم

00:09:52.157 --> 00:09:55.086
و نمیخوام که ربات من مثل من رفتار کنه.

00:09:55.110 --> 00:09:58.544
من نصف شب دزدکی میرم سر یخچال.

00:09:58.568 --> 00:09:59.736
یا فلان کار رو میکنم.»

00:09:59.760 --> 00:10:02.557
خیلی از کارها هست که شما دوست ندارین
رباتتون انجامشون بده

00:10:02.581 --> 00:10:04.652
ولی در واقع، اصل مطلب اینطوری نیست.

00:10:04.676 --> 00:10:06.831
به صرف اینکه شما بد رفتار میکنین،

00:10:06.855 --> 00:10:09.478
دلیل نمیشه ربات هم از رفتار شما تقلید کنه.

00:10:09.502 --> 00:10:13.412
ربات قراره که انگیزه‌های شما رو بفهمه،
و شاید در راستای رسیدن بهش کمکتون کنه،

00:10:13.436 --> 00:10:14.756
البته اگر مناسب باشه.

00:10:16.026 --> 00:10:17.490
ولی همچنان کاری سختیه.

00:10:18.122 --> 00:10:20.667
در واقع، کاری که ما سعی میکنیم انجام بدیم

00:10:20.691 --> 00:10:26.487
اینه که به ماشین‌ها اجازه بدیم
برای هر فرد و هر زندگی ممکن پیش‌بینی کنند

00:10:26.511 --> 00:10:27.672
که آیا میتونن زنده بمونن

00:10:27.696 --> 00:10:29.293
و البته جان بقیه:

00:10:29.317 --> 00:10:31.834
این که کدوم رو ترجیح میدن؟

00:10:33.881 --> 00:10:36.835
و سختی‌های بسیار زیادی
برای انجام این کار وجود دارن؛

00:10:36.859 --> 00:10:39.791
و من انتظار ندارم
که این مساله به زودی‌‌ حل بشه.

00:10:39.815 --> 00:10:42.458
و چالش اصلی، در واقع خود ما هستیم.

00:10:43.969 --> 00:10:47.086
همانطور که قبلا اشاره کردم،
ما بد رفتار میکنیم.

00:10:47.110 --> 00:10:49.431
در واقعا بعضی از ما
کاملا بدجنس هستیم.

00:10:50.251 --> 00:10:53.303
حالا همانطور که گفتم،
ربات مجبور نیست که رفتار رو تقلید کنه.

00:10:53.327 --> 00:10:56.118
ربات از خودش هیچ هدفی ندارد.

00:10:56.142 --> 00:10:57.879
و کاملا نوع دوست هست.

00:10:59.113 --> 00:11:04.334
و برای این طراحی نشده که خواسته های
یک انسان یا کاربر رو برآورده کنه،

00:11:04.358 --> 00:11:07.496
در حقیقت،‌ ربات باید
به ترجیحات همه احترام بگذارد.

00:11:09.083 --> 00:11:11.653
پس میتونه مقدار مشخصی
از بدرفتاری رو تحمل کنه،

00:11:11.677 --> 00:11:15.378
و حتی میتونه سوءرفتار شما رو درک کنه،

00:11:15.402 --> 00:11:18.073
مثلا اینکه شما
به عنوان مامور اداره گذرنامه رشوه میگیرد

00:11:18.097 --> 00:11:21.909
دلیلش اینه که شما نان آور خانواده اید
و بچه هاتون رو به مدرسه بفرستین.

00:11:21.933 --> 00:11:24.839
ربات میتونه این موضوع رو بفهمه،
و اینطوری نیست که دزدی کنه.

00:11:24.863 --> 00:11:27.542
در واقع کمکتون خواهد کرد
که بچه‌هاتون رو به مدرسه بفرستین.

00:11:28.796 --> 00:11:31.808
ما همچنین از نظر محاسباتی هم محدود هستیم.

00:11:31.832 --> 00:11:34.337
Lee Sedol یک بازیکن با استعداد Go هست.

00:11:34.361 --> 00:11:35.686
ولی با این وجود بازنده است.

00:11:35.710 --> 00:11:39.949
پس اگر به حرکاتش دقت کنیم،
یک حرکتی بود که به شکستش منجر شد.

00:11:39.973 --> 00:11:42.134
این به این معنی نیست که اون میخواست ببازه.

00:11:43.160 --> 00:11:45.200
پس برای فهمیدن رفتارش

00:11:45.224 --> 00:11:48.868
ما باید از یک مدل
انسان شناختی استفاده کنیم

00:11:48.892 --> 00:11:53.869
که شامل محدودیت‌های محاسباتی می‌شود.
که یک مدل بسیار پیچیده است.

00:11:53.893 --> 00:11:56.886
ولی همچنان این چیزی هست
که برای فهمیدن میتونیم روش کار کنیم.

00:11:57.696 --> 00:12:02.016
به نظر من به عنوان یک محقق هوش مصنوعی،
دشوارترین بخش ماجرا

00:12:02.040 --> 00:12:04.615
این حقیقته که 
تعداد زیادی از ما(انسان‌ها) وجود دارد.

00:12:06.114 --> 00:12:09.695
بنابراین ماشین‌ها باید به طریقی
یک مصالحه‌ و توازن

00:12:09.719 --> 00:12:11.944
بین ترجیحات افراد مختلف برقرار کنن.

00:12:11.968 --> 00:12:13.874
و راه‌های زیادی برای این کار وجود دارند.

00:12:13.898 --> 00:12:17.587
اقتصاددان‌ها، جامعه شناس ها،
فلاسفه اخلاق مدار متوجه شده اند

00:12:17.611 --> 00:12:20.066
که ما فعالانه در حال همکاری هستیم.

00:12:20.090 --> 00:12:23.341
بیاین ببینیم وقتی که
اشتباه برداشت کنید چه اتفاقی خواهد افتاد.

00:12:23.365 --> 00:12:25.498
شما میتونین یک گفتگو داشته باشین،

00:12:25.522 --> 00:12:27.466
مثلا با دستیار شخصی هوشمندتان،

00:12:27.490 --> 00:12:29.775
که احتمالا در آینده نزدیک در دسترس باشد.

00:12:29.799 --> 00:12:32.323
مثلا به یک نمونه قوی از Siri فکر کنید

00:12:33.447 --> 00:12:37.769
Siri بهتون میگه که: «همسرتون زنگ زد
تا برای شام امشب بهتون یادآوری کنه»

00:12:38.436 --> 00:12:40.944
و البته شما هم فراموش کرده بودید
چی؟ کدوم شام؟

00:12:40.968 --> 00:12:42.393
راجع به چی حرف میزنی؟

00:12:42.417 --> 00:12:46.163
امم...بیستمین سالگرد ازدواجتون ساعت ۷ شب

00:12:48.735 --> 00:12:52.454
من نمیتونم برم.
ساعت ۷:۳۰ با دبیرکل سازمان ملل جلسه دارم.

00:12:52.478 --> 00:12:54.170
چطوری اینطوری شد؟»

00:12:54.194 --> 00:12:58.854
«من بهت هشدار داده بودم،
ولی تو پیشنهاد من رو نشنیده گرفتی.»

00:12:59.966 --> 00:13:03.294
«خب حالا چیکار کنم؟
نمیتونم بهش بگم که من خیلی سرم شلوغه.

00:13:04.310 --> 00:13:07.591
نگران نباش. من پروازش رو طوری گذاشتم
که تاخیر داشته باشه.»

00:13:07.615 --> 00:13:09.297
(خنده حاضرین)

00:13:10.069 --> 00:13:12.170
«یک نوع خطای کارکرد کامپیوتری.»

00:13:12.194 --> 00:13:13.406
(خنده حاضرین)

00:13:13.430 --> 00:13:15.047
«جدی؟ تو میتونی این کارو بکنی؟»

00:13:16.220 --> 00:13:18.399
«ایشان ازتون عمیقاً عذرخواهی خواهد کرد.

00:13:18.423 --> 00:13:20.978
و از شما خواهد خواست که
فردا ناهار همدیگررو ببینین.»

00:13:21.002 --> 00:13:22.301
(خنده حاضرین)

00:13:22.325 --> 00:13:26.728
ولی اینجا یک مشکلی هست.

00:13:26.752 --> 00:13:29.761
ربات به وضوح داره
به ارزش‌های همسر من توجه میکنه:

00:13:29.785 --> 00:13:31.854
که همون "همسر شاد، زندگی شاد" است.

00:13:31.878 --> 00:13:33.461
(خنده حاضرین)

00:13:33.485 --> 00:13:34.929
میتونست جور دیگه ای باشه.

00:13:35.641 --> 00:13:37.842
ممکن بود شما بعد از یک روز سخت کاری
بیاین خونه،

00:13:37.866 --> 00:13:40.061
و کامپیوتر بگه:
«روز سخت و طولانی بود؟»

00:13:40.085 --> 00:13:42.373
«آره، حتی برای ناهار خوردن هم وقت نداشتم.»

00:13:42.397 --> 00:13:43.679
«باید خیلی گرسنت باشه.»

00:13:43.703 --> 00:13:46.349
«آره دارم از گشنگی میمیرم.
میتونی برام شام درست کنی؟»

00:13:47.890 --> 00:13:49.980
«یه چیزی هست که باید بهت بگم.»

00:13:50.004 --> 00:13:51.159
(خنده حاضرین)

00:13:52.013 --> 00:13:56.918
«یه سری آدم در سودان جنوبی هستن
که بیشتر از تو در شرایط اضطراری قرار دارن»

00:13:56.942 --> 00:13:58.046
(خنده حاضرین)

00:13:58.070 --> 00:14:00.145
«پس من میرم. خودت شامت رو درست کن.»

00:14:00.169 --> 00:14:02.169
(خنده حاضرین)

00:14:02.643 --> 00:14:04.382
بنابراین باید این مشکلات رو حل کنیم

00:14:04.406 --> 00:14:06.921
و من با رغبت دارم
روی این مشکلات کار میکنم.

00:14:06.945 --> 00:14:08.788
دلایلی برای خوشبین بودن وجود دارن.

00:14:08.812 --> 00:14:09.971
یک دلیل،

00:14:09.995 --> 00:14:11.863
اینه که حجم زیادی از داده داریم.

00:14:11.887 --> 00:14:14.681
چون اگر یادتون باشه،
من گفتم که ربات‌ها هر چیزی که

00:14:14.705 --> 00:14:16.251
انسان تا کنون نوشته
را خواهند خواند.

00:14:16.275 --> 00:14:18.999
بیشتر چیزهایی که ما مینویسیم
درباره کارهای بشر هستش.

00:14:19.023 --> 00:14:20.937
و بقیه مردم به خاطرش ناراحت میشن.

00:14:20.961 --> 00:14:23.359
پس انبوهی از داده برای یادگیری وجود داره.

00:14:23.383 --> 00:14:25.619
یک انگیزه اقتصادی قوی هم برای انجام

00:14:27.151 --> 00:14:28.337
این کار هست.

00:14:28.361 --> 00:14:30.362
ربات خدمتکارتون در منزل رو تصور کنید.

00:14:30.386 --> 00:14:33.453
شما باز هم دیر از سر کار بر میگردین،
و ربات باید به بچه‌ها غذا بده.

00:14:33.477 --> 00:14:36.300
بچه‌ها گشنه هستند
و هیچ غذایی در یخچال نیست.

00:14:36.324 --> 00:14:38.929
و ربات گربه رو میبینه!

00:14:38.953 --> 00:14:40.645
(خنده حاضرین)

00:14:40.669 --> 00:14:44.859
و ربات تابع ارزش گذاری انسان ها رو
به خوبی یاد نگرفته

00:14:44.883 --> 00:14:46.134
بنابراین نمیفهمه

00:14:46.158 --> 00:14:51.002
که ارزش معنوی گربه
از ارزش غذایی آن بیشتر است.

00:14:51.026 --> 00:14:52.121
(خنده حضار)

00:14:52.145 --> 00:14:53.893
بعدش چی میشه؟

00:14:53.917 --> 00:14:57.214
یه چیزی تو این مایه‌ها:

00:14:57.238 --> 00:15:00.202
"ربات ظالم، برای شام خانواده گربه می‌پزد"

00:15:00.226 --> 00:15:04.749
این حادثه احتمالا
پایان صنعت ربات‌های خانگی باشد.

00:15:04.773 --> 00:15:08.145
پس انگیزه زیادی
برای درست شدن این موضوع وجود دارد.

00:15:08.169 --> 00:15:10.884
خیلی زودتر از اینکه
به ماشین‌های فوق هوشمند برسیم.

00:15:11.948 --> 00:15:13.483
پس برای جمع بندی:

00:15:13.507 --> 00:15:16.388
من در اصل دارم سعی میکنم
که تعریف هوش مصنوعی رو طوری عوض کنم

00:15:16.412 --> 00:15:19.405
که ماشین های سودمندی داشته باشیم.

00:15:19.429 --> 00:15:20.651
و این اصول عبارتند از:

00:15:20.675 --> 00:15:22.073
ماشین‌ها نوع دوست هستند.

00:15:22.097 --> 00:15:24.901
و فقط میخوان به اهدافی که ما داریم برسن.

00:15:24.925 --> 00:15:28.041
ولی درباره چیستی این اهداف مطمئن نیستند.

00:15:28.065 --> 00:15:30.063
و به ما نگاه میکنند

00:15:30.087 --> 00:15:33.290
تا یادبگیرند
این اهدافی که ما میخواهیم چه هستند.

00:15:34.193 --> 00:15:37.752
و امیدوارم در این فرآیند،
ما هم یادبگیریم که مردم بهتری باشیم.

00:15:37.776 --> 00:15:38.967
خیلی ممنون.

00:15:38.991 --> 00:15:42.700
(تشویق حاضرین)

00:15:42.724 --> 00:15:44.592
کریس اندسون: خیلی جالب بود استوارت.

00:15:44.616 --> 00:15:47.786
ما یکم باید اینجا وایستیم
چون عوامل دارن صحنه رو

00:15:47.810 --> 00:15:48.961
برای سخنرانی بعدی آماده میکنن.

00:15:48.985 --> 00:15:50.523
چندتا سوال:

00:15:50.547 --> 00:15:56.000
ایده برنامه‌ریزی در بی خبری 
به نظر خیلی قدرتمند میاد.

00:15:56.024 --> 00:15:57.618
وقتی که که به هوش قوی برسیم.

00:15:57.642 --> 00:15:59.900
چه چیزی مانع رباتی میشه که

00:15:59.924 --> 00:16:02.776
متنی رو میخونه و به این ایده میرسه که

00:16:02.800 --> 00:16:04.372
اون دانش بهتر از ندانستن است

00:16:04.396 --> 00:16:08.614
و اهدافش رو تغییر بده
و دوباره برنامه ریزی کنه؟

00:16:09.512 --> 00:16:15.868
استوارت راسل: آره همانطور که گفتم،
ما میخوایم که ربات درباره اهداف ما

00:16:15.892 --> 00:16:17.179
بیشتر یاد بگیره.

00:16:17.203 --> 00:16:22.724
ربات فقط وقتی مطمئن تر میشه
که موضوع درست‌تر باشه.

00:16:22.748 --> 00:16:24.693
شواهد گویا هستند.

00:16:24.717 --> 00:16:27.441
و ربات طوری طراحی میشه که درست تفسیر کنه.

00:16:27.465 --> 00:16:31.421
مثلا میفهمه که کتاب‌ها خیلی
در شواهدی که دارند

00:16:31.445 --> 00:16:32.928
جانبدارانه عمل میکنند.

00:16:32.952 --> 00:16:35.349
فقط درباره پادشاهان و شاهزادگان حرف میزنن.

00:16:35.373 --> 00:16:38.173
و مردان سفید پوست فوق‌ العاده ای
که مشغول کاری هستند.

00:16:38.197 --> 00:16:40.293
بنابراین این یک مشکل پیچیده است.

00:16:40.317 --> 00:16:44.189
ولی همانطور که ربات
داره درباره اهداف ما یاد میگیره.

00:16:44.213 --> 00:16:46.276
برای ما بیشتر و بیشتر کاربردی میشه.

00:16:46.300 --> 00:16:48.826
ک آ: پس نمیشه اینو تبدیل به یک قانون کنیم

00:16:48.850 --> 00:16:50.500
و به صورت کورکورانه بگیم که:

00:16:50.524 --> 00:16:53.817
«اگر هر انسانی سعی کرد منو خاموش کنه

00:16:53.841 --> 00:16:55.776
من پیروی میکنم. من پیروی میکنم.»

00:16:55.800 --> 00:16:56.982
قطعا اینطور نیست.

00:16:57.006 --> 00:16:58.505
اینطوری خیلی بد میشد.

00:16:58.529 --> 00:17:01.218
مثلا تصور کنید که یک ماشین خودران دارید.

00:17:01.242 --> 00:17:03.675
و میخواهید بچه پنج سالتون رو

00:17:03.699 --> 00:17:04.873
به مدرسه بفرستید.

00:17:04.897 --> 00:17:07.998
آیا شما میخواین که بچه پنج سالتون
بتونه ماشین رو موقع رانندگی

00:17:08.022 --> 00:17:09.235
خاموش کنه؟

00:17:09.259 --> 00:17:10.418
احتمالا نه.

00:17:10.442 --> 00:17:15.145
پس ربات باید بتونه درک کنه
که آدم چقدر منطقی و معقول هست.

00:17:15.169 --> 00:17:16.845
هرچه انسان منطقی تر باشد،

00:17:16.869 --> 00:17:18.972
ربات بیشتر تمایل دارد
که بگذارد خاموش شود.

00:17:18.996 --> 00:17:21.539
اگر شخص کاملا تصادفی یا خرابکار باشد

00:17:21.563 --> 00:17:24.075
ربات کمتر تمایل دارد که بگذارد خاموش شود.

00:17:24.099 --> 00:17:25.965
خب استوارت، فقط میتونم بگم که

00:17:25.989 --> 00:17:28.303
من واقعا، واقعا امیدوارم تو برای ما
این مشکل رو حل کنی.

00:17:28.327 --> 00:17:30.702
ممنون بابت سخنرانی.
فوق العاده بود.

00:17:30.726 --> 00:17:31.893
ممنون.

00:17:31.917 --> 00:17:33.754
(تشویق حضار)

