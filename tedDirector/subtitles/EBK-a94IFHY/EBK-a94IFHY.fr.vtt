WEBVTT
Kind: captions
Language: fr

00:00:00.000 --> 00:00:07.000
Traducteur: Antoine Combeau
Relecteur: Mathieu Marthe

00:00:12.532 --> 00:00:14.084
Voici Lee Sedol.

00:00:14.108 --> 00:00:18.105
Lee Sedol est l'un des meilleurs
joueurs de go au monde,

00:00:18.129 --> 00:00:21.114
et il vient d'avoir ce que mes amis
de Silicon Valley appellent

00:00:21.114 --> 00:00:22.548
un moment « Mince alors... »

00:00:22.572 --> 00:00:23.645
(Rires)

00:00:23.669 --> 00:00:25.857
Ce moment où nous réalisons

00:00:25.881 --> 00:00:29.177
que l'intelligence artificielle progresse
plus rapidement que prévu.

00:00:29.884 --> 00:00:33.141
Les hommes ont donc perdu au jeu de go.
Qu'en est-il du monde réel ?

00:00:33.141 --> 00:00:35.145
Eh bien, le monde réel est
bien plus grand,

00:00:35.169 --> 00:00:37.418
plus compliqué d'un plateau de go.

00:00:37.442 --> 00:00:39.261
C'est beaucoup moins visible

00:00:39.285 --> 00:00:41.323
mais ça reste un problème de décision.

00:00:42.768 --> 00:00:45.089
Si nous pensons à certaines technologies

00:00:45.113 --> 00:00:46.862
qui apparaissent...

00:00:47.558 --> 00:00:51.893
Noriko [Arai] a dit que la lecture n'est
pas encore au point pour les machines,

00:00:51.917 --> 00:00:53.417
du moins avec compréhension.

00:00:53.441 --> 00:00:54.977
Mais cela se produira,

00:00:55.001 --> 00:00:56.772
et quand ça se produira,

00:00:56.796 --> 00:00:57.983
très peu de temps après,

00:00:58.007 --> 00:01:02.579
les machines liront tout
ce que la race humaine a jamais écrit.

00:01:03.670 --> 00:01:05.494
Cela permettra aux machines,

00:01:05.504 --> 00:01:08.644
qui ont la capacité de voir plus loin
que les hommes ne le peuvent,

00:01:08.668 --> 00:01:10.348
comme démontré avec le jeu de go,

00:01:10.372 --> 00:01:12.536
si elles ont accès à plus d'informations,

00:01:12.560 --> 00:01:16.828
de prendre de meilleures 
décisions dans le monde réel que nous.

00:01:18.612 --> 00:01:20.218
Est-ce donc une bonne chose ?

00:01:21.718 --> 00:01:23.950
Eh bien, je l'espère.

00:01:26.514 --> 00:01:29.769
Notre civilisation tout entière,
tout ce que nous apprécions,

00:01:29.793 --> 00:01:31.861
est basé sur notre intelligence.

00:01:31.885 --> 00:01:35.579
Et si nous avions accès
à une plus large intelligence,

00:01:35.603 --> 00:01:38.905
alors il n'y aurait aucune limite
à ce que la race humaine peut faire.

00:01:40.485 --> 00:01:43.810
Je pense que cela pourrait être,
comme certains l'ont décrit,

00:01:43.834 --> 00:01:45.850
le plus grand moment
de l'histoire humaine.

00:01:48.485 --> 00:01:51.314
Alors, pourquoi les gens
racontent-ils des choses comme :

00:01:51.338 --> 00:01:54.214
« l'IA pourrait signifier
la fin de la race humaine ? »

00:01:55.258 --> 00:01:56.917
Est-ce une chose nouvelle ?

00:01:56.941 --> 00:02:01.051
Cela ne concerne-t-il qu' Elon Musk, 
Bill Gates et Stephen Hawking ?

00:02:01.773 --> 00:02:05.035
En fait, non.
Cette idée existe depuis un moment.

00:02:05.059 --> 00:02:07.021
Voici une citation :

00:02:07.045 --> 00:02:11.349
« Même si nous pouvions garder les
machines dans une position subalterne,

00:02:11.349 --> 00:02:14.443
par exemple, en coupant
l'énergie à des moments stratégiques... »

00:02:14.443 --> 00:02:17.664
et je reviendrai sur cette idée
de « couper l'énergie » plus tard...

00:02:17.688 --> 00:02:20.492
« Nous devrions, en tant qu'espèce,
faire preuve d'humilité. »

00:02:21.997 --> 00:02:25.445
Donc qui a dit ça ?
C'est Alan Turing en 1951.

00:02:26.120 --> 00:02:28.883
Alan Turing, vous le savez,
est le père de l'informatique

00:02:28.907 --> 00:02:31.955
et à bien des égards,
le père de l'IA également.

00:02:33.059 --> 00:02:34.941
Si nous pensons à ce problème,

00:02:34.965 --> 00:02:38.752
celui de créer quelque chose de plus
intelligent que notre propre espèce,

00:02:38.776 --> 00:02:41.398
appelons cela
« le problème du gorille »,

00:02:42.165 --> 00:02:45.915
parce que leurs ancêtres ont vécu cela
il y a quelques millions d'années,

00:02:45.939 --> 00:02:47.684
nous pouvons donc leur demander :

00:02:48.372 --> 00:02:49.752
« Était-ce une bonne idée ? »

00:02:49.756 --> 00:02:53.286
Donc, ils ont eu une réunion
pour savoir si c'était une bonne idée

00:02:53.310 --> 00:02:56.656
et après un petit moment,
ils conclurent que non,

00:02:56.680 --> 00:02:58.025
c'était une mauvaise idée.

00:02:58.049 --> 00:02:59.831
« Notre espèce est en difficulté. »

00:03:00.358 --> 00:03:04.621
En fait, vous pouvez voir de la tristesse
existentielle dans leurs yeux.

00:03:04.645 --> 00:03:06.285
(Rires)

00:03:06.309 --> 00:03:10.143
Ce sentiment gênant d'avoir créé
quelque chose de plus intelligent

00:03:10.163 --> 00:03:13.706
que votre propre espèce alors que
ce n'est peut-être pas une bonne idée...

00:03:14.308 --> 00:03:15.799
« Que pouvons-nous y faire ? »

00:03:15.823 --> 00:03:20.590
Eh bien, rien,
sauf abandonner l'IA.

00:03:20.614 --> 00:03:23.134
Mais à cause de tous les avantages
que j'ai mentionnés

00:03:23.134 --> 00:03:24.894
et parce que je suis chercheur en IA,

00:03:24.894 --> 00:03:26.679
je ne peux m'y résoudre.

00:03:27.103 --> 00:03:29.571
Je veux réellement continuer
à travailler sur l'IA.

00:03:30.435 --> 00:03:33.113
Nous avons besoin de définir
un peu plus le problème.

00:03:33.137 --> 00:03:34.508
Quel est donc le problème ?

00:03:34.532 --> 00:03:37.778
Pourquoi une meilleure IA
pourrait être une catastrophe ?

00:03:39.218 --> 00:03:40.716
Voici une autre citation :

00:03:41.755 --> 00:03:45.090
« Nous devrions être certains
que l'objectif introduit dans la machine

00:03:45.114 --> 00:03:47.412
est bien l'objectif que nous souhaitons. »

00:03:48.102 --> 00:03:51.600
C'est une citation de 
Norbert Wienner en 1960,

00:03:51.624 --> 00:03:55.626
peu après qu'il a observé
l'un des premiers systèmes d'apprentissage

00:03:55.650 --> 00:03:58.233
apprendre à mieux jouer aux échecs
que son créateur.

00:04:00.422 --> 00:04:03.105
Mais ça aurait aussi pu être

00:04:03.129 --> 00:04:04.851
le roi Midas, qui a dit :

00:04:04.851 --> 00:04:07.893
« Je souhaite que tout
ce que je touche se transforme en or »

00:04:07.893 --> 00:04:10.534
et qui a obtenu exactement
ce qu'il avait demandé.

00:04:10.558 --> 00:04:13.309
C'était l'objectif
qu'il a introduit dans la machine,

00:04:13.333 --> 00:04:14.783
pour ainsi dire.

00:04:14.807 --> 00:04:18.251
Sa nourriture, sa boisson et
sa famille se sont alors changées en or.

00:04:18.275 --> 00:04:20.556
Il est mort de faim dans la misère.

00:04:22.264 --> 00:04:24.605
Nous appellerons ça
« le problème du roi Midas »,

00:04:24.629 --> 00:04:27.934
le fait de déclarer un objectif
qui n'est pas, en fait,

00:04:27.958 --> 00:04:30.371
en adéquation avec ce que nous voulons.

00:04:30.395 --> 00:04:33.958
Aujourd'hui, nous appelons cela 
« un problème d'alignement de valeur ».

00:04:36.867 --> 00:04:40.352
Établir le mauvais objectif
n'est qu'une partie du problème.

00:04:40.376 --> 00:04:41.578
Il en existe une autre.

00:04:41.860 --> 00:04:44.143
Si vous introduisez un objectif
dans une machine,

00:04:44.153 --> 00:04:46.395
même une chose
simple comme « acheter du café »,

00:04:47.728 --> 00:04:49.569
la machine se dit :

00:04:50.553 --> 00:04:53.176
« Comment pourrais-je échouer
à apporter du café ?

00:04:53.200 --> 00:04:54.780
Quelqu'un pourrait m'éteindre.

00:04:55.415 --> 00:04:57.852
Alors, je dois prendre
des mesures pour éviter cela.

00:04:57.876 --> 00:04:59.782
Je vais désactiver mon interrupteur.

00:05:00.354 --> 00:05:03.313
Je ferai tout pour me défendre
contre les interférences

00:05:03.337 --> 00:05:05.966
contre l'objectif
qu'on m'a donné. »

00:05:05.990 --> 00:05:08.002
Cette quête obsessionnelle,

00:05:08.633 --> 00:05:11.978
avec une attitude très défensive
envers un objectif qui n'est, en fait,

00:05:12.002 --> 00:05:14.816
pas aligné sur les vrais objectifs
de la race humaine...

00:05:15.912 --> 00:05:18.164
C'est le problème auquel
nous sommes confrontés.

00:05:18.827 --> 00:05:22.784
Et c'est aussi la notion importante
à retenir de cette présentation.

00:05:22.788 --> 00:05:25.063
Si vous ne devez retenir qu'une chose,

00:05:25.087 --> 00:05:28.742
c'est que vous ne pourrez pas aller
chercher le café si vous êtes mort.

00:05:28.742 --> 00:05:29.457
(Rires)

00:05:29.481 --> 00:05:33.310
C'est très simple. Rappelez-vous de cela.
Répétez-le-vous trois fois par jour.

00:05:33.334 --> 00:05:35.155
(Rires)

00:05:35.179 --> 00:05:37.933
En fait, c'est exactement l'intrigue

00:05:37.957 --> 00:05:40.605
de « 2001, l'Odyssée de l'espace. »

00:05:41.046 --> 00:05:43.136
HAL a un objectif, une mission,

00:05:43.160 --> 00:05:46.892
qui n'est pas en adéquation
avec les objectifs des êtres humains

00:05:46.916 --> 00:05:48.726
et cela conduit à ce conflit.

00:05:49.314 --> 00:05:52.283
Heureusement, HAL
n'est pas super-intelligent.

00:05:52.307 --> 00:05:55.894
Il est assez malin,
mais finalement Dave le surpasse

00:05:55.918 --> 00:05:57.767
et parvient à l'éteindre.

00:06:01.648 --> 00:06:03.417
Nous pourrions avoir moins de chance.

00:06:08.013 --> 00:06:09.605
Alors qu'allons-nous faire ?

00:06:12.191 --> 00:06:14.792
J'essaie de redéfinir l'IA,

00:06:14.816 --> 00:06:17.044
afin de nous éloigner
de cette notion classique

00:06:17.044 --> 00:06:21.468
de machines qui poursuivent
leurs objectifs de manière intelligente.

00:06:22.532 --> 00:06:24.330
Cela repose sur trois principes.

00:06:24.354 --> 00:06:27.643
Le premier est un principe d'altruisme,
si vous voulez.

00:06:27.667 --> 00:06:30.929
L'unique objectif du robot

00:06:30.953 --> 00:06:35.199
est de maximiser la réalisation
des objectifs des êtres humains,

00:06:35.223 --> 00:06:36.613
des valeurs humaines.

00:06:36.637 --> 00:06:39.967
Je ne parle pas de celles qui sont
sentimentales ou sainte-nitouche.

00:06:39.991 --> 00:06:43.778
Je parle de la vie que
les êtres humains voudraient

00:06:43.802 --> 00:06:45.145
par n'importe quels moyens.

00:06:47.184 --> 00:06:49.493
Cela viole la loi d'Asimov,
selon laquelle

00:06:49.517 --> 00:06:51.846
le robot doit protéger
sa propre existence.

00:06:51.870 --> 00:06:55.593
Il n'a aucun intérêt
à préserver son existence.

00:06:57.240 --> 00:07:01.008
La deuxième loi est une loi d'humilité,
si vous préférez.

00:07:01.794 --> 00:07:05.537
Elle s'avère très importante
afin de rendre le robot inoffensif.

00:07:05.561 --> 00:07:08.703
L'idée, c'est que le robot ne sait pas

00:07:08.727 --> 00:07:10.755
ce que sont les valeurs humaines.

00:07:10.779 --> 00:07:13.957
Il doit les maximiser,
mais il ne sait pas ce qu'elles sont.

00:07:15.074 --> 00:07:17.700
Pour éviter une quête obsessionnelle

00:07:17.724 --> 00:07:18.936
d'un objectif,

00:07:18.960 --> 00:07:21.132
cette incertitude s'avère cruciale.

00:07:21.546 --> 00:07:23.185
Mais pour nous être utiles,

00:07:23.209 --> 00:07:25.940
il doit avoir une idée de nos désirs.

00:07:27.043 --> 00:07:32.470
Il obtient cette information surtout
par l'observation des choix humains.

00:07:32.494 --> 00:07:35.295
Ces choix révèlent donc des informations

00:07:35.319 --> 00:07:38.619
quant à ce que nous désirons
pour notre vie.

00:07:40.452 --> 00:07:42.119
Ce sont donc les trois principes.

00:07:42.119 --> 00:07:44.497
Voyons comment cela s'applique
à la question

00:07:44.517 --> 00:07:47.420
de Turing :
« Pouvez-vous éteindre la machine ? »

00:07:48.893 --> 00:07:51.013
Voici un robot PR2.

00:07:51.037 --> 00:07:53.009
C'est celui que nous avons au laboratoire

00:07:53.009 --> 00:07:55.912
Il possède un gros interrupteur rouge
directement sur le dos.

00:07:56.341 --> 00:07:59.006
La question est :
« Va-t-il nous laisser l'éteindre ? »

00:07:59.006 --> 00:08:00.465
Selon la méthode classique,

00:08:00.489 --> 00:08:03.091
nous lui donnons pour objectif de
« chercher du café,

00:08:03.111 --> 00:08:06.575
je dois aller chercher du café,
je ne peux pas y aller si je suis mort. »

00:08:06.599 --> 00:08:09.940
Alors, évidemment, le PR2
a écouté ma présentation

00:08:09.964 --> 00:08:13.717
et il se dit :
« Je dois désactiver mon interrupteur

00:08:14.796 --> 00:08:17.521
et tirer sur toutes les autres
personnes dans le Starbucks

00:08:17.521 --> 00:08:19.334
qui pourraient interférer avec moi. »

00:08:19.378 --> 00:08:21.160
(Rires)

00:08:21.184 --> 00:08:23.337
Cela semble inévitable, n'est-ce pas ?

00:08:23.361 --> 00:08:25.759
Ce genre d'échec
semble être inévitable

00:08:25.783 --> 00:08:29.326
et résulte de
l'objectif concret et défini.

00:08:30.632 --> 00:08:33.776
Qu'arrive-t-il si la machine
ne connaît pas l'objectif ?

00:08:33.800 --> 00:08:35.927
Eh bien, elle raisonne différemment.

00:08:35.951 --> 00:08:38.375
Elle se dit :
« OK, l'humain pourrait m'éteindre,

00:08:38.964 --> 00:08:40.830
si je fais ce qui ne va pas.

00:08:41.567 --> 00:08:44.042
Eh bien, je ne sais pas vraiment
ce qui est mal,

00:08:44.066 --> 00:08:46.110
mais je sais
que je ne veux pas le faire. »

00:08:46.134 --> 00:08:49.144
Voilà le premier et le deuxième principes.

00:08:49.168 --> 00:08:52.527
« Je devrais donc laisser
l'humain m'éteindre. »

00:08:53.541 --> 00:08:57.497
En fait, vous pouvez calculer
l'incitation que le robot a

00:08:57.521 --> 00:09:00.014
à se laisser éteindre.

00:09:00.038 --> 00:09:01.952
C'est directement lié au degré

00:09:01.976 --> 00:09:04.722
d'incertitude de
l'objectif sous-jacent.

00:09:05.797 --> 00:09:08.636
Et c'est lorsque la machine est éteinte

00:09:08.670 --> 00:09:10.575
que ce troisième principe
entre en jeu.

00:09:10.589 --> 00:09:13.755
Elle apprend des choses sur
les objectifs qu'elle doit poursuivre

00:09:13.765 --> 00:09:16.248
en constatant que ce qu'elle a fait
n'était pas bien.

00:09:16.248 --> 00:09:19.812
En fait, avec une utilisation
appropriée des symboles grecs,

00:09:19.836 --> 00:09:21.967
comme le font souvent les mathématiciens,

00:09:21.991 --> 00:09:23.975
nous pouvons prouver un théorème

00:09:23.999 --> 00:09:27.552
qui dit qu'un tel robot
est manifestement bénéfique pour l'humain.

00:09:27.576 --> 00:09:31.379
Vous êtes probablement mieux
avec une machine conçue de cette façon

00:09:31.403 --> 00:09:32.649
que sans.

00:09:33.017 --> 00:09:35.963
C'est donc un exemple très simple,
mais c'est la première étape

00:09:35.987 --> 00:09:39.890
dans ce que nous essayons de faire avec
l'IA compatible avec les êtres humains.

00:09:42.477 --> 00:09:44.374
Quant au troisième principe,

00:09:44.398 --> 00:09:48.870
je pense que vous êtes en train
de vous gratter la tête à ce sujet.

00:09:48.894 --> 00:09:52.133
Vous pensez probablement :
« Eh bien, vous savez, je me comporte mal.

00:09:52.157 --> 00:09:55.086
Je ne veux pas que mon robot
se comporte comme moi.

00:09:55.110 --> 00:09:58.544
Je me faufile au milieu de la nuit
et je picore dans le frigo.

00:09:58.568 --> 00:09:59.736
Je fais ceci et cela. »

00:09:59.760 --> 00:10:02.817
Il y a plein de choses
que vous ne voulez pas qu'un robot fasse.

00:10:02.841 --> 00:10:04.652
Mais cela ne fonctionne pas ainsi.

00:10:04.676 --> 00:10:06.815
Votre mauvais comportement

00:10:06.815 --> 00:10:09.538
ne va pas inciter le robot
à vous copier.

00:10:09.538 --> 00:10:13.412
Il va comprendre vos motivations
et peut-être vous aider à résister,

00:10:13.436 --> 00:10:14.756
le cas échéant.

00:10:16.026 --> 00:10:17.490
Mais ça restera difficile.

00:10:18.122 --> 00:10:20.857
Ce que nous essayons de faire,
en fait, c'est de permettre

00:10:20.857 --> 00:10:23.937
aux machines de se demander,
pour toute personne et

00:10:24.291 --> 00:10:27.662
pour toute vie qu'ils pourraient vivre

00:10:27.696 --> 00:10:29.293
et les vies de tous les autres :

00:10:29.317 --> 00:10:31.834
« Laquelle préfèreraient-ils ? »

00:10:33.881 --> 00:10:36.695
Et cela engendre
beaucoup, beaucoup de difficultés.

00:10:36.739 --> 00:10:39.851
Je ne m'attends pas à ce que nous
résolvions cela très rapidement.

00:10:39.905 --> 00:10:42.458
Le vrai problème, en fait, c'est nous.

00:10:43.969 --> 00:10:47.086
Comme je l'ai déjà mentionné,
nous nous comportons mal.

00:10:47.110 --> 00:10:49.711
Certains d'entre nous sont
même foncièrement méchants.

00:10:50.251 --> 00:10:53.823
Le robot, comme je l'ai dit, n'est pas
obligé de copier ce comportement.

00:10:53.867 --> 00:10:56.118
Il n'a aucun objectif propre.

00:10:56.142 --> 00:10:57.879
Il est purement altruiste.

00:10:59.113 --> 00:11:04.334
Il ne doit pas uniquement satisfaire
les désirs d'une personne, l'utilisateur,

00:11:04.358 --> 00:11:07.496
mais, en fait, il doit respecter
les préférences de tous.

00:11:09.083 --> 00:11:11.653
Il peut donc faire face
à une certaine négligence.

00:11:11.677 --> 00:11:15.378
Il peut même comprendre
votre malveillance, par exemple,

00:11:15.402 --> 00:11:18.563
si vous acceptez des pots-de-vin en tant
que préposé aux passeports

00:11:18.597 --> 00:11:21.559
afin de nourrir votre famille
et envoyer vos enfants à l'école.

00:11:21.583 --> 00:11:25.109
Il peut comprendre cela, ce qui ne
signifie pas qu'il va se mettre à voler.

00:11:25.109 --> 00:11:27.722
En fait, il vous aidera
à envoyer vos enfants à l'école.

00:11:28.796 --> 00:11:31.808
Nous sommes limités
par la puissance de calcul.

00:11:31.832 --> 00:11:34.337
Lee Sedol est un brillant joueur de go,

00:11:34.361 --> 00:11:35.686
mais il a quand même perdu.

00:11:35.710 --> 00:11:39.949
Si on regarde ses actions, il a pris
une décision qui lui a coûté le match.

00:11:39.973 --> 00:11:42.134
Ça ne veut pas dire qu'il voulait perdre.

00:11:43.160 --> 00:11:45.200
Pour comprendre son comportement,

00:11:45.224 --> 00:11:48.868
nous devons l'observer à travers 
un modèle de cognition humaine

00:11:48.892 --> 00:11:53.869
qui inclut nos limites en calcul.
Un modèle très compliqué.

00:11:53.893 --> 00:11:56.886
Ça demande un effort,
mais nous pouvons le comprendre.

00:11:57.696 --> 00:12:02.016
Ce qui est le plus difficile,
de mon point de vue de chercheur en IA,

00:12:02.040 --> 00:12:04.615
c'est le fait que nous sommes si nombreux.

00:12:06.114 --> 00:12:09.695
La machine doit faire des choix, 
évaluer les préférences

00:12:09.719 --> 00:12:11.944
d'un grand nombre
de personnes différentes

00:12:11.948 --> 00:12:14.014
et il existe différentes
façons de le faire.

00:12:14.144 --> 00:12:17.587
Les économistes, les sociologues,
les philosophes l'ont bien compris

00:12:17.611 --> 00:12:20.066
et nous cherchons activement
leur collaboration.

00:12:20.090 --> 00:12:23.341
Voyons ce qui se passe
lorsque vous avez un problème.

00:12:23.365 --> 00:12:25.452
Vous pouvez discuter,
par exemple,

00:12:25.452 --> 00:12:27.466
avec votre assistant personnel intelligent

00:12:27.470 --> 00:12:29.805
qui pourrait être disponible
dans quelques années.

00:12:29.805 --> 00:12:32.323
Pensez à un Siri sous stéroïdes.

00:12:33.447 --> 00:12:37.769
Siri dit : « Votre femme a appelé
pour vous rappeler le dîner de ce soir. »

00:12:38.436 --> 00:12:40.944
Bien sûr, vous aviez oublié.
« Quoi ? Quel dîner ?

00:12:40.968 --> 00:12:42.393
De quoi me parles-tu ? »

00:12:42.417 --> 00:12:46.163
« Euh, votre 20ème anniversaire à 19h00. »

00:12:48.735 --> 00:12:52.454
« Je ne peux pas. J'ai une réunion
avec le secrétaire général à 19h30.

00:12:52.478 --> 00:12:54.170
Comment cela a-t-il pu arriver ? »

00:12:54.194 --> 00:12:58.854
« Eh bien, je vous ai prévenu,
mais vous avez ignoré mon avertissement. »

00:12:59.966 --> 00:13:03.634
« Qu'est-ce que je vais faire ? Je ne
peux pas lui dire que je suis occupé. »

00:13:04.110 --> 00:13:07.681
« Ne vous inquiétez pas. J'ai fait
en sorte que son avion ait du retard. »

00:13:07.681 --> 00:13:09.297
(Rires)

00:13:10.069 --> 00:13:12.170
« Une sorte de bug de l'ordinateur. »

00:13:12.194 --> 00:13:13.406
(Rires)

00:13:13.430 --> 00:13:15.047
« Vraiment ? Tu peux faire ça ? »

00:13:16.220 --> 00:13:18.399
« Il vous présente ses excuses

00:13:18.423 --> 00:13:20.978
et voudrait vous rencontrer
demain pour le déjeuner. »

00:13:21.002 --> 00:13:22.301
(Rires)

00:13:22.325 --> 00:13:26.728
Donc là...
Il y a une légère erreur.

00:13:26.752 --> 00:13:29.761
Ce scénario suit clairement
la philosophie de ma femme.

00:13:29.785 --> 00:13:31.854
qui est :
« femme heureuse, vie heureuse. »

00:13:31.878 --> 00:13:33.461
(Rires)

00:13:33.485 --> 00:13:35.329
Mais ça pourrait aussi
aller autrement.

00:13:35.641 --> 00:13:37.842
Vous pourriez rentrer
après une dure journée

00:13:37.866 --> 00:13:40.061
et l'ordinateur vous dit :
« Dure journée ? »

00:13:40.085 --> 00:13:42.373
« Oui, je n'ai même pas
eu le temps de manger. »

00:13:42.397 --> 00:13:43.679
« Tu dois avoir faim. »

00:13:43.703 --> 00:13:46.349
« Oui, très faim.
Tu peux me faire à dîner ? »

00:13:47.890 --> 00:13:49.980
« Je dois te dire quelque chose. »

00:13:50.004 --> 00:13:51.159
(Rires)

00:13:52.013 --> 00:13:56.918
« Il y a des gens au Soudan du Sud qui ont
bien plus besoin de nourriture que toi. »

00:13:56.942 --> 00:13:58.046
(Rires)

00:13:58.070 --> 00:14:00.145
« Je te quitte.
Fais-toi à dîner toi-même. »

00:14:00.169 --> 00:14:02.169
(Rires)

00:14:02.643 --> 00:14:04.382
Nous devons résoudre ces problèmes

00:14:04.406 --> 00:14:06.831
et je suis impatient
de travailler là-dessus.

00:14:06.865 --> 00:14:08.788
Nous avons des raisons
d'être optimistes.

00:14:08.812 --> 00:14:09.971
La première, c'est que

00:14:09.995 --> 00:14:11.863
nous disposons d'une masse de données.

00:14:11.887 --> 00:14:14.681
Parce que, souvenez-vous,
j'ai dit que l'IA va lire tout

00:14:14.705 --> 00:14:16.251
ce que l'homme a jamais écrit.

00:14:16.265 --> 00:14:19.169
La plupart du temps,
nous écrivons sur ce que les humains font

00:14:19.169 --> 00:14:20.937
et sur les gens que ça contrarie.

00:14:20.961 --> 00:14:23.539
Alors il y a une masse de données
dans laquelle puiser.

00:14:23.539 --> 00:14:26.399
Il y a également une très forte
incitation économique à cela.

00:14:27.151 --> 00:14:28.337
Donc,

00:14:28.361 --> 00:14:30.336
imaginez votre robot domestique.

00:14:30.336 --> 00:14:33.523
Vous êtes en retard au travail
et le robot doit nourrir les enfants,

00:14:33.523 --> 00:14:36.330
les enfants ont faim
et il n'y a rien dans le réfrigérateur.

00:14:36.330 --> 00:14:38.929
Le robot voit le chat.

00:14:38.953 --> 00:14:40.645
(Rires)

00:14:40.669 --> 00:14:44.859
Le robot n'a pas bien appris
les valeurs humaines,

00:14:44.883 --> 00:14:46.134
donc il ne comprend pas

00:14:46.158 --> 00:14:51.002
que la valeur sentimentale du chat
l'emporte sur sa valeur nutritionnelle.

00:14:51.026 --> 00:14:52.121
(Rires)

00:14:52.145 --> 00:14:53.893
Que se passe-t-il ?

00:14:53.917 --> 00:14:57.214
Eh bien, que se passe-t-il ?

00:14:57.238 --> 00:15:00.202
« Le robot fou cuisine le chat
de la famille pour le dîner. »

00:15:00.226 --> 00:15:04.749
Cet incident sonnerait la fin
de l'industrie du robot domestique.

00:15:04.773 --> 00:15:08.145
Il y a donc une incitation énorme
à régler cela

00:15:08.169 --> 00:15:11.244
bien avant que nous n'arrivions
aux machines supra-intelligentes.

00:15:11.948 --> 00:15:13.483
Pour résumer.

00:15:13.507 --> 00:15:16.388
J'essaie de modifier la définition de l'IA

00:15:16.412 --> 00:15:19.405
afin que nous ayons des machines
irréfutablement bénéfiques.

00:15:19.429 --> 00:15:20.651
Les principes sont :

00:15:20.675 --> 00:15:22.073
des machines altruistes,

00:15:22.097 --> 00:15:24.901
ne cherchant qu'à atteindre nos objectifs,

00:15:24.925 --> 00:15:28.041
mais ayant une incertitude
quant à ces objectifs

00:15:28.065 --> 00:15:30.063
et qui nous observerons

00:15:30.087 --> 00:15:33.290
afin d'en savoir plus sur
ce que nous voulons vraiment.

00:15:34.193 --> 00:15:37.752
J'espère que dans le processus, nous
apprendrons aussi à devenir meilleurs.

00:15:37.776 --> 00:15:38.967
Merci beaucoup.

00:15:38.991 --> 00:15:42.170
(Applaudissements)

00:15:42.544 --> 00:15:44.592
Chris Anderson :
Très intéressant, Stuart.

00:15:44.616 --> 00:15:47.740
Nous allons rester un peu ici,
car je crois qu'ils préparent

00:15:47.740 --> 00:15:49.001
la prochaine intervention.

00:15:49.001 --> 00:15:50.523
Plusieurs questions.

00:15:50.547 --> 00:15:56.000
L'idée d'une programmation limitée
semble intuitivement très puissante.

00:15:56.024 --> 00:15:58.028
En se rapprochant
de la supra-intelligence,

00:15:58.028 --> 00:15:59.900
qu'est-ce qui empêchera un robot

00:15:59.924 --> 00:16:03.256
de lire la littérature et de découvrir
cette notion que la connaissance

00:16:03.256 --> 00:16:04.992
est en fait supérieure à l'ignorance

00:16:04.992 --> 00:16:08.614
et de changer ses propres objectifs
en réécrivant cette programmation ?

00:16:09.512 --> 00:16:15.868
Stuart Russell : Oui, nous voulons qu'il
en apprenne davantage, comme je l'ai dit,

00:16:15.892 --> 00:16:17.179
à propos de nos objectifs.

00:16:17.203 --> 00:16:22.724
Il gagnera en confiance
avec l'expérience,

00:16:22.748 --> 00:16:24.693
la preuve est là,

00:16:24.717 --> 00:16:27.571
et il sera conçu pour interpréter
correctement nos objectifs.

00:16:27.611 --> 00:16:31.421
Il comprendra, par exemple,
que les livres sont très biaisés

00:16:31.445 --> 00:16:32.928
en fonction de leur contenu.

00:16:32.952 --> 00:16:35.349
Ils ne parlent que de rois et de princes

00:16:35.373 --> 00:16:38.173
et des choses
que fait l'élite blanche.

00:16:38.197 --> 00:16:40.293
C'est donc un problème complexe,

00:16:40.317 --> 00:16:44.189
mais comme il en apprend plus
sur nos objectifs,

00:16:44.213 --> 00:16:46.110
il nous sera de plus en plus utile.

00:16:46.120 --> 00:16:49.026
CA : Et vous ne pourriez pas
résumer cela en une seule loi,

00:16:49.026 --> 00:16:50.500
vous savez, une ligne de code :

00:16:50.524 --> 00:16:53.817
« Si un humain essaie de me débrancher,

00:16:53.841 --> 00:16:55.776
je coopère, je coopère. »

00:16:55.800 --> 00:16:56.982
SR : Impossible.

00:16:57.006 --> 00:16:58.505
Ce serait une mauvaise idée.

00:16:58.529 --> 00:17:01.218
Imaginez que vous ayez
une voiture sans chauffeur

00:17:01.242 --> 00:17:03.675
et vous souhaitez envoyer
votre enfant de cinq ans

00:17:03.699 --> 00:17:04.873
à la maternelle.

00:17:04.887 --> 00:17:08.028
Voulez-vous que votre enfant de cinq ans
puisse éteindre la voiture

00:17:08.028 --> 00:17:09.235
alors qu'elle roule ?

00:17:09.259 --> 00:17:10.418
Probablement pas.

00:17:10.442 --> 00:17:15.145
Alors l'IA doit pouvoir évaluer si la
personne est rationnelle et raisonnable.

00:17:15.169 --> 00:17:16.845
Plus la personne est rationnelle,

00:17:16.869 --> 00:17:18.972
plus elle devrait avoir de contrôle.

00:17:18.996 --> 00:17:21.539
Si la personne est incohérente
ou même malveillante,

00:17:21.563 --> 00:17:24.075
alors elle devrait avoir
un contrôle plus limité.

00:17:24.089 --> 00:17:25.949
CA : Très bien. Stuart,

00:17:25.949 --> 00:17:28.343
j'espère que vous allez
régler cela pour nous.

00:17:28.343 --> 00:17:31.262
Merci beaucoup pour cette présentation.
C'était incroyable.

00:17:31.262 --> 00:17:32.093
SR : Merci.

00:17:32.093 --> 00:17:33.754
(Applaudissements)

