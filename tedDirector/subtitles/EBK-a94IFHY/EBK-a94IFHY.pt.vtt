WEBVTT
Kind: captions
Language: pt

00:00:00.000 --> 00:00:07.000
Tradutor: Maurício Kakuei Tanaka
Revisor: Maricene Crus

00:00:12.532 --> 00:00:14.084
Este é Lee Sedol.

00:00:14.108 --> 00:00:16.985
Lee Sedol é um dos maiores
jogadores de Go do mundo,

00:00:18.129 --> 00:00:20.648
e está tendo o que meus amigos
do Vale do Silício

00:00:20.648 --> 00:00:22.548
chamam de momento "Caramba!",

00:00:22.572 --> 00:00:23.645
(Risos)

00:00:23.669 --> 00:00:25.857
um momento em que percebemos

00:00:25.881 --> 00:00:29.397
que a IA está progredindo realmente
muito mais rápido do que esperávamos.

00:00:29.974 --> 00:00:31.894
O homem perde no tabuleiro de Go.

00:00:31.894 --> 00:00:33.045
E no mundo real?

00:00:33.045 --> 00:00:35.145
Bem, o mundo real é muito maior,

00:00:35.169 --> 00:00:37.418
muito mais complicado
do que o tabuleiro de Go.

00:00:37.442 --> 00:00:41.321
É muito menos visível,
mas ainda é um problema de decisão.

00:00:42.768 --> 00:00:46.859
Se pensarmos sobre algumas
das tecnologias que estão surgindo...

00:00:47.558 --> 00:00:51.893
Noriko [Arai] mencionou que a leitura
ainda não acontece nos computadores,

00:00:51.917 --> 00:00:53.417
pelo menos, com compreensão,

00:00:53.441 --> 00:00:54.977
mas isso irá acontecer.

00:00:55.001 --> 00:00:56.772
E quando acontecer,

00:00:56.796 --> 00:01:02.483
muito em breve, os computadores
terão lido tudo que o homem tiver escrito.

00:01:03.670 --> 00:01:05.700
Isso permitirá aos computadores,

00:01:05.724 --> 00:01:08.644
junto com a capacidade de olhar
mais adiante do que o homem,

00:01:08.668 --> 00:01:10.348
como já vimos no Go,

00:01:10.372 --> 00:01:12.536
se também tiverem acesso
a mais informação,

00:01:12.560 --> 00:01:16.828
serem capazes de tomar decisões
melhores no mundo real do que nós.

00:01:18.612 --> 00:01:20.218
Isso é bom?

00:01:21.718 --> 00:01:23.950
Bem, espero que sim.

00:01:26.514 --> 00:01:29.769
Toda a nossa civilização,
tudo o que valorizamos,

00:01:29.793 --> 00:01:31.861
está baseada em nossa inteligência.

00:01:31.885 --> 00:01:35.579
Se tivéssemos acesso
a muito mais informações,

00:01:35.603 --> 00:01:38.905
não haveria limites para o homem.

00:01:40.485 --> 00:01:43.810
Creio que seria, como alguns descreveram,

00:01:43.834 --> 00:01:45.925
o maior evento na história da humanidade.

00:01:45.925 --> 00:01:48.485
[Bem-vindo à Utopia.
Aproveite sua viagem.]

00:01:48.485 --> 00:01:51.314
Então, por que as pessoas
dizem coisas como esta,

00:01:51.338 --> 00:01:54.214
que a IA pode ser o sinal
do fim da raça humana?

00:01:55.258 --> 00:01:56.917
Isso é novidade?

00:01:56.941 --> 00:02:01.051
Trata-se apenas de Elon Musk,
Bill Gates e Stephen Hawking?

00:02:01.773 --> 00:02:05.035
Na verdade, não. Esta ideia
está por aí há algum tempo.

00:02:05.059 --> 00:02:07.021
Aqui está uma citação:

00:02:07.045 --> 00:02:11.395
"Mesmo que pudéssemos manter
os computadores em posição submissa,

00:02:11.419 --> 00:02:14.277
desligando, por exemplo, a energia
em momentos estratégicos",

00:02:14.277 --> 00:02:17.038
voltarei mais tarde com essa ideia
de "desligar a energia",

00:02:17.048 --> 00:02:20.702
"deveríamos, como espécie,
nos sentir muito humilhados".

00:02:21.917 --> 00:02:23.140
Quem disse isso?

00:02:23.140 --> 00:02:25.460
Foi Alan Turing, em 1951.

00:02:26.120 --> 00:02:28.883
Alan Turing, como sabem,
é o pai da informática

00:02:28.907 --> 00:02:31.955
e, de muitas formas, o pai da IA também.

00:02:33.059 --> 00:02:34.941
Se pensarmos sobre o problema

00:02:34.965 --> 00:02:38.752
de criar algo mais inteligente
do que a própria espécie,

00:02:38.776 --> 00:02:41.398
podemos chamar isso
de "problema do gorila",

00:02:42.165 --> 00:02:45.915
porque os ancestrais dos gorilas
fizeram isso há milhões de anos,

00:02:45.939 --> 00:02:48.124
e podemos agora perguntar a eles:

00:02:48.572 --> 00:02:49.732
"Foi uma boa ideia?"

00:02:49.756 --> 00:02:53.286
Aqui estão eles tendo uma reunião
para discutir se foi uma boa ideia,

00:02:53.310 --> 00:02:56.656
e, depois de um tempo, concluem que não,

00:02:56.680 --> 00:02:58.025
foi uma péssima ideia.

00:02:58.049 --> 00:02:59.831
Nossa espécie está em apuros.

00:03:00.358 --> 00:03:04.621
Sim, você pode ver a tristeza
existencial nos olhos deles.

00:03:04.645 --> 00:03:06.285
(Risos)

00:03:06.309 --> 00:03:11.149
Esta sensação desconfortável de algo
mais inteligente do que a própria espécie

00:03:11.173 --> 00:03:13.538
talvez não seja uma boa ideia.

00:03:14.308 --> 00:03:15.799
O que podemos fazer a respeito?

00:03:15.823 --> 00:03:20.590
Bem, realmente nada,
a não ser parar de fazer IA,

00:03:20.614 --> 00:03:23.124
e, por causa de todos
os benefícios que mencionei

00:03:23.148 --> 00:03:25.948
e, por ser pesquisador de IA,
não permitirei isso.

00:03:27.103 --> 00:03:29.571
Quero mesmo poder continuar a fazer IA.

00:03:30.435 --> 00:03:33.113
Temos, na realidade,
que decidir sobre o problema.

00:03:33.137 --> 00:03:34.788
Qual é o problema exatamente?

00:03:34.788 --> 00:03:37.778
Por que a IA pode ser uma catástrofe?

00:03:39.218 --> 00:03:41.116
Aqui está uma outra citação:

00:03:41.755 --> 00:03:45.090
"É melhor termos certeza
de que a missão passada ao computador

00:03:45.114 --> 00:03:47.412
é o que realmente desejamos".

00:03:48.102 --> 00:03:51.600
Isso foi dito por Norbert Wiener, em 1960,

00:03:51.624 --> 00:03:55.626
pouco depois de ter visto
um dos sistemas de aprendizagem

00:03:55.650 --> 00:03:58.233
aprender a jogar damas
melhor do que seu criador.

00:04:00.422 --> 00:04:03.105
Mas isso também poderia ter sido dito

00:04:03.129 --> 00:04:04.296
pelo Rei Midas.

00:04:04.903 --> 00:04:08.037
O Rei Midas disse: "Quero
que tudo o que eu tocar vire ouro",

00:04:08.061 --> 00:04:10.534
e ele conseguiu exatamente o que pediu.

00:04:10.558 --> 00:04:13.309
Essa foi a missão passada ao computador,

00:04:13.333 --> 00:04:14.783
por assim dizer,

00:04:14.807 --> 00:04:18.251
e então, sua comida, bebida
e seus parentes se transformaram em ouro,

00:04:18.275 --> 00:04:20.556
e ele morreu de tristeza e fome.

00:04:22.264 --> 00:04:24.605
Chamaremos isso de "problema do Rei Midas"

00:04:24.629 --> 00:04:27.934
de dar uma missão que não está, de fato,

00:04:27.958 --> 00:04:30.371
verdadeiramente alinhada
com aquilo que queremos.

00:04:30.395 --> 00:04:34.328
Em termos modernos, chamamos
de "problema de alinhamento de valor".

00:04:36.867 --> 00:04:40.352
Atribuir a missão errada
não é a única parte do problema.

00:04:40.376 --> 00:04:41.528
Há outro elemento.

00:04:41.980 --> 00:04:43.923
Se você passar uma missão ao computador,

00:04:43.947 --> 00:04:46.395
mesmo algo tão simples
como "Traga o café",

00:04:47.728 --> 00:04:49.569
o computador dirá a si mesmo:

00:04:50.553 --> 00:04:53.176
"Bem, como posso falhar ao trazer o café?

00:04:53.200 --> 00:04:54.780
Alguém pode me desligar.

00:04:55.465 --> 00:04:57.852
Certo, tenho que fazer algo
para evitar isso.

00:04:57.876 --> 00:04:59.782
Desabilitarei meu botão liga e desliga.

00:05:00.354 --> 00:05:03.313
Farei de tudo para me defender
contra interferências

00:05:03.337 --> 00:05:05.966
a esta missão que recebi".

00:05:05.990 --> 00:05:08.002
Esta busca determinada,

00:05:09.033 --> 00:05:11.978
de modo muito defensivo,
de uma missão que não está,

00:05:12.002 --> 00:05:14.816
de fato, alinhada com os reais
objetivos do homem,

00:05:15.942 --> 00:05:17.984
é o problema que enfrentamos.

00:05:18.827 --> 00:05:23.594
Essa é, na verdade,
a conclusão valiosa desta palestra.

00:05:23.618 --> 00:05:25.673
Se quiserem se lembrar de uma coisa,

00:05:25.697 --> 00:05:28.372
é que não podem trazer
o café se estiverem mortos.

00:05:28.396 --> 00:05:29.457
(Risos)

00:05:29.481 --> 00:05:33.310
É muito simples. Lembrem-se apenas disso.
Repitam a si mesmos três vezes ao dia.

00:05:33.334 --> 00:05:35.155
(Risos)

00:05:35.179 --> 00:05:40.613
Este é exatamente o enredo
de "2001: Uma Odisseia no Espaço".

00:05:41.046 --> 00:05:43.136
HAL tem um objetivo, uma missão,

00:05:43.160 --> 00:05:46.892
que não está alinhada
aos objetivos do homem,

00:05:46.916 --> 00:05:48.726
e que leva a este conflito.

00:05:49.314 --> 00:05:52.283
Felizmente, HAL não é superinteligente.

00:05:52.307 --> 00:05:55.894
É bem inteligente, mas Dave
é mais esperto do que ele no final

00:05:55.918 --> 00:05:57.767
e consegue desligá-lo.

00:06:01.648 --> 00:06:03.767
Mas podemos não ter tanta sorte.

00:06:05.006 --> 00:06:07.386
[Desculpe, Dave,
mas não posso fazer isso.]

00:06:08.013 --> 00:06:09.605
Então, o que faremos?

00:06:12.191 --> 00:06:14.792
Estou tentando redefinir
a Inteligência Artificial

00:06:14.816 --> 00:06:16.877
para escapar dessa ideia tradicional

00:06:16.901 --> 00:06:21.468
de computadores que se dedicam
aos objetivos de forma inteligente.

00:06:22.532 --> 00:06:23.964
Há três princípios envolvidos.

00:06:23.964 --> 00:06:27.643
O primeiro é o princípio do altruísmo

00:06:27.667 --> 00:06:30.929
segundo o qual o único objetivo do robô

00:06:30.953 --> 00:06:35.199
é maximizar a realização
de objetivos do homem,

00:06:35.223 --> 00:06:36.613
de valores humanos.

00:06:36.637 --> 00:06:39.967
Por valores aqui, não me refiro
a valores morais, sentimentais.

00:06:39.991 --> 00:06:45.148
Refiro-me apenas ao que o homem
prefere que seja sua vida.

00:06:47.184 --> 00:06:49.493
Isso realmente viola a lei de Asimov

00:06:49.517 --> 00:06:51.660
pela qual o robô
deve proteger sua existência.

00:06:51.660 --> 00:06:55.513
Ele não tem interesse em preservar
sua existência de forma alguma.

00:06:57.240 --> 00:07:01.008
A segunda lei é a lei da humildade.

00:07:01.794 --> 00:07:05.537
Isso vem a ser realmente importante
para fazer com que os robôs sejam seguros.

00:07:05.561 --> 00:07:10.763
Segundo ela, o robô não sabe
quais são esses valores humanos.

00:07:10.783 --> 00:07:13.957
Então, ele tem que maximizá-los,
mas não sabe quais são eles.

00:07:15.074 --> 00:07:18.620
Isso evita este problema de busca
determinada por um objetivo.

00:07:18.620 --> 00:07:21.132
Esta incerteza revela-se crucial.

00:07:21.546 --> 00:07:25.935
Para ser útil a nós, ele precisa
ter uma ideia do que queremos.

00:07:27.043 --> 00:07:32.470
Ele obtém essa informação principalmente
pela observação das escolhas humanas.

00:07:32.494 --> 00:07:35.295
Assim nossas próprias escolhas
revelam informação

00:07:35.319 --> 00:07:38.619
sobre como preferimos
que sejam nossas vidas.

00:07:40.272 --> 00:07:41.635
São três princípios.

00:07:41.635 --> 00:07:44.477
Vejamos como isso
se aplica a esta questão:

00:07:44.501 --> 00:07:47.290
"Você pode desligar o computador?"
como sugeriu Turing.

00:07:48.893 --> 00:07:52.863
Aqui está o robô PR2,
que temos em nosso laboratório,

00:07:52.882 --> 00:07:55.785
com um grande botão
liga e desliga vermelho nas costas.

00:07:56.361 --> 00:07:58.976
A questão é: ele deixará você desligá-lo?

00:07:59.000 --> 00:08:01.245
Pelo modo tradicional,
damos a ele a missão

00:08:01.245 --> 00:08:03.971
"Traga o café, devo trazer o café,

00:08:03.995 --> 00:08:06.575
não posso trazer o café
se eu estiver morto".

00:08:06.599 --> 00:08:09.120
É claro que o PR2 estava
ouvindo minha conversa,

00:08:09.964 --> 00:08:13.717
e diz então: "Devo desabilitar
meu botão liga e desliga,

00:08:14.796 --> 00:08:19.110
e talvez dar um choque nas pessoas
do Starbucks que mexerem comigo".

00:08:19.110 --> 00:08:21.160
(Risos)

00:08:21.184 --> 00:08:23.337
Isso parece inevitável, não?

00:08:23.361 --> 00:08:25.759
Este tipo de modo de falha
parece inevitável,

00:08:25.783 --> 00:08:29.326
e resulta de um objetivo
concreto, definido.

00:08:30.632 --> 00:08:33.776
O que acontece se o computador
não tem certeza do objetivo?

00:08:33.800 --> 00:08:35.927
Bem, ele raciocina de modo diferente.

00:08:35.951 --> 00:08:38.375
Diz: "Tudo bem, o homem pode me desligar,

00:08:38.964 --> 00:08:41.350
mas só se estiver fazendo algo errado.

00:08:41.567 --> 00:08:45.682
Bem, não sei o que é errado,
mas sei que não quero fazer isso".

00:08:45.684 --> 00:08:48.574
Ali estão o primeiro
e o segundo princípios.

00:08:49.168 --> 00:08:52.527
"Então, deveria deixar
o homem me desligar".

00:08:53.541 --> 00:08:56.341
De fato, você pode calcular o estímulo

00:08:56.341 --> 00:09:00.014
que o robô tem para deixar
o homem desligá-lo,

00:09:00.038 --> 00:09:04.722
e está diretamente ligado ao grau
de incerteza sobre o objetivo fundamental.

00:09:05.797 --> 00:09:10.226
Então, quando o computador é desligado,
o terceiro princípio entra em campo.

00:09:10.226 --> 00:09:13.355
Ele aprende algo sobre os objetivos
aos quais deveria se dedicar

00:09:13.355 --> 00:09:15.148
porque aprende que não fez o certo.

00:09:16.242 --> 00:09:19.812
De fato, com uso adequado
de símbolos gregos,

00:09:19.836 --> 00:09:21.967
como costumavam fazer os matemáticos,

00:09:21.991 --> 00:09:23.975
podemos até provar um teorema

00:09:23.999 --> 00:09:27.552
segundo o qual tal robô
é provavelmente benéfico ao homem.

00:09:27.576 --> 00:09:31.379
Talvez você esteja melhor
com um computador projetado desta forma

00:09:31.403 --> 00:09:32.649
do que sem ele.

00:09:33.057 --> 00:09:35.963
Este é um exemplo muito simples,
mas é o primeiro passo

00:09:35.987 --> 00:09:39.890
para o que estamos tentando fazer
com IA compatível com o homem.

00:09:43.857 --> 00:09:45.734
Há este terceiro princípio,

00:09:45.758 --> 00:09:48.074
pelo qual você deve estar
coçando a cabeça.

00:09:48.074 --> 00:09:52.133
Você deve estar pensando:
"Bem, sabe, eu me comportei mal.

00:09:52.157 --> 00:09:55.086
Não quero que meu robô
se comporte como eu.

00:09:55.110 --> 00:09:58.544
Ando às escondidas, no meio da noite,
e pego coisas da geladeira.

00:09:58.568 --> 00:09:59.520
Faço isso e aquilo".

00:09:59.520 --> 00:10:02.557
Há muitas coisas que você
não quer que o robô faça.

00:10:02.581 --> 00:10:04.486
Mas, na verdade, não funciona bem assim.

00:10:04.486 --> 00:10:06.831
Só porque você se comporta mal

00:10:06.855 --> 00:10:09.422
não quer dizer que o robô
irá copiar seu comportamento.

00:10:09.422 --> 00:10:13.412
Ele irá entender suas motivações
e talvez ajudá-lo a resistir a elas,

00:10:13.436 --> 00:10:14.756
se for adequado.

00:10:16.026 --> 00:10:17.490
Mas ainda é difícil.

00:10:18.122 --> 00:10:20.667
O que estamos tentando fazer,

00:10:20.691 --> 00:10:24.651
é permitir que computadores
prevejam para qualquer pessoa

00:10:24.651 --> 00:10:28.932
e para qualquer vida que ela poderia ter,
e a vida de todos os demais:

00:10:29.827 --> 00:10:31.834
qual vida eles iriam preferir?

00:10:33.881 --> 00:10:36.639
Há muitas dificuldades
envolvidas para fazer isso.

00:10:36.639 --> 00:10:39.111
Não espero que isso seja
resolvido muito rapidamente.

00:10:39.815 --> 00:10:42.458
As dificuldades reais,
na verdade, somos nós.

00:10:43.969 --> 00:10:47.086
Como já havia mencionado,
nós nos comportamos mal.

00:10:47.110 --> 00:10:49.431
Alguns de nós somos muito maus.

00:10:50.251 --> 00:10:53.303
Já o robô, como eu disse,
não tem que copiar o comportamento.

00:10:53.327 --> 00:10:56.118
O robô não tem nenhum objetivo próprio.

00:10:56.142 --> 00:10:57.879
Ele é meramente altruísta.

00:10:59.113 --> 00:11:04.334
Não é projetado apenas para satisfazer
os desejos de uma pessoa, o consumidor,

00:11:04.358 --> 00:11:07.496
mas ele tem que respeitar
as preferências de todos.

00:11:09.083 --> 00:11:11.653
Ele pode lidar com um pouco de maldade,

00:11:11.677 --> 00:11:14.018
e pode até entender essa sua maldade.

00:11:14.442 --> 00:11:18.073
Por exemplo, você pode aceitar
suborno como funcionário público

00:11:18.097 --> 00:11:21.909
porque precisa alimentar sua família
e pagar a escola dos seus filhos.

00:11:21.933 --> 00:11:24.839
O robô pode entender isso.
Não significa que ele irá roubar.

00:11:24.863 --> 00:11:27.772
Ele só o ajudará a pagar
a escola de seus filhos.

00:11:28.796 --> 00:11:31.808
Também somos computacionalmente limitados.

00:11:31.832 --> 00:11:35.687
Lee Sedol é um jogador de Go genial,
mas ele ainda perde.

00:11:35.717 --> 00:11:39.949
Se examinarmos suas ações,
vemos que uma delas o fez perder o jogo.

00:11:39.973 --> 00:11:42.394
Isso não significa que ele queria perder.

00:11:43.160 --> 00:11:45.200
Para entender o comportamento dele,

00:11:45.224 --> 00:11:48.868
temos realmente que inverter
pelo modelo de conhecimento humano

00:11:48.892 --> 00:11:53.869
que inclui limitações computacionais,
um modelo muito complexo.

00:11:53.893 --> 00:11:56.886
Mas ainda é algo que podemos
trabalhar para compreender.

00:11:57.696 --> 00:12:01.516
Talvez, o mais difícil, do meu ponto
de vista como pesquisador de IA,

00:12:02.040 --> 00:12:04.615
seja o fato de que há muitos de nós,

00:12:06.114 --> 00:12:09.695
e o computador precisa, de algum modo,
trocar, considerar as preferências

00:12:09.719 --> 00:12:11.944
de muitas pessoas diferentes,

00:12:11.968 --> 00:12:13.874
e há modos diferentes para fazer isso.

00:12:13.898 --> 00:12:17.587
Economistas, sociólogos,
filósofos morais entenderam isso,

00:12:17.611 --> 00:12:20.066
e estamos procurando
ativamente por colaboração.

00:12:20.090 --> 00:12:23.341
Vamos ver o que acontece
quando você interpreta isso mal.

00:12:23.365 --> 00:12:27.140
Você pode ter uma conversa, por exemplo,
com seu assistente pessoal inteligente

00:12:27.140 --> 00:12:29.775
que pode estar disponível
daqui a alguns anos.

00:12:29.799 --> 00:12:32.323
Pense em um assistente virtual.

00:12:33.447 --> 00:12:37.769
O assistente lhe diz: "Sua esposa ligou
para lembrá-lo do jantar de hoje à noite",

00:12:38.436 --> 00:12:43.018
mas você havia esquecido: "O quê?
Que jantar? Do que você está falando?"

00:12:43.087 --> 00:12:46.183
"Seu aniversário de 20 anos, às 19h."

00:12:48.735 --> 00:12:52.454
"Não vai dar. Tenho um encontro
com o secretário geral às 19h30.

00:12:52.478 --> 00:12:54.170
Como foi que isso aconteceu?"

00:12:54.194 --> 00:12:58.854
"Bem, eu o avisei, mas você ignorou
minha recomendação."

00:12:59.966 --> 00:13:03.294
"O que vou fazer? Não posso falar
que estou muito ocupado."

00:13:04.310 --> 00:13:07.591
"Não se preocupe. Dei um jeito
para o avião dele atrasar."

00:13:07.615 --> 00:13:09.297
(Risos)

00:13:10.069 --> 00:13:12.170
"Algum tipo de defeito no computador".

00:13:12.194 --> 00:13:13.406
(Risos)

00:13:13.430 --> 00:13:15.047
"Sério? Consegue fazer isso?"

00:13:16.220 --> 00:13:18.399
"Ele manda suas profundas desculpas

00:13:18.423 --> 00:13:20.978
e não vê a hora de encontrá-lo
amanhã para o almoço".

00:13:21.002 --> 00:13:22.301
(Risos)

00:13:22.885 --> 00:13:26.758
Está acontecendo um pequeno erro,

00:13:26.758 --> 00:13:29.761
que é, obviamente, seguir
os valores de minha esposa:

00:13:29.785 --> 00:13:31.854
"Esposa feliz, vida feliz".

00:13:31.878 --> 00:13:33.461
(Risos)

00:13:33.485 --> 00:13:35.269
Poderia seguir outro rumo.

00:13:35.641 --> 00:13:40.062
Você chega depois de um dia de trabalho,
e o computador diz: "Foi um longo dia?"

00:13:40.085 --> 00:13:42.373
"Sim, nem consegui almoçar."

00:13:42.397 --> 00:13:43.679
"Você deve estar faminto."

00:13:43.703 --> 00:13:46.349
"Sim, faminto. Pode fazer o jantar?"

00:13:47.890 --> 00:13:49.980
"Precisamos conversar."

00:13:50.004 --> 00:13:51.159
(Risos)

00:13:52.013 --> 00:13:56.918
"Tem gente no Sudão com necessidades
mais urgentes do que as suas."

00:13:56.942 --> 00:13:58.046
(Risos)

00:13:58.070 --> 00:14:00.145
"Vou sair. Faça você mesmo o seu jantar."

00:14:00.169 --> 00:14:02.169
(Risos)

00:14:02.643 --> 00:14:04.382
Temos que resolver esses problemas,

00:14:04.406 --> 00:14:06.831
e estou esperando ansiosamente
para trabalhar neles.

00:14:06.945 --> 00:14:08.788
Há razões para o otimismo.

00:14:08.812 --> 00:14:11.851
Uma delas é que há
uma enorme quantidade de dados.

00:14:11.851 --> 00:14:15.135
Lembrem-se: eu disse que eles irão ler
tudo que o homem tiver escrito.

00:14:15.135 --> 00:14:18.999
A maioria do que escrevemos
é sobre pessoas fazendo coisas

00:14:19.023 --> 00:14:20.831
e outras ficando aborrecidas com isso.

00:14:20.831 --> 00:14:23.359
Temos uma enorme quantidade
de dados para aprender.

00:14:23.383 --> 00:14:25.619
Há também um incentivo
econômico muito forte

00:14:27.151 --> 00:14:28.337
para resolver isso.

00:14:28.361 --> 00:14:30.362
Imagine então seu robô doméstico em casa.

00:14:30.386 --> 00:14:33.453
Você chega tarde em casa
e o robô precisa alimentar as crianças,

00:14:33.477 --> 00:14:36.300
elas estão com fome
e não tem nada na geladeira.

00:14:36.324 --> 00:14:38.929
E o robô vê o gato.

00:14:38.953 --> 00:14:40.645
(Risos)

00:14:41.189 --> 00:14:44.859
O robô não aprendeu bem
a função do valor humano.

00:14:44.883 --> 00:14:46.134
Então, ele não compreende

00:14:46.158 --> 00:14:51.002
que o valor sentimental pelo gato
pesa mais do que seu valor nutritivo.

00:14:51.026 --> 00:14:52.121
(Risos)

00:14:52.145 --> 00:14:53.893
O que acontece então?

00:14:55.597 --> 00:14:57.214
Bem, acontece o seguinte:

00:14:57.238 --> 00:15:00.202
"Robô louco cozinha gatinho
para o jantar".

00:15:00.226 --> 00:15:04.749
Esse único incidente seria o fim
da indústria de robôs domésticos.

00:15:04.773 --> 00:15:07.435
Há um enorme incentivo para resolver isso

00:15:08.169 --> 00:15:10.884
antes de chegarmos
aos computadores superinteligentes.

00:15:11.948 --> 00:15:13.483
Em resumo:

00:15:13.507 --> 00:15:16.388
estou tentando realmente
mudar a definição de IA

00:15:16.412 --> 00:15:19.405
para que tenhamos computadores úteis.

00:15:19.429 --> 00:15:20.651
Os princípios são:

00:15:20.675 --> 00:15:22.073
computadores altruístas,

00:15:22.097 --> 00:15:24.901
que querem alcançar
apenas nossos objetivos,

00:15:24.925 --> 00:15:28.041
mas estão incertos sobre quais são eles,

00:15:28.065 --> 00:15:30.063
e irão observar todos nós

00:15:30.087 --> 00:15:33.290
para aprender mais
sobre o que realmente queremos.

00:15:34.193 --> 00:15:37.752
E tomara que no processo,
aprendamos a ser pessoas melhores.

00:15:37.776 --> 00:15:38.967
Muito obrigado.

00:15:38.991 --> 00:15:40.990
(Aplausos)

00:15:42.724 --> 00:15:44.592
Chris Anderson: Interessante, Stuart.

00:15:44.616 --> 00:15:47.230
Ficaremos aqui um pouco
porque acho que estão preparando

00:15:47.230 --> 00:15:48.745
para o próximo palestrante.

00:15:48.745 --> 00:15:50.523
Algumas questões.

00:15:50.547 --> 00:15:56.000
A ideia de programar na ignorância
parece realmente convincente.

00:15:56.024 --> 00:15:57.618
Ao chegar à superinteligência,

00:15:57.642 --> 00:16:01.200
o que irá impedir um robô
de ler literatura

00:16:01.200 --> 00:16:04.366
e descobrir que o conhecimento
é melhor que a ignorância

00:16:04.396 --> 00:16:08.614
e ainda mudar seus próprios objetivos
e reescrever essa programação?

00:16:09.512 --> 00:16:12.092
Stuart Russell: Sim, queremos...

00:16:13.452 --> 00:16:17.179
Queremos que ele aprenda mais,
como eu disse, sobre nossos objetivos.

00:16:17.203 --> 00:16:22.174
Ele só se tornará mais seguro
quando se tornar mais correto.

00:16:23.198 --> 00:16:27.443
A evidência está lá, e ele será projetado
para interpretá-la corretamente.

00:16:27.465 --> 00:16:31.421
Ele entenderá, por exemplo,
que os livros são muito tendenciosos

00:16:31.445 --> 00:16:32.928
na evidência que contêm.

00:16:32.952 --> 00:16:37.629
Eles só falam sobre reis e príncipes
e a elite do homem branco fazendo coisas.

00:16:38.197 --> 00:16:40.293
É um problema complicado,

00:16:40.317 --> 00:16:44.189
mas, à medida que aprende
mais sobre nossos objetivos,

00:16:44.213 --> 00:16:46.276
ele será cada vez mais útil para nós.

00:16:46.300 --> 00:16:50.526
CA: E você não poderia apenas
reduzir a uma regra, integrada em:

00:16:50.526 --> 00:16:55.777
"Se qualquer pessoa tentar me desligar,
eu concordo. Eu concordo"?

00:16:55.800 --> 00:16:58.222
SR: De jeito nenhum.
Seria uma ideia terrível.

00:16:58.222 --> 00:17:01.218
Imagine que você tem
um carro que dirige sozinho

00:17:01.242 --> 00:17:04.865
e você quer mandar seu filho
de cinco anos para a escola.

00:17:04.897 --> 00:17:08.259
Quer que seu filho de cinco anos
consiga desligar o carro em movimento?

00:17:08.259 --> 00:17:09.852
Provavelmente não.

00:17:09.852 --> 00:17:15.145
Ele tem que compreender a racionalidade
e a sensibilidade da pessoa.

00:17:15.169 --> 00:17:18.686
Quanto mais racional for a pessoa,
mais disposto estará para ser desligado.

00:17:18.686 --> 00:17:21.539
Se a pessoa for sem noção
ou mal-intencionada,

00:17:21.563 --> 00:17:24.075
menos disposto você estará
para ser desligado.

00:17:24.099 --> 00:17:27.377
CA: Tudo bem. Stuart, espero realmente
que você resolva isso para nós.

00:17:27.377 --> 00:17:30.452
Muito obrigado por esta palestra.
Foi incrível. Obrigado.

00:17:30.452 --> 00:17:32.449
(Aplausos)

