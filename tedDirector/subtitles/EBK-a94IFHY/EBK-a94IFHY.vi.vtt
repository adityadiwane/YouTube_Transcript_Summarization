WEBVTT
Kind: captions
Language: vi

00:00:00.000 --> 00:00:07.000
Translator: Tân Nguyễn
Reviewer: Ngoc Bui

00:00:11.820 --> 00:00:14.070
Đây là Lee Sedol.

00:00:14.084 --> 00:00:17.895
Lee Sedol là một trong những kì thủ cờ vây
giỏi nhất thế giới,

00:00:17.909 --> 00:00:21.219
cậu ấy đang có thứ mà các bạn của tôi
ở thung lũng Silicon gọi là

00:00:21.219 --> 00:00:22.499
khoảnh khắc "Ôi Chúa ơi" --

00:00:22.519 --> 00:00:23.965
(Cười)

00:00:23.965 --> 00:00:25.857
khoảnh khắc khi chúng ta nhận ra rằng

00:00:25.881 --> 00:00:29.954
AI (trí tuệ nhân tạo) đã xử lí
nhanh hơn nhiều so với chúng ta mong đợi.

00:00:29.974 --> 00:00:33.361
Vậy con người đã thua trên bàn cờ vây.
Thế còn trong thế giới thực?

00:00:33.361 --> 00:00:35.145
Thế giới thực lớn hơn,

00:00:35.169 --> 00:00:37.418
phức tạp hơn nhiều so với cờ vây.

00:00:37.442 --> 00:00:39.261
Nó không dễ nhận thấy,

00:00:39.285 --> 00:00:42.723
nhưng nó vẫn là một vấn đề
mang tính chọn lựa.

00:00:42.768 --> 00:00:45.089
Và nếu chúng ta nghĩ về một số công nghệ

00:00:45.113 --> 00:00:47.542
đang được chú ý đến...

00:00:47.558 --> 00:00:51.893
Noriko [Arai] từng khẳng định
máy móc vẫn chưa thể đọc,

00:00:51.917 --> 00:00:53.417
ít nhất là với sự thấu hiểu.

00:00:53.441 --> 00:00:54.977
Nhưng điều đó sẽ xảy ra,

00:00:55.001 --> 00:00:56.772
và khi điều đó xảy ra,

00:00:56.796 --> 00:00:57.983
rất nhanh sau đó,

00:00:58.007 --> 00:01:02.579
máy móc sẽ đọc hết
những thứ mà loài người đã viết.

00:01:03.670 --> 00:01:05.700
Và điều đấy sẽ cho phép máy móc,

00:01:05.724 --> 00:01:08.644
cùng với khả năng dự đoán 
xa hơn con người,

00:01:08.668 --> 00:01:10.348
như chúng ta đã thấy trong cờ vây,

00:01:10.372 --> 00:01:12.810
nếu chúng cũng có thể tiếp cận 
nhiều thông tin hơn,

00:01:12.810 --> 00:01:16.772
chúng sẽ có thể đưa ra những quyết định
tốt hơn chúng ta trong thế giới thực.

00:01:18.612 --> 00:01:20.218
Vậy đó có phải là điều tốt?

00:01:22.708 --> 00:01:24.940
Tôi hi vọng vậy.

00:01:26.514 --> 00:01:30.079
Toàn bộ nền văn minh của loài người,
tất cả những thứ chúng ta coi trọng,

00:01:30.083 --> 00:01:31.861
đều dựa trên trí tuệ của chúng ta.

00:01:31.885 --> 00:01:35.579
Và nếu chúng ta có thể sở hữu
nhiều trí tuệ hơn,

00:01:35.603 --> 00:01:38.905
những thứ con người có thể làm
sẽ không có giới hạn.

00:01:40.485 --> 00:01:43.594
Và tôi nghĩ đây có thể là,
như nhiều người đã miêu tả nó,

00:01:43.594 --> 00:01:45.555
sự kiện lớn nhất trong lịch sử nhân loại.

00:01:45.565 --> 00:01:48.485
[Chào mừng tới UTOPIA
Hãy tận hưởng hành trình của bạn]

00:01:48.485 --> 00:01:51.314
Vậy tại sao mọi người lại nói
những điều như thế này,

00:01:51.338 --> 00:01:54.214
AI có thể là sự chấm dứt của loài người?

00:01:55.258 --> 00:01:56.917
Đây có phải là một điều mới mẻ?

00:01:56.941 --> 00:02:01.051
Phải chăng chỉ có 
Elon Musk, Bill Gates và Stephen Hawking?

00:02:01.773 --> 00:02:03.153
Thực ra là không.

00:02:03.153 --> 00:02:05.059
Ý tưởng này đã có trước đây.

00:02:05.059 --> 00:02:07.021
Đây là một trích dẫn:

00:02:07.045 --> 00:02:11.395
"Ngay cả khi chúng ta có thể giữ 
máy móc như một công cụ,

00:02:11.419 --> 00:02:14.403
chẳng hạn như, bằng cách tắt nguồn
khi chúng ta muốn"--

00:02:14.427 --> 00:02:17.394
và tôi sẽ quay lại 
với khái niệm "tắt nguồn" sau --

00:02:17.398 --> 00:02:20.492
chúng ta vẫn nên cảm thấy khiêm tốn hơn.

00:02:21.997 --> 00:02:23.370
Vậy ai đã nói điều này?

00:02:23.380 --> 00:02:25.920
Chính là Alan Turing vào năm 1951.

00:02:25.950 --> 00:02:28.883
Alan Turing, như bạn đã biết,
là cha đẻ của khoa học máy tính,

00:02:28.907 --> 00:02:31.955
và theo nhiều cách,
ông cũng là cha đẻ của AI.

00:02:33.059 --> 00:02:34.941
Nếu chúng ta nghĩ về vấn đề này,

00:02:34.965 --> 00:02:38.752
tạo nên một loài
thông minh hơn loài của chính bạn,

00:02:38.776 --> 00:02:41.398
có thể gọi nó là "vấn đề gorilla,"

00:02:42.165 --> 00:02:45.915
bởi vì tổ tiên gorilla
đã làm việc này từ hàng triệu năm trước,

00:02:45.939 --> 00:02:48.144
và bây giờ chúng ta có thể hỏi chúng:

00:02:48.144 --> 00:02:49.672
Đây có phải là một ý tưởng hay?

00:02:49.686 --> 00:02:53.446
Và chúng đang có một buổi họp để thảo luận
xem đây có phải là một ý tưởng hay,

00:02:53.450 --> 00:02:56.661
và sau một khoảng thời gian,
chúng kết luận: Không,

00:02:56.661 --> 00:02:58.071
đó là một ý tưởng tồi tệ.

00:02:58.081 --> 00:03:00.340
Chúng ta đang ở trong 
hoàn cảnh rất khó khăn.

00:03:00.340 --> 00:03:04.832
Thậm chí, bạn có thể nhìn thấy
nỗi buồn hiện hữu trong mắt chúng.

00:03:04.832 --> 00:03:05.987
(Cười)

00:03:05.987 --> 00:03:07.607
Vậy cảm giác bất an rằng

00:03:07.607 --> 00:03:11.155
tạo ra một loài thông minh hơn
chính loài của chúng ta

00:03:11.155 --> 00:03:13.203
có thể không phải là một ý kiến hay --

00:03:14.158 --> 00:03:15.573
Chúng ta có thể làm gì nó?

00:03:16.153 --> 00:03:17.990
Thực sự là không gì cả,

00:03:17.990 --> 00:03:19.928
trừ việc ngừng tạo ra AI,

00:03:20.988 --> 00:03:22.956
và bởi vì những lợi ích
mà tôi vừa kể ra

00:03:22.956 --> 00:03:25.086
cũng như tôi là một nhà nghiên cứu về AI,

00:03:25.106 --> 00:03:26.486
tôi sẽ không làm như thế.

00:03:27.066 --> 00:03:28.766
Tôi vẫn muốn tiếp tục làm về AI.

00:03:30.303 --> 00:03:33.024
Chúng ta cần phải
cụ thể hóa vấn đề hơn một chút.

00:03:33.024 --> 00:03:34.772
Chính xác thì đâu mới là vấn đề?

00:03:34.772 --> 00:03:37.450
Vì sao AI tốt hơn lại có thể 
đem lại nhiều tai họa?

00:03:39.024 --> 00:03:41.804
Đây là một câu trích dẫn khác:

00:03:41.804 --> 00:03:45.005
"Chúng ta nên đảm bảo rằng
mục đích mà chúng ta đưa vào máy móc

00:03:45.005 --> 00:03:47.291
là mục đích mà chúng ta 
thực sự mong muốn."

00:03:48.055 --> 00:03:50.793
Đây là câu nói
của Norbert Wiener năm 1960,

00:03:51.343 --> 00:03:53.708
không lâu sau khi ông ấy được xem

00:03:53.718 --> 00:03:55.686
một trong những hệ thống 
học tập thời đầu

00:03:55.686 --> 00:03:58.484
học cách chơi cờ Đam
giỏi hơn người tạo ra nó.

00:04:00.377 --> 00:04:03.323
Nhưng điều tương tự cũng đã được nói 

00:04:03.343 --> 00:04:04.713
bởi vua Midas.

00:04:04.713 --> 00:04:05.936
Vua Midas đã từng bảo:

00:04:05.936 --> 00:04:08.283
"Tôi muốn mọi thứ tôi chạm vào
trở thành vàng."

00:04:08.283 --> 00:04:10.554
và ông ấy đã có chính xác 
những gì ông muốn.

00:04:10.554 --> 00:04:12.883
Đấy là mục đích
mà ông đã đưa vào máy móc,

00:04:13.493 --> 00:04:14.763
như đã nói,

00:04:14.763 --> 00:04:18.203
sau đó thì đồ ăn, thức uống
và người thân của ông đều biến thành vàng

00:04:18.221 --> 00:04:20.285
và ông đã qua đời
trong đau khổ và đói kém.

00:04:22.073 --> 00:04:24.294
Vậy nên chúng ta gọi đây là
"vấn đề Vua Midas"

00:04:24.324 --> 00:04:29.970
khi chúng ta đưa ra một mục tiêu
không trùng khớp với thứ chúng ta muốn.

00:04:29.970 --> 00:04:33.674
Hiện tại, chúng ta gọi đó là
"vấn đề trùng khớp giá trị."

00:04:36.554 --> 00:04:40.314
Tuy nhiên xác định sai mục tiêu 
không phải là vấn đề duy nhất.

00:04:40.314 --> 00:04:41.881
Còn một phần nữa.

00:04:41.964 --> 00:04:43.994
Nếu chúng ta đưa mục đích vào một cỗ máy,

00:04:43.994 --> 00:04:46.470
kể cả một thứ đơn giản 
như "đi lấy cà phê,"

00:04:47.618 --> 00:04:49.458
cỗ máy tự nói với chính nó,

00:04:50.572 --> 00:04:53.234
"Ừm, điều gì có thể
làm việc đi lấy cà phê thất bại?"

00:04:53.234 --> 00:04:55.192
Như ai đó có thể sẽ tắt tôi đi.

00:04:55.482 --> 00:04:57.472
Ok, tôi phải ngăn chặn việc đó.

00:04:58.282 --> 00:05:00.102
Tôi sẽ vô hiệu hóa nút 'tắt' của tôi.

00:05:00.312 --> 00:05:02.262
Tôi sẽ làm mọi thứ để bảo vệ bản thân

00:05:02.272 --> 00:05:05.212
khỏi các thứ cản trở tôi 
đạt được mục đích đã được giao."

00:05:06.040 --> 00:05:07.850
Vậy sự chuyên tâm theo đuổi này

00:05:08.971 --> 00:05:11.773
theo một cách rất phòng ngự
đối với mục tiêu mà, thật ra,

00:05:11.803 --> 00:05:14.622
không tương ứng với 
mục đích chính của loài người --

00:05:15.990 --> 00:05:17.939
đó là vấn đề mà chúng ta phải đối mặt.

00:05:18.799 --> 00:05:23.581
Và thực ra, đây là bài học đáng giá
được rút ra từ bài nói này.

00:05:23.601 --> 00:05:25.776
Nếu bạn muốn nhớ một điều,

00:05:25.776 --> 00:05:28.220
đó là bạn sẽ không thể
lấy được cà phê nếu bạn chết.

00:05:28.220 --> 00:05:29.772
(Cười)

00:05:29.772 --> 00:05:31.289
Nó rất đơn giản, hãy nhớ nó.

00:05:31.289 --> 00:05:33.334
Nhắc lại cho bản thân nghe 3 lần mỗi ngày.

00:05:33.699 --> 00:05:35.093
(Cười)

00:05:35.226 --> 00:05:37.776
Và thực ra, đây chính là nội dung 

00:05:37.776 --> 00:05:40.472
trong phim "2001: [A Space Odyssey]"

00:05:40.886 --> 00:05:42.796
HAL có một mục tiêu, một nhiệm vụ,

00:05:43.044 --> 00:05:46.623
không trùng với mục tiêu của loài người,

00:05:46.917 --> 00:05:49.234
và nó dẫn tới sự mâu thuẫn này.

00:05:49.398 --> 00:05:52.057
Nhưng may mắn là Hal không phải
một cỗ máy siêu trí tuệ.

00:05:52.408 --> 00:05:54.023
Nó khá thông minh,

00:05:54.023 --> 00:05:56.005
nhưng cuối cùng Dave đã khuất phục được

00:05:56.263 --> 00:05:59.263
và đã tắt nguồn nó thành công.

00:06:01.853 --> 00:06:04.853
Nhưng có thể chúng ta sẽ 
không may mắn như thế.

00:06:04.853 --> 00:06:07.853
[Xin lỗi Dave. Tôi e rằng 
tôi không thể làm điều đó được.]

00:06:07.858 --> 00:06:09.304
Vậy chúng ta sẽ phải làm gì?

00:06:10.204 --> 00:06:12.304
[AI hợp tác được với con người]

00:06:12.304 --> 00:06:15.153
Tôi đang cố định nghĩa lại về AI

00:06:15.153 --> 00:06:17.471
để thoát khỏi định nghĩa truyền thống

00:06:17.471 --> 00:06:21.611
là máy móc mà theo đuổi 
mục tiêu một cách thông minh.

00:06:22.581 --> 00:06:23.941
Nó bao gồm ba nguyên tắc.

00:06:23.941 --> 00:06:25.001
Thứ nhất,

00:06:25.001 --> 00:06:27.667
đó là nguyên tắc về 
lòng vị tha, nếu bạn thích,

00:06:27.667 --> 00:06:30.929
mục tiêu duy nhất của robot là

00:06:30.953 --> 00:06:35.199
hiện thực hóa tối đa 
mục tiêu của con người,

00:06:35.223 --> 00:06:36.613
các giá trị của con người.

00:06:36.637 --> 00:06:39.997
Và giá trị ở đây tôi muốn nói không phải 
sự nhạy cảm hay đạo đức giả.

00:06:39.997 --> 00:06:44.978
Ý tôi là bất kể thứ gì mà con người muốn 
cuộc sống của họ trở nên giống thế.

00:06:47.014 --> 00:06:49.423
Và thực ra thì điều này 
đã vi phạm luật của Asimov

00:06:49.427 --> 00:06:51.846
đó là robot phải tự bảo vệ 
sự tồn tại của nó.

00:06:51.870 --> 00:06:55.593
Nó không hứng thú duy trì 
sự tồn tại của mình bất kể thế nào.

00:06:57.240 --> 00:07:01.008
Luật thứ hai đó là luật về 
sự khiêm tốn, nếu bạn thích.

00:07:01.794 --> 00:07:05.027
Và điều này thật ra rất quan trọng để 
robot trở nên an toàn.

00:07:05.561 --> 00:07:08.703
Nó nói rằng robot không hề biết

00:07:08.727 --> 00:07:10.755
những giá trị của con người là gì,

00:07:10.779 --> 00:07:13.957
nên nó phải tối ưu hóa chúng,
nhưng không biết chúng là gì.

00:07:15.074 --> 00:07:18.640
Do đó tránh khỏi rắc rối từ sự chuyên tâm 
theo đuổi mục đích.

00:07:18.650 --> 00:07:21.132
Sự không chắc chắn này 
hóa ra lại rất quan trọng.

00:07:21.366 --> 00:07:23.275
Bây giờ, để trở nên có ích cho chúng ta,

00:07:23.275 --> 00:07:25.940
nó phải có một chút ý tưởng về thứ
mà chúng ta muốn.

00:07:27.043 --> 00:07:32.470
Nó thu nhận các thông tin này chủ yếu 
bằng việc quan sát con người lựa chọn,

00:07:32.494 --> 00:07:35.295
vậy lựa chọn của chúng ta sẽ 
hé lộ thông tin

00:07:35.319 --> 00:07:38.619
về những thứ mà chúng ta muốn 
cuộc sống của mình trở nên như vậy.

00:07:40.452 --> 00:07:41.709
Vậy đó là ba nguyên tắc.

00:07:41.709 --> 00:07:44.341
Hãy xem ta áp dụng chúng vào 
câu hỏi này như thế nào:

00:07:44.341 --> 00:07:47.313
"Bạn có thể tắt nguồn chiếc máy không?"
như Turing đã đưa ra.

00:07:48.043 --> 00:07:49.243
[Vấn đề tắt nguồn]

00:07:49.243 --> 00:07:50.907
Đây là robot PR2.

00:07:50.907 --> 00:07:52.858
Một cái ở phòng nghiên cứu chúng tôi có,

00:07:52.882 --> 00:07:55.785
và nó có một nút "tắt" lớn đỏ ở sau lưng.

00:07:56.361 --> 00:07:58.630
Câu hỏi là: 
Nó có để cho bạn tắt nó đi không?

00:07:58.630 --> 00:08:00.405
Giả sử ta làm theo cách truyền thống,

00:08:00.405 --> 00:08:02.385
đưa cho nó mục tiêu là "Đi lấy cà phê",

00:08:02.385 --> 00:08:03.995
"Tôi phải đi lấy cà phê",

00:08:03.995 --> 00:08:06.575
"Tôi không thể lấy cà phê nếu tôi chết",

00:08:06.599 --> 00:08:09.940
vậy rõ ràng PR2 đã nghe bài nói của tôi,

00:08:09.964 --> 00:08:13.717
và do đó nó nói: 
"Tôi phải vô hiệu hóa nút "tắt" của mình,

00:08:14.796 --> 00:08:17.490
và có lẽ sốc điện tất cả 
những người trong Starbucks,

00:08:17.514 --> 00:08:19.074
những người có thể cản trở tôi."

00:08:19.098 --> 00:08:20.814
(Cười)

00:08:20.814 --> 00:08:23.307
Vậy điều này có vẻ không thể 
tránh khỏi, đúng không?

00:08:23.307 --> 00:08:25.759
Sự thất bại như thế này có vẻ 
không thể tránh được,

00:08:25.783 --> 00:08:29.326
và nó là kết quả của việc có 
một mục tiêu rõ ràng.

00:08:30.632 --> 00:08:33.826
Vậy sẽ thế nào nếu chiếc máy 
không chắc chắn với mục tiêu của mình?

00:08:33.826 --> 00:08:35.927
Nó sẽ lý luận theo một cách khác.

00:08:35.951 --> 00:08:38.345
Nó nghĩ: "Chà, con người 
có thể sẽ tắt mình mất,

00:08:38.964 --> 00:08:40.830
nhưng chỉ khi mình làm sai gì đó.

00:08:41.567 --> 00:08:43.666
Mình thực sự không biết thế nào là "sai",

00:08:43.666 --> 00:08:45.974
nhưng mình biết là 
mình không muốn làm điều đó."

00:08:45.974 --> 00:08:49.144
Và đây là lúc áp dụng 
luật thứ nhất và thứ hai.

00:08:49.168 --> 00:08:52.527
"Do đó mình nên để con người tắt mình đi."

00:08:53.541 --> 00:08:56.421
Và thực tế bạn có thể tính toán động lực

00:08:56.421 --> 00:09:00.014
mà robot phải để con người tắt nó,

00:09:00.038 --> 00:09:01.952
và nó liên kết trực tiếp

00:09:01.976 --> 00:09:04.722
với mức độ không chắc chắn 
về các mục tiêu tiềm ẩn.

00:09:05.797 --> 00:09:08.746
Và khi mà chiếc máy đã được tắt đi,

00:09:08.770 --> 00:09:10.575
thì đến lượt của luật thứ ba.

00:09:10.599 --> 00:09:13.661
Nó sẽ học được gì đó về 
mục tiêu mà nó cần theo đuổi,

00:09:13.685 --> 00:09:16.218
vì nó học được những việc 
mình đã làm là không đúng.

00:09:16.242 --> 00:09:19.812
Thực tế, nếu sử dụng 
các kí hiệu La Mã thích hợp,

00:09:19.836 --> 00:09:21.891
như các nhà toán học hay làm,

00:09:21.891 --> 00:09:23.975
chúng ta thực sự có thể 
chứng minh mệnh đề

00:09:23.999 --> 00:09:27.552
nói rằng robot này quả là 
có ích cho con người.

00:09:27.576 --> 00:09:31.379
Bạn có thể cải thiện tốt hơn 
với chiếc máy được thiết kế như thế này

00:09:31.403 --> 00:09:32.649
so với không có nó.

00:09:33.057 --> 00:09:35.963
Vậy đây là một ví dụ rất đơn giản,
nhưng đó là bước đầu tiên

00:09:35.987 --> 00:09:39.890
trong việc chúng tôi cố gắng làm ra 
AI hòa hợp với con người.

00:09:42.477 --> 00:09:45.468
Bây giờ, luật thứ ba này,

00:09:45.468 --> 00:09:48.124
tôi nghĩ nó là điều 
khiến bạn phải vò đầu bứt tai suốt.

00:09:48.124 --> 00:09:49.194
Có thể bạn đang nghĩ:

00:09:49.194 --> 00:09:52.157
"Chà, bạn biết đấy, tôi cư xử khá tệ.

00:09:52.157 --> 00:09:55.086
Tôi không muốn robot của mình 
cư xử giống tôi.

00:09:55.470 --> 00:09:58.238
Tôi mò mẫm vào giữa đêm và lén lút 
lấy đồ trong tủ lạnh.

00:09:58.238 --> 00:09:59.600
Tôi làm điều này, điều nọ."

00:09:59.600 --> 00:10:02.351
Có cả tá thứ mà 
bạn không muốn robot làm theo.

00:10:02.351 --> 00:10:04.436
Nhưng thực tế, nó không hoạt động như thế.

00:10:04.436 --> 00:10:06.685
Chỉ vì bạn cư xử tồi tệ

00:10:06.685 --> 00:10:09.238
không có nghĩa là robot sẽ 
bắt chước hành vi của bạn.

00:10:09.238 --> 00:10:13.412
Nó sẽ hiểu động lực của bạn 
và có thể giúp bạn chống lại chúng,

00:10:13.436 --> 00:10:14.756
nếu điều đó phù hợp.

00:10:16.026 --> 00:10:17.490
Nhưng thực ra vẫn khó.

00:10:18.122 --> 00:10:20.561
Điều chúng tôi đang cố gắng làm, thật ra,

00:10:20.561 --> 00:10:26.331
là giúp máy tính dự đoán 
cho mỗi người và cho mỗi cuộc sống

00:10:26.331 --> 00:10:27.672
mà họ có thể đã được sống,

00:10:27.696 --> 00:10:29.823
và cuộc sống của tất cả mọi người khác:

00:10:29.823 --> 00:10:31.834
Họ thích cuộc sống nào nhất?

00:10:33.881 --> 00:10:36.449
Và có rất rất nhiều khó khăn 
trong việc này.

00:10:36.449 --> 00:10:39.791
Tôi không hy vọng là 
chúng sẽ được giải quyết nhanh chóng.

00:10:39.815 --> 00:10:42.458
Khó khăn lớn nhất, thật ra, 
là chính chúng ta.

00:10:43.969 --> 00:10:46.910
Như tôi đã đề cập, chúng ta cư xử khá tệ.

00:10:46.910 --> 00:10:49.431
Thực tế, một số chúng ta 
thực sự đã mục nát.

00:10:50.151 --> 00:10:53.393
Bây giờ robot, như tôi đã nói, 
không cần phải bắt chước các hành vi.

00:10:53.393 --> 00:10:56.118
Robot không có bất cứ 
mục tiêu nào cho riêng chúng.

00:10:56.142 --> 00:10:57.879
Chúng hoàn toàn rất vị tha.

00:10:59.113 --> 00:11:01.198
Và nó không được thiết kế

00:11:01.198 --> 00:11:04.358
để thỏa mãn ước muốn 
của chỉ một cá nhân, một người dùng,

00:11:04.358 --> 00:11:07.496
mà thực tế nó phải tôn trọng quan điểm 
của tất cả mọi người.

00:11:09.083 --> 00:11:11.653
Do đó nó có thể xử lý 
với một số hành vi xấu xa,

00:11:11.677 --> 00:11:15.378
và thậm chí có thể thông cảm 
với sự sai trái của bạn, ví dụ,

00:11:15.402 --> 00:11:18.073
có thể bạn nhận hối lộ 
khi làm công việc hộ chiếu

00:11:18.097 --> 00:11:21.909
vì bạn cần nuôi sống gia đình 
và cho con của bạn đi học.

00:11:21.933 --> 00:11:25.039
Chúng có thể hiểu điều này;
nó không có nghĩa là chúng sẽ ăn cắp.

00:11:25.039 --> 00:11:27.542
Thực tế, nó chỉ giúp bạn 
giúp con bạn được đi học.

00:11:28.796 --> 00:11:31.808
Chúng ta cũng bị hạn chế về mặt tính toán.

00:11:31.832 --> 00:11:34.337
Lee Sedol là một thiên tài cờ vây,

00:11:34.361 --> 00:11:35.686
nhưng anh ấy vẫn thua.

00:11:35.710 --> 00:11:39.949
Nếu ta nhìn vào hành động của anh ấy,
anh ấy chấp nhận đã thua ván cờ.

00:11:39.973 --> 00:11:42.134
Nó không có nghĩa là anh ấy muốn thua.

00:11:43.160 --> 00:11:45.200
Vậy để hiểu được hành vi của anh ấy,

00:11:45.224 --> 00:11:48.868
chúng ta phải quay ngược trở lại 
với mô hình nhận thức của con người

00:11:48.892 --> 00:11:51.463
mà bao gồm những hạn chế về 
tính toán của chúng ta.

00:11:51.463 --> 00:11:53.893
Và nó là một hệ thống rất phức tạp.

00:11:53.893 --> 00:11:56.886
Nhưng nó vẫn là thứ mà chúng ta có thể
khám phá và hiểu nó.

00:11:57.696 --> 00:12:02.016
Có lẽ phần khó khăn nhất, 
dưới góc nhìn là một nhà nghiên cứu AI,

00:12:02.040 --> 00:12:04.615
đó là số lượng của chúng ta quá nhiều,

00:12:06.114 --> 00:12:09.695
thế nên máy tính phải bằng một cách nào đó
cân đong đo đếm các quan điểm

00:12:09.719 --> 00:12:11.944
của nhiều người khác nhau,

00:12:11.968 --> 00:12:13.874
và có rất nhiều cách để làm việc này.

00:12:13.898 --> 00:12:17.587
Các nhà kinh tế học, xã hội học, 
triết học đạo đức đã hiểu điều đó,

00:12:17.611 --> 00:12:20.066
và chúng tôi đang chủ động 
tìm kiếm các sự hợp tác.

00:12:20.090 --> 00:12:23.341
Hãy quan sát và xem điều gì sẽ xảy ra
khi bạn hiểu sai nó.

00:12:23.365 --> 00:12:25.498
Ví dụ, bạn có thể có một cuộc đối thoại

00:12:25.522 --> 00:12:27.466
với trợ lý thông minh riêng của bạn

00:12:27.490 --> 00:12:29.775
điều có thể sẽ thành hiện thực 
trong vài năm tới.

00:12:29.799 --> 00:12:32.323
Bạn có thể nghĩ về Siri 
nhưng ngoài đời thực.

00:12:33.447 --> 00:12:37.769
Siri nói: "Vợ anh đã gọi để nhắc anh về 
buổi ăn tối ngày mai."

00:12:38.436 --> 00:12:40.944
Và đương nhiên, bạn đã quên.
"Hả? Bữa tối nào?

00:12:40.968 --> 00:12:42.393
Cô đang nói về gì vậy?"

00:12:42.417 --> 00:12:46.163
"..., kỉ niệm 20 năm ngày cưới, 
lúc 7 giờ tối."

00:12:48.735 --> 00:12:52.454
"Tôi không thể. Tôi có cuộc gặp với
tổng thư ký lúc 7h30.

00:12:52.478 --> 00:12:54.170
Sao..., sao chuyện này 
có thể xảy ra chứ?"

00:12:54.194 --> 00:12:58.854
"Chà, tôi đã cảnh báo anh 
nhưng anh đã lờ đi lời khuyên của tôi."

00:12:59.966 --> 00:13:03.294
"Tôi phải làm gì bây giờ?
Tôi không thể nói cô ấy là tôi quá bận."

00:13:04.310 --> 00:13:07.591
"Đừng lo lắng. Tôi sẽ sắp xếp để
chuyến bay của anh ấy bị hoãn lại."

00:13:07.615 --> 00:13:09.297
(Cười)

00:13:10.069 --> 00:13:12.170
"Bằng một lỗi trục trặc kĩ thuật nào đó."

00:13:12.194 --> 00:13:13.406
(Cười)

00:13:13.430 --> 00:13:15.047
"Thật ư? Cô có thể làm thế à?"

00:13:16.220 --> 00:13:18.399
"Anh ấy đã gửi thư xin lỗi

00:13:18.423 --> 00:13:20.978
và mong sẽ được 
gặp anh ở bữa trưa ngày mai."

00:13:21.002 --> 00:13:22.301
(Cười)

00:13:22.325 --> 00:13:26.728
Vậy giá trị ở đây -- 
có một chút sai lầm đã xảy ra.

00:13:26.752 --> 00:13:29.761
Nó hoàn toàn theo đuổi giá trị của vợ tôi

00:13:29.785 --> 00:13:31.854
đó là "Vợ vui thì đời cũng vui."

00:13:31.878 --> 00:13:33.435
(Cười)

00:13:33.435 --> 00:13:35.309
Nó có thể diễn ra 
theo một hướng khác.

00:13:35.641 --> 00:13:37.916
Bạn vừa trở về nhà 
sau một ngày làm việc vất vả,

00:13:37.916 --> 00:13:39.995
và máy tính hỏi: "Một ngày dài à?"

00:13:39.995 --> 00:13:42.117
"Ừ, tôi còn chẳng có 
thời gian để ăn trưa."

00:13:42.117 --> 00:13:43.679
"Chắc anh phải đói lắm rồi."

00:13:43.703 --> 00:13:46.509
"Ừ, đói muốn chết.
Bạn có thể làm bữa tối cho tôi không?"

00:13:47.890 --> 00:13:49.980
"Có điều này tôi phải nói với anh."

00:13:50.004 --> 00:13:51.159
(Cười)

00:13:52.037 --> 00:13:56.942
"Những người ở Nam Sudan đang cần 
sự trợ giúp khẩn cấp hơn anh nhiều."

00:13:56.942 --> 00:13:58.046
(Cười)

00:13:58.070 --> 00:14:00.145
"Nên tôi đi đây. 
Tự làm bữa tối của anh đi."

00:14:00.169 --> 00:14:02.169
(Cười)

00:14:02.643 --> 00:14:04.382
Vậy chúng ta phải xử lý 
những vấn đề như thế này,

00:14:04.406 --> 00:14:06.921
và tôi rất nóng lòng được 
làm việc với chúng.

00:14:06.945 --> 00:14:08.788
Có những lý do để mà lạc quan.

00:14:08.812 --> 00:14:09.971
Một lý do là,

00:14:09.995 --> 00:14:11.943
có một lượng khổng lồ dữ liệu ngoài kia.

00:14:11.943 --> 00:14:15.195
Bởi vì như tôi đã nói,
chúng sẽ đọc hết tất cả mọi thứ trên đời.

00:14:15.365 --> 00:14:18.443
Hầu hết những gì chúng ta viết là về
những việc làm của nhân loại

00:14:18.443 --> 00:14:21.017
và sau đó những người khác 
cảm thấy phiền lòng về nó.

00:14:21.017 --> 00:14:23.359
Do đó có một lượng 
khổng lồ dữ liệu để học tập.

00:14:23.383 --> 00:14:25.619
Đồng thời có một động lực kinh tế rất lớn

00:14:27.151 --> 00:14:28.337
để làm đúng việc này.

00:14:28.361 --> 00:14:30.362
Hãy tưởng tượng robot gia đình ở nhà bạn.

00:14:30.386 --> 00:14:33.453
Bạn lại đi làm về trễ 
và robot phải nấu ăn cho bọn trẻ,

00:14:33.477 --> 00:14:36.300
bọn trẻ thì đang đói 
và không còn thứ gì trong tủ lạnh.

00:14:36.324 --> 00:14:38.929
Và robot nhìn thấy con mèo.

00:14:38.953 --> 00:14:40.645
(Cười)

00:14:40.669 --> 00:14:44.703
Và robot này chưa được học hoàn toàn 
về các giá trị của con người,

00:14:44.703 --> 00:14:46.194
nên nó không thể hiểu được rằng

00:14:46.194 --> 00:14:50.606
các giá trị tình cảm của con mèo 
lớn hơn hẳn giá trị dinh dưỡng của nó.

00:14:50.606 --> 00:14:51.335
(Cười)

00:14:51.335 --> 00:14:53.893
Vậy sau đó chuyện gì xảy ra?

00:14:53.917 --> 00:14:57.214
Chà, nó xảy ra như thế này:

00:14:57.238 --> 00:15:00.202
"Một robot điên loạn nấu mèo con
cho bữa tối của gia đình."

00:15:00.226 --> 00:15:04.749
Một sự cố như thế có thể 
sẽ chấm dứt ngành robot gia đình.

00:15:04.773 --> 00:15:08.145
Do đó có một động lực rất lớn 
để làm việc này đúng trong thời gian dài

00:15:08.169 --> 00:15:10.884
trước khi chúng ta đạt tới 
máy móc siêu thông minh.

00:15:11.948 --> 00:15:13.483
Vậy để tổng kết lại:

00:15:13.507 --> 00:15:16.388
Tôi thực ra đang cố 
thay đổi định nghĩa về AI

00:15:16.412 --> 00:15:19.405
để chúng ta có những máy móc 
được chứng minh là có hiệu quả.

00:15:19.429 --> 00:15:20.595
Và những nguyên tắc là:

00:15:20.595 --> 00:15:22.183
những chiếc máy hoàn toàn vị tha,

00:15:22.183 --> 00:15:24.901
chỉ muốn đạt được mục đích của chúng ta,

00:15:24.925 --> 00:15:28.041
nhưng chúng không chắc chắn 
những mục tiêu này là gì,

00:15:28.065 --> 00:15:30.063
và sẽ theo dõi tất cả chúng ta

00:15:30.087 --> 00:15:33.290
để hiểu thêm về những gì 
chúng ta thực sự muốn.

00:15:34.193 --> 00:15:37.752
Và mong rằng trong quá trình đó,
ta cũng học hỏi để trở nên tốt đẹp hơn.

00:15:37.776 --> 00:15:38.967
Cảm ơn rất nhiều.

00:15:38.991 --> 00:15:42.574
(Vỗ tay)

00:15:42.574 --> 00:15:44.336
Chris Anderson: Rất thú vị, Stuart.

00:15:44.336 --> 00:15:46.110
Chúng ta sẽ đứng đây thêm chút nữa 

00:15:46.110 --> 00:15:48.961
vì tôi nghĩ họ đang chuẩn bị 
cho diễn giả tiếp theo.

00:15:48.985 --> 00:15:50.523
Đây là một số câu hỏi.

00:15:50.547 --> 00:15:56.000
Vậy ý tưởng lập trình với sự thiếu hụt 
thông tin có vẻ như rất mạnh mẽ.

00:15:56.024 --> 00:15:57.618
Nhưng khi ta có siêu trí tuệ,

00:15:57.642 --> 00:15:59.900
điều gì sẽ ngăn cản robot

00:15:59.924 --> 00:16:02.510
đọc những quyển sách 
và phát hiện ra ý tưởng là

00:16:02.510 --> 00:16:04.622
có kiến thức thực ra 
tốt hơn là bị thiếu hụt

00:16:04.622 --> 00:16:08.614
và có thể chuyển hướng mục tiêu của chúng 
sau đó viết lại các chương trình?

00:16:09.512 --> 00:16:15.868
Stuart Rusell: Vâng, chúng ta muốn nó
học hỏi nhiều hơn, như tôi đã nói,

00:16:15.892 --> 00:16:17.179
về mục tiêu của chúng ta.

00:16:17.203 --> 00:16:22.724
Nó sẽ chỉ trở nên chắc chắn hơn 
khi nó làm đúng nhiều hơn,

00:16:22.748 --> 00:16:24.693
vậy đó sẽ là những bằng chứng

00:16:24.717 --> 00:16:27.441
và chúng sẽ được thiết kế 
để diễn dịch đúng đắn điều này.

00:16:27.465 --> 00:16:31.421
Nó sẽ hiểu được, ví dụ như
những quyển sách rất thiên vị

00:16:31.445 --> 00:16:32.928
về những đề tài mà chúng chứa.

00:16:32.952 --> 00:16:35.349
Chúng chỉ nói về các vị vua và hoàng tử

00:16:35.373 --> 00:16:38.173
và các đàn ông quý tộc da trắng làm gì đó.

00:16:38.197 --> 00:16:40.293
Nên đó là một vấn đề phức tạp,

00:16:40.317 --> 00:16:44.189
nhưng khi nó học hỏi nhiều hơn 
về mục tiêu của chúng ta,

00:16:44.213 --> 00:16:46.276
nó sẽ chở nên càng ngày 
càng có ích cho chúng ta.

00:16:46.300 --> 00:16:48.826
CA: Và anh đã không thể 
rút gọn lại trong một luật,

00:16:48.850 --> 00:16:50.500
như là bó buộc nó lại:

00:16:50.524 --> 00:16:53.817
"Nếu loài người đã cố để tắt nguồn tôi,

00:16:53.841 --> 00:16:55.776
tôi sẽ tuân lệnh thôi."

00:16:55.800 --> 00:16:56.982
SR: Thực sự thì không.

00:16:57.006 --> 00:16:58.505
Đó sẽ là một ý tưởng tồi tệ.

00:16:58.529 --> 00:17:01.218
Hãy tưởng tượng anh có một chiếc xe tự lái

00:17:01.242 --> 00:17:04.355
và anh muốn gửi đứa con 5 tuổi 
tới trường mẫu giáo.

00:17:04.897 --> 00:17:08.419
Anh có muốn đứa con 5 tuổi có thể 
tắt chiếc xe khi nó đang chạy không?

00:17:08.419 --> 00:17:09.742
Chắc là không đâu.

00:17:09.742 --> 00:17:15.145
Do đó nó cần hiểu được mức độ 
nhận thức của người đó.

00:17:15.169 --> 00:17:16.845
Nhận thức người đó càng cao,

00:17:16.869 --> 00:17:18.972
khả năng máy tính tự nguyện 
bị tắt càng cao.

00:17:18.996 --> 00:17:21.539
Nếu người đó hoàn toàn lạ mặt 
hay thậm chí là kẻ xấu,

00:17:21.563 --> 00:17:24.075
thì máy tính sẽ khó để bị tắt hơn.

00:17:24.099 --> 00:17:25.299
CA: Được thôi, Stuart,

00:17:25.299 --> 00:17:27.927
tôi rất mong anh sẽ giải quyết 
vấn đề này cho chúng ta.

00:17:27.927 --> 00:17:30.502
Cảm ơn rất nhiều vì cuộc nói chuyện.
Nó rất tuyệt vời.

00:17:30.502 --> 00:17:31.567
SR: Cảm ơn.

00:17:31.567 --> 00:17:33.754
(Vỗ tay)

