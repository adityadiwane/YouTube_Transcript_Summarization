WEBVTT
Kind: captions
Language: ko

00:00:00.000 --> 00:00:07.000
번역: Hyein Jeng
검토: keun_young Lee

00:00:12.532 --> 00:00:14.084
이 사람은 이세돌입니다.

00:00:14.108 --> 00:00:18.105
이세돌은 세계에서 가장 뛰어난
바둑 기사 중 한명이죠.

00:00:18.129 --> 00:00:21.014
보고 계신 이 사진의 순간에
실리콘벨리의 제 친구들은

00:00:21.038 --> 00:00:22.548
"세상에나!" 라고 외쳤습니다.

00:00:22.572 --> 00:00:23.645
(웃음)

00:00:23.669 --> 00:00:25.857
바로 이 순간에

00:00:25.881 --> 00:00:29.177
인공지능이 생각했던 것보다 빠르게
발전하고 있음을 깨닫게 되었죠.

00:00:29.974 --> 00:00:33.021
자, 바둑판에서 인간들은 패배했습니다.
그럼 현실 세상에서는 어떨까요?

00:00:33.045 --> 00:00:35.145
현실 세계는 훨씬 크고

00:00:35.169 --> 00:00:37.418
바둑판보다 훨씬 복잡합니다.

00:00:37.442 --> 00:00:39.261
한 눈에 들어오지도 않고

00:00:39.285 --> 00:00:41.323
결정에 관한 문제도
여전히 남아있습니다.

00:00:42.768 --> 00:00:45.089
그리고 새롭게 떠오르는 
기술들을 살펴보면

00:00:45.113 --> 00:00:46.862
----------------------------------

00:00:47.558 --> 00:00:51.817
노리코 아라이 교수가 말한대로 
독서하는 인공지능은 아직 없습니다.

00:00:51.817 --> 00:00:53.417
적어도 이해를 동반한 독서 말이죠.

00:00:53.441 --> 00:00:54.977
하지만 그런 인공지능도 출현할 겁니다.

00:00:55.001 --> 00:00:56.772
그때가 되면

00:00:56.796 --> 00:00:57.983
얼마 안돼서

00:00:58.007 --> 00:01:02.579
인공지능은 인류가 지금까지 쓴
모든 것을 읽게 될 것입니다.

00:01:03.670 --> 00:01:05.700
그리고 인공지능 기계들이 
그렇게 할 수 있게 되면

00:01:05.724 --> 00:01:08.644
인간보다 더 멀리 예측하는 
능력을 갖게 되고

00:01:08.668 --> 00:01:10.348
바둑 시합에서 이미 드러났듯이

00:01:10.372 --> 00:01:12.536
인공지능 기계들이 더 많은 
정보에 접근할 수 있으면

00:01:12.560 --> 00:01:16.828
실제 세계에서 우리보다 더 나은 
결정들을 할 수 있을 것입니다.

00:01:18.612 --> 00:01:20.218
그러면 좋은 것일까요?

00:01:21.718 --> 00:01:23.950
음, 그랬으면 좋겠네요.

00:01:26.514 --> 00:01:29.769
우리 인류 문명을 통틀어
가치 있다고 여기는 모든 것들은

00:01:29.793 --> 00:01:31.861
우리 지식에 바탕한 것입니다.

00:01:31.885 --> 00:01:35.579
그리고 만약 우리가 더 많은
지식에 접근할 수 있다면

00:01:35.603 --> 00:01:38.905
인류가 할 수 있는 것들에서 
진정한 한계는 없을 것입니다.

00:01:40.485 --> 00:01:43.810
누군가 말했듯이 그렇게만 된다면

00:01:43.834 --> 00:01:45.850
인류 역사상 가장 큰 사건이 되겠죠.

00:01:48.485 --> 00:01:51.314
그런데 왜 사람들은 이런 말을 할까요?

00:01:51.338 --> 00:01:54.214
인공지능(AI)이 우리 인류의 
종말을 가져올 거라고 말이죠.

00:01:55.258 --> 00:01:56.917
인공지능이 새로운 것일까요?

00:01:56.941 --> 00:02:01.051
엘런 머스크, 빌 게이츠, 스티븐 호킹.
이런 사람들만 아는 것인가요?

00:02:01.773 --> 00:02:05.035
사실, 아닙니다. 
이 개념이 나온 지는 좀 되었죠.

00:02:05.059 --> 00:02:07.021
이런 말이 있습니다.

00:02:07.045 --> 00:02:11.395
"중요한 순간에 전원을 꺼버리는 식으로

00:02:11.419 --> 00:02:14.403
기계를 우리 인간에게 계속 
복종하도록 만들 수 있더라도.."

00:02:14.427 --> 00:02:17.098
"전원을 끈다"는 개념은 나중에 
다시 설명하겠습니다.

00:02:17.098 --> 00:02:20.492
"우리는 인류로서 
겸손함을 느껴야 합니다. "

00:02:21.997 --> 00:02:25.445
누가 한 말일까요? 앨런 튜링이
1951년에 한 말입니다.

00:02:26.120 --> 00:02:28.883
앨런 튜링은 아시다시피 
컴퓨터 과학의 아버지입니다.

00:02:28.907 --> 00:02:31.955
그리고 여러 측면에서 
인공지능의 아버지이기도 하죠.

00:02:33.069 --> 00:02:34.951
이런 문제를 생각해볼까요.

00:02:34.965 --> 00:02:38.752
우리 인류의 지능을 뛰어넘는
무언가를 창조하는 문제입니다.

00:02:38.776 --> 00:02:41.398
이른바 "고릴라 문제"라 할 수 있죠.

00:02:42.165 --> 00:02:45.915
수만 년 전 고릴라들의 조상들도 
같은 고민을 했을테니까요.

00:02:45.939 --> 00:02:47.684
그럼 이제 고릴라들에게 
이렇게 물어보면 어떨까요.

00:02:48.572 --> 00:02:49.732
"좋은 아이디어였나요?"

00:02:49.756 --> 00:02:53.286
고릴라들은 좋은 생각이었는지
의논하기 위해 모였습니다.

00:02:53.310 --> 00:02:56.320
그리고 잠시 후 결론을 내립니다.

00:02:56.320 --> 00:02:58.025
최악의 아이디어였다고 결론짓죠.

00:02:58.049 --> 00:03:00.091
그 때문에 자신들이 
곤경에 처했다면서요.

00:03:00.358 --> 00:03:04.621
실제로 그들 눈에서 
존재론적 슬픔이 엿보이네요.

00:03:04.645 --> 00:03:06.285
(웃음)

00:03:06.309 --> 00:03:11.149
따라서 우리보다 더 뛰어난 무언가를 
만들어낸다는 이러한 초조한 느낌은

00:03:11.173 --> 00:03:13.538
좋은 아이디어가 아닐 겁니다.

00:03:14.308 --> 00:03:15.799
그럼 어떻게 해야 할까요?

00:03:15.823 --> 00:03:20.590
음, 사실 인공지능을 중단시키는 것 
말고는 딱히 방법이 없습니다.

00:03:20.614 --> 00:03:23.124
그리고 제가 앞서 말씀드렸던 
모든 장점들 때문이기도 하고

00:03:23.148 --> 00:03:24.864
물론 제가 인공지능을 
연구하고 있어서도 그렇지만

00:03:24.888 --> 00:03:26.679
저는 중단을 고려하지는 않습니다.

00:03:27.103 --> 00:03:29.571
저는 사실 계속 인공지능을
가능하게 하고 싶습니다.

00:03:30.435 --> 00:03:33.017
여기서 이 문제에 대해 좀 더 
살펴볼 필요가 있습니다.

00:03:33.017 --> 00:03:34.422
문제가 정확히 무엇일까요?

00:03:34.422 --> 00:03:37.778
왜 뛰어난 인공지능은 재앙을 뜻할까요?

00:03:39.218 --> 00:03:40.716
자, 이런 말도 있습니다.

00:03:41.755 --> 00:03:45.090
"기계에 부여한 그 목적이
우리가 정말 원했던 목적인지를

00:03:45.114 --> 00:03:47.412
좀 더 확실히 해두었어야 합니다."

00:03:48.102 --> 00:03:51.600
이건 노버트 위너가 
1960년에 한 말입니다.

00:03:51.624 --> 00:03:55.626
매우 초기의 기계 학습 장치가
체스 두는 법을 배우고

00:03:55.650 --> 00:03:58.233
사람보다 체스를 더 잘 두게 
된 것을 보고 바로 한 말이죠.

00:04:00.422 --> 00:04:02.079
하지만 어떻게 보자면

00:04:02.079 --> 00:04:04.296
마이더스 왕과 같다고 
할 수도 있습니다.

00:04:04.903 --> 00:04:07.871
손대는 모든 것이 금으로 
변하길 원했던 왕이죠.

00:04:07.871 --> 00:04:10.534
그리고 그가 원했던대로 
실제로 그렇게 되었습니다.

00:04:10.558 --> 00:04:13.309
그게 바로 그가 기계에 
입력한 목표입니다.

00:04:13.333 --> 00:04:14.783
비유를 하자면 그렇죠.

00:04:14.807 --> 00:04:18.251
그리고 그의 음식, 술, 가족까지 
모두 금으로 변했습니다.

00:04:18.275 --> 00:04:20.556
그리고 그는 처절함과 
배고픔 속에서 죽었습니다.

00:04:22.264 --> 00:04:24.605
우리는 이것을 "마이더스 왕의 
딜레마"라고 부릅니다.

00:04:24.629 --> 00:04:27.934
이 딜레마는 그가 말한 목적이
실제로 그가 정말 원했던 것과

00:04:27.958 --> 00:04:30.371
동일하지 않게 되는 문제입니다.

00:04:30.395 --> 00:04:33.648
현대 용어로는 
"가치 조합 문제"라고 하죠.

00:04:36.867 --> 00:04:40.352
잘못된 목표를 입력하는 것만 
문제가 되는 것은 아닙니다.

00:04:40.376 --> 00:04:41.528
또 다른 문제가 있습니다.

00:04:41.980 --> 00:04:43.923
여러분이 기계에 목표를 입력할 때

00:04:43.947 --> 00:04:46.395
"커피를 갖고 와"같이
단순한 것이라고 해도

00:04:47.728 --> 00:04:49.569
기계는 이렇게 생각할지 모릅니다.

00:04:50.553 --> 00:04:53.176
"자, 어떻게 하면 내가 
커피를 못가져가게 될까?

00:04:53.200 --> 00:04:54.780
누가 전원을 꺼버릴 수도 있잖아.

00:04:55.465 --> 00:04:57.852
좋아, 그런 일을 막기 위해 
조치를 취해야겠어.

00:04:57.876 --> 00:04:59.782
내 '꺼짐' 버튼을 고장 내야겠어.

00:05:00.354 --> 00:05:03.313
주어진 임무를 못하게 
방해하는 것들로부터

00:05:03.337 --> 00:05:05.486
나를 보호하기 위해 
뭐든지 할 거야. "

00:05:05.990 --> 00:05:08.002
사실, 이렇게 방어적 자세로

00:05:09.033 --> 00:05:11.978
오직 목표 달성만을 추구하는 것은

00:05:12.002 --> 00:05:14.816
인류의 진실된 목표와 
일치하지는 않습니다.

00:05:15.942 --> 00:05:17.804
우리가 직면한 문제점이 
바로 이것입니다.

00:05:18.827 --> 00:05:23.594
이건 사실 이번 강연 에서 
무척 고차원적인 부분인데요.

00:05:23.618 --> 00:05:25.673
만약 하나만 기억해야 한다면

00:05:25.697 --> 00:05:28.372
여러분이 죽으면 커피를 가져다 
주지 않을 거라는 것입니다.

00:05:28.396 --> 00:05:29.457
(웃음)

00:05:29.481 --> 00:05:33.310
간단하죠. 
하루에 세 번씩 외우세요.

00:05:33.334 --> 00:05:35.155
(웃음)

00:05:35.179 --> 00:05:37.933
그리고 사실 이게 바로

00:05:37.957 --> 00:05:40.605
"2001:스페이스 오디세이"
영화의 줄거리입니다.

00:05:41.046 --> 00:05:43.136
인공지능인 '할(HAL)'은 목표, 
즉 임무를 갖고 있습니다.

00:05:43.160 --> 00:05:46.892
이건 인류의 목표와 
일치하지는 않습니다.

00:05:46.916 --> 00:05:48.726
그래서 결국 서로 충돌하죠.

00:05:49.314 --> 00:05:52.283
다행히도 HAL의 지능이 
아주 뛰어나진 않았습니다.

00:05:52.307 --> 00:05:55.894
꽤 똑똑했지만 결국 데이브가 
HAL보다 한 수 위였고

00:05:55.918 --> 00:05:57.767
HAL의 전원을 끌 수 있게 됩니다.

00:06:01.648 --> 00:06:03.537
하지만 우리는 영화처럼 
운이 좋지 않을 수 있습니다.

00:06:08.013 --> 00:06:09.605
그럼 어떻게 해야 할까요?

00:06:12.191 --> 00:06:14.792
저는 인공지능을 
다시 정의하려 합니다.

00:06:14.816 --> 00:06:16.877
고전적 개념을 깨려고 해요. 

00:06:16.901 --> 00:06:21.468
기계는 지능적으로 목표를 
달성하려 한다는 개념이죠.

00:06:22.532 --> 00:06:24.064
여기에는 세 가지 원칙이 있습니다.

00:06:24.064 --> 00:06:27.643
첫번째는 이타주의 원칙입니다.

00:06:27.667 --> 00:06:30.929
로봇의 유일한 목적은

00:06:30.953 --> 00:06:33.093
인간의 목표와 

00:06:33.093 --> 00:06:36.613
인간의 가치를 최대한 
실현하는 것입니다.

00:06:36.637 --> 00:06:39.967
여기서 말하는 가치는 닭살돋는 
숭고한 가치를 의미하진 않습니다.

00:06:39.991 --> 00:06:41.212
제가 말씀드리는 가치는 

00:06:41.212 --> 00:06:45.145
어떻게 해야 사람들의 삶이
더 나아질지를 의미하는 것입니다.

00:06:47.184 --> 00:06:49.493
그리고 사실 이건 
아시모프의 원칙 중에서

00:06:49.517 --> 00:06:51.846
로봇은 스스로를 보호해야
한다는 원칙과 충돌합니다.

00:06:51.870 --> 00:06:55.593
어쨌든 로봇은 스스로를 
보호해서 얻는 것이 없죠.

00:06:57.240 --> 00:07:01.008
두 번째 원칙은 겸손에 관한 
법칙이라고 말할 수 있습니다.

00:07:01.794 --> 00:07:05.537
로봇의 안전을 지키기 위한 
아주 중요한 원칙이죠.

00:07:05.561 --> 00:07:06.877
이 원칙은

00:07:06.877 --> 00:07:10.755
로봇들은 인간이 추구하는 가치가
무엇인지는 알 수 없다는 것입니다.

00:07:10.779 --> 00:07:13.957
가치를 극대화해야 하지만 그것이 
무엇인지는 알지 못한다는 것이죠.

00:07:15.074 --> 00:07:16.534
그렇기 때문에 이를 통해서

00:07:16.534 --> 00:07:18.760
목표만을 맹목적으로 추구하는
문제를 피할 수 있습니다.

00:07:18.760 --> 00:07:21.132
이런 불확실성은 
매우 중요한 부분입니다.

00:07:21.546 --> 00:07:23.185
자, 우리에게 도움이 되기 위해서는

00:07:23.209 --> 00:07:25.940
우리가 무엇을 원하는지에 대한
개념을 갖고 있어야 합니다.

00:07:27.043 --> 00:07:32.470
로봇은 주로 사람들이 선택하는 
것을 관찰해서 정보를 얻습니다.

00:07:32.494 --> 00:07:35.295
그래서 우리들의 선택 안에는

00:07:35.319 --> 00:07:38.619
우리 삶이 어떻게 되기를 
바라는지에 관한 정보가 있습니다

00:07:40.452 --> 00:07:41.899
이것들이 세 개의 원칙입니다.

00:07:41.899 --> 00:07:44.477
자 그러면 이것이 다음 질문에 
어떻게 적용되는지 살펴봅시다.

00:07:44.501 --> 00:07:47.290
기계의 전원을 끌 수 있을지에 관해 
앨런 튜링이 제기했던 문제입니다.

00:07:48.893 --> 00:07:51.013
여기에 PR2 로봇이 있습니다.

00:07:51.037 --> 00:07:52.858
저희 연구소에 있는 로봇입니다.

00:07:52.882 --> 00:07:55.785
이 로봇 뒤에는 커다랗고 빨간
"꺼짐" 버튼이 있습니다.

00:07:56.361 --> 00:07:58.976
자 여기에서 질문입니다. 
로봇은 여러분이 전원을 끄도록 할까요?

00:07:59.000 --> 00:08:01.355
고전적 방법대로 로봇에게 
이런 목표를 부여한다고 치죠.

00:08:01.355 --> 00:08:03.971
"커피를 갖고 간다. 
나는 커피를 가져가야만 한다.

00:08:03.995 --> 00:08:06.575
내 전원이 꺼지면
커피를 갖고 갈 수 없다."

00:08:06.599 --> 00:08:10.900
PR2는 제 명령을 그대로 
따르기 위해 이렇게 말하겠죠.

00:08:10.900 --> 00:08:13.717
"좋아, 내 '꺼짐' 버튼을 
망가뜨려야 겠어.

00:08:14.796 --> 00:08:17.490
그리고 스타벅스에 있는 모든 
사람들을 전기총으로 쏴야겠어.

00:08:17.514 --> 00:08:19.074
이 사람들이 장애물이 될 테니까."

00:08:19.098 --> 00:08:21.160
(웃음)

00:08:21.184 --> 00:08:23.337
이렇게 될 수 밖에 없잖아요.

00:08:23.361 --> 00:08:25.759
이러한 종류의 실패 모드는
피할 수 없을 겁니다.

00:08:25.783 --> 00:08:29.326
확실하고 명확한 목적을 갖고 
그대로 따르고 있으니까요.

00:08:30.632 --> 00:08:33.330
그러면 이 기계가 목표를 부정확하게 
알고 있으면 어떤 일이 일어날까요?

00:08:33.330 --> 00:08:35.927
음, 좀 다른 방식으로 
이렇게 생각하게 될 겁니다.

00:08:35.951 --> 00:08:38.375
"좋아, 인간이 
내 전원을 끌 수 있어.

00:08:38.964 --> 00:08:40.830
하지만 내가 뭔가 
잘못했을 때만 그럴 거야.

00:08:41.567 --> 00:08:44.042
그런데 나는 잘못한다는 게 
뭔지는 모르지만

00:08:44.066 --> 00:08:45.674
잘못하고 싶지는 않아."

00:08:45.674 --> 00:08:48.574
자, 여기에 첫 번째와 
두 번째 원칙이 있습니다.

00:08:49.168 --> 00:08:52.527
"그러면 나는 인간이 
전원을 끄게 놔둬야겠다."

00:08:53.541 --> 00:08:57.497
사실 로봇이 인간으로 하여금 
전원을 끌 수 있도록 놔둘 때

00:08:57.521 --> 00:09:00.014
어떤 이득이 있을지 예측할 수 있고

00:09:00.038 --> 00:09:01.636
이것은 기저에 깔린

00:09:01.636 --> 00:09:04.722
목적에 대한 불확실성의 정도와
직접적으로 연결됩니다.

00:09:05.797 --> 00:09:08.746
그리고 로봇의 전원이 꺼지면

00:09:08.770 --> 00:09:10.575
세 번째 원칙이 작동하게 됩니다.

00:09:10.599 --> 00:09:13.385
로봇은 추구해야 하는 
목표에 대해 이해하게 되죠.

00:09:13.385 --> 00:09:15.738
목표 수행에서 무엇이 잘못됐는지 
알게 되기 때문입니다.

00:09:16.242 --> 00:09:19.812
사실 우리가 그리이스 문자를 사용해서

00:09:19.836 --> 00:09:21.967
수학을 늘 공부하는 것처럼

00:09:21.991 --> 00:09:23.975
우리는 하나의 가설을 
증명할 수 있습니다.

00:09:23.999 --> 00:09:27.552
어떤 정의냐면, 이런 로봇은 아마도
인간에게 도움이 될 거라는 겁니다.

00:09:27.576 --> 00:09:31.379
아마도 이렇게 작동하게 만들어진 
기계로 인해 혜택을 보게 될 겁니다.

00:09:31.403 --> 00:09:32.969
이런 기계가 없을 때보다 말이죠.

00:09:33.057 --> 00:09:34.847
이것은 매우 간단한 예시입니다.

00:09:34.847 --> 00:09:39.890
인간과 공존할 수 있는 인공지능으로
무얼 할 수 있는지에 대한 첫걸음이죠.

00:09:42.477 --> 00:09:45.734
자, 이제 세 번째 원칙입니다.

00:09:45.758 --> 00:09:48.870
이걸 보고 어쩌면 여러분이 
머리를 긁적이실 것 같은데요.

00:09:48.894 --> 00:09:52.133
이렇게 생각할 수도 있습니다.
"음, 내가 좀 못됐잖아.

00:09:52.157 --> 00:09:55.086
나는 내 로봇이 나처럼 
행동하는 건 별로야.

00:09:55.110 --> 00:09:58.138
나는 한밤중에 몰래 냉장고를 
뒤져서 먹을 걸 찾기도 하잖아.

00:09:58.138 --> 00:09:59.410
그것 말고도 많지."

00:09:59.410 --> 00:10:02.557
로봇이 하지 않았으면 하는 
여러 행동들이 있을 겁니다.

00:10:02.581 --> 00:10:04.236
그런데, 그런 일은 없을 거예요.

00:10:04.236 --> 00:10:06.831
단순히 여러분이 
나쁜 행동을 했다고 해서

00:10:06.855 --> 00:10:09.042
로봇이 그 행동을 그대로 
따라서 하는 건 아닙니다.

00:10:09.042 --> 00:10:13.412
로봇은 여러분의 동기를 이해하고 
그 나쁜 행동을 하지 않도록 돕습니다.

00:10:13.436 --> 00:10:14.756
만약 그게 옳다면 말이죠.

00:10:16.026 --> 00:10:17.490
하지만 이건 어려운 문제입니다.

00:10:18.122 --> 00:10:20.667
사실 우리가 하려고 하는 건

00:10:20.691 --> 00:10:26.487
기계로 하여금 누군가를 대상으로 
앞으로 그가 살게 될 삶이 어떠한지를

00:10:26.511 --> 00:10:27.672
예측하게 하는 겁니다.

00:10:27.696 --> 00:10:29.293
그리고 다른 여러 모두의 
사람들의 삶을 말이죠.

00:10:29.317 --> 00:10:31.834
어떤 것을 그들이 더 선호할까요?

00:10:33.881 --> 00:10:36.835
이를 가능하게 하려면 너무나
많은 난관을 넘어야 합니다.

00:10:36.859 --> 00:10:39.791
저는 이 문제들이 단기간에
해결되리라고 보지 않습니다.

00:10:39.815 --> 00:10:42.458
사실 그중 가장 큰 난관은 
바로 우리 자신입니다.

00:10:43.969 --> 00:10:46.680
앞서 말씀드렸듯이 
우리는 나쁜 행동을 합니다.

00:10:46.680 --> 00:10:49.431
사실 우리 중에 누군가는 
정말 형편 없을지도 모르죠.

00:10:50.251 --> 00:10:53.303
말씀드렸듯이 로봇은 그런 행동을 
그대로 따라 하지는 않습니다.

00:10:53.327 --> 00:10:56.118
로봇은 스스로를 위한 
어떤 목적도 갖지 않습니다.

00:10:56.142 --> 00:10:57.879
로봇은 순전히 이타적이죠.

00:10:59.113 --> 00:11:04.334
그리고 사용자라는 한 사람만의 욕구를
충족하기 위해 만들어진 것도 아닙니다.

00:11:04.358 --> 00:11:07.496
사실은 모든 사람들이 
원하는 바를 고려해야 하죠.

00:11:09.083 --> 00:11:11.653
어느 정도의 형편없는 상황은
감내하게 될 것입니다.

00:11:11.677 --> 00:11:14.612
여러분이 아무리 형편없어도 
어느 정도는 양해해 주겠죠.

00:11:14.612 --> 00:11:18.073
예를 들어 여러분이 여권 발급 
공무원인데 뇌물을 받았다고 칩시다.

00:11:18.097 --> 00:11:21.909
그런데 뇌물을 받은 이유가 생활비와 
아이들 교육을 위한 것이었다면

00:11:21.933 --> 00:11:24.573
로봇은 이런 걸 이해하고, 
빼앗아가지는 않을 겁니다.

00:11:24.573 --> 00:11:27.542
오히려 로봇은 여러분의 자녀가 
학교에 갈 수 있도록 도울 거예요.

00:11:28.796 --> 00:11:31.808
우리의 연산능력은 한계가 있습니다.

00:11:31.832 --> 00:11:34.337
이세들은 천재 바둑기사입니다.

00:11:34.361 --> 00:11:35.686
하지만 그는 로봇에게 졌죠.

00:11:35.710 --> 00:11:39.949
그의 수를 살펴보면, 그는 로봇에게
질 수 밖에 없는 수를 두었습니다.

00:11:39.973 --> 00:11:42.134
그렇다고 해서 그가 패배를
원했다는 건 아닙니다.

00:11:43.160 --> 00:11:45.200
따라서 그의 수를 이해하려면

00:11:45.224 --> 00:11:48.868
우리는 인간의 지각 모델을 
아예 뒤집어 봐야 합니다

00:11:48.892 --> 00:11:53.869
인간의 지각 모델은 우리의 계산적 
한계를 담고 있어서 매우 복잡합니다.

00:11:53.893 --> 00:11:56.886
하지만 연구를 통해서 
계속 발전하고 있습니다.

00:11:57.696 --> 00:12:02.016
인공지능 연구자의 입장에서 
제가 보기에 가장 어려운 부분은

00:12:02.040 --> 00:12:04.615
인간이 너무 많다는 것입니다.

00:12:06.114 --> 00:12:09.695
그래서 인공지능 기계는 수많은 
사람들의 선호도를 비교하면서

00:12:09.719 --> 00:12:11.944
균형을 유지해야 하죠.

00:12:11.968 --> 00:12:13.874
거기에는 여러 방법이 있습니다.

00:12:13.898 --> 00:12:17.587
그에 관해 경제학자, 사회학자, 
윤리학자들이 연구를 수행했고

00:12:17.611 --> 00:12:20.066
각 분야가 서로 협력하는 
시도를 하고 있습니다.

00:12:20.090 --> 00:12:23.341
자 여러분이 잘못했을 때 
어떤 일이 일어날지 살펴보죠.

00:12:23.365 --> 00:12:25.498
예를 들어 여러분이 
대화를 하고 있습니다.

00:12:25.522 --> 00:12:27.466
여러분의 인공지능 비서와
이야기를 하고 있습니다.

00:12:27.490 --> 00:12:29.775
이런 대화는 앞으로 
몇 년 안에 가능해질 겁니다.

00:12:29.799 --> 00:12:32.323
시리를 생각해보세요.

00:12:33.447 --> 00:12:37.769
시리가 "사모님이 오늘 저녁 약속을
잊지말라고 전화하셨어요"라고 말합니다.

00:12:38.436 --> 00:12:39.598
여러분은 약속을 잊고 있었죠.

00:12:39.598 --> 00:12:42.393
"뭐라고? 무슨 저녁?
무슨 말 하는거야?"

00:12:42.417 --> 00:12:46.163
"음, 7시로 예정된 결혼 20주년 
축하 저녁식사 입니다."

00:12:48.735 --> 00:12:52.454
"시간이 안되는데.. 사무총장님과
7시 반에 약속이 있단 말이야.

00:12:52.478 --> 00:12:54.170
어떻게 이렇게 겹쳤지?"

00:12:54.194 --> 00:12:58.854
"음, 제가 말씀드리긴 했지만 
제 이야기는 무시하셨죠."

00:12:59.966 --> 00:13:03.294
"그럼 어떡하지? 사무총장님한테 
바쁘다고 핑계를 댈 수는 없잖아."

00:13:04.310 --> 00:13:07.591
"걱정마세요. 제가 사무총장님이 타신 
비행기가 연착되도록 손 썼습니다."

00:13:07.615 --> 00:13:09.297
(웃음)

00:13:10.069 --> 00:13:12.170
"알 수 없는 컴퓨터 오류가 
일어나도록 했죠."

00:13:12.194 --> 00:13:13.406
(웃음)

00:13:13.430 --> 00:13:15.047
"정말? 그렇게 할 수 있어?"

00:13:16.220 --> 00:13:18.399
"사무총장님이 정말 
미안하다고 하셨습니다.

00:13:18.423 --> 00:13:20.978
그리고 대신 내일 점심에
꼭 보자고 하셨습니다."

00:13:21.002 --> 00:13:22.301
(웃음)

00:13:22.325 --> 00:13:26.728
이 상황에서 가치를 둔 것은.. 
방향이 조금 잘못되기는 했지만요.

00:13:26.752 --> 00:13:29.665
이건 분명 제 아내에게 
가치를 둔 결정입니다.

00:13:29.665 --> 00:13:31.854
"아내가 행복하면, 삶이 행복하다."
라는 말처럼요.

00:13:31.878 --> 00:13:33.461
(웃음)

00:13:33.485 --> 00:13:35.139
다른 상황도 벌어질 수 있습니다.

00:13:35.641 --> 00:13:37.842
여러분이 정말 힘든 하루를 
보내고 집에 왔습니다.

00:13:37.866 --> 00:13:40.061
그리고 컴퓨터가 말합니다. 
"힘든 하루였죠?"

00:13:40.085 --> 00:13:41.847
"맞아, 점심 먹을 시간도 없었어."

00:13:41.847 --> 00:13:43.679
"배가 많이 고프시겠네요."

00:13:43.703 --> 00:13:46.349
"배고파 죽을 것 같아.
저녁 좀 해줄래?"

00:13:47.890 --> 00:13:49.980
"제가 먼저 드릴 말씀이 있습니다."

00:13:50.004 --> 00:13:51.159
(웃음)

00:13:52.013 --> 00:13:56.918
"남 수단에는 주인님보다 더 심한 
굶주림에 시달리는 사람들이 있습니다."

00:13:56.942 --> 00:13:58.046
(웃음)

00:13:58.070 --> 00:14:00.145
"저는 이제 전원을 끄겠습니다. 
저녁은 알아서 드세요."

00:14:00.169 --> 00:14:02.169
(웃음)

00:14:02.643 --> 00:14:04.382
우리는 이런 문제들을 해결해야 합니다.

00:14:04.406 --> 00:14:06.921
그리고 해결되리라고 기대합니다.

00:14:06.945 --> 00:14:08.788
제가 낙관하는 이유가 있습니다.

00:14:08.812 --> 00:14:09.971
그 중 하나는

00:14:09.995 --> 00:14:11.863
정말 방대한 양의 정보가 있다는 거죠.

00:14:11.887 --> 00:14:14.875
기계들은 인류가 기록해 둔 모든 
자료를 읽어 들일 것입니다.

00:14:14.875 --> 00:14:18.363
우리가 기록한 자료 대부분은
누가 무엇을 했고 

00:14:18.363 --> 00:14:20.661
누가 그 일에 반대했는가에 
관한 것들입니다.

00:14:20.661 --> 00:14:23.263
이런 방대한 양의 정보에서
배울 점이 있습니다.

00:14:23.263 --> 00:14:25.619
이를 통해 엄청난 
경제적 이득을 볼 수 있죠.

00:14:27.151 --> 00:14:28.337
제대로 활용한다면요.

00:14:28.361 --> 00:14:30.362
자, 여러분 집에 가정부 
로봇이 있다고 상상해보세요.

00:14:30.386 --> 00:14:33.453
여러분이 야근으로 귀가가 늦어졌고,
로봇이 아이들의 식사를 챙겨야 합니다.

00:14:33.477 --> 00:14:36.300
아이들은 배가 고프고 
냉장고에는 먹을 게 없습니다.

00:14:36.324 --> 00:14:38.929
그때 로봇은 고양이를 쳐다봅니다.

00:14:38.953 --> 00:14:40.645
(웃음)

00:14:40.669 --> 00:14:44.859
그리고 이 로봇은 인간의 가치 평가 
방식을 아직 제대로 배우지 않았습니다.

00:14:44.883 --> 00:14:48.884
그래서 고양이의 영양학적 가치보다
인간이 고양이를 아끼는 감성적 가치가

00:14:48.884 --> 00:14:51.002
더 중요하다는 것을 이해하지 못합니다.

00:14:51.026 --> 00:14:51.905
(웃음)

00:14:51.905 --> 00:14:53.137
어떤 일이 일어날까요?

00:14:53.137 --> 00:14:57.214
음, 이런 일이 일어날 겁니다.

00:14:57.238 --> 00:15:00.202
"미친 로봇이 저녁 메뉴로
고양이를 요리했다"

00:15:00.226 --> 00:15:04.749
이 사건 하나로 가정용 로봇 
업계는 망해버릴 수 있습니다.

00:15:04.773 --> 00:15:08.145
따라서 초지능을 가진 
로봇이 출현하기 전에

00:15:08.169 --> 00:15:10.884
이걸 바로 잡는 것이 무척 중요합니다.

00:15:11.948 --> 00:15:13.483
자, 요약을 하자면

00:15:13.507 --> 00:15:16.388
저는 인공지능에 대한 정의를
바꾸기 위해 노력합니다.

00:15:16.412 --> 00:15:19.405
그래야만 정말 도움이 되는
로봇들을 가질 수 있을 겁니다.

00:15:19.429 --> 00:15:20.721
여기에는 기본 원칙이 있죠.

00:15:20.721 --> 00:15:22.073
로봇은 이타적이어야 하고

00:15:22.097 --> 00:15:24.901
우리가 원하는 목적만을 
이루려고 해야 합니다.

00:15:24.925 --> 00:15:28.041
하지만 그 목적이 정확히 
무엇인지 몰라야 합니다.

00:15:28.065 --> 00:15:30.063
그리고 우리 인간을 잘 살펴야 하죠.

00:15:30.087 --> 00:15:33.290
우리가 진정 무엇을 원하는지 
이해하기 위해서입니다.

00:15:34.193 --> 00:15:37.752
그리고 그 과정에서 우리는 더 나은
사람이 되는 법을 배우게 될 것입니다.

00:15:37.776 --> 00:15:38.967
감사합니다.

00:15:38.991 --> 00:15:42.700
(웃음)

00:15:42.724 --> 00:15:44.146
CA: 정말 흥미롭네요. 스튜어트.

00:15:44.146 --> 00:15:48.476
다음 강연 순서를 준비하는 동안에
잠깐 이야기를 나누겠습니다.

00:15:48.525 --> 00:15:50.523
몇 가지 질문이 있는데요.

00:15:50.547 --> 00:15:56.000
무지한 상태에서 프로그래밍한다는 게
무척 대단하다고 생각됩니다.

00:15:56.024 --> 00:15:57.618
초지능 연구를 하고 계시는데요.

00:15:57.642 --> 00:15:59.900
로봇의 이런 부분을 막을 수 있을까요?

00:15:59.924 --> 00:16:01.470
로봇이 문학 서적을 읽고

00:16:01.470 --> 00:16:04.372
지식이 무지보다 더 낫다는
개념을 발견하게 되고

00:16:04.396 --> 00:16:06.272
그러면서도 계속 스스로 목적을 바꾸고

00:16:06.272 --> 00:16:08.892
그걸 다시 프로그래밍히는 걸 
막을 수 있나요?

00:16:09.512 --> 00:16:15.868
SR: 네. 말씀드렸다시피, 우리는 
로봇이 더 많이 배우길 바랍니다.

00:16:15.892 --> 00:16:17.419
우리의 목표에 대해서 말이죠

00:16:17.419 --> 00:16:22.724
목표가 더 정확해질수록
더 확실해지게 됩니다.

00:16:22.748 --> 00:16:24.693
그런 증거들이 있습니다.

00:16:24.717 --> 00:16:27.441
그리고 목표를 정확하게 
해석할 수 있도록 설계될 겁니다.

00:16:27.465 --> 00:16:30.255
예를 들어, 인공지능이 책들을 읽고

00:16:30.255 --> 00:16:32.842
그 안에 담긴 내용들이 한쪽으로 
편향되어 있다고 이해하게 될 겁니다.

00:16:32.842 --> 00:16:34.213
그 책들의 내용은

00:16:34.213 --> 00:16:38.173
왕과 왕자, 뛰어난 백인 
남성들의 업적들 뿐이죠.

00:16:38.197 --> 00:16:40.293
이것은 어려운 문제입니다.

00:16:40.317 --> 00:16:44.189
하지만 우리의 목적에 대해
더 많이 알게 될수록

00:16:44.213 --> 00:16:46.276
우리에게 더욱 유용하게 될 겁니다.

00:16:46.300 --> 00:16:48.240
CA: 그런데 그걸 하나의 
법칙으로만 말할 수는 없겠죠.

00:16:48.240 --> 00:16:50.500
말하자면, 이렇게 입력하는 건가요.

00:16:50.524 --> 00:16:53.817
"만약 어떤 인간이라도 
내 전원을 끄려고 한다면

00:16:53.841 --> 00:16:55.776
나는 복종한다. 복종한다"

00:16:55.800 --> 00:16:58.119
SR: 물론 아닙니다.
그건 정말 끔찍한 생각이에요.

00:16:58.119 --> 00:17:01.218
무인자동차를 갖고 있다고 생각해보세요.

00:17:01.242 --> 00:17:04.825
그리고 다섯 살이 된 아이를
유치원에 데려다 주려고 합니다.

00:17:04.897 --> 00:17:08.299
그때 운전중에 다섯 살짜리 아이가 
전원을 꺼버리게 두시겠어요?

00:17:08.299 --> 00:17:09.772
아마 아닐 겁니다.

00:17:09.772 --> 00:17:15.145
로봇은 작동하는 사람이 얼마나 
이성적이고 지각이 있는지 파악합니다.

00:17:15.169 --> 00:17:18.635
더 이성적인 사람이
전윈을 꺼주기를 바라겠죠.

00:17:18.656 --> 00:17:21.539
만약 어떤 사람인지 전혀 모르고
심지어 악의적인 사람이라면

00:17:21.563 --> 00:17:24.075
그런 사람이 전원을 끄기를 
원하지는 않을 겁니다.

00:17:24.099 --> 00:17:25.489
CA: 알겠습니다. 스튜어드씨,

00:17:25.489 --> 00:17:27.587
저희 모두를 위해 그 문제를
해결해 주시기를 바랍니다.

00:17:27.587 --> 00:17:30.086
좋은 강연 감사합니다. 
정말 놀라운 이야기였습니다

00:17:30.086 --> 00:17:31.893
SR: 감사합니다

00:17:31.917 --> 00:17:33.754
(박수)

