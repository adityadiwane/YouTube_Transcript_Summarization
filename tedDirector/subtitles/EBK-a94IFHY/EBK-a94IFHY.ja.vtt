WEBVTT
Kind: captions
Language: ja

00:00:00.000 --> 00:00:07.000
翻訳: Yasushi Aoki
校正: Yuko Yoshida

00:00:12.462 --> 00:00:13.978
これは李世ドルです

00:00:13.978 --> 00:00:17.879
李世ドルは 世界で最も強い
碁打ちの１人ですが

00:00:17.879 --> 00:00:19.674
シリコンバレーの
友人たちなら

00:00:19.674 --> 00:00:22.768
「なんてこった」と言う
瞬間を迎えています

00:00:22.768 --> 00:00:23.645
(笑)

00:00:23.669 --> 00:00:25.857
我々が予想していたよりも
ずっと早く

00:00:25.881 --> 00:00:29.177
AIが進歩していることに
気付いた瞬間です

00:00:29.814 --> 00:00:33.371
人間は碁盤上で機械に負けましたが 
実際の世の中ではどうでしょう？

00:00:33.371 --> 00:00:34.419
実際の世界は

00:00:34.419 --> 00:00:37.342
碁盤よりもずっと大きく 
ずっと複雑で

00:00:37.342 --> 00:00:39.065
ずっと見通し難いですが

00:00:39.065 --> 00:00:41.603
決定問題であることに
違いはありません

00:00:42.688 --> 00:00:44.093
到来しつつある

00:00:44.093 --> 00:00:46.908
テクノロジーのことを考えるなら —

00:00:46.908 --> 00:00:50.637
機械は 本当に理解して文を読めるようには
まだなっていないことに

00:00:50.637 --> 00:00:53.181
新井紀子氏が
触れていましたが

00:00:53.181 --> 00:00:55.067
それもやがて
できるようになるでしょう

00:00:55.067 --> 00:00:56.576
そして そうなったとき

00:00:56.576 --> 00:00:59.843
機械は人類がかつて書いた
すべてのものを

00:00:59.843 --> 00:01:02.679
速やかに読破することでしょう

00:01:03.670 --> 00:01:05.674
そうなると機械は

00:01:05.674 --> 00:01:07.218
碁において見せた

00:01:07.218 --> 00:01:10.152
人間より遠くまで
見通す力と合わせ

00:01:10.152 --> 00:01:12.846
より多くの情報に
触れられるようになることで

00:01:12.846 --> 00:01:17.397
実際の世の中でも 人間より優れた
判断ができるようになるでしょう

00:01:18.352 --> 00:01:20.628
それは良いこと
なのでしょうか？

00:01:21.648 --> 00:01:24.080
そうだと望みたいです

00:01:26.354 --> 00:01:29.749
我々の文明そのもの 
我々が価値を置くすべては

00:01:29.749 --> 00:01:32.001
我々の知性を
拠り所としています

00:01:32.001 --> 00:01:35.579
はるかに多くの知性が
使えるようになったなら

00:01:35.603 --> 00:01:39.285
人類に可能なことに
限界はないでしょう

00:01:40.405 --> 00:01:42.964
ある人々が言っているように

00:01:42.964 --> 00:01:46.310
これは人類史上最大の出来事に
なるかもしれません

00:01:48.345 --> 00:01:52.334
ではなぜ「AIは人類の終焉を
意味するかもしれない」などと

00:01:52.334 --> 00:01:54.344
言われているのでしょう？

00:01:55.118 --> 00:01:57.037
これは新しいこと
なのでしょうか？

00:01:57.037 --> 00:02:01.511
ただイーロン・マスクと ビル・ゲイツと
ホーキングが言っているだけなのか？

00:02:01.773 --> 00:02:05.035
違います
この考えは結構前からありました

00:02:05.059 --> 00:02:06.905
ここに ある人の
言葉があります

00:02:06.905 --> 00:02:09.489
「重大な瞬間にスイッチを切る
といったことによって

00:02:09.489 --> 00:02:14.057
機械を 従属的な位置に
保てたとしても —

00:02:14.057 --> 00:02:17.408
この “スイッチを切る” ことについては
後でまた戻ってきます —

00:02:17.408 --> 00:02:20.952
種としての我々は
謙虚に捉えるべきである」

00:02:21.936 --> 00:02:25.925
誰の言葉でしょう？ アラン・チューリングが
1951年に言ったことです

00:02:25.930 --> 00:02:29.363
ご存じのように チューリングは
コンピューター科学の父であり

00:02:29.363 --> 00:02:32.265
いろいろな意味で
AIの父でもあります

00:02:32.969 --> 00:02:34.941
この問題を考えてみると

00:02:34.965 --> 00:02:38.752
つまり自分の種よりも知的なものを
生み出してしまうという問題ですが

00:02:38.776 --> 00:02:41.728
これは「ゴリラの問題」と呼んでも
良いかもしれません

00:02:42.025 --> 00:02:45.915
なぜなら数百万年前に
ゴリラの祖先がそうしているからで

00:02:45.939 --> 00:02:48.184
ゴリラたちに
尋ねることができます

00:02:48.192 --> 00:02:49.832
「いいアイデアだったと思う？」

00:02:49.832 --> 00:02:53.466
ゴリラたちが いいアイデアだったのか
議論するために 集まっていますが

00:02:53.466 --> 00:02:55.550
しばらくして
出した結論は

00:02:55.550 --> 00:02:57.925
「あれは酷いアイデアだった」
というものです

00:02:57.925 --> 00:03:00.278
おかげで我々の種は
ひどい苦境に置かれていると

00:03:00.278 --> 00:03:04.621
彼らの目に実存的な悲哀を
見て取れるでしょう

00:03:04.645 --> 00:03:06.285
(笑)

00:03:06.309 --> 00:03:09.553
「自分の種より知的なものを
生み出すのは

00:03:09.553 --> 00:03:13.638
良い考えではないのでは？」
という不安な感覚があります

00:03:14.038 --> 00:03:16.039
それについて
何ができるのでしょう？

00:03:16.039 --> 00:03:20.484
AIの開発をやめてしまう以外
ないかもしれませんが

00:03:20.484 --> 00:03:22.728
AIのもたらす様々な利点や

00:03:22.728 --> 00:03:24.974
私自身AI研究者である
という理由によって

00:03:24.974 --> 00:03:27.169
私にはそういう選択肢は
ありません

00:03:27.169 --> 00:03:29.941
実際AIは続けたいと
思っています

00:03:30.245 --> 00:03:33.047
この問題をもう少し
明確にする必要があるでしょう

00:03:33.047 --> 00:03:34.508
正確に何が問題なのか？

00:03:34.532 --> 00:03:38.278
優れたAIが我々の破滅に繋がりうるのは
なぜなのか？

00:03:39.218 --> 00:03:41.046
ここにもう１つ
引用があります

00:03:41.755 --> 00:03:43.844
「機械に与える目的については

00:03:43.844 --> 00:03:47.912
それが本当に望むものだと
確信があるものにする必要がある」

00:03:48.102 --> 00:03:51.454
これはノーバート・ウィーナーが
1960年に言ったことで

00:03:51.454 --> 00:03:53.826
最初期の学習システムが

00:03:53.826 --> 00:03:58.233
作り手よりもうまくチェッカーを
指すのを見た すぐ後のことです

00:04:00.262 --> 00:04:03.105
しかしこれはミダス王の
言葉だったとしても

00:04:03.129 --> 00:04:04.603
おかしくないでしょう

00:04:04.603 --> 00:04:08.037
ミダス王は「自分の触れたものすべてが
金になってほしい」と望み

00:04:08.061 --> 00:04:10.534
そして その望みが
叶えられました

00:04:10.558 --> 00:04:11.863
これはいわば

00:04:11.863 --> 00:04:14.647
彼が「機械に与えた目的」です

00:04:14.647 --> 00:04:18.401
そして彼の食べ物や飲み物や親類は 
みんな金に変わってしまい

00:04:18.401 --> 00:04:21.206
彼は悲嘆と飢えの中で
死んでいきました

00:04:22.034 --> 00:04:26.505
だから自分が本当に望むことと合わない
目的を掲げることを

00:04:26.505 --> 00:04:28.343
「ミダス王の問題」と

00:04:28.343 --> 00:04:30.325
呼ぶことにしましょう

00:04:30.325 --> 00:04:34.467
現代的な用語では これを
「価値整合の問題」と言います

00:04:36.657 --> 00:04:40.512
間違った目的を与えてしまうというのが
問題のすべてではありません

00:04:40.512 --> 00:04:41.930
別の側面もあります

00:04:41.930 --> 00:04:43.923
「コーヒーを取ってくる」というような

00:04:43.947 --> 00:04:46.905
ごく単純な目的を
機械に与えたとします

00:04:47.658 --> 00:04:49.569
機械は考えます

00:04:50.213 --> 00:04:53.276
「コーヒーを取ってくるのに失敗する
どんな状況がありうるだろう？

00:04:53.276 --> 00:04:55.370
誰かが自分のスイッチを
切るかもしれない

00:04:55.395 --> 00:04:57.852
そのようなことを防止する
手を打たなければ

00:04:57.876 --> 00:05:00.332
自分の「オフ」スイッチを
無効にしておこう

00:05:00.354 --> 00:05:04.063
与えられた目的の遂行を阻むものから
自分を守るためであれば

00:05:04.063 --> 00:05:05.820
何だってやろう」

00:05:05.820 --> 00:05:07.833
１つの目的を

00:05:07.833 --> 00:05:11.502
非常に防御的に
一途に追求すると

00:05:11.502 --> 00:05:15.662
人類の本当の目的に
沿わなくなるというのが

00:05:15.662 --> 00:05:18.394
我々の直面する問題です

00:05:18.677 --> 00:05:23.478
実際それが この講演から学べる
価値ある教訓です

00:05:23.478 --> 00:05:25.767
もし１つだけ覚えておくとしたら
それは —

00:05:25.767 --> 00:05:28.442
「死んだらコーヒーを取ってこれない」
ということです

00:05:28.442 --> 00:05:29.457
(笑)

00:05:29.481 --> 00:05:33.310
簡単でしょう
記憶して１日３回唱えてください

00:05:33.334 --> 00:05:35.155
(笑)

00:05:35.179 --> 00:05:38.637
実際 映画『2001年宇宙の旅』の筋は

00:05:38.637 --> 00:05:40.965
そういうものでした

00:05:40.996 --> 00:05:43.136
HALの目的・ミッションは

00:05:43.160 --> 00:05:46.636
人間の目的とは合わず

00:05:46.636 --> 00:05:49.136
そのため衝突が起きます

00:05:49.164 --> 00:05:53.083
幸いHALは非常に賢くはあっても
超知的ではありませんでした

00:05:53.083 --> 00:05:55.818
それで最終的には
主人公が出し抜いて

00:05:55.818 --> 00:05:58.287
スイッチを切ることができました

00:06:01.388 --> 00:06:03.967
でも私たちはそんなに幸運では
ないかもしれません

00:06:07.873 --> 00:06:09.925
では どうしたらいいのでしょう？

00:06:12.191 --> 00:06:14.792
「知的に目的を追求する機械」という

00:06:14.816 --> 00:06:16.877
古典的な見方から離れて

00:06:16.901 --> 00:06:21.608
AIの再定義を試みようと思います

00:06:22.332 --> 00:06:24.134
３つの原則があります

00:06:24.134 --> 00:06:27.643
第１は「利他性の原則」で

00:06:27.667 --> 00:06:30.673
ロボットの唯一の目的は

00:06:30.673 --> 00:06:34.213
人間の目的
人間にとって価値あることが

00:06:34.213 --> 00:06:36.683
最大限に実現される
ようにすることです

00:06:36.683 --> 00:06:40.207
ここで言う価値は 
善人ぶった崇高そうな価値ではありません

00:06:40.207 --> 00:06:41.842
単に何であれ

00:06:41.842 --> 00:06:45.695
人間が自分の生活に
望むものということです

00:06:47.044 --> 00:06:48.117
この原則は

00:06:48.117 --> 00:06:51.846
「ロボットは自己を守らなければならない」
というアシモフの原則に反します

00:06:51.870 --> 00:06:55.833
自己の存在維持には
まったく関心を持たないのです

00:06:57.050 --> 00:07:01.368
第２の原則は 
言うなれば「謙虚の原則」です

00:07:01.624 --> 00:07:05.537
これはロボットを安全なものにする上で
非常に重要であることがわかります

00:07:05.561 --> 00:07:06.857
この原則は

00:07:06.857 --> 00:07:10.755
ロボットが人間の価値が何か
知らないものとしています

00:07:10.779 --> 00:07:14.537
ロボットは最大化すべきものが何か
知らないということです

00:07:14.884 --> 00:07:17.434
１つの目的を
一途に追求することの問題を

00:07:17.434 --> 00:07:19.056
これで避けることができます

00:07:19.056 --> 00:07:21.452
この不確定性が
極めて重要なのです

00:07:21.466 --> 00:07:23.365
人間にとって有用であるためには

00:07:23.365 --> 00:07:26.993
我々が何を望むのかについて
大まかな理解は必要です

00:07:26.993 --> 00:07:32.470
ロボットはその情報を主として
人間の選択を観察することで得ます

00:07:32.494 --> 00:07:35.495
我々が自分の生活に望むのが
何かという情報が

00:07:35.495 --> 00:07:39.029
我々のする選択を通して
明かされるわけです

00:07:39.992 --> 00:07:41.639
以上が３つの原則です

00:07:41.639 --> 00:07:45.267
これがチューリングの提起した
「機械のスイッチを切れるか」という問題に

00:07:45.267 --> 00:07:47.930
どう適用できるか
見てみましょう

00:07:48.723 --> 00:07:50.757
これは PR2 ロボットです

00:07:50.757 --> 00:07:52.858
私たちの研究室にあるもので

00:07:52.858 --> 00:07:56.001
背中に大きな赤い
「オフ」スイッチがあります

00:07:56.001 --> 00:07:58.934
問題は ロボットがスイッチを
切らせてくれるかということです

00:07:58.934 --> 00:08:00.519
古典的なやり方をするなら

00:08:00.519 --> 00:08:02.425
「コーヒーを取ってくる」
という目的に対し

00:08:02.425 --> 00:08:06.569
「コーヒーを取ってこなければならない」
「死んだらコーヒーを取ってこれない」と考え

00:08:06.569 --> 00:08:09.354
私の講演を聴いていたPR2は

00:08:09.354 --> 00:08:13.987
「オフ・スイッチは無効にしなければ」
と判断し

00:08:14.266 --> 00:08:16.654
「スターバックスで邪魔になる
他の客はみんな

00:08:16.654 --> 00:08:19.144
テーザー銃で眠らせよう」
となります

00:08:19.144 --> 00:08:21.084
(笑)

00:08:21.084 --> 00:08:23.211
これは避けがたい
ように見えます

00:08:23.211 --> 00:08:25.759
このような故障モードは
不可避に見え

00:08:25.783 --> 00:08:29.826
そしてそれは具体的で絶対的な
目的があることから来ています

00:08:30.232 --> 00:08:33.776
目的が何なのか機械に
確信がないとしたら どうなるでしょう？

00:08:33.800 --> 00:08:35.927
違ったように推論するはずです

00:08:35.951 --> 00:08:38.704
「人間は自分のスイッチを
切るかもしれないが

00:08:38.704 --> 00:08:41.360
それは自分が何か
悪いことをしたときだけだ

00:08:41.367 --> 00:08:44.006
悪いことが何か
よく分からないけど

00:08:44.006 --> 00:08:45.679
悪いことはしたくない」

00:08:45.679 --> 00:08:49.077
ここで 第１ および第２の原則が
効いています

00:08:49.077 --> 00:08:52.857
「だからスイッチを切るのを
人間に許すべきだ」

00:08:53.541 --> 00:08:56.871
実際ロボットが人間に
スイッチを切ることを許す

00:08:56.871 --> 00:08:59.908
インセンティブを
計算することができ

00:08:59.908 --> 00:09:02.412
それは目的の不確かさの度合いと

00:09:02.412 --> 00:09:04.872
直接的に結びついています

00:09:05.797 --> 00:09:08.400
機械のスイッチが切られると

00:09:08.400 --> 00:09:10.419
第３の原則が働いて

00:09:10.419 --> 00:09:13.605
追求すべき目的について
何かを学びます

00:09:13.605 --> 00:09:16.308
自分の間違った行いから
学ぶのです

00:09:16.308 --> 00:09:18.876
数学者がよくやるように

00:09:18.876 --> 00:09:21.591
ギリシャ文字をうまく使って

00:09:21.591 --> 00:09:25.425
そのようなロボットが
人間にとって有益であるという定理を

00:09:25.425 --> 00:09:27.466
証明することができます

00:09:27.466 --> 00:09:31.623
そのようにデザインされた機械の方が
そうでないものより良い結果になると

00:09:31.623 --> 00:09:33.029
証明可能なのです

00:09:33.057 --> 00:09:34.937
これは単純な例ですが

00:09:34.937 --> 00:09:39.890
人間互換のAIを手にするための
第一歩です

00:09:42.477 --> 00:09:45.518
３番目の原則については

00:09:45.518 --> 00:09:48.744
皆さん困惑しているのでは
と思います

00:09:48.744 --> 00:09:51.987
「自分の行動は
見上げたものではない

00:09:51.987 --> 00:09:54.960
ロボットに自分のように
振る舞って欲しくはない

00:09:54.960 --> 00:09:58.148
真夜中にこっそり台所に行って
冷蔵庫から食べ物を失敬したり

00:09:58.148 --> 00:10:00.266
あんなことや こんなことを
しているから」

00:10:00.266 --> 00:10:02.561
ロボットにしてほしくない
様々なことがあります

00:10:02.561 --> 00:10:04.766
でも実際そういう風に
働くわけではありません

00:10:04.766 --> 00:10:06.575
自分がまずい振る舞いをしたら

00:10:06.575 --> 00:10:09.262
ロボットがそれを真似する
というわけではありません

00:10:09.262 --> 00:10:11.656
人がそのようにする
動機を理解して

00:10:11.656 --> 00:10:15.076
誘惑に抵抗する手助けさえ
してくれるかもしれません

00:10:16.026 --> 00:10:17.840
それでも難しいです

00:10:18.002 --> 00:10:19.981
私たちがやろうとしているのは

00:10:19.981 --> 00:10:22.951
あらゆる状況にある

00:10:22.951 --> 00:10:25.436
あらゆる人のことを

00:10:25.436 --> 00:10:28.997
機械に予測させる
ということです

00:10:28.997 --> 00:10:32.124
その人たちは
どちらを好むのか？

00:10:33.701 --> 00:10:36.459
これには難しいことが
たくさんあって

00:10:36.459 --> 00:10:39.791
ごく速やかに解決されるだろうとは
思っていません

00:10:39.815 --> 00:10:42.888
本当に難しい部分は
私たちにあります

00:10:43.779 --> 00:10:47.086
言いましたように 私たちは
まずい振る舞いをします

00:10:47.110 --> 00:10:49.681
人によっては
悪質でさえあります

00:10:50.251 --> 00:10:53.227
しかしロボットは人間の振るまいを
真似する必要はありません

00:10:53.227 --> 00:10:56.338
ロボットは それ自身の目的
というのを持ちません

00:10:56.338 --> 00:10:58.229
純粋に利他的です

00:10:59.113 --> 00:11:04.108
そして１人の人間の望みだけ
満たそうとするのではなく

00:11:04.108 --> 00:11:08.076
みんなの好みに敬意を払うよう
デザインされています

00:11:08.903 --> 00:11:11.477
だからある程度
悪いことも扱え

00:11:11.477 --> 00:11:14.412
人間の悪い面も
理解できます

00:11:14.412 --> 00:11:18.037
例えば入国審査官が
賄賂を受け取っているけれど

00:11:18.037 --> 00:11:21.503
それは家族を食べさせ 
子供を学校に行かせるためなのだとか

00:11:21.503 --> 00:11:24.779
ロボットはそれを理解できますが 
そのために盗みをするわけではありません

00:11:24.779 --> 00:11:27.552
ただ子供が学校に行けるよう
手助けをするだけです

00:11:28.796 --> 00:11:31.808
また人間は計算能力の点で
限界があります

00:11:31.832 --> 00:11:34.337
李世ドルは
素晴らしい碁打ちですが

00:11:34.361 --> 00:11:35.686
それでも負けました

00:11:35.710 --> 00:11:39.893
彼の行動を見れば 勝負に負けることになる
手を打ったのが分かるでしょう

00:11:39.893 --> 00:11:42.714
しかしそれは 彼が負けを
望んだことを意味しません

00:11:43.160 --> 00:11:45.200
彼の行動を理解するためには

00:11:45.224 --> 00:11:48.868
人の認知モデルを
逆にたどる必要がありますが

00:11:48.892 --> 00:11:53.869
それは計算能力の限界も含む 
とても複雑なモデルです

00:11:53.893 --> 00:11:57.336
それでも私たちが理解すべく
取り組めるものではあります

00:11:57.696 --> 00:12:01.610
AI研究者として見たとき
最も難しいと思える部分は

00:12:01.610 --> 00:12:04.975
私たち人間が
沢山いるということです

00:12:06.064 --> 00:12:08.319
だから機械は
トレードオフを考え

00:12:08.319 --> 00:12:11.848
沢山の異なる人間の好みを
比較考量する必要があり

00:12:11.848 --> 00:12:13.874
それには いろいろな
やり方があります

00:12:13.898 --> 00:12:17.587
経済学者 社会学者 倫理学者は
そういうことを分かっており

00:12:17.611 --> 00:12:20.066
私たちは協同の道を
探っています

00:12:20.090 --> 00:12:23.341
そこをうまくやらないと
どうなるか見てみましょう

00:12:23.365 --> 00:12:25.498
たとえばこんな会話を
考えてみます

00:12:25.522 --> 00:12:27.120
知的な秘書AIが

00:12:27.120 --> 00:12:29.775
数年内に利用可能に
なるかもしれません

00:12:29.799 --> 00:12:32.583
強化されたSiriのようなものです

00:12:33.347 --> 00:12:38.129
Siriが「今晩のディナーについて
奥様から確認の電話がありました」と言います

00:12:38.146 --> 00:12:40.944
あなたはもちろん忘れています
「何のディナーだって？

00:12:40.968 --> 00:12:42.553
何の話をしているんだ？」

00:12:42.553 --> 00:12:46.353
「20周年のディナーですよ
夜７時の」

00:12:48.565 --> 00:12:52.258
「無理だよ　７時半に
事務総長と会わなきゃならない

00:12:52.258 --> 00:12:54.170
どうして こんなことに
なったんだ？」

00:12:54.170 --> 00:12:59.260
「警告は致しましたが 
あなたは推奨案を無視されました」

00:12:59.796 --> 00:13:03.644
「どうしたらいいんだ？ 
忙しくて行けないなんて言えないぞ」

00:13:03.960 --> 00:13:08.071
「ご心配には及びません
事務総長の飛行機が遅れるように手配済みです」

00:13:08.071 --> 00:13:09.507
(笑)

00:13:09.949 --> 00:13:12.170
「コンピューターに
細工しておきました」

00:13:12.194 --> 00:13:13.370
(笑)

00:13:13.370 --> 00:13:15.647
「えっ そんなことできるのか？」

00:13:16.220 --> 00:13:17.413
「大変恐縮して

00:13:17.413 --> 00:13:20.978
明日のランチでお会いするのを
楽しみにしている とのことです」

00:13:21.002 --> 00:13:22.301
(笑)

00:13:22.325 --> 00:13:26.728
ここでは価値について
ちょっと行き違いが起きています

00:13:26.752 --> 00:13:29.695
Siri は明らかに
妻の価値観に従っています

00:13:29.695 --> 00:13:32.024
「妻の幸せが 夫の幸せ」です

00:13:32.024 --> 00:13:33.345
(笑)

00:13:33.345 --> 00:13:35.181
別の方向に行くことも
あり得ます

00:13:35.181 --> 00:13:38.332
忙しい仕事を終え 帰宅すると 
コンピューターが言います

00:13:38.332 --> 00:13:40.061
「大変な１日だったようですね」

00:13:40.085 --> 00:13:42.047
「昼を食べる時間もなかったよ」

00:13:42.047 --> 00:13:43.679
「お腹が空いたことでしょう」

00:13:43.703 --> 00:13:46.739
「ああ 腹ペコだよ
何か夕食を作ってもらえるかな？」

00:13:47.470 --> 00:13:50.410
「そのことで お話ししなければ
ならないことがあります」

00:13:50.410 --> 00:13:51.579
(笑)

00:13:52.013 --> 00:13:56.732
「南スーダンには あなたよりも
必要に迫られている人々がいます」

00:13:56.732 --> 00:13:57.820
(笑)

00:13:57.820 --> 00:14:00.805
「行くことに致しましたので 
夕食はご自分で作ってください」

00:14:00.805 --> 00:14:02.169
(笑)

00:14:02.443 --> 00:14:04.482
こういった問題を
解かなければなりません

00:14:04.482 --> 00:14:06.921
そういう問題に取り組むのは
楽しみです

00:14:06.945 --> 00:14:08.788
楽観しているのには
理由があります

00:14:08.812 --> 00:14:09.971
１つには

00:14:09.995 --> 00:14:11.863
膨大なデータがあること

00:14:11.887 --> 00:14:13.125
思い出してください

00:14:13.125 --> 00:14:15.955
機械は人類が書いたあらゆるものを
読むことになるでしょう

00:14:15.955 --> 00:14:17.833
人間の書いたものはたいがい

00:14:17.833 --> 00:14:20.937
誰かが何かをし 
他の人がそれに腹を立てたというものです

00:14:20.961 --> 00:14:23.449
学べるデータが膨大にあります

00:14:23.449 --> 00:14:25.341
また これを正しくやるための

00:14:25.341 --> 00:14:28.021
強い経済的インセンティブが
存在します

00:14:28.021 --> 00:14:30.112
家に家事ロボットがいると
想像してください

00:14:30.112 --> 00:14:33.773
あなたはまた仕事で帰りが遅く 
ロボットは子供達に食べさせなければなりません

00:14:33.773 --> 00:14:36.490
子供達はお腹を空かせていますが 
冷蔵庫は空っぽです

00:14:36.490 --> 00:14:38.999
そこでロボットは
猫に目を止めます

00:14:38.999 --> 00:14:40.645
(笑)

00:14:40.669 --> 00:14:44.859
ロボットは人間の価値観を
ちゃんと学んでいないため

00:14:44.883 --> 00:14:47.244
猫の持つ感情的価値が

00:14:47.244 --> 00:14:50.406
猫の栄養的価値を上回ることを
理解しません

00:14:50.406 --> 00:14:51.305
(笑)

00:14:51.305 --> 00:14:52.977
するとどうなるでしょう？

00:14:52.977 --> 00:14:57.574
「狂ったロボット 
子猫を料理して夕食に出す」

00:14:57.574 --> 00:15:00.226
みたいな見出しを
見ることになります

00:15:00.226 --> 00:15:04.563
このような出来事１つで
家事ロボット産業はお終いです

00:15:04.563 --> 00:15:07.569
だから超知的な機械に到達する
ずっと以前に

00:15:07.569 --> 00:15:11.494
この問題を正すよう
大きなインセンティブが働きます

00:15:11.948 --> 00:15:13.483
要約すると

00:15:13.507 --> 00:15:15.702
私はAIの定義を変えて

00:15:15.702 --> 00:15:19.405
人間のためになると証明可能な機械が
得られるよう試みています

00:15:19.429 --> 00:15:20.635
その原則は

00:15:20.635 --> 00:15:22.073
機械は利他的であり

00:15:22.097 --> 00:15:24.845
人間の目的のみを
達成しようとするが

00:15:24.845 --> 00:15:27.945
その目的が何かは
確信を持たず

00:15:27.945 --> 00:15:30.253
そしてすべての人間を
観察することで

00:15:30.253 --> 00:15:33.840
我々の本当に望むことが何かを学ぶ
ということです

00:15:33.853 --> 00:15:37.676
その過程で 人類がより良い者になる術を
学ぶことを望みます

00:15:37.676 --> 00:15:39.117
ありがとうございました

00:15:39.117 --> 00:15:42.524
(拍手)

00:15:42.524 --> 00:15:44.452
(クリス・アンダーソン) すごく興味深いね 
スチュワート

00:15:44.452 --> 00:15:46.970
次のスピーカーのための
準備があるので

00:15:46.970 --> 00:15:48.665
少しここで話しましょう

00:15:48.665 --> 00:15:50.523
質問があるんですが

00:15:50.547 --> 00:15:55.964
「無知にプログラムする」というアイデアは
とても強力であるように思えます

00:15:55.964 --> 00:15:58.158
超知的になったロボットが
文献を読んで

00:15:58.158 --> 00:16:01.170
無知よりも知識がある方が
良いと気付き

00:16:01.170 --> 00:16:03.896
自分の目的を変えて
プログラムを書き換えてしまう —

00:16:03.896 --> 00:16:06.182
そういうことに
ならないためには

00:16:06.182 --> 00:16:08.614
どうすれば
良いのでしょう？

00:16:09.432 --> 00:16:12.062
(スチュワート・ラッセル) 私たちはロボットに

00:16:12.062 --> 00:16:17.179
人間の目的をよく学んで
ほしいと思っています

00:16:17.203 --> 00:16:22.258
ロボットは より正しくなるほど
確信を強めます

00:16:22.258 --> 00:16:24.693
手がかりはそこに
あるわけですから

00:16:24.693 --> 00:16:27.501
それを正しく解釈するよう
デザインするのです

00:16:27.501 --> 00:16:29.955
たとえば本の内容には

00:16:29.955 --> 00:16:32.902
バイアスがあることを
理解するでしょう

00:16:32.902 --> 00:16:36.649
王や王女や
エリートの白人男性がしたことばかり

00:16:36.649 --> 00:16:38.173
書かれているといった風に

00:16:38.197 --> 00:16:40.293
だから複雑な問題ではありますが

00:16:40.317 --> 00:16:44.013
ロボットが我々の目的を
学べは学ぶほど

00:16:44.013 --> 00:16:46.276
我々にとって
有用なものになるでしょう

00:16:46.300 --> 00:16:48.826
(クリス) １つの原則に
まとめられないんですか？

00:16:48.850 --> 00:16:50.500
固定したプログラムとして

00:16:50.524 --> 00:16:53.817
「人間がスイッチを切ろうとしたら

00:16:53.841 --> 00:16:55.700
無条件に従う」みたいな

00:16:55.700 --> 00:16:57.062
(スチュワート) それは駄目ですね

00:16:57.062 --> 00:16:58.469
まずいアイデアです

00:16:58.469 --> 00:17:00.278
自動運転車で

00:17:00.278 --> 00:17:03.419
５歳の子を幼稚園に
送るところを

00:17:03.419 --> 00:17:04.657
考えてみてください

00:17:04.657 --> 00:17:06.292
車に１人で乗っている
５歳児が

00:17:06.292 --> 00:17:08.769
車のスイッチを切れるように
したいと思いますか？

00:17:08.769 --> 00:17:09.612
違うでしょう

00:17:09.612 --> 00:17:14.789
ロボットは その人間がどれほど理性的で
分別があるかを理解する必要があります

00:17:14.789 --> 00:17:16.759
人間が理性的であるほど

00:17:16.759 --> 00:17:18.785
スイッチを切らせる見込みは
高くなります

00:17:18.785 --> 00:17:21.539
まったくランダムな相手や 
悪意ある人間に対しては

00:17:21.562 --> 00:17:24.075
なかなかスイッチを切らせようとは
しないでしょう

00:17:24.099 --> 00:17:25.319
(クリス) スチュワート

00:17:25.319 --> 00:17:28.247
あなたが みんなのために
この問題を解決してくれることを切に望みます

00:17:28.247 --> 00:17:29.976
ありがとうございました
素晴らしいお話でした

00:17:29.976 --> 00:17:30.927
(スチュワート) どうもありがとう

00:17:30.927 --> 00:17:33.754
(拍手)

