WEBVTT
Kind: captions
Language: hu

00:00:00.000 --> 00:00:07.000
Fordító: Krisztian Rassay
Lektor: Tímea Hegyessy

00:00:12.532 --> 00:00:14.084
Ő Lee Sedol,

00:00:14.108 --> 00:00:17.335
a világ egyik legjobb Go-játékosa,

00:00:17.629 --> 00:00:21.014
aki itt épp arra gondol, amit
Szilícium-völgyi barátaim csak úgy hívnak:

00:00:21.038 --> 00:00:22.548
"azt a leborult szivarvégit!".

00:00:22.572 --> 00:00:23.645
(Nevetés)

00:00:23.669 --> 00:00:25.857
A pillanat, amikor rádöbbenünk,

00:00:25.881 --> 00:00:29.177
hogy a MI sokkal gyorsabban fejlődik,
mint amire számítottunk.

00:00:29.974 --> 00:00:33.021
A Go táblán az ember alul maradt.
De mi lesz a való életben?

00:00:33.045 --> 00:00:35.145
A való világ sokkal nagyobb,

00:00:35.169 --> 00:00:37.418
sokszorta összetettebb, mint a Go.

00:00:37.442 --> 00:00:39.261
Nem annyira látható,

00:00:39.285 --> 00:00:41.323
de az is döntési probléma.

00:00:42.768 --> 00:00:45.089
Ha néhány új technológiára gondolunk,

00:00:45.113 --> 00:00:46.862
melyek szárnyaikat bontogatják...

00:00:47.558 --> 00:00:51.893
Noriko [Arai] megemlítette,
hogy a gépek még nem olvasnak,

00:00:51.917 --> 00:00:53.417
legalábbis még nem értik.

00:00:53.441 --> 00:00:54.977
De ez is el fog jönni,

00:00:55.001 --> 00:00:56.772
és amikor bekövetkezik,

00:00:56.796 --> 00:00:57.983
onnan már nem kell sok,

00:00:58.007 --> 00:01:02.579
hogy elolvassanak mindent,
amit az emberi faj valaha leírt.

00:01:03.670 --> 00:01:05.700
Ez képessé fogja tenni őket,

00:01:05.724 --> 00:01:08.644
hogy távolabbra lássanak,
mint amire az ember képes,

00:01:08.668 --> 00:01:10.348
ahogy a Go-ban már láttuk.

00:01:10.372 --> 00:01:12.536
Ha még több információhoz is hozzáférnek,

00:01:12.560 --> 00:01:16.828
képesek lesznek a való világban is
jobb döntéseket hozni nálunk.

00:01:18.612 --> 00:01:20.218
Vajon ez jó hír?

00:01:21.718 --> 00:01:23.950
Remélem.

00:01:26.514 --> 00:01:29.769
A teljes civilizációnk, minden,
amit számunkra értéket képvisel,

00:01:29.793 --> 00:01:31.861
az intelligenciánkon alapul.

00:01:31.885 --> 00:01:35.579
Ha sokkal több intelligenciához
férnénk hozzá,

00:01:35.603 --> 00:01:38.905
határtalan lehetőségek nyílnának meg
az emberi faj előtt.

00:01:40.485 --> 00:01:43.810
Azt hiszem, ez lehet -
ahogy néhányan megfogalmazták -

00:01:43.834 --> 00:01:45.850
az emberi történelem legnagyobb eseménye.

00:01:48.485 --> 00:01:51.314
Mégis miért hangzanak el akkor olyanok,

00:01:51.338 --> 00:01:54.214
hogy a MI az emberi faj végét jelentheti?

00:01:55.258 --> 00:01:56.917
Ez valami új dolog?

00:01:56.941 --> 00:02:01.051
Csak Elon Musk, Bill Gates
és Stephen Hawking gondolja így?

00:02:01.773 --> 00:02:05.035
Valójában nem. 
Ez már régebb óta kísértő elgondolás.

00:02:05.059 --> 00:02:07.021
Nézzük ezt az idézetet:

00:02:07.045 --> 00:02:11.395
"Még ha képesek is lennénk a gépeket
alárendelt szolgai szerepben tartani,

00:02:11.419 --> 00:02:14.403
például úgy, hogy kritikus helyzetekben
kikapcsolnánk őket," -

00:02:14.427 --> 00:02:17.664
később még visszatérek
erre a kikapcsolás-ötletre -

00:02:17.688 --> 00:02:20.492
"mi, mint élőlények, nagyon megalázva
éreznénk magunkat."

00:02:21.997 --> 00:02:25.445
Ki mondta ezt?
Alan Turing 1951-ben.

00:02:26.120 --> 00:02:28.883
Alarn Turingról tudjuk,
hogy a számítástudomány atyja,

00:02:28.907 --> 00:02:31.955
és sok tekintetben a MI atyja is.

00:02:33.059 --> 00:02:34.941
Ezt a problémakört,

00:02:34.965 --> 00:02:38.752
amikor saját fajunknál intelligensebb
teremtményeket hozunk létre,

00:02:38.776 --> 00:02:41.398
hívhatjuk a "gorilla-problémának",

00:02:42.165 --> 00:02:45.915
mivel gorilla őseink épp ezt csinálták,
néhány millió évvel ezelőtt,

00:02:45.939 --> 00:02:47.794
így most megkérdezhetjük a gorillákat:

00:02:48.572 --> 00:02:49.732
Jó ötlet volt?

00:02:49.756 --> 00:02:53.286
Itt éppen arról értekeznek,
hogy jó ötlet volt-e,

00:02:53.310 --> 00:02:56.656
és kisvártatva arra jutnak, hogy nem,

00:02:56.680 --> 00:02:58.025
ez szörnyű ötlet volt.

00:02:58.049 --> 00:02:59.831
Fajunk a kihalás szélén áll.

00:03:00.358 --> 00:03:04.621
Valójában a lét szomorúsága
tükröződik a szemükben.

00:03:04.645 --> 00:03:06.285
(Nevetés)

00:03:06.309 --> 00:03:11.149
A saját fajunknál valami
okosabbat létrehozni,

00:03:11.173 --> 00:03:13.538
talán nem is jó ötlet,
ami felkavaró érzés -

00:03:14.308 --> 00:03:15.799
mégis hogyan kerülhetjük el?

00:03:15.823 --> 00:03:20.590
Tulajdonképpen sehogy,
kivéve, ha leállítjuk a MI kutatást,

00:03:20.614 --> 00:03:23.124
de a sok előny, amit említettem,

00:03:23.148 --> 00:03:24.864
és mert én MI-kutató vagyok,

00:03:24.888 --> 00:03:26.679
nem állítanám le.

00:03:27.103 --> 00:03:29.571
Igazából folytatni szeretném 
a MI fejlesztést.

00:03:30.435 --> 00:03:33.113
Egy kissé pontosítanunk kell a problémát.

00:03:33.137 --> 00:03:34.508
Mi is igazából a probléma?

00:03:34.532 --> 00:03:37.778
Miért jelenthet katasztrófát egy jobb MI?

00:03:39.218 --> 00:03:40.716
Vegyünk egy másik idézetet:

00:03:41.755 --> 00:03:45.090
"Jó lesz nagyon odafigyelnünk rá,
hogy a gépbe táplált cél

00:03:45.114 --> 00:03:47.412
valóban az a cél, amit tényleg akarunk."

00:03:48.102 --> 00:03:51.600
Ezt Norbert Wiener mondta 1960-ban,

00:03:51.624 --> 00:03:55.626
közvetlen azután, hogy látott 
egy nagyon korai tanuló rendszert,

00:03:55.650 --> 00:03:58.233
amelyik az alkotójánál
jobb volt a dámajátékban.

00:04:00.422 --> 00:04:03.105
De ugyanezt elmondhatta volna

00:04:03.129 --> 00:04:04.296
Midász király is.

00:04:04.853 --> 00:04:08.037
Így szólt: "Azt akarom, változzon 
minden arannyá, amit megérintek",

00:04:08.061 --> 00:04:10.534
és pontosan azt kapta, amit kért.

00:04:10.558 --> 00:04:13.309
Ez volt a cél, amit betáplált a gépbe,

00:04:13.333 --> 00:04:14.783
mondhatjuk így is,

00:04:14.807 --> 00:04:18.251
így aztán az étele, az itala,
és a rokonai is arannyá változtak,

00:04:18.275 --> 00:04:20.556
ő pedig nyomorúságos éhhalált halt.

00:04:22.264 --> 00:04:24.605
Szóval ezt hívjuk 
"Midász király problémának",

00:04:24.629 --> 00:04:27.934
amikor olyan célt nevezünk meg,
ami valójában

00:04:27.958 --> 00:04:30.371
nem igazodik jól szándékainkhoz.

00:04:30.395 --> 00:04:33.648
Mai kifejezéssel ezt 
"érték-illesztési problémának" nevezzük.

00:04:36.867 --> 00:04:40.352
A probléma nem csak a rossz cél
betáplálását jelenti.

00:04:40.376 --> 00:04:41.528
Van egy másik oldala is.

00:04:41.980 --> 00:04:43.923
Ha egy célt előírunk egy gépnek,

00:04:43.947 --> 00:04:46.395
akár olyan egyszerűt is, mint:
"Hozd ide a kávét",

00:04:47.728 --> 00:04:49.569
a gép azt mondja magának:

00:04:50.553 --> 00:04:53.176
"Miképp vallhatok kudarcot,
a kávé oda vitelében?"

00:04:53.200 --> 00:04:54.780
Például valaki közben kikapcsol.

00:04:55.465 --> 00:04:57.852
Rendben, akkor ezt meg kell akadályoznom.

00:04:57.876 --> 00:04:59.782
Le fogom tiltani a kikapcsológombomat.

00:05:00.354 --> 00:05:03.313
Mindent megteszek
az akadályok elhárításáért

00:05:03.337 --> 00:05:05.966
a cél érdekében, amit feladatul kaptam."

00:05:05.990 --> 00:05:08.002
Szóval ez az együgyű törekvés

00:05:09.033 --> 00:05:11.978
ilyen önvédelmező módon
olyan cél felé viszi,

00:05:12.002 --> 00:05:15.036
ami igazából nem illeszkedik jól
az emberi faj valós céljaihoz -

00:05:15.942 --> 00:05:17.804
ez a probléma, amivel szembesülünk.

00:05:18.827 --> 00:05:23.594
Valójában ez a nagyon értékes
útravaló üzenete ennek az előadásnak.

00:05:23.618 --> 00:05:25.673
Ha csak egy dologra szeretnének emlékezni,

00:05:25.697 --> 00:05:28.372
ez legyen: nem tudod felszolgálni a kávét,
ha meghaltál.

00:05:28.396 --> 00:05:29.457
(Nevetés)

00:05:29.481 --> 00:05:33.310
Roppant egyszerű. Csak erre emlékezzenek!
Ismételjék el naponta háromszor!

00:05:33.334 --> 00:05:35.155
(Nevetés)

00:05:35.179 --> 00:05:37.933
Tulajdonképpen erről szól

00:05:37.957 --> 00:05:40.605
a "2001: Űrodüsszeia".

00:05:41.046 --> 00:05:43.136
HAL-nak van egy célja, egy küldetése,

00:05:43.160 --> 00:05:46.892
mely nincs az emberi igényekhez igazítva,

00:05:46.916 --> 00:05:48.726
és ez nézeteltérésekhez vezet.

00:05:49.314 --> 00:05:52.283
De szerencsére HAL nem szuperintelligens.

00:05:52.307 --> 00:05:55.894
Meglehetősen okos,
de végül Dave túljár az eszén,

00:05:55.918 --> 00:05:57.767
és sikerül kikapcsolnia.

00:06:01.648 --> 00:06:04.077
De lehet, hogy mi nem leszünk
ilyen szerencsések.

00:06:08.013 --> 00:06:09.605
Akkor mit tegyünk majd?

00:06:12.191 --> 00:06:14.486
Megpróbálom újraértelmezni az MI-t,

00:06:14.486 --> 00:06:16.877
hogy eltávolodjunk ettől
a klasszikus szemlélettől,

00:06:16.901 --> 00:06:21.468
mely szerint az intelligens gépek
célokért küzdenek.

00:06:22.532 --> 00:06:24.330
Három alapelvről van szó.

00:06:24.354 --> 00:06:27.643
Az első az altruizmus alapelve,
ha úgy tetszik:

00:06:27.667 --> 00:06:30.929
a robot egyetlen célja,

00:06:30.953 --> 00:06:35.199
hogy végsőkig segítse
az emberi értékekhez igazodó

00:06:35.223 --> 00:06:36.613
emberi célok megvalósulását.

00:06:36.637 --> 00:06:39.967
Értékek alatt nem mézes-mázas,
szuper jó értékekre gondolok.

00:06:39.991 --> 00:06:43.778
Hanem arra, amilyennek az emberek

00:06:43.802 --> 00:06:45.145
az életüket szeretnék.

00:06:47.184 --> 00:06:49.493
Ez valójában megsérti Asimov törvényét,

00:06:49.517 --> 00:06:51.846
hogy a robotnak meg kell védenie
saját magát.

00:06:51.870 --> 00:06:55.593
Semmiféle önmegóvási célja nincs.

00:06:57.240 --> 00:07:01.008
A második törvény, ha úgy tetszik,
az alázatosság törvénye.

00:07:01.794 --> 00:07:05.537
Úgy tűnik, ez rendkívül fontos,
hogy a robotokat biztonságossá tegyük.

00:07:05.561 --> 00:07:08.703
Azt mondja ki, hogy a robot nem ismeri

00:07:08.727 --> 00:07:10.755
az emberi értékeket,

00:07:10.779 --> 00:07:13.957
maximalizálni igyekszik,
ám mégsem tudja, mik azok.

00:07:15.074 --> 00:07:17.700
Ez elkerüli az együgyű célratörő

00:07:17.724 --> 00:07:18.936
magatartást.

00:07:18.960 --> 00:07:21.132
Ez a bizonytalanság kulcsfontosságú.

00:07:21.546 --> 00:07:23.185
Hogy ennek hasznát vegyük,

00:07:23.209 --> 00:07:25.940
némi elképzelésének kell lennie,
hogy mit is akarunk.

00:07:27.043 --> 00:07:32.470
Ehhez az információt az emberi döntések
megfigyeléséből szerzi,

00:07:32.494 --> 00:07:35.295
mert a döntéseink árulkodnak arról,

00:07:35.319 --> 00:07:38.619
hogy milyenné szeretnénk tenni
az életünket.

00:07:40.452 --> 00:07:42.149
Ez a három alapelv.

00:07:42.149 --> 00:07:44.547
Nézzük, hogyan alkalmazhatóak
a következő kérdésre:

00:07:44.557 --> 00:07:47.290
"Ki tudod kapcsolni a gépet?" -
ahogy Turing javasolta.

00:07:48.893 --> 00:07:51.013
Tehát itt egy PR2-es robot.

00:07:51.037 --> 00:07:52.858
Ez van nálunk a laborban,

00:07:52.882 --> 00:07:55.785
és van egy nagy,
piros "off" kapcsoló a hátán.

00:07:56.361 --> 00:07:58.976
A kérdés: 
engedni fogja, hogy kikapcsoljuk?

00:07:59.000 --> 00:08:00.465
Ha hagyományosan csináljuk,

00:08:00.489 --> 00:08:03.775
azt a parancsot adjuk neki,
hogy: "Hozd ide a kávét!",

00:08:03.775 --> 00:08:06.575
"Hoznom kell a kávét,
nem tudom hozni, ha halott vagyok" -

00:08:06.599 --> 00:08:09.940
tehát a PR2 nyilván hallgat rám,

00:08:09.964 --> 00:08:13.717
ezért azt mondja:
"Le kell tiltanom az 'off' gombom,

00:08:14.766 --> 00:08:17.490
és jobb lenne ártalmatlanítani
mindenkit a Starbuksnál is,

00:08:17.514 --> 00:08:19.074
aki utamat állhatja."

00:08:19.098 --> 00:08:21.160
(Nevetés)

00:08:21.184 --> 00:08:23.337
Ez tehát elkerülhetetlennek tűnik, igaz?

00:08:23.361 --> 00:08:25.759
Ez a zátonyra futás
elkerülhetetlennek látszik,

00:08:25.783 --> 00:08:29.326
és ez egyszerűen a konkrét, 
meghatározott célból következik.

00:08:30.632 --> 00:08:33.776
Mi történik, ha a gép 
nem biztos a céljában?

00:08:33.800 --> 00:08:35.927
Akkor másképp okoskodik.

00:08:35.951 --> 00:08:38.375
Azt mondja magában: 
"Talán kikapcsol az ember,

00:08:38.964 --> 00:08:40.920
de csak akkor, ha valami rosszat teszek.

00:08:41.567 --> 00:08:44.042
Nem igazán tudom, mi rossz,

00:08:44.066 --> 00:08:46.110
de azt tudom, 
hogy olyat nem akarok tenni."

00:08:46.134 --> 00:08:49.144
Ez volt eddig az első 
és a második alapelv.

00:08:49.168 --> 00:08:52.527
"Engednem kell tehát, 
hogy az ember kikapcsoljon."

00:08:53.541 --> 00:08:57.497
Tényleg ki lehet számolni
a robotnak szükséges ösztönzést,

00:08:57.521 --> 00:08:59.928
hogy megengedje a kikapcsolását,

00:08:59.928 --> 00:09:01.952
és ez közvetlenül
azzal van összefüggésben,

00:09:01.976 --> 00:09:04.722
hogy az elérni kívánt cél
mennyire bizonytalan.

00:09:05.797 --> 00:09:08.746
Itt jön képbe a harmadik alapelv,

00:09:08.770 --> 00:09:10.575
amikor a gép ki van kapcsolva.

00:09:10.599 --> 00:09:13.661
Megtanul valami újat a célokról,
amikért küzd,

00:09:13.685 --> 00:09:16.218
mivel rájön, hogy amit tett,
nem volt helyes.

00:09:16.242 --> 00:09:19.812
Ténylegesen képesek vagyunk ezt levezetni
egy halom görög betűvel.

00:09:19.836 --> 00:09:21.967
Ahogy a matematikusoknál ez szokás:

00:09:21.991 --> 00:09:23.975
be tudjuk bizonyítani a tételt,

00:09:23.999 --> 00:09:27.552
mely szerint egy ilyen robot
garantáltan hasznos az ember számára.

00:09:27.576 --> 00:09:31.379
Jobban járunk egy olyan géppel,
amit így terveztek,

00:09:31.403 --> 00:09:32.649
mint nélküle.

00:09:33.057 --> 00:09:35.963
Szóval ez egy egyszerű példa volt,
de ez az első lépés abban,

00:09:35.987 --> 00:09:39.890
amin dolgozunk,
hogy ember-kompatibilis MI-t alkossunk.

00:09:42.477 --> 00:09:45.734
Ez a harmadik alapelv...

00:09:45.758 --> 00:09:48.870
Talán már többen merengenek rajta

00:09:48.894 --> 00:09:52.133
ezen gondolatmenettel:
"Néha én nem viselkedek túl jól,

00:09:52.157 --> 00:09:55.086
nem akarom, hogy a robot
úgy viselkedjen, mint én.

00:09:55.110 --> 00:09:58.544
Éjjel lesettenkedek és kifosztom a hűtőt.

00:09:58.568 --> 00:09:59.736
Csinálok még ezt azt."

00:09:59.760 --> 00:10:02.557
Sok mindent nem szeretnénk, 
hogy a robot csináljon.

00:10:02.581 --> 00:10:04.652
Szerencsére azért ez nem így működik.

00:10:04.676 --> 00:10:06.831
Csak mert rosszul viselkedünk,

00:10:06.855 --> 00:10:09.478
a robot nem fog leutánozni.

00:10:09.502 --> 00:10:13.412
Meg fogja érteni a motivációnk,
és talán segít ellenállni a kísértésnek,

00:10:13.436 --> 00:10:14.756
alkalomadtán.

00:10:16.026 --> 00:10:17.490
Azért ez még így is nehéz.

00:10:18.122 --> 00:10:20.667
Azon dolgozunk,

00:10:20.691 --> 00:10:25.871
hogy a gépek képessé váljanak arra,
hogy bárkinek

00:10:25.871 --> 00:10:27.672
bárrmilyen életkörülményei is vannak,

00:10:27.696 --> 00:10:29.293
és egyáltalán mindenki esetén

00:10:29.317 --> 00:10:31.834
meg tudják jósolni, hogy ő mit szeretne.

00:10:33.881 --> 00:10:36.835
Ebben azonban nagyon sok a nehézség.

00:10:36.859 --> 00:10:39.791
Nem számítok rá,
hogy ez egyhamar megoldódik.

00:10:39.815 --> 00:10:42.458
A fő nehézség éppen mi magunk vagyunk.

00:10:43.969 --> 00:10:47.086
Ahogy már említettem,
csúnyán viselkedünk,

00:10:47.110 --> 00:10:49.431
néhányunk egyenesen gazemberként.

00:10:50.251 --> 00:10:53.303
A robotnak pedig, ahogy mondtam,
nem kell másolnia minket.

00:10:53.327 --> 00:10:56.118
Nincs is önálló célja.

00:10:56.142 --> 00:10:57.879
Csak tisztán altruista.

00:10:59.113 --> 00:11:04.334
Nem is úgy van tervezve, hogy csak
egy ember kívánságait tartsa szem előtt,

00:11:04.358 --> 00:11:07.496
hanem mindenki igényeire
tekintettel kell lennie.

00:11:09.083 --> 00:11:11.653
El kell tehát boldoguljon
bizonyos mértékű galádsággal,

00:11:11.677 --> 00:11:15.378
sőt, meg kell értenie, miért viselkedünk
renitens módon:

00:11:15.402 --> 00:11:18.073
például, hogy vámkezelőként
csúszópénzt fogadunk el,

00:11:18.097 --> 00:11:21.773
mert el kell látnunk a családunkat 
és fizetnünk a gyerekeink iskoláztatását.

00:11:21.773 --> 00:11:24.839
Megérteni a cselekedetet,
de ez nem jelenti, hogy ő is lopni fog.

00:11:24.863 --> 00:11:27.542
Azon lesz, hogy segítsen
a gyerekek beiskolázásában.

00:11:28.796 --> 00:11:31.808
Továbbá az agyunk számítási
teljesítménye is korlátozott.

00:11:31.832 --> 00:11:34.337
Lee Sedol briliáns Go játékos,

00:11:34.361 --> 00:11:35.686
mégis veszített.

00:11:35.710 --> 00:11:39.949
Ha konkrétan elemezzük a lépéseit,
volt egy lépése, ami miatt veszített.

00:11:39.973 --> 00:11:42.134
Holott nem akart veszíteni.

00:11:43.160 --> 00:11:45.200
Hogy megértsük a viselkedését,

00:11:45.224 --> 00:11:48.868
igazából meg kell fordítanunk
az emberi értelem modelljét,

00:11:48.892 --> 00:11:53.869
melyben helyet kap a véges számítási
kapacitás - ez egy bonyolult modell.

00:11:53.893 --> 00:11:56.886
De ez is olyasmi, aminek megértésén
van még mit dolgozni.

00:11:57.696 --> 00:12:02.016
Szerintem egy MI-kutatónak
valószínűleg az a legnehezebb,

00:12:02.040 --> 00:12:04.615
hogy sokan vagyunk,

00:12:06.114 --> 00:12:09.695
és a gépnek valamiképp súlyoznia kell
és egyszerre optimalizálni

00:12:09.719 --> 00:12:11.944
megannyi különböző emberre tekintettel,

00:12:11.968 --> 00:12:13.874
és erre különböző megoldások vannak.

00:12:13.898 --> 00:12:17.587
Közgazdászok, szociológusok,
filozófusok rájöttek már erre,

00:12:17.611 --> 00:12:20.066
s mi élénken keressük az együttműködést.

00:12:20.090 --> 00:12:23.341
Nézzük meg, mi történik,
ha nem jól alkalmazzuk az eddigieket.

00:12:23.365 --> 00:12:25.152
Megbeszéljük a dolgokat,

00:12:25.152 --> 00:12:27.466
például az intelligens
személyi asszisztensünkkel

00:12:27.490 --> 00:12:29.775
ami akár néhány éven belül elérhető lehet.

00:12:29.799 --> 00:12:32.323
Olyan, mint egy felturbózott Siri.

00:12:33.447 --> 00:12:37.769
Siri megszólal: "Hívott a feleséged,
hogy figyelmeztessen a mai vacsorára."

00:12:38.436 --> 00:12:40.944
Mi persze elfeledkeztünk róla:
"Mi? Miféle vacsora?

00:12:40.968 --> 00:12:42.393
Miről beszélsz?"

00:12:42.417 --> 00:12:46.163
"Hát, a 20-adik évfordulótok, este 7-kor."

00:12:48.735 --> 00:12:52.454
"Ez nem fog menni. 
7:30-kor a főtitkárral van találkozóm.

00:12:52.478 --> 00:12:54.170
Hogyan fordulhatott ez elő?"

00:12:54.194 --> 00:12:58.854
"Nos, én figyelmeztettelek,
de te máshogy döntöttél."

00:12:59.966 --> 00:13:03.294
"Most mitévő legyek?
Nem mondhatom neki, hogy sok a dolgom."

00:13:04.310 --> 00:13:07.591
"Aggodalomra semmi ok. 
Elintéztem, hogy késsen a gépe."

00:13:07.615 --> 00:13:09.297
(Nevetés)

00:13:10.069 --> 00:13:12.170
"Lesz valami számítástechnikai gubanc."

00:13:12.194 --> 00:13:13.406
(Nevetés)

00:13:13.430 --> 00:13:15.047
"Komolyan? Te erre is képes vagy?"

00:13:16.220 --> 00:13:18.399
"Üzent, hogy mélységesen sajnálja,

00:13:18.423 --> 00:13:20.978
és már nagyon várja
a holnapi közös ebédet."

00:13:21.002 --> 00:13:22.301
(Nevetés)

00:13:22.325 --> 00:13:26.728
Itt az értékek tekintetében
van egy kis megbicsaklás...

00:13:26.752 --> 00:13:29.761
Ez a feleségem értékrendjéről szól,

00:13:29.785 --> 00:13:31.854
ami úgy szól:
"Boldog feleség, boldog élet."

00:13:31.878 --> 00:13:33.461
(Nevetés)

00:13:33.485 --> 00:13:34.969
De ez elsülhet ellenkezőleg is.

00:13:35.641 --> 00:13:37.842
Hazaérünk a munkából egy nehéz nap után.

00:13:37.866 --> 00:13:40.061
és a számítógép így szól: "Hosszú nap?"

00:13:40.085 --> 00:13:42.347
"Igen, még ebédelni sem volt időm."

00:13:42.347 --> 00:13:43.679
"Akkor nagyon éhes lehetsz."

00:13:43.703 --> 00:13:46.349
"Igen, éhen halok.
Készítenél valami vacsorát?"

00:13:47.890 --> 00:13:49.980
"Valamit el kell mondanom..."

00:13:50.004 --> 00:13:51.159
(Nevetés)

00:13:52.013 --> 00:13:56.918
"Dél-Szudánban emberi lényeknek sokkal
nagyobb szükségük van az ételre."

00:13:56.942 --> 00:13:57.900
(Nevetés)

00:13:57.900 --> 00:14:00.145
"Szóval én távoztam.
Csinálj magadnak vacsorát!"

00:14:00.169 --> 00:14:02.169
(Nevetés)

00:14:02.363 --> 00:14:04.382
Ezeket a problémákat
még meg kell oldanunk,

00:14:04.406 --> 00:14:06.921
de alig várom, hogy dolgozhassak rajtuk.

00:14:06.945 --> 00:14:08.788
Több okunk is van optimizmusra.

00:14:08.812 --> 00:14:09.971
Az első,

00:14:09.995 --> 00:14:11.863
hogy rengeteg adat áll rendelkezésre.

00:14:11.887 --> 00:14:14.665
Mert mindent el fognak olvasni,

00:14:14.665 --> 00:14:16.285
amit az emberi faj valaha is írt.

00:14:16.285 --> 00:14:18.999
Az írásaink zöme arról szól,
hogy emberek tesznek valamit,

00:14:19.023 --> 00:14:20.937
és ettől más emberi lények dühösek.

00:14:20.961 --> 00:14:23.359
Rengeteg ismeretanyag van,
amiből tanulni lehet.

00:14:23.383 --> 00:14:25.619
Továbbá van egy nagyon erős
gazdasági nyomás,

00:14:26.931 --> 00:14:28.337
hogy ezt jól valósítsuk meg.

00:14:28.341 --> 00:14:30.492
Képzeljük el az otthoni
háztartási robotunkat.

00:14:30.492 --> 00:14:33.573
Megint későn érünk haza,
a robotnak kell megetetnie a gyerekeket.

00:14:33.573 --> 00:14:36.300
A gyerekek éhesek, de üres a hűtőszekrény.

00:14:36.324 --> 00:14:38.929
Ekkor a robot meglátja a macskát.

00:14:38.953 --> 00:14:40.645
(Nevetés)

00:14:40.669 --> 00:14:44.859
Mivel a robot nem sajátította el
elég jól az emberi értékrendet,

00:14:44.883 --> 00:14:46.134
nem érti,

00:14:46.158 --> 00:14:51.002
hogy a macska szentimentális értéke
túlmutat a tápértékén.

00:14:51.026 --> 00:14:52.121
(Nevetés)

00:14:52.145 --> 00:14:53.893
Vajon mi történik?

00:14:53.917 --> 00:14:57.214
Valami ilyesmi:

00:14:57.238 --> 00:15:00.202
"A zsarnok robot feltálalta
vacsorára a család cicáját."

00:15:00.226 --> 00:15:04.749
Egyetlen ilyen fiaskó
a háztartási robotipar végét jelentené.

00:15:04.773 --> 00:15:08.145
Szóval nagy nyomás van rajtunk,
hogy ügyesen kezeljük ezt,

00:15:08.169 --> 00:15:11.094
még jóval azelőtt, hogy elérnénk
a szuperintelligens gépekig.

00:15:11.948 --> 00:15:13.483
Összefoglalva tehát:

00:15:13.507 --> 00:15:16.388
megpróbálom megváltoztatni
a MI definícióját úgy,

00:15:16.412 --> 00:15:19.405
hogy bizonyíthatóan hasznunkra
levő gépeket jelentsenek.

00:15:19.429 --> 00:15:20.651
Az alapelvek pedig:

00:15:20.675 --> 00:15:22.073
a gépek altruisták,

00:15:22.097 --> 00:15:24.901
azaz csak a mi céljaink
elérésével foglalkoznak,

00:15:24.925 --> 00:15:28.041
miközben bizonytalanok a mi céljainkban,

00:15:28.065 --> 00:15:30.063
de figyelnek minket,

00:15:30.087 --> 00:15:33.290
hogy megértsék, mit is akarunk valójában.

00:15:34.193 --> 00:15:37.752
És remélhetően ebben a folyamatban
mi is jobb emberekké válunk.

00:15:37.776 --> 00:15:38.967
Nagyon köszönöm.

00:15:38.991 --> 00:15:42.700
(Taps)

00:15:42.724 --> 00:15:44.592
Chris Anderson: Nagyon érdekes, Stuart.

00:15:44.616 --> 00:15:47.740
Álljunk arrébb kicsit, mert rendezkednek

00:15:47.740 --> 00:15:48.961
a következő előadó miatt.

00:15:48.985 --> 00:15:50.523
Pár kérdés.

00:15:50.547 --> 00:15:55.934
Ez a tudatlanság alapú programozás
elég hatékony dolognak tűnik.

00:15:55.934 --> 00:15:57.618
Ha elérjük a szuperintelligenciát,

00:15:57.642 --> 00:15:59.900
mi akadályozza meg a robotot abban,

00:15:59.924 --> 00:16:02.560
hogy az irodalmat olvasgatva
arra a felismerésre jusson,

00:16:02.560 --> 00:16:04.372
hogy a tudás jobb, mint a tudatlanság,

00:16:04.396 --> 00:16:08.614
és ezért átírja a saját programját,
hogy legyenek saját céljai?

00:16:09.512 --> 00:16:15.868
Stuart Russell: Igen, ahogy mondtam is,
jobban meg kell ismerjük

00:16:15.892 --> 00:16:17.179
a saját céljainkat is.

00:16:17.203 --> 00:16:22.724
Csak annyival válik magabiztosabbá,
amennyivel jobban átlátja a dolgokat,

00:16:22.748 --> 00:16:24.693
ez rá a garancia,

00:16:24.717 --> 00:16:27.441
és úgy lesz tervezve,
hogy ezt jól értelmezze.

00:16:27.465 --> 00:16:31.295
Például rá fog jönni,
hogy a könyvek nagyon elfogultak

00:16:31.295 --> 00:16:32.928
a megismert bizonyítékaik alapján.

00:16:32.952 --> 00:16:35.349
Kizárólag királyokról
és hercegekről szólnak,

00:16:35.373 --> 00:16:38.173
csak fehér férfiak csinálnak mindent.

00:16:38.197 --> 00:16:40.293
Szóval komplikált a probléma,

00:16:40.317 --> 00:16:44.189
de ahogy egyre jobban
megismeri a céljainkat,

00:16:44.213 --> 00:16:46.276
annál inkább lesz hasznunkra.

00:16:46.300 --> 00:16:48.826
CA: Nem lehetne csak
egy szabályra egyszerűsíteni,

00:16:48.850 --> 00:16:50.500
simán betáplálva, hogy:

00:16:50.524 --> 00:16:53.817
"Ha egy ember megpróbál kikapcsolni,

00:16:53.841 --> 00:16:55.776
elfogadom, elfogadom."

00:16:55.800 --> 00:16:56.982
SR: Egyáltalán nem.

00:16:57.006 --> 00:16:58.505
Ez egy szörnyű elképzelés.

00:16:58.529 --> 00:17:01.218
Képzeld csak el,
hogy van egy önvezető autód,

00:17:01.242 --> 00:17:03.675
és el akarod vele küldeni 
az ötéves gyereked

00:17:03.699 --> 00:17:04.873
az óvodába.

00:17:04.897 --> 00:17:07.722
Azt akarod, 
hogy az ötéves képes legyen

00:17:07.722 --> 00:17:09.235
kikapcsolni az autót útközben?

00:17:09.259 --> 00:17:10.418
Valószínűleg nem.

00:17:10.442 --> 00:17:15.145
Tehát szükséges, hogy értse, mennyire
racionális és józan az adott ember.

00:17:15.169 --> 00:17:16.749
Minél racionálisabb az ember,

00:17:16.749 --> 00:17:19.042
annál inkább hajlandó engedni,
hogy kikapcsolják.

00:17:19.042 --> 00:17:21.579
Ha a személy teljesen zavart,
vagy akár ártó szándékú,

00:17:21.579 --> 00:17:24.075
akkor kevésbé lesz hajlandó rá,
hogy kikapcsolják.

00:17:24.099 --> 00:17:25.965
CA: Rendben, Stuart,
csak annyit mondok,

00:17:25.989 --> 00:17:28.303
nagyon-nagyon remélem,
hogy megoldod ezt nekünk.

00:17:28.327 --> 00:17:30.702
Igazán köszönöm az előadást.
Lenyűgöző volt.

00:17:30.726 --> 00:17:31.893
SR: Köszönöm.

00:17:31.917 --> 00:17:33.754
(Taps)

