WEBVTT
Kind: captions
Language: es

00:00:00.000 --> 00:00:07.000
Traductor: Denise RQ
Revisor: Marta Palacio

00:00:12.532 --> 00:00:14.244
Este es Lee Sedol.

00:00:14.244 --> 00:00:17.174
Lee Sedol es uno de los mejores
jugadores de Go del mundo.

00:00:17.174 --> 00:00:20.629
Y está teniendo lo que mis amigos
de Silicon Valley llaman

00:00:20.629 --> 00:00:22.549
un momento "¡Bendito Dios!".

00:00:22.549 --> 00:00:23.650
(Risas)

00:00:23.650 --> 00:00:25.823
Un momento en el que nos damos cuenta

00:00:25.823 --> 00:00:29.618
de que la IA está avanzando 
mucho más rápido de lo que esperábamos.

00:00:29.618 --> 00:00:31.848
Los humanos han perdido 
en el tablero de Go.

00:00:31.848 --> 00:00:33.114
¿Y en el mundo real?

00:00:33.114 --> 00:00:36.035
Bueno, el mundo real es mucho 
más grande y complicado

00:00:36.035 --> 00:00:37.335
que el tablero de Go.

00:00:37.335 --> 00:00:38.755
Es mucho menos visible.

00:00:38.755 --> 00:00:41.205
Pero sigue siendo un problema de decisión.

00:00:42.585 --> 00:00:46.843
Y si pensamos en algunas
de las tecnologías que están por venir

00:00:47.368 --> 00:00:51.703
Noriko [Arai] mencionó 
que las máquinas aún no saben leer,

00:00:51.703 --> 00:00:53.503
al menos no comprendiendo,

00:00:53.503 --> 00:00:56.017
pero lo harán, y cuando eso suceda,

00:00:56.917 --> 00:01:02.479
poco después las máquinas habrán leído 
todo lo que la raza humana ha escrito.

00:01:03.670 --> 00:01:05.498
Eso permitirá a las máquinas,

00:01:05.498 --> 00:01:08.608
junto a su habilidad mirar más allá 
de lo que pueden los humanos,

00:01:08.608 --> 00:01:09.988
como ya hemos visto en el Go,

00:01:09.988 --> 00:01:12.372
si también tienen acceso 
a más información,

00:01:12.372 --> 00:01:15.612
serán capaces de tomar 
mejores decisiones en el mundo real

00:01:15.612 --> 00:01:16.682
que nosotros.

00:01:18.612 --> 00:01:20.218
¿Es eso bueno?

00:01:21.718 --> 00:01:23.950
Bueno, espero que sí.

00:01:26.514 --> 00:01:29.769
Toda nuestra civilización,
todo lo que valoramos,

00:01:29.793 --> 00:01:31.861
se basa en nuestra inteligencia.

00:01:31.885 --> 00:01:35.579
Y si tuviéramos acceso
a mucha más inteligencia,

00:01:35.603 --> 00:01:39.165
entonces no existirían límites
para lo que la raza humana pueda hacer.

00:01:40.485 --> 00:01:43.224
Y creo que este podría ser, 
como han dicho algunos,

00:01:43.224 --> 00:01:45.850
el mayor acontecimiento
de la historia de la humanidad.

00:01:48.485 --> 00:01:51.314
Entonces, ¿por qué la gente afirma
cosas como esta?

00:01:51.338 --> 00:01:54.862
Que la inteligencia artificial podría
significar el fin de la raza humana.

00:01:55.258 --> 00:01:56.917
¿Es esto algo nuevo?

00:01:56.941 --> 00:02:01.051
¿Se trata solo de Elon Musk 
y Bill Gates y Stephen Hawking?

00:02:01.773 --> 00:02:05.035
En realidad, no. Esta idea no es nueva.

00:02:05.059 --> 00:02:07.021
He aquí una cita:

00:02:07.045 --> 00:02:11.395
"Incluso si pudiéramos mantener las
máquinas en una posición servil,

00:02:11.419 --> 00:02:14.403
por ejemplo, desconectándolas
en momentos estratégicos"

00:02:14.427 --> 00:02:17.664
--volveré a esa idea de 
"quitar la corriente" más adelante--

00:02:17.688 --> 00:02:20.546
"deberíamos, como especie,
sentirnos humillados".

00:02:21.996 --> 00:02:25.445
¿Quién dijo esto?
Este es Alan Turing, en 1951.

00:02:26.120 --> 00:02:28.883
Alan Turing, como Uds. saben,
es el padre de la informática

00:02:28.907 --> 00:02:31.955
y en muchos sentidos 
también el padre de la IA.

00:02:33.059 --> 00:02:34.941
Así que si pensamos en este problema,

00:02:34.965 --> 00:02:38.752
el problema de crear algo
más inteligente que tu propia especie,

00:02:38.776 --> 00:02:41.398
podríamos llamar a esto
"el problema del gorila".

00:02:42.165 --> 00:02:45.915
Porque los antepasados de los gorilas
hicieron esto hace unos millones de años,

00:02:45.939 --> 00:02:47.979
y ahora podríamos
preguntar a los gorilas:

00:02:48.572 --> 00:02:49.732
¿Fue una buena idea?

00:02:49.756 --> 00:02:53.746
Aquí están, reunidos para discutir 
si fue una buena idea,

00:02:53.746 --> 00:02:56.736
y pasado un tiempo concluyen que no.

00:02:56.736 --> 00:02:58.095
Fue una idea terrible.

00:02:58.095 --> 00:02:59.831
Nuestra especie está en apuros.

00:03:00.358 --> 00:03:04.621
De hecho, pueden ver 
la tristeza existencial en sus ojos.

00:03:04.645 --> 00:03:06.285
(Risas)

00:03:06.309 --> 00:03:10.103
Así que esta sensación mareante
de que crear algo más inteligente

00:03:10.103 --> 00:03:13.538
que tu propia especie
tal vez no sea buena idea...

00:03:14.308 --> 00:03:15.799
¿Qué podemos hacer al respecto?

00:03:15.823 --> 00:03:20.590
Bueno, nada en realidad,
excepto dejar de hacer IA.

00:03:20.614 --> 00:03:23.164
Y por todos los beneficios 
que he mencionado

00:03:23.164 --> 00:03:24.864
y porque soy un investigador de IA,

00:03:24.888 --> 00:03:26.679
no voy a tomar eso.

00:03:27.103 --> 00:03:29.571
Sin duda quiero seguir creando IA.

00:03:30.435 --> 00:03:33.113
Así que necesitamos precisar
el problema un poco más.

00:03:33.137 --> 00:03:34.508
¿Cuál es el problema?

00:03:34.532 --> 00:03:37.778
¿Por qué tener mejor IA 
puede ser una catástrofe?

00:03:39.218 --> 00:03:40.716
Aquí hay otra cita:

00:03:41.755 --> 00:03:43.955
"Más nos vale estar seguros
de que el propósito

00:03:43.955 --> 00:03:46.824
que introducimos en la máquina
es el que de verdad deseamos".

00:03:47.954 --> 00:03:51.132
Esto fue dicho por Norbert Wiener en 1960,

00:03:51.132 --> 00:03:55.310
poco después de ver a uno de
los primeros sistemas de aprendizaje

00:03:55.310 --> 00:03:58.156
aprender a jugar a las damas
mejor que su creador.

00:04:00.422 --> 00:04:03.985
Pero esto podría haberlo dicho 
de igual modo el Rey Midas.

00:04:03.985 --> 00:04:07.966
El Rey Midas dijo, "Deseo que todo 
lo que toque se convierta en oro".

00:04:07.966 --> 00:04:10.487
Y obtuvo justo lo que pidió.

00:04:10.487 --> 00:04:13.934
Fue el propósito que introdujo
en la máquina, por así decirlo.

00:04:14.807 --> 00:04:18.175
Y luego su comida, su bebida
y sus familiares se convirtieron en oro

00:04:18.175 --> 00:04:20.556
y murió miserable y muerto de hambre.

00:04:22.264 --> 00:04:24.875
Así que llamaremos a esto
"el problema del rey Midas",

00:04:24.875 --> 00:04:28.033
el de indicar un objetivo
que no está realmente

00:04:28.033 --> 00:04:30.371
alineado con lo que de verdad queremos.

00:04:30.395 --> 00:04:34.110
En términos modernos, lo llamamos
"el problema de alineación de valor".

00:04:36.867 --> 00:04:40.352
Introducir un objetivo equivocado
no es la única parte del problema.

00:04:40.376 --> 00:04:41.528
Hay otra parte.

00:04:41.980 --> 00:04:43.923
Al introducir un objetivo en una máquina

00:04:43.947 --> 00:04:46.395
incluso algo tan simple como
"Trae el café",

00:04:47.728 --> 00:04:49.569
la máquina se dice a sí misma,

00:04:50.553 --> 00:04:52.953
"¿Cómo podría fallar 
yendo a buscar el café?

00:04:52.953 --> 00:04:54.773
Alguien podría desconectarme.

00:04:54.773 --> 00:04:57.475
Vale, debo tomar medidas para evitarlo.

00:04:57.839 --> 00:04:59.970
Desactivaré mi interruptor de 'apagado'.

00:04:59.970 --> 00:05:03.082
Haré cualquier cosa para protegerme
de interferencias

00:05:03.082 --> 00:05:05.328
con este objetivo que me han dado.

00:05:05.988 --> 00:05:08.108
Así que esta persecución obsesiva

00:05:08.884 --> 00:05:11.984
de un modo muy defensivo
para lograr un objetivo

00:05:11.984 --> 00:05:14.314
que no está alineado
con los verdaderos objetivos

00:05:14.314 --> 00:05:15.467
de la raza humana...

00:05:15.467 --> 00:05:17.826
ese es el problema 
al que nos enfrentamos.

00:05:18.526 --> 00:05:23.317
Y de hecho esa es la lección
más valiosa de esta charla.

00:05:23.437 --> 00:05:25.638
Si quieren recordar una cosa

00:05:25.697 --> 00:05:28.372
es que no se puede ir a buscar
el café si se está muerto.

00:05:28.396 --> 00:05:29.457
(Risas)

00:05:29.481 --> 00:05:33.310
Es muy simple. Solo recuerden eso.
Repítanlo tres veces al día.

00:05:33.334 --> 00:05:35.155
(Risas)

00:05:35.179 --> 00:05:37.933
Y de hecho, este es el mismo argumento

00:05:37.957 --> 00:05:40.605
de "2001: [Una odisea del espacio]".

00:05:41.046 --> 00:05:43.136
HAL tiene un objetivo, una misión,

00:05:43.160 --> 00:05:46.892
que no está alineada
con los objetivos de los humanos,

00:05:46.916 --> 00:05:48.726
y eso conduce a este conflicto.

00:05:49.314 --> 00:05:52.283
Por suerte HAL no es superinteligente.

00:05:52.307 --> 00:05:54.848
Es bastante inteligente,
pero llegado el momento,

00:05:54.848 --> 00:05:57.767
Dave lo supera y logra apagarlo.

00:06:01.648 --> 00:06:03.458
Pero tal vez no tengamos tanta suerte.

00:06:08.013 --> 00:06:09.605
Entonces, ¿qué vamos a hacer?

00:06:12.191 --> 00:06:14.792
Estoy tratando de redefinir la IA

00:06:14.816 --> 00:06:16.877
para alejarnos de esta noción clásica

00:06:16.901 --> 00:06:21.468
de máquinas que persiguen objetivos
de manera inteligente.

00:06:22.532 --> 00:06:24.330
Hay tres principios implicados.

00:06:24.354 --> 00:06:27.643
El primero es un principio
de altruismo, por así decirlo,

00:06:27.667 --> 00:06:30.929
el único objetivo del robot

00:06:30.953 --> 00:06:35.199
es maximizar la realización
de los objetivos humanos,

00:06:35.223 --> 00:06:36.613
de los valores humanos.

00:06:36.637 --> 00:06:39.967
Y por valores aquí no me refiero
a valores sentimentales o de bondad.

00:06:39.991 --> 00:06:43.778
Solo quiero decir aquello
más similar a la vida

00:06:43.802 --> 00:06:45.145
que un humano preferiría.

00:06:47.184 --> 00:06:49.307
Y esto viola la ley de Asimov

00:06:49.307 --> 00:06:51.786
de que el robot debe proteger
su propia existencia.

00:06:51.786 --> 00:06:55.593
No tiene ningún interés en preservar
su existencia en absoluto.

00:06:57.240 --> 00:07:01.008
La segunda ley es una ley
de humildad, digamos.

00:07:01.794 --> 00:07:05.537
Y resulta muy importante
para que los robots sean seguros.

00:07:05.561 --> 00:07:08.703
Dice que el robot no sabe

00:07:08.727 --> 00:07:10.755
cuáles son esos valores humanos,

00:07:10.779 --> 00:07:13.957
así que debe maximizarlos,
pero no sabe lo que son.

00:07:15.074 --> 00:07:17.700
Lo cual evita el problema 
de la búsqueda obsesiva

00:07:17.724 --> 00:07:18.936
de un objetivo.

00:07:18.960 --> 00:07:21.132
Esta incertidumbre resulta crucial.

00:07:21.546 --> 00:07:23.255
Claro que para sernos útiles,

00:07:23.255 --> 00:07:25.940
deben tener alguna idea 
de lo que queremos.

00:07:27.043 --> 00:07:32.470
Obtiene esa información sobre todo
observando elecciones humanas,

00:07:32.494 --> 00:07:35.295
para que nuestras propias
decisiones revelen información

00:07:35.319 --> 00:07:38.619
sobre lo que nosotros preferimos
para nuestras vidas.

00:07:40.452 --> 00:07:42.135
Estos son los tres principios.

00:07:42.159 --> 00:07:44.477
Veamos cómo se aplica a esta cuestión

00:07:44.501 --> 00:07:47.290
de "apagar la máquina", 
como sugirió Turing.

00:07:48.893 --> 00:07:50.657
He aquí un robot PR2.

00:07:50.657 --> 00:07:52.858
Es uno que tenemos
en nuestro laboratorio,

00:07:52.882 --> 00:07:56.311
y tiene un gran botón rojo de 'apagado'
en la parte posterior.

00:07:56.361 --> 00:07:58.760
La pregunta es: ¿Va a
dejar que lo apaguen?

00:07:58.760 --> 00:08:00.465
Si lo hacemos a la manera clásica,

00:08:00.489 --> 00:08:03.971
le damos el objetivo de traer
el café. "Debo traer el café.

00:08:03.995 --> 00:08:06.575
No puedo traer el café 
si estoy muerto".

00:08:06.599 --> 00:08:09.940
Obviamente el PR2 
ha escuchado mi charla,

00:08:09.964 --> 00:08:11.866
y por tanto, decide

00:08:11.866 --> 00:08:14.796
"Debo inhabilitar mi botón de 'apagado'".

00:08:14.796 --> 00:08:17.980
"Y probablemente electrocutar 
al resto de personas en el Starbucks

00:08:17.980 --> 00:08:19.254
que podrían interferir".

00:08:19.254 --> 00:08:21.160
(Risas)

00:08:21.184 --> 00:08:23.337
Así que esto parece ser
inevitable, ¿verdad?

00:08:23.361 --> 00:08:25.759
Este tipo de error 
parece ser inevitable,

00:08:25.783 --> 00:08:29.326
y sucede por tener 
un objetivo concreto, definido.

00:08:30.632 --> 00:08:33.776
Entonces, ¿qué pasa si la máquina
no tiene claro el objetivo?

00:08:33.800 --> 00:08:35.927
Bueno, razona de una manera diferente.

00:08:35.951 --> 00:08:40.485
Dice, "El humano podría desconectarme,
pero solo si hago algo malo.

00:08:41.577 --> 00:08:45.596
No tengo claro lo que es malo
pero sé que no quiero hacerlo".

00:08:45.606 --> 00:08:48.179
Ahí están el primer 
y el segundo principio.

00:08:49.179 --> 00:08:52.363
"Así que debería dejar 
que el humano me desconecte".

00:08:53.541 --> 00:08:57.161
De hecho se puede calcular 
el incentivo que tiene el robot

00:08:57.161 --> 00:09:00.031
para permitir que el humano lo apague.

00:09:00.038 --> 00:09:02.802
Y está directamente ligado 
al grado de incertidumbre

00:09:02.802 --> 00:09:04.852
sobre el objetivo subyacente.

00:09:05.246 --> 00:09:08.842
Y entonces cuando la máquina está apagada,

00:09:08.842 --> 00:09:10.575
el tercer principio entra en juego.

00:09:10.599 --> 00:09:13.661
Aprende algo sobre los objetivos
que debe perseguir,

00:09:13.685 --> 00:09:16.218
porque aprende que
lo que hizo no estaba bien.

00:09:16.242 --> 00:09:19.812
De hecho, podemos, con el uso adecuado
de los símbolos griegos,

00:09:19.836 --> 00:09:21.967
como suelen hacer los matemáticos,

00:09:21.991 --> 00:09:23.975
podemos probar un teorema

00:09:23.999 --> 00:09:27.552
que dice que tal robot es probablemente
beneficioso para el humano.

00:09:27.576 --> 00:09:31.379
Se está demostrablemente mejor con
una máquina que se diseña de esta manera

00:09:31.403 --> 00:09:32.649
que sin ella.

00:09:33.057 --> 00:09:35.963
Este es un ejemplo muy simple,
pero este es el primer paso

00:09:35.987 --> 00:09:39.890
en lo que estamos tratando de hacer
con IA compatible con humanos.

00:09:42.477 --> 00:09:45.734
Ahora, este tercer principio,

00:09:45.758 --> 00:09:48.870
es probablemente el que está haciendo
que se rasquen la cabeza.

00:09:48.894 --> 00:09:51.904
Probablemente piensen: 
"Yo me comporto mal.

00:09:51.904 --> 00:09:54.994
No quiero que mi robot 
se comporte como yo.

00:09:54.994 --> 00:09:58.084
Me escabullo en mitad de la noche 
y tomo cosas de la nevera,

00:09:58.084 --> 00:09:59.365
hago esto y hago aquello".

00:09:59.365 --> 00:10:02.016
Hay todo tipo de cosas que no
quieres que haga el robot.

00:10:02.416 --> 00:10:04.644
Pero lo cierto es que 
no funciona así.

00:10:04.644 --> 00:10:06.806
Solo porque uno se comporte mal

00:10:06.806 --> 00:10:09.591
no significa que el robot
vaya a copiar su comportamiento.

00:10:09.591 --> 00:10:12.862
Va a entender sus motivaciones
y tal vez a ayudarle a resistirlas,

00:10:13.436 --> 00:10:14.756
si es apropiado.

00:10:16.026 --> 00:10:17.490
Pero sigue siendo difícil.

00:10:18.122 --> 00:10:20.667
Lo que estamos tratando
de hacer, de hecho,

00:10:20.691 --> 00:10:24.711
es permitir que las máquinas predigan
para cualquier persona

00:10:24.711 --> 00:10:27.672
y para cualquier vida posible 
que podrían vivir,

00:10:27.696 --> 00:10:29.293
y las vidas de todos los demás

00:10:29.317 --> 00:10:31.834
lo que preferirían.

00:10:33.881 --> 00:10:36.835
Y hay muchas, muchas 
dificultades ligadas a hacer esto.

00:10:36.859 --> 00:10:39.791
No espero que vaya a resolverse pronto.

00:10:39.815 --> 00:10:42.458
Las verdaderas dificultades,
de hecho, somos nosotros.

00:10:43.969 --> 00:10:47.086
Como ya he mencionado,
nos comportamos mal.

00:10:47.110 --> 00:10:50.063
De hecho, algunos de nosotros
somos francamente desagradables.

00:10:50.251 --> 00:10:53.303
Como he dicho, el robot
no tiene que copiar el comportamiento.

00:10:53.327 --> 00:10:56.118
El robot no tiene ningún objetivo propio.

00:10:56.142 --> 00:10:57.879
Es puramente altruista.

00:10:59.113 --> 00:11:04.334
Y no está diseñado solo para satisfacer
los deseos de una persona, el usuario,

00:11:04.358 --> 00:11:07.496
sino que tiene que respetar
las preferencias de todos.

00:11:09.083 --> 00:11:11.653
Así que puede lidiar 
con cierta cantidad de maldad,

00:11:11.677 --> 00:11:15.378
e incluso puede entender
que su maldad, por ejemplo...

00:11:15.402 --> 00:11:18.213
Ud. puede aceptar sobornos 
como controlador de pasaportes

00:11:18.213 --> 00:11:21.909
porque necesita alimentar a su familia 
y que sus hijos vayan a la escuela.

00:11:21.933 --> 00:11:24.839
Puede entender eso;
no significa que vaya a robar.

00:11:24.863 --> 00:11:27.728
De hecho, solo le ayudará
a que sus hijos vayan al colegio.

00:11:28.796 --> 00:11:31.808
También estamos limitados
computacionalmente.

00:11:31.832 --> 00:11:34.337
Lee Sedol es un jugador brillante de Go,

00:11:34.361 --> 00:11:35.686
pero aun así perdió.

00:11:35.710 --> 00:11:39.949
Si nos fijamos en sus acciones,
tomó una decisión que le hizo perder.

00:11:39.973 --> 00:11:42.134
Eso no significa que él quisiera perder.

00:11:43.160 --> 00:11:45.200
Así que para entender su comportamiento,

00:11:45.224 --> 00:11:48.868
en realidad tenemos que invertir,
a través de un modelo cognitivo humano

00:11:48.892 --> 00:11:51.633
que incluye nuestras 
limitaciones computacionales,

00:11:51.633 --> 00:11:53.893
y se trata de un modelo muy complicado.

00:11:53.893 --> 00:11:56.816
Pero es algo en lo que podemos 
trabajar para comprender.

00:11:57.406 --> 00:12:00.100
Puede que la parte más difícil, 
desde mi punto de vista

00:12:00.100 --> 00:12:01.320
como investigador de IA,

00:12:01.320 --> 00:12:04.615
es el hecho de que hay muchos de nosotros,

00:12:06.114 --> 00:12:08.969
con lo cual la máquina tiene que sopesar

00:12:08.969 --> 00:12:11.944
las preferencias de mucha gente diferente.

00:12:11.968 --> 00:12:13.874
Hay diferentes maneras de hacer eso.

00:12:13.898 --> 00:12:17.587
Economistas, sociólogos, 
filósofos morales han comprendido esto

00:12:17.611 --> 00:12:20.066
y estamos buscando 
colaboración de manera activa.

00:12:20.090 --> 00:12:23.341
Vamos a ver lo que sucede
cuando esto se hace mal.

00:12:23.365 --> 00:12:25.498
Ud. puede estar hablando, por ejemplo,

00:12:25.522 --> 00:12:27.466
con su asistente personal inteligente

00:12:27.490 --> 00:12:29.775
que podría estar disponible
dentro de unos años.

00:12:29.799 --> 00:12:32.323
Piensen en Siri con esteroides.

00:12:33.447 --> 00:12:37.769
Siri dice "Su esposa llamó para 
recordarle la cena de esta noche".

00:12:38.436 --> 00:12:41.724
Por supuesto, lo había olvidado.
¿Qué cena? ¿De qué está hablando?

00:12:42.614 --> 00:12:44.626
"Su 20 aniversario, a las 7pm".

00:12:48.566 --> 00:12:51.975
"No puedo, me reúno con el 
secretario general a las 7:30.

00:12:51.975 --> 00:12:54.315
¿Cómo ha podido suceder esto?".

00:12:54.315 --> 00:12:58.744
"Bueno, le advertí, pero ignoró
mi recomendación".

00:12:58.994 --> 00:13:02.412
"¿Qué voy a hacer? No puedo decirles 
que estoy demasiado ocupado".

00:13:03.948 --> 00:13:07.976
"No se preocupe, he hecho
que su avión se retrase".

00:13:07.976 --> 00:13:10.174
(Risas)

00:13:10.174 --> 00:13:12.625
"Algún tipo de error en el ordenador".

00:13:12.625 --> 00:13:13.430
(Risas)

00:13:13.430 --> 00:13:15.047
"¿En serio? ¿Puede hacer eso?".

00:13:16.220 --> 00:13:18.399
"Le envía sinceras disculpas

00:13:18.423 --> 00:13:20.978
y espera poder conocerle
mañana para el almuerzo".

00:13:21.002 --> 00:13:22.301
(Risas)

00:13:22.325 --> 00:13:26.728
Así que los valores aquí...
aquí hay un pequeño fallo.

00:13:26.752 --> 00:13:29.761
Claramente está siguiendo
los valores de mi esposa

00:13:29.785 --> 00:13:31.854
que son "esposa feliz, vida feliz".

00:13:31.878 --> 00:13:33.461
(Risas)

00:13:33.485 --> 00:13:34.929
Podría suceder al revés.

00:13:35.461 --> 00:13:37.842
Podría llegar a casa
tras un duro día de trabajo,

00:13:37.866 --> 00:13:40.111
y el ordenador dice "¿Un día duro?".

00:13:40.111 --> 00:13:41.947
"Sí, ni tuve tiempo de almorzar".

00:13:41.947 --> 00:13:43.759
"Debe tener mucha hambre".

00:13:43.759 --> 00:13:46.709
"Me muero de hambre, sí,
¿podría preparar algo de cena?".

00:13:47.890 --> 00:13:49.980
"Hay algo que necesito decirle".

00:13:50.004 --> 00:13:51.159
(Risas)

00:13:52.013 --> 00:13:56.918
"Hay humanos en Sudán del Sur
más necesitados que Ud.".

00:13:56.942 --> 00:13:58.046
(Risas)

00:13:58.070 --> 00:14:00.145
"Así que me voy, hágase su propia cena".

00:14:00.169 --> 00:14:02.169
(Risas)

00:14:02.403 --> 00:14:04.522
Así que tenemos que
resolver estos problemas,

00:14:04.522 --> 00:14:06.431
y tengo ganas de trabajar en ellos.

00:14:06.945 --> 00:14:08.788
Hay razones para ser optimistas.

00:14:08.812 --> 00:14:11.891
Una razón es que hay 
gran cantidad de datos

00:14:11.891 --> 00:14:15.321
Recuerden, leerán todo lo que 
la raza humana ha escrito.

00:14:15.321 --> 00:14:18.943
La mayoría de lo que escribimos
trata sobre humanos haciendo cosas

00:14:18.943 --> 00:14:20.937
y cómo estas molestan a otras personas.

00:14:20.961 --> 00:14:23.359
Así que hay muchos datos
de los que aprender.

00:14:23.383 --> 00:14:25.619
También hay un fuerte incentivo económico

00:14:26.791 --> 00:14:28.151
para que esto funcione bien.

00:14:28.151 --> 00:14:30.252
Imagine que su robot 
doméstico está en casa

00:14:30.252 --> 00:14:31.772
Ud. llega tarde del trabajo,

00:14:31.772 --> 00:14:33.907
el robot tiene que dar 
de comer a los niños,

00:14:33.907 --> 00:14:36.330
los niños tienen hambre
y no hay nada en la nevera.

00:14:36.330 --> 00:14:38.929
Y el robot ve al gato.

00:14:38.953 --> 00:14:40.645
(Risas)

00:14:40.669 --> 00:14:44.773
Y el robot no ha aprendido del todo bien
la función del valor humano

00:14:44.773 --> 00:14:46.284
por lo que no entiende

00:14:46.284 --> 00:14:50.586
que el valor sentimental del gato supera
el valor nutricional del gato.

00:14:50.586 --> 00:14:51.681
(Risas)

00:14:51.681 --> 00:14:53.177
Entonces, ¿qué pasa?

00:14:53.177 --> 00:14:57.214
Bueno, sucede lo siguiente:

00:14:57.238 --> 00:15:00.202
"Robot desquiciado cocina a un gatito
para la cena familiar".

00:15:00.226 --> 00:15:04.749
Ese único incidente acabaría
con la industria de robots domésticos.

00:15:04.773 --> 00:15:08.145
Así que hay un gran incentivo
para hacer esto bien.

00:15:08.169 --> 00:15:10.884
mucho antes de llegar
a las máquinas superinteligentes.

00:15:11.948 --> 00:15:13.483
Así que para resumir:

00:15:13.507 --> 00:15:16.388
Estoy intentando cambiar
la definición de IA

00:15:16.412 --> 00:15:19.405
para que tengamos máquinas
demostrablemente beneficiosas.

00:15:19.429 --> 00:15:20.651
Y los principios son:

00:15:20.675 --> 00:15:22.193
Máquinas que son altruistas,

00:15:22.193 --> 00:15:24.845
que desean lograr solo nuestros objetivos,

00:15:24.845 --> 00:15:27.885
pero que no están seguras
de cuáles son esos objetivos

00:15:27.885 --> 00:15:30.123
y nos observarán a todos

00:15:30.123 --> 00:15:33.290
para aprender qué es
lo que realmente queremos.

00:15:34.193 --> 00:15:37.752
Y con suerte, en el proceso, 
aprenderemos a ser mejores personas.

00:15:37.776 --> 00:15:38.967
Muchas gracias.

00:15:38.991 --> 00:15:42.484
(Aplausos)

00:15:42.484 --> 00:15:44.642
Chris Anderson: Muy interesante, Stuart.

00:15:44.642 --> 00:15:47.400
Vamos a estar aquí un poco
porque creo que están preparando

00:15:47.400 --> 00:15:48.961
a nuestro próximo orador.

00:15:48.985 --> 00:15:50.523
Un par de preguntas.

00:15:50.547 --> 00:15:55.974
La idea de programar ignorancia
parece intuitivamente muy poderosa.

00:15:55.974 --> 00:15:57.738
Al llegar a la superinteligencia,

00:15:57.738 --> 00:16:00.000
¿qué puede impedir que un robot

00:16:00.000 --> 00:16:02.770
lea literatura y descubra esta idea
de que el conocimiento

00:16:02.770 --> 00:16:04.452
es mejor que la ignorancia,

00:16:04.452 --> 00:16:08.614
cambiando sus propios objetivos
y reescribiendo su programación?

00:16:09.512 --> 00:16:15.802
Stuart Russell: Queremos
que aprenda más, como he dicho,

00:16:15.802 --> 00:16:17.279
sobre nuestros objetivos.

00:16:17.279 --> 00:16:22.724
Solo ganará seguridad
cuanto más acierte.

00:16:22.748 --> 00:16:24.693
La evidencia estará ahí,

00:16:24.717 --> 00:16:27.441
y estará diseñado para 
interpretarla adecuadamente.

00:16:27.465 --> 00:16:31.325
Comprenderá, por ejemplo,
que los libros son muy sesgados

00:16:31.325 --> 00:16:33.008
en la evidencia que contienen.

00:16:33.008 --> 00:16:35.459
Solo hablan de reyes y príncipes

00:16:35.459 --> 00:16:38.173
y hombres blancos poderosos 
haciendo cosas.

00:16:38.197 --> 00:16:40.177
Es un problema complicado,

00:16:40.177 --> 00:16:44.113
pero conforme aprenda más
sobre nuestros objetivos

00:16:44.113 --> 00:16:46.276
será cada vez más útil
para nosotros.

00:16:46.300 --> 00:16:48.906
CA: Y no podría reducirse a una ley,

00:16:48.906 --> 00:16:50.620
ya sabe, grabada a fuego,

00:16:50.620 --> 00:16:53.771
"Si un humano alguna vez
intenta apagarme

00:16:53.771 --> 00:16:55.776
yo obedezco, obedezco".

00:16:55.800 --> 00:16:57.122
SR: Absolutamente no.

00:16:57.122 --> 00:16:58.565
Sería una idea terrible.

00:16:58.565 --> 00:17:01.328
Imagine, tiene un auto que se conduce solo

00:17:01.328 --> 00:17:04.715
y quiere llevar a su hijo de cinco años
al jardín de infancia.

00:17:04.715 --> 00:17:08.198
¿Quiere que su hijo de cinco años pueda
apagar el coche mientras conduce?

00:17:08.198 --> 00:17:09.456
Probablemente no.

00:17:09.456 --> 00:17:15.169
Por tanto necesita entender 
cuán racional y sensata es la persona.

00:17:15.169 --> 00:17:16.885
Cuanto más racional sea la persona,

00:17:16.885 --> 00:17:18.992
más dispuesto estará 
a dejar que lo apaguen.

00:17:18.992 --> 00:17:21.649
Si la persona es impredecible
o incluso malintencionada

00:17:21.649 --> 00:17:24.039
estará menos dispuesto
a permitir que lo apaguen.

00:17:24.039 --> 00:17:25.353
CA: Stuart, permítame decir

00:17:25.353 --> 00:17:28.058
que de veras espero que resuelva esto
por todos nosotros.

00:17:28.058 --> 00:17:30.893
Muchas gracias por su charla. 
Ha sido increíble, gracias.

00:17:30.893 --> 00:17:32.323
(Aplausos)

