WEBVTT
Kind: captions
Language: ru

00:00:00.000 --> 00:00:07.000
Переводчик: Nataliia Pysemska
Редактор: Yekaterina Jussupova

00:00:12.712 --> 00:00:14.264
Это Ли Седоль.

00:00:14.288 --> 00:00:18.285
Ли Седоль — один из лучших
игроков в го в мире,

00:00:18.309 --> 00:00:20.558
и сейчас он в ситуации,
которую мои друзья

00:00:20.558 --> 00:00:22.828
из Силиконовой долины
называют «Мама дорогая!» —

00:00:22.828 --> 00:00:23.825
(Смех)

00:00:23.849 --> 00:00:26.037
момент осознания того,

00:00:26.061 --> 00:00:29.437
что искусственный интеллект развивается
гораздо быстрее, чем мы ожидали.

00:00:30.144 --> 00:00:33.271
Итак, человечество проиграло партию в го.
Но как же реальный мир?

00:00:33.271 --> 00:00:35.325
Ну, реальный мир гораздо больше

00:00:35.349 --> 00:00:37.552
и сложнее, чем доска для игры в го.

00:00:37.552 --> 00:00:39.571
Несмотря на то,
что это не столь очевидно,

00:00:39.571 --> 00:00:41.503
всё упирается в принятие решений.

00:00:42.948 --> 00:00:45.269
Если мы задумаемся о грядущих

00:00:45.293 --> 00:00:47.042
технологических инновациях...

00:00:47.738 --> 00:00:52.073
Норико [Араи] упомянула,
что машины ещё не понимают

00:00:52.097 --> 00:00:53.597
смысл прочитанного.

00:00:53.621 --> 00:00:55.157
Но в будущем это произойдёт,

00:00:55.181 --> 00:00:56.952
и в этот момент,

00:00:56.976 --> 00:00:58.163
совсем скоро,

00:00:58.187 --> 00:01:02.759
машины прочитают всё,
что когда-либо написали люди.

00:01:03.850 --> 00:01:05.880
Благодаря этому машины

00:01:05.904 --> 00:01:08.808
смогут предугадывать
развитие событий лучше, чем люди;

00:01:08.808 --> 00:01:10.528
что и случилось во время игры в го.

00:01:10.552 --> 00:01:12.716
Получив больший доступ к информации,

00:01:12.740 --> 00:01:17.008
они смогут принимать
решения лучше, чем мы.

00:01:18.792 --> 00:01:20.398
Хорошо ли это?

00:01:21.898 --> 00:01:24.130
Надеюсь, что да.

00:01:26.694 --> 00:01:29.949
Вся наша цивилизация, всё, чем мы дорожим,

00:01:29.973 --> 00:01:32.041
основывается на нашем интеллекте.

00:01:32.065 --> 00:01:35.759
Если бы наш интеллект увеличился,

00:01:35.783 --> 00:01:39.085
человеческим возможностям
не было бы предела.

00:01:40.665 --> 00:01:43.990
Думаю, это стало бы тем,
что некоторые называют

00:01:44.014 --> 00:01:46.030
величайшим событием
в истории человечества.

00:01:48.665 --> 00:01:51.494
Почему же тогда люди говорят,

00:01:51.518 --> 00:01:54.394
что ИИ может положить конец человечеству?

00:01:55.438 --> 00:01:57.097
Это что-то новое?

00:01:57.121 --> 00:02:01.231
Это лишь мнение Илона Маска,
Билла Гейтса и Стивена Хокинга?

00:02:01.953 --> 00:02:05.215
На самом деле нет.
Эта идея появилась довольно давно.

00:02:05.239 --> 00:02:07.201
Вот цитата:

00:02:07.225 --> 00:02:11.575
«Даже если бы мы сохранили
власть над машинами,

00:02:11.599 --> 00:02:14.583
например, имея возможность
отключить их в любой момент, —

00:02:14.607 --> 00:02:17.844
чуть позже я вернусь
к этой идее отключения, —

00:02:17.868 --> 00:02:20.672
мы, как вид, должны чувствовать себя
довольно униженными».

00:02:22.177 --> 00:02:25.625
Кто же сказал это?
Алан Тьюринг в 1951 году.

00:02:26.300 --> 00:02:29.063
Как известно, Алан Тьюринг
является отцом информатики,

00:02:29.087 --> 00:02:32.135
а также, во многих аспектах, отцом ИИ.

00:02:33.239 --> 00:02:35.121
Если мы задумаемся

00:02:35.145 --> 00:02:38.932
над проблемой создания чего-то
более разумного, чем наш вид,

00:02:38.956 --> 00:02:41.578
это можно будет назвать
«проблемой гориллы»,

00:02:42.345 --> 00:02:46.095
потому что предки горилл думали об этом
несколько миллионов лет назад,

00:02:46.119 --> 00:02:47.954
и теперь гориллам можно задать вопрос:

00:02:48.752 --> 00:02:49.912
«Это была хорошая идея?»

00:02:49.936 --> 00:02:53.466
Они бы собрались, чтобы решить,
была ли это хорошая идея,

00:02:53.490 --> 00:02:56.836
и через некоторое время
пришли бы к заключению,

00:02:56.860 --> 00:02:58.205
что это была ужасная идея.

00:02:58.229 --> 00:03:00.061
Наш вид находится в ужасном состоянии.

00:03:00.538 --> 00:03:04.801
Фактически в их глазах можно увидеть
экзистенциальную печаль.

00:03:04.825 --> 00:03:06.465
(Смех)

00:03:06.489 --> 00:03:11.329
Что нам делать с тошнотворным ощущением,

00:03:11.353 --> 00:03:13.718
что создание кого-то умнее нас

00:03:14.438 --> 00:03:15.979
может оказаться не лучшей идеей?

00:03:16.003 --> 00:03:20.770
Единственный выход —
прекратить разработку ИИ.

00:03:20.794 --> 00:03:23.304
Но, учитывая упомянутые мной преимущества

00:03:23.328 --> 00:03:25.044
и тот факт, что я исследователь ИИ,

00:03:25.068 --> 00:03:26.859
лично я против.

00:03:27.283 --> 00:03:29.751
Я хочу продолжать создавать ИИ.

00:03:30.615 --> 00:03:33.293
Поэтому нам следует
обсудить проблему более подробно.

00:03:33.317 --> 00:03:34.688
В чём суть проблемы?

00:03:34.712 --> 00:03:37.958
Почему совершенный ИИ может
обернуться катастрофой?

00:03:39.398 --> 00:03:40.896
Вот ещё цитата:

00:03:41.935 --> 00:03:45.270
«Нам следует быть уверенными,
что цели, заложенные в машину,

00:03:45.294 --> 00:03:47.592
совпадают с нашими желаниями».

00:03:48.282 --> 00:03:51.780
Это сказал Норберт Винер в 1960 году,

00:03:51.804 --> 00:03:55.806
вскоре после того, как увидел,
как одна из первых обучающихся систем

00:03:55.830 --> 00:03:58.413
научилась играть в шашки
лучше своего создателя.

00:04:00.602 --> 00:04:03.285
Нечто подобное сказал

00:04:03.309 --> 00:04:04.476
царь Мидас:

00:04:05.083 --> 00:04:08.217
«Хочу, чтобы всё, к чему я прикоснусь,
превращалось в золото».

00:04:08.241 --> 00:04:10.714
Он получил именно то, что просил.

00:04:10.738 --> 00:04:13.489
Именно эту цель он вложил в машину,

00:04:13.513 --> 00:04:14.963
так сказать,

00:04:14.987 --> 00:04:18.431
и затем еда, напитки и его семья
превратились в золото,

00:04:18.455 --> 00:04:20.736
а он сам умер мучительной
и голодной смертью.

00:04:22.444 --> 00:04:24.785
Итак, назовём это проблемой царя Мидаса:

00:04:24.809 --> 00:04:28.114
постановка цели,
которая в действительности

00:04:28.138 --> 00:04:30.551
не совпадает с нашим желанием.

00:04:30.575 --> 00:04:33.828
В современном мире это называется
проблемой согласования.

00:04:37.047 --> 00:04:40.532
Некорректная постановка цели —
лишь часть проблемы.

00:04:40.556 --> 00:04:41.708
Есть ещё кое-что.

00:04:42.160 --> 00:04:44.103
Если вы закладываете в машину цель,

00:04:44.127 --> 00:04:46.575
даже такую простую как сварить кофе,

00:04:47.908 --> 00:04:49.749
машина задастся вопросом:

00:04:50.733 --> 00:04:53.356
«Как я могу потерпеть неудачу
в приготовлении кофе?

00:04:53.380 --> 00:04:54.960
Кто-то может отключить меня.

00:04:55.645 --> 00:04:58.032
Тогда я должен предотвратить это.

00:04:58.056 --> 00:04:59.962
Я сломаю свой выключатель.

00:05:00.534 --> 00:05:03.493
Я сделаю что угодно, чтобы защититься
от любых вмешательств

00:05:03.517 --> 00:05:06.146
в процесс выполнения
поставленной мне задачи».

00:05:06.170 --> 00:05:08.182
На деле этот режим

00:05:09.213 --> 00:05:12.158
крайней самозащиты

00:05:12.182 --> 00:05:14.996
не имеет ничего общего
с первоначальной целью человечества —

00:05:16.122 --> 00:05:17.984
именно с этой проблемой мы столкнулись.

00:05:19.007 --> 00:05:23.774
Фактически это главный тезис выступления.

00:05:23.798 --> 00:05:25.853
Единственное, что вам нужно помнить:

00:05:25.877 --> 00:05:28.552
вы не сможете сварить кофе,
если вы покойник.

00:05:28.576 --> 00:05:29.637
(Смех)

00:05:29.661 --> 00:05:33.490
Всё очень просто. Всего лишь помните это.
Повторяйте себе это три раза в день.

00:05:33.514 --> 00:05:35.335
(Смех)

00:05:35.359 --> 00:05:38.113
На самом деле, в этом
заключается сюжет фильма

00:05:38.137 --> 00:05:40.785
«2001: [Космическая Одиссея]»:

00:05:41.226 --> 00:05:43.316
у HAL есть цель, миссия,

00:05:43.340 --> 00:05:47.072
которая не совпадает с целью людей,

00:05:47.096 --> 00:05:48.906
что и приводит к конфликту.

00:05:49.494 --> 00:05:52.463
К счастью, HAL не обладает сверхразумом.

00:05:52.487 --> 00:05:56.074
Он довольно умный,
но в итоге Дэйв смог перехитрить

00:05:56.098 --> 00:05:57.947
и отключить его.

00:06:01.828 --> 00:06:03.447
Но нам может повезти меньше.

00:06:08.193 --> 00:06:09.785
Что же нам делать?

00:06:12.371 --> 00:06:14.972
Я пытаюсь переосмыслить ИИ,

00:06:14.996 --> 00:06:17.057
чтобы уйти от классического понятия машин,

00:06:17.081 --> 00:06:21.648
которые благоразумно преследуют цель.

00:06:22.712 --> 00:06:24.510
Здесь задействованы три правила.

00:06:24.534 --> 00:06:27.823
Первое, правило альтруизма:

00:06:27.847 --> 00:06:31.109
единственная цель робота —

00:06:31.133 --> 00:06:35.379
помочь человеку добиться всего,
к чему он стремится,

00:06:35.403 --> 00:06:36.793
и приумножить его ценности.

00:06:36.817 --> 00:06:40.147
Я подразумеваю
не сентиментальные ценности,

00:06:40.171 --> 00:06:43.958
а любые представления людей

00:06:43.982 --> 00:06:45.325
об идеальной жизни.

00:06:47.364 --> 00:06:49.673
И это правило нарушает закон Азимова,

00:06:49.697 --> 00:06:52.026
который гласит: робот
должен защищать себя.

00:06:52.050 --> 00:06:55.773
Он вообще не заинтересован
в сохранении своей жизни.

00:06:57.420 --> 00:07:01.188
Второе, правило покорности.

00:07:01.974 --> 00:07:05.717
Это действительно важно для того,
чтобы обезопасить нас от роботов.

00:07:05.741 --> 00:07:08.787
Оно гласит: робот не знает,

00:07:08.787 --> 00:07:10.935
что именно относится
к человеческим ценностям,

00:07:10.959 --> 00:07:14.137
он должен приумножать их,
не зная, что они из себя представляют.

00:07:15.254 --> 00:07:17.880
Это позволяет избежать
проблемы концентрации

00:07:17.904 --> 00:07:19.116
на одной цели.

00:07:19.140 --> 00:07:21.312
Эта неопределённость оказывается решающей.

00:07:21.726 --> 00:07:23.365
Но, чтобы приносить пользу,

00:07:23.389 --> 00:07:26.120
у него должно быть
общее понимание наших желаний.

00:07:27.223 --> 00:07:32.650
Он узнаёт об этом, в первую очередь,
из решений, принимаемых людьми,

00:07:32.674 --> 00:07:35.475
так что принятые нами решения
содержат информацию о том,

00:07:35.499 --> 00:07:38.799
какой мы хотим сделать нашу жизнь.

00:07:40.632 --> 00:07:42.315
Итак, вот эти три правила.

00:07:42.339 --> 00:07:44.657
Теперь рассмотрим,
как они применяются к вопросу,

00:07:44.681 --> 00:07:47.470
поставленному Тьюрингом:
«Можно ли отключить машину?»

00:07:49.073 --> 00:07:51.193
Итак, это робот PR2

00:07:51.217 --> 00:07:53.038
из нашей лаборатории.

00:07:53.062 --> 00:07:55.965
Прямо на спине у него
есть красная кнопка «Выкл».

00:07:56.541 --> 00:07:59.156
Вопрос: позволит ли он вам отключить его?

00:07:59.180 --> 00:08:00.645
Если мы пойдём старым путём,

00:08:00.669 --> 00:08:04.151
у него будет цель «Варить кофе;
я должен варить кофе;

00:08:04.175 --> 00:08:06.755
я не могу варить кофе, если я мёртв».

00:08:06.779 --> 00:08:10.120
Очевидно, PR2 слушал моё выступление,

00:08:10.144 --> 00:08:13.897
и поэтому он говорит:
«Я должен сломать свой выключатель

00:08:14.976 --> 00:08:17.670
и вырубить работников Starbucks,

00:08:17.694 --> 00:08:19.254
которые могут мне помешать».

00:08:19.278 --> 00:08:21.340
(Смех)

00:08:21.364 --> 00:08:23.341
Это кажется неизбежным, не так ли?

00:08:23.341 --> 00:08:26.009
Кажется, этого фатального режима
невозможно избежать,

00:08:26.009 --> 00:08:29.506
он следует из конкретно определённой цели.

00:08:30.812 --> 00:08:33.956
Что же случится, если у машины
не будет чётко поставленной цели?

00:08:33.980 --> 00:08:36.107
Тогда она будет рассуждать по-другому

00:08:36.131 --> 00:08:38.555
и скажет: «Человек может отключить меня,

00:08:39.144 --> 00:08:41.010
но только если я допущу ошибку.

00:08:41.747 --> 00:08:44.222
Я точно не знаю, что правильно, а что нет,

00:08:44.246 --> 00:08:46.290
но я точно знаю, что не хочу ошибаться».

00:08:46.314 --> 00:08:49.324
Здесь действуют первые два правила.

00:08:49.348 --> 00:08:52.707
«Следовательно, я должен
разрешить людям отключить меня».

00:08:53.721 --> 00:08:57.677
Фактически вы можете
просчитать стимул, благодаря которому

00:08:57.701 --> 00:09:00.194
робот позволит людям отключить себя,

00:09:00.218 --> 00:09:02.132
и этот стимул тесно связан

00:09:02.156 --> 00:09:04.902
с неопределённостью поставленной цели.

00:09:05.977 --> 00:09:08.926
А когда машина отключена,

00:09:08.950 --> 00:09:10.755
начинает действовать третье правило.

00:09:10.779 --> 00:09:13.841
Она узнаёт что-то о целях,
которые должна преследовать,

00:09:13.865 --> 00:09:16.398
потому что понимает,
что сделала что-то не так.

00:09:16.422 --> 00:09:19.992
По сути, мы, подобно математикам,

00:09:20.016 --> 00:09:22.147
можем применить греческие символы

00:09:22.171 --> 00:09:24.155
для доказательства теоремы,

00:09:24.179 --> 00:09:27.732
согласно которой именно такой робот
принесёт человеку пользу.

00:09:27.756 --> 00:09:31.559
Вероятно, вам будет лучше
работать с машиной, которая разработана

00:09:31.583 --> 00:09:32.829
с учётом этого момента.

00:09:33.237 --> 00:09:36.143
Данный пример очень прост,

00:09:36.167 --> 00:09:40.070
но это лишь начало наших попыток
разработки робота-компаньона.

00:09:42.657 --> 00:09:45.914
Итак, думаю, третье правило

00:09:45.938 --> 00:09:49.050
озадачивает вас.

00:09:49.074 --> 00:09:52.313
Вероятно, вы думаете:
«Допустим, я поступаю плохо,

00:09:52.337 --> 00:09:55.266
я не хочу, чтобы робот
поступал так же, как я.

00:09:55.290 --> 00:09:58.724
По ночам я пробираюсь
к холодильнику за едой.

00:09:58.748 --> 00:09:59.916
Я делаю то-то и то-то».

00:09:59.940 --> 00:10:02.721
Вы бы не хотели, чтобы робот
повторял за вами всё.

00:10:02.731 --> 00:10:04.832
На самом деле система
работает немного иначе.

00:10:04.856 --> 00:10:07.011
Робот не будет копировать

00:10:07.035 --> 00:10:09.658
ваше плохое поведение.

00:10:09.682 --> 00:10:13.592
Он попытается понять ваши мотивы
и, возможно, поможет вам противостоять им

00:10:13.616 --> 00:10:14.936
в случае необходимости.

00:10:16.206 --> 00:10:17.670
Но это сложная задача.

00:10:18.302 --> 00:10:20.847
Фактически мы пытаемся

00:10:20.871 --> 00:10:26.667
разрешить машинам прогнозировать
все возможные варианты

00:10:26.691 --> 00:10:27.852
своей жизни

00:10:27.876 --> 00:10:29.473
и жизни каждого человека.

00:10:29.497 --> 00:10:32.014
Какую жизнь они бы предпочли?

00:10:34.061 --> 00:10:37.015
Этот процесс сопряжён
с множеством затруднений.

00:10:37.039 --> 00:10:39.971
Я не ожидаю, что с ними
можно справиться быстро.

00:10:39.995 --> 00:10:42.638
Главная сложность
заключается именно в нас.

00:10:44.149 --> 00:10:47.266
Как я уже упоминал, мы ведём себя плохо.

00:10:47.290 --> 00:10:49.611
Некоторые из нас просто ужасны.

00:10:50.431 --> 00:10:53.483
Но роботу, как я уже говорил,
не нужно копировать поведение.

00:10:53.507 --> 00:10:56.298
У робота нет своих собственных целей.

00:10:56.322 --> 00:10:58.059
Он абсолютно альтруистичный

00:10:59.293 --> 00:11:04.514
и создан не только для удовлетворения
потребностей своего пользователя,

00:11:04.538 --> 00:11:07.676
он должен уважать предпочтения каждого.

00:11:09.263 --> 00:11:11.833
Он может справиться с плохими поступками

00:11:11.857 --> 00:11:15.558
и даже понять, что вы поступаете плохо,

00:11:15.582 --> 00:11:18.253
например, берёте взятки,
работая паспортистом,

00:11:18.277 --> 00:11:22.089
потому что вам нужно кормить семью
и оплачивать обучение детей.

00:11:22.113 --> 00:11:25.019
Он может это понять, и это не значит,
что он будет воровать.

00:11:25.043 --> 00:11:27.722
По сути, это лишь помогает вам
оплатить учёбу детей.

00:11:28.976 --> 00:11:31.988
Наши вычислительные
способности также ограничены.

00:11:32.012 --> 00:11:34.517
Ли Седоль прекрасно играет в го,

00:11:34.541 --> 00:11:35.866
но он всё равно проиграл.

00:11:35.890 --> 00:11:40.129
Сделанные им ходы привели к поражению.

00:11:40.153 --> 00:11:42.314
Это не значит, что он хотел проиграть.

00:11:43.340 --> 00:11:45.380
Чтобы понять его поведение,

00:11:45.404 --> 00:11:49.048
нам следует смотреть сквозь призму
человеческого познания, включающего

00:11:49.072 --> 00:11:54.049
наши вычислительные ограничения
и являющегося очень сложной моделью.

00:11:54.073 --> 00:11:57.066
Но нам всё ещё есть над чем поразмыслить.

00:11:57.876 --> 00:12:02.196
С моей точки зрения,
самым сложным в исследовании ИИ

00:12:02.220 --> 00:12:04.795
является тот факт, что нас очень много,

00:12:06.294 --> 00:12:09.875
и поэтому машине нужно идти
на компромисс, сопоставлять пожелания

00:12:09.899 --> 00:12:12.124
множества разных людей,

00:12:12.148 --> 00:12:14.054
и для этого существуют разные способы.

00:12:14.078 --> 00:12:17.767
Экономисты, социологи,
нравственные философы поняли это,

00:12:17.791 --> 00:12:20.246
и мы стараемся активно сотрудничать.

00:12:20.270 --> 00:12:23.521
Давайте посмотрим, что произойдёт,
если вы допу́стите ошибку.

00:12:23.545 --> 00:12:25.678
Итак, например, вы беседуете

00:12:25.702 --> 00:12:27.646
с умным личным помощником,

00:12:27.670 --> 00:12:29.955
который может появиться
через несколько лет.

00:12:29.979 --> 00:12:32.503
Представьте Siri на стероидах.

00:12:33.627 --> 00:12:37.949
Siri говорит: «Звонила ваша жена,
чтобы напомнить о сегодняшнем вечере».

00:12:38.616 --> 00:12:41.124
И, разумеется, вы забыли.
«Что? Какой ужин?

00:12:41.148 --> 00:12:42.573
О чём ты говоришь?»

00:12:42.597 --> 00:12:46.343
«Ваша 20-я годовщина в 19:00».

00:12:48.915 --> 00:12:52.634
«Я не могу, у меня встреча
с генеральным секретарём в 19:30.

00:12:52.658 --> 00:12:54.350
Как такое могло случиться?»

00:12:54.374 --> 00:12:59.034
«Я предупреждала вас,
но вы проигнорировали мой совет».

00:13:00.146 --> 00:13:03.474
«Что же мне делать? Я же не могу
просто сказать ему, что занят».

00:13:04.490 --> 00:13:07.771
«Не беспокойтесь. Я задержала его рейс».

00:13:07.795 --> 00:13:09.477
(Смех)

00:13:10.249 --> 00:13:12.350
«Какая-то компьютерная неисправность».

00:13:12.374 --> 00:13:13.586
(Смех)

00:13:13.610 --> 00:13:15.227
«Ты и такое можешь сделать?»

00:13:16.400 --> 00:13:18.579
«Он приносит глубочайшие извинения

00:13:18.603 --> 00:13:21.158
и приглашает вас на обед завтра».

00:13:21.182 --> 00:13:22.481
(Смех)

00:13:22.505 --> 00:13:26.908
Итак, есть небольшая ошибка
в оценке приоритетов.

00:13:26.932 --> 00:13:29.941
Чёткое следование ценностям
моей жены и её принципу

00:13:29.965 --> 00:13:32.144
«Счастливая жена — счастливая жизнь».

00:13:32.144 --> 00:13:33.641
(Смех)

00:13:33.665 --> 00:13:35.109
Могло быть и по-другому.

00:13:35.821 --> 00:13:38.022
Вы приходите домой
после тяжёлого рабочего дня,

00:13:38.046 --> 00:13:40.241
а компьютер спрашивает: «Трудный день?»

00:13:40.265 --> 00:13:42.553
«Даже не успел пообедать».

00:13:42.577 --> 00:13:43.859
«Думаю, вы очень голодны».

00:13:43.883 --> 00:13:46.529
«Умираю с голоду.
Можешь приготовить ужин?»

00:13:48.070 --> 00:13:50.160
«Я должна кое-что вам сказать».

00:13:50.184 --> 00:13:51.339
(Смех)

00:13:52.193 --> 00:13:57.098
«Люди в Южном Судане нуждаются
в помощи больше, чем вы».

00:13:57.122 --> 00:13:58.226
(Смех)

00:13:58.250 --> 00:14:00.325
«Так что я ухожу.
Готовьте себе ужин сами».

00:14:00.349 --> 00:14:02.349
(Смех)

00:14:02.823 --> 00:14:04.562
Итак, нам нужно решить эти проблемы,

00:14:04.586 --> 00:14:07.101
и мне не терпится поработать над ними.

00:14:07.125 --> 00:14:08.968
Для оптимизма есть основания.

00:14:08.992 --> 00:14:10.151
Во-первых,

00:14:10.175 --> 00:14:12.043
собран огромный объём данных.

00:14:12.067 --> 00:14:14.861
Помните, я говорил, что они прочитают всё,

00:14:14.885 --> 00:14:16.431
что когда-либо написали люди?

00:14:16.455 --> 00:14:19.179
В основном мы писали о том,
как одни люди что-либо делают,

00:14:19.203 --> 00:14:21.117
а другие расстраиваются по этому поводу.

00:14:21.141 --> 00:14:23.539
Так что у нас есть множество
данных для изучения.

00:14:23.563 --> 00:14:25.799
Также есть важный экономический стимул

00:14:27.331 --> 00:14:28.517
сделать всё правильно.

00:14:28.541 --> 00:14:30.542
Представьте вашего робота у себя дома.

00:14:30.566 --> 00:14:33.673
Вы снова задерживаетесь на работе,
и робот должен накормить детей,

00:14:33.673 --> 00:14:36.480
а кормить их нечем,
потому что холодильник пуст.

00:14:36.504 --> 00:14:39.109
И робот видит кошку.

00:14:39.133 --> 00:14:40.825
(Смех)

00:14:40.849 --> 00:14:45.039
Робот не изучил сущность
человеческих ценностей

00:14:45.063 --> 00:14:46.314
и не понимает,

00:14:46.338 --> 00:14:51.182
что сентиментальная ценность кошки
превосходит её питательную ценность.

00:14:51.206 --> 00:14:52.301
(Смех)

00:14:52.325 --> 00:14:54.073
Что происходит тогда?

00:14:54.097 --> 00:14:57.394
Что вроде этого:

00:14:57.418 --> 00:15:00.382
«Ненормальный робот готовит
котёнка на семейный ужин».

00:15:00.406 --> 00:15:04.929
Этот инцидент положил бы конец
производству домашних роботов.

00:15:04.953 --> 00:15:08.325
Так что у нас есть огромный стимул
сделать всё правильно

00:15:08.349 --> 00:15:11.064
до того, как мы создадим
суперумную машину.

00:15:12.128 --> 00:15:13.663
Итак, подытожим.

00:15:13.687 --> 00:15:16.568
По сути, я пытаюсь
изменить определение ИИ,

00:15:16.592 --> 00:15:19.585
чтобы наши машины
были действительно полезными.

00:15:19.609 --> 00:15:20.831
Основные моменты:

00:15:20.855 --> 00:15:22.253
это альтруистичные машины,

00:15:22.277 --> 00:15:25.081
которые хотят добиваться лишь наших целей,

00:15:25.105 --> 00:15:28.221
но не знают, каких именно целей,

00:15:28.245 --> 00:15:30.243
и будут наблюдать за всеми нами,

00:15:30.267 --> 00:15:33.470
чтобы узнать, чего мы на самом деле хотим.

00:15:34.373 --> 00:15:37.932
Надеюсь, в процессе мы сами станем лучше.

00:15:37.956 --> 00:15:39.147
Большое спасибо.

00:15:39.171 --> 00:15:42.880
(Аплодисменты)

00:15:42.904 --> 00:15:44.772
Крис Андерсон: Очень интересно, Стюарт.

00:15:44.796 --> 00:15:47.950
Мы задержимся здесь немного,
потому что, сейчас идёт подготовка

00:15:47.950 --> 00:15:49.141
к следующему выступлению.

00:15:49.165 --> 00:15:50.703
Пара вопросов.

00:15:50.727 --> 00:15:56.180
Идея программирования незнания
подсознательно кажется очень мощной.

00:15:56.204 --> 00:15:57.798
Что помешает роботу,

00:15:57.822 --> 00:16:00.080
получившему суперразум,

00:16:00.104 --> 00:16:02.956
почитать книги и обнаружить,
что на самом деле знания

00:16:02.980 --> 00:16:04.552
лучше невежества,

00:16:04.576 --> 00:16:08.794
начать следовать другим целям
и перепрограммировать себя?

00:16:09.692 --> 00:16:16.048
Стюарт Рассел: Как я уже сказал,
мы хотим, чтобы роботы узнали больше

00:16:16.072 --> 00:16:17.359
о наших целях.

00:16:17.383 --> 00:16:22.904
Они станут более определёнными
лишь когда станут правильными,

00:16:22.928 --> 00:16:24.873
робот будет запрограммирован

00:16:24.897 --> 00:16:27.621
правильно понимать полученные данные.

00:16:27.645 --> 00:16:31.601
Например, он будет понимать,
что книги описывают мир

00:16:31.625 --> 00:16:33.148
довольно однобоко.

00:16:33.148 --> 00:16:35.529
Они повествуют лишь о королях, принцах

00:16:35.553 --> 00:16:38.353
и знатных белых мужчинах,
которые делают свою работу.

00:16:38.377 --> 00:16:40.473
Это сложная проблема,

00:16:40.497 --> 00:16:44.369
но чем больше он будет
узнавать о наших целях,

00:16:44.393 --> 00:16:46.456
тем больше пользы он будет приносить нам.

00:16:46.480 --> 00:16:49.006
КА: Вы не можете свести
всё к одному закону,

00:16:49.030 --> 00:16:50.680
который бы гласил:

00:16:50.704 --> 00:16:53.997
«Если когда-нибудь человек
попытается меня выключить,

00:16:54.021 --> 00:16:55.956
я подчинюсь. Я подчинюсь»?

00:16:55.980 --> 00:16:57.162
СР: Абсолютно исключено.

00:16:57.186 --> 00:16:58.685
Это была бы ужасная идея.

00:16:58.709 --> 00:17:01.398
Представьте, что у вас есть
беспилотный автомобиль,

00:17:01.422 --> 00:17:03.855
и вы хотите отправить на нём
своего пятилетнего сына

00:17:03.879 --> 00:17:05.053
в детский сад.

00:17:05.077 --> 00:17:08.178
Хотели бы вы, чтобы ваш пятилетний сын
мог отключить машину,

00:17:08.202 --> 00:17:09.415
когда она едет?

00:17:09.439 --> 00:17:10.598
Вероятно, нет.

00:17:10.622 --> 00:17:15.325
Поэтому нужно понимать степень
рациональности и здравомыслия личности.

00:17:15.349 --> 00:17:17.025
Чем более рационален человек,

00:17:17.049 --> 00:17:19.152
тем вероятнее робот согласится
на отключение.

00:17:19.176 --> 00:17:21.719
Если это случайный человек
или даже злоумышленник,

00:17:21.743 --> 00:17:24.255
он отключится с меньшей охотой.

00:17:24.279 --> 00:17:26.145
КА: Хорошо Стюарт. Я очень надеюсь,

00:17:26.169 --> 00:17:28.483
что вы разберётесь с этим ради нас.

00:17:28.507 --> 00:17:30.942
Большое спасибо за выступление.
Это было потрясающе.

00:17:30.942 --> 00:17:32.073
СР: Спасибо.

00:17:32.097 --> 00:17:33.934
(Аплодисменты)

