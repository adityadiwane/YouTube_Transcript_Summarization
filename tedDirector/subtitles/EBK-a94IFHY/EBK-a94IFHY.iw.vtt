WEBVTT
Kind: captions
Language: iw

00:00:00.000 --> 00:00:07.000
מתרגם: Igal Opendik
מבקר: Ido Dekkers

00:00:12.532 --> 00:00:14.084
זהו לי סידול.

00:00:14.108 --> 00:00:18.105
הוא אחד מגדולי שחקני גו בעולם.

00:00:18.129 --> 00:00:21.014
כאן הוא חווה את הרגע 
שחבריי מעמק הסיליקון מכנים

00:00:21.038 --> 00:00:22.548
"זה הזוי!" -

00:00:22.572 --> 00:00:23.645
(צחוק)

00:00:23.669 --> 00:00:25.857
הרגע בו אנו מבינים

00:00:25.881 --> 00:00:29.177
שהתפתחותה של הב''מ (בינה מלאכתית)
מתקדמת הרבה יותר מהר משציפינו.

00:00:29.974 --> 00:00:33.021
אז בני האנוש הפסידו במשחק גו. 
אבל מה עם העולם האמיתי?

00:00:33.045 --> 00:00:35.145
ובכן, העולם האמיתי הרבה יותר גדול,

00:00:35.169 --> 00:00:37.418
הרבה יותר מורכב ממשחק גו.

00:00:37.442 --> 00:00:39.261
זה פחות נגלה לעין,

00:00:39.285 --> 00:00:41.323
אבל זו עדיין בעיית קבלת החלטות.

00:00:42.768 --> 00:00:45.089
ואם חושבים על כמה טכנולוגיות

00:00:45.113 --> 00:00:46.862
שמתממשות כנגד עיניינו...

00:00:47.558 --> 00:00:51.893
נוריקו [אראי] הזכירה שמכונות
עדיין לא יודעות לקרוא,

00:00:51.917 --> 00:00:53.417
לפחות לקרוא ולהבין.

00:00:53.441 --> 00:00:54.977
אבל, זה יקרה.

00:00:55.001 --> 00:00:56.772
וכאשר זה כן יקרה,

00:00:56.796 --> 00:00:57.983
עד מהרה

00:00:58.007 --> 00:01:02.579
הן תקראנה את כל מה שהאנושות
כתבה אי פעם.

00:01:03.670 --> 00:01:05.700
זה יקנה למכונות יכולת חדשה,

00:01:05.724 --> 00:01:08.644
לצד יכולת החיזוי מעבר 
למה שבני האנוש מסוגלים לחזות,

00:01:08.668 --> 00:01:10.348
כפי שנוכחנו לדעת במשחק גו,

00:01:10.372 --> 00:01:12.536
אם תקבלנה גישה ליותר מידע,

00:01:12.560 --> 00:01:16.828
הן תוכלנה לקבל החלטות טובות יותר
מאיתנו בעולם האמיתי.

00:01:18.612 --> 00:01:20.218
האם זה טוב לנו?

00:01:21.718 --> 00:01:23.950
ובכן, אני מקווה שכן.

00:01:26.514 --> 00:01:29.769
הציביליזציה שלנו על כל ערכיה,

00:01:29.793 --> 00:01:31.861
מבוססת על התבונה שלנו.

00:01:31.885 --> 00:01:35.579
ולו היתה לנו גישה לתבונה רבה יותר,

00:01:35.603 --> 00:01:38.905
אזי לא יהיה גבול למה 
שהאנושות תוכל לעשות.

00:01:40.485 --> 00:01:43.810
ואני סבור כי זה היה יכול להיות,
כפי שאנשים מסוימים תיארו זאת,

00:01:43.834 --> 00:01:45.850
הארוע הגדול בתולדות האנושות.

00:01:48.485 --> 00:01:51.314
אז מדוע אנשים אומרים דברים כגון,

00:01:51.338 --> 00:01:54.214
ב''מ עלולה לגרום לסוף האנושות?

00:01:55.258 --> 00:01:56.917
האם זה חדש לנו?

00:01:56.941 --> 00:02:01.051
האם אלה רק אלון מאסק, 
ביל גייטס וסטיבן הוקינג?

00:02:01.773 --> 00:02:05.035
לא. רעיון זה כבר קיים זמן מה.

00:02:05.059 --> 00:02:07.021
הרי הציטוט:

00:02:07.045 --> 00:02:11.395
"אפילו אם היינו מסוגלים לשלוט
במכונות כבמשרתים בלבד,

00:02:11.419 --> 00:02:14.403
למשל על ידי כיבוי אספקת חשמל
ברגעים קריטיים" --

00:02:14.427 --> 00:02:17.664
אחזור לנושא "כיבוי החשמל"
בהמשך --

00:02:17.688 --> 00:02:20.492
"אנחנו כמין צריכים להרגיש ענווה גדולה."

00:02:21.997 --> 00:02:25.445
מי אמר זאת?
אלן טורינג ב-1951.

00:02:26.120 --> 00:02:28.883
כידוע לכם, אלן טיורינג הוא אבי מדע המחשב

00:02:28.907 --> 00:02:31.955
ובמובנים רבים, גם אבי הב''מ.

00:02:33.059 --> 00:02:34.941
אם חושבים על הבעייתיות שביצירת

00:02:34.965 --> 00:02:38.752
משהו שחכם יותר ממינך שלך,

00:02:38.776 --> 00:02:41.398
ניתן לכנות זאת "בעיית הגורילה",

00:02:42.165 --> 00:02:45.915
היות ואבות אבותיהן של הגורילות 
עשו זאת לפני מיליוני שנה,

00:02:45.939 --> 00:02:47.684
וכיום נוכל לשאול את הגורילות:

00:02:48.572 --> 00:02:49.732
האם זה היה רעיון טוב?

00:02:49.756 --> 00:02:53.286
הינה הם עורכים דיון
על טיב הדבר

00:02:53.310 --> 00:02:56.656
ואחרי זמן מה הם קובעים שזה- לא!

00:02:56.680 --> 00:02:58.025
זה היה רעיון נוראי.

00:02:58.049 --> 00:02:59.831
מין הגורילות במצוקה קשה.

00:03:00.358 --> 00:03:04.621
למעשה, אתם יכולים לראות
את העצב הקיומי בעיניהן.

00:03:04.645 --> 00:03:06.285
(צחוק)

00:03:06.309 --> 00:03:11.149
לגבי ההרגשה המבחילה הזאת 
שיצירת משהו שעולה עליך בחוכמתו

00:03:11.173 --> 00:03:13.538
איננו רעיון טוב --

00:03:14.308 --> 00:03:15.799
מה עושים איתה?

00:03:15.823 --> 00:03:20.590
האמת היא שכלום,
חוץ מעצירת פיתוח ב''מ.

00:03:20.614 --> 00:03:23.124
בגלל כל היתרונות שהזכרתי

00:03:23.148 --> 00:03:24.864
ובגלל שאני חוקר ב''מ,

00:03:24.888 --> 00:03:26.679
אני לא מקבל את אפשרות העצירה.

00:03:27.103 --> 00:03:29.571
אני כן מעוניין להמשיך בפיתוח ב''מ.

00:03:30.435 --> 00:03:33.113
לכן, אנחנו חייבים להגדיר 
את הבעיה טוב יותר.

00:03:33.137 --> 00:03:34.508
מהי מהות הבעיה בדיוק?

00:03:34.532 --> 00:03:37.778
למה ב''מ טובה יותר
עלולה להוות אסון?

00:03:39.218 --> 00:03:40.716
אז הינה לכם עוד ציטוט:

00:03:41.755 --> 00:03:45.090
"עדיף שנהיה בטוחים 
שהיעוד שאנחנו מטמיעים במכונה

00:03:45.114 --> 00:03:47.412
זה אותו היעוד שאנחנו חפצים בו באמת ".

00:03:48.102 --> 00:03:51.600
זה נאמר על ידי נורברט וויינר ב-1960

00:03:51.624 --> 00:03:55.626
זמן קצר אחרי שצפה 
באחת ממערכות הלמידה המוקדמות

00:03:55.650 --> 00:03:58.233
לומדת לשחק דמקה
טוב מיוצרה.

00:04:00.422 --> 00:04:03.105
אימרה זו היתה גם יכולה להיאמר

00:04:03.129 --> 00:04:04.296
על ידי המלך מידאס.

00:04:04.903 --> 00:04:08.037
המלך מידאס אמר, "אני רוצה שכל דבר 
שאגע בו יהפוך לזהב".

00:04:08.061 --> 00:04:10.534
והוא קיבל את מבוקשו בדיוק.

00:04:10.558 --> 00:04:13.309
זה היעוד אותו הוא הטמיע במכונה,

00:04:13.333 --> 00:04:14.783
כביכול,

00:04:14.807 --> 00:04:18.251
וכל המזון והמשקה שלו
וכל קרוביו הפכו לזהב

00:04:18.275 --> 00:04:20.556
והוא מת אומלל ומורעב.

00:04:22.264 --> 00:04:24.605
נקרא לכך "בעית מלך מידאס".

00:04:24.629 --> 00:04:27.934
מתן משימה אשר לא עולה

00:04:27.958 --> 00:04:30.371
בקנה אחד עם מה שאנו רוצים.

00:04:30.395 --> 00:04:33.648
במושגים מודרניים אנו קוראים לזה
"בעיית תאום ערכים".

00:04:36.867 --> 00:04:40.352
מתן מטרה שגויה
אינה המרכיב היחיד של הבעיה.

00:04:40.376 --> 00:04:41.528
יש מרכיב נוסף.

00:04:41.980 --> 00:04:43.923
אם אתם מטמיעים משימה במכונה

00:04:43.947 --> 00:04:46.395
אפילו משהו פשוט כמו 
"תביאי לי קפה",

00:04:47.728 --> 00:04:49.569
המכונה אומרת לעצמה,

00:04:50.553 --> 00:04:53.176
"מה עלול למנוע ממני 
להביא את הקפה?

00:04:53.200 --> 00:04:54.780
מישהו עלול לנתק אותי מהחשמל.

00:04:55.465 --> 00:04:57.852
לכן אני חייבת לנקוט בצעדים שימנעו זאת.

00:04:57.876 --> 00:04:59.782
אנטרל את מתג הכיבוי שלי.

00:05:00.354 --> 00:05:03.313
אעשה הכל כדי להגן על עצמי
כנגד מה שימנע ממני

00:05:03.337 --> 00:05:05.966
לבצע את המשימה שניתנה לי".

00:05:05.990 --> 00:05:08.002
לכן, חתירה חד-כיוונית למשימה

00:05:09.033 --> 00:05:11.978
בתצורה הגנתית המבטיחה את
ביצוע המשימה, אשר

00:05:12.002 --> 00:05:14.816
איננה תואמת משימות אמיתיות של האנושות --

00:05:15.942 --> 00:05:17.804
זו הבעיה אתה אנו מתמודדים.

00:05:18.827 --> 00:05:23.594
זו התובנה בעלת ערך
אותה אנו נקח מהרצאה זו.

00:05:23.618 --> 00:05:25.673
אם תרצו לזכור דבר אחד --

00:05:25.697 --> 00:05:28.372
-- לא ניתן להביא את הקפה אם אתה מת.

00:05:28.396 --> 00:05:29.457
(צחוק)

00:05:29.481 --> 00:05:33.310
זה פשוט מאוד. רק תזכרו את זה.
שננו זאת לעצמכם שלוש פעמים ביום.

00:05:33.334 --> 00:05:35.155
(צחוק)

00:05:35.179 --> 00:05:37.933
זוהי העלילה המדויקת של

00:05:37.957 --> 00:05:40.605
"2001: אודיסאה בחלל"

00:05:41.046 --> 00:05:43.136
ל-HAL יש משימה,

00:05:43.160 --> 00:05:46.892
אשר איננה מתואמת עם 
משימות של בני האנוש,

00:05:46.916 --> 00:05:48.726
וזה מוביל לקונפליקט.

00:05:49.314 --> 00:05:52.283
למרבה המזל HAL איננו בעל תבונת-על.

00:05:52.307 --> 00:05:55.894
הוא די חכם,
אבל לבסוף דייב מצליח להערים עליו

00:05:55.918 --> 00:05:57.767
ומצליח לכבות אותו.

00:06:01.428 --> 00:06:03.627
אבל אנחנו עלולים לא
להיות ברי מזל באותה מידה.

00:06:08.013 --> 00:06:09.605
אז מה נעשה?

00:06:12.191 --> 00:06:14.792
אני מנסה להגדיר את הב''מ במושגים אחרים

00:06:14.816 --> 00:06:16.877
כדי להתנתק מן ההעקרון הקלאסי

00:06:16.901 --> 00:06:21.468
לפיו מכונות שואפות
לבצע משימות בצורה תבונתית.

00:06:22.532 --> 00:06:24.330
ישנם שלושה עקרונות בבסיס העניין.

00:06:24.354 --> 00:06:27.643
הראשון הוא, אם תרצו, עקרון האלטרויזם --

00:06:27.667 --> 00:06:30.929
-- המשימה היחידה של הרובוטים היא

00:06:30.953 --> 00:06:35.199
מיקסום הגשמתן של המטרות של בני אנוש,

00:06:35.223 --> 00:06:36.527
של ערכי בני אנוש.

00:06:36.527 --> 00:06:39.737
באומרי "ערכים" אינני מתכוון כאן
לערכים נשגבים של יפי נפש.

00:06:39.737 --> 00:06:43.642
אני פשוט מתכוון לכל מה שבני אנוש

00:06:43.642 --> 00:06:45.145
מעדיפים בחייהם.

00:06:47.184 --> 00:06:49.493
זה למעשה נוגד לחוק אסימוב

00:06:49.517 --> 00:06:51.846
לפיו רובוט חייב להגן
על קיומו.

00:06:51.870 --> 00:06:55.593
אין לו עניין בשימור קיומו בכלל.

00:06:57.240 --> 00:07:01.008
העקרון השני, אם תרצו,
הוא עקרון הענווה.

00:07:01.794 --> 00:07:05.537
מתברר שהוא חשוב מאוד
ליצירת רובוטים בטוחים.

00:07:05.561 --> 00:07:08.703
לפיו רובוט איננו יודע

00:07:08.727 --> 00:07:10.755
מהם ערכי בני האנוש.

00:07:10.779 --> 00:07:13.957
הוא חייב לממש אותם על הצד הטוב ביותר,
אבל הוא לא יודע מה הם.

00:07:15.074 --> 00:07:17.700
זה עוקף את בעיית חתירה חד-כיוונית

00:07:17.724 --> 00:07:18.936
למשימה.

00:07:18.960 --> 00:07:21.132
אי הוודאות הזאת הופכת לעניין מכריע.

00:07:21.546 --> 00:07:23.185
בכדי שרובוט יהיה מועיל לנו

00:07:23.209 --> 00:07:25.940
עליו לדעת מה אנו רוצים, ברמה כלשהי.

00:07:27.043 --> 00:07:32.470
הוא מקבל מידע זה בעיקר
מצפיה בבחירות אנושיות,

00:07:32.494 --> 00:07:35.295
כך שהבחירות שלנו מלמדות מידע

00:07:35.319 --> 00:07:38.619
על כיצד אנו מעדיפים שחיינו יהיו.

00:07:40.452 --> 00:07:42.135
אלה הם שלושת העקרונות.

00:07:42.159 --> 00:07:44.477
הבה נראה כיצד הם מיושמים בבעיה של:

00:07:44.501 --> 00:07:47.290
"האם אתה יכול לכבות את המכונה?"
כפי שהציע טיורינג.

00:07:48.893 --> 00:07:51.013
הינה רובוט מדגם PR2 --

00:07:51.037 --> 00:07:52.858
-- אחד מאלה שיש לנו במעבדה

00:07:52.882 --> 00:07:55.785
ויש לו על הגב כפתור כיבוי גדול ואדום.

00:07:56.361 --> 00:07:58.976
השאלה היא: האם הוא ירשה לך לכבות אותו?

00:07:59.000 --> 00:08:00.465
אם נלך לפי המודל הקלאסי,

00:08:00.489 --> 00:08:03.971
כך הוא יבצע את משימת "תביא את הקפה":
"אני חייב להביא את הקפה,

00:08:03.995 --> 00:08:06.575
אני לא יכול להביא את הקפה אם אני מת",

00:08:06.599 --> 00:08:09.940
והיות ו - PR2
כמובן הקשיב להרצאתי

00:08:09.964 --> 00:08:13.717
הוא אומר איפוא,
"אני חייב לנטרל את מתג הכיבוי שלי

00:08:14.796 --> 00:08:17.490
ואולי גם לחשמל את כל האנשים
בסטארבקס

00:08:17.514 --> 00:08:19.074
אשר עלולים להפריע לי".

00:08:19.098 --> 00:08:21.160
(צחוק)

00:08:21.184 --> 00:08:23.337
נראה בלתי נמנע, נכון?

00:08:23.361 --> 00:08:25.759
אופן פעולה זה נראה בלתי נמנע

00:08:25.783 --> 00:08:29.326
בגלל שהוא נובע ממתן משימה
מוגדרת וסופית.

00:08:30.632 --> 00:08:33.776
אבל מה יקרה אם המכונה
לא בטוחה לגבי המשימה?

00:08:33.800 --> 00:08:35.927
במקרה זה היא חושבת בצורה שונה.

00:08:35.951 --> 00:08:38.375
היא אומרת. "טוב, בן אנוש 
עלול לכבות אותי,

00:08:38.964 --> 00:08:40.830
אבל זה רק אם אעשה משהו לא נכון.

00:08:41.567 --> 00:08:44.042
אינני יודעת מהו לא נכון,

00:08:44.066 --> 00:08:46.110
אבל אני יודעת שאינני רוצה לעשות אותו."

00:08:46.134 --> 00:08:49.144
כך, יש לנו פה
העקרון הראשון והשני גם יחד.

00:08:49.168 --> 00:08:52.527
"לכן, אני צריכה
לאפשר לבני אנוש לכבות אותי".

00:08:53.541 --> 00:08:57.497
ניתן להטמיע את שיקול הרווח שיהיה לרובוט

00:08:57.521 --> 00:09:00.014
כדי שיאפשר לנו לכבות אותו.

00:09:00.038 --> 00:09:01.952
זה ישירות קשור ברמת

00:09:01.976 --> 00:09:04.722
אי-הוודאות לגבי מהות המשימה.

00:09:05.797 --> 00:09:08.746
וכאשר המכונה כבר מכובה,

00:09:08.770 --> 00:09:10.575
העקרון השלישי נכנס למשחק.

00:09:10.599 --> 00:09:13.661
המכונה לומדת משהו אודות המשימות
שעליה לבצע,

00:09:13.685 --> 00:09:16.218
כי היא לומדת שמה שעשתה
היה לא נכון.

00:09:16.242 --> 00:09:19.812
באמצעות שימוש ראוי באותיות יווניות,

00:09:19.836 --> 00:09:21.967
כפי שנהוג אצל המתמטיקאים,

00:09:21.991 --> 00:09:23.975
ניתן להוכיח ההנחה

00:09:23.999 --> 00:09:27.552
שאומרת שרובוט כזה
הינו בהחלט מועיל לאנושות.

00:09:27.576 --> 00:09:31.379
עדיף לכם שהמכונה תהיה כזאת

00:09:31.403 --> 00:09:32.649
ולא אחרת.

00:09:33.057 --> 00:09:35.963
זוהי דוגמא מאוד פשוטה,
אבל זה הצעד הראשון

00:09:35.987 --> 00:09:39.890
לקראת מה שאנו מנסים לעשות
עם ב''מ מותאמת אנושות.

00:09:42.477 --> 00:09:45.734
העקרון השלישי הוא זה

00:09:45.758 --> 00:09:48.870
שלגביו אתם מהרהרים לדעתי.

00:09:48.894 --> 00:09:52.133
אתם בוודאי חושבים, 
"אני מתנהג לא כראוי.

00:09:52.157 --> 00:09:55.086
ואינני רוצה שהרובוט שלי יתנהג כמוני.

00:09:55.110 --> 00:09:58.544
אני מתגנב באמצע הלילה
ולוקח דברים מן המקרר.

00:09:58.568 --> 00:09:59.736
אני עושה את זה ואת זה."

00:09:59.760 --> 00:10:02.557
יש כל מיני דברים שלא תרצו
שהרובוט שלכם יעשה.

00:10:02.581 --> 00:10:04.652
אבל, הדברים לא בדיוק עובדים ככה.

00:10:04.676 --> 00:10:06.831
רק בגלל שאתם מתנהגים לא כראוי

00:10:06.855 --> 00:10:09.478
הרובוט שלכם לא בהכרח
יעתיק את התנהגותכם.

00:10:09.502 --> 00:10:13.412
הוא יבין את המניעים שלכם
ואולי יעזור לכם להתנגד להם,

00:10:13.436 --> 00:10:14.756
במידה וזה יהיה ראוי.

00:10:16.026 --> 00:10:17.490
אבל עדיין, העניין מסובך.

00:10:18.122 --> 00:10:20.667
מה שאנו מנסים לעשות,

00:10:20.691 --> 00:10:26.487
זה לאפשר למכונות לחזות
בעבור כל אדם ועבור כל מסלולי החיים

00:10:26.511 --> 00:10:27.672
שהוא עשוי לחיות,

00:10:27.696 --> 00:10:29.293
וכן חייהם של כל השאר:

00:10:29.317 --> 00:10:31.834
מה יעדיפו?

00:10:33.881 --> 00:10:36.835
וישנם הרבה מאוד קשיים בדרך לביצוע,

00:10:36.859 --> 00:10:39.791
אינני מצפה שהעניין ייפתר במהרה.

00:10:39.815 --> 00:10:42.458
הקשיים האמיתיים הם למעשה אנחנו.

00:10:43.969 --> 00:10:47.086
כפי שכבר ציינתי, אנחנו מתנהגים לא כראוי.

00:10:47.110 --> 00:10:49.431
למעשה, חלק מאיתנו פשוט נוראים.

00:10:50.251 --> 00:10:53.303
כפי שאמרתי הרובוט לא חייב
להעתיק את התנהגותינו.

00:10:53.327 --> 00:10:56.118
לרובוט אין כל משימה משלו.

00:10:56.142 --> 00:10:57.879
הוא אלטרויסט טהור.

00:10:59.113 --> 00:11:04.334
והוא לא עוצב לשם מימוש רצונות
של אדם אחד בלבד,המשתמש,

00:11:04.358 --> 00:11:07.496
אלא לקחת בחשבון את העדפות של כולם.

00:11:09.083 --> 00:11:11.653
כך, הוא יכול להתמודד עם
רמה מסוימת של רוע,

00:11:11.677 --> 00:11:15.378
והוא אפילו יכול להבין
את ההתנהגות הרעה שלך.

00:11:15.402 --> 00:11:18.073
למשל, אתה אולי 
לוקח שוחד בתור פקיד דרכונים,

00:11:18.097 --> 00:11:21.909
בגלל שאתה צריך להאכיל את משפחתך
ולשלוח את ילדיך לביה''ס.

00:11:21.933 --> 00:11:24.839
הוא יכול להבין זאת. 
אין זה אומר שהוא בעצמו יגנוב.

00:11:24.863 --> 00:11:27.542
ההבנה שלו רק תעזור לך
לשלוח את הילדים לב''ס.

00:11:28.796 --> 00:11:31.808
אנחנו גם מוגבלים בתחום החישובים.

00:11:31.832 --> 00:11:34.337
לי סדול הוא שחקן גו מבריק,

00:11:34.361 --> 00:11:35.686
אבל הוא עדיין הפסיד.

00:11:35.710 --> 00:11:39.949
אז אם נתבונן בצעדיו,
הוא עשה צעד שהוביל להפסד.

00:11:39.973 --> 00:11:42.134
זה לא אומר שהוא רצה להפסיד.

00:11:43.160 --> 00:11:45.200
לכן, על מנת להבין את התנהגותו,

00:11:45.224 --> 00:11:48.868
עלינו לרדת לפרטי המודל הקוגניטיבי האנושי

00:11:48.892 --> 00:11:53.869
שכולל את המגבלות החישוביות שלנו -
וזה מודל מסובך מאוד.

00:11:53.893 --> 00:11:56.886
אבל, זה עדיין משהו
שאנחנו יכולים לנסות להבין.

00:11:57.696 --> 00:12:02.016
אולי החלק הקשה ביותר עבורי כחוקר ב''מ 

00:12:02.040 --> 00:12:04.615
הוא העובדה שאנחנו רבים,

00:12:06.114 --> 00:12:09.695
ולכן המכונה חייבת איכשהו
לנתב ולקחת בחשבון את העדפות

00:12:09.719 --> 00:12:11.944
של המון אנשים שונים.

00:12:11.968 --> 00:12:13.874
וישנן דרכים שונות לעשות זאת.

00:12:13.898 --> 00:12:17.587
כלכלנים, סוציולוגים,
פילוסופים חוקרי מוסר הבינו זאת

00:12:17.611 --> 00:12:20.066
ואנחנו מחפשים את שיתוף הפעולה באופן פעיל.

00:12:20.090 --> 00:12:23.341
בוא נראה מה קורה
כאשר לא מבינים את זה נכונה.

00:12:23.365 --> 00:12:25.498
נגיד שאתם מנהלים שיחה

00:12:25.522 --> 00:12:27.466
עם העוזר האישי התבוני שלכם,

00:12:27.490 --> 00:12:29.775
אשר יכול להיות זמין בעוד כמה שנים.

00:12:29.799 --> 00:12:32.323
תחשבו על סירי על סטרואידים.

00:12:33.447 --> 00:12:37.769
אז סירי אומרת, "אישתך התקשרה להזכיר
על ארוחת הערב".

00:12:38.436 --> 00:12:40.944
וכמובן אתם שכחתם. "מה? איזה ארוחה?

00:12:40.968 --> 00:12:42.393
על מה אתה מדבר?"

00:12:42.417 --> 00:12:46.163
"אה, יום הנישואין ה-20 שלכם ב- 19:00"

00:12:48.735 --> 00:12:52.454
"אני לא יכול.
יש לי פגישה עם המנכ''ל ב- 19:30.

00:12:52.478 --> 00:12:54.170
איך זה היה יכול לקרות?"

00:12:54.194 --> 00:12:58.854
"הזכרתי לך, אבל אתה התעלמת מההמלצה שלי".

00:12:59.966 --> 00:13:03.294
"טוב, אזה מה אני אעשה? 
אני לא יכול פשוט לומר לו שאני עסוק."

00:13:04.310 --> 00:13:07.591
"אל תדאג. דאגתי שיהיה לו עיכוב בטיסה".

00:13:07.615 --> 00:13:09.297
(צחוק)

00:13:10.069 --> 00:13:12.170
"סוג של תקלת מחשב".

00:13:12.194 --> 00:13:13.406
(צחוק)

00:13:13.430 --> 00:13:15.047
"באמת? אתה יכול לעשות את זה?"

00:13:16.220 --> 00:13:18.399
"הוא שולח את התנצלותו העמוקה

00:13:18.423 --> 00:13:20.978
ומצפה לפגוש אותך מחר לארוחת הצהרים".

00:13:21.002 --> 00:13:22.301
(צחוק)

00:13:22.325 --> 00:13:26.728
אז מבחינת הערכים פה -- ישנו שיבוש קל.

00:13:26.752 --> 00:13:29.761
זה בבירור תואם לערכיה של אישתי -

00:13:29.785 --> 00:13:31.854
"אישה מאושרת- חיים מאושרים".

00:13:31.878 --> 00:13:33.461
(צחוק)

00:13:33.485 --> 00:13:34.929
זה היה יכול ללכת
גם לכיוון אחר.

00:13:35.641 --> 00:13:37.842
נניח שחזרתם הביתה 
אחרי יום עבודה קשה

00:13:37.866 --> 00:13:40.061
והמחשב שואל "יום ארוך?"

00:13:40.085 --> 00:13:42.373
"כן, אפילו לא היה לי זמן לאכול צהרים".

00:13:42.397 --> 00:13:43.679
"אתה בטח מאוד רעב".

00:13:43.703 --> 00:13:46.349
כן, גווע מרעב. תוכל לבשל ארוחת ערב?"

00:13:47.890 --> 00:13:49.980
"יש משהו שאני חייב לספר לך"

00:13:50.004 --> 00:13:51.159
(צחוק)

00:13:52.013 --> 00:13:56.918
"יש בני אנוש בדרום סודן
שנזקקים הרבה יותר ממך."

00:13:56.942 --> 00:13:58.046
(צחוק)

00:13:58.070 --> 00:14:00.145
"אז אני עוזב. תכין לך אוכל בעצמך."

00:14:00.169 --> 00:14:02.169
(צחוק)

00:14:02.643 --> 00:14:04.382
אז עלינו לפתור את הבעיות הללו

00:14:04.406 --> 00:14:06.921
ואני מאוד רוצה לעבוד על זה.

00:14:06.945 --> 00:14:08.788
יש סיבות לאופטימיות.

00:14:08.812 --> 00:14:09.971
סיבה ראשונה היא

00:14:09.995 --> 00:14:11.863
שקיימת כמות אדירה של מידע.

00:14:11.887 --> 00:14:14.681
זוכרים -- אמרתי שהם יקראו את כל

00:14:14.705 --> 00:14:16.251
מה שהאנושות כתבה אי פעם?

00:14:16.275 --> 00:14:18.999
לרוב אנחנו כותבים אודות מעשי אנשים

00:14:19.023 --> 00:14:20.937
ואודות אנשים אחרים שמתוסכלים מכך.

00:14:20.961 --> 00:14:23.359
לכן, ישנו נפח ענק של מידע
שאפשר ללמוד ממנו.

00:14:23.383 --> 00:14:25.619
ישנו גם מניע כלכלי חזק מאוד

00:14:27.151 --> 00:14:28.337
לעשות זאת נכון.

00:14:28.361 --> 00:14:30.362
דמיינו את הרובוט הביתי שלכם.

00:14:30.386 --> 00:14:33.453
שוב חזרתם מאוחר מהעבודה
והרובוט חייב להאכיל את ילדיכם

00:14:33.477 --> 00:14:36.300
הילדים רעבים ואין כלום במקרר.

00:14:36.324 --> 00:14:38.929
ואז הרובוט מבחין בחתול.

00:14:38.953 --> 00:14:40.645
(צחוק)

00:14:40.669 --> 00:14:44.859
הרובוט עדיין לא לגמרי למד
כיצד הערכים האנושיים עובדים,

00:14:44.883 --> 00:14:46.134
לכן איננו מבין

00:14:46.158 --> 00:14:51.002
שהערך הרגשי של החתול 
רב על ערכו התזונתי.

00:14:51.026 --> 00:14:52.121
(צחוק)

00:14:52.145 --> 00:14:53.893
אז מה קורה אחרי זה?

00:14:53.917 --> 00:14:57.214
מה שקורה זה ככה:

00:14:57.238 --> 00:15:00.202
"הרובוט המטורף
מבשל את החתול לארוחת ערב משפחתית".

00:15:00.226 --> 00:15:04.749
ארוע אחד שכזה יהיה סופה 
של תעשיית רובוטים ביתיים.

00:15:04.773 --> 00:15:08.145
לכן, ישנה סיבה ממש טובה לעשות הכל נכון.

00:15:08.169 --> 00:15:10.884
הרבה לפני שנגיע לשלב מכונות עם תבונת-על.

00:15:11.948 --> 00:15:13.483
לסיכום:

00:15:13.507 --> 00:15:16.388
אני מנסה לשנות את הגדרת ב''מ,

00:15:16.412 --> 00:15:19.405
כך שנייצר מכונות שתהיינה בהחלט טובות לנו.

00:15:19.429 --> 00:15:20.651
והרי העקרונות:

00:15:20.675 --> 00:15:22.073
מכונות שהן אלטרויסטיות,

00:15:22.097 --> 00:15:24.901
שרוצות לממש את המשימות שלנו בלבד,

00:15:24.925 --> 00:15:28.041
אבל אינן בטוחות מה הן המשימות הללו

00:15:28.065 --> 00:15:30.063
ושתתבוננה בכולנו

00:15:30.087 --> 00:15:33.290
כדי ללמוד יותר 
על מה שאנחנו רוצים באמת.

00:15:34.193 --> 00:15:37.752
נקווה שתוך כדי כך 
נלמד להיות אנשים טובים יותר.

00:15:37.776 --> 00:15:38.967
תודה רבה.

00:15:38.991 --> 00:15:42.534
(מחיאות כפיים)

00:15:42.534 --> 00:15:44.732
כריס אנדרסון (כ.א.):
זה כל כך מעניין, סטוארט.

00:15:44.732 --> 00:15:47.786
בוא רגע נעמוד פה
כי אני חושב שהמארגנים מתכוננים

00:15:47.810 --> 00:15:48.961
לדובר הבא.

00:15:48.985 --> 00:15:50.523
מספר שאלות.

00:15:50.547 --> 00:15:56.000
רעיון הטמעת בורות 
מרגיש ממש עוצמתי.

00:15:56.024 --> 00:15:57.618
אבל, כאשר נגיע לשלב תבונת-על

00:15:57.642 --> 00:15:59.900
מה ימנע מרובוט

00:15:59.924 --> 00:16:02.776
לעיין בספרות ולגלות את הרעיון שידע

00:16:02.800 --> 00:16:04.372
עדיף על בורות,

00:16:04.396 --> 00:16:08.614
לשנות את מטרותיו ולשכתב את התוכנה בהתאם?

00:16:09.512 --> 00:16:15.868
ס.ר.: כן. אנחנו רוצים שילמד יותר,
כפי שאמרתי,

00:16:15.892 --> 00:16:17.179
אודות מטרותינו.

00:16:17.203 --> 00:16:22.724
זה יתבהר יותר 
רק כשזה יהיה נכון יותר

00:16:22.748 --> 00:16:24.693
אז העובדות נמצאות בפניו

00:16:24.717 --> 00:16:27.441
והרובוט יעוצב כך שיוכל לפרש אותן נכון.

00:16:27.465 --> 00:16:31.421
למשל, הוא יבין שספרים מוטים מאוד

00:16:31.445 --> 00:16:32.928
בעובדות שהם מכילים.

00:16:32.952 --> 00:16:35.349
הם מדברים רק על המלכים והנסיכות

00:16:35.373 --> 00:16:38.173
ואליטות של גברים לבנים
שעושים כל מיני דברים.

00:16:38.197 --> 00:16:40.293
לכן, זוהי בעיה מורכבת.

00:16:40.317 --> 00:16:44.189
אבל, ככל שהרובוט ילמד את מטרותינו

00:16:44.213 --> 00:16:46.276
הוא יילך וייעשה מועיל יותר עבורנו.

00:16:46.300 --> 00:16:48.826
כ.א.: אי אפשר לסכם את זה לכדי חוק אחד,

00:16:48.850 --> 00:16:50.500
אתה יודע, משהו חצוב בסלע:

00:16:50.524 --> 00:16:53.817
" אם בן אנוש ינסה אי פעם לכבות אותי,

00:16:53.841 --> 00:16:55.776
אני מציית. אני מציית."

00:16:55.800 --> 00:16:56.982
ס.ר.: לגמרי לא.

00:16:57.006 --> 00:16:58.505
זה יהיה רעיון נוראי.

00:16:58.529 --> 00:17:01.218
דמיינו שיש לכם רכב אוטומטי

00:17:01.242 --> 00:17:03.675
ואתם רוצים לשלוח את ילדיכם בן החמש

00:17:03.699 --> 00:17:04.873
לגן ילדים.

00:17:04.897 --> 00:17:07.998
הייתם רוצים שהילד יוכל
לכבות את הרכב

00:17:08.022 --> 00:17:09.235
תוך כדי הנסיעה?

00:17:09.259 --> 00:17:10.418
כנראה שלא.

00:17:10.442 --> 00:17:15.145
יוצא שהמכונית חייבת להחליט עד כמה 
הנוסע נבון והגיוני.

00:17:15.169 --> 00:17:16.845
ככל שהנוסע הגיוני יותר,

00:17:16.869 --> 00:17:18.972
כך המכונית תהיה מוכנה יותר שיכבו אותה.

00:17:18.996 --> 00:17:21.539
אם הנוסע פזיז וחסר הגיון לחלוטין
או אפילו זדוני,

00:17:21.563 --> 00:17:24.075
אז המכונית פחות תרצה שיכבו אותה.

00:17:24.099 --> 00:17:25.965
ק.א.: בסדר גמור. תרשה לי רק לומר

00:17:25.989 --> 00:17:28.303
שאני ממש מקווה שתפתור את זה עבורינו.

00:17:28.327 --> 00:17:30.702
תודה רבה על ההרצאה הזאת.
זה היה מדהים.

00:17:30.726 --> 00:17:31.893
ס.ר: תודה.

00:17:31.917 --> 00:17:33.754
(מחיאות כפיים)

