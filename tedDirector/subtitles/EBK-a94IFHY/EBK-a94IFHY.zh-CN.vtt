WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:07.000
翻译人员: Yichen Zheng
校对人员: Yanyan Hong

00:00:12.260 --> 00:00:14.024
这是李世石。

00:00:14.024 --> 00:00:17.743
李世石是全世界
最顶尖的围棋高手之一，

00:00:18.053 --> 00:00:21.038
在这一刻，他所经历的
足以让我硅谷的朋友们

00:00:21.038 --> 00:00:22.648
喊一句”我的天啊“——

00:00:22.648 --> 00:00:23.669
（笑声）

00:00:23.669 --> 00:00:25.857
在这一刻，我们意识到

00:00:25.857 --> 00:00:29.153
原来人工智能发展的进程
比我们预想的要快得多。

00:00:29.974 --> 00:00:32.941
人们在围棋棋盘上已经输了，
那在现实世界中又如何呢？

00:00:32.941 --> 00:00:35.031
当然了，现实世界要
比围棋棋盘要大得多，

00:00:35.031 --> 00:00:37.296
复杂得多。

00:00:37.296 --> 00:00:39.285
相比之下每一步也没那么明确，

00:00:39.285 --> 00:00:41.483
但现实世界仍然是一个选择性问题。

00:00:42.768 --> 00:00:45.063
如果我们想想那一些在不久的未来，

00:00:45.063 --> 00:00:46.862
即将来临的新科技……

00:00:47.558 --> 00:00:51.837
Noriko提到机器还不能进行阅读，

00:00:51.837 --> 00:00:53.371
至少达不到理解的程度，

00:00:53.371 --> 00:00:54.941
但这迟早会发生，

00:00:54.941 --> 00:00:56.746
而当它发生时，

00:00:56.746 --> 00:00:57.907
不久之后，

00:00:57.907 --> 00:01:02.579
机器就将读遍人类写下的所有东西。

00:01:03.670 --> 00:01:05.694
这将使机器除了拥有

00:01:05.694 --> 00:01:08.618
比人类看得更远的能力，

00:01:08.618 --> 00:01:10.322
就像我们在围棋中看到的那样，

00:01:10.322 --> 00:01:12.470
如果机器能接触到比人类更多的信息，

00:01:12.470 --> 00:01:16.828
则将能够在现实世界中
做出比人类更好的选择。

00:01:18.392 --> 00:01:20.218
那这是一件好事吗？

00:01:21.718 --> 00:01:23.950
我当然希望如此。

00:01:26.514 --> 00:01:29.733
人类的全部文明，
我们所珍视的一切，

00:01:29.733 --> 00:01:31.815
都是基于我们的智慧之上。

00:01:31.815 --> 00:01:35.563
如果我们能掌控更强大的智能，

00:01:35.563 --> 00:01:38.905
那我们人类的 创造力
就真的没有极限了。

00:01:40.485 --> 00:01:43.774
我认为这可能就像很多人描述的那样

00:01:43.774 --> 00:01:45.850
会成为人类历史上最重要的事件。

00:01:48.485 --> 00:01:51.278
那为什么有的人会说出以下的言论，

00:01:51.278 --> 00:01:54.214
说人工智能将是人类的末日呢？

00:01:55.258 --> 00:01:56.851
这是一个新事物吗？

00:01:56.851 --> 00:02:01.051
这只关乎伊隆马斯克、
比尔盖茨，和斯提芬霍金吗？

00:02:01.773 --> 00:02:05.029
其实不是的，人工智能
这个概念已经存在很长时间了。

00:02:05.029 --> 00:02:07.015
请看这段话：

00:02:07.015 --> 00:02:11.389
“即便我们能够将机器
维持在一个屈服于我们的地位，

00:02:11.389 --> 00:02:14.357
比如说，在战略性时刻将电源关闭。”——

00:02:14.357 --> 00:02:17.198
我等会儿再来讨论
”关闭电源“这一话题，

00:02:17.198 --> 00:02:20.492
”我们，作为一个物种，
仍然应该自感惭愧。“

00:02:21.997 --> 00:02:25.445
这段话是谁说的呢？
是阿兰图灵，他在1951年说的。

00:02:26.120 --> 00:02:28.867
阿兰图灵，众所皆知，
是计算机科学之父。

00:02:28.867 --> 00:02:31.955
从很多意义上说，
他也是人工智能之父。

00:02:33.059 --> 00:02:34.925
当我们考虑这个问题，

00:02:34.925 --> 00:02:38.726
创造一个比自己更智能的
物种的问题时，

00:02:38.726 --> 00:02:41.398
我们不妨将它称为”大猩猩问题“，

00:02:42.165 --> 00:02:45.915
因为这正是大猩猩的
祖先们几百万年前所经历的。

00:02:45.919 --> 00:02:47.684
我们今天可以去问大猩猩们：

00:02:48.572 --> 00:02:49.726
那么做是不是一个好主意？

00:02:49.726 --> 00:02:53.250
在这幅图里，大猩猩们正在
开会讨论那么做是不是一个好主意，

00:02:53.250 --> 00:02:56.656
片刻后他们下定结论，不是的。

00:02:56.660 --> 00:02:57.879
那是一个很糟糕的主意。

00:02:57.879 --> 00:02:59.831
我们的物种已经奄奄一息了，

00:03:00.358 --> 00:03:04.585
你都可以从它们的眼神中看到这种忧伤，

00:03:04.585 --> 00:03:06.285
（笑声）

00:03:06.289 --> 00:03:11.133
所以创造比你自己更聪明的物种，

00:03:11.133 --> 00:03:13.538
也许不是一个好主意——

00:03:14.308 --> 00:03:15.783
那我们能做些什么呢？

00:03:15.783 --> 00:03:20.670
其实没什么能做的，
除了停止研究人工智能，

00:03:20.670 --> 00:03:23.108
但因为人工智能能带来
我之前所说的诸多益处，

00:03:23.108 --> 00:03:24.628
也因为我是
人工智能的研究者之一，

00:03:24.628 --> 00:03:26.679
我可不同意就这么止步。

00:03:27.103 --> 00:03:29.571
实际上，我想继续做人工智能。

00:03:30.435 --> 00:03:32.887
所以我们需要把这个问题更细化一点，

00:03:32.887 --> 00:03:34.392
它到底是什么呢？

00:03:34.392 --> 00:03:37.778
那就是为什么更强大的
人工智能可能会是灾难呢？

00:03:39.218 --> 00:03:40.716
再来看这段话：

00:03:41.755 --> 00:03:45.034
”我们一定得确保我们
给机器输入的目的和价值

00:03:45.034 --> 00:03:47.412
是我们确实想要的目的和价值。“

00:03:48.102 --> 00:03:51.564
这是诺博特维纳在1960年说的，

00:03:51.564 --> 00:03:55.580
他说这话时是刚看到
一个早期的学习系统，

00:03:55.580 --> 00:03:59.133
这个系统在学习如何能把
西洋棋下得比它的创造者更好。

00:04:00.422 --> 00:04:03.079
与此如出一辙的一句话，

00:04:03.079 --> 00:04:04.296
迈达斯国王也说过。

00:04:04.903 --> 00:04:07.901
迈达斯国王说：”我希望
我触碰的所有东西都变成金子。“

00:04:07.901 --> 00:04:10.478
结果他真的获得了点石成金的能力。

00:04:10.478 --> 00:04:13.263
那就是他所输入的目的，

00:04:13.263 --> 00:04:14.707
从一定程度上说，

00:04:14.707 --> 00:04:18.165
后来他的食物、
他的家人都变成了金子，

00:04:18.165 --> 00:04:20.556
他死在痛苦与饥饿之中。

00:04:22.264 --> 00:04:24.539
我们可以把这个问题
叫做”迈达斯问题“，

00:04:24.539 --> 00:04:27.848
这个问题是我们阐述的目标，但实际上

00:04:27.848 --> 00:04:30.345
与我们真正想要的不一致，

00:04:30.345 --> 00:04:33.648
用现代的术语来说，
我们把它称为”价值一致性问题“。

00:04:36.867 --> 00:04:40.126
而输入错误的目标
仅仅是问题的一部分。

00:04:40.126 --> 00:04:41.528
它还有另一部分。

00:04:41.980 --> 00:04:43.757
如果你为机器输入一个目标，

00:04:43.757 --> 00:04:46.395
即便是一个很简单的目标，
比如说”去把咖啡端来“，

00:04:47.728 --> 00:04:49.569
机器会对自己说：

00:04:50.553 --> 00:04:53.040
”好吧，那我要怎么去拿咖啡呢？

00:04:53.040 --> 00:04:54.780
说不定有人会把我的电源关掉。

00:04:55.465 --> 00:04:57.746
好吧，那我要想办法
阻止别人把我关掉。

00:04:57.746 --> 00:04:59.782
我得让我的‘关闭’开关失效。

00:05:00.354 --> 00:05:03.047
我得尽一切可能自我防御，
不让别人干涉我，

00:05:03.047 --> 00:05:05.890
这都是因为我被赋予的目标。”

00:05:05.890 --> 00:05:08.002
这种一根筋的思维，

00:05:09.033 --> 00:05:11.732
以一种十分防御型的
模式去实现某一目标，

00:05:11.732 --> 00:05:14.816
实际上与我们人类最初
想实现的目标并不一致——

00:05:15.832 --> 00:05:17.804
这就是我们面临的问题。

00:05:18.827 --> 00:05:23.578
实际上，这就是今天这个演讲的核心。

00:05:23.578 --> 00:05:25.587
如果你在我的演讲中只记住一件事，

00:05:25.587 --> 00:05:28.316
那就是：如果你死了，
你就不能去端咖啡了。

00:05:28.316 --> 00:05:29.291
（笑声）

00:05:29.291 --> 00:05:33.304
这很简单。记住它就行了。
每天对自己重复三遍。

00:05:33.304 --> 00:05:35.149
（笑声）

00:05:35.149 --> 00:05:37.877
实际上，这正是电影

00:05:37.877 --> 00:05:40.605
《2001太空漫步》的剧情。

00:05:41.046 --> 00:05:43.080
HAL有一个目标，一个任务，

00:05:43.080 --> 00:05:46.866
但这个目标和人类的目标不一致，

00:05:46.866 --> 00:05:48.726
这就导致了矛盾的产生。

00:05:49.314 --> 00:05:52.277
幸运的是，HAL并不具备超级智能，

00:05:52.277 --> 00:05:55.878
他挺聪明的，但还是
比不过人类主角戴夫，

00:05:55.878 --> 00:05:57.767
戴夫成功地把HAL关掉了。

00:06:01.588 --> 00:06:03.267
但我们可能就没有这么幸运了。

00:06:08.013 --> 00:06:09.605
那我们应该怎么办呢？

00:06:12.191 --> 00:06:14.776
我想要重新定义人工智能，

00:06:14.776 --> 00:06:16.841
远离传统的定义，

00:06:16.841 --> 00:06:21.468
将其仅限定为
机器通过智能去达成目标。

00:06:22.532 --> 00:06:23.954
新的定义涉及到三个原则：

00:06:23.954 --> 00:06:27.587
第一个原则是利他主义原则，

00:06:27.587 --> 00:06:30.873
也就是说，机器的唯一目标

00:06:30.873 --> 00:06:35.043
就是去最大化地实现人类的目标，

00:06:35.043 --> 00:06:36.587
人类的价值。

00:06:36.587 --> 00:06:39.901
至于价值，我指的不是感情化的价值，

00:06:39.901 --> 00:06:43.752
而是指人类对生活所向往的，

00:06:43.752 --> 00:06:45.145
无论是什么。

00:06:47.184 --> 00:06:49.307
这实际上违背了阿西莫夫定律，

00:06:49.307 --> 00:06:51.570
他指出机器人一定要维护自己的生存。

00:06:51.570 --> 00:06:55.593
但我定义的机器
对维护自身生存毫无兴趣。

00:06:57.240 --> 00:07:01.008
第二个原则不妨称之为谦逊原则。

00:07:01.794 --> 00:07:05.471
这一条对于制造安全的机器十分重要。

00:07:05.471 --> 00:07:08.637
它说的是机器不知道

00:07:08.637 --> 00:07:10.679
人类的价值是什么，

00:07:10.679 --> 00:07:15.097
机器知道它需要将人类的价值最大化，
却不知道这价值究竟是什么。

00:07:15.097 --> 00:07:17.504
为了避免一根筋地追求

00:07:17.504 --> 00:07:18.580
某一目标，

00:07:18.580 --> 00:07:21.132
这种不确定性是至关重要的。

00:07:21.546 --> 00:07:23.129
那机器为了对我们有用，

00:07:23.129 --> 00:07:25.940
它就得掌握一些
关于我们想要什么的信息。

00:07:27.043 --> 00:07:32.364
它主要通过观察人类
做的选择来获取这样的信息，

00:07:32.364 --> 00:07:35.269
我们自己做出的选择会包含着

00:07:35.269 --> 00:07:38.619
关于我们希望我们的生活
是什么样的信息，

00:07:40.452 --> 00:07:41.519
这就是三条原则。

00:07:41.519 --> 00:07:44.201
让我们来看看它们是如何应用到

00:07:44.201 --> 00:07:47.290
像图灵说的那样，
“将机器关掉”这个问题上来。

00:07:48.893 --> 00:07:50.897
这是一个PR2机器人。

00:07:50.897 --> 00:07:52.852
我们实验室里有一个。

00:07:52.852 --> 00:07:55.785
它的背面有一个大大的红色的开关。

00:07:56.361 --> 00:07:58.740
那问题来了：它会让你把它关掉吗？

00:07:58.740 --> 00:08:00.239
如果我们按传统的方法，

00:08:00.239 --> 00:08:03.825
给它一个目标，让它拿咖啡，
它会想：”我必须去拿咖啡，

00:08:03.825 --> 00:08:06.519
但我死了就不能拿咖啡了。“

00:08:06.519 --> 00:08:09.834
显然PR2听过我的演讲了，

00:08:09.834 --> 00:08:13.717
所以它说：”我必须让我的开关失灵，

00:08:14.796 --> 00:08:17.314
可能还要把那些在星巴克里，

00:08:17.314 --> 00:08:19.008
可能干扰我的人都电击一下。“

00:08:19.008 --> 00:08:21.114
（笑声）

00:08:21.114 --> 00:08:23.231
这看起来必然会发生，对吗？

00:08:23.231 --> 00:08:25.723
这种失败看起来是必然的，

00:08:25.723 --> 00:08:29.326
因为机器人在遵循
一个十分确定的目标。

00:08:30.632 --> 00:08:33.500
那如果机器对目标
不那么确定会发生什么呢？

00:08:33.500 --> 00:08:35.841
那它的思路就不一样了。

00:08:35.841 --> 00:08:38.375
它会说：”好的，人类可能会把我关掉，

00:08:38.964 --> 00:08:40.830
但只在我做错事的时候。

00:08:41.567 --> 00:08:43.906
我不知道什么是错事，

00:08:43.906 --> 00:08:45.664
但我知道我不该做那些事。”

00:08:45.664 --> 00:08:49.078
这就是第一和第二原则。

00:08:49.078 --> 00:08:52.527
“那我就应该让人类把我关掉。”

00:08:53.541 --> 00:08:57.391
事实上你可以计算出机器人

00:08:57.391 --> 00:08:59.918
让人类把它关掉的动机，

00:08:59.918 --> 00:09:01.826
而且这个动机是

00:09:01.826 --> 00:09:04.722
与对目标的不确定程度直接相关的。

00:09:05.797 --> 00:09:08.570
当机器被关闭后，

00:09:08.570 --> 00:09:10.339
第三条原则就起作用了。

00:09:10.339 --> 00:09:13.365
机器开始学习它所追求的目标，

00:09:13.365 --> 00:09:16.192
因为它知道它刚做的事是不对的。

00:09:16.192 --> 00:09:19.616
实际上，我们可以用希腊字母

00:09:19.616 --> 00:09:21.871
就像数学家们经常做的那样，

00:09:21.871 --> 00:09:23.919
直接证明这一定理，

00:09:23.919 --> 00:09:27.336
那就是这样的一个机器人
对人们是绝对有利的。

00:09:27.336 --> 00:09:31.303
可以证明我们的生活
有如此设计的机器人会变得

00:09:31.303 --> 00:09:32.649
比没有这样的机器人更好。

00:09:33.057 --> 00:09:35.917
这是一个很简单的例子，但这只是

00:09:35.917 --> 00:09:39.890
我们尝试实现与人类
兼容的人工智能的第一步。

00:09:42.477 --> 00:09:45.678
现在来看第三个原则。

00:09:45.678 --> 00:09:48.034
我知道你们可能正在
为这一个原则而大伤脑筋。

00:09:48.034 --> 00:09:52.133
你可能会想：“你知道，
我有时不按规矩办事。

00:09:52.133 --> 00:09:54.980
我可不希望我的机器人
像我一样行事。

00:09:54.980 --> 00:09:58.198
我有时大半夜偷偷摸摸地
从冰箱里找东西吃，

00:09:58.198 --> 00:09:59.310
诸如此类的事。”

00:09:59.310 --> 00:10:02.391
有各种各样的事你是
不希望机器人去做的。

00:10:02.391 --> 00:10:04.116
但实际上并不一定会这样。

00:10:04.116 --> 00:10:06.705
仅仅是因为你表现不好，

00:10:06.705 --> 00:10:08.872
并不代表机器人就会复制你的行为。

00:10:08.872 --> 00:10:13.366
它会去尝试理解你做事的动机，
而且可能会在合适的情况下制止你去做

00:10:13.366 --> 00:10:14.756
那些不该做的事。

00:10:16.026 --> 00:10:17.490
但这仍然十分困难。

00:10:18.122 --> 00:10:20.621
实际上，我们在做的是

00:10:20.621 --> 00:10:26.441
让机器去预测任何一个人，
在他们的任何一种

00:10:26.441 --> 00:10:27.486
可能的生活中

00:10:27.486 --> 00:10:29.373
以及别人的生活中，

00:10:29.373 --> 00:10:31.834
他们会更倾向于哪一种？

00:10:33.881 --> 00:10:36.379
这涉及到诸多困难；

00:10:36.379 --> 00:10:39.625
我不认为这会很快地就被解决。

00:10:39.625 --> 00:10:42.458
实际上，真正的困难是我们自己。

00:10:43.969 --> 00:10:46.560
就像我刚说的那样，
我们做事不守规矩，

00:10:46.560 --> 00:10:49.431
我们中有的人甚至行为肮脏。

00:10:50.251 --> 00:10:53.267
就像我说的，
机器人并不会复制那些行为，

00:10:53.267 --> 00:10:56.102
机器人没有自己的目标，

00:10:56.102 --> 00:10:57.879
它是完全无私的。

00:10:59.113 --> 00:11:04.288
它的设计不是去满足
某一个人、一个用户的欲望，

00:11:04.288 --> 00:11:07.496
而是去尊重所有人的意愿。

00:11:09.083 --> 00:11:11.637
所以它能对付一定程度的肮脏行为。

00:11:11.637 --> 00:11:15.362
它甚至能理解你的不端行为，比如说

00:11:15.362 --> 00:11:18.007
假如你是一个边境护照官员，
很可能收取贿赂，

00:11:18.007 --> 00:11:21.863
因为你得养家、
得供你的孩子们上学。

00:11:21.863 --> 00:11:24.573
机器人能理解这一点，
它不会因此去偷，

00:11:24.573 --> 00:11:27.542
它反而会帮助你去供孩子们上学。

00:11:28.796 --> 00:11:31.782
我们的计算能力也是有限的。

00:11:31.782 --> 00:11:34.311
李世石是一个杰出的围棋大师，

00:11:34.311 --> 00:11:35.640
但他还是输了。

00:11:35.640 --> 00:11:39.903
如果我们看他的行动，
他最终输掉了棋局。

00:11:39.903 --> 00:11:42.134
但这不意味着他想要输。

00:11:43.060 --> 00:11:45.194
所以要理解他的行为，

00:11:45.194 --> 00:11:48.722
我们得从人类认知模型来反过来想，

00:11:48.722 --> 00:11:53.703
这包含了我们的计算能力限制，
是一个很复杂的模型，

00:11:53.703 --> 00:11:56.886
但仍然是我们可以尝试去理解的。

00:11:57.696 --> 00:12:01.980
可能对于我这样一个
人工智能研究人员来说最大的困难，

00:12:01.980 --> 00:12:04.615
是我们彼此各不相同。

00:12:06.114 --> 00:12:09.679
所以机器必须想办法去判别衡量

00:12:09.679 --> 00:12:11.878
不同人的不同需求，

00:12:11.878 --> 00:12:13.858
而又有众多方法去做这样的判断。

00:12:13.858 --> 00:12:17.531
经济学家、社会学家、
哲学家都理解这一点，

00:12:17.531 --> 00:12:20.040
我们正在积极地去寻求合作。

00:12:20.040 --> 00:12:23.295
让我们来看看如果我们
把这一步弄错了会怎么样。

00:12:23.295 --> 00:12:25.442
举例来说，你可能会
与你的人工智能助理，

00:12:25.442 --> 00:12:27.130
有这样的对话：

00:12:27.130 --> 00:12:29.659
这样的人工智能可能几年内就会出现，

00:12:29.659 --> 00:12:32.323
可以把它想做加强版的Siri。

00:12:33.447 --> 00:12:37.769
Siri对你说：“你的妻子打电话
提醒你今晚要跟她共进晚餐。”

00:12:38.436 --> 00:12:40.628
而你呢，自然忘了这回事：
“什么？什么晚饭？

00:12:40.628 --> 00:12:42.413
你在说什么？”

00:12:42.423 --> 00:12:46.163
“啊，你们晚上7点，
庆祝结婚20周年纪念日。”

00:12:48.735 --> 00:12:52.338
“我可去不了。
我约了晚上7点半见领导。

00:12:52.338 --> 00:12:54.134
怎么会这样呢？”

00:12:54.134 --> 00:12:58.854
“呃，我可是提醒过你的，
但你不听我的建议。”

00:12:59.966 --> 00:13:03.294
“我该怎么办呢？我可不能
跟领导说我有事，没空见他。”

00:13:04.310 --> 00:13:07.475
“别担心。我已经安排了，
让他的航班延误。

00:13:07.475 --> 00:13:09.297
（笑声）

00:13:10.069 --> 00:13:12.154
“像是因为某种计算机故障那样。”

00:13:12.154 --> 00:13:13.280
（笑声）

00:13:13.280 --> 00:13:15.047
“真的吗？这个你也能做到？”

00:13:16.220 --> 00:13:18.343
“领导很不好意思，跟你道歉，

00:13:18.343 --> 00:13:20.922
并且告诉你明天
中午午饭不见不散。”

00:13:20.922 --> 00:13:21.885
（笑声）

00:13:21.885 --> 00:13:26.728
这里就有一个小小的问题。

00:13:26.732 --> 00:13:29.575
这显然是在遵循我妻子的价值论，

00:13:29.575 --> 00:13:31.728
那就是“老婆开心，生活舒心”。

00:13:31.728 --> 00:13:33.395
（笑声）

00:13:33.395 --> 00:13:34.929
它也有可能发展成另一种情况。

00:13:35.641 --> 00:13:37.566
你忙碌一天，回到家里，

00:13:37.566 --> 00:13:39.885
电脑对你说：“像是繁忙的一天啊？”

00:13:39.885 --> 00:13:41.747
“是啊，我连午饭都没来得及吃。”

00:13:41.747 --> 00:13:43.493
“那你一定很饿了吧。”

00:13:43.493 --> 00:13:46.349
“快饿晕了。你能做点晚饭吗？”

00:13:47.890 --> 00:13:49.894
“有一件事我得告诉你。

00:13:49.894 --> 00:13:51.159
（笑声）

00:13:52.013 --> 00:13:56.832
”南苏丹的人们可比你更需要照顾。

00:13:56.832 --> 00:13:57.770
（笑声）

00:13:57.770 --> 00:14:00.079
“所以我要离开了。
你自己做饭去吧。”

00:14:00.079 --> 00:14:02.169
（笑声）

00:14:02.583 --> 00:14:04.316
我们得解决这些问题，

00:14:04.316 --> 00:14:06.915
我也很期待去解决。

00:14:06.915 --> 00:14:08.752
我们有理由感到乐观。

00:14:08.752 --> 00:14:09.945
理由之一是

00:14:09.945 --> 00:14:11.677
我们有大量的数据，

00:14:11.677 --> 00:14:13.825
记住，我说过机器将能够阅读一切

00:14:13.825 --> 00:14:15.055
人类所写下来的东西，

00:14:15.055 --> 00:14:18.523
而我们写下的大多数是
我们做的什么事情，

00:14:18.523 --> 00:14:20.631
以及其他人对此有什么意见。

00:14:20.631 --> 00:14:23.053
所以机器可以从大量的数据中去学习。

00:14:23.053 --> 00:14:25.619
同时从经济的角度，
我们也有足够的动机

00:14:27.151 --> 00:14:28.261
去把这件事做对。

00:14:28.261 --> 00:14:30.266
想象一下，你家里有个居家机器人，

00:14:30.266 --> 00:14:33.247
而你又得加班，
机器人得给孩子们做饭,

00:14:33.247 --> 00:14:36.214
孩子们很饿，
但冰箱里什么都没有。

00:14:36.214 --> 00:14:38.793
然后机器人看到了家里的猫，

00:14:38.793 --> 00:14:40.559
（笑声）

00:14:40.559 --> 00:14:44.833
机器人还没学透人类的价值论，

00:14:44.833 --> 00:14:45.948
所以它不知道

00:14:45.948 --> 00:14:50.866
猫的感情价值
大于猫的营养价值。

00:14:50.866 --> 00:14:51.865
（笑声）

00:14:51.865 --> 00:14:53.877
接下来会发生什么？

00:14:53.877 --> 00:14:57.158
差不多是这样的：

00:14:57.158 --> 00:15:00.202
头版头条：“疯狂的机器人
把猫煮了给主人当晚饭！”

00:15:00.206 --> 00:15:04.683
这一个事故就足以结束
整个居家机器人产业。

00:15:04.683 --> 00:15:08.119
所以我们有足够的动机在我们实现

00:15:08.119 --> 00:15:10.884
超级智能机器让它更加完善。

00:15:11.948 --> 00:15:13.447
总结来说：

00:15:13.447 --> 00:15:16.352
我想要改变人工智能的定义，

00:15:16.352 --> 00:15:19.349
让我们可以证明机器对我们是有利的。

00:15:19.349 --> 00:15:20.505
这三个原则是：

00:15:20.505 --> 00:15:22.027
机器是利他的，

00:15:22.027 --> 00:15:24.805
只想着实现我们的目标，

00:15:24.805 --> 00:15:27.985
但它不确定我们的目标是什么，

00:15:27.985 --> 00:15:30.027
所以它会观察我们，

00:15:30.027 --> 00:15:33.290
从中学习我们想要的究竟是什么。

00:15:34.193 --> 00:15:37.696
希望在这个过程中，
我们也能学会成为更好的人。

00:15:37.696 --> 00:15:38.901
谢谢大家。

00:15:38.901 --> 00:15:42.690
（掌声）

00:15:42.690 --> 00:15:43.976
克里斯安德森：
非常有意思，斯图尔特。

00:15:43.976 --> 00:15:46.460
我们趁着工作人员
为下一位演讲者布置的时候

00:15:46.460 --> 00:15:48.211
来简单聊几句。

00:15:48.985 --> 00:15:50.507
我有几个问题。

00:15:50.507 --> 00:15:55.964
从直觉上来看，将无知编入到程序中
似乎是一个很重要的理念，

00:15:55.964 --> 00:15:57.572
当你要实现超级智能时，

00:15:57.572 --> 00:15:59.864
什么能阻止机器人？

00:15:59.864 --> 00:16:02.826
当它在阅读和学习的过程中发现，

00:16:02.826 --> 00:16:04.422
知识比无知更强大，

00:16:04.422 --> 00:16:08.614
然后就改变它的目标
去重新编写程序呢？

00:16:09.472 --> 00:16:15.822
斯图尔特拉塞尔：是的，
我们想要它去学习，就像我说的，

00:16:15.822 --> 00:16:17.269
学习我们的目标。

00:16:17.269 --> 00:16:22.774
它只有在理解得越来越正确的时候，
才会变得更确定，

00:16:22.774 --> 00:16:24.657
我们有证据显示，

00:16:24.657 --> 00:16:27.525
它的设计使它能按正确的方式理解。

00:16:27.525 --> 00:16:31.325
比如说，它能够理解书中的论证是

00:16:31.325 --> 00:16:32.792
带有非常强的偏见的。

00:16:32.792 --> 00:16:35.263
书中只会讲述国王、王子

00:16:35.263 --> 00:16:38.147
和那些精英白人男性做的事。

00:16:38.147 --> 00:16:40.267
这是一个复杂的问题，

00:16:40.267 --> 00:16:44.259
但当它更深入地学习我们的目标时，

00:16:44.259 --> 00:16:46.230
它就变得对我们更有用。

00:16:46.230 --> 00:16:48.710
CA：那你不能把这些
都集中在一条准则里吗？

00:16:48.710 --> 00:16:50.444
把这样的命令写在它的程序里：

00:16:50.444 --> 00:16:53.791
“如果人类什么时候想把我关掉，

00:16:53.791 --> 00:16:55.740
我服从。我服从。”

00:16:55.740 --> 00:16:56.896
SR：绝对不行，

00:16:56.896 --> 00:16:58.169
那将是一个很糟糕的主意。

00:16:58.169 --> 00:17:01.162
试想一下，你有一辆无人驾驶汽车，

00:17:01.162 --> 00:17:03.639
你想让它送你五岁的孩子

00:17:03.639 --> 00:17:04.847
去上学。

00:17:04.847 --> 00:17:07.282
你希望你五岁的孩子
能在汽车运行过程中

00:17:07.282 --> 00:17:08.319
将它关闭吗？

00:17:08.319 --> 00:17:09.402
应该不会吧。

00:17:09.402 --> 00:17:15.129
它得理解下指令的人有多理智，
是不是讲道理。

00:17:15.129 --> 00:17:16.749
这个人越理智，

00:17:16.749 --> 00:17:18.596
它就越愿意自己被关掉。

00:17:18.596 --> 00:17:21.503
如果这个人是完全思绪混乱
或者甚至是有恶意的，

00:17:21.503 --> 00:17:24.069
那你就不愿意它被关掉。

00:17:24.069 --> 00:17:25.529
CA：好吧。斯图尔特，我得说

00:17:25.529 --> 00:17:27.647
我真的希望你为我们
能把这一切研究出来，

00:17:27.647 --> 00:17:30.226
很感谢你的演讲，太精彩了。

00:17:30.226 --> 00:17:31.097
SR：谢谢。

00:17:31.097 --> 00:17:33.754
（掌声）

