WEBVTT
Kind: captions
Language: el

00:00:00.000 --> 00:00:07.000
Μετάφραση: Dimitris Tsampallas
Επιμέλεια: Lucas Kaimaras

00:00:12.532 --> 00:00:13.844
Αυτός είναι ο Λι Σεντόλ.

00:00:13.849 --> 00:00:17.115
Ο Λι Σεντόλ είναι από τους καλύτερους
παίκτες Γκο στον κόσμο,

00:00:18.129 --> 00:00:21.014
και έχει κάτι που οι φίλοι μου 
στη Σίλικον Βάλλεϋ αποκαλούν

00:00:21.038 --> 00:00:22.548
μια στιγμή ξαφνιάσματος...

00:00:22.572 --> 00:00:23.645
(Γέλια)

00:00:23.669 --> 00:00:25.773
μια στιγμή όπου συνειδητοποιούμε

00:00:25.773 --> 00:00:29.025
ότι η ΤΝ προοδεύει αρκετά γρηγορότερα
από ό,τι περιμέναμε.

00:00:29.974 --> 00:00:32.841
Άρα χάσαμε στο Γκο.
Τι γίνεται όμως στον πραγματικό κόσμο;

00:00:32.841 --> 00:00:35.145
Ο πραγματικός κόσμος
είναι πολύ μεγαλύτερος

00:00:35.169 --> 00:00:37.418
και πολύ πιο πολύπλοκος
από το ταμπλό του Γκο.

00:00:37.442 --> 00:00:39.261
Δεν είναι τόσο ευδιάκριτο,

00:00:39.285 --> 00:00:41.451
αλλά παραμένει ένα πρόβλημα αποφάσεων.

00:00:42.580 --> 00:00:45.089
Και αν αναλογιστούμε 
μερικές από τις τεχνολογίες

00:00:45.113 --> 00:00:47.002
που εμφανίζονται...

00:00:47.558 --> 00:00:51.893
Η Νορίκο [Αράι] ανέφερε πως οι μηχανές
δεν έχουν ακόμα την ικανότητα ανάγνωσης,

00:00:51.917 --> 00:00:53.417
τουλάχιστον με κατανόηση.

00:00:53.441 --> 00:00:54.779
Αλλά αυτό θα συμβεί,

00:00:55.001 --> 00:00:56.772
και όταν συμβεί,

00:00:56.796 --> 00:00:57.882
πολύ σύντομα

00:00:57.882 --> 00:01:02.544
οι μηχανές θα έχουν διαβάσει οτιδήποτε
έχει γραφτεί ποτέ από την ανθρωπότητα.

00:01:03.670 --> 00:01:05.700
Και αυτό θα επιτρέψει στις μηχανές,

00:01:05.724 --> 00:01:08.748
μαζί με την ικανότητα να βλέπουν
πιο μπροστά από τους ανθρώπους,

00:01:08.748 --> 00:01:09.938
όπως είδαμε ήδη στο Γκο,

00:01:10.372 --> 00:01:12.896
κι αν έχουν πρόσβαση
σε περισσότερες πληροφορίες,

00:01:12.896 --> 00:01:16.833
να μπορούν να πάρουν πιο σωστές αποφάσεις
στον πραγματικό κόσμο από εμάς.

00:01:18.579 --> 00:01:20.218
Είναι καλό, όμως, αυτό;

00:01:21.718 --> 00:01:23.950
Το ελπίζω.

00:01:26.514 --> 00:01:29.769
Ολόκληρος ο πολιτισμός μας,
οτιδήποτε έχουμε σε εκτίμηση,

00:01:29.793 --> 00:01:31.443
είναι βασισμένο στην ευφυία μας.

00:01:31.885 --> 00:01:35.319
Και αν είχαμε πρόσβαση
σε πολύ περισσότερη ευφυΐα,

00:01:35.603 --> 00:01:39.019
τότε δεν υπάρχει κανένα όριο
στο τι μπορεί να κάνει η ανθρωπότητα.

00:01:40.485 --> 00:01:43.336
Και αυτό θα ήταν,
όπως μερικοί το έχουν περιγράψει,

00:01:43.834 --> 00:01:46.015
το σημαντικότερο γεγονός της ιστορίας μας.

00:01:48.485 --> 00:01:51.029
Άρα, γιατί λέμε μερικά πράγματα όπως,

00:01:51.217 --> 00:01:54.307
ότι η ΤΝ θα σημάνει
το τέλος της ανθρωπότητας;

00:01:55.258 --> 00:01:56.917
Πρόκειται για κάτι καινούργιο;

00:01:56.941 --> 00:02:01.051
Τα λένε αυτά μόνο οι Έλον Μασκ,
Μπιλ Γκέιτς και Στίβεν Χόκινγκ;

00:02:01.773 --> 00:02:05.035
Στην πραγματικότητα, όχι. 
Η ιδέα αυτή κυκλοφορεί εδώ και καιρό.

00:02:05.059 --> 00:02:06.043
Δείτε μια παράθεση,

00:02:06.474 --> 00:02:11.021
«Ακόμα και αν μπορούσαμε να κρατήσουμε
τις μηχανές σε θέση υποταγής,

00:02:11.021 --> 00:02:14.403
για παράδειγμα, απενεργοποιώντας τες
σε στρατηγικές στιγμές»

00:02:14.427 --> 00:02:17.240
-θα επανέλθω στο ζήτημα
της «απενεργοποίησης» αργότερα-

00:02:17.240 --> 00:02:20.344
«θα έπρεπε ως είδος να νιώθουμε
ιδιαίτερα ταπεινωμένοι».

00:02:21.962 --> 00:02:25.298
Ποιος το είπε αυτό;
Ο Άλαν Τούρινγκ το 1951.

00:02:26.120 --> 00:02:28.883
Όπως γνωρίζετε, είναι ο πατέρας 
της επιστήμης υπολογιστών

00:02:28.907 --> 00:02:32.051
και κατά πολλούς τρόπους,
ο πατέρας της ΤΝ επίσης.

00:02:32.982 --> 00:02:34.941
Οπότε, αν σκεφτούμε το πρόβλημα,

00:02:34.965 --> 00:02:38.422
να δημιουργήσουμε κάτι 
πιο ευφυές από το είδος μας,

00:02:38.776 --> 00:02:41.494
ας το αποκαλέσουμε «πρόβλημα του γορίλα»,

00:02:42.021 --> 00:02:45.559
επειδή οι πρόγονοι του γορίλα
το έκαναν αυτό πριν εκατομμύρια χρόνια,

00:02:45.939 --> 00:02:47.781
και ας τους ρωτήσουμε τώρα:

00:02:48.506 --> 00:02:49.732
Ήταν καλή ιδέα;

00:02:49.756 --> 00:02:53.060
Οπότε, τώρα έχουν συνάντηση
για να κουβεντιάσουν εάν ήταν καλή ιδέα,

00:02:53.310 --> 00:02:56.656
και μετά από λίγο καταλήγουν πως,

00:02:56.680 --> 00:02:58.025
όχι, ήταν μια φρικτή ιδέα.

00:02:58.049 --> 00:02:59.831
Το είδος τους είναι σε δεινή θέση.

00:03:00.358 --> 00:03:04.394
Πραγματικά, μπορείς να δεις
την υπαρξιακή κρίση στα μάτια τους.

00:03:04.645 --> 00:03:06.285
(Γέλια)

00:03:06.309 --> 00:03:10.883
Αυτό το ανησυχητικό προαίσθημα
ότι μάλλον δεν είναι καλή ιδέα

00:03:10.883 --> 00:03:13.526
η δημιουργία κάτι 
πιο έξυπνου από το είδος μας,

00:03:14.168 --> 00:03:15.799
τι μπορούμε να κάνουμε γι' αυτό;

00:03:15.823 --> 00:03:20.322
Βασικά τίποτα, εκτός από το
να σταματήσουμε να φτιάχνουμε ΤΝ,

00:03:20.614 --> 00:03:23.124
που λόγω όλων των πλεονεκτημάτων
που προανέφερα,

00:03:23.148 --> 00:03:24.864
και επειδή είμαι ερευνητής της,

00:03:24.888 --> 00:03:26.443
δεν το ανέχομαι.

00:03:27.103 --> 00:03:29.539
Πραγματικά, θέλω να συνεχίσω
την έρευνα της ΤΝ.

00:03:30.435 --> 00:03:33.113
Οπότε, χρειάζεται να προσδιορίσουμε
καλύτερα το πρόβλημα.

00:03:33.137 --> 00:03:34.508
Ποιο είναι ακριβώς το θέμα;

00:03:34.532 --> 00:03:37.969
Γιατί μια καλύτερη ΤΝ είναι
πιθανώς μια καταστροφή;

00:03:39.218 --> 00:03:40.716
Ορίστε άλλη μια παράθεση,

00:03:41.755 --> 00:03:45.090
«Πρέπει να σιγουρευτούμε ότι ο σκοπός
που θα αναθέσουμε σε μια μηχανή,

00:03:45.114 --> 00:03:47.681
είναι ο σκοπός που πραγματικά επιθυμούμε».

00:03:48.102 --> 00:03:51.600
Αυτό ειπώθηκε από
τον Νόρμπερτ Βίνερ το 1960,

00:03:51.624 --> 00:03:55.626
λίγο αφότου παρακολούθησε ένα από τα πρώτα
συστήματα που μαθαίνουν μόνα τους

00:03:55.650 --> 00:03:58.373
να παίζει ντάμα καλύτερα
από τον δημιουργό του.

00:04:00.422 --> 00:04:03.105
Αλλά το ίδιο θα μπορούσε να έχει ειπωθεί

00:04:03.129 --> 00:04:04.536
από τον Βασιλιά Μίδα.

00:04:04.903 --> 00:04:08.037
Ο Βασιλιάς Μίδας είπε, 
«Θέλω ό,τι ακουμπώ να γίνεται χρυσάφι,»

00:04:08.061 --> 00:04:10.534
και πήρε ακριβώς ό,τι του άξιζε.

00:04:10.558 --> 00:04:13.280
Αυτός ήταν ο σκοπός
που ανέθεσε στη μηχανή,

00:04:13.280 --> 00:04:14.645
κατ' αναλογίαν,

00:04:14.807 --> 00:04:18.251
και έπειτα το φαγητό, το ποτό
και οι συγγενείς του έγιναν χρυσάφι

00:04:18.275 --> 00:04:20.556
και πέθανε μίζερος, λιμοκτονώντας.

00:04:22.264 --> 00:04:24.605
Αποκαλούμε λοιπόν «πρόβλημα του Μίδα»

00:04:24.629 --> 00:04:27.934
τη δήλωση ενός σκοπού
που στην πραγματικότητα

00:04:27.958 --> 00:04:30.267
δεν είναι απόλυτα σύμφωνος
με αυτό που θέλουμε.

00:04:30.395 --> 00:04:33.648
Αυτό με σύγχρονους όρους το αποκαλούμε
«πρόβλημα εναρμόνισης αξιών».

00:04:36.867 --> 00:04:40.352
Η εισαγωγή λάθους σκοπού
δεν είναι το μόνο μέρος του προβλήματος.

00:04:40.376 --> 00:04:41.738
Υπάρχει και άλλο μέρος.

00:04:41.980 --> 00:04:43.923
Εάν βάλουμε ένα σκοπό σε μια μηχανή,

00:04:43.947 --> 00:04:46.715
ακόμα και κάτι τόσο απλό όπως,
«Φέρε τον καφέ»,

00:04:47.728 --> 00:04:49.769
η μηχανή λέει στον εαυτό της,

00:04:50.553 --> 00:04:53.176
«Πώς μπορώ να αποτύχω να φέρω τον καφέ;

00:04:53.200 --> 00:04:55.070
Κάποιος ίσως να με απενεργοποιήσει.

00:04:55.465 --> 00:04:57.852
Οπότε, πρέπει να πάρω μέτρα
να μην συμβεί αυτό.

00:04:57.876 --> 00:04:59.972
Θα απομονώσω τον διακόπτη απενεργοποίησης.

00:05:00.354 --> 00:05:03.134
Θα κάνω τα πάντα ώστε να προστατευτώ
εναντίον παρεμβάσεων

00:05:03.134 --> 00:05:05.626
στον σκοπό που μου ανατέθηκε».

00:05:05.990 --> 00:05:08.242
Αυτή η επίμονη επιδίωξη,

00:05:09.005 --> 00:05:11.978
μέσω μιας υπεραμυντικής λειτουργίας,
ενός σκοπού που πραγματικά

00:05:12.002 --> 00:05:14.816
δεν είναι εναρμονισμένος
με τον αληθινό σκοπό των ανθρώπων,

00:05:15.822 --> 00:05:18.144
είναι το πρόβλημα που αντιμετωπίζουμε.

00:05:18.827 --> 00:05:23.594
Και αυτό είναι, στην πραγματικότητα,
το σημαντικότερο μήνυμα της ομιλίας μου.

00:05:23.618 --> 00:05:25.403
Εάν θέλετε να θυμάστε κάτι,

00:05:25.403 --> 00:05:28.370
είναι πως δεν μπορείτε να φέρετε
τον καφέ, εάν είστε νεκροί.

00:05:28.370 --> 00:05:29.215
(Γέλια)

00:05:29.215 --> 00:05:33.310
Είναι πολύ απλό. Απλά να θυμάστε
να το επαναλαμβάνετε τρεις φορές τη μέρα.

00:05:33.334 --> 00:05:34.945
(Γέλια)

00:05:35.179 --> 00:05:37.539
Αυτή είναι πραγματικά η πλοκή

00:05:37.957 --> 00:05:40.875
της ταινίας
«2001: Η Οδύσσεια του Διαστήματος»,

00:05:41.046 --> 00:05:43.136
όπου ο HAL έχει ένα σκοπό, μια αποστολή,

00:05:43.160 --> 00:05:46.581
που δεν είναι εναρμονισμένη
με τους σκοπούς των ανθρώπων,

00:05:46.916 --> 00:05:48.726
και αυτό οδηγεί σε σύγκρουση.

00:05:49.314 --> 00:05:52.086
Ευτυχώς, ο HAL δεν είναι υπερ-ευφυής.

00:05:52.117 --> 00:05:55.565
Είναι αρκέτα έξυπνος,
αλλά εν τέλει ο Ντέιβ τον υπερνικά

00:05:55.918 --> 00:05:57.767
και καταφέρνει να τον απενεργοποιήσει.

00:06:01.648 --> 00:06:04.067
Όμως εμείς μπορεί 
να μην είμαστε τόσο τυχεροί.

00:06:08.013 --> 00:06:09.835
Οπότε τι θα κάνουμε;

00:06:12.191 --> 00:06:14.792
Προσπαθώ να επαναπροσδιορίσω
την Τεχνητή Νοημοσύνη,

00:06:14.816 --> 00:06:16.877
ώστε να ξεφύγω από την κλασσική αντίληψη

00:06:16.901 --> 00:06:21.468
των μηχανών που ευφυώς ακολουθούν σκοπούς.

00:06:22.468 --> 00:06:24.096
Υπάρχουν τρεις εμπλεκόμενες αρχές.

00:06:24.096 --> 00:06:27.532
Η πρώτη είναι η αρχή του αλτρουισμού,
εάν μου επιτρέπετε,

00:06:27.532 --> 00:06:30.929
όπου ο μόνος σκοπός του ρομπότ

00:06:30.953 --> 00:06:35.100
είναι η απόλυτη συνειδητοποίηση
των ανθρώπινων σκοπών,

00:06:35.100 --> 00:06:36.436
των ανθρώπινων αξιών.

00:06:36.436 --> 00:06:39.795
Και δεν αναφέρομαι σε αγνές,
αθώες και συναισθηματικές αξίες.

00:06:39.795 --> 00:06:43.778
Μιλάω για τον οποιοδήποτε τρόπο
που οι άνθρωποι προτιμούν

00:06:43.802 --> 00:06:45.505
να είναι η ζωή τους.

00:06:47.055 --> 00:06:49.279
Πράγματι, έτσι παραβιάζεται
ο νόμος του Ασίμωφ,

00:06:49.279 --> 00:06:51.313
ότι το ρομπότ πρέπει να αυτοπροστατευτεί.

00:06:51.313 --> 00:06:55.402
Πλέον, δεν έχει κανένα απολύτως 
ενδιαφέρον στην διατήρηση της ύπαρξής του.

00:06:57.240 --> 00:07:00.939
Η δεύτερη αρχή είναι ο νόμος
της ταπεινοφροσύνης, εάν μου επιτρέπετε.

00:07:01.794 --> 00:07:05.367
Αποδεικνύεται ιδιαίτερα σημαντικός
για την δημιουργία ασφαλών ρομπότ.

00:07:05.561 --> 00:07:08.703
Αναφέρει ότι το ρομπότ δεν γνωρίζει

00:07:08.727 --> 00:07:10.755
ποιες είναι οι ανθρώπινες αξίες,

00:07:10.779 --> 00:07:14.068
οπότε πρέπει να τις μεγιστοποιήσει
χωρίς να γνωρίζει ποιες είναι.

00:07:15.074 --> 00:07:16.774
Έτσι αποφεύγεται το πρόβλημα

00:07:16.774 --> 00:07:18.936
της επίμονης επιδίωξης ενός σκοπού.

00:07:18.960 --> 00:07:20.940
Αυτή η αβεβαιότητα αποδεικνύεται κρίσιμη.

00:07:21.546 --> 00:07:23.185
Για να είναι όμως χρήσιμο σε εμάς

00:07:23.209 --> 00:07:25.940
πρέπει να έχει κάποια ιδέα του τι θέλουμε.

00:07:27.043 --> 00:07:32.470
Αποκτά αυτή τη πληροφορία κυρίως μέσω
της παρατήρησης των ανθρώπινων επιλογών,

00:07:32.494 --> 00:07:35.295
άρα οι ίδιες οι επιλογές μας
δίνουν πληροφορίες

00:07:35.319 --> 00:07:39.109
για το πως προτιμούμε
να ζούμε τις ζωές μας.

00:07:40.392 --> 00:07:42.059
Αυτές είναι λοιπόν, οι τρεις αρχές.

00:07:42.059 --> 00:07:45.877
Ας δούμε πώς εφαρμόζονται στο ερώτημα
«Μπορούμε να απενεργοποιήσουμε τη μηχανή;»

00:07:45.877 --> 00:07:47.690
όπως τέθηκε από τον Τούρινγκ.

00:07:48.893 --> 00:07:51.013
Έχουμε, λοιπόν, ένα ρομπότ PR2.

00:07:51.037 --> 00:07:52.858
Αυτό είναι εκείνο του εργαστηρίου μας

00:07:52.882 --> 00:07:55.855
και έχει ένα μεγάλο, κόκκινο κουμπί
απενεργοποίησης στην πλάτη.

00:07:56.281 --> 00:07:58.960
Το ερώτημα είναι αν θα σε αφήσει
να το απενεργοποιήσεις.

00:07:58.960 --> 00:08:00.465
Αν κινηθούμε κλασσικά,

00:08:00.489 --> 00:08:03.971
δίνουμε την εντολή, «Φέρε τον καφέ,
πρέπει να φέρω τον καφέ,

00:08:03.995 --> 00:08:06.575
δεν μπορώ να φέρω τον καφέ
άμα είμαι κατεστραμμένος»,

00:08:06.599 --> 00:08:09.940
άρα προφανώς το PR2
παρακολουθεί την ομιλία μου,

00:08:09.964 --> 00:08:14.227
οπότε λέει, «Πρέπει να χαλάσω
το διακόπτη απενεργοποίησης

00:08:14.796 --> 00:08:17.490
και να ρίξω αναίσθητο
οποιονδήποτε στα Starbucks

00:08:17.514 --> 00:08:19.074
που ίσως με σταματήσει».

00:08:19.098 --> 00:08:21.160
(Γέλια)

00:08:21.184 --> 00:08:23.337
Αυτό, λοιπόν, φαίνεται αναπόφευκτο, σωστά;

00:08:23.361 --> 00:08:25.759
Αυτή η λειτουργία αποτυχίας
φαίνεται αναπόφευκτη

00:08:25.783 --> 00:08:29.326
και προέρχεται από την ύπαρξη
ενός σαφούς και ξεκάθαρου σκοπού.

00:08:30.632 --> 00:08:33.776
Τι συμβαίνει όμως, αν η μηχανή
είναι αβέβαιη για το σκοπό;

00:08:33.800 --> 00:08:35.927
Θα επιχειρηματολογήσει διαφορετικά.

00:08:35.951 --> 00:08:38.375
Λέει, «Εντάξει, ο άνθρωπος 
ίσως με απενεργοποιήσει,

00:08:38.964 --> 00:08:41.070
αλλά μόνο εάν κάνω κάτι λάθος.

00:08:41.567 --> 00:08:44.042
Δεν γνωρίζω, όμως, τι είναι λάθος,

00:08:44.066 --> 00:08:46.110
αλλά γνωρίζω ότι δεν θέλω να το κάνω».

00:08:46.134 --> 00:08:49.144
Εδώ βρίσκουμε λοιπόν, την πρώτη
και τη δεύτερη αρχή.

00:08:49.168 --> 00:08:52.527
«Άρα θα έπρεπε να αφήσω τον άνθρωπο
να με απενεργοποιήσει».

00:08:53.541 --> 00:08:57.497
Στην πραγματικότητα, μπορούμε 
να εκτιμήσουμε ότι το κίνητρο του ρομπότ

00:08:57.521 --> 00:09:00.014
να αφήσει τον άνθρωπο
να το απενεργοποιήσει,

00:09:00.038 --> 00:09:01.952
είναι απευθείας συνδεδεμένο με τον βαθμό

00:09:01.976 --> 00:09:04.722
αβεβαιότητας για τον υποβόσκοντα σκοπό.

00:09:05.797 --> 00:09:08.746
Και όταν η μηχανή είναι απενεργοποιημένη,

00:09:08.770 --> 00:09:10.575
η τρίτη αρχή αποκτά σημασία.

00:09:10.599 --> 00:09:13.661
Η μηχανή μαθαίνει για τους σκοπούς
που πρέπει να ακολουθεί,

00:09:13.685 --> 00:09:16.218
διότι μαθαίνει ότι αυτό που έκανε
δεν ήταν σωστό.

00:09:16.242 --> 00:09:19.812
Στην πραγματικότητα,
με κατάλληλη χρήση Ελληνικών συμβόλων,

00:09:19.836 --> 00:09:21.967
όπως συχνά κάνουν οι μαθηματικοί,

00:09:21.991 --> 00:09:23.975
μπορούμε να αποδείξουμε ένα θεώρημα

00:09:23.999 --> 00:09:27.552
που δηλώνει ότι ένα τέτοιο ρομπότ
είναι πιθανά χρήσιμο στον άνθρωπο.

00:09:27.576 --> 00:09:31.379
Είμαστε καλύτερα με μία μηχανή
σχεδιασμένη με αυτό τον τρόπο,

00:09:31.403 --> 00:09:32.649
παρά χωρίς αυτή.

00:09:33.057 --> 00:09:35.963
Να, λοιπόν, ένα απλό παράδειγμα,
που όμως είναι το πρώτο βήμα

00:09:35.987 --> 00:09:39.890
προς αυτό που προσπαθούμε να κάνουμε
με την ανθρωπίνως συμβατή ΤΝ.

00:09:42.477 --> 00:09:45.734
Αυτή η τρίτη αρχή,

00:09:45.758 --> 00:09:48.674
θεωρώ πως είναι εκείνη
που σας κινεί το ενδιαφέρον.

00:09:48.674 --> 00:09:52.133
Πιθανώς σκέφτεστε,
«Μάλλον, συμπεριφέρομαι άσχημα.

00:09:52.157 --> 00:09:55.086
Δεν θέλω το ρομπότ μου 
να συμπεριφέρεται σαν εμένα.

00:09:55.110 --> 00:09:58.544
Κατεβαίνω στα κρυφά μέσα στη νύχτα
και παίρνω φαγητό από το ψυγείο.

00:09:58.568 --> 00:09:59.736
Κάνω διάφορα πράγματα».

00:09:59.760 --> 00:10:02.557
Υπάρχουν διάφορα πράγματα
που δεν θέλω να κάνει το ρομπότ.

00:10:02.581 --> 00:10:04.652
Αλλά ουσιαστικά, δεν γίνεται έτσι.

00:10:04.676 --> 00:10:06.831
Μόνο και μόνο επειδή
συμπεριφέρεστε άσχημα,

00:10:06.855 --> 00:10:09.312
δεν σημαίνει ότι το ρομπότ
θα σας αντιγράψει.

00:10:09.312 --> 00:10:13.412
Θα καταλάβει τα κίνητρα σας
και ίσως βοηθήσει να αντισταθείτε,

00:10:13.436 --> 00:10:14.756
εάν σας αρμόζει.

00:10:16.026 --> 00:10:17.490
Αλλά και πάλι είναι δύσκολο.

00:10:18.122 --> 00:10:20.667
Ουσιαστικά, αυτό που προσπαθούμε
να κάνουμε είναι

00:10:20.691 --> 00:10:23.291
να επιτρέψουμε στις μηχανές να προβλέπουν

00:10:23.291 --> 00:10:27.672
για κάθε άνθρωπο
και για κάθε πιθανό τρόπο ζωής τους,

00:10:27.696 --> 00:10:29.293
όπως και για τις ζωές κάθε άλλου:

00:10:29.317 --> 00:10:31.834
Τι θα προτιμούσαν;

00:10:33.881 --> 00:10:36.835
Υπάρχουν πάρα πολλές δυσκολίες
που εμπλέκονται σε αυτό,

00:10:36.859 --> 00:10:39.211
και δεν περιμένω να λυθούν πολύ γρήγορα.

00:10:39.815 --> 00:10:42.458
Οι πραγματικές δυσκολίες,
στην ουσία, είμαστε εμείς.

00:10:43.969 --> 00:10:47.086
Όπως προανέφερα, συμπεριφερόμαστε άσχημα.

00:10:47.110 --> 00:10:49.431
Πράγματι, κάποιοι από εμάς
είμαστε ελεεινοί.

00:10:50.251 --> 00:10:53.453
Το ρομπότ όμως, όπως προανέφερα,
δεν χρειάζεται να μας αντιγράψει.

00:10:53.453 --> 00:10:56.118
Το ρομπότ δεν έχει κανένα σκοπό
από μόνο του.

00:10:56.142 --> 00:10:58.099
Είναι καθαρά ανιδιοτελές.

00:10:59.113 --> 00:11:04.334
Και δεν είναι σχεδιασμένο να ικανοποιεί
μόνο τις επιθυμίες του χρήστη του,

00:11:04.358 --> 00:11:07.496
αλλά στην ουσία πρέπει να σέβεται
τις επιλογές όλων.

00:11:09.083 --> 00:11:11.653
Οπότε, μπορεί να ανεχτεί
κάποια ποσότητα αθλιότητας,

00:11:11.677 --> 00:11:15.378
ακόμα και να την καταλάβει.
Για παράδειγμα,

00:11:15.402 --> 00:11:18.073
μπορεί να δωροδοκείστε
ως ελεγκτής διαβατηρίων

00:11:18.097 --> 00:11:21.909
διότι πρέπει να ταΐσετε την οικογένεια σας
και να πάνε τα παιδιά σας σχολείο.

00:11:21.933 --> 00:11:24.839
Το καταλαβαίνει αυτό,
οπότε δεν πρόκειται να κλέψει.

00:11:24.863 --> 00:11:27.542
Απλώς θα σας βοηθήσει
να τα στείλετε σχολείο.

00:11:28.796 --> 00:11:31.808
Επίσης είμαστε περιορισμένοι υπολογιστικά.

00:11:31.832 --> 00:11:34.337
Ο Λι Σεντόλ είναι
καταπληκτικός παίκτης Γκο,

00:11:34.361 --> 00:11:35.686
παρ' όλ' αυτά έχασε.

00:11:35.710 --> 00:11:39.949
Αν κοιτάξουμε τις κινήσεις του,
έκανε μία που του στοίχισε το παιχνίδι.

00:11:39.973 --> 00:11:42.134
Αυτό δεν σημαίνει ότι ήθελε να χάσει.

00:11:43.160 --> 00:11:45.200
Άρα, για να καταλάβουμε
τη συμπεριφορά του,

00:11:45.224 --> 00:11:49.168
πρέπει βασικά να το επεξεργαστούμε με ένα
μοντέλο κατανόησης της ανθρώπινης γνώσης

00:11:49.168 --> 00:11:53.549
που να περιλαμβάνει τους υπολογιστικούς
περιορισμούς μας - ένα πολύπλοκο μοντέλο.

00:11:53.893 --> 00:11:56.886
Αλλά είναι ακόμα κάτι που
θα δουλέψουμε για να καταλάβουμε.

00:11:57.696 --> 00:12:02.016
Πιθανά το πιο δύσκολο μέρος,
κατά τη γνώμη μου ως ερευνητής ΤΝ

00:12:02.040 --> 00:12:04.615
είναι το γεγονός ότι υπάρχουν
πολλοί ερευνητές,

00:12:06.114 --> 00:12:09.609
οπότε η μηχανή πρέπει
κάπως να συμβιβάσει τις προτιμήσεις

00:12:09.609 --> 00:12:11.778
τόσων πολλών διαφορετικών ανθρώπων

00:12:11.778 --> 00:12:13.874
και υπάρχουν πολλοί τρόποι να γίνει αυτό.

00:12:13.898 --> 00:12:17.587
Οικονομολόγοι, κοινωνιολόγοι
και ηθικοί φιλόσοφοι το έχουν καταλάβει

00:12:17.611 --> 00:12:20.066
και ενεργά ψάχνουν για συνεργασίες.

00:12:20.090 --> 00:12:23.061
Ας δούμε τώρα τι συμβαίνει
όταν κάνεις λάθος σε αυτό.

00:12:23.365 --> 00:12:25.498
Μπορεί, για παράδειγμα, να συνομιλείς

00:12:25.522 --> 00:12:27.466
με τον ευφυή προσωπικό σου βοηθό

00:12:27.490 --> 00:12:29.775
που ίσως να είναι διαθέσιμος 
σε μερικά χρόνια.

00:12:29.799 --> 00:12:32.503
Σκεφτείτε κάτι σαν την Σίρι σε αναβολικά.

00:12:33.447 --> 00:12:37.769
Λέει, λοιπόν η Σίρι, «Η σύζυγος σου κάλεσε
για να σου υπενθυμίσει το αποψινό δείπνο».

00:12:38.436 --> 00:12:39.908
Και προφανώς το έχεις ξεχάσει.

00:12:39.908 --> 00:12:42.213
«Δείπνο; Ποιο δείπνο;
Για ποιο πράγμα μιλάς;»

00:12:42.417 --> 00:12:46.163
«Το δείπνο για την 20η επέτειό σας
στις 7μμ».

00:12:48.735 --> 00:12:52.454
«Δεν προλαβαίνω. Θα συναντήσω
τον γενικό γραμματέα στις 7:30.

00:12:52.478 --> 00:12:54.040
Πώς συνέβει αυτό;»

00:12:54.194 --> 00:12:58.854
«Σε ειδοποίησα, αλλά παρέκαμψες
την πρότασή μου».

00:12:59.966 --> 00:13:03.294
«Τι θα κάνω τώρα; Δεν μπορώ απλώς
να του πω ότι είμαι απασχολημένος».

00:13:04.310 --> 00:13:07.591
«Μην ανησυχείς.
Κανόνισα να καθυστερήσει η πτήση του».

00:13:07.615 --> 00:13:09.297
(Γέλια)

00:13:10.069 --> 00:13:12.170
«Κάποιου είδους βλάβη στον υπολογιστή».

00:13:12.194 --> 00:13:13.406
(Γέλια)

00:13:13.430 --> 00:13:15.407
«Αλήθεια; Μπορείς να το κάνεις αυτό;»

00:13:16.220 --> 00:13:18.399
«Σου ζητά ειλικρινά συγγνώμη

00:13:18.423 --> 00:13:20.978
και αναμένει να σε συναντήσει
για μεσημεριανό αύριο».

00:13:21.002 --> 00:13:22.301
(Γέλια)

00:13:22.325 --> 00:13:26.422
Το θέμα εδώ είναι
πως υπάρχει ένα μικρό λάθος.

00:13:26.422 --> 00:13:29.605
Αυτό είναι ότι σαφώς ακολουθούνται
οι σκοποί της γυναίκας μου,

00:13:29.605 --> 00:13:31.924
που είναι,
«Χαρούμενη σύζυγος, Χαρούμενη ζωή».

00:13:31.924 --> 00:13:33.461
(Γέλια)

00:13:33.485 --> 00:13:34.929
Θα μπορούσε να γίνει αλλιώς.

00:13:35.641 --> 00:13:37.842
Γυρνάς σπίτι μετά από μια μέρα
σκληρής δουλειάς

00:13:37.866 --> 00:13:40.061
και ο υπολογιστής λέει, «Δύσκολη μέρα;»

00:13:40.085 --> 00:13:42.373
«Ναι, δεν είχα χρόνο
ούτε για μεσημεριανό».

00:13:42.397 --> 00:13:43.679
«Θα πεινάς πολύ τότε».

00:13:43.703 --> 00:13:46.349
«Σαν λύκος. Μπορείς να μαγειρέψεις;»

00:13:47.890 --> 00:13:49.980
«Υπάρχει κάτι που πρέπει να σου πω».

00:13:50.004 --> 00:13:51.159
(Γέλια)

00:13:52.013 --> 00:13:56.918
«Υπάρχουν άνθρωποι στο Νότιο Σουδάν
που έχουν περισσότερη ανάγκη από εσένα».

00:13:56.942 --> 00:13:58.046
(Γέλια)

00:13:58.070 --> 00:14:00.145
«Οπότε, φεύγω. Μαγείρεψε μόνος σου».

00:14:00.169 --> 00:14:02.169
(Γέλια)

00:14:02.643 --> 00:14:04.382
Πρέπει να λύσουμε
αυτά τα προβλήματα

00:14:04.406 --> 00:14:06.921
και ανυπομονώ να δουλέψω πάνω σε αυτά.

00:14:06.945 --> 00:14:08.788
Υπάρχουν λόγοι να αισιοδοξούμε.

00:14:08.812 --> 00:14:09.875
Ένας λόγος είναι,

00:14:09.875 --> 00:14:12.033
πως υπάρχουν 
τεράστιες ποσότητες δεδομένων.

00:14:12.033 --> 00:14:14.261
Θυμηθείτε πως είπα ότι
πρόκειται να διαβάσουν

00:14:14.261 --> 00:14:15.641
οτιδήποτε έχει γραφτεί ποτέ.

00:14:15.641 --> 00:14:18.595
Τα περισσότερα που έχουν γραφτεί
είναι για ανθρώπους να δρουν

00:14:18.595 --> 00:14:20.443
και άλλους να εκνευρίζονται με αυτό.

00:14:20.443 --> 00:14:23.461
Οπότε υπάρχουν τεράστιες ποσότητες
δεδομένων από όπου θα μάθουν.

00:14:23.461 --> 00:14:26.009
Υπάρχει επίσης ένα ισχυρό,
οικονομικό κίνητρο

00:14:27.151 --> 00:14:28.337
ώστε να πετύχει αυτό.

00:14:28.361 --> 00:14:30.226
Φανταστείτε το οικιακό σας ρομπότ.

00:14:30.226 --> 00:14:33.453
Έχετε καθυστερήσει πάλι στη δουλειά,
αυτό πρέπει να ταΐσει τα παιδιά

00:14:33.477 --> 00:14:36.300
τα οποία είναι πεινασμένα
και δεν υπάρχει τίποτα στο ψυγείο.

00:14:36.324 --> 00:14:38.929
Και το ρομπότ βλέπει τη γάτα.

00:14:38.953 --> 00:14:40.645
(Γέλια)

00:14:40.669 --> 00:14:44.859
Το ρομπότ, όμως, δεν έχει μάθει
σωστά τη λειτουργία των ανθρώπινων αξιών

00:14:44.883 --> 00:14:46.134
και δεν καταλαβαίνει

00:14:46.158 --> 00:14:51.002
ότι η συναισθηματική αξία της γάτας
υπερισχύει της θρεπτικής της αξίας.

00:14:51.026 --> 00:14:52.121
(Γέλια)

00:14:52.145 --> 00:14:53.893
Τι συμβαίνει τότε, λοιπόν;

00:14:53.917 --> 00:14:57.214
Συμβαίνει κάτι τέτοιο,

00:14:57.238 --> 00:15:00.202
«Ανεξέλεγκτο ρομπότ μαγειρεύει
γατάκι για οικογενειακό δείπνο».

00:15:00.226 --> 00:15:04.749
Αυτό το συμβάν είναι αρκετό για το τέλος
της βιομηχανίας οικιακών ρομπότ.

00:15:04.773 --> 00:15:08.145
Άρα υπάρχει ένα ισχυρό κίνητρο
για να επιτευχθεί σωστά η έρευνα

00:15:08.169 --> 00:15:11.164
πολύ πριν φτάσουμε
στις υπερ-ευφυείς μηχανές.

00:15:11.948 --> 00:15:13.483
Για να συνοψίσω,

00:15:13.507 --> 00:15:16.388
Προσπαθώ να αλλάξω τον ορισμό της ΤΝ

00:15:16.412 --> 00:15:19.405
ώστε να έχουμε πιθανά ωφέλιμες μηχανές.

00:15:19.429 --> 00:15:20.651
Και οι αρχές είναι,

00:15:20.675 --> 00:15:22.073
ανιδιοτελείς μηχανές

00:15:22.097 --> 00:15:24.901
που θέλουν να πετύχουν
μόνο το δικό μας σκοπό

00:15:24.925 --> 00:15:28.041
και που είναι αβέβαιες
για το ποιος είναι αυτός ο σκοπός

00:15:28.065 --> 00:15:30.063
και που θα μας παρακολουθούν όλους

00:15:30.087 --> 00:15:33.290
για να μάθουν περισσότερα
για το τι πραγματικά θέλουμε.

00:15:34.193 --> 00:15:37.752
Αν όλα πάνε καλά, στη πορεία θα μάθουμε
πώς να γίνουμε καλύτεροι άνθρωποι.

00:15:37.776 --> 00:15:38.967
Σας ευχαριστώ πολύ.

00:15:38.991 --> 00:15:42.544
(Χειροκρότημα)

00:15:42.544 --> 00:15:44.592
Κρις Άντερσον: Πολύ ενδιαφέρον, Στιούαρτ.

00:15:44.616 --> 00:15:46.080
Θα περιμένουμε λιγάκι εδώ

00:15:46.080 --> 00:15:48.671
διότι νομίζω πως ετοιμάζονται
για τον επόμενο ομιλητή.

00:15:48.855 --> 00:15:50.223
Μερικές ερωτήσεις.

00:15:50.547 --> 00:15:55.754
Η ιδέα του προγραμματισμού εν αγνοία
φαίνεται ενστικτωδώς πολύ ισχυρή.

00:15:55.754 --> 00:15:57.818
Όσο προχωράς προς την υπερνοημοσύνη,

00:15:57.818 --> 00:15:59.900
τι πρόκειται να σταματήσει ένα ρομπότ

00:15:59.924 --> 00:16:02.330
που διαβάζει λογοτεχνία
και ανακαλύπτει την ιδέα

00:16:02.330 --> 00:16:04.372
ότι η γνώση είναι καλύτερη από την άγνοια,

00:16:04.396 --> 00:16:08.614
να μην αλλάξει τους σκοπούς του
και ξαναγράψει τον κώδικά του;

00:16:09.512 --> 00:16:15.868
Στιούαρτ Ράσελ: Ναι, εμείς θέλουμε
να μάθει περισσότερα, όπως είπα,

00:16:15.892 --> 00:16:17.179
για τους σκοπούς μας.

00:16:17.203 --> 00:16:22.724
Θα γίνει πιο βέβαιο όσο γίνεται πιο σωστό,

00:16:22.748 --> 00:16:24.693
άρα τα στοιχεία υπάρχουν

00:16:24.717 --> 00:16:27.441
και θα σχεδιαστεί
για να τα ερμηνεύει σωστά.

00:16:27.465 --> 00:16:31.421
Θα καταλαβαίνει, για παράδειγμα,
ότι τα βιβλία είναι πολύ προκατειλημμένα

00:16:31.445 --> 00:16:32.928
στο περιεχόμενό τους.

00:16:32.952 --> 00:16:35.349
Μιλούν μόνο για βασιλιάδες και πρίγκιπες

00:16:35.373 --> 00:16:38.173
και προνομιούχους λευκούς άνδρες
να κάνουν διάφορα.

00:16:38.197 --> 00:16:40.293
Είναι, λοιπόν, ένα σύνθετο πρόβλημα,

00:16:40.317 --> 00:16:44.189
αλλά όσο η μηχανή μαθαίνει περισσότερα
για τους σκοπούς μας,

00:16:44.213 --> 00:16:46.276
θα μας γίνεται όλο και πιο χρήσιμη.

00:16:46.300 --> 00:16:48.826
ΚΑ: Δεν μπορείς
να το συμπτύξεις όλο σε ένα νόμο,

00:16:48.850 --> 00:16:50.500
προγραμματισμένο μέσα τους,

00:16:50.524 --> 00:16:53.817
«Αν κάποιος άνθρωπος προσπαθήσει
να με απενεργοποιήσει

00:16:53.841 --> 00:16:55.776
θα υπακούσω».

00:16:55.800 --> 00:16:56.982
ΣΡ: Όχι, βέβαια.

00:16:57.006 --> 00:16:58.505
Αυτή είναι μια φρικτή ιδέα.

00:16:58.529 --> 00:17:01.218
Φαντάσου ότι έχεις
ένα αυτο-οδηγούμενο αυτοκίνητο

00:17:01.242 --> 00:17:03.675
και θέλεις να στείλεις
τον πεντάχρονο γιο σου

00:17:03.699 --> 00:17:04.873
στον παιδικό σταθμό.

00:17:04.897 --> 00:17:07.998
Θα ήθελες το πεντάχρονο παιδί σου
να μπορεί να το απενεργοποιήσει

00:17:08.022 --> 00:17:09.235
όσο είναι εν κινήσει;

00:17:09.259 --> 00:17:10.418
Μάλλον όχι.

00:17:10.442 --> 00:17:15.145
Άρα το ρομπότ πρέπει να καταλαβαίνει
πόσο λογικό είναι ένα άτομο.

00:17:15.169 --> 00:17:16.729
Όσο λογικότερο το άτομο,

00:17:16.729 --> 00:17:18.972
τόσο πιθανότερο να θες
να το απενεργοποιήσεις.

00:17:18.996 --> 00:17:21.539
Αν το άτομο είναι τελείως
ασυνάρτητο ή κακόβουλο,

00:17:21.563 --> 00:17:24.075
τότε είναι πιο απίθανο
να το απενεργοποιήσεις.

00:17:24.075 --> 00:17:25.941
ΚΑ: Εντάξει. Στιούαρτ, απλώς να πω,

00:17:25.941 --> 00:17:28.255
πραγματικά ελπίζω να καταφέρεις
να βρεις τη λύση.

00:17:28.255 --> 00:17:30.530
Ευχαριστώ πολύ για την απίθανη ομιλία σου.

00:17:30.530 --> 00:17:32.597
ΣΡ: Ευχαριστώ.
(Χειροκρότημα)

