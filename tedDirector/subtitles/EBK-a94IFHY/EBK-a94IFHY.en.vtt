WEBVTT
Kind: captions
Language: en

00:00:12.532 --> 00:00:14.084
This is Lee Sedol.

00:00:14.108 --> 00:00:18.105
Lee Sedol is one of the world's
greatest Go players,

00:00:18.129 --> 00:00:21.014
and he's having what my friends
in Silicon Valley call

00:00:21.038 --> 00:00:22.548
a "Holy Cow" moment --

00:00:22.572 --> 00:00:23.645
(Laughter)

00:00:23.669 --> 00:00:25.857
a moment where we realize

00:00:25.881 --> 00:00:29.177
that AI is actually progressing
a lot faster than we expected.

00:00:29.974 --> 00:00:33.021
So humans have lost on the Go board.
What about the real world?

00:00:33.045 --> 00:00:35.145
Well, the real world is much bigger,

00:00:35.169 --> 00:00:37.418
much more complicated than the Go board.

00:00:37.442 --> 00:00:39.261
It's a lot less visible,

00:00:39.285 --> 00:00:41.323
but it's still a decision problem.

00:00:42.768 --> 00:00:45.089
And if we think about some
of the technologies

00:00:45.113 --> 00:00:46.862
that are coming down the pike ...

00:00:47.558 --> 00:00:51.893
Noriko [Arai] mentioned that reading
is not yet happening in machines,

00:00:51.917 --> 00:00:53.417
at least with understanding.

00:00:53.441 --> 00:00:54.977
But that will happen,

00:00:55.001 --> 00:00:56.772
and when that happens,

00:00:56.796 --> 00:00:57.983
very soon afterwards,

00:00:58.007 --> 00:01:02.579
machines will have read everything
that the human race has ever written.

00:01:03.670 --> 00:01:05.700
And that will enable machines,

00:01:05.724 --> 00:01:08.644
along with the ability to look
further ahead than humans can,

00:01:08.668 --> 00:01:10.348
as we've already seen in Go,

00:01:10.372 --> 00:01:12.536
if they also have access
to more information,

00:01:12.560 --> 00:01:16.828
they'll be able to make better decisions
in the real world than we can.

00:01:18.612 --> 00:01:20.218
So is that a good thing?

00:01:21.718 --> 00:01:23.950
Well, I hope so.

00:01:26.514 --> 00:01:29.769
Our entire civilization,
everything that we value,

00:01:29.793 --> 00:01:31.861
is based on our intelligence.

00:01:31.885 --> 00:01:35.579
And if we had access
to a lot more intelligence,

00:01:35.603 --> 00:01:38.905
then there's really no limit
to what the human race can do.

00:01:40.485 --> 00:01:43.810
And I think this could be,
as some people have described it,

00:01:43.834 --> 00:01:45.850
the biggest event in human history.

00:01:48.485 --> 00:01:51.314
So why are people saying things like this,

00:01:51.338 --> 00:01:54.214
that AI might spell the end
of the human race?

00:01:55.258 --> 00:01:56.917
Is this a new thing?

00:01:56.941 --> 00:02:01.051
Is it just Elon Musk and Bill Gates
and Stephen Hawking?

00:02:01.773 --> 00:02:05.035
Actually, no. This idea
has been around for a while.

00:02:05.059 --> 00:02:07.021
Here's a quotation:

00:02:07.045 --> 00:02:11.395
"Even if we could keep the machines
in a subservient position,

00:02:11.419 --> 00:02:14.403
for instance, by turning off the power
at strategic moments" --

00:02:14.427 --> 00:02:17.664
and I'll come back to that
"turning off the power" idea later on --

00:02:17.688 --> 00:02:20.492
"we should, as a species,
feel greatly humbled."

00:02:21.997 --> 00:02:25.445
So who said this?
This is Alan Turing in 1951.

00:02:26.120 --> 00:02:28.883
Alan Turing, as you know,
is the father of computer science

00:02:28.907 --> 00:02:31.955
and in many ways,
the father of AI as well.

00:02:33.059 --> 00:02:34.941
So if we think about this problem,

00:02:34.965 --> 00:02:38.752
the problem of creating something
more intelligent than your own species,

00:02:38.776 --> 00:02:41.398
we might call this "the gorilla problem,"

00:02:42.165 --> 00:02:45.915
because gorillas' ancestors did this
a few million years ago,

00:02:45.939 --> 00:02:47.684
and now we can ask the gorillas:

00:02:48.572 --> 00:02:49.732
Was this a good idea?

00:02:49.756 --> 00:02:53.286
So here they are having a meeting
to discuss whether it was a good idea,

00:02:53.310 --> 00:02:56.656
and after a little while,
they conclude, no,

00:02:56.680 --> 00:02:58.025
this was a terrible idea.

00:02:58.049 --> 00:02:59.831
Our species is in dire straits.

00:03:00.358 --> 00:03:04.621
In fact, you can see the existential
sadness in their eyes.

00:03:04.645 --> 00:03:06.285
(Laughter)

00:03:06.309 --> 00:03:11.149
So this queasy feeling that making
something smarter than your own species

00:03:11.173 --> 00:03:13.538
is maybe not a good idea --

00:03:14.308 --> 00:03:15.799
what can we do about that?

00:03:15.823 --> 00:03:20.590
Well, really nothing,
except stop doing AI,

00:03:20.614 --> 00:03:23.124
and because of all
the benefits that I mentioned

00:03:23.148 --> 00:03:24.864
and because I'm an AI researcher,

00:03:24.888 --> 00:03:26.679
I'm not having that.

00:03:27.103 --> 00:03:29.571
I actually want to be able
to keep doing AI.

00:03:30.435 --> 00:03:33.113
So we actually need to nail down
the problem a bit more.

00:03:33.137 --> 00:03:34.508
What exactly is the problem?

00:03:34.532 --> 00:03:37.778
Why is better AI possibly a catastrophe?

00:03:39.218 --> 00:03:40.716
So here's another quotation:

00:03:41.755 --> 00:03:45.090
"We had better be quite sure
that the purpose put into the machine

00:03:45.114 --> 00:03:47.412
is the purpose which we really desire."

00:03:48.102 --> 00:03:51.600
This was said by Norbert Wiener in 1960,

00:03:51.624 --> 00:03:55.626
shortly after he watched
one of the very early learning systems

00:03:55.650 --> 00:03:58.233
learn to play checkers
better than its creator.

00:04:00.422 --> 00:04:03.105
But this could equally have been said

00:04:03.129 --> 00:04:04.296
by King Midas.

00:04:04.903 --> 00:04:08.037
King Midas said, "I want everything
I touch to turn to gold,"

00:04:08.061 --> 00:04:10.534
and he got exactly what he asked for.

00:04:10.558 --> 00:04:13.309
That was the purpose
that he put into the machine,

00:04:13.333 --> 00:04:14.783
so to speak,

00:04:14.807 --> 00:04:18.251
and then his food and his drink
and his relatives turned to gold

00:04:18.275 --> 00:04:20.556
and he died in misery and starvation.

00:04:22.264 --> 00:04:24.605
So we'll call this
"the King Midas problem"

00:04:24.629 --> 00:04:27.934
of stating an objective
which is not, in fact,

00:04:27.958 --> 00:04:30.371
truly aligned with what we want.

00:04:30.395 --> 00:04:33.648
In modern terms, we call this
"the value alignment problem."

00:04:36.867 --> 00:04:40.352
Putting in the wrong objective
is not the only part of the problem.

00:04:40.376 --> 00:04:41.528
There's another part.

00:04:41.980 --> 00:04:43.923
If you put an objective into a machine,

00:04:43.947 --> 00:04:46.395
even something as simple as,
"Fetch the coffee,"

00:04:47.728 --> 00:04:49.569
the machine says to itself,

00:04:50.553 --> 00:04:53.176
"Well, how might I fail
to fetch the coffee?

00:04:53.200 --> 00:04:54.780
Someone might switch me off.

00:04:55.465 --> 00:04:57.852
OK, I have to take steps to prevent that.

00:04:57.876 --> 00:04:59.782
I will disable my 'off' switch.

00:05:00.354 --> 00:05:03.313
I will do anything to defend myself
against interference

00:05:03.337 --> 00:05:05.966
with this objective
that I have been given."

00:05:05.990 --> 00:05:08.002
So this single-minded pursuit

00:05:09.033 --> 00:05:11.978
in a very defensive mode
of an objective that is, in fact,

00:05:12.002 --> 00:05:14.816
not aligned with the true objectives
of the human race --

00:05:15.942 --> 00:05:17.804
that's the problem that we face.

00:05:18.827 --> 00:05:23.594
And in fact, that's the high-value
takeaway from this talk.

00:05:23.618 --> 00:05:25.673
If you want to remember one thing,

00:05:25.697 --> 00:05:28.372
it's that you can't fetch
the coffee if you're dead.

00:05:28.396 --> 00:05:29.457
(Laughter)

00:05:29.481 --> 00:05:33.310
It's very simple. Just remember that.
Repeat it to yourself three times a day.

00:05:33.334 --> 00:05:35.155
(Laughter)

00:05:35.179 --> 00:05:37.933
And in fact, this is exactly the plot

00:05:37.957 --> 00:05:40.605
of "2001: [A Space Odyssey]"

00:05:41.046 --> 00:05:43.136
HAL has an objective, a mission,

00:05:43.160 --> 00:05:46.892
which is not aligned
with the objectives of the humans,

00:05:46.916 --> 00:05:48.726
and that leads to this conflict.

00:05:49.314 --> 00:05:52.283
Now fortunately, HAL
is not superintelligent.

00:05:52.307 --> 00:05:55.894
He's pretty smart,
but eventually Dave outwits him

00:05:55.918 --> 00:05:57.767
and manages to switch him off.

00:06:01.648 --> 00:06:03.267
But we might not be so lucky.

00:06:08.013 --> 00:06:09.605
So what are we going to do?

00:06:12.191 --> 00:06:14.792
I'm trying to redefine AI

00:06:14.816 --> 00:06:16.877
to get away from this classical notion

00:06:16.901 --> 00:06:21.468
of machines that intelligently
pursue objectives.

00:06:22.532 --> 00:06:24.330
There are three principles involved.

00:06:24.354 --> 00:06:27.643
The first one is a principle
of altruism, if you like,

00:06:27.667 --> 00:06:30.929
that the robot's only objective

00:06:30.953 --> 00:06:35.199
is to maximize the realization
of human objectives,

00:06:35.223 --> 00:06:36.613
of human values.

00:06:36.637 --> 00:06:39.967
And by values here I don't mean
touchy-feely, goody-goody values.

00:06:39.991 --> 00:06:43.778
I just mean whatever it is
that the human would prefer

00:06:43.802 --> 00:06:45.145
their life to be like.

00:06:47.184 --> 00:06:49.493
And so this actually violates Asimov's law

00:06:49.517 --> 00:06:51.846
that the robot has to protect
its own existence.

00:06:51.870 --> 00:06:55.593
It has no interest in preserving
its existence whatsoever.

00:06:57.240 --> 00:07:01.008
The second law is a law
of humility, if you like.

00:07:01.794 --> 00:07:05.537
And this turns out to be really
important to make robots safe.

00:07:05.561 --> 00:07:08.703
It says that the robot does not know

00:07:08.727 --> 00:07:10.755
what those human values are,

00:07:10.779 --> 00:07:13.957
so it has to maximize them,
but it doesn't know what they are.

00:07:15.074 --> 00:07:17.700
And that avoids this problem
of single-minded pursuit

00:07:17.724 --> 00:07:18.936
of an objective.

00:07:18.960 --> 00:07:21.132
This uncertainty turns out to be crucial.

00:07:21.546 --> 00:07:23.185
Now, in order to be useful to us,

00:07:23.209 --> 00:07:25.940
it has to have some idea of what we want.

00:07:27.043 --> 00:07:32.470
It obtains that information primarily
by observation of human choices,

00:07:32.494 --> 00:07:35.295
so our own choices reveal information

00:07:35.319 --> 00:07:38.619
about what it is that we prefer
our lives to be like.

00:07:40.452 --> 00:07:42.135
So those are the three principles.

00:07:42.159 --> 00:07:44.477
Let's see how that applies
to this question of:

00:07:44.501 --> 00:07:47.290
"Can you switch the machine off?"
as Turing suggested.

00:07:48.893 --> 00:07:51.013
So here's a PR2 robot.

00:07:51.037 --> 00:07:52.858
This is one that we have in our lab,

00:07:52.882 --> 00:07:55.785
and it has a big red "off" switch
right on the back.

00:07:56.361 --> 00:07:58.976
The question is: Is it
going to let you switch it off?

00:07:59.000 --> 00:08:00.465
If we do it the classical way,

00:08:00.489 --> 00:08:03.971
we give it the objective of, "Fetch
the coffee, I must fetch the coffee,

00:08:03.995 --> 00:08:06.575
I can't fetch the coffee if I'm dead,"

00:08:06.599 --> 00:08:09.940
so obviously the PR2
has been listening to my talk,

00:08:09.964 --> 00:08:13.717
and so it says, therefore,
"I must disable my 'off' switch,

00:08:14.796 --> 00:08:17.490
and probably taser all the other
people in Starbucks

00:08:17.514 --> 00:08:19.074
who might interfere with me."

00:08:19.098 --> 00:08:21.160
(Laughter)

00:08:21.184 --> 00:08:23.337
So this seems to be inevitable, right?

00:08:23.361 --> 00:08:25.759
This kind of failure mode
seems to be inevitable,

00:08:25.783 --> 00:08:29.326
and it follows from having
a concrete, definite objective.

00:08:30.632 --> 00:08:33.776
So what happens if the machine
is uncertain about the objective?

00:08:33.800 --> 00:08:35.927
Well, it reasons in a different way.

00:08:35.951 --> 00:08:38.375
It says, "OK, the human
might switch me off,

00:08:38.964 --> 00:08:40.830
but only if I'm doing something wrong.

00:08:41.567 --> 00:08:44.042
Well, I don't really know what wrong is,

00:08:44.066 --> 00:08:46.110
but I know that I don't want to do it."

00:08:46.134 --> 00:08:49.144
So that's the first and second
principles right there.

00:08:49.168 --> 00:08:52.527
"So I should let the human switch me off."

00:08:53.541 --> 00:08:57.497
And in fact you can calculate
the incentive that the robot has

00:08:57.521 --> 00:09:00.014
to allow the human to switch it off,

00:09:00.038 --> 00:09:01.952
and it's directly tied to the degree

00:09:01.976 --> 00:09:04.722
of uncertainty about
the underlying objective.

00:09:05.797 --> 00:09:08.746
And then when the machine is switched off,

00:09:08.770 --> 00:09:10.575
that third principle comes into play.

00:09:10.599 --> 00:09:13.661
It learns something about the objectives
it should be pursuing,

00:09:13.685 --> 00:09:16.218
because it learns that
what it did wasn't right.

00:09:16.242 --> 00:09:19.812
In fact, we can, with suitable use
of Greek symbols,

00:09:19.836 --> 00:09:21.967
as mathematicians usually do,

00:09:21.991 --> 00:09:23.975
we can actually prove a theorem

00:09:23.999 --> 00:09:27.552
that says that such a robot
is provably beneficial to the human.

00:09:27.576 --> 00:09:31.379
You are provably better off
with a machine that's designed in this way

00:09:31.403 --> 00:09:32.649
than without it.

00:09:33.057 --> 00:09:35.963
So this is a very simple example,
but this is the first step

00:09:35.987 --> 00:09:39.890
in what we're trying to do
with human-compatible AI.

00:09:42.477 --> 00:09:45.734
Now, this third principle,

00:09:45.758 --> 00:09:48.870
I think is the one that you're probably
scratching your head over.

00:09:48.894 --> 00:09:52.133
You're probably thinking, "Well,
you know, I behave badly.

00:09:52.157 --> 00:09:55.086
I don't want my robot to behave like me.

00:09:55.110 --> 00:09:58.544
I sneak down in the middle of the night
and take stuff from the fridge.

00:09:58.568 --> 00:09:59.736
I do this and that."

00:09:59.760 --> 00:10:02.557
There's all kinds of things
you don't want the robot doing.

00:10:02.581 --> 00:10:04.652
But in fact, it doesn't
quite work that way.

00:10:04.676 --> 00:10:06.831
Just because you behave badly

00:10:06.855 --> 00:10:09.478
doesn't mean the robot
is going to copy your behavior.

00:10:09.502 --> 00:10:13.412
It's going to understand your motivations
and maybe help you resist them,

00:10:13.436 --> 00:10:14.756
if appropriate.

00:10:16.026 --> 00:10:17.490
But it's still difficult.

00:10:18.122 --> 00:10:20.667
What we're trying to do, in fact,

00:10:20.691 --> 00:10:26.487
is to allow machines to predict
for any person and for any possible life

00:10:26.511 --> 00:10:27.672
that they could live,

00:10:27.696 --> 00:10:29.293
and the lives of everybody else:

00:10:29.317 --> 00:10:31.834
Which would they prefer?

00:10:33.881 --> 00:10:36.835
And there are many, many
difficulties involved in doing this;

00:10:36.859 --> 00:10:39.791
I don't expect that this
is going to get solved very quickly.

00:10:39.815 --> 00:10:42.458
The real difficulties, in fact, are us.

00:10:43.969 --> 00:10:47.086
As I have already mentioned,
we behave badly.

00:10:47.110 --> 00:10:49.431
In fact, some of us are downright nasty.

00:10:50.251 --> 00:10:53.303
Now the robot, as I said,
doesn't have to copy the behavior.

00:10:53.327 --> 00:10:56.118
The robot does not have
any objective of its own.

00:10:56.142 --> 00:10:57.879
It's purely altruistic.

00:10:59.113 --> 00:11:04.334
And it's not designed just to satisfy
the desires of one person, the user,

00:11:04.358 --> 00:11:07.496
but in fact it has to respect
the preferences of everybody.

00:11:09.083 --> 00:11:11.653
So it can deal with a certain
amount of nastiness,

00:11:11.677 --> 00:11:15.378
and it can even understand
that your nastiness, for example,

00:11:15.402 --> 00:11:18.073
you may take bribes as a passport official

00:11:18.097 --> 00:11:21.909
because you need to feed your family
and send your kids to school.

00:11:21.933 --> 00:11:24.839
It can understand that;
it doesn't mean it's going to steal.

00:11:24.863 --> 00:11:27.542
In fact, it'll just help you
send your kids to school.

00:11:28.796 --> 00:11:31.808
We are also computationally limited.

00:11:31.832 --> 00:11:34.337
Lee Sedol is a brilliant Go player,

00:11:34.361 --> 00:11:35.686
but he still lost.

00:11:35.710 --> 00:11:39.949
So if we look at his actions,
he took an action that lost the game.

00:11:39.973 --> 00:11:42.134
That doesn't mean he wanted to lose.

00:11:43.160 --> 00:11:45.200
So to understand his behavior,

00:11:45.224 --> 00:11:48.868
we actually have to invert
through a model of human cognition

00:11:48.892 --> 00:11:53.869
that includes our computational
limitations -- a very complicated model.

00:11:53.893 --> 00:11:56.886
But it's still something
that we can work on understanding.

00:11:57.696 --> 00:12:02.016
Probably the most difficult part,
from my point of view as an AI researcher,

00:12:02.040 --> 00:12:04.615
is the fact that there are lots of us,

00:12:06.114 --> 00:12:09.695
and so the machine has to somehow
trade off, weigh up the preferences

00:12:09.719 --> 00:12:11.944
of many different people,

00:12:11.968 --> 00:12:13.874
and there are different ways to do that.

00:12:13.898 --> 00:12:17.587
Economists, sociologists,
moral philosophers have understood that,

00:12:17.611 --> 00:12:20.066
and we are actively
looking for collaboration.

00:12:20.090 --> 00:12:23.341
Let's have a look and see what happens
when you get that wrong.

00:12:23.365 --> 00:12:25.498
So you can have
a conversation, for example,

00:12:25.522 --> 00:12:27.466
with your intelligent personal assistant

00:12:27.490 --> 00:12:29.775
that might be available
in a few years' time.

00:12:29.799 --> 00:12:32.323
Think of a Siri on steroids.

00:12:33.447 --> 00:12:37.769
So Siri says, "Your wife called
to remind you about dinner tonight."

00:12:38.436 --> 00:12:40.944
And of course, you've forgotten.
"What? What dinner?

00:12:40.968 --> 00:12:42.393
What are you talking about?"

00:12:42.417 --> 00:12:46.163
"Uh, your 20th anniversary at 7pm."

00:12:48.735 --> 00:12:52.454
"I can't do that. I'm meeting
with the secretary-general at 7:30.

00:12:52.478 --> 00:12:54.170
How could this have happened?"

00:12:54.194 --> 00:12:58.854
"Well, I did warn you, but you overrode
my recommendation."

00:12:59.966 --> 00:13:03.294
"Well, what am I going to do?
I can't just tell him I'm too busy."

00:13:04.310 --> 00:13:07.591
"Don't worry. I arranged
for his plane to be delayed."

00:13:07.615 --> 00:13:09.297
(Laughter)

00:13:10.069 --> 00:13:12.170
"Some kind of computer malfunction."

00:13:12.194 --> 00:13:13.406
(Laughter)

00:13:13.430 --> 00:13:15.047
"Really? You can do that?"

00:13:16.220 --> 00:13:18.399
"He sends his profound apologies

00:13:18.423 --> 00:13:20.978
and looks forward to meeting you
for lunch tomorrow."

00:13:21.002 --> 00:13:22.301
(Laughter)

00:13:22.325 --> 00:13:26.728
So the values here --
there's a slight mistake going on.

00:13:26.752 --> 00:13:29.761
This is clearly following my wife's values

00:13:29.785 --> 00:13:31.854
which is "Happy wife, happy life."

00:13:31.878 --> 00:13:33.461
(Laughter)

00:13:33.485 --> 00:13:34.929
It could go the other way.

00:13:35.641 --> 00:13:37.842
You could come home
after a hard day's work,

00:13:37.866 --> 00:13:40.061
and the computer says, "Long day?"

00:13:40.085 --> 00:13:42.373
"Yes, I didn't even have time for lunch."

00:13:42.397 --> 00:13:43.679
"You must be very hungry."

00:13:43.703 --> 00:13:46.349
"Starving, yeah.
Could you make some dinner?"

00:13:47.890 --> 00:13:49.980
"There's something I need to tell you."

00:13:50.004 --> 00:13:51.159
(Laughter)

00:13:52.013 --> 00:13:56.918
"There are humans in South Sudan
who are in more urgent need than you."

00:13:56.942 --> 00:13:58.046
(Laughter)

00:13:58.070 --> 00:14:00.145
"So I'm leaving. Make your own dinner."

00:14:00.169 --> 00:14:02.169
(Laughter)

00:14:02.643 --> 00:14:04.382
So we have to solve these problems,

00:14:04.406 --> 00:14:06.921
and I'm looking forward
to working on them.

00:14:06.945 --> 00:14:08.788
There are reasons for optimism.

00:14:08.812 --> 00:14:09.971
One reason is,

00:14:09.995 --> 00:14:11.863
there is a massive amount of data.

00:14:11.887 --> 00:14:14.681
Because remember -- I said
they're going to read everything

00:14:14.705 --> 00:14:16.251
the human race has ever written.

00:14:16.275 --> 00:14:18.999
Most of what we write about
is human beings doing things

00:14:19.023 --> 00:14:20.937
and other people getting upset about it.

00:14:20.961 --> 00:14:23.359
So there's a massive amount
of data to learn from.

00:14:23.383 --> 00:14:25.619
There's also a very
strong economic incentive

00:14:27.151 --> 00:14:28.337
to get this right.

00:14:28.361 --> 00:14:30.362
So imagine your domestic robot's at home.

00:14:30.386 --> 00:14:33.453
You're late from work again
and the robot has to feed the kids,

00:14:33.477 --> 00:14:36.300
and the kids are hungry
and there's nothing in the fridge.

00:14:36.324 --> 00:14:38.929
And the robot sees the cat.

00:14:38.953 --> 00:14:40.645
(Laughter)

00:14:40.669 --> 00:14:44.859
And the robot hasn't quite learned
the human value function properly,

00:14:44.883 --> 00:14:46.134
so it doesn't understand

00:14:46.158 --> 00:14:51.002
the sentimental value of the cat outweighs
the nutritional value of the cat.

00:14:51.026 --> 00:14:52.121
(Laughter)

00:14:52.145 --> 00:14:53.893
So then what happens?

00:14:53.917 --> 00:14:57.214
Well, it happens like this:

00:14:57.238 --> 00:15:00.202
"Deranged robot cooks kitty
for family dinner."

00:15:00.226 --> 00:15:04.749
That one incident would be the end
of the domestic robot industry.

00:15:04.773 --> 00:15:08.145
So there's a huge incentive
to get this right

00:15:08.169 --> 00:15:10.884
long before we reach
superintelligent machines.

00:15:11.948 --> 00:15:13.483
So to summarize:

00:15:13.507 --> 00:15:16.388
I'm actually trying to change
the definition of AI

00:15:16.412 --> 00:15:19.405
so that we have provably
beneficial machines.

00:15:19.429 --> 00:15:20.651
And the principles are:

00:15:20.675 --> 00:15:22.073
machines that are altruistic,

00:15:22.097 --> 00:15:24.901
that want to achieve only our objectives,

00:15:24.925 --> 00:15:28.041
but that are uncertain
about what those objectives are,

00:15:28.065 --> 00:15:30.063
and will watch all of us

00:15:30.087 --> 00:15:33.290
to learn more about what it is
that we really want.

00:15:34.193 --> 00:15:37.752
And hopefully in the process,
we will learn to be better people.

00:15:37.776 --> 00:15:38.967
Thank you very much.

00:15:38.991 --> 00:15:42.700
(Applause)

00:15:42.724 --> 00:15:44.592
Chris Anderson: So interesting, Stuart.

00:15:44.616 --> 00:15:47.786
We're going to stand here a bit
because I think they're setting up

00:15:47.810 --> 00:15:48.961
for our next speaker.

00:15:48.985 --> 00:15:50.523
A couple of questions.

00:15:50.547 --> 00:15:56.000
So the idea of programming in ignorance
seems intuitively really powerful.

00:15:56.024 --> 00:15:57.618
As you get to superintelligence,

00:15:57.642 --> 00:15:59.900
what's going to stop a robot

00:15:59.924 --> 00:16:02.776
reading literature and discovering
this idea that knowledge

00:16:02.800 --> 00:16:04.372
is actually better than ignorance

00:16:04.396 --> 00:16:08.614
and still just shifting its own goals
and rewriting that programming?

00:16:09.512 --> 00:16:15.868
Stuart Russell: Yes, so we want
it to learn more, as I said,

00:16:15.892 --> 00:16:17.179
about our objectives.

00:16:17.203 --> 00:16:22.724
It'll only become more certain
as it becomes more correct,

00:16:22.748 --> 00:16:24.693
so the evidence is there

00:16:24.717 --> 00:16:27.441
and it's going to be designed
to interpret it correctly.

00:16:27.465 --> 00:16:31.421
It will understand, for example,
that books are very biased

00:16:31.445 --> 00:16:32.928
in the evidence they contain.

00:16:32.952 --> 00:16:35.349
They only talk about kings and princes

00:16:35.373 --> 00:16:38.173
and elite white male people doing stuff.

00:16:38.197 --> 00:16:40.293
So it's a complicated problem,

00:16:40.317 --> 00:16:44.189
but as it learns more about our objectives

00:16:44.213 --> 00:16:46.276
it will become more and more useful to us.

00:16:46.300 --> 00:16:48.826
CA: And you couldn't
just boil it down to one law,

00:16:48.850 --> 00:16:50.500
you know, hardwired in:

00:16:50.524 --> 00:16:53.817
"if any human ever tries to switch me off,

00:16:53.841 --> 00:16:55.776
I comply. I comply."

00:16:55.800 --> 00:16:56.982
SR: Absolutely not.

00:16:57.006 --> 00:16:58.505
That would be a terrible idea.

00:16:58.529 --> 00:17:01.218
So imagine that you have
a self-driving car

00:17:01.242 --> 00:17:03.675
and you want to send your five-year-old

00:17:03.699 --> 00:17:04.873
off to preschool.

00:17:04.897 --> 00:17:07.998
Do you want your five-year-old
to be able to switch off the car

00:17:08.022 --> 00:17:09.235
while it's driving along?

00:17:09.259 --> 00:17:10.418
Probably not.

00:17:10.442 --> 00:17:15.145
So it needs to understand how rational
and sensible the person is.

00:17:15.169 --> 00:17:16.845
The more rational the person,

00:17:16.869 --> 00:17:18.972
the more willing you are
to be switched off.

00:17:18.996 --> 00:17:21.539
If the person is completely
random or even malicious,

00:17:21.563 --> 00:17:24.075
then you're less willing
to be switched off.

00:17:24.099 --> 00:17:25.965
CA: All right. Stuart, can I just say,

00:17:25.989 --> 00:17:28.303
I really, really hope you
figure this out for us.

00:17:28.327 --> 00:17:30.702
Thank you so much for that talk.
That was amazing.

00:17:30.726 --> 00:17:31.893
SR: Thank you.

00:17:31.917 --> 00:17:33.754
(Applause)

