WEBVTT
Kind: captions
Language: tr

00:00:00.000 --> 00:00:07.000
Çeviri: Gunperi Sisman
Gözden geçirme: Sevkan Uzel

00:00:12.532 --> 00:00:14.184
Burada gördüğünüz Lee Sedol.

00:00:14.184 --> 00:00:17.945
Lee Sedol dünyadaki en iyi
Go oyuncularından biri.

00:00:18.129 --> 00:00:21.254
Burada Silikon Vadisi'ndeki 
arkadaşlarımın deyişiyle

00:00:21.254 --> 00:00:22.827
bir 'Aman Tanrım' anı yaşıyor

00:00:22.827 --> 00:00:23.645
(Gülüşmeler)

00:00:23.669 --> 00:00:25.967
Bu, yapay zekanın beklediğimizden

00:00:25.967 --> 00:00:29.177
çok daha hızlı ilerlediğini 
anladığımız bir an.

00:00:29.914 --> 00:00:33.469
İnsanlar Go tahtasında kaybetti.
Peki ya gerçek hayatta?

00:00:33.469 --> 00:00:35.145
Gerçek dünya Go tahtasından

00:00:35.169 --> 00:00:37.418
çok daha büyük ve 
çok daha karmaşık.

00:00:37.442 --> 00:00:39.261
Daha az şeffaf

00:00:39.285 --> 00:00:41.323
ama yine de bir karar
sorunu oluşturuyor.

00:00:42.708 --> 00:00:45.133
Eğer gelmekte olan
bazı teknolojileri

00:00:45.133 --> 00:00:46.922
düşünecek olursak...

00:00:47.558 --> 00:00:51.747
Noriko [Arai] makinelerin henüz
okuyamadığından bahsetti;

00:00:51.747 --> 00:00:53.527
en azından okuduklarını anlamadıklarından

00:00:53.527 --> 00:00:54.977
Ama bu gerçekleşecek

00:00:55.001 --> 00:00:57.442
ve bunun gerçekleşmesinin
hemen ardından makineler,

00:00:57.442 --> 00:01:00.053
insan türünün tarih boyunca yazdığı

00:01:00.053 --> 00:01:02.579
her şeyi okumuş olacak.

00:01:03.430 --> 00:01:05.700
Bu da makinelere
insanlardan daha öteye

00:01:05.724 --> 00:01:08.644
bakma yetisini verecek.

00:01:08.668 --> 00:01:10.348
Go oyununda gördüğümüz gibi

00:01:10.372 --> 00:01:12.536
eğer daha fazla bilgiye erişebilirlerse,

00:01:12.560 --> 00:01:15.972
gerçek hayatta bizden daha iyi
kararlar alabilecekler.

00:01:18.122 --> 00:01:20.162
Peki bu iyi bir şey mi?

00:01:22.674 --> 00:01:24.730
Umarım öyledir.

00:01:26.328 --> 00:01:28.221
Tüm uygarlığımız,

00:01:28.221 --> 00:01:31.245
değer verdiğimiz her şey
zekamıza dayanıyor.

00:01:31.848 --> 00:01:35.523
Eğer bizlerin daha fazla zekaya
erişebilme şansı olsa

00:01:35.523 --> 00:01:38.905
insan türünün yapabileceği şeylerin
bir sınırı olmayacak.

00:01:40.485 --> 00:01:43.810
Sanırım bu, bazılarının
tanımladığı gibi

00:01:43.834 --> 00:01:46.980
dünya tarihindeki en önemli
olay olacaktır.

00:01:48.485 --> 00:01:51.314
Peki neden bazıları
yapay zekanın insan türünün

00:01:51.338 --> 00:01:54.414
sonunu getirebileceğini söylüyor?

00:01:55.098 --> 00:01:56.917
Bu yeni bir endişe mi?

00:01:56.941 --> 00:02:01.051
Yani bunu düşünen sadece Elon
Musk, Bill Gates ve Stephen Hawking mi?

00:02:01.773 --> 00:02:05.035
Aslında hayır. Bu fikir
uzun zamandır var.

00:02:05.059 --> 00:02:07.021
İşte size bir alıntı:

00:02:07.045 --> 00:02:11.395
''Makineleri itaatkâr bir pozisyonda
tutabiliyor olsak bile

00:02:11.419 --> 00:02:14.403
-örneğin güç ünitelerini
stratejik anlarda kapatarak-

00:02:14.427 --> 00:02:17.664
(birazdan bu ''güç ünitesini kapama''
fikrine geri döneceğim)

00:02:17.688 --> 00:02:21.062
insan türü olarak bizler
çok aşağılanmış gibi hissedeceğiz."

00:02:21.997 --> 00:02:25.445
Peki bunu kim söyledi?
1951 yılında Alan Turing.

00:02:26.120 --> 00:02:28.897
Alan Turing bildiğiniz gibi
bilgisayar biliminin

00:02:28.897 --> 00:02:32.355
ve ayrıca pek çok yönden
yapay zekanın da babasıdır.

00:02:33.059 --> 00:02:34.971
Eğer bu problemi kendi türümüzden

00:02:34.971 --> 00:02:38.345
daha zeki bir tür yaratma
problemi olarak düşünürsek,

00:02:38.636 --> 00:02:41.398
buna ''goril problemi'' de 
diyebiliriz.

00:02:41.615 --> 00:02:45.915
Çünkü gorillerin ataları
bunu birkaç milyon yıl önce yaptı:

00:02:45.939 --> 00:02:48.489
Gorillere bugün şu soruyu sorabiliriz:

00:02:48.572 --> 00:02:50.069
Bu iyi bir fikir miydi?

00:02:50.069 --> 00:02:53.609
Burada toplantıdalar 
ve bu fikri değerlendiriyorlar.

00:02:53.609 --> 00:02:55.959
Sonunda da bunun kötü bir fikir 

00:02:55.959 --> 00:02:57.649
olduğuna karar veriyorlar.

00:02:57.649 --> 00:03:00.131
Türümüz zor durumda.

00:03:00.358 --> 00:03:04.621
Aslında gözlerindeki varoluşsal
üzüntüyü görebiliyorsunuz.

00:03:04.645 --> 00:03:06.285
(Gülüşmeler)

00:03:06.309 --> 00:03:09.873
Kendi türünüzden daha zeki 
bir şey yaratmanın

00:03:09.873 --> 00:03:13.738
pek de iyi bir fikir olmayabileceğine
dair tatsız bir his...

00:03:14.088 --> 00:03:15.799
Bu konuda ne yapabiliriz?

00:03:15.823 --> 00:03:20.590
Doğrusu, yapay zeka ile uğraşmayı
bırakmaktan başka bir şey yapamayız.

00:03:20.614 --> 00:03:23.244
Ama az önce sözünü ettiğim
tüm yararlarından dolayı

00:03:23.244 --> 00:03:25.534
ve ben de bir YZ araştırmacısı
olduğumdan,

00:03:25.534 --> 00:03:27.109
buna katılmıyorum.

00:03:27.109 --> 00:03:30.263
Aslında ben YZ yapmaya
çalışmayı sürdürmek istiyorum.

00:03:30.295 --> 00:03:33.113
Yani aslında soruna biraz daha 
yakından bakmalıyız.

00:03:33.137 --> 00:03:34.508
Sorun tam olarak nedir?

00:03:34.532 --> 00:03:38.558
Neden daha iyi bir YZ
muhtemelen bir felaket olsun?

00:03:38.878 --> 00:03:40.986
İşte bir başka alıntı:

00:03:41.495 --> 00:03:45.074
''Makinelere verdiğimiz amaçların 
aslında bizlerin arzuladığı

00:03:45.074 --> 00:03:47.772
amaçlar olduğuna emin olmalıyız.''

00:03:48.102 --> 00:03:51.910
Norbert Wiener bunu 1960 yılında,
ilk öğrenim sistemlerinden birinin,

00:03:51.910 --> 00:03:58.564
dama oynamayı, yaratıcısından daha iyi
başardığını izledikten sonra söylemişti.

00:04:00.202 --> 00:04:04.609
Ama bu pekâlâ Kral Midas
tarafından da söylenmiş olabilirdi.

00:04:04.733 --> 00:04:08.547
Kral Midas şöyle demişti: ''Dokunduğum
her şeyin altın olmasını istiyorum,''

00:04:08.547 --> 00:04:10.894
ve istediğini tam olarak elde etti.

00:04:10.894 --> 00:04:14.327
Onun makineye verdiği amacın
bu olduğu söylenebilir.

00:04:14.807 --> 00:04:18.521
Bunun üstüne, yiyeceği ve içeceği
ve akrabaları altına dönüştü.

00:04:18.521 --> 00:04:21.136
Kendisi de açlık ve sefalet içinde öldü.

00:04:21.784 --> 00:04:24.605
O zaman, belirttiğimiz ereklerin,

00:04:24.629 --> 00:04:27.934
aslında istediğimiz şey ile
gerçekte uyuşmaması durumuna

00:04:27.958 --> 00:04:30.371
''Kral Midas Problemi'' diyelim.

00:04:30.395 --> 00:04:34.268
Modern terimlerle buna
''değer uyuşmazlığı problemi'' diyoruz.

00:04:36.337 --> 00:04:40.352
Problem, yanlış amaç
yerleştirmekten ibaret değil.

00:04:40.376 --> 00:04:41.938
Bir yönü daha var.

00:04:41.980 --> 00:04:43.923
Bir makineye bir amaç verirseniz,

00:04:43.947 --> 00:04:47.155
örneğin ''kahvemi getir'' gibi 
basit bir hedef olsa bile

00:04:47.238 --> 00:04:49.709
makine kendi kendine şöyle der:

00:04:49.843 --> 00:04:53.176
''Kahveyi getirmekte nasıl
başarısızlığa uğrayabilirim?

00:04:53.200 --> 00:04:55.390
Biri beni kapatabilir.

00:04:55.405 --> 00:04:57.882
Peki. Bunu önlemek için 
bir şeyler yapmalıyım.

00:04:57.882 --> 00:04:59.972
'Kapama' düğmemi işlev 
dışı bırakacağım.

00:05:00.174 --> 00:05:03.017
Bana verilen komutu yerine
getirmemi engelleyebilecek

00:05:03.017 --> 00:05:06.536
her türlü müdahaleye karşı kendimi
savunmak için ne gerekiyorsa yapacağım."

00:05:06.536 --> 00:05:09.032
Sonuç olarak,
gayet savunmacı bir moddaki

00:05:09.033 --> 00:05:11.232
bu tek-hedef odaklı hareket,

00:05:11.232 --> 00:05:15.266
insan türünün gerçek
hedefleriyle örtüşmüyor.

00:05:15.712 --> 00:05:18.444
Karşı karşıya kaldığımız sorun aslında bu.

00:05:18.687 --> 00:05:23.594
Doğrusu, tam da bu,
bu konuşmanın en değerli dersi.

00:05:23.618 --> 00:05:25.828
Hatırlamanız gereken bir şey varsa,

00:05:25.828 --> 00:05:28.308
o da ölüyken kahve
getiremeyeceğiniz olmalı.

00:05:28.537 --> 00:05:29.457
(Gülüşmeler)

00:05:29.481 --> 00:05:33.310
Bu çok basit. Sadece bunu hatırlayın.
Kendinize bunu günde üç kez tekrar edin.

00:05:33.334 --> 00:05:35.155
(Gülüşmeler)

00:05:35.179 --> 00:05:37.357
Aslında, bu tam da

00:05:37.357 --> 00:05:40.945
''2001: [Bir Uzay Destanı]'' filmindeki
hikayedir.

00:05:41.046 --> 00:05:44.096
HAL'in insanların hedefleriyle örtüşmeyen

00:05:44.096 --> 00:05:46.892
bir hedefi, bir misyonu vardır.

00:05:46.916 --> 00:05:49.166
Bu da bir çatışmaya neden olur.

00:05:49.314 --> 00:05:52.283
Neyse ki, HAL süper-zekalı değildir.

00:05:52.307 --> 00:05:55.894
Çok zeki olsa da,
sonunda Dave onu atlatır

00:05:55.918 --> 00:05:58.387
ve onu kapatmayı başarır.

00:06:01.328 --> 00:06:03.996
Ama biz o kadar şanslı olamayabiliriz.

00:06:07.873 --> 00:06:10.045
Peki ne yapacağız o zaman?

00:06:12.161 --> 00:06:15.182
Ben komutları zekice izleyen
makinelerle ilgili

00:06:15.182 --> 00:06:18.087
bu klasik düşünceden
kurtulmamız için

00:06:18.087 --> 00:06:21.468
yapay zekayı yeniden
tanımlamaya çalışıyorum.

00:06:22.532 --> 00:06:24.330
Bununla ilgili üç ilke var.

00:06:24.354 --> 00:06:27.643
Birincisi, çıkar gözetmemezlik de 
denilebilecek bir ilke.

00:06:27.667 --> 00:06:31.259
Yani robotların tek hedeflerinin
insanların hedeflerine,

00:06:31.259 --> 00:06:35.199
insanların değerlerine
hitap etmek olması.

00:06:36.637 --> 00:06:39.967
'Değerler' derken, samimiyete dayanan
değerleri kastetmiyorum.

00:06:39.991 --> 00:06:43.888
Sadece, insanların hayatlarının
nasıl olması gerektiği hakkındaki

00:06:43.888 --> 00:06:46.056
tercihlerini kastetiyorum.

00:06:46.656 --> 00:06:48.174
Bu, aslında Asimov'un

00:06:48.174 --> 00:06:52.154
"robotların kendi varlıklarını korumaları
gerektiği" kuralını çiğniyor.

00:06:52.154 --> 00:06:55.442
Robotlar, kendi varlıklarını
korumakla ilgilenmez.

00:06:57.162 --> 00:07:01.505
İkinci kuralın
mütevazilikle ilgili olduğu söylenebilir.

00:07:01.794 --> 00:07:05.537
Bunun, bir robotun güvenilir olmasında
büyük katkısı olduğu ortaya çıktı.

00:07:05.561 --> 00:07:08.316
Bu kural, robotların insanların 
değerlerinin ne olduğunu

00:07:08.338 --> 00:07:11.159
bilmeden onları arttırmaya
çalıştığını söylüyor.

00:07:11.159 --> 00:07:14.329
Yani bilmediği değerleri
arttırmaya çalışıyor.

00:07:14.614 --> 00:07:18.460
Bu da tek-hedefli uğraş
problemini önler.

00:07:18.530 --> 00:07:21.132
Bu belirsizliğin çok önemli
olduğu ortaya çıktı.

00:07:21.386 --> 00:07:23.495
Bunun bize yararlı olabilmesi için

00:07:23.495 --> 00:07:26.890
robotların bizlerin ne isteyebileceği 
hakkında bir fikri olmalı.

00:07:27.043 --> 00:07:32.274
Bu bilgiyi de birincil olarak insanların
tercihlerini gözlemleyerek elde eder.

00:07:32.274 --> 00:07:34.879
Dolayısıyla bizim tercihlerimiz,

00:07:34.879 --> 00:07:39.259
yaşamlarımızın nasıl olmasını
istediğimize ilişkin bilgi sunuyor.

00:07:40.012 --> 00:07:41.795
İşte üç ilke bunlar.

00:07:41.809 --> 00:07:43.611
O zaman bu ilkelerin,

00:07:43.611 --> 00:07:46.073
Turing'in önerdiği
''makineyi kapatabilir misin?''

00:07:46.073 --> 00:07:48.753
sorusuna nasıl
uygulandığını inceleyelim.

00:07:48.893 --> 00:07:51.013
Burada gördüğünüz bir PR2 robotu.

00:07:51.037 --> 00:07:52.998
Bu gördüğünüz bizim
laboratuvarımızdaki PR2.

00:07:52.998 --> 00:07:56.435
Gördüğünüz gibi arkasında kocaman 
kırmızı bir ''kapat'' düğmesi var.

00:07:56.435 --> 00:07:58.976
Sorumuz şu:
Onu kapatmanıza izin verecek mi?

00:07:59.000 --> 00:08:00.625
Bunu klasik şekilde
yapacak olursak

00:08:00.625 --> 00:08:03.135
ve ona ''kahveyi getir, 
kahveyi getirmeliyim,

00:08:03.135 --> 00:08:06.159
ölürsem kahveyi getiremem''
amacını verirsek,

00:08:06.159 --> 00:08:09.264
tabi PR2 konuşmamı dinlemekteydi,

00:08:09.264 --> 00:08:13.606
dolayısıyla ''kapama düğmemi 
işlev dışı bırakmalıyım

00:08:14.516 --> 00:08:16.746
ve Starbucks'ta bana
müdahale etmeye çalışan

00:08:16.746 --> 00:08:19.486
diğer insanları da belki
elektrikle şoklarım'' der.

00:08:19.486 --> 00:08:21.064
(Gülüşmeler)

00:08:21.064 --> 00:08:23.517
Yani bu kaçınılmaz
görünüyor, değil mi?

00:08:23.517 --> 00:08:26.309
Bu çeşit bir hata modu
kaçınılmaz görünüyor ve

00:08:26.309 --> 00:08:29.726
bu da kesin ve net bir hedef 
konulmasından kaynaklanıyor.

00:08:30.632 --> 00:08:33.776
Peki, eğer makine hedef hakkında
emin değilse ne olur?

00:08:33.800 --> 00:08:35.927
O zaman daha farklı bir mantık yürütür.

00:08:35.951 --> 00:08:38.735
O zaman der ki, 
''Peki, insan beni kapatabilir

00:08:38.964 --> 00:08:41.290
ama sadece yanlış bişey yaparsam.

00:08:41.567 --> 00:08:44.042
Ben neyin yanlış olduğunu
tam bilmiyorum

00:08:44.066 --> 00:08:46.110
ama yanlış yapmak
istemediğimi biliyorum.''

00:08:46.134 --> 00:08:49.144
Burada birinci ve ikinci
ilkeleri görebiliyorsunuz.

00:08:49.168 --> 00:08:53.037
''Öyleyse insanın beni
kapatmasına izin vermeliyim.''

00:08:53.541 --> 00:08:57.777
Aslında bir robotun insana
kendini kapatması için izin vermesindeki

00:08:57.777 --> 00:09:00.014
motivasyonu hesaplayabilirsiniz.

00:09:00.038 --> 00:09:02.282
Bu doğrudan, komutun belirsizlik

00:09:02.282 --> 00:09:04.722
derecesinin ne olduğuna bağlıdır.

00:09:05.797 --> 00:09:08.317
Sonra makine kapatılınca,

00:09:08.320 --> 00:09:10.730
üçüncü ilke devreye girer.

00:09:10.730 --> 00:09:13.156
Çünkü izlediği amaçlara
ilişkin bir şey öğrenir,

00:09:13.156 --> 00:09:15.655
çünkü yapmış olduğu şeyin
yanlış olduğunu öğrenir.

00:09:16.025 --> 00:09:19.046
Aslında, matematikçilerin
sıkça yaptığı gibi

00:09:19.046 --> 00:09:21.921
Yunan harflerinin uygun kullanımıyla,

00:09:21.921 --> 00:09:25.405
böyle bir robotun,
insanlara yararının kanıtlanabileceğini

00:09:25.405 --> 00:09:27.552
söyleyen bir teoremi kanıtlayabiliriz.

00:09:27.576 --> 00:09:30.683
Bu şekilde tasarlanmış
bir makinenin varlığı,

00:09:30.683 --> 00:09:32.889
yokluğuna oranla
sizi daha iyi kılacaktır.

00:09:33.057 --> 00:09:36.293
Bu çok basit örnek aslında 
başarmaya çalıştığımız

00:09:36.293 --> 00:09:39.890
insan-uyumlu YZ
yolunda ilk adımımızdır.

00:09:42.517 --> 00:09:45.158
Şimdi bu üçüncü ilke

00:09:45.158 --> 00:09:48.754
biraz kafanızı karıştırıyor olabilir.

00:09:48.754 --> 00:09:52.133
Herhalde şunu düşünüyorsunuz:
''Eğer ben yanlış hareket ediyorsam,

00:09:52.157 --> 00:09:55.086
robotumun da benim gibi hareket 
etmesini istemiyorum.

00:09:55.090 --> 00:09:58.138
Geceleri usulca gidip buzluktan
bişeyler aşırıyorum.

00:09:58.138 --> 00:09:59.786
Bunun gibi şeyler yapıyorum.''

00:09:59.786 --> 00:10:02.557
Robotun yapmamasını istediğiniz
birçok şey olabilir.

00:10:02.581 --> 00:10:04.652
Ama aslında bu pek öyle çalışmıyor.

00:10:04.676 --> 00:10:06.831
Sırf siz kötü bir şey yaptınız diye,

00:10:06.855 --> 00:10:09.478
robot da sizin hareketinizi
taklit edecek değil.

00:10:09.482 --> 00:10:12.716
Sizi buna iteni anlayacak ve
belki de direnmeniz için

00:10:12.716 --> 00:10:14.926
size yardım edecek, 
eğer uygunsa.

00:10:16.026 --> 00:10:17.790
Ama bu hâlâ zor.

00:10:18.122 --> 00:10:20.611
Aslında yapmaya çalıştığımız şey,

00:10:20.611 --> 00:10:25.637
makinelerin herhangi biri ve
yaşayabilecekleri herhangi bir olası yaşam

00:10:25.637 --> 00:10:29.316
ve diğer herkesin yaşamı için
öngörüler yapmalarını sağlamak:

00:10:29.733 --> 00:10:32.814
Neyi tercih ederlerdi?

00:10:33.881 --> 00:10:36.689
Bunu yapmak konusunda
çok fazla güçlük var.

00:10:36.689 --> 00:10:39.681
Bunun yakın zamanda
çözüleceğini de sanmıyorum.

00:10:39.815 --> 00:10:42.888
Gerçek zorluk, aslında biziz.

00:10:43.969 --> 00:10:47.086
Bahsettiğim gibi,
kötü davranışlarımız var.

00:10:47.110 --> 00:10:49.731
Aslına bakarsanız,
bazılarımız gerçekten kötü.

00:10:50.251 --> 00:10:53.303
Dediğim gibi robot bu davranışları
tekrarlamak zorunda değil.

00:10:53.327 --> 00:10:56.118
Robotun kendine ait bir hedefi yok.

00:10:56.142 --> 00:10:58.159
Onlar bütünüyle fedakâr.

00:10:59.113 --> 00:11:02.368
Bu sadece bir kişinin,
kullanıcının arzularını

00:11:02.368 --> 00:11:05.143
tatmin etmek üzere
tasarlanmış değil,

00:11:05.143 --> 00:11:08.713
aslında herkesin tercihlerine saygı
duymak için tasarlanırlar.

00:11:08.813 --> 00:11:11.597
Makineler bir kısım
kötü davranışı algılayabilir

00:11:11.597 --> 00:11:14.992
ve hatta kötü davranışın
ardındaki nedenleri de anlayabilir.

00:11:14.992 --> 00:11:18.047
Örneğin görev başında rüşvet
alan bir pasaport memuruysanız,

00:11:18.047 --> 00:11:21.909
bunu aile geçindirmek ve çocukları okula 
göndermek için yaptığınızı anlayabilir.

00:11:21.933 --> 00:11:24.633
Bu onun da çalacağı 
anlamına gelmez.

00:11:24.633 --> 00:11:28.312
Aslında çocuklarınızı okula
gönderebilmeniz için size yardım edecek.

00:11:28.796 --> 00:11:31.808
Bizler ayrıca hesaplama
açısından sınırlıyız.

00:11:31.832 --> 00:11:34.337
Lee Sedol parlak bir Go oyuncusu,

00:11:34.361 --> 00:11:36.153
ama yine de kaybetti.

00:11:36.153 --> 00:11:38.086
Hamlelerine bakılırsa,

00:11:38.086 --> 00:11:40.409
oyunu kaybetmesine
neden olan bir hamle yaptı.

00:11:40.409 --> 00:11:42.964
Bu onun kaybetmek istediği
anlamına gelmiyor.

00:11:43.160 --> 00:11:46.930
Yani onun davranışını anlamak için,
bizim hesaplama konusundaki

00:11:46.930 --> 00:11:51.408
sınırlarımızı da içeren bir insan zekası
modeli süzgecinden geçirmek gerek.

00:11:51.408 --> 00:11:53.613
Bu da çok karmaşık bir model.

00:11:53.613 --> 00:11:56.886
Ama yine de anlamaya 
çalışabileceğimiz bir kavram.

00:11:57.696 --> 00:12:02.016
Bir YZ araştırmacısı olarak baktığımda,
en zor olan kısım belki de

00:12:02.040 --> 00:12:05.225
aslında çok fazla sayıda insan oluşu.

00:12:05.724 --> 00:12:09.509
Makineler bir şekilde tüm farklı 
insanların tercihlerini

00:12:09.509 --> 00:12:11.944
ve bunların ağırlıklarını
analiz etmeliler.

00:12:11.968 --> 00:12:13.874
Bunu yapmanın farklı yolları var.

00:12:13.898 --> 00:12:16.921
Ekonomistler, sosyologlar,
filozoflar bunu anlamıştı

00:12:16.921 --> 00:12:20.066
ve biz de aktif olarak bir 
işbirliği arayışındayız.

00:12:20.090 --> 00:12:23.341
O zaman bunu yanlış anladığınızda
neler olabileceğine bir bakalım.

00:12:23.365 --> 00:12:25.498
Örneğin zeki kişisel asistanınızla

00:12:25.522 --> 00:12:27.466
bir sohbet yapabilirsiniz;

00:12:27.490 --> 00:12:29.775
bu birkaç yıl içinde mümkün olabilir.

00:12:29.799 --> 00:12:32.953
Yani steroidler verilmiş
bir Siri düşünün.

00:12:33.447 --> 00:12:37.769
Siri diyor ki, ''Eşin bu akşamki yemek
randevunuzu hatırlatmak için aradı.''

00:12:38.436 --> 00:12:40.944
Tabi siz bunu unutmuştunuz.
''Ne? Ne yemeği?

00:12:40.968 --> 00:12:42.393
Neden bahsediyorsun?''

00:12:42.417 --> 00:12:46.163
''Bu akşam 7'de olan
20. yıldönümü yemeğiniz.''

00:12:48.735 --> 00:12:52.454
''Ama yemeğe katılamam, saat 7:30'da 
Genel Sekreter ile buluşuyorum.

00:12:52.478 --> 00:12:54.170
Bu nasıl olabildi?''

00:12:54.194 --> 00:12:58.854
''Sizi daha önce bununla ilgili uyarmıştım
ama önerimi çiğnediniz.''

00:12:59.966 --> 00:13:03.904
''Peki ne yapacağım? Genel sekretere
çok meşgul olduğumu söyleyemem.''

00:13:04.310 --> 00:13:07.941
"Merak etme, uçağının gecikmesini
sağlıyorum.''

00:13:07.941 --> 00:13:09.297
(Gülüşmeler)

00:13:10.069 --> 00:13:12.300
''Bir çeşit bilgisayar hatası.''

00:13:12.300 --> 00:13:13.406
(Gülüşmeler)

00:13:13.430 --> 00:13:15.557
''Gerçekten mi? 
Bunu yapabiliyor musun?''

00:13:16.060 --> 00:13:18.339
''Size en içten üzüntülerini
iletiyor ve

00:13:18.339 --> 00:13:21.158
sizinle yarın öğlen yemeğinde
görüşmeyi bekliyor.''

00:13:21.158 --> 00:13:22.301
(Gülüşmeler)

00:13:22.325 --> 00:13:26.728
Yani buradaki değerler...
Yanlış olan bir şeyler var.

00:13:26.752 --> 00:13:29.841
Bu kesinlikle eşimin
değerlerini takip etmekte,

00:13:29.841 --> 00:13:32.174
ki bu da ''mutlu bir eş,
mutlu bir hayat'' oluyor.

00:13:32.174 --> 00:13:33.461
(Gülüşmeler)

00:13:33.485 --> 00:13:35.469
Ama bu başka türlü de gidebilir.

00:13:35.641 --> 00:13:37.842
Yorucu bir iş gününden
eve geldiğinizde,

00:13:37.866 --> 00:13:40.061
bilgisayara size ''uzun bir 
gün müydü?'' diye soruyor.

00:13:40.085 --> 00:13:42.373
''Evet, öğle yemeğine
bile vaktim olmadı.''

00:13:42.397 --> 00:13:43.769
''Çok aç olmalısın.''

00:13:43.769 --> 00:13:46.349
''Evet, açlıktan ölüyorum.
Yiyecek bir şeyler hazırlayabilir misin?''

00:13:47.810 --> 00:13:50.130
''Sana söylemem gereken bişey var.''

00:13:50.130 --> 00:13:51.159
(Gülüşmeler)

00:13:52.013 --> 00:13:56.918
''Güney Sudan'da yemeğe 
senden daha muhtaç insanlar var.''

00:13:56.942 --> 00:13:58.046
(Gülüşmeler)

00:13:58.070 --> 00:14:01.125
''Bu yüzden ben ayrılıyorum.
Kendi yemeğini kendin yap.''

00:14:01.125 --> 00:14:02.169
(Gülüşmeler)

00:14:02.643 --> 00:14:04.382
Bu sorunları çözmeliyiz.

00:14:04.406 --> 00:14:07.171
Ben de bu sorunların üstünde çalışmayı
sabırsızlıkla bekliyorum.

00:14:07.171 --> 00:14:08.978
İyimser olmamız için 
sebeplerimiz var.

00:14:08.978 --> 00:14:09.971
Bir nedenimiz

00:14:09.995 --> 00:14:11.863
varolan büyük bir veri oluşu.

00:14:11.887 --> 00:14:14.365
Çünkü hatırlarsanız, insanların
tarih boyunca yazdığı

00:14:14.395 --> 00:14:16.055
her şeyi okuyacaklarını 
söylemiştim.

00:14:16.055 --> 00:14:18.693
Yazdığımız çoğu şey,
insanların yaptığı şeyler ve

00:14:18.693 --> 00:14:21.453
başkalarının bunlardan
dolayı üzülmesi ile ilgili.

00:14:21.453 --> 00:14:23.549
Yani öğrenilecek birçok şey var.

00:14:23.549 --> 00:14:25.601
Ayrıca bunu 
gerçekleştirmek için

00:14:25.601 --> 00:14:28.361
güçlü bir ekonomik itki de var.

00:14:28.361 --> 00:14:30.632
Evcil robotunuzun
evinizde olduğunu varsayın.

00:14:30.632 --> 00:14:34.093
Yine evinize geç gelmişsiniz ve
robotun çocuklara yemek yedirmesi lazım.

00:14:34.093 --> 00:14:36.820
Çocuklar aç ve buzdolabında
hiçbir şey yok.

00:14:36.820 --> 00:14:38.929
Robot kedinizi görüyor.

00:14:38.953 --> 00:14:40.645
(Gülüşmeler)

00:14:40.669 --> 00:14:43.693
Robot henüz insanların değer yargılarını

00:14:43.693 --> 00:14:46.104
tam olarak kullanmayı
öğrenemediğinden,

00:14:46.104 --> 00:14:48.596
kedinin duygusal değerinin, 
kedinin besin değerinden

00:14:48.606 --> 00:14:50.796
daha ağır bastığını anlamıyor.

00:14:50.796 --> 00:14:52.121
(Gülüşmeler)

00:14:52.145 --> 00:14:53.893
O zaman ne olur?

00:14:53.917 --> 00:14:57.018
Şöyle bir son dakika haberi:

00:14:57.018 --> 00:15:00.202
''Bozuk robot akşam yemeği
için kediyi pişirdi.''

00:15:00.226 --> 00:15:04.633
Böyle tek bir olay, evcil robot
endüstrisinin sonu olurdu.

00:15:04.633 --> 00:15:10.755
Bu yüzden süper-zeki makinelerden önce
bunu hatasız yapmamız gerekiyor.

00:15:11.948 --> 00:15:13.483
Kısaca özetleyecek olursak:

00:15:13.507 --> 00:15:16.388
Ben yapay zekanın tanımını
değiştirmeye çalışıyorum,

00:15:16.412 --> 00:15:19.405
ki yararları kanıtlanmış
makinelere sahip olabilelim.

00:15:19.429 --> 00:15:20.651
İlkeler şunlar:

00:15:20.675 --> 00:15:22.073
Fedakâr makineler,

00:15:22.097 --> 00:15:24.901
sadece bizim hedeflerimize 
ulaşmak istesinler.

00:15:24.925 --> 00:15:28.041
Ama bu hedeflerin ne olduğundan
emin olmasınlar

00:15:28.065 --> 00:15:30.063
ve öğrenmek için

00:15:30.087 --> 00:15:33.290
hepimizi izleyip, gerçekte
ne istediğimizi anlasınlar.

00:15:34.093 --> 00:15:37.752
Biz de ümit ederim ki bu süreçte
daha iyi insanlar olmayı öğrenelim.

00:15:37.776 --> 00:15:39.397
Çok teşekkür ederim.

00:15:39.397 --> 00:15:42.700
(Alkışlar)

00:15:42.724 --> 00:15:44.592
Chris Anderson: Bu çok ilginçti Stuart.

00:15:44.616 --> 00:15:47.786
Seninle biraz burada duracağız,
çünkü sanırım bir sonraki konuşmacı

00:15:47.810 --> 00:15:48.961
için sahneyi kuruyorlar.

00:15:48.985 --> 00:15:50.523
Sana birkaç sorum var.

00:15:50.547 --> 00:15:53.024
Robotlara cehaleti de programlamak 

00:15:53.024 --> 00:15:56.024
içgüdüsel olarak çok güçlü bir fikir
gibi duruyor.

00:15:56.024 --> 00:15:57.708
Süper-zekaya yaklaştıkça

00:15:57.708 --> 00:16:00.040
bir robotun tüm araştırmaları okuyarak,

00:16:00.070 --> 00:16:03.920
bilginin aslında cehaletten
daha iyi olduğunu keşfetmesi

00:16:03.920 --> 00:16:06.252
ve kendi hedeflerini belirleyip

00:16:06.252 --> 00:16:09.062
kendini programını yeniden
yazmasını ne durdurabilir?

00:16:09.132 --> 00:16:12.112
Stuart Russell: Evet, 
aslında dediğim gibi

00:16:12.112 --> 00:16:15.112
biz de onun kendi hedeflerimiz hakkında

00:16:15.112 --> 00:16:17.179
daha çok şey bilmesini istiyoruz.

00:16:17.203 --> 00:16:22.724
Onu, bunu doğru şekilde yorumlayacak
biçimde tasarlayacağız.

00:16:22.748 --> 00:16:26.973
Örneğin, kitapların içerdikleri
kanıtlar açısından

00:16:26.973 --> 00:16:32.261
son derece taraflı olduğunu anlayacak.

00:16:32.261 --> 00:16:36.051
Sadece kralların, prenslerin
ve elit beyaz erkeklerin

00:16:36.051 --> 00:16:38.222
yaptıklarından bahsediyorlar.

00:16:38.222 --> 00:16:40.473
Yani bu karmaşık bir problem,

00:16:40.473 --> 00:16:43.533
ama bizim hedeflerimizi öğrendikçe,

00:16:43.533 --> 00:16:46.417
bize giderek daha da yararlı olacaklar.

00:16:46.417 --> 00:16:49.000
CA: Ve bunu da tek bir 
kurala indirgeyemeyiz,

00:16:49.000 --> 00:16:49.780
değil mi, yani

00:16:49.780 --> 00:16:53.567
''eğer insan beni kapatmayı denerse,

00:16:53.567 --> 00:16:55.627
uyacağım. Uyacağım.''

00:16:55.627 --> 00:16:56.726
SR: Kesinlikle hayır.

00:16:56.726 --> 00:16:58.412
Bu berbat bir fikir olurdu.

00:16:58.412 --> 00:17:00.535
Yani kendini süren bir arabanız
olduğunu varsayın

00:17:00.535 --> 00:17:02.998
ve beş yaşındaki çocuğunuzu

00:17:02.998 --> 00:17:04.435
anaokuluna göndermek istiyorsunuz.

00:17:04.435 --> 00:17:05.435
Peki beş yaşındaki çocuğunuzun

00:17:05.435 --> 00:17:08.773
araba seyir halindeyken
arabayı kapatabilmesini ister miydiniz?

00:17:08.782 --> 00:17:10.855
Muhtemelen hayır.
Yani bu durumda robotun,

00:17:10.855 --> 00:17:14.972
bir insanın ne kadar mantıklı ve
rasyonel olduğunu anlaması gerekiyor.

00:17:14.972 --> 00:17:16.845
Bir insan ne kadar rasyonel ise

00:17:16.869 --> 00:17:18.972
siz de o kadar kapatılmayı
kabul edersiniz.

00:17:18.996 --> 00:17:21.586
Eğer bir kişi tamamen plansız
ve hatta kötü niyetliyse

00:17:21.643 --> 00:17:24.375
o zaman siz de kapatılmak için
daha az hevesli olursunuz.

00:17:24.555 --> 00:17:26.665
CA: Peki. Stuart,
sadece şunu söyleyebilirim.

00:17:26.705 --> 00:17:28.563
Umarım bunu hepimiz için çözebilirsin.

00:17:28.563 --> 00:17:30.672
Konuşman için çok teşekkürler.
Muhteşemdi.

00:17:30.672 --> 00:17:31.863
SR: Teşekkürler.

00:17:31.863 --> 00:17:33.540
(Alkışlar)

