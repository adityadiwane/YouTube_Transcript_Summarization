WEBVTT
Kind: captions
Language: ru

00:00:00.000 --> 00:00:07.000
Переводчик: Marina Moshnogorskaya
Редактор: Yulia Kallistratova

00:00:12.861 --> 00:00:15.995
Привет, я Джой, поэт кода.

00:00:16.019 --> 00:00:21.012
Моя миссия — остановить невидимую,
набирающую обороты силу,

00:00:21.036 --> 00:00:23.892
силу, которую я называю
«закодированный взгляд»,

00:00:23.916 --> 00:00:27.225
моё определение для алгоритмического сбоя.

00:00:27.249 --> 00:00:31.549
Необъективность алгоритма, как и
человека, ведёт к несправедливости.

00:00:31.573 --> 00:00:37.595
Алгоритмы, как вирусы, способны
распространять ошибки в больших масштабах

00:00:37.619 --> 00:00:39.201
и очень быстро.

00:00:39.763 --> 00:00:44.150
Алгоритмические отклонения
могут приводить к отчуждению

00:00:44.174 --> 00:00:46.302
и дискриминации.

00:00:46.326 --> 00:00:48.387
Давайте я покажу, что я имею в виду.

00:00:48.800 --> 00:00:51.236
(Видео) Джой: Привет,
камера. Вот моё лицо.

00:00:51.982 --> 00:00:53.846
Ты видишь моё лицо?

00:00:53.871 --> 00:00:55.496
Лицо без очков?

00:00:55.521 --> 00:00:57.735
Ты видишь её лицо.

00:00:58.057 --> 00:01:00.302
А моё?

00:01:03.710 --> 00:01:07.460
Теперь с маской. Видишь маску?

00:01:08.294 --> 00:01:10.659
Джой Буоламвини: Как так получилось?

00:01:10.683 --> 00:01:13.824
Почему я сижу за компьютером

00:01:13.848 --> 00:01:15.272
в белой маске,

00:01:15.296 --> 00:01:18.946
пытаясь быть распознанной
дешёвой камерой?

00:01:18.970 --> 00:01:21.261
Когда я не воюю с закодированным взглядом,

00:01:21.285 --> 00:01:22.805
будучи поэтом кода,

00:01:22.829 --> 00:01:26.101
я аспирант Медиа-лаборатории в МИТ,

00:01:26.125 --> 00:01:31.042
где у меня есть возможность работать
над всяческими фантастическими проектами,

00:01:31.066 --> 00:01:33.093
включая Зеркало стремлений,

00:01:33.117 --> 00:01:38.251
которое позволяет проецировать
цифровые маски на своё отражение.

00:01:38.275 --> 00:01:40.755
Например, если утром я хочу
чувствовать себя сильной,

00:01:40.785 --> 00:01:42.083
я могу наложить маску льва.

00:01:42.107 --> 00:01:45.603
Если я хочу приободриться,
то можно обойтись цитатой.

00:01:45.627 --> 00:01:48.616
Я использовала обычную
программу распознавания лиц,

00:01:48.640 --> 00:01:49.991
чтобы создать эту систему,

00:01:50.015 --> 00:01:55.118
но оказалось, что её очень трудно
тестировать, если на мне нет белой маски.

00:01:56.102 --> 00:02:00.448
К сожалению, я и раньше
сталкивалась с этой проблемой.

00:02:00.472 --> 00:02:04.775
Когда я изучала информатику
в Технологическом институте Джорджии,

00:02:04.799 --> 00:02:06.854
я работала над социальными роботами,

00:02:06.878 --> 00:02:10.655
и одной из моих задач было научить
робота играть в «ку-ку».

00:02:10.679 --> 00:02:12.362
Это простая игра со сменой ролей,

00:02:12.386 --> 00:02:16.707
когда участники закрывают лицо ладонями,
и, открывая, говорят: «Ку-ку!»

00:02:16.731 --> 00:02:21.160
Проблема заключается в том, что
если вас не видно, то игра теряет смысл,

00:02:21.184 --> 00:02:23.683
а мой робот меня не видел.

00:02:23.707 --> 00:02:27.657
Моя соседка одолжила мне своё лицо,
чтобы я смогла закончить проект,

00:02:27.681 --> 00:02:29.061
я сдала задание

00:02:29.085 --> 00:02:32.838
и подумала, что кто-нибудь другой
решит эту проблему.

00:02:33.489 --> 00:02:35.492
Через некоторое время

00:02:35.516 --> 00:02:39.675
я участвовала в конкурсе
предринимателей в Гонконге.

00:02:40.159 --> 00:02:42.853
Организаторы решили показать участникам

00:02:42.877 --> 00:02:45.249
местные стартапы.

00:02:45.273 --> 00:02:47.988
В одном стартапе был социальный робот,

00:02:48.012 --> 00:02:49.924
и они решили его продемонстрировать.

00:02:49.948 --> 00:02:52.928
Демонстрация сработала на всех,
пока дело не дошло до меня,

00:02:52.952 --> 00:02:54.875
и вы, наверное, уже догадались.

00:02:54.899 --> 00:02:57.864
Он не смог распознать моё лицо.

00:02:57.888 --> 00:03:00.399
Я спросила у разработчиков, в чём дело,

00:03:00.423 --> 00:03:05.956
и оказалось, что мы использовали одну
и ту же программу для распознавания лиц.

00:03:05.980 --> 00:03:07.630
На другой половине земного шара

00:03:07.654 --> 00:03:11.506
до меня дошло, что алгоритмическая ошибка
может распространяться так же быстро,

00:03:11.530 --> 00:03:14.700
как быстро можно скачивать
файлы из интернета.

00:03:15.565 --> 00:03:18.641
Так что же происходит? Почему
моё лицо не распознаётся?

00:03:18.665 --> 00:03:22.021
Для начала рассмотрим,
как мы создаём машинное зрение.

00:03:22.045 --> 00:03:25.454
Компьютерное зрение использует
техники машинного обучения,

00:03:25.478 --> 00:03:27.358
чтобы распознавать лица.

00:03:27.382 --> 00:03:31.279
Это работает таким образом: вы создаёте
тренировочный набор с примерами лиц.

00:03:31.303 --> 00:03:34.121
Это лицо. Это лицо. Это не лицо.

00:03:34.145 --> 00:03:38.664
И постепенно вы можете научить
компьютер распознавать другие лица.

00:03:38.688 --> 00:03:42.677
Однако, если тренировочные наборы
не очень разнообразны,

00:03:42.701 --> 00:03:46.050
любое лицо, которое сильно отличается
от установленной нормы,

00:03:46.074 --> 00:03:47.723
будет сложно распознать,

00:03:47.747 --> 00:03:49.710
что и происходило со мной.

00:03:49.734 --> 00:03:52.116
Но не переживайте —
есть и хорошие новости.

00:03:52.140 --> 00:03:54.911
Тренировочные наборы
не появляются из ниоткуда.

00:03:54.935 --> 00:03:56.723
Мы вообще-то можем их создавать.

00:03:56.747 --> 00:04:00.923
Есть возможность создавать полный спектр
тренировочных наборов,

00:04:00.947 --> 00:04:04.771
которые отражают более насыщенный
портрет человечества.

00:04:04.795 --> 00:04:07.016
Вы увидели в моих примерах,

00:04:07.040 --> 00:04:08.808
как социальные роботы

00:04:08.832 --> 00:04:13.443
позволили мне обнаружить исключения
из-за алгоритмической погрешности.

00:04:13.467 --> 00:04:18.282
Но алгоритмическая ошибка может также
приводить к дискриминации.

00:04:19.257 --> 00:04:20.710
В США

00:04:20.734 --> 00:04:24.932
отделения полиции начинают использовать
программы распознавания лиц

00:04:24.956 --> 00:04:27.415
в их арсенале борьбы с преступностью.

00:04:27.439 --> 00:04:29.452
Школа права университета Джорджтаун

00:04:29.476 --> 00:04:36.239
опубликовала отчёт, согласно которому
в США один из двух взрослых —

00:04:36.263 --> 00:04:39.797
это 117 миллионов человек —
имеют свои лица в сетях распознавания лиц.

00:04:39.821 --> 00:04:44.373
Отделения полиции сейчас могут
неограниченно пользоваться этими сетями,

00:04:44.397 --> 00:04:48.683
используя алгоритмы, точность
которых не была проверена.

00:04:48.707 --> 00:04:52.571
Но мы знаем, что
распознавание лиц несовершенно

00:04:52.595 --> 00:04:56.774
и что последовательное определение лиц
остаётся сложной задачей.

00:04:56.798 --> 00:04:58.560
Можно столкнуться с этим на Фейсбуке.

00:04:58.584 --> 00:05:01.572
Мы с друзьями всё время смеёмся,
когда видим других людей,

00:05:01.596 --> 00:05:04.054
отмеченных на наших фото.

00:05:04.078 --> 00:05:09.669
Но ошибочное определение уголовно
подозреваемого — это не шутка

00:05:09.693 --> 00:05:12.520
и не просто нарушение гражданских свобод.

00:05:12.544 --> 00:05:15.749
Машинное обучение используется
для распознавания лиц,

00:05:15.773 --> 00:05:20.278
но не ограничивается компьютерным зрением.

00:05:21.086 --> 00:05:25.102
В своей книге «Оружие
математического поражения»

00:05:25.126 --> 00:05:31.807
учёный в области данных Кэти О'нейл
рассказывает о новом ОМП —

00:05:31.831 --> 00:05:36.184
охватывающих, малоизвестных
и поражающих алгоритмах,

00:05:36.208 --> 00:05:39.172
которые всё чаще используются
для принятия решений,

00:05:39.196 --> 00:05:42.373
которые влияют на всё большее количество
аспектов нашей жизни.

00:05:42.397 --> 00:05:44.267
Кто будет принят на работу или уволен?

00:05:44.291 --> 00:05:46.403
Дадут ли вам этот кредит? Страховку?

00:05:46.427 --> 00:05:49.930
Попадёте ли вы в тот колледж,
в который собирались?

00:05:49.954 --> 00:05:53.463
Одинаковы ли цены для вас и для меня
на один и тот же товар,

00:05:53.487 --> 00:05:55.929
купленный на той же платформе?

00:05:55.953 --> 00:05:59.712
Правоохранительные органы также
начинают применять машинное обучение

00:05:59.736 --> 00:06:02.025
для прогнозирования.

00:06:02.049 --> 00:06:05.543
Судьи используют рейтинги риска,
сгенерированные машиной, для определения

00:06:05.567 --> 00:06:09.969
срока, который человек
должен будет провести в тюрьме.

00:06:09.993 --> 00:06:12.447
Мы должны крепко задуматься
над этими решениями.

00:06:12.471 --> 00:06:13.653
Справедливы ли они?

00:06:13.677 --> 00:06:16.567
Мы видели, что алгоритмический сбой

00:06:16.591 --> 00:06:19.965
ведёт к не совсем правильным результатам.

00:06:19.989 --> 00:06:21.953
Что же мы можем с этим сделать?

00:06:21.977 --> 00:06:25.657
Для начала, мы можем задуматься о том,
как создавать более всестороний код

00:06:25.681 --> 00:06:28.671
и применять инклюзивные практики
в программировании.

00:06:28.695 --> 00:06:31.004
Это всё начинается с людей.

00:06:31.528 --> 00:06:33.489
Важно, кто пишет код.

00:06:33.513 --> 00:06:37.632
Создаём ли мы группы
из разнообразных личностей,

00:06:37.656 --> 00:06:40.067
способных видеть слепые зоны друг друга?

00:06:40.091 --> 00:06:43.636
С технической стороны, имеет значение
то, как мы пишем код.

00:06:43.660 --> 00:06:47.311
Движет ли нами справедливость
во время разработки систем?

00:06:47.335 --> 00:06:50.248
И наконец, важно, для чего мы пишем код.

00:06:50.605 --> 00:06:55.688
Использование компьютерного творчества
открыло нам невероятное богатство.

00:06:55.712 --> 00:07:00.159
Сейчас у нас есть возможность
добиться ещё большего равенства,

00:07:00.183 --> 00:07:03.113
если мы сделаем социальное
изменение приоритетной,

00:07:03.137 --> 00:07:05.307
а не запоздалой идеей.

00:07:05.828 --> 00:07:10.350
Итак, есть три принципа, которые
составляют основу движения «инкодинг».

00:07:10.374 --> 00:07:12.026
Важно, кто пишет код,

00:07:12.050 --> 00:07:13.593
важно, как мы пишем код,

00:07:13.617 --> 00:07:15.640
и важно, для чего мы пишем код.

00:07:15.664 --> 00:07:18.763
Двигаясь в сторону инкодинга,
мы можем начать задумываться

00:07:18.787 --> 00:07:21.951
о создании платформ,
которые могут находить ошибку,

00:07:21.975 --> 00:07:25.053
собирая опыт людей, таких как я,

00:07:25.077 --> 00:07:28.147
а также проверяющих существующее
программное обеспечение.

00:07:28.171 --> 00:07:31.936
Также мы можем приступить к созданию
более полных тренировочных наборов.

00:07:31.960 --> 00:07:34.763
Представьте себе кампанию
«Селфи для присоединения»,

00:07:34.787 --> 00:07:38.442
где вы и я можем помочь разработчикам
тестировать и создавать

00:07:38.466 --> 00:07:40.559
более полные тренировочные наборы.

00:07:41.122 --> 00:07:43.950
Также мы можем начать думать
более добросовестно

00:07:43.974 --> 00:07:49.365
о социальном влиянии технологии,
над которой мы работаем.

00:07:49.389 --> 00:07:51.782
Чтобы запустить движение инкодинга,

00:07:51.806 --> 00:07:54.653
я создала Лигу алгоритмической
справедливости,

00:07:54.677 --> 00:08:00.549
где каждый, кто заботится о равенстве,
может помочь бороться со сбоями.

00:08:00.573 --> 00:08:03.869
На codedgaze.com вы
можете сообщить об ошибке,

00:08:03.893 --> 00:08:06.338
запросить проверку, стать тестировщиком

00:08:06.362 --> 00:08:09.133
и присоединиться к обсуждению.

00:08:09.157 --> 00:08:11.444
#codedgaze.

00:08:12.562 --> 00:08:15.049
Я приглашаю вас присоединиться ко мне,

00:08:15.073 --> 00:08:18.792
чтобы создать мир, в котором технология
работает на всех нас,

00:08:18.816 --> 00:08:20.713
а не только на некоторых,

00:08:20.737 --> 00:08:25.325
мир, в котором мы ценим инклюзию и
сосредотачиваемся на социальных переменах.

00:08:25.349 --> 00:08:26.524
Спасибо.

00:08:26.548 --> 00:08:30.819
(Аплодисменты)

00:08:32.693 --> 00:08:35.547
И у меня один вопрос:

00:08:35.571 --> 00:08:37.630
«А вы присоединитесь
ко мне в этой борьбе?»

00:08:37.654 --> 00:08:38.939
(Смех)

00:08:38.963 --> 00:08:42.650
(Аплодисменты)

