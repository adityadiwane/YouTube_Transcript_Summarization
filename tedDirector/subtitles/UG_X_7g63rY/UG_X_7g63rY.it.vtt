WEBVTT
Kind: captions
Language: it

00:00:00.000 --> 00:00:07.000
Traduttore: Federico MINELLE
Revisore: Nadia Ahmed

00:00:12.861 --> 00:00:15.995
Ciao, sono Joy, una poetessa 
della programmazione,

00:00:16.019 --> 00:00:21.012
in missione per fermare
una crescente forza inattesa.

00:00:21.036 --> 00:00:23.892
Una forza che ho chiamato 
"sguardo programmato",

00:00:23.916 --> 00:00:27.225
la mia espressione 
per la parzialità algoritmica.

00:00:27.249 --> 00:00:31.549
La parzialità algoritmica,
come quella umana, produce ingiustizia.

00:00:31.573 --> 00:00:37.595
Tuttavia gli algoritmi, come i virus,
si possono diffondere su ampia scala

00:00:37.619 --> 00:00:39.201
in tempi rapidi.

00:00:39.763 --> 00:00:44.150
La parzialità algoritmica può anche 
portare a esperienze di esclusione

00:00:44.174 --> 00:00:46.302
e pratiche discriminatorie.

00:00:46.326 --> 00:00:48.387
Vi mostro cosa voglio dire.

00:00:48.800 --> 00:00:51.236
(Video) Joy Buolamwini: Ciao.
Ho una faccia.

00:00:51.982 --> 00:00:53.846
Puoi vedere la mia faccia?

00:00:53.871 --> 00:00:55.496
Senza occhiali?

00:00:55.521 --> 00:00:57.735
Puoi vedere la sua faccia.

00:00:58.057 --> 00:01:00.302
Che dici della mia faccia?

00:01:03.710 --> 00:01:07.460
Ho qui una maschera. 
Vedi la mia maschera?

00:01:08.294 --> 00:01:10.659
Joy Buolamwini: Com'è successo?

00:01:10.683 --> 00:01:13.824
Perché ero seduta davanti al computer

00:01:13.848 --> 00:01:15.272
con una maschera bianca,

00:01:15.296 --> 00:01:18.946
tentando di essere individuata
da una webcam economica?

00:01:18.970 --> 00:01:21.325
Quando non combatto
"lo sguardo programmato"

00:01:21.325 --> 00:01:22.855
da poetessa
della programmazione,

00:01:22.855 --> 00:01:26.101
sono una studentessa di laurea magistrale
al Media Lab del MIT,

00:01:26.125 --> 00:01:31.042
dove ho avuto la opportunità di lavorare
su ogni tipo di progetto stravagante,

00:01:31.066 --> 00:01:33.093
incluso lo Specchio dei Desideri,

00:01:33.117 --> 00:01:38.251
un progetto che mi consente di proiettare
una maschera digitale sul mio riflesso.

00:01:38.275 --> 00:01:40.625
Così la mattina, se voglio sentirmi forte,

00:01:40.649 --> 00:01:42.083
posso indossare leone.

00:01:42.107 --> 00:01:45.603
Se volessi essere rialzata, 
potrei avere un'altezza.

00:01:45.627 --> 00:01:48.616
Ho utilizzzato un generico software
di riconoscimento facciale

00:01:48.640 --> 00:01:49.991
per costruire il sistema,

00:01:50.015 --> 00:01:55.118
ma ho trovato molto difficile provarlo
a meno di indossare una maschera bianca.

00:01:56.102 --> 00:02:00.448
Sfortunatamente, 
ho già toccato questo tema.

00:02:00.472 --> 00:02:04.775
Quando ero unastudentessa di Informatica
al Georgia Tech

00:02:04.799 --> 00:02:06.854
ero abituata a lavorare sui robot sociali,

00:02:06.878 --> 00:02:10.655
e uno dei miei compiti era di far giocare
un robot a "booh! chi sono?",

00:02:10.679 --> 00:02:12.362
un semplice gioco a turni,

00:02:12.386 --> 00:02:16.707
dove i compagni nascondono la faccia e
poi la scoprono dicendo "booh! chi sono?"

00:02:16.731 --> 00:02:21.160
Il problema è che il gioco non funziona 
veramente se non vi vedo,

00:02:21.184 --> 00:02:23.683
e il mio robot non mi poteva vedere.

00:02:23.707 --> 00:02:27.657
Allora mi sono fatta prestare il volto
della mia compagna di stanza,

00:02:27.681 --> 00:02:29.061
e ho consegnato il compito,

00:02:29.085 --> 00:02:32.838
e ho pensato che qualcun altro 
risolvesse il problema.

00:02:33.489 --> 00:02:35.492
Non molto dopo,

00:02:35.516 --> 00:02:39.675
ero a Hong Kong per una gara 
sulla imprenditorialità.

00:02:40.159 --> 00:02:42.853
Gli organizzatori decisero 
di scegliere i partecipanti

00:02:42.877 --> 00:02:45.249
esaminando le start-up locali.

00:02:45.273 --> 00:02:47.988
Una delle start-up aveva un robot sociale,

00:02:48.012 --> 00:02:49.924
e loro decisero di fare un demo.

00:02:49.948 --> 00:02:52.928
Il demo funzionò finché non giunsero a me,

00:02:52.952 --> 00:02:54.875
e forse indovinate perché.

00:02:54.899 --> 00:02:57.864
Non poteva individuare la mia faccia.

00:02:57.888 --> 00:03:00.399
Ho chiesto agli sviluppatori
cosa stava succedendo,

00:03:00.423 --> 00:03:05.956
e risultò che usavano lo stesso software
generico di riconoscimento facciale.

00:03:05.980 --> 00:03:07.630
Dall'altra parte del mondo,

00:03:07.654 --> 00:03:11.506
ho imparato che la parzialità algoritmica
può viaggiare velocemente

00:03:11.530 --> 00:03:14.700
visto che basta scaricare qualche file
da internet.

00:03:15.565 --> 00:03:18.641
Cosa stava succedendo?
Perchè la mia faccia non era riconosciuta?

00:03:18.665 --> 00:03:22.021
Dobbiamo vedere come 
diamo la vista alle macchine.

00:03:22.045 --> 00:03:25.454
La visione tramite computer usa
tecniche di auto-apprendimento

00:03:25.478 --> 00:03:27.358
per il riconoscimento facciale.

00:03:27.382 --> 00:03:31.279
Come funziona: si creano esempi di facce
per l'apprendimento.

00:03:31.303 --> 00:03:34.121
Questa è una faccia. Anche questa.
Questa non è una faccia.

00:03:34.145 --> 00:03:38.664
Con il tempo, si può insegnare al computer
come riconoscere altre facce.

00:03:38.688 --> 00:03:42.677
Ma se gli esempi per l'addestramento
non sono veramente diversi,

00:03:42.701 --> 00:03:46.050
qualsiasi faccia che devia troppo
dalla norma stabilita

00:03:46.074 --> 00:03:47.723
sarà difficile da riconoscere,

00:03:47.747 --> 00:03:49.710
che era quanto mi stava capitando.

00:03:49.734 --> 00:03:52.116
Ma non vi preoccupate,
ci sono buone notizie.

00:03:52.140 --> 00:03:54.911
Gli esempi per l'addestramento
non nascono dal nulla.

00:03:54.935 --> 00:03:56.723
Siamo noi a crearli.

00:03:56.747 --> 00:04:00.923
C'è quindi la possibilità di creare gruppi
di esempi ad ampio spettro

00:04:00.947 --> 00:04:04.771
che riflettono un più ricco 
ritratto dell'umanità.

00:04:04.795 --> 00:04:07.016
Avete ora visto nei miei esempi

00:04:07.040 --> 00:04:08.808
come tramite i robot sociali

00:04:08.832 --> 00:04:13.443
ho scopertoa l'esclusione
dovuta alla parzialità algoritmica.

00:04:13.467 --> 00:04:18.282
Ma la parzialità algoritmica può condurre 
anche a pratiche discriminatorie.

00:04:19.257 --> 00:04:20.710
Negli USA,

00:04:20.734 --> 00:04:24.932
i dipartimenti di polizia iniziano a usare
software di riconoscimento facciale

00:04:24.956 --> 00:04:27.415
nel loro arsenale di battaglia al crimine.

00:04:27.439 --> 00:04:29.452
Georgetown Law ha pubblicato un rapporto

00:04:29.476 --> 00:04:36.239
che mostra come uno su due adulti in USA,
cioè 117 milioni di persone,

00:04:36.263 --> 00:04:39.797
ha la sua faccia nelle reti 
per il riconoscimento facciale.

00:04:39.821 --> 00:04:44.373
I dipartimenti di polizia possono ora 
utilizzare queste reti non regolamentate,

00:04:44.397 --> 00:04:48.683
che usano algoritmi non verificati
sulla loro accuratezza.

00:04:48.707 --> 00:04:52.571
Certo si sa che il riconoscimento 
facciale non è a prova di errore,

00:04:52.595 --> 00:04:56.774
ed etichettare le facce coerentemente
rimane una sfida.

00:04:56.798 --> 00:04:58.560
Si può vedere ciò anche su Facebook.

00:04:58.584 --> 00:05:01.572
I miei amici ed io ridiamo quando
vediamo altre persone

00:05:01.596 --> 00:05:04.054
etichettate male nelle nostre foto.

00:05:04.078 --> 00:05:09.669
Ma identificare male un sospetto criminale
non è una cosa da riderci su,

00:05:09.693 --> 00:05:12.520
neppure violare le nostre libertà civili.

00:05:12.544 --> 00:05:15.749
Si usa l'apprendimento automatico
per il riconoscimento facciale,

00:05:15.773 --> 00:05:20.278
ma si sta estendendo oltre il campo 
della visione via computer.

00:05:21.086 --> 00:05:25.102
Nel suo libro,
"Armi per la Distruzione Matematica",

00:05:25.126 --> 00:05:31.807
la scienzata sui dati Cathy O'Neil
ci parla dei crescenti nuovi WMD,

00:05:31.831 --> 00:05:36.184
algoritmi diffusi, 
misteriosi e distruttivi

00:05:36.208 --> 00:05:39.172
che sono sempre più usati
per prendere decisioni

00:05:39.196 --> 00:05:42.373
che influiscono su molti aspetti
delle nostre vite.

00:05:42.397 --> 00:05:44.267
Così, chi viene assunto o licenziato?

00:05:44.291 --> 00:05:46.403
Ottenete quel mutuo?
E la assicurazione?

00:05:46.427 --> 00:05:49.930
Siete ammessi al college
al quale volete andare?

00:05:49.954 --> 00:05:53.463
Voi ed io paghiamo lo stesso prezzo
per lo stesso prodotto

00:05:53.487 --> 00:05:55.929
comprato sulla stessa piattaforma?
[e-commerce]

00:05:55.953 --> 00:05:59.712
Le forze dell'ordine stanno iniziando
a usare l'apprendimento automatico

00:05:59.736 --> 00:06:02.025
per compiti di "polizia predittiva".

00:06:02.049 --> 00:06:05.543
Alcuni giudici usano valutazioni
di rischio automatiche per determinare

00:06:05.567 --> 00:06:09.969
quanto tempo una persona
deve passare in prigione.

00:06:09.993 --> 00:06:12.447
Veramente dobbiamo riflettere
su queste decisioni.

00:06:12.471 --> 00:06:13.653
Sono giuste?

00:06:13.677 --> 00:06:16.567
Abbiamo visto che
la parzialità algoritmica

00:06:16.591 --> 00:06:19.965
non necessariamente porta 
a esiti giusti.

00:06:19.989 --> 00:06:21.953
Cosa possiamo fare per questo?

00:06:21.977 --> 00:06:25.657
Possiamo iniziare a riflettere su come
possiamo creare codice più inclusivo

00:06:25.681 --> 00:06:28.671
e impiegare criteri di codifica inclusivi.

00:06:28.695 --> 00:06:31.004
In effetti si parte dalle persone.

00:06:31.528 --> 00:06:33.489
Con chi è interessato al codice.

00:06:33.513 --> 00:06:37.632
Stiamo veramente creando squadre 
ad ampio spettro, con diversi individui

00:06:37.656 --> 00:06:40.067
che possano verificare 
i punti oscuri dell'altro?

00:06:40.091 --> 00:06:43.636
Da un punto di vista tecnico,
il modo in cui codifichiamo è importante.

00:06:43.660 --> 00:06:47.311
Stiamo puntando sulla imparzialità
quando sviluppiamo i sistemi?

00:06:47.335 --> 00:06:50.248
E alla fine, perché
ci interessa la codifica.

00:06:50.605 --> 00:06:55.688
Usiamo strumenti di sviluppo elaborativo 
per liberare immense ricchezze.

00:06:55.712 --> 00:07:00.159
Ora abbiamo l'opportunità di dar vita
a una uguaglianza più vasta

00:07:00.183 --> 00:07:03.113
se diamo priorità al cambiamento sociale

00:07:03.137 --> 00:07:05.307
e non lo consideriamo 
un pensiero aggiuntivo.

00:07:05.828 --> 00:07:10.350
Vi sono quindi tre principi che sorreggono
il movimento di "codifica inclusiva".

00:07:10.374 --> 00:07:12.026
A chi interessa il codice,

00:07:12.050 --> 00:07:13.593
come ci interessa il codice

00:07:13.617 --> 00:07:15.640
e perché ci interessa il codice.

00:07:15.664 --> 00:07:18.763
Per puntare alla "codifica inclusiva",
possiamo partire pensando

00:07:18.787 --> 00:07:21.951
di costruire piattaforme che possano
identificare le parzialità

00:07:21.975 --> 00:07:25.053
raccogliendo le esperienze delle persone
come quelle condivise,

00:07:25.077 --> 00:07:28.147
ma anche verificando
il software esistente.

00:07:28.171 --> 00:07:31.936
Possiamo anche iniziare a creare
un modello formativo più inclusivo.

00:07:31.960 --> 00:07:34.763
Pensate a una campagna mediatica
"Selfie per l'inclusione"

00:07:34.787 --> 00:07:38.442
dove voi ed io possiamo aiutare 
gli sviluppatori a provare e creare

00:07:38.466 --> 00:07:40.559
modelli formativi più inclusivi.

00:07:41.122 --> 00:07:43.950
Possiamo anche iniziare a riflettere
più consciamente

00:07:43.974 --> 00:07:49.365
sull'impatto sociale della tecnologia
che stiamo sviluppando.

00:07:49.389 --> 00:07:51.782
Per avviare il movimento
di "codifica inclusiva",

00:07:51.806 --> 00:07:54.653
ho lanciato la Lega per
la Giustizia Algoritmica,

00:07:54.677 --> 00:08:00.549
dove ognuno che ha a cuore la giustizia 
può combattere "lo sguardo programmato".

00:08:00.573 --> 00:08:03.869
Su codedgaze.com si possono
segnalare parzialità,

00:08:03.893 --> 00:08:06.338
richiedere verifiche, 
diventare un collaudatore

00:08:06.362 --> 00:08:09.133
e unirsi alla conversazione corrente,

00:08:09.157 --> 00:08:11.444
#codedgaze.

00:08:12.562 --> 00:08:15.049
Vi invito quidi ad unirvi a me

00:08:15.073 --> 00:08:18.792
nel creare un mondo dove la tecnologia
lavori per tutti noi,

00:08:18.816 --> 00:08:20.713
non solo per qualcuno di noi,

00:08:20.737 --> 00:08:25.325
un mondo dove diamo valore alla inclusione
e ci concentriamo sul cambiamento sociale.

00:08:25.349 --> 00:08:26.524
Grazie.

00:08:26.548 --> 00:08:30.819
(Applausi)

00:08:32.693 --> 00:08:35.547
Ma ho una domanda:

00:08:35.571 --> 00:08:37.630
Vi unirete a me nella battaglia?

00:08:37.654 --> 00:08:38.939
(Risate)

00:08:38.963 --> 00:08:42.650
(Applausi)

