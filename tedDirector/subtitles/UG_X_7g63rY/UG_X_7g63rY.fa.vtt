WEBVTT
Kind: captions
Language: fa

00:00:00.000 --> 00:00:07.000
Translator: Iman Mirzadeh
Reviewer: soheila Jafari

00:00:12.861 --> 00:00:15.995
سلام ، من جوی هستم ، شاعر کد،

00:00:16.019 --> 00:00:21.012
در یک ماموریت برای متوقف کردن
یک نیروی دیده نشده در حال ظهور،

00:00:21.036 --> 00:00:23.892
نیرویی که من آن را "نگاه رمزی" صدا می کنم

00:00:23.916 --> 00:00:27.225
اصطلاح من برای تبعیض الگوریتمی.

00:00:27.249 --> 00:00:31.549
تبعیض الگوریتمی، مانند تبعیض انسانی
باعث نابرابری میشود.

00:00:31.573 --> 00:00:37.595
با این حال،الگوریتم ها، مانند ویروس ها
می تونن در ابعاد عظیم

00:00:37.619 --> 00:00:39.201
و سرعت زیادی، تبعیض رو گسترش یابند

00:00:39.763 --> 00:00:44.150
تبعیض الگوریتمی همچنین میتونه باعث
تجربیات انحصاری

00:00:44.174 --> 00:00:46.302
و اعمال تبعیض آمیز بشه.

00:00:46.326 --> 00:00:48.387
بذارین منظورمو بهتون نشون بدم.

00:00:48.800 --> 00:00:51.236
[فیلم] جوی: سلام دوربین.
من یک صورت دارم.

00:00:51.982 --> 00:00:53.846
میتونی صورتمو ببینی

00:00:53.871 --> 00:00:55.496
صورت بدون عینکم رو؟

00:00:55.521 --> 00:00:57.735
تو میتونی صورتشو ببینی.

00:00:58.057 --> 00:01:00.302
صورت من رو چطور؟

00:01:03.710 --> 00:01:07.460
من یک ماسک دارم. می تونی ماسک من رو ببینی؟

00:01:08.294 --> 00:01:10.659
پس چطور این اتفاق افتاد؟

00:01:10.683 --> 00:01:13.824
چرا من روبه روی یک کامپیوتر نشستم

00:01:13.848 --> 00:01:15.272
به همراه یک ماسک سفید

00:01:15.296 --> 00:01:18.946
و تلاش می کنم تا به وسیله یک وب کم
ارزان قیمت شناخته شوم؟

00:01:18.970 --> 00:01:21.261
خب وقتی که من
با "نگاه رمزی" مبارزه نمی کنم

00:01:21.285 --> 00:01:22.805
به عنوان یک شاعر کد،

00:01:22.829 --> 00:01:26.101
من یک دانشجوی تحصیلات تکمیلی
در آزمایشگاه رسانه MIT هستم

00:01:26.125 --> 00:01:31.042
من آنجا فرصت دارم تا روی انواع پروژه های
جالب و خیالبافانه کار کنم

00:01:31.066 --> 00:01:33.093
مثل آینه آرزو

00:01:33.117 --> 00:01:38.251
پروژه ای که می تواند
روی تصویر من ماسک بگذارد

00:01:38.275 --> 00:01:40.625
پس در صبح، اگر من بخوام احساس قدرت کنم،

00:01:40.649 --> 00:01:42.083
میتونم ماسک یک شیر رو بذارم.

00:01:42.107 --> 00:01:45.603
یا اگر بخواهم با روحیه باشم
می توانم یک نقل قول ببینم.

00:01:45.627 --> 00:01:48.616
بنابراین از یک دستگاه تشخیص چهره
استفاده کردم

00:01:48.640 --> 00:01:49.991
تا این سیستم رو بسازم،

00:01:50.015 --> 00:01:55.118
ولی فهمیدم که واقعا بدون ماسک سفید
امکان تست این سیستم وجود نداره.

00:01:56.102 --> 00:02:00.448
متاسفانه قبلاً هم
به این مشکل برخورد کرده بودم

00:02:00.472 --> 00:02:04.775
وقتی که در دانشگاه Gerogia Tech
دانشجوی کارشناسی رشته علوم کامپیوتر بودم،

00:02:04.799 --> 00:02:06.854
روی ربات‌های اجتماعی کار میکردم،

00:02:06.878 --> 00:02:10.655
و یکی از وظایفم این بود که به ربات
بازی peek-a-boo رو یاد بدم

00:02:10.679 --> 00:02:12.362
یک بازی نوبتی ساده
(مثل دالّی کردن)

00:02:12.386 --> 00:02:16.707
که تو اون به نوبت هرکی صورتشو میپوشونه و 
بعد پوشش رو بر میداره و میگه "peek-a-boo"

00:02:16.731 --> 00:02:21.160
مشکل اینه که، این بازی
اگر من نتونم ببینمتون فایده نداره

00:02:21.184 --> 00:02:23.683
و ربات من نمی‌تونست من رو ببینه.

00:02:23.707 --> 00:02:27.657
ولی من صورت هم اتاقیم رو قرض گرفتم
تا پروژم رو انجام بدم

00:02:27.681 --> 00:02:29.061
تمرین رو انجام دادم،

00:02:29.085 --> 00:02:32.838
و فهمیدم که، میدونی چیه؟
یکی دیگه این مشکل رو حل خواهد کرد.

00:02:33.489 --> 00:02:35.492
مدت کوتاهی بعد از این ماجرا،

00:02:35.516 --> 00:02:39.675
من برای یک مسابقه کارآفرینی
در هنگ کنگ بودم.

00:02:40.159 --> 00:02:42.853
برگزارکنندگان تصیمیم گرفتن
که شرکت کننده ها رو

00:02:42.877 --> 00:02:45.249
به یک تور بازدید از استارتاپ های محلی ببرن

00:02:45.273 --> 00:02:47.988
یکی از استارتاپ‌ها یک ربات اجتماعی داشت،

00:02:48.012 --> 00:02:49.924
و تصمیم گرفتن که یک دمو برگزار کنن.

00:02:49.948 --> 00:02:52.928
دمو روی همه کار کرد تا اینکه نوبت من شد

00:02:52.952 --> 00:02:54.875
و احتمالا میتونین حدس بزنین.

00:02:54.899 --> 00:02:57.864
که ربات نتونست صورت من رو تشخیص بده.

00:02:57.888 --> 00:03:00.399
من از توسعه‌دهندگان ربات پرسیدم جریان چیه

00:03:00.423 --> 00:03:05.956
و معلوم شد که همون نرم‌افزار تشخیص چهره‌ای
رو استفاده میکردن که ما میکردیم.

00:03:05.980 --> 00:03:07.630
اون طرف دنیا،

00:03:07.654 --> 00:03:11.506
من یاد گرفتم که تبعیض الگوریتمی
میتونه خیلی سریع سفر کنه

00:03:11.530 --> 00:03:14.700
به سرعت دانلود چندتا فایل از اینترنت.

00:03:15.565 --> 00:03:18.641
حالا جریان چیه؟
چرا صورت من تشخیص داده نشد؟

00:03:18.665 --> 00:03:22.021
باید ببینیم که ما چطوری به ماشین‌ها
توانایی دیدن میدیم.

00:03:22.045 --> 00:03:25.454
بینایی کامپیوتر برای تشخیص چهره
از تکنیک‌های یادگیری ماشین استفاده میکنه

00:03:27.382 --> 00:03:31.279
روش کارش اینطوریه که یک مجموعه داده آموزشی
از نمونه صورت ها درست میکنید.

00:03:31.303 --> 00:03:34.121
این یک صورته. این یک صورته. این صورت نیست.

00:03:34.145 --> 00:03:38.664
و به مرور زمان، میتونین به کامپیوتر
یاد بدین که چطوری صورت هارو تشخیص بده.

00:03:38.688 --> 00:03:42.677
اما اگه مجموعه داده آموزشی
واقعا متنوع نباشه.

00:03:42.701 --> 00:03:46.050
هر چهره‌ای که خیلی با معیار فرق کنه،

00:03:46.074 --> 00:03:47.723
تشخیصش سخت ‌تر میشه

00:03:47.747 --> 00:03:49.710
که این برای من هم پیش اومد.

00:03:49.734 --> 00:03:52.116
ولی نگران نباشین
خبرهای خوبی برای شنیدن هستن

00:03:52.140 --> 00:03:54.911
مجموعه داده آموزشی
همینطوری از ناکجا آباد نمیان.

00:03:54.935 --> 00:03:56.723
در واقع ما میسازیمشون.

00:03:56.747 --> 00:04:00.923
پس این فرصت برای ساخت مجموعه‌ داده آموزشی
متنوع و فراگیر وجود داره.

00:04:00.947 --> 00:04:04.771
که تصویر غنی‌تری از انسانیت رو نشون بدن.

00:04:04.795 --> 00:04:07.016
شما در مثال‌های من دیدین که

00:04:07.040 --> 00:04:08.808
چجوری ربات‌های اجتماعی

00:04:08.832 --> 00:04:13.443
باعث کشف من درباره
محرومیت به وسیله تبعیض الگوریتمی شدن.

00:04:13.467 --> 00:04:18.282
ولی تبعیض الگوریتمی میتونه
باعث رفتارهای تبعیض‌آمیز بشه

00:04:19.257 --> 00:04:20.710
در سراسر ایالات متحده

00:04:20.734 --> 00:04:24.932
دپارتمان‌های پلیس شروع به استفاده
از نرم‌افزار تشخیص چهره کردن.

00:04:24.956 --> 00:04:27.415
و اون رو به تجهیزاتشون
در جنگ با جرم اضافه کردن.

00:04:27.439 --> 00:04:29.452
Georgetown Law گزارشی رو منتشر کرده که

00:04:29.476 --> 00:04:36.239
نشون میده، در ایالت متحده
از هر دو نفر یک نفر

00:04:36.263 --> 00:04:39.797
چیزی حدود ۱۱۷ میلیون نفر
تصاویرشون در شبکه‌های تشخیص چهره قرار داره

00:04:39.821 --> 00:04:44.373
پلیس بدون نظارت مراجع مربوطه
میتونه به این شبکه‌ها نگاه کنه

00:04:44.397 --> 00:04:48.683
اونم با استفاده از الگوریتم‌هایی که
صحتشون تایید نشده.

00:04:48.707 --> 00:04:52.571
با این وجود میدونیم که
تشخیص چهره، عاری از خطا نیست

00:04:52.595 --> 00:04:56.774
و مشخص کردن چهره‌ها
همیشه یک چالش باقی خواهد ماند.

00:04:56.798 --> 00:04:58.560
شاید شما اینو تو فیسبوک دیده باشین.

00:04:58.584 --> 00:05:03.452
من و دوستام همیشه ازینکه بقیه
اشتباه تو عکسهامون مشخص شدن خندمون میگیره.

00:05:04.078 --> 00:05:09.669
ولی تشخیص اشتباه یک مظنون جنایی
اصلا چیز خنده داری نیست.

00:05:09.693 --> 00:05:12.520
نقض آزادی‌های مدنی هم همینطور.

00:05:12.544 --> 00:05:15.749
برای تشخیص چهره،
از یادگیری ماشین داره استفاده میشه

00:05:15.773 --> 00:05:20.278
ولی یادگیری ماشین
فراتر از محدوده بینایی ماشین هست.

00:05:21.086 --> 00:05:25.102
در کتاب "سلاح‌های ریاضی مخرب"

00:05:25.126 --> 00:05:31.807
Cathy O'Neil که یک داده شناس هست،
درباره ظهور این سلاح‌های جدید صحبت میکنه

00:05:31.831 --> 00:05:36.184
الگوریتم‌هایی مرموز، فراگیر و مخرب

00:05:36.208 --> 00:05:39.172
که به طور روز افزون در حال استفاده شدن
در تصمیم گیری‌ها هستند.

00:05:39.196 --> 00:05:42.373
و روی جنبه های بیشتری از
زندگی‌های ما تاثیر میذارن.

00:05:42.397 --> 00:05:44.267
مثلا کی استخدام شه کی اخراج؟

00:05:44.291 --> 00:05:46.663
فلان وام رو بگیرین؟
بیمه بشین؟

00:05:46.663 --> 00:05:49.930
در دانشگاهی که میخواهین برین پذیرفته شین؟

00:05:49.954 --> 00:05:53.463
من و شما یک کالا رو از یک مکان
با یک قیمت بخریم؟

00:05:55.953 --> 00:05:59.712
مجریان قانونی هم شروع به
استفاده از یادگیری ماشین

00:05:59.736 --> 00:06:02.025
برای محافظت پیشگیرانه کرده اند.

00:06:02.049 --> 00:06:05.943
برخی از قاضی ها از مقدار ریسک محاسبه شده
توسط ماشین برای مشخص کردن اینکه

00:06:05.943 --> 00:06:09.969
یک فرد چقدر در زندان باشد، استفاده میکنند.

00:06:09.993 --> 00:06:12.447
پس ما باید خیلی جدی
درباره این تصمیمات فکر کنیم.

00:06:12.471 --> 00:06:13.653
آیا منصفانه هستن؟

00:06:13.677 --> 00:06:16.567
و ما تبعیض الگوریتمی رو دیده ایم

00:06:16.591 --> 00:06:19.965
که لزوماً باعث نتایج منصفانه نمیشن.

00:06:19.989 --> 00:06:21.953
خب ما چه کار میتونیم بکنیم؟

00:06:21.977 --> 00:06:25.657
ما میتونیم شروع کنیم به فکر کردن
درباره ساختن کدهای غیر انحصاری تر

00:06:25.681 --> 00:06:28.671
و رفتارهای غیر انحصاری رو
برای کد زدن استفاده کنیم.

00:06:28.695 --> 00:06:31.004
همه چی واقعاً از آدم‌ها شروع میشه.

00:06:31.528 --> 00:06:33.489
اینکه چه کسی کُد میزنه مهمه.

00:06:33.513 --> 00:06:37.632
آیا ما داریم تیم هایی
با طیف مختلفی از افراد درست میکنیم؟

00:06:37.656 --> 00:06:40.067
که بتونن نقاط ضعف همدیگه رو بررسی کنن؟

00:06:40.091 --> 00:06:43.636
در سمت تکنیکال، اینکه چطوری کد میزنیم مهمه

00:06:43.660 --> 00:06:47.311
آیا وقتی که یک سیستم رو توسعه میدیم
برامون برابری یک شاخص هست؟

00:06:47.335 --> 00:06:50.248
و در پایان، اینکه چرا کد میزنیم مهمه.

00:06:50.605 --> 00:06:55.688
ما از ابزارهای کامپیوتری برای
بدست آوردن ثروت زیادی استفاده کرده ایم.

00:06:55.712 --> 00:07:00.159
حالا این فرصت رو داریم
که حتی برابری بیشتری هم بدست بیاریم.

00:07:00.183 --> 00:07:03.113
به شرط اینکه
تغییر اجتماعی رو یک اولویت بذاریم

00:07:03.137 --> 00:07:05.587
و به عنوان نه چیزی که 
حالا بعدا بهش فکر میکنیم

00:07:05.828 --> 00:07:10.350
پس برای جنبش "کد غیرانحصاری"
این سه اصل رو داریم:

00:07:10.374 --> 00:07:12.026
اینکه کی کد میزنه مهمه،

00:07:12.050 --> 00:07:13.593
اینکه چطوری کد میزنیم مهمه

00:07:13.617 --> 00:07:15.640
و اینکه چرا کد میزنیم هم مهمه.

00:07:15.664 --> 00:07:18.763
پس برای اینکه به این سمت بریم،
میتونیم شروع کنیم به فکر کردن

00:07:18.787 --> 00:07:21.951
درباره ساختن پلتفرم‌هایی که
تبعیض رو تشخیص میدن

00:07:21.975 --> 00:07:25.053
استفاده از تجربیات افراد،
مثل اونایی که من به اشتراک گذاشتم

00:07:25.077 --> 00:07:28.147
و البته نظارت و بازرسی بر نرم افزارها.

00:07:28.171 --> 00:07:31.936
همچنین میتونیم شروع کنیم به ساختن
مجموعه داده‌های آموزشی غیرانحصاری تر.

00:07:31.960 --> 00:07:34.763
کمپین "سلفی‌ برای باهم بودن" رو تصور کنید

00:07:34.787 --> 00:07:37.636
که من و شما میتونیم
به برنامه‌نویس‌ها کمک کنیم

00:07:37.636 --> 00:07:40.469
تا مجموعه داده‌های غیرانحصاری‌تری
بسازن و تست کنن

00:07:41.122 --> 00:07:43.950
و میتونیم با وجدان بیشتری
شروع به فکر کردن کنیم

00:07:43.974 --> 00:07:49.365
درباره مسائلی مثل تاثیر اجتماعی
تکنولوژی که خودمون توسعش میدیم.

00:07:49.389 --> 00:07:51.782
برای شروع این جنبش،

00:07:51.806 --> 00:07:54.653
من لیگ عدالت الگوریتمی رو راه‌ اندازی کردم

00:07:54.677 --> 00:08:00.549
که هرکس که به برابری اهمیت میده
میتونه برای مبارزه با نگاه رمزی مبارزه کنه

00:08:00.573 --> 00:08:03.869
شما میتونین در سایت codedgaze.com
تبعیض ها رو گزارش بدین،

00:08:03.893 --> 00:08:06.338
درخواست بررسی کنین،
یک تست کننده بشین

00:08:06.362 --> 00:08:09.133
و به گفتگو در جریان بپیوندید.

00:08:09.157 --> 00:08:11.444
codedgaze#

00:08:12.562 --> 00:08:15.049
من شما رو دعوت میکنم که به من ملحق شین

00:08:15.073 --> 00:08:18.792
برای ساخت جهانی که در آن، تکنولوژی
برای همه‌ی ما، نه فقط بعضی‌ها کار کنه.

00:08:20.737 --> 00:08:25.325
جهانی که به غیر انحصاری بودن ارزش میدیم
و تغییرات اجتماعی مورد توجهمون هست.

00:08:25.349 --> 00:08:26.524
خیلی ممنون.

00:08:26.548 --> 00:08:30.819
(تشویق حاضرین)

00:08:32.693 --> 00:08:35.547
ولی من یک سوال ازتون دارم:

00:08:35.571 --> 00:08:37.630
آیا به من در این مبارزه می‌پیوندید؟

00:08:37.654 --> 00:08:38.939
(خنده حاضرین)

00:08:38.963 --> 00:08:42.650
(تشویق حاضرین)

