WEBVTT
Kind: captions
Language: pt

00:00:00.000 --> 00:00:07.000
Tradutor: Leonardo Silva
Revisor: Wanderley Jesus

00:00:12.861 --> 00:00:15.995
Olá, sou Joy, uma "poetisa dos códigos",

00:00:16.019 --> 00:00:21.012
a minha missão é deter
uma força invisível que tem crescido,

00:00:21.036 --> 00:00:23.892
uma força que chamo de "olhar codificado",

00:00:23.916 --> 00:00:27.225
é como chamo o viés algorítmico.

00:00:27.249 --> 00:00:31.549
Tal como o preconceito humano,
ele resulta em desigualdade.

00:00:31.573 --> 00:00:34.259
Porém, os algoritmos, assim como os vírus,

00:00:34.279 --> 00:00:39.201
podem espalhar o viés
em grande escala e rapidamente.

00:00:39.763 --> 00:00:44.150
O viés algorítmico também
pode levar a experiências de exclusão

00:00:44.174 --> 00:00:46.302
e a práticas discriminatórias.

00:00:46.326 --> 00:00:48.387
Vou mostrar o que quero dizer.

00:00:48.800 --> 00:00:51.236
(Vídeo) Joy Boulamwini:
Oi, câmera. Tenho um rosto.

00:00:51.982 --> 00:00:53.846
Consegue ver meu rosto?

00:00:53.871 --> 00:00:55.496
Um rosto sem óculos?

00:00:55.521 --> 00:00:57.735
Você consegue ver o rosto dela...

00:00:58.057 --> 00:00:59.967
E o meu?

00:01:03.710 --> 00:01:07.460
Estou usando uma máscara. Consegue vê-la?

00:01:08.294 --> 00:01:10.659
Joy Boulamwini: Como isso aconteceu?

00:01:10.683 --> 00:01:15.274
Por que estou diante de um computador,
usando uma máscara branca,

00:01:15.304 --> 00:01:18.676
tentando ser detectada
por uma câmera barata?

00:01:18.680 --> 00:01:21.261
Bom, quando não estou lutando
contra o olhar codificado

00:01:21.285 --> 00:01:22.805
como uma poetisa dos códigos,

00:01:22.829 --> 00:01:26.101
faço pós-graduação
no Laboratório de Mídia do MIT,

00:01:26.125 --> 00:01:31.042
na qual tenho a oportunidade de trabalhar
em diversos projetos bacanas,

00:01:31.056 --> 00:01:33.093
inclusive no "Aspire Mirror",

00:01:33.117 --> 00:01:38.251
projeto que criei para poder projetar
máscaras digitais sobre minha imagem.

00:01:38.275 --> 00:01:41.875
De manhã, se eu quisesse me sentir
poderosa, poderia usar uma de leão.

00:01:41.887 --> 00:01:45.603
Se precisasse de uma inspiração,
usaria uma citação.

00:01:45.627 --> 00:01:48.616
Então, usei um software genérico
de reconhecimento facial

00:01:48.640 --> 00:01:49.991
para criar o sistema,

00:01:50.015 --> 00:01:55.118
mas descobri que era bem difícil testá-lo,
a não ser que usasse uma máscara branca.

00:01:56.102 --> 00:02:00.448
Infelizmente, já tive esse problema antes.

00:02:00.472 --> 00:02:04.775
Quando cursava minha graduação
em ciência da computação na Georgia Tech,

00:02:04.799 --> 00:02:06.854
eu trabalhava com robôs sociais,

00:02:06.878 --> 00:02:10.655
e uma das minhas tarefas era fazer com que
um robô brincasse de "Achou!",

00:02:10.679 --> 00:02:12.362
um jogo simples de revezamento

00:02:12.386 --> 00:02:16.707
em que uma pessoa cobre o rosto e depois
o mostra à outra, dizendo: "Achou!"

00:02:16.731 --> 00:02:21.160
O problema é que a brincadeira
não dá certo se você não vê o outro,

00:02:21.184 --> 00:02:23.683
e meu robô não me via.

00:02:23.707 --> 00:02:27.657
Aí, peguei emprestado o rosto
de uma amiga para fazer o projeto,

00:02:27.681 --> 00:02:29.061
entreguei a tarefa

00:02:29.085 --> 00:02:32.838
e pensei: "Sabe de uma coisa?
Outra pessoa vai resolver esse problema".

00:02:33.489 --> 00:02:35.492
Pouco tempo depois,

00:02:35.516 --> 00:02:39.675
eu estava em Hong Kong,
numa competição de empreendedorismo.

00:02:40.159 --> 00:02:42.853
Os organizadores decidiram
levar os participantes

00:02:42.877 --> 00:02:45.249
pra visitar "start-ups" locais.

00:02:45.273 --> 00:02:49.778
Uma das start-ups tinha um robô social,
e eles decidiram fazer uma demonstração.

00:02:49.788 --> 00:02:51.642
A demonstração funcionou com todos,

00:02:51.652 --> 00:02:54.875
até que chegou a minha vez
e, como vocês já podem imaginar,

00:02:54.899 --> 00:02:57.864
ele não detectou meu rosto.

00:02:57.888 --> 00:03:00.399
Perguntei aos desenvolvedores por quê,

00:03:00.423 --> 00:03:05.926
e descobri que usaram o mesmo software
genérico de reconhecimento facial que eu.

00:03:05.944 --> 00:03:09.620
Do outro lado do mundo,
descobri que o viés algorítmico

00:03:09.630 --> 00:03:14.700
consegue viajar tão rápido
quanto um download da internet.

00:03:15.435 --> 00:03:18.641
O que estava acontecendo?
Por que meu rosto não era detectado?

00:03:18.665 --> 00:03:22.021
Bem, precisamos analisar
como damos "visão" às máquinas.

00:03:22.045 --> 00:03:25.454
A visão de computador utiliza
técnicas de aprendizagem automática

00:03:25.478 --> 00:03:27.358
para fazer o reconhecimento facial.

00:03:27.382 --> 00:03:31.279
Funciona assim: você cria uma série
de treinamento, com alguns rostos.

00:03:31.303 --> 00:03:34.121
"Isto é um rosto. Isto é um isto.
Isto não é um rosto."

00:03:34.145 --> 00:03:38.664
Com o tempo, você ensina o computador
a reconhecer outros rostos.

00:03:38.688 --> 00:03:42.677
Porém, se as séries não forem
diversificadas o bastante,

00:03:42.701 --> 00:03:46.050
qualquer rosto que seja
muito diferente dos demais

00:03:46.074 --> 00:03:49.723
será mais difícil de detectar,
e era isso que acontecia comigo.

00:03:49.734 --> 00:03:52.116
Mas não se preocupem. Tenho boas notícias.

00:03:52.140 --> 00:03:54.911
As séries de treinamento
não surgem do nada.

00:03:54.935 --> 00:03:56.723
Nós é que as criamos.

00:03:56.747 --> 00:04:00.923
Então, podemos criar
séries de amplo espectro,

00:04:00.947 --> 00:04:04.771
que reflitam rostos humanos
de forma mais diversa.

00:04:04.795 --> 00:04:07.016
Vocês já viram nos exemplos que dei

00:04:07.040 --> 00:04:08.808
como os robôs sociais

00:04:08.832 --> 00:04:13.443
me fizeram ver a exclusão causada
pelo viés algorítmico,

00:04:13.467 --> 00:04:18.282
mas o viés algorítmico também
pode acarretar práticas discriminatórias.

00:04:19.257 --> 00:04:20.710
Em todos os Estados Unidos,

00:04:20.734 --> 00:04:24.932
departamentos de polícia estão começando
a usar softwares de reconhecimento facial

00:04:24.956 --> 00:04:27.415
como parte de seu arsenal
na luta contra o crime.

00:04:27.439 --> 00:04:29.452
A Georgetown Law publicou um relatório

00:04:29.476 --> 00:04:36.239
mostrando que um em cada dois adultos
nos EUA, ou seja, 117 milhões de pessoas,

00:04:36.263 --> 00:04:39.797
tiveram seus rostos incluídos
em redes de reconhecimento facial.

00:04:39.821 --> 00:04:44.373
Hoje, os departamentos de polícia podem
usar essas redes sem qualquer regulação,

00:04:44.397 --> 00:04:48.683
usando algoritmos que não tiveram
sua precisão auditada.

00:04:48.707 --> 00:04:52.571
Ainda assim, sabemos que
o reconhecimento facial não é infalível,

00:04:52.595 --> 00:04:56.598
e identificar rostos de forma consistente
continua sendo um desafio.

00:04:56.618 --> 00:04:58.530
Talvez já tenham visto isso no Facebook.

00:04:58.554 --> 00:05:01.572
Eu e meus amigos rimos o tempo todo
quando vemos outras pessoas

00:05:01.596 --> 00:05:04.054
sendo marcadas incorretamente
em nossas fotos.

00:05:04.078 --> 00:05:09.669
Mas errar na identificação de um suspeito
de crime não é nada engraçado,

00:05:09.693 --> 00:05:12.520
nem violar liberdades civis.

00:05:12.544 --> 00:05:15.749
A aprendizagem automática vem sendo
usada no reconhecimento facial,

00:05:15.773 --> 00:05:20.278
mas também vem se expandindo
além da visão de computador.

00:05:21.086 --> 00:05:25.102
Em seu livro "Weapons
of Math Destruction",

00:05:25.126 --> 00:05:31.807
a cientista de dados Cathy O'Neil
fala sobre a ascensão dos novos "DMDs"

00:05:31.831 --> 00:05:36.184
algoritmos "disseminados,
misteriosos e destrutivos"

00:05:36.208 --> 00:05:39.172
que têm sido cada vez mais utilizados
na tomada de decisões

00:05:39.196 --> 00:05:42.373
que impactam mais aspectos
das nossas vidas.

00:05:42.397 --> 00:05:44.267
Quem será contratado ou demitido?

00:05:44.291 --> 00:05:46.403
Vai conseguir aquele 
empréstimo, ou seguro?

00:05:46.427 --> 00:05:49.930
Vai entrar na faculdade que você queria?

00:05:49.954 --> 00:05:53.463
Eu e você pagamos o mesmo valor
pelo mesmo produto

00:05:53.487 --> 00:05:55.929
vendido na mesma loja?

00:05:55.953 --> 00:05:59.712
A segurança pública também está começando
a usar a aprendizagem automática

00:05:59.736 --> 00:06:02.025
no policiamento preditivo.

00:06:02.049 --> 00:06:04.903
Alguns juízes utilizam índices
de risco gerados por máquinas

00:06:04.927 --> 00:06:09.969
para determinar quanto tempo
um indivíduo ficará na prisão.

00:06:09.993 --> 00:06:13.577
Temos realmente que refletir
sobre essas decisões. Será que são justas?

00:06:13.597 --> 00:06:19.967
E já vimos que o viés algorítmico
nem sempre leva a resultados justos.

00:06:19.989 --> 00:06:21.953
Então, o que podemos fazer?

00:06:21.977 --> 00:06:25.657
Bem, podemos começar a pensar
em como criar codificação mais inclusiva

00:06:25.681 --> 00:06:28.671
e adotar práticas
de codificação inclusivas.

00:06:28.695 --> 00:06:31.004
Tudo começa com pessoas.

00:06:31.528 --> 00:06:33.489
Então, é importante saber quem codifica.

00:06:33.513 --> 00:06:37.632
Estamos criando equipes diversificadas,
com indivíduos diferentes

00:06:37.656 --> 00:06:40.067
que possam verificar
pontos cegos uns dos outros?

00:06:40.091 --> 00:06:43.636
Quanto ao aspecto técnico,
a forma como codificamos é relevante.

00:06:43.660 --> 00:06:47.311
Estamos levando em conta a equidade
no desenvolvimento de sistemas?

00:06:47.335 --> 00:06:50.248
Finalmente, a razão pela qual
codificamos é relevante.

00:06:50.605 --> 00:06:55.688
Utilizamos ferramentas de criação
computacional para gerar imensas riquezas.

00:06:55.712 --> 00:07:00.159
Hoje temos a oportunidade
de gerar igualdade ainda maior,

00:07:00.183 --> 00:07:03.113
se considerarmos a mudança
social como uma prioridade

00:07:03.137 --> 00:07:05.307
e não como algo de menos importância.

00:07:05.828 --> 00:07:10.350
Esses são os três princípios na criação
do movimento pela codificação inclusiva.

00:07:10.374 --> 00:07:12.026
É importante quem codifica,

00:07:12.050 --> 00:07:13.593
é importante como se codifica

00:07:13.617 --> 00:07:15.640
e é importante por que se codifica.

00:07:15.664 --> 00:07:18.673
Então, para uma codificação inclusiva,
podemos começar a pensar

00:07:18.687 --> 00:07:21.811
na criação de plataformas
que identifiquem o viés,

00:07:21.835 --> 00:07:25.053
coletando as experiências das pessoas,
como as que eu contei aqui,

00:07:25.077 --> 00:07:28.147
mas também auditando
softwares já existentes.

00:07:28.171 --> 00:07:31.936
Também podemos começar a criar
séries de treinamento mais inclusivas.

00:07:31.960 --> 00:07:34.763
Imaginem uma campanha
de "'Selfies' pela Inclusão",

00:07:34.787 --> 00:07:37.656
em que eu e vocês possamos ajudar
os desenvolvedores a testar

00:07:37.656 --> 00:07:40.559
e criar séries de treinamento
mais inclusivas.

00:07:41.122 --> 00:07:43.950
Também podemos começar
a pensar de forma mais consciente

00:07:43.974 --> 00:07:49.229
sobre o impacto social das tecnologias
que temos desenvolvido.

00:07:49.249 --> 00:07:51.782
Pra iniciarmos o movimento
de codificação inclusiva

00:07:51.806 --> 00:07:54.653
lancei a Liga da Justiça Algorítmica,

00:07:54.677 --> 00:08:00.549
onde todos que se importem com a equidade
podem lutar contra o olhar codificado.

00:08:00.573 --> 00:08:03.869
Em codedgaze.com,
vocês podem relatar vieses,

00:08:03.893 --> 00:08:06.338
solicitar auditorias, 
participar dos testes

00:08:06.362 --> 00:08:09.133
e se juntar ao debate que vem ocorrendo,

00:08:09.157 --> 00:08:11.444
#codedgaze.

00:08:12.562 --> 00:08:15.049
Convido vocês a se juntarem a mim

00:08:15.073 --> 00:08:18.792
na criação de um mundo onde a tecnologia
trabalhe em favor de todos,

00:08:18.816 --> 00:08:20.713
não apenas em favor de alguns,

00:08:20.737 --> 00:08:25.325
um mundo onde valorizemos a inclusão
e tenhamos como foco a mudança social.

00:08:25.349 --> 00:08:26.524
Obrigada.

00:08:26.548 --> 00:08:29.533
(Aplausos)

00:08:32.693 --> 00:08:35.547
Mas tenho uma pergunta:

00:08:35.571 --> 00:08:37.630
Vocês vão se juntar a mim nessa luta?

00:08:37.654 --> 00:08:38.939
(Risos)

00:08:38.963 --> 00:08:41.519
(Aplausos)

