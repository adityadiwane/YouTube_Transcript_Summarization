WEBVTT
Kind: captions
Language: tr

00:00:00.000 --> 00:00:07.000
Çeviri: esra alantepe
Gözden geçirme: Suleyman Cengiz

00:00:12.861 --> 00:00:15.995
Merhaba ben kodlar şairi Joy

00:00:16.019 --> 00:00:21.012
görünmez bir gücün yükselişini
durdurma görevindeyim,

00:00:21.036 --> 00:00:23.892
algoritmik yanlılık için kullandığım

00:00:23.916 --> 00:00:27.225
"kodlu bakış" adını verdiğim bir güç.

00:00:27.249 --> 00:00:31.549
Algoritmik yanlılık, insan yanlılığı gibi,
adaletsizlikle sonuçlanır.

00:00:31.573 --> 00:00:37.595
Ama algoritmalar virüsler gibi büyük çapta

00:00:37.619 --> 00:00:39.201
ve hızda yanlılığı yayabilirler.

00:00:39.763 --> 00:00:44.150
Algoritmik yanlılık dışlayıcı deneyimlere

00:00:44.174 --> 00:00:46.302
ve ayrımcı uygulamalara yol açabilir.

00:00:46.326 --> 00:00:48.387
Ne demek istediğimi göstereyim.

00:00:48.800 --> 00:00:51.236
(Video) Joy Boulamwini: Selam kamera.
Bir yüzüm var.

00:00:51.982 --> 00:00:53.846
Yüzümü görebiliyor musunuz?

00:00:53.871 --> 00:00:55.496
Gözlüksüz yüzüm?

00:00:55.521 --> 00:00:57.735
Onun yüzünü görebilirsin.

00:00:58.057 --> 00:01:00.302
Ya benimkini?

00:01:03.710 --> 00:01:07.460
Maskem var. Maskemi görebiliyor musun?

00:01:08.294 --> 00:01:10.659
Joy Boulamwini: Bu nasıl oldu?

00:01:10.683 --> 00:01:13.668
Neden bilgisayarın önünde bir beyaz maske

00:01:13.668 --> 00:01:15.272
ile oturuyor ve ucuz bir kamera

00:01:15.296 --> 00:01:18.946
tarafından algılanmaya çalışıyorum?

00:01:18.970 --> 00:01:21.261
Kod şairi olarak kodlu bakış ile

00:01:21.285 --> 00:01:22.805
savaşmadığımda

00:01:22.829 --> 00:01:26.101
MIT Medya Lab'ında lisanüstü öğrencisiyim

00:01:26.125 --> 00:01:31.042
ve her tür garip projede
çalışma şansım var.

00:01:31.066 --> 00:01:33.093
Buna yaptığım Aspire Mirror da dâhil.

00:01:33.117 --> 00:01:38.251
Sayesinde yansımamın üzerine
dijital maskeler tasarlayabilirim.

00:01:38.275 --> 00:01:40.625
Sabah eğer güçlü hissetmek istersem,

00:01:40.649 --> 00:01:42.083
bir aslanı takabilirim.

00:01:42.107 --> 00:01:45.603
Sevinçli olmak istiyorsam,
bir alıntı yapabilirim.

00:01:45.627 --> 00:01:48.616
Sistemi oluşturmak için genel yüz tanıma

00:01:48.640 --> 00:01:49.991
yazılımını kullandım.

00:01:50.015 --> 00:01:55.118
Ama beyaz bir maske takmadıkça
sistemi test etmek gerçekten zordu.

00:01:56.102 --> 00:02:00.448
Maalesef bu sorunla daha önce karşılaştım.

00:02:00.472 --> 00:02:04.775
Georgia Tech de bilgisayar
bilimleri okurken,

00:02:04.799 --> 00:02:06.854
sosyal robotlar üzerinde çalıştım

00:02:06.878 --> 00:02:10.655
ve görevlerimden biri ce-ee
oynayan bir robot yapmaktı.

00:02:10.679 --> 00:02:12.362
Partnerlerinizin yüzlerini kapayıp

00:02:12.386 --> 00:02:16.707
sonra açıp ce-ee dedikleri bir oyun.

00:02:16.731 --> 00:02:21.160
Problem şu ki sizi göremezsem
ce-ee işe yaramaz

00:02:21.184 --> 00:02:23.683
ve robotum beni göremiyordu.

00:02:23.707 --> 00:02:27.657
Projeyi tamamlamak için
arkadaşımın yüzünü ödünç aldım,

00:02:27.681 --> 00:02:29.061
ödevimi sundum ve bu sorunu

00:02:29.085 --> 00:02:32.838
başkasının çözeceğini varsaydım.

00:02:33.489 --> 00:02:35.492
Daha sonra Hong Kong'da

00:02:35.516 --> 00:02:39.675
bir girişimcilik yarışmasına katıldım.

00:02:40.159 --> 00:02:42.853
Organizatörler yarışmacıların başlangıç

00:02:42.877 --> 00:02:45.249
turuna katılmasına karar verdi.

00:02:45.273 --> 00:02:47.988
Yarışmacılardan birinin 
sosyal robotu vardı

00:02:48.012 --> 00:02:49.924
ve demo yapmaya karar verdi.

00:02:49.948 --> 00:02:52.928
Demo bana gelene kadar işe yaradı

00:02:52.952 --> 00:02:54.875
ve ne olduğunu tahmin edebilirsiniz.

00:02:54.899 --> 00:02:57.864
Robot yüzümü algılayamadı.

00:02:57.888 --> 00:03:00.399
Geliştiricisine ne olduğunu sordum

00:03:00.423 --> 00:03:05.956
ve fark ettik ki aynı genel
yüz tanıma yazılımını kullanıyormuşuz.

00:03:05.980 --> 00:03:07.630
Dünyanın öbür ucunda

00:03:07.654 --> 00:03:11.506
algoritmik yanlılığın
internetten dosya indirmek kadar

00:03:11.530 --> 00:03:14.700
hızlı yolculuk yaptığını öğrendim.

00:03:15.565 --> 00:03:18.641
Yani ne oluyor? Neden beni algılamıyor?

00:03:18.665 --> 00:03:22.021
Makinelere nasıl görme yetisi
verdiğimize göz atmalıyız.

00:03:22.045 --> 00:03:25.454
Bilgisayar görüşü yüzü tanımlamak için

00:03:25.478 --> 00:03:27.358
makine öğrenme tekniklerini kullanır.

00:03:27.382 --> 00:03:31.279
Bunun çalışma şekli sizin yüz örnekleriyle
eğitim seti oluşturmanızdır.

00:03:31.303 --> 00:03:34.121
Bu bir yüz. Bu bir yüz.
Bu bir yüz değil.

00:03:34.145 --> 00:03:38.664
Zamanla bilgisayara diğer yüzleri
nasıl tanımlayacağını öğretirsiniz.

00:03:38.688 --> 00:03:42.677
Fakat eğitim seti kapsamlı değilse

00:03:42.701 --> 00:03:46.050
sabit normdan çok fazla sapan bir yüzü

00:03:46.074 --> 00:03:47.723
tanımlamak zor olacaktır.

00:03:47.747 --> 00:03:49.710
Bana olduğu gibi.

00:03:49.734 --> 00:03:52.116
Ama merak etmeyin - iyi haberlerim var.

00:03:52.140 --> 00:03:54.911
Eğitim setleri bir anda ortaya çıkmaz.

00:03:54.935 --> 00:03:56.723
Aslında biz onları yaratabiliriz.

00:03:56.747 --> 00:04:00.923
Yani insanlığın daha zengin portresini

00:04:00.947 --> 00:04:04.771
çıkaran tam spektrumlu
eğitim seti oluşturabiliriz.

00:04:04.795 --> 00:04:07.016
Şimdi örneklerimde sosyal robotlar

00:04:07.040 --> 00:04:08.808
sayesinde algoritmik dışlanmayı

00:04:08.832 --> 00:04:13.443
nasıl ortaya çıkardığımı gördünüz.

00:04:13.467 --> 00:04:18.282
Ama algoritmik yanlılık ayrımcı
uygulamalara da yol açabilir.

00:04:19.257 --> 00:04:20.710
ABD çapında, polis merkezleri

00:04:20.734 --> 00:04:24.932
suçla mücadele cephaneliklerinde

00:04:24.956 --> 00:04:27.415
yüz tanıma yazılımını kullanmaya başladı.

00:04:27.439 --> 00:04:29.452
Georgetown Yasası ABD'de iki yetişkinden

00:04:29.476 --> 00:04:36.239
birinin - 117 milyon insanın- yüz tanıma

00:04:36.263 --> 00:04:39.797
ağında yüzlerinin bulunduğunu açıkladı.

00:04:39.821 --> 00:04:44.373
Polis merkezleri doğrulukları
denetlenmemiş

00:04:44.397 --> 00:04:48.683
algoritmalar kullanarak
bu ağları izinsiz görebilir.

00:04:48.707 --> 00:04:52.571
Biliyoruz ki yüz tanıma yanılmaz değil

00:04:52.595 --> 00:04:56.738
ve yüzlerin tutarlı etiketlenmesi
hâlâ bir sorun.

00:04:56.738 --> 00:04:58.560
Bunu Facebook da görmüş olabilirsiniz.

00:04:58.584 --> 00:05:01.572
Arkadaşlarımla diğer insanların yanlış

00:05:01.596 --> 00:05:04.054
etiketlendiğini gördüğümüzde gülüyoruz.

00:05:04.078 --> 00:05:09.669
Şüpheli bir suçluyu yanlış tanımlamak
gülünç bir mesele

00:05:09.693 --> 00:05:12.520
veya sivil özgürlükleri ihlal değil.

00:05:12.544 --> 00:05:15.749
Makine öğrenme yüz tanımada kullanılır

00:05:15.773 --> 00:05:20.278
ama bilgisayar görüş alanının
ötesine de uzanıyor.

00:05:21.086 --> 00:05:25.102
Veri bilimcisi Cathy O'Neil

00:05:25.126 --> 00:05:31.807
"Matemetiksel İmha Silahları"
kitabında, hayatımızın

00:05:31.831 --> 00:05:36.184
daha çok boyutunu etkileyen kararlar

00:05:36.208 --> 00:05:39.172
almak için kullanılan yaygın,
gizemli ve yıkıcı

00:05:39.196 --> 00:05:42.373
algoritmalarla KİS'lerden bahsediyor.

00:05:42.397 --> 00:05:44.267
Peki işe alınan ya da kovulan kim?

00:05:44.291 --> 00:05:46.403
O krediyi alıyor musun? Sigortan var mı?

00:05:46.427 --> 00:05:49.930
Girmek istediğin üniversiteye
kabul edildin mi?

00:05:49.954 --> 00:05:53.463
Aynı platformda satın alınan
aynı ürün için

00:05:53.487 --> 00:05:55.929
sen ve ben aynı fiyatı mı ödüyoruz?

00:05:55.953 --> 00:05:59.712
Güvenlik polisi öngörücü polislik için

00:05:59.736 --> 00:06:02.025
otomatik öğrenmeyi kullanmaya başlıyor.

00:06:02.049 --> 00:06:05.543
Bazı hakimler hapis cezasını

00:06:05.567 --> 00:06:09.969
belirlemek için makine üretimli
risk puanlarını kullanıyor.

00:06:09.993 --> 00:06:12.447
Bu kararları gerçekten 
düşünmek zorundayız.

00:06:12.471 --> 00:06:13.653
Adiller mi?

00:06:13.677 --> 00:06:16.567
Gördük ki algoritmik yanlılık her zaman

00:06:16.591 --> 00:06:19.965
adil sonuçlar getirmez.

00:06:19.989 --> 00:06:21.953
Peki biz ne yapabiliriz?

00:06:21.977 --> 00:06:25.657
Daha kapsamlı kod oluşturup
kapsamlı kod uygulamalarını

00:06:25.681 --> 00:06:28.671
nasıl kullandığımızı düşünebiliriz.

00:06:28.695 --> 00:06:31.004
Bu, insanlarla başlar.

00:06:31.528 --> 00:06:33.489
Yani kimin kodladığı önemlidir.

00:06:33.513 --> 00:06:37.632
Birbirlerinin kör noktalarını görebilen
farklı kişilerle tam spektrumlu

00:06:37.656 --> 00:06:40.067
takımlar oluşturabiliyor muyuz?

00:06:40.091 --> 00:06:43.636
Teknik olarak, nasıl kodladığımız önemli.

00:06:43.660 --> 00:06:47.311
Sistemi geliştirirken adaletle
mi oluşturuyoruz?

00:06:47.335 --> 00:06:50.248
Son olarak niçin kodladığımız önemlidir.

00:06:50.605 --> 00:06:55.688
Harika zenginliğin kilidini açmak için
işlemsel yaratma araçlarını kullandık.

00:06:55.712 --> 00:07:00.159
Sosyal değişimi sonraki düşünce değil,

00:07:00.183 --> 00:07:03.113
öncelik hâline getirirsek daha geniş

00:07:03.137 --> 00:07:05.307
eşitliği açma fırsatımız olur.

00:07:05.828 --> 00:07:10.350
"Incoding" hareketini oluşturan
gerekli üç ilke var:

00:07:10.374 --> 00:07:12.026
Kimin kodladığı,

00:07:12.050 --> 00:07:13.593
nasıl kodladığı

00:07:13.617 --> 00:07:15.640
ve niçin kodladığı.

00:07:15.664 --> 00:07:18.763
Incoding'e yönelmek için
benim paylaştıklarım

00:07:18.787 --> 00:07:21.951
gibi insanların deneyimlerini toplayarak

00:07:21.975 --> 00:07:25.053
yanlılığı belirleyen ve var olan yazılımı

00:07:25.077 --> 00:07:28.147
denetleyen platformlar oluşturmayı
düşünmeye başlayabiliriz.

00:07:28.171 --> 00:07:31.936
Ayrıca daha kapsamlı
eğitim setleri hazırlayabiliriz.

00:07:31.960 --> 00:07:34.763
Geliştiricilerin daha kapsamlı
eğitim setleri oluşturmasına

00:07:34.787 --> 00:07:38.442
ve deneyebilmesine yardım
ettiğimiz "Kaynaşmak için Özçekimler"

00:07:38.466 --> 00:07:40.559
kampanyasını düşünün.

00:07:41.122 --> 00:07:43.950
Ve şu an geliştirdiğimiz
teknolojinin toplumsal etkileri

00:07:43.974 --> 00:07:49.365
konusunda daha vicdani
düşünmeye başlayabiliriz.

00:07:49.389 --> 00:07:51.782
Incoding hareketini başlatmak için

00:07:51.806 --> 00:07:54.653
adaletle ilgilenen birinin kodlu 
bakışla savaşabileceği

00:07:54.677 --> 00:08:00.549
Algoritmik Adalet Ligi'ni başlattım.

00:08:00.573 --> 00:08:03.853
codedgaze.com'da yanlılığı bildirebilir,

00:08:03.853 --> 00:08:06.338
denetleme isteyebilir, deneyen olabilir,

00:08:06.362 --> 00:08:08.352
süren tartışmalara katılabilirsiniz,

00:08:09.162 --> 00:08:11.147
#codedgaze

00:08:12.559 --> 00:08:14.784
Sizi dâhil olmaya ve
merkez sosyal değişime

00:08:14.814 --> 00:08:18.581
değer verdiğimiz ve teknolojinin 
sadece bir kısım için değil,

00:08:18.601 --> 00:08:20.730
herkes için işe yaradığı

00:08:20.740 --> 00:08:25.037
bir dünyaya davet ediyorum.

00:08:25.077 --> 00:08:26.533
Teşekkürler.

00:08:26.553 --> 00:08:27.728
(Alkış)

00:08:32.693 --> 00:08:35.104
Bir sorum var.

00:08:35.574 --> 00:08:37.548
Bu savaşta benimle misiniz?

00:08:37.648 --> 00:08:38.877
(Gülüşmeler)

00:08:38.957 --> 00:08:40.089
(Alkış)

