WEBVTT
Kind: captions
Language: ja

00:00:00.000 --> 00:00:07.000
翻訳: Yoichi Fukuoka
校正: Yuko Yoshida

00:00:12.861 --> 00:00:15.995
こんにちは
コードの詩人 ジョイです

00:00:16.019 --> 00:00:21.012
私の使命は 強まりつつある
目に見えない力

00:00:21.036 --> 00:00:23.892
「コード化された視線」
と私が呼ぶ―

00:00:23.916 --> 00:00:26.975
アルゴリズムにおける偏見を
阻止することです

00:00:27.249 --> 00:00:31.369
アルゴリズムも 人と同じで
偏見が入り込むと不公正な結果を生みます

00:00:31.573 --> 00:00:37.565
しかも アルゴリズムはウイルスのように
偏見をどんどん拡散してしまいます

00:00:37.619 --> 00:00:39.201
それも急速にです

00:00:39.763 --> 00:00:44.150
アルゴリズムの偏見は
排他的な扱いや

00:00:44.174 --> 00:00:46.302
差別行為を生む恐れもあります

00:00:46.326 --> 00:00:48.387
具体的にお見せしましょう

00:00:48.800 --> 00:00:51.236
（ビデオ）ねえ カメラくん
私には顔があるわ

00:00:51.982 --> 00:00:53.846
私の顔が見える？

00:00:53.871 --> 00:00:55.496
メガネを外した方がいいかな？

00:00:55.521 --> 00:00:57.735
この人の顔は見えるわね

00:00:58.057 --> 00:01:00.302
私の顔はどう？

00:01:03.710 --> 00:01:07.460
お面をつけるわ
このお面は見える？

00:01:08.294 --> 00:01:10.659
（ジョイ）何が起きたのでしょう？

00:01:10.683 --> 00:01:13.824
私が白い仮面をつけて

00:01:13.848 --> 00:01:15.272
コンピュータの前に座り

00:01:15.296 --> 00:01:18.946
安いウェブカメラで検知されるか
試している理由は？

00:01:18.970 --> 00:01:21.261
コードの詩人として
コード化された視線と

00:01:21.285 --> 00:01:22.805
戦うとき以外の私は

00:01:22.829 --> 00:01:26.101
MITメディアラボの大学院生で

00:01:26.125 --> 00:01:31.042
ちょっと変わったプロジェクトに
いろいろ関わっています

00:01:31.066 --> 00:01:33.093
その１つの
「Aspire Mirror（鼓舞する鏡）」は

00:01:33.117 --> 00:01:38.251
鏡に映った自分の顔に
デジタルの仮面を重ねるものです

00:01:38.275 --> 00:01:40.625
朝起きてパワフルな気分に
なりたければ

00:01:40.649 --> 00:01:42.083
ライオンの面をつけたり

00:01:42.107 --> 00:01:45.603
気分を高めるために
何かを「引用」したりできます

00:01:45.627 --> 00:01:48.540
このとき 私は
一般的な顔認識ソフトを使い

00:01:48.540 --> 00:01:50.041
システムを組もうとしたのですが

00:01:50.041 --> 00:01:55.118
白い仮面をつけないと
テストすら難しいと気づきました

00:01:56.102 --> 00:02:00.448
実は この問題に悩まされたのは
初めてではありませんでした

00:02:00.472 --> 00:02:04.775
ジョージア工科大学で
コンピュータ科学を学んでいたとき

00:02:04.799 --> 00:02:06.854
ソーシャルロボットを研究していて

00:02:06.878 --> 00:02:09.205
ロボット相手に
代わりばんこに

00:02:09.209 --> 00:02:12.402
「いないいないばあ」をするという
課題がありました

00:02:12.402 --> 00:02:16.707
顔を覆った手を 「ばあ！」のところで
どけて顔を見せる遊びですね

00:02:16.731 --> 00:02:21.160
相手の顔が見えないと
成立しないゲームですが

00:02:21.184 --> 00:02:23.683
ロボットは私の顔を
認識できませんでした

00:02:23.707 --> 00:02:27.657
私はルームメイトの顔を借りて
どうにか作業を終え

00:02:27.681 --> 00:02:29.061
課題を提出しました

00:02:29.085 --> 00:02:32.838
そのときは このバグも誰かが
直してくれると思っていました

00:02:33.489 --> 00:02:35.492
それから少しして 私は

00:02:35.516 --> 00:02:39.675
起業コンテストに参加するため
香港に行きました

00:02:40.159 --> 00:02:42.853
参加者たちは
いくつかの地元の新興企業に

00:02:42.877 --> 00:02:45.249
案内されました

00:02:45.273 --> 00:02:47.988
ある会社の製品は
ソーシャルロボットで

00:02:48.012 --> 00:02:49.924
デモを見せてもらうことに
なりました

00:02:49.948 --> 00:02:52.928
デモは成功し
私の番がやって来ます

00:02:52.952 --> 00:02:54.875
結果はご想像のとおり

00:02:54.899 --> 00:02:57.864
私の顔は検知されません

00:02:57.888 --> 00:03:00.399
「どうなってるの？」
と開発者たちに尋ねたら

00:03:00.423 --> 00:03:05.956
私と同じ顔認識ソフトを
使っていることが判明しました

00:03:05.980 --> 00:03:07.630
世界を半周して分かったのは

00:03:07.654 --> 00:03:11.506
アルゴリズムの偏見が
広がるのはすごく早いことです

00:03:11.530 --> 00:03:14.700
ネットからダウンロードする
時間しかかかりません

00:03:15.565 --> 00:03:18.641
どうなっているのか？
なぜ私の顔は検知されないのか？

00:03:18.665 --> 00:03:22.021
機械に視覚を持たせる仕組みを
考えてみましょう

00:03:22.045 --> 00:03:25.454
コンピュータビジョンでは
機械学習の技術を使って

00:03:25.478 --> 00:03:27.358
顔を認識します

00:03:27.382 --> 00:03:31.279
顔のサンプルを集めて
トレーニングするのです

00:03:31.303 --> 00:03:34.121
これは顔 これは顔
これは顔じゃない

00:03:34.145 --> 00:03:38.664
やがてコンピュータは
顔を認識する方法を習得します

00:03:38.688 --> 00:03:42.677
でもトレーニング用のサンプルに
あまり多様性がなかったら

00:03:42.701 --> 00:03:46.050
そこで確立された基準から
大きく外れた顔は

00:03:46.074 --> 00:03:47.723
検知するのが難しくなります

00:03:47.747 --> 00:03:49.710
私の場合はそれでした

00:03:49.734 --> 00:03:52.116
でも大丈夫
希望の持てる面もあります

00:03:52.140 --> 00:03:54.911
顔のサンプル集は ひとりでに
できるわけではありません

00:03:54.935 --> 00:03:56.723
私たちにだって作れます

00:03:56.747 --> 00:04:00.923
ですから 網羅的な
トレーニング用のサンプルを作り

00:04:00.947 --> 00:04:04.771
さまざまな人間の姿形を
しっかり反映させれば いいんです

00:04:04.795 --> 00:04:07.016
私の例でお見せしたように

00:04:07.040 --> 00:04:08.808
ソーシャルロボットの振る舞いから

00:04:08.832 --> 00:04:13.443
アルゴリズムの偏見による
切り捨てが見つかりました

00:04:13.467 --> 00:04:18.282
この偏見は 差別行為にも
つながりうるものです

00:04:19.257 --> 00:04:20.710
米国では全国的に

00:04:20.734 --> 00:04:24.932
警察が顔認識ソフトを使って

00:04:24.956 --> 00:04:27.415
犯罪に対処し始めています

00:04:27.439 --> 00:04:29.782
ジョージタウン大学
ロースクールの報告によると

00:04:29.786 --> 00:04:36.239
米国の成人２人に１人
１億1700万人の顔が

00:04:36.263 --> 00:04:39.797
何らかの顔認識用のデータベースに
記録されています

00:04:39.821 --> 00:04:44.373
警察は現在 制限なしに
これらのシステムを参照できますが

00:04:44.397 --> 00:04:48.683
使っているアルゴリズムの
正確さは検証されていません

00:04:48.707 --> 00:04:52.571
でも 顔認識は完璧でなく

00:04:52.595 --> 00:04:56.768
顔と名前を一致させるのは
なおも難しい課題です

00:04:56.768 --> 00:04:58.640
Facebookで
こんな経験をしたことは？

00:04:58.640 --> 00:05:01.572
写真の中の人が正しく認識されず
おかしなタグが付いていて

00:05:01.596 --> 00:05:04.054
私は いつも友達と大笑いしています

00:05:04.078 --> 00:05:09.669
でも 犯罪者と間違われたら
笑い事では済みません

00:05:09.693 --> 00:05:12.520
市民の自由が侵された
どころでもありません

00:05:12.544 --> 00:05:15.749
機械学習は顔認識に
利用されるとともに

00:05:15.773 --> 00:05:20.278
コンピュータビジョンの
利用範囲を拡大しています

00:05:21.086 --> 00:05:25.102
データ・サイエンティストの
キャシー・オニールは

00:05:25.126 --> 00:05:31.807
『Weapons of Math Destruction
（数学破壊兵器）』の中で

00:05:31.831 --> 00:05:36.184
不可解で破壊的なアルゴリズムが
「数学破壊兵器」として台頭し

00:05:36.208 --> 00:05:39.172
ますます広く意思決定に
使われるようになり

00:05:39.196 --> 00:05:42.373
私たちの暮らしの多くの場面に
影響を与えていると述べています

00:05:42.397 --> 00:05:44.377
仕事の採用・不採用を分けるものは？

00:05:44.377 --> 00:05:46.393
ローンが組めるかどうか
保険に入れるかどうか

00:05:46.393 --> 00:05:49.930
志望大学に入学できるかどうか

00:05:49.954 --> 00:05:53.463
同じところで同じ製品を
購入するのなら

00:05:53.487 --> 00:05:55.929
誰でも値段は同じかどうか
といったことにもです

00:05:55.953 --> 00:05:59.712
警察は機械学習を
予測警備にも

00:05:59.736 --> 00:06:02.025
使い始めています

00:06:02.049 --> 00:06:05.543
裁判官の中には
機械が出したリスク評価をもとに

00:06:05.567 --> 00:06:09.969
何年の懲役にすべきか
決める人もいます

00:06:09.973 --> 00:06:12.467
だから こうした判断について
よく考えなければなりません

00:06:12.471 --> 00:06:13.803
判断は公平なのでしょうか？

00:06:13.807 --> 00:06:16.567
すでに見た通り
アルゴリズムの偏見により

00:06:16.591 --> 00:06:19.965
出てくる結果は必ずしも
公平ではありません

00:06:19.989 --> 00:06:21.953
私たちに何ができるでしょう

00:06:21.977 --> 00:06:25.657
どうすれば
より包括的なコードを作り

00:06:25.681 --> 00:06:28.671
それを実践していけるのか
考えることから始めましょう

00:06:28.695 --> 00:06:31.004
まずは人からです

00:06:31.528 --> 00:06:33.439
つまり コードを書く人が大事です

00:06:33.489 --> 00:06:37.526
多様なメンバーで
チームを構成し

00:06:37.536 --> 00:06:40.067
互いの盲点を
チェックできているか？

00:06:40.091 --> 00:06:43.636
技術的な面では
コードをどう作るかが重要です

00:06:43.660 --> 00:06:47.311
システム開発の要素に
公正さを組み込んでいるか？

00:06:47.335 --> 00:06:50.248
最後に 何のために
コードを書くかも重要です

00:06:50.605 --> 00:06:55.688
私たちはコンピュータを使って
莫大な富を生み出してきましたが

00:06:55.712 --> 00:07:00.159
今 より一層の平等も
生み出すことができるのです

00:07:00.183 --> 00:07:03.113
社会の変革を
後付けでなく

00:07:03.137 --> 00:07:05.307
優先事項とすれば良いのです

00:07:05.828 --> 00:07:10.350
このように「インコーディング」運動には
３つの柱があるわけです

00:07:10.374 --> 00:07:12.026
誰がコードを書くかが重要

00:07:12.050 --> 00:07:13.593
どのようにコードを書くかが重要

00:07:13.617 --> 00:07:15.640
なぜコードを書くかが重要 なのです

00:07:15.664 --> 00:07:19.043
インコーディングに向けて
まず手を付けるべきは

00:07:19.043 --> 00:07:21.951
偏りを発見できる
プラットフォームを作ること

00:07:21.975 --> 00:07:25.053
私が紹介したような
経験を集積して

00:07:25.077 --> 00:07:27.817
今あるソフトウェアの
点検もします

00:07:28.171 --> 00:07:31.936
トレーニング用サンプルにも
もっと多様性を盛り込みましょう

00:07:31.960 --> 00:07:34.763
「インクルージョンに向けた自撮り」
キャンペーンを展開して

00:07:34.787 --> 00:07:38.442
多様性のあるサンプルを
開発者が作成しテストするのを

00:07:38.466 --> 00:07:40.379
支援することもできます

00:07:41.122 --> 00:07:43.950
さらには 開発中の技術が
社会に与える影響について

00:07:43.974 --> 00:07:48.975
もっと注意深く
考えるようにしましょう

00:07:49.389 --> 00:07:51.802
インコーディング運動を
始めるために

00:07:51.806 --> 00:07:54.683
私は「Algorithmic Justice League」
を設立しました

00:07:54.683 --> 00:08:00.399
公正さに関心がある人なら誰でも
コード化された視線との戦いに参加できます

00:08:00.573 --> 00:08:03.869
偏ったコードの通報や監査依頼

00:08:03.893 --> 00:08:06.338
試験者の参加申し込みは
codedgaze.comへ

00:08:06.362 --> 00:08:09.133
話題の共有には

00:08:09.157 --> 00:08:11.444
#codedgazeを使ってください

00:08:12.562 --> 00:08:15.049
ぜひ皆さんに
参加してほしいんです

00:08:15.073 --> 00:08:18.792
テクノロジーが
一部の人でなく みんなの役に立つ―

00:08:18.816 --> 00:08:20.713
世界を創りましょう

00:08:20.737 --> 00:08:25.325
インクルージョンを大事にし
自ら社会を変える そんな世界にしましょう

00:08:25.349 --> 00:08:26.524
ありがとうございました

00:08:26.548 --> 00:08:30.819
（拍手）

00:08:32.693 --> 00:08:35.547
でも ほんとに大丈夫？

00:08:35.571 --> 00:08:37.630
本気で参加してくれますね？

00:08:37.654 --> 00:08:38.939
（笑）

00:08:38.963 --> 00:08:42.650
（拍手）

