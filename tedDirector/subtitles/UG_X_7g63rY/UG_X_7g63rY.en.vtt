WEBVTT
Kind: captions
Language: en

00:00:12.861 --> 00:00:15.995
Hello, I'm Joy, a poet of code,

00:00:16.019 --> 00:00:21.012
on a mission to stop
an unseen force that's rising,

00:00:21.036 --> 00:00:23.892
a force that I called "the coded gaze,"

00:00:23.916 --> 00:00:27.225
my term for algorithmic bias.

00:00:27.249 --> 00:00:31.549
Algorithmic bias, like human bias,
results in unfairness.

00:00:31.573 --> 00:00:37.595
However, algorithms, like viruses,
can spread bias on a massive scale

00:00:37.619 --> 00:00:39.201
at a rapid pace.

00:00:39.763 --> 00:00:44.150
Algorithmic bias can also lead
to exclusionary experiences

00:00:44.174 --> 00:00:46.302
and discriminatory practices.

00:00:46.326 --> 00:00:48.387
Let me show you what I mean.

00:00:48.800 --> 00:00:51.236
(Video) Joy Buolamwini: Hi, camera.
I've got a face.

00:00:51.982 --> 00:00:53.846
Can you see my face?

00:00:53.871 --> 00:00:55.496
No-glasses face?

00:00:55.521 --> 00:00:57.735
You can see her face.

00:00:58.057 --> 00:01:00.302
What about my face?

00:01:03.710 --> 00:01:07.460
I've got a mask. Can you see my mask?

00:01:08.294 --> 00:01:10.659
Joy Buolamwini: So how did this happen?

00:01:10.683 --> 00:01:13.824
Why am I sitting in front of a computer

00:01:13.848 --> 00:01:15.272
in a white mask,

00:01:15.296 --> 00:01:18.946
trying to be detected by a cheap webcam?

00:01:18.970 --> 00:01:21.261
Well, when I'm not fighting the coded gaze

00:01:21.285 --> 00:01:22.805
as a poet of code,

00:01:22.829 --> 00:01:26.101
I'm a graduate student
at the MIT Media Lab,

00:01:26.125 --> 00:01:31.042
and there I have the opportunity to work
on all sorts of whimsical projects,

00:01:31.066 --> 00:01:33.093
including the Aspire Mirror,

00:01:33.117 --> 00:01:38.251
a project I did so I could project
digital masks onto my reflection.

00:01:38.275 --> 00:01:40.625
So in the morning, if I wanted
to feel powerful,

00:01:40.649 --> 00:01:42.083
I could put on a lion.

00:01:42.107 --> 00:01:45.603
If I wanted to be uplifted,
I might have a quote.

00:01:45.627 --> 00:01:48.616
So I used generic
facial recognition software

00:01:48.640 --> 00:01:49.991
to build the system,

00:01:50.015 --> 00:01:55.118
but found it was really hard to test it
unless I wore a white mask.

00:01:56.102 --> 00:02:00.448
Unfortunately, I've run
into this issue before.

00:02:00.472 --> 00:02:04.775
When I was an undergraduate
at Georgia Tech studying computer science,

00:02:04.799 --> 00:02:06.854
I used to work on social robots,

00:02:06.878 --> 00:02:10.655
and one of my tasks was to get a robot
to play peek-a-boo,

00:02:10.679 --> 00:02:12.362
a simple turn-taking game

00:02:12.386 --> 00:02:16.707
where partners cover their face
and then uncover it saying, "Peek-a-boo!"

00:02:16.731 --> 00:02:21.160
The problem is, peek-a-boo
doesn't really work if I can't see you,

00:02:21.184 --> 00:02:23.683
and my robot couldn't see me.

00:02:23.707 --> 00:02:27.657
But I borrowed my roommate's face
to get the project done,

00:02:27.681 --> 00:02:29.061
submitted the assignment,

00:02:29.085 --> 00:02:32.838
and figured, you know what,
somebody else will solve this problem.

00:02:33.489 --> 00:02:35.492
Not too long after,

00:02:35.516 --> 00:02:39.675
I was in Hong Kong
for an entrepreneurship competition.

00:02:40.159 --> 00:02:42.853
The organizers decided
to take participants

00:02:42.877 --> 00:02:45.249
on a tour of local start-ups.

00:02:45.273 --> 00:02:47.988
One of the start-ups had a social robot,

00:02:48.012 --> 00:02:49.924
and they decided to do a demo.

00:02:49.948 --> 00:02:52.928
The demo worked on everybody
until it got to me,

00:02:52.952 --> 00:02:54.875
and you can probably guess it.

00:02:54.899 --> 00:02:57.864
It couldn't detect my face.

00:02:57.888 --> 00:03:00.399
I asked the developers what was going on,

00:03:00.423 --> 00:03:05.956
and it turned out we had used the same
generic facial recognition software.

00:03:05.980 --> 00:03:07.630
Halfway around the world,

00:03:07.654 --> 00:03:11.506
I learned that algorithmic bias
can travel as quickly

00:03:11.530 --> 00:03:14.700
as it takes to download
some files off of the internet.

00:03:15.565 --> 00:03:18.641
So what's going on?
Why isn't my face being detected?

00:03:18.665 --> 00:03:22.021
Well, we have to look
at how we give machines sight.

00:03:22.045 --> 00:03:25.454
Computer vision uses
machine learning techniques

00:03:25.478 --> 00:03:27.358
to do facial recognition.

00:03:27.382 --> 00:03:31.279
So how this works is, you create
a training set with examples of faces.

00:03:31.303 --> 00:03:34.121
This is a face. This is a face.
This is not a face.

00:03:34.145 --> 00:03:38.664
And over time, you can teach a computer
how to recognize other faces.

00:03:38.688 --> 00:03:42.677
However, if the training sets
aren't really that diverse,

00:03:42.701 --> 00:03:46.050
any face that deviates too much
from the established norm

00:03:46.074 --> 00:03:47.723
will be harder to detect,

00:03:47.747 --> 00:03:49.710
which is what was happening to me.

00:03:49.734 --> 00:03:52.116
But don't worry -- there's some good news.

00:03:52.140 --> 00:03:54.911
Training sets don't just
materialize out of nowhere.

00:03:54.935 --> 00:03:56.723
We actually can create them.

00:03:56.747 --> 00:04:00.923
So there's an opportunity to create
full-spectrum training sets

00:04:00.947 --> 00:04:04.771
that reflect a richer
portrait of humanity.

00:04:04.795 --> 00:04:07.016
Now you've seen in my examples

00:04:07.040 --> 00:04:08.808
how social robots

00:04:08.832 --> 00:04:13.443
was how I found out about exclusion
with algorithmic bias.

00:04:13.467 --> 00:04:18.282
But algorithmic bias can also lead
to discriminatory practices.

00:04:19.257 --> 00:04:20.710
Across the US,

00:04:20.734 --> 00:04:24.932
police departments are starting to use
facial recognition software

00:04:24.956 --> 00:04:27.415
in their crime-fighting arsenal.

00:04:27.439 --> 00:04:29.452
Georgetown Law published a report

00:04:29.476 --> 00:04:36.239
showing that one in two adults
in the US -- that's 117 million people --

00:04:36.263 --> 00:04:39.797
have their faces
in facial recognition networks.

00:04:39.821 --> 00:04:44.373
Police departments can currently look
at these networks unregulated,

00:04:44.397 --> 00:04:48.683
using algorithms that have not
been audited for accuracy.

00:04:48.707 --> 00:04:52.571
Yet we know facial recognition
is not fail proof,

00:04:52.595 --> 00:04:56.774
and labeling faces consistently
remains a challenge.

00:04:56.798 --> 00:04:58.560
You might have seen this on Facebook.

00:04:58.584 --> 00:05:01.572
My friends and I laugh all the time
when we see other people

00:05:01.596 --> 00:05:04.054
mislabeled in our photos.

00:05:04.078 --> 00:05:09.669
But misidentifying a suspected criminal
is no laughing matter,

00:05:09.693 --> 00:05:12.520
nor is breaching civil liberties.

00:05:12.544 --> 00:05:15.749
Machine learning is being used
for facial recognition,

00:05:15.773 --> 00:05:20.278
but it's also extending beyond the realm
of computer vision.

00:05:21.086 --> 00:05:25.102
In her book, "Weapons
of Math Destruction,"

00:05:25.126 --> 00:05:31.807
data scientist Cathy O'Neil
talks about the rising new WMDs --

00:05:31.831 --> 00:05:36.184
widespread, mysterious
and destructive algorithms

00:05:36.208 --> 00:05:39.172
that are increasingly being used
to make decisions

00:05:39.196 --> 00:05:42.373
that impact more aspects of our lives.

00:05:42.397 --> 00:05:44.267
So who gets hired or fired?

00:05:44.291 --> 00:05:46.403
Do you get that loan?
Do you get insurance?

00:05:46.427 --> 00:05:49.930
Are you admitted into the college
you wanted to get into?

00:05:49.954 --> 00:05:53.463
Do you and I pay the same price
for the same product

00:05:53.487 --> 00:05:55.929
purchased on the same platform?

00:05:55.953 --> 00:05:59.712
Law enforcement is also starting
to use machine learning

00:05:59.736 --> 00:06:02.025
for predictive policing.

00:06:02.049 --> 00:06:05.543
Some judges use machine-generated
risk scores to determine

00:06:05.567 --> 00:06:09.969
how long an individual
is going to spend in prison.

00:06:09.993 --> 00:06:12.447
So we really have to think
about these decisions.

00:06:12.471 --> 00:06:13.653
Are they fair?

00:06:13.677 --> 00:06:16.567
And we've seen that algorithmic bias

00:06:16.591 --> 00:06:19.965
doesn't necessarily always
lead to fair outcomes.

00:06:19.989 --> 00:06:21.953
So what can we do about it?

00:06:21.977 --> 00:06:25.657
Well, we can start thinking about
how we create more inclusive code

00:06:25.681 --> 00:06:28.671
and employ inclusive coding practices.

00:06:28.695 --> 00:06:31.004
It really starts with people.

00:06:31.528 --> 00:06:33.489
So who codes matters.

00:06:33.513 --> 00:06:37.632
Are we creating full-spectrum teams
with diverse individuals

00:06:37.656 --> 00:06:40.067
who can check each other's blind spots?

00:06:40.091 --> 00:06:43.636
On the technical side,
how we code matters.

00:06:43.660 --> 00:06:47.311
Are we factoring in fairness
as we're developing systems?

00:06:47.335 --> 00:06:50.248
And finally, why we code matters.

00:06:50.605 --> 00:06:55.688
We've used tools of computational creation
to unlock immense wealth.

00:06:55.712 --> 00:07:00.159
We now have the opportunity
to unlock even greater equality

00:07:00.183 --> 00:07:03.113
if we make social change a priority

00:07:03.137 --> 00:07:05.307
and not an afterthought.

00:07:05.828 --> 00:07:10.350
And so these are the three tenets
that will make up the "incoding" movement.

00:07:10.374 --> 00:07:12.026
Who codes matters,

00:07:12.050 --> 00:07:13.593
how we code matters

00:07:13.617 --> 00:07:15.640
and why we code matters.

00:07:15.664 --> 00:07:18.763
So to go towards incoding,
we can start thinking about

00:07:18.787 --> 00:07:21.951
building platforms that can identify bias

00:07:21.975 --> 00:07:25.053
by collecting people's experiences
like the ones I shared,

00:07:25.077 --> 00:07:28.147
but also auditing existing software.

00:07:28.171 --> 00:07:31.936
We can also start to create
more inclusive training sets.

00:07:31.960 --> 00:07:34.763
Imagine a "Selfies for Inclusion" campaign

00:07:34.787 --> 00:07:38.442
where you and I can help
developers test and create

00:07:38.466 --> 00:07:40.559
more inclusive training sets.

00:07:41.122 --> 00:07:43.950
And we can also start thinking
more conscientiously

00:07:43.974 --> 00:07:49.365
about the social impact
of the technology that we're developing.

00:07:49.389 --> 00:07:51.782
To get the incoding movement started,

00:07:51.806 --> 00:07:54.653
I've launched the Algorithmic
Justice League,

00:07:54.677 --> 00:08:00.549
where anyone who cares about fairness
can help fight the coded gaze.

00:08:00.573 --> 00:08:03.869
On codedgaze.com, you can report bias,

00:08:03.893 --> 00:08:06.338
request audits, become a tester

00:08:06.362 --> 00:08:09.133
and join the ongoing conversation,

00:08:09.157 --> 00:08:11.444
#codedgaze.

00:08:12.562 --> 00:08:15.049
So I invite you to join me

00:08:15.073 --> 00:08:18.792
in creating a world where technology
works for all of us,

00:08:18.816 --> 00:08:20.713
not just some of us,

00:08:20.737 --> 00:08:25.325
a world where we value inclusion
and center social change.

00:08:25.349 --> 00:08:26.524
Thank you.

00:08:26.548 --> 00:08:30.819
(Applause)

00:08:32.693 --> 00:08:35.547
But I have one question:

00:08:35.571 --> 00:08:37.630
Will you join me in the fight?

00:08:37.654 --> 00:08:38.939
(Laughter)

00:08:38.963 --> 00:08:42.650
(Applause)

