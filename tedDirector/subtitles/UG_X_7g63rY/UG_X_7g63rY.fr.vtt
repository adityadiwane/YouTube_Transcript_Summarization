WEBVTT
Kind: captions
Language: fr

00:00:00.000 --> 00:00:07.000
Traducteur: Marie P
Relecteur: eric vautier

00:00:12.861 --> 00:00:15.995
Bonjour, je suis Joy, une poète du code

00:00:16.019 --> 00:00:21.012
en mission pour arrêter 
une force invisible qui prend de l'ampleur

00:00:21.036 --> 00:00:23.892
une force que j'appelle
« le regard codé »,

00:00:23.916 --> 00:00:27.225
mon terme pour le biais algorithmique.

00:00:27.249 --> 00:00:31.549
Le biais algorithmique, comme le biais
cognitif, crée de l'injustice.

00:00:31.573 --> 00:00:37.595
Mais les algorithmes, comme les virus, 
peuvent massivement générer un biais

00:00:37.619 --> 00:00:39.201
et ce, très rapidement.

00:00:39.763 --> 00:00:44.150
Le biais algorithmique peut aussi
créer des sentiments d'exclusion

00:00:44.174 --> 00:00:46.302
et mener à des pratiques discriminatoires.

00:00:46.326 --> 00:00:48.421
Laissez-moi vous montrer
ce que je veux dire.

00:00:48.800 --> 00:00:51.323
(Video) Joy Boulamwini : Salut
webcam. J'ai un visage.

00:00:51.982 --> 00:00:53.846
Est-ce que tu peux le voir?

00:00:53.871 --> 00:00:55.496
Et sans lunettes ?

00:00:55.521 --> 00:00:57.735
Tu peux voir son visage à elle.

00:00:58.057 --> 00:01:00.302
Et le mien ?

00:01:03.710 --> 00:01:07.460
J'ai un masque. Est-ce que tu peux 
voir mon masque ?

00:01:08.294 --> 00:01:10.659
Joy Boulamwini : Ça, comment 
est-ce arrivé ?

00:01:10.683 --> 00:01:13.824
Pourquoi est-ce que je me retrouve
assise devant un ordinateur

00:01:13.848 --> 00:01:15.272
portant un masque blanc

00:01:15.296 --> 00:01:18.946
pour essayer d'être détectée
par une webcam premier prix ?

00:01:18.970 --> 00:01:21.261
Quand je ne me bats pas 
contre le regard codé

00:01:21.285 --> 00:01:22.805
en tant que poète du code,

00:01:22.829 --> 00:01:26.101
Je suis doctorante au Media Lab du MIT

00:01:26.125 --> 00:01:31.042
et j'ai l'opportunité de plancher 
sur plein de projets fantaisistes

00:01:31.066 --> 00:01:33.093
dont le Miroir Aspire

00:01:33.117 --> 00:01:38.251
que j'ai construit pour pouvoir projeter
des masques digitaux sur mon reflet.

00:01:38.275 --> 00:01:40.625
Comme ça le matin, pour
me sentir plus forte,

00:01:40.649 --> 00:01:42.083
je pouvais projeter un lion.

00:01:42.107 --> 00:01:45.603
Si j'avais besoin d'encouragements,
je pouvais choisir une citation.

00:01:45.627 --> 00:01:48.616
J'ai donc utilisé un logiciel 
de reconnaissance faciale

00:01:48.640 --> 00:01:49.991
pour construire le système,

00:01:50.015 --> 00:01:55.118
mais j'ai réalisé que je ne pouvais pas 
le tester à moins de porter un masque.

00:01:56.102 --> 00:02:00.448
Malheureusement, j'avais déjà 
rencontré ce problème.

00:02:00.472 --> 00:02:04.775
Quand j'étais étudiante 
en informatique à Georgia Tech,

00:02:04.799 --> 00:02:06.854
je travaillais sur les robots sociaux

00:02:06.878 --> 00:02:10.655
et l'un de mes devoirs était de programmer
un robot pour qu'il joue à « Caché »,

00:02:10.659 --> 00:02:12.362
un jeu qui se joue à tour de rôle

00:02:12.386 --> 00:02:16.707
dans lequel chacun couvre son visage, 
puis le découvre en disant « Coucou ! »

00:02:16.731 --> 00:02:21.160
Le problème, c'est que ce jeu ne peut 
pas marcher si je ne peux pas vous voir

00:02:21.184 --> 00:02:23.683
et mon robot ne pouvait pas me voir.

00:02:23.707 --> 00:02:27.657
Mais j'ai emprunté le visage de ma 
colocataire pour finir le projet,

00:02:27.681 --> 00:02:29.061
j'ai rendu le devoir,

00:02:29.085 --> 00:02:32.838
et je me suis dit que
quelqu'un d'autre résoudrait le problème.

00:02:33.489 --> 00:02:35.492
Peu de temps après,

00:02:35.516 --> 00:02:39.675
j'étais à Hong Kong 
pour une compétition d'entrepreneuriat.

00:02:40.159 --> 00:02:42.853
Les organisateurs ont décidé 
d'emmener les participants

00:02:42.877 --> 00:02:45.249
faire le tour des start-up locales.

00:02:45.273 --> 00:02:49.958
L'une d'elles avait un robot social, ils
ont décidé de faire une démonstration.

00:02:49.958 --> 00:02:52.928
Ça a marché avec tout le monde
jusqu'à ce que vienne mon tour,

00:02:52.952 --> 00:02:54.875
et vous pouvez sans doute deviner.

00:02:54.899 --> 00:02:57.864
Le robot ne pouvait pas
détecter mon visage.

00:02:57.888 --> 00:03:00.399
J'ai demandé aux développeurs
ce qu'il se passait,

00:03:00.423 --> 00:03:05.956
et en fait nous avions utilisé le même
outil de reconnaissance faciale.

00:03:05.980 --> 00:03:07.630
À l'autre bout du monde,

00:03:07.654 --> 00:03:11.506
j'avais appris que le biais algorithmique 
peut voyager aussi rapidement

00:03:11.530 --> 00:03:14.700
qu'un téléchargement de fichiers.

00:03:15.565 --> 00:03:18.641
Qu'est-ce qui se passe ?
Pourquoi mon visage n'est pas détecté ?

00:03:18.665 --> 00:03:22.021
Pour répondre, il faut comprendre comment
on donne la vue aux machines.

00:03:22.045 --> 00:03:25.454
La vision informatique utilise 
des techniques de machine learning

00:03:25.478 --> 00:03:27.358
pour reconnaître des visages.

00:03:27.382 --> 00:03:31.279
Pour que ça marche, vous créez un ensemble
de formation avec des exemples.

00:03:31.303 --> 00:03:34.121
Ceci est un visage. Ceci est un visage.
Mais pas ça.

00:03:34.145 --> 00:03:38.664
Au fur et à mesure, l'ordinateur apprend
comment reconnaître d'autres visages.

00:03:38.688 --> 00:03:42.677
Mais si les jeux de tests
ne sont pas très variés,

00:03:42.701 --> 00:03:46.050
n'importe quel visage qui dévie trop
de la norme établie

00:03:46.074 --> 00:03:47.723
sera plus compliqué à détecter,

00:03:47.747 --> 00:03:49.710
et c'était ce qui se passait avec moi.

00:03:49.734 --> 00:03:52.116
Mais pas d'angoisse -- 
il y a de bonnes nouvelles.

00:03:52.140 --> 00:03:54.911
Les jeux de tests 
n'apparaissent pas par magie.

00:03:54.935 --> 00:03:56.723
On peut les créer nous-mêmes.

00:03:56.747 --> 00:04:00.923
Il y a la possibilité de créer
des jeux de tests plus variés

00:04:00.947 --> 00:04:04.771
qui offrent un portrait 
plus riche de l'humanité.

00:04:04.795 --> 00:04:07.016
Vous avez vu dans mes exemples

00:04:07.040 --> 00:04:08.808
que c'est via les robots sociaux

00:04:08.832 --> 00:04:13.443
que je me suis rendu compte de l'existence
du biais algorithmique.

00:04:13.467 --> 00:04:18.282
Mais le biais algorithmique peut aussi
mener à des pratiques discriminatoires.

00:04:19.257 --> 00:04:20.710
Aux États-Unis,

00:04:20.734 --> 00:04:24.932
la police commence à utiliser 
des logiciels de reconnaissance faciale

00:04:24.956 --> 00:04:27.415
dans son arsenal contre le crime.

00:04:27.439 --> 00:04:29.452
Georgetown Law a publié un rapport

00:04:29.476 --> 00:04:36.239
montrant qu'un adulte sur deux aux
États-Unis - 117 millions de personnes--

00:04:36.263 --> 00:04:39.797
ont leur visage dans un système de
reconnaissance faciale.

00:04:39.821 --> 00:04:44.373
La police peut en ce moment consulter 
ces systèmes non régulés,

00:04:44.397 --> 00:04:48.683
en utilisant des algorithmes
dont la fiabilité n'a pas été testée.

00:04:48.707 --> 00:04:52.571
Mais on sait que la reconnaissance 
faciale a des failles,

00:04:52.595 --> 00:04:56.774
et que correctement étiqueter 
un visage reste un défi.

00:04:56.798 --> 00:04:58.560
Vous l'avez sûrement vu sur Facebook.

00:04:58.584 --> 00:05:01.572
Avec mes amis, on rit souvent
quand on voit d'autres personnes

00:05:01.596 --> 00:05:04.054
mal identifiées dans nos photos.

00:05:04.078 --> 00:05:09.669
Mais mal identifier un suspect comme étant
un criminel n'est pas drôle,

00:05:09.693 --> 00:05:12.520
et porter atteinte aux libertés civiles
non plus.

00:05:12.544 --> 00:05:15.749
Le machine learning est utilisé 
pour la reconnaissance faciale,

00:05:15.773 --> 00:05:20.278
mais s'utilise dans d'autres
domaines que la vision informatique.

00:05:21.086 --> 00:05:25.102
Dans son livre « Weapons
of Math Destruction »,

00:05:25.126 --> 00:05:31.807
la data scientist Cathy O'Neil 
parle des risques de ces nouvelles armes,

00:05:31.831 --> 00:05:36.184
des algorithmes répandus, 
mystérieux et destructeurs

00:05:36.208 --> 00:05:39.172
qui sont de plus en plus utilisés 
dans des prises de décision

00:05:39.196 --> 00:05:42.373
qui ont un impact sur nos vies.

00:05:42.397 --> 00:05:44.267
Qui est embauché ou renvoyé ?

00:05:44.291 --> 00:05:46.403
Aurez-vous ce prêt ?
Une assurance ?

00:05:46.427 --> 00:05:49.930
Serez-vous admis dans cette université
que vous voulez vraiment ?

00:05:49.954 --> 00:05:53.463
Est-ce que vous et moi payons
le même prix pour le même produit

00:05:53.487 --> 00:05:55.929
acheté sur la même plateforme ?

00:05:55.953 --> 00:05:59.712
Les autorités policières commencent 
à utiliser le machine learning

00:05:59.736 --> 00:06:02.025
dans le cadre de la prévention policière.

00:06:02.049 --> 00:06:05.543
Certains juges utilisent des scores
générés par des machines

00:06:05.567 --> 00:06:09.969
pour déterminer combien de temps
un individu passera derrière les barreaux.

00:06:09.993 --> 00:06:12.447
Nous devons donc réfléchir 
à ces décisions.

00:06:12.471 --> 00:06:13.653
Sont-elles justes ?

00:06:13.677 --> 00:06:16.567
Et nous avons vu que
le biais algorithmique

00:06:16.591 --> 00:06:19.965
ne mène pas forcément
à des décisions justes.

00:06:19.989 --> 00:06:21.953
Que pouvons-nous faire ?

00:06:21.977 --> 00:06:25.657
Nous pouvons commencer à penser
à une manière de coder plus inclusivement

00:06:25.681 --> 00:06:28.671
et à utiliser des pratiques
de code plus inclusives.

00:06:28.695 --> 00:06:31.004
Tout commence avec les gens.

00:06:31.528 --> 00:06:33.489
Qui code a une importance.

00:06:33.513 --> 00:06:37.632
Créons-nous des équipes 
composées d'individus variés

00:06:37.656 --> 00:06:40.067
qui puissent vérifier mutuellement 
leurs travaux ?

00:06:40.091 --> 00:06:43.636
D'un point de vue technique,
comment on code a de l'importance.

00:06:43.660 --> 00:06:47.311
Ajoutons-nous la justice à l'équation 
quand nous développons des systèmes ?

00:06:47.335 --> 00:06:50.248
Finalement, pourquoi
on code a une importance.

00:06:50.605 --> 00:06:55.688
Nous avons utilisé des outils numériques
pour générer d'immenses richesses.

00:06:55.712 --> 00:07:00.159
Nous avons maintenant l'opportunité 
de créer encore plus d'égalité

00:07:00.183 --> 00:07:03.113
si nous faisons du changement 
social une priorité.

00:07:03.137 --> 00:07:05.307
et pas une préoccupation secondaire.

00:07:05.828 --> 00:07:10.350
Ceci seront les trois piliers
du mouvement « incoding ».

00:07:10.374 --> 00:07:12.026
Qui code a de l'importance,

00:07:12.050 --> 00:07:13.593
la manière dont on code aussi

00:07:13.617 --> 00:07:15.640
et pourquoi on code également.

00:07:15.664 --> 00:07:18.763
Pour aller vers l'incoding,
nous pouvons commencer à réfléchir

00:07:18.787 --> 00:07:21.951
à comment construire des outils
pouvant identifier ce biais

00:07:21.975 --> 00:07:25.053
via la collecte de témoignages
comme celui que j'ai partagé,

00:07:25.077 --> 00:07:28.147
mais qui pourraient aussi tester
des logiciels existants.

00:07:28.171 --> 00:07:31.936
Nous pouvons commencer à créer
des jeux de tests plus complets.

00:07:31.960 --> 00:07:34.763
Imaginez une campagne
« Selfies pour l'inclusion »,

00:07:34.787 --> 00:07:38.442
où vous et moi pourrions aider
les développeurs à tester et créer

00:07:38.466 --> 00:07:40.559
ces jeux de tests plus variés.

00:07:41.122 --> 00:07:43.950
Nous pouvons commencer à penser
plus consciencieusement

00:07:43.974 --> 00:07:49.365
à l'impact social
des technologies que nous développons.

00:07:49.389 --> 00:07:51.782
Pour commencer le mouvement incoding,

00:07:51.806 --> 00:07:54.653
J'ai lancé l'Algorithmic Justice League,

00:07:54.677 --> 00:08:00.549
où n'importe qui se souciant du problème
peut aider à combattre le regard codé.

00:08:00.573 --> 00:08:03.869
Sur codedgaze.com, vous pouvez
dénoncer des biais,

00:08:03.893 --> 00:08:06.338
demander des tests, être testeur vous-même

00:08:06.362 --> 00:08:09.133
et rejoindre la conversation,

00:08:09.157 --> 00:08:11.444
#codedgaze.

00:08:12.562 --> 00:08:15.049
Donc je vous invite à me rejoindre

00:08:15.073 --> 00:08:18.792
pour créer un monde où la technologie 
marche pour nous tous,

00:08:18.816 --> 00:08:20.713
pas seulement pour certains,

00:08:20.737 --> 00:08:25.325
un monde où l'inclusion 
et le changement social ont de la valeur.

00:08:25.349 --> 00:08:26.524
Merci.

00:08:26.548 --> 00:08:30.819
(Applaudissements)

00:08:32.693 --> 00:08:35.547
Mais j'ai une question :

00:08:35.571 --> 00:08:37.630
Me rejoindrez-vous dans ce combat?

00:08:37.654 --> 00:08:38.939
(Rires)

00:08:38.963 --> 00:08:42.650
(Applaudissements)

