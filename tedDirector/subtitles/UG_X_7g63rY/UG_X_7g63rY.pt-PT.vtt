WEBVTT
Kind: captions
Language: pt-PT

00:00:00.000 --> 00:00:07.000
Tradutor: Adília Correia
Revisora: Margarida Ferreira

00:00:12.861 --> 00:00:15.995
Olá, sou a Joy, uma poetisa de código,

00:00:16.100 --> 00:00:21.012
numa missão de fazer parar
uma força invisível em ascensão,

00:00:21.099 --> 00:00:23.892
uma força a que eu chamo
"olhar codificado,"

00:00:23.961 --> 00:00:27.225
o meu termo para preconceito algorítmico.

00:00:27.367 --> 00:00:31.549
O preconceito algorítmico, como o
preconceito humano, resulta da injustiça.

00:00:31.791 --> 00:00:34.573
Porém, os algoritmos, tal como os vírus,

00:00:34.619 --> 00:00:37.655
podem espalhar preconceitos
numa grande escala

00:00:37.728 --> 00:00:39.464
num ritmo rápido.

00:00:39.763 --> 00:00:44.150
O preconceito em algoritmos também
pode levar a experiências de exclusão

00:00:44.174 --> 00:00:46.302
e a práticas discriminatórias.

00:00:46.507 --> 00:00:48.568
Vou mostrar o que quero dizer.

00:00:48.800 --> 00:00:51.299
(Vídeo) Olá, câmara, eu tenho um rosto.

00:00:51.982 --> 00:00:53.700
Podes ver o meu rosto?

00:00:53.871 --> 00:00:55.705
Um rosto sem óculos?

00:00:55.775 --> 00:00:57.735
Podes ver o rosto dela.

00:00:58.057 --> 00:00:59.965
E o meu rosto?

00:01:03.710 --> 00:01:07.114
Tenho uma máscara.
Vês a minha máscara?

00:01:08.294 --> 00:01:10.659
Joy: Então, como é que isso aconteceu?

00:01:10.683 --> 00:01:13.824
Porque é que eu estou em frente
de um computador

00:01:13.848 --> 00:01:15.472
com uma máscara branca,

00:01:15.514 --> 00:01:18.946
a tentar ser detetada por
uma câmara de vídeo barata?

00:01:19.179 --> 00:01:21.551
Quando não estou a lutar
contra o olhar codificado

00:01:21.575 --> 00:01:23.114
como uma poetisa de código,

00:01:23.138 --> 00:01:26.355
sou uma estudante de pós-graduação
no laboratório de "media" do MIT.

00:01:26.406 --> 00:01:31.042
Aí tenho a oportunidade de trabalhar
em todo tipo de projetos bizarros,

00:01:31.066 --> 00:01:33.247
incluindo o Espelho de Desejar,

00:01:33.280 --> 00:01:38.251
um projeto que fiz para poder projetar
máscaras digitais para o meu reflexo.

00:01:38.275 --> 00:01:40.625
Então, pela manhã,
se quisesse sentir-me poderosa,

00:01:40.649 --> 00:01:42.164
eu podia usar um leão.

00:01:42.216 --> 00:01:45.603
Se quisesse ficar inspirada,
podia ter uma citação.

00:01:45.781 --> 00:01:48.616
Então eu usei o software
genérico de reconhecimento facial

00:01:48.640 --> 00:01:50.209
para construir o sistema,

00:01:50.278 --> 00:01:55.118
mas descobri que era difícil testá-lo
a menos que usasse uma máscara branca.

00:01:56.102 --> 00:02:00.202
Infelizmente, eu já tinha esbarrado
nesse problema.

00:02:00.472 --> 00:02:04.775
Quando era universitária em Georgia Tech
e estudava ciência informática,

00:02:04.799 --> 00:02:06.899
eu costumava trabalhar em robôs sociais,

00:02:06.959 --> 00:02:10.655
e uma das minhas tarefas era fazer
com que um robô jogasse às escondidas,

00:02:10.679 --> 00:02:12.562
um simples jogo de turnos

00:02:12.595 --> 00:02:16.707
em que os parceiros escondem a cara
e depois destapam-na, dizendo "Espreita!"

00:02:16.767 --> 00:02:21.160
O problema é que isso só
funciona se eu puder ver o outro,

00:02:21.284 --> 00:02:23.573
e o meu robô não me via.

00:02:23.707 --> 00:02:27.657
Pedi emprestada a cara da minha
colega de quarto para terminar o projeto,

00:02:27.681 --> 00:02:29.533
apresentei a tarefa e pensei:

00:02:29.594 --> 00:02:33.028
"Sabem que mais,
outra pessoa que resolva esse problema".

00:02:33.489 --> 00:02:35.492
Pouco tempo depois,

00:02:35.516 --> 00:02:39.747
eu estava em Hong Kong para uma
competição de empreendedorismo.

00:02:40.159 --> 00:02:42.853
Os organizadores decidiram
levar os participantes

00:02:42.877 --> 00:02:45.249
numa visita às "start-ups" locais.

00:02:45.273 --> 00:02:47.988
Uma das "start-ups" tinha um robô social,

00:02:48.012 --> 00:02:50.105
e decidiram fazer uma demonstração.

00:02:50.148 --> 00:02:53.264
A demonstração funcionou com toda a gente
até chegar a minha vez.

00:02:53.315 --> 00:02:55.129
Provavelmente já adivinham.

00:02:55.171 --> 00:02:57.800
Não conseguiu detetar o meu rosto.

00:02:57.888 --> 00:03:00.580
Perguntei aos responsáveis
o que é que se passava,

00:03:00.613 --> 00:03:06.056
e acontece que tínhamos usado o mesmo
software genérico de reconhecimento facial.

00:03:06.134 --> 00:03:07.866
Do outro lado do mundo,

00:03:07.917 --> 00:03:11.651
aprendi que o preconceito do algoritmo
pode viajar tão depressa

00:03:11.702 --> 00:03:15.081
quanto uma descarga
de ficheiros da Internet.

00:03:15.565 --> 00:03:18.904
Então, o que é que se passa?
Porque é que a minha cara não é detetada?

00:03:18.946 --> 00:03:22.075
Temos de olhar para o modo
como damos visão às máquinas.

00:03:22.145 --> 00:03:25.454
A visão informática usa
técnicas de aprendizagem de máquina

00:03:25.478 --> 00:03:27.530
para fazer o reconhecimento facial.

00:03:27.591 --> 00:03:31.388
Funciona assim: criamos um grupo
de formação com exemplos de rostos.

00:03:31.457 --> 00:03:34.157
Isto é um rosto. Isto é um rosto.
Isto não é um rosto.

00:03:34.235 --> 00:03:38.664
Com o tempo, podemos ensinar
o computador a reconhecer rostos.

00:03:38.688 --> 00:03:42.677
Contudo, se os grupos de formação
não forem diversificados,

00:03:42.701 --> 00:03:46.050
qualquer rosto que se desvie
demasiado da norma estabelecida

00:03:46.074 --> 00:03:47.723
será difícil de detetar.

00:03:47.919 --> 00:03:49.828
Era o que estava a acontecer comigo.

00:03:49.888 --> 00:03:52.116
Mas não se preocupem,
há boas notícias.

00:03:52.140 --> 00:03:54.911
Os grupos de formação
não se materializam do nada.

00:03:54.935 --> 00:03:56.923
Na verdade, podemos criá-los.

00:03:56.983 --> 00:04:01.123
Portanto, há a oportunidade de criar
grupos de formação com um espetro completo

00:04:01.174 --> 00:04:04.771
que reflitam um retrato
mais rico da humanidade.

00:04:04.795 --> 00:04:07.016
Vocês viram nos meus exemplos

00:04:07.040 --> 00:04:08.808
com os robôs sociais

00:04:08.832 --> 00:04:13.443
que foi como eu descobri a exclusão
com o preconceito algorítmico.

00:04:13.585 --> 00:04:18.182
Mas o preconceito algorítmico também
pode levar a práticas discriminatórias.

00:04:19.257 --> 00:04:20.882
Nos Estados Unidos da América,

00:04:20.943 --> 00:04:24.932
os departamentos da polícia começam
a usar o software de reconhecimento facial

00:04:24.956 --> 00:04:27.415
no seu arsenal de luta contra o crime.

00:04:27.439 --> 00:04:30.215
A Faculdade de Direito de Georgetown
publicou um relatório

00:04:30.257 --> 00:04:33.263
mostrando que um
em dois adultos, nos EUA

00:04:33.299 --> 00:04:36.263
— ou seja, 117 milhões de pessoas —

00:04:36.326 --> 00:04:39.797
têm os rostos em redes
de reconhecimento facial.

00:04:39.884 --> 00:04:44.454
Os departamentos da polícia podem procurar
nessas redes não regulamentadas,

00:04:44.487 --> 00:04:48.683
usando algoritmos que não foram
auditados quanto ao seu rigor.

00:04:48.861 --> 00:04:52.689
No entanto, sabemos que o reconhecimento
facial não é à prova de falhas,

00:04:52.749 --> 00:04:56.919
e rotular rostos consistentemente
continua a ser um problema.

00:04:57.034 --> 00:04:58.678
Podem ter visto isto no Facebook.

00:04:58.738 --> 00:05:01.962
Os meus amigos e eu estamos sempre a rir
quando vemos outras pessoas

00:05:01.986 --> 00:05:04.208
mal rotuladas nas nossas fotos.

00:05:04.332 --> 00:05:09.669
Mas a má identificação de um possível
criminoso não é motivo para rir,

00:05:09.693 --> 00:05:12.774
e o mesmo acontece com
a violação das liberdades civis.

00:05:12.834 --> 00:05:16.176
A aprendizagem de máquinas está a ser
usada para reconhecimento facial,

00:05:16.236 --> 00:05:20.378
mas está a estender-se para além
do domínio da visão por computador.

00:05:21.086 --> 00:05:25.183
No seu livro, "Armas de
Destruição Matemática" (ADM),

00:05:25.244 --> 00:05:31.588
a cientista de dados Cathy O'Neil
fala sobre o aumento de novas ADM

00:05:31.831 --> 00:05:36.256
algoritmos difundidos,
misteriosos e destrutivos

00:05:36.335 --> 00:05:39.353
que estão a ser cada vez mais usados
para tomar decisões

00:05:39.396 --> 00:05:42.373
que afetam mais aspetos da nossa vida.

00:05:42.397 --> 00:05:44.503
Por exemplo, quem é contratado
ou despedido?

00:05:44.545 --> 00:05:46.703
Recebemos esse empréstimo?
Recebemos o seguro?

00:05:46.754 --> 00:05:50.093
Somos admitidos na faculdade
em que queremos entrar?

00:05:50.235 --> 00:05:53.463
Pagamos todos o mesmo preço
para o mesmo produto

00:05:53.487 --> 00:05:55.856
comprado na mesma plataforma?

00:05:55.953 --> 00:05:59.712
A polícia também está a começar
a usar a aprendizagem de máquinas

00:05:59.736 --> 00:06:01.970
para policiamento preditivo.

00:06:02.049 --> 00:06:05.706
Alguns juízes usam a avaliação de risco
gerada por máquinas para determinar

00:06:05.748 --> 00:06:09.969
quanto tempo um indivíduo
vai passar na prisão.

00:06:10.047 --> 00:06:12.565
Portanto, temos mesmo que pensar
nessas decisões.

00:06:12.616 --> 00:06:14.053
Elas sãos justas?

00:06:14.113 --> 00:06:16.785
Já vimos que o preconceito algorítmico

00:06:16.809 --> 00:06:19.965
nem sempre conduz
a resultados justos.

00:06:20.070 --> 00:06:22.034
Então, o que podemos fazer quanto a isso?

00:06:22.186 --> 00:06:25.657
Podemos começar a pensar 
em criar códigos mais inclusivos

00:06:25.717 --> 00:06:28.671
e usar práticas de codificação inclusiva.

00:06:28.813 --> 00:06:31.222
Isto começa com as pessoas.

00:06:31.528 --> 00:06:33.598
Por isso, é importante quem codifica.

00:06:33.640 --> 00:06:37.632
Estaremos a criar equipas de espetro
completo com diversos indivíduos

00:06:37.656 --> 00:06:40.330
que podem verificar os pontos cegos
uns dos outros?

00:06:40.391 --> 00:06:43.636
No lado técnico,
é importante a forma como codificamos.

00:06:43.750 --> 00:06:47.311
Estaremos a considerar a equidade
enquanto desenvolvemos os sistemas?

00:06:47.435 --> 00:06:50.548
E, finalmente, é importante a razão
por que codificamos.

00:06:50.605 --> 00:06:53.202
Temos usado ferramentas
de criação informática

00:06:53.229 --> 00:06:55.839
para desbloquear uma riqueza imensa.

00:06:55.866 --> 00:07:00.213
Agora temos a oportunidade
de desbloquear uma igualdade ainda maior

00:07:00.273 --> 00:07:03.322
se dermos prioridade à mudança social

00:07:03.346 --> 00:07:05.734
e não uma reflexão tardia.

00:07:05.828 --> 00:07:10.350
Portanto, estes são os três princípios
que formam o movimento "de codificação".

00:07:10.374 --> 00:07:12.126
É importante quem codifica,

00:07:12.177 --> 00:07:13.865
é importante como codificamos,

00:07:13.917 --> 00:07:16.040
e é importante a razão
por que codificamos.

00:07:16.082 --> 00:07:18.908
Para avançarmos para a codificação,
podemos começar a pensar

00:07:18.959 --> 00:07:22.041
em construir plataformas que
possam identificar preconceitos

00:07:22.102 --> 00:07:25.289
reunindo as experiências de pessoas
como as que eu contei,

00:07:25.367 --> 00:07:28.147
e também auditando
os softwares existentes.

00:07:28.243 --> 00:07:31.936
Também podemos começar a criar
grupos de formação mais inclusivos.

00:07:32.078 --> 00:07:34.926
Imaginem uma campanha
"Selfies para Inclusão"

00:07:34.977 --> 00:07:38.560
em que qualquer um pode ajudar
os desenvolvedores a criar e testar

00:07:38.620 --> 00:07:40.931
grupos de formação mais inclusivos.

00:07:41.122 --> 00:07:44.050
Também podemos começar
a pensar com maior consciência

00:07:44.083 --> 00:07:49.046
no impacto social da tecnologia
que estamos a desenvolver.

00:07:49.389 --> 00:07:51.782
Para iniciar o movimento de codificação,

00:07:51.806 --> 00:07:54.816
lancei o Algoritmo Liga da Justiça,

00:07:54.867 --> 00:07:57.736
em que todos os que
se interessam pela justiça

00:07:57.782 --> 00:08:00.573
podem ajudar a lutar
contra o olhar codificado.

00:08:00.736 --> 00:08:04.023
Em codedgaze.com,
podem relatar preconceitos,

00:08:04.102 --> 00:08:06.501
exigir auditos, fazerem testes

00:08:06.562 --> 00:08:09.323
e participar das conversas em curso,

00:08:09.429 --> 00:08:11.716
#codedgaze.

00:08:12.562 --> 00:08:15.167
Portanto, convido-os a juntarem-se a mim

00:08:15.227 --> 00:08:18.919
para criar um mundo em que a tecnologia
funcione para todos nós,

00:08:18.970 --> 00:08:20.940
não apenas para alguns,

00:08:20.982 --> 00:08:25.325
um mundo em que valorizamos a inclusão
e nos centramos na mudança social.

00:08:25.467 --> 00:08:26.524
Obrigada.

00:08:26.693 --> 00:08:29.691
(Aplausos).

00:08:32.693 --> 00:08:35.156
Mas eu tenho uma pergunta:

00:08:35.571 --> 00:08:37.757
Vão juntar-se a mim nesta luta?

00:08:37.826 --> 00:08:38.939
(Risos)

00:08:39.035 --> 00:08:41.813
(Aplausos)

