WEBVTT
Kind: captions
Language: ko

00:00:00.000 --> 00:00:07.000
번역: HASTSanggyu Lee
검토: Gichung Lee

00:00:12.861 --> 00:00:15.995
안녕하세요.
코드의 시인, 조이입니다.

00:00:16.019 --> 00:00:18.596
저는 보이지 않는 힘이 일어나는 것을

00:00:18.596 --> 00:00:21.036
막기 위한 사명을 띠고 있어요.

00:00:21.036 --> 00:00:23.892
제가 '코드화된 시선'이라
부르는 힘인데요.

00:00:23.916 --> 00:00:27.225
다른 사람은
알고리즘의 편견이라 부르죠.

00:00:27.249 --> 00:00:28.673
알고리즘의 편견은 인간의 편견처럼
불평등을 초래하지만

00:00:31.573 --> 00:00:34.369
알고리즘은 바이러스처럼 대규모의
편견을 빠른 속도로 퍼뜨릴 수 있어요.

00:00:39.763 --> 00:00:42.844
또한, 알고리즘의 편견은 
자신이 배제되는 경험이나

00:00:42.844 --> 00:00:46.302
차별적인 대우로 이어질 수 있어요.

00:00:46.326 --> 00:00:48.387
자세히 설명해 드리죠.

00:00:48.800 --> 00:00:51.976
(비디오) 안녕, 카메라야. 
이게 내 얼굴이야.

00:00:51.976 --> 00:00:53.846
내 얼굴이 보이니?

00:00:53.871 --> 00:00:55.496
안경을 벗으면 보이니?

00:00:55.521 --> 00:00:57.735
이 친구의 얼굴은 보이잖아.

00:00:58.057 --> 00:01:00.302
내 얼굴은 보여?

00:01:03.710 --> 00:01:07.460
그럼 가면을 쓸게. 
내 가면이 보이니?

00:01:08.294 --> 00:01:11.089
이런 일이 왜 일어난 걸까요?

00:01:11.089 --> 00:01:13.824
제가 왜 컴퓨터 앞에 앉아서

00:01:13.848 --> 00:01:15.272
하얀 가면을 쓰고

00:01:15.296 --> 00:01:18.946
싸구려 웹캠에 인식이 되도록
노력하고 있을까요?

00:01:18.970 --> 00:01:20.855
제가 코드의 시인으로서

00:01:20.855 --> 00:01:22.925
'코드화된 시선'과 싸우기 전에

00:01:22.925 --> 00:01:26.101
저는 MIT 미디어랩의 
대학원생이었어요.

00:01:26.125 --> 00:01:31.042
그곳에서 많은 기발한 프로젝트에
참여할 수 있었는데

00:01:31.066 --> 00:01:33.093
염원의 거울도 있었습니다.

00:01:33.117 --> 00:01:38.251
제가 참여한 프로젝트로, 거울에 비친 제 
모습에 디지털 가면을 씌우는 것이죠.

00:01:38.275 --> 00:01:40.625
아침에 힘찬 느낌을 원하면

00:01:40.649 --> 00:01:42.223
사자 가면을 씌울 수 있고

00:01:42.223 --> 00:01:45.603
희망찬 느낌을 받고 싶다면
명언을 띄울 수 있었죠.

00:01:45.627 --> 00:01:48.616
저는 일반적인 얼굴 인식 
소프트웨어를 사용하여

00:01:48.640 --> 00:01:49.991
시스템을 만들었지만

00:01:50.015 --> 00:01:52.582
제가 흰 가면을 쓰지 않으면
굉장히 테스트하기 어려웠어요.

00:01:56.102 --> 00:01:57.762
불행하게도, 저는 전에도
이런 문제에 부딪힌 적이 있어요.

00:02:00.472 --> 00:02:01.799
제가 조지아 공대에서
컴퓨터 공학 전공생이었을 때

00:02:04.799 --> 00:02:06.854
저는 사회적 로봇을 연구했어요.

00:02:06.878 --> 00:02:08.348
과제들 중 하나는 까꿍놀이하는 
로봇을 만들기였죠.

00:02:10.679 --> 00:02:12.362
간단한 순서 교대 게임으로,

00:02:12.386 --> 00:02:14.031
얼굴을 가렸다가 보이며
"까꿍!"이라고 말하는 게임이죠.

00:02:16.731 --> 00:02:21.160
문제는, 까꿍 놀이는 제가 여러분의
얼굴을 볼 수 있어야 하는데

00:02:21.184 --> 00:02:23.683
로봇이 저를 보지 못했어요.

00:02:23.707 --> 00:02:27.657
하지만 저는 룸메이트의 얼굴을 
빌려서 프로젝트를 끝냈고

00:02:27.681 --> 00:02:29.061
과제를 제출한 다음

00:02:29.085 --> 00:02:32.838
다른 누군가가 이 문제를 해결하겠지
라고 생각했어요.

00:02:33.489 --> 00:02:35.492
그로부터 오래 지나지 않아

00:02:35.516 --> 00:02:39.675
창업 대회 참가를 위해 홍콩에 갔어요.

00:02:40.159 --> 00:02:42.117
주최 측은 참여자들이

00:02:42.117 --> 00:02:45.249
그 지역의 스타트업 기업들을
방문하도록 했어요.

00:02:45.273 --> 00:02:47.988
한 스타트업에 사회적 로봇이 있었고

00:02:48.012 --> 00:02:49.924
시범을 보여주기로 했어요.

00:02:49.948 --> 00:02:52.928
로봇은 모두에게 잘 작동했죠.
저만 빼고요.

00:02:52.952 --> 00:02:54.875
아마 짐작하셨을 거예요.

00:02:54.899 --> 00:02:57.864
제 얼굴을 인식하지 못했어요.

00:02:57.888 --> 00:03:00.399
저는 개발자들에게
무슨 일이냐고 물었고

00:03:00.423 --> 00:03:05.956
제가 썼던 그 얼굴 인식
소프트웨어를 쓴 게 문제였어요.

00:03:05.980 --> 00:03:07.630
지구 반대편에서

00:03:07.654 --> 00:03:11.506
저는 알고리즘의 편견이 인터넷에서 
파일을 다운로드받는 것처럼

00:03:11.530 --> 00:03:14.700
빠르게 퍼질 수
있다는 걸 알았어요.

00:03:15.565 --> 00:03:16.865
어떻게 된 걸까요?
왜 제 얼굴은 인식되지 않죠?

00:03:18.665 --> 00:03:22.021
자, 우리가 어떻게 기계가 볼 수 
있게 하는지 알아보세요.

00:03:22.045 --> 00:03:25.454
컴퓨터의 시야는
머신 러닝 기술을 사용해

00:03:25.478 --> 00:03:27.358
얼굴을 인식해요.

00:03:27.382 --> 00:03:31.279
우리는 여러 예시 얼굴들로 이루어진
연습 세트를 만들어 놓죠.

00:03:31.303 --> 00:03:34.121
이건 얼굴이다. 이건 얼굴이다.
이건 얼굴이 아니다.

00:03:34.145 --> 00:03:38.664
그리고 시간이 지나면, 컴퓨터에게
얼굴 인식을 가르칠 수 있어요.

00:03:38.688 --> 00:03:42.677
하지만, 연습 세트가
그렇게 다양하지 않다면

00:03:42.701 --> 00:03:46.050
규정된 표준에서 너무 벗어나는 얼굴들은

00:03:46.074 --> 00:03:47.723
인식하기 어려울 거예요.

00:03:47.747 --> 00:03:49.710
저한테 일어났던 일과 같죠.

00:03:49.734 --> 00:03:52.116
하지만 걱정하지 마세요.
좋은 소식도 있어요.

00:03:52.140 --> 00:03:54.911
연습 세트는 하늘에서
뚝 떨어지지 않아요.

00:03:54.935 --> 00:03:56.723
우리가 직접 만들 수 있죠.

00:03:56.747 --> 00:04:00.923
따라서 전 영역을 아울러
다양한 인류의 얼굴을 반영하는

00:04:00.947 --> 00:04:04.771
연습 세트를 만들 기회가 있어요.

00:04:04.795 --> 00:04:07.016
여러분은 방금

00:04:07.040 --> 00:04:09.378
사회적 로봇이 어떤지

00:04:09.378 --> 00:04:11.388
제가 어떻게 알고리즘의 편견에 의한 
배제에 대해 알게되었는지 보셨어요.

00:04:13.467 --> 00:04:16.137
하지만 알고리즘의 편견은 차별적 
관행으로 이어질 수도 있습니다.

00:04:19.257 --> 00:04:20.710
미국 전역에서

00:04:20.734 --> 00:04:24.932
경찰서들이 범죄 근절의 무기로

00:04:24.956 --> 00:04:27.415
얼굴 인식 소프트웨어를
사용하기 시작했어요.

00:04:27.439 --> 00:04:29.452
조지타운대 법학센터에 따르면

00:04:29.476 --> 00:04:33.263
총 1억1천7백만명에 달하는 
미국 성인 둘 중 한 명의 얼굴이

00:04:36.263 --> 00:04:39.797
얼굴 인식 네트워크에
올려져 있어요.

00:04:39.821 --> 00:04:42.227
경찰은 현재 이 네트워크를
제한 없이 살펴볼 수 있어요.

00:04:44.397 --> 00:04:48.683
정확성이 검증되지 않은 
알고리즘을 사용하면서요.

00:04:48.707 --> 00:04:52.571
우리는 얼굴 인식이 잘못될 수 
있다는 것을 알고 있고

00:04:52.595 --> 00:04:56.774
얼굴을 일관되게 표시하는 것은
과제로 남아있어요.

00:04:56.798 --> 00:04:58.560
아마 페이스북에서 보셨을 거예요.

00:04:58.584 --> 00:05:01.572
저와 제 친구들은 다른 사람의 이름이

00:05:01.596 --> 00:05:04.054
우리 사진에 표시된 것을
보고 매번 웃어요.

00:05:04.078 --> 00:05:07.703
하지만 범죄 용의자를 
잘못 파악하는 것은

00:05:07.703 --> 00:05:12.520
웃을 일이 아니며 
시민의 자유를 침해하죠.

00:05:12.544 --> 00:05:15.749
머신러닝은 현재 
얼굴인식에 사용되지만

00:05:15.773 --> 00:05:20.278
컴퓨터 시각을 넘어선 곳까지
확장되고 있어요.

00:05:21.086 --> 00:05:25.102
'대량살상무기 (WMD)'라는 책에서

00:05:25.126 --> 00:05:28.831
데이터 과학자 캐시 오닐은
새로운 대량살상무기에 대해서 말해요.

00:05:31.831 --> 00:05:36.184
널리 퍼진, 알 수 없는
파괴적인 알고리즘이죠.

00:05:36.208 --> 00:05:39.172
이들은 우리 삶에
큰 영향을 미치는 선택에

00:05:39.196 --> 00:05:41.933
점점 많이 사용되고 있어요.

00:05:42.397 --> 00:05:44.267
누가 고용되고 누가 해고되는가?

00:05:44.291 --> 00:05:46.403
빚을 질까?
보험에 가입할까?

00:05:46.427 --> 00:05:49.930
원하는 대학에 합격하는가?

00:05:49.954 --> 00:05:53.463
여러분과 당신이 같은 상품에 대해서

00:05:53.487 --> 00:05:55.929
같은 가격을 지불하는가?

00:05:55.953 --> 00:05:59.712
법 집행에서도 예방적 치안을 위해

00:05:59.736 --> 00:06:02.025
머신 러닝 사용을 시작했어요.

00:06:02.049 --> 00:06:05.543
몇몇 판사들은 기계가 만든
위험 점수를 사용하여

00:06:05.567 --> 00:06:09.703
사람들의 형량을 결정하기도 해요.

00:06:09.703 --> 00:06:12.447
그래서 우린 이런 선택에
대해 생각해 봐야 해요.

00:06:12.471 --> 00:06:13.653
이 선택이 공정한가?

00:06:13.677 --> 00:06:16.567
게다가 우리는 알고리즘의 선택이

00:06:16.591 --> 00:06:19.965
매번 공정하지는 않다는 걸 봤어요.

00:06:19.989 --> 00:06:21.953
그럼 어떻게 해야 할까요?

00:06:21.977 --> 00:06:25.657
우리는 포괄적인 코드를 만들고

00:06:25.681 --> 00:06:28.671
포괄적인 코딩 선례를 도입해야 해요.

00:06:28.695 --> 00:06:31.004
이것은 사람들로부터 시작됩니다.

00:06:31.008 --> 00:06:33.489
따라서, 누가 코딩을 
하는지가 중요하죠.

00:06:33.513 --> 00:06:36.576
우리는 지금 다양한 개인들로 이루어져

00:06:36.576 --> 00:06:40.067
서로의 맹점을 볼 수 있는
팀을 만들고 있나요?

00:06:40.091 --> 00:06:43.636
기술적인 면에서 우리가 어떻게
코딩을 하는지가 중요해요.

00:06:43.660 --> 00:06:47.311
지금 우리는 시스템을 개발하면서
공정함을 염두에 두고 있나요?

00:06:47.335 --> 00:06:50.248
마지막으로, 우리가 왜 코딩을 
하는지가 중요해요.

00:06:50.605 --> 00:06:55.688
우리는 엄청난 부를 위하여
컴퓨터를 도구로 사용했어요.

00:06:55.712 --> 00:07:00.159
이제 우리에겐 더 큰 평등을
얻을 기회가 있어요.

00:07:00.183 --> 00:07:03.113
우리가 사회적 변화를 미루지 않고

00:07:03.137 --> 00:07:05.307
우선순위에 둔다면요.

00:07:05.828 --> 00:07:10.350
이 세 가지 요소가
'인코딩' 운동을 구성합니다.

00:07:10.374 --> 00:07:12.026
누가 코딩을 하는지

00:07:12.050 --> 00:07:13.593
어떻게 코딩을 하는지

00:07:13.617 --> 00:07:15.640
왜 코딩을 하는지가 중요해요.

00:07:15.664 --> 00:07:17.184
그리고 인코딩을 향해 가며
우리는 편견을 분별하는 플랫폼을

00:07:19.527 --> 00:07:21.951
구축할 수 있어요.

00:07:21.975 --> 00:07:25.053
제가 공유한 것과 같은
다른 사람들의 경험을 모으고

00:07:25.077 --> 00:07:28.147
현존하는 소프트웨어를
검사하면서 말이죠.

00:07:28.171 --> 00:07:30.251
우리는 또한 더욱 포용적인 연습 세트를
만들기 시작할 수 있어요.

00:07:31.960 --> 00:07:34.763
"포괄적인 셀카" 캠페인을
상상해 보세요.

00:07:34.787 --> 00:07:38.442
여러분과 제가 더욱 포용적인
연습 세트를 만드는 데

00:07:38.466 --> 00:07:40.919
셀카를 보내면서
도움을 줄 수 있는 거예요.

00:07:41.122 --> 00:07:44.820
그리고 우리가 개발하는 기술의
사회적 영향에 대해

00:07:44.820 --> 00:07:48.705
보다 양심적으로 생각할 수 있어요.

00:07:49.389 --> 00:07:51.782
인코딩 운동을 시작하기 위해서

00:07:51.806 --> 00:07:54.653
저는 알고리즘 정의 연합을 창설했어요.

00:07:54.677 --> 00:07:57.573
공정함을 중요시 여기는 사람 누구든
'코딩된 시선'에 맞서 싸우는 걸 도와줍니다.

00:08:00.463 --> 00:08:01.783
codedgaze.com에서
편견을 보고하거나

00:08:03.273 --> 00:08:04.853
검사를 요청하거나
테스터가 될 수 있으며

00:08:06.362 --> 00:08:09.133
진행되는 대화에 참여할 수도 있어요.

00:08:09.157 --> 00:08:11.444
해시태그 codedgaze입니다.

00:08:12.562 --> 00:08:15.049
그래서 저는 여러분이 저와 함께

00:08:15.073 --> 00:08:17.816
기술이 일부만이 아닌

00:08:17.816 --> 00:08:20.713
모두를 위해 쓰이는 세상을

00:08:20.737 --> 00:08:23.259
포용성을 중요시여기고 사회적 변화를
중시하는 세상을 만드는데 동참하셨으면 합니다.

00:08:25.349 --> 00:08:26.524
감사합니다.

00:08:26.548 --> 00:08:30.819
(박수)

00:08:32.693 --> 00:08:35.547
하지만 여러분에게 질문이 하나 있어요.

00:08:35.571 --> 00:08:37.630
여러분은 이 싸움에 동참하실 건가요?

00:08:37.654 --> 00:08:38.939
(웃음)

00:08:38.963 --> 00:08:42.650
(박수)

