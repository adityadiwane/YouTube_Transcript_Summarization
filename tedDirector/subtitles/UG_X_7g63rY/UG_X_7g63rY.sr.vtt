WEBVTT
Kind: captions
Language: sr

00:00:00.000 --> 00:00:07.000
Prevodilac: Ivana Krivokuća
Lektor: Tijana Mihajlović

00:00:12.861 --> 00:00:15.895
Zdravo, ja sam Džoj, pesnikinja kodova,

00:00:15.929 --> 00:00:21.012
na misiji da zaustavim
neviđenu silu u usponu,

00:00:21.036 --> 00:00:23.892
silu koju nazivam „kodirani pogled“,

00:00:23.916 --> 00:00:27.105
što je moj termin
za algoritamsku pristrasnost.

00:00:27.149 --> 00:00:31.549
Algoritamska pristrasnost, kao i ljudska,
ima za posledicu nepravednost.

00:00:31.573 --> 00:00:37.595
Međutim, algoritmi, poput virusa,
mogu raširiti pristrasnost u ogromnoj meri

00:00:37.619 --> 00:00:39.201
velikom brzinom.

00:00:39.763 --> 00:00:44.150
Algoritamska pristrasnost može dovesti
i do izloženosti isključivanju

00:00:44.174 --> 00:00:46.302
i prakse diskriminacije.

00:00:46.326 --> 00:00:48.387
Dozvolite da vam pokažem
šta hoću da kažem.

00:00:48.800 --> 00:00:51.236
(Video) Džoj Buolamvini:
Zdravo, kamero. Imam lice.

00:00:51.982 --> 00:00:53.846
Možeš li da vidiš moje lice?

00:00:53.871 --> 00:00:55.496
Lice bez naočara?

00:00:55.521 --> 00:00:57.305
Možeš videti njeno lice.

00:00:57.967 --> 00:00:59.792
A moje?

00:01:03.710 --> 00:01:07.460
Imam masku. Možeš li da vidiš moju masku?

00:01:08.294 --> 00:01:10.659
Džoj Buolamvini: Pa, kako se ovo dogodilo?

00:01:10.683 --> 00:01:13.824
Zašto sedim ispred kompjutera

00:01:13.848 --> 00:01:15.272
sa belom maskom,

00:01:15.296 --> 00:01:18.946
pokušavajući da me prepozna
jeftina kamera?

00:01:18.970 --> 00:01:21.261
Kada se ne borim protiv kodiranog pogleda

00:01:21.285 --> 00:01:22.805
kao pesnikinja kodova,

00:01:22.829 --> 00:01:26.101
postdiplomac sam
medijske laboratorije MIT-a

00:01:26.125 --> 00:01:31.042
i tamo imam priliku da radim
na raznim neobičnim projektima,

00:01:31.066 --> 00:01:34.353
uključujući „Ogledalo aspiracije“,
projekat koji sam sprovela

00:01:34.373 --> 00:01:38.141
tako da mogu da projektujem
digitalne maske na svoj odraz.

00:01:38.165 --> 00:01:40.405
Tako bih ujutru,
ako želim da se osećam snažno,

00:01:40.439 --> 00:01:41.883
mogla da stavim lava.

00:01:41.917 --> 00:01:45.413
Ako bih htela da podignem raspoloženje,
možda bih dobila citat.

00:01:45.447 --> 00:01:48.436
Koristila sam generički softver
za prepoznavanje lica

00:01:48.450 --> 00:01:49.871
da bih napravila sistem,

00:01:49.915 --> 00:01:55.018
ali sam otkrila da ga je teško testirati
ukoliko ne nosim belu masku.

00:01:56.102 --> 00:02:00.212
Nažalost, već sam ranije nailazila
na ovaj problem.

00:02:00.212 --> 00:02:03.649
Kada sam bila na osnovnim studijama
na Tehnološkom institutu u Džordžiji,

00:02:03.649 --> 00:02:05.109
gde sam studirala informatiku,

00:02:05.109 --> 00:02:06.854
radila sam na društvenim robotima,

00:02:06.878 --> 00:02:10.655
a jedan od mojih zadataka je bio
da navedem robota da se igra skrivanja,

00:02:10.679 --> 00:02:12.362
jednostavne igre menjanja uloga

00:02:12.386 --> 00:02:16.707
u kojoj partneri pokrivaju lice,
a zatim ga otkriju i kažu: „Uja!“

00:02:16.731 --> 00:02:21.160
Problem je što igra skrivanja
ne funkcioniše ako ne mogu da vas vidim,

00:02:21.184 --> 00:02:23.683
a moj robot nije mogao da me vidi.

00:02:23.707 --> 00:02:27.657
No, pozajmila sam lice svoje cimerke
da bih završila projekat,

00:02:27.681 --> 00:02:29.061
predala sam zadatak,

00:02:29.085 --> 00:02:32.838
i mislila sam: „Znate šta,
neko drugi će rešiti ovaj problem.“

00:02:33.489 --> 00:02:35.492
Nedugo potom,

00:02:35.516 --> 00:02:39.675
bila sam u Hongkongu
na takmičenju preduzetnika.

00:02:40.159 --> 00:02:42.853
Organizatori su rešili da povedu učesnike

00:02:42.877 --> 00:02:45.179
u obilazak lokalnih startapova.

00:02:45.193 --> 00:02:47.988
Jedan od startapova
imao je društvenog robota

00:02:48.012 --> 00:02:49.924
i rešili su da naprave demonstraciju.

00:02:49.948 --> 00:02:52.742
Demonstracija je radila kod svih
dok nisu stigli do mene

00:02:52.742 --> 00:02:55.045
i možete verovatno pretpostaviti
šta se dogodilo.

00:02:55.045 --> 00:02:57.864
Nije mogao da prepozna moje lice.

00:02:57.888 --> 00:03:00.159
Pitala sam programere šta se dešava

00:03:00.193 --> 00:03:05.956
i ispostavilo se da smo koristili
isti generički softver prepoznavanja lica.

00:03:05.980 --> 00:03:07.630
Preko pola sveta,

00:03:07.654 --> 00:03:11.506
saznala sam da algoritamska pristrasnost
može putovati toliko brzo

00:03:11.530 --> 00:03:14.700
koliko treba da se skine
nešto fajlova sa interneta.

00:03:15.455 --> 00:03:18.531
Pa, šta se dešava?
Zašto se moje lice ne prepoznaje?

00:03:18.565 --> 00:03:21.921
Pa, moramo pogledati
kako mašini dajemo vid.

00:03:21.945 --> 00:03:25.354
Kompjuterski vid koristi
tehnike mašinskog učenja

00:03:25.378 --> 00:03:27.258
da bi prepoznao lica.

00:03:27.292 --> 00:03:31.129
To funkcioniše tako što napravite
komplet za vežbanje sa primerima lica.

00:03:31.153 --> 00:03:33.971
Ovo je lice. Ovo je lice. Ovo nije lice.

00:03:34.005 --> 00:03:38.614
Vremenom možete naučiti kompjuter
kako da prepoznaje druga lica.

00:03:38.628 --> 00:03:42.517
Međutim, ako kompleti za vežbanje
baš i nisu tako raznovrsni,

00:03:42.551 --> 00:03:45.950
svako lice koje previše odstupa
od uspostavljene norme

00:03:45.984 --> 00:03:47.633
biće teže da se prepozna,

00:03:47.647 --> 00:03:49.560
a to je ono što se događa sa mnom.

00:03:49.594 --> 00:03:51.976
Ali ne brinite, ima dobrih vesti.

00:03:52.020 --> 00:03:54.791
Kompleti za vežbanje
ne dolaze tek tako niotkuda.

00:03:54.825 --> 00:03:56.613
Možemo ih stvoriti.

00:03:56.657 --> 00:04:00.833
Postoji mogućnost za stvaranje
kompleta za vežbu celokupnog spektra

00:04:00.847 --> 00:04:04.671
koji odražavaju
bogatiji portret čovečanstva.

00:04:04.705 --> 00:04:06.836
Videli ste u mojim primerima

00:04:06.880 --> 00:04:08.808
da sam preko društvenih robota

00:04:08.832 --> 00:04:13.443
saznala za isključivanje
kroz algoritamsku pristrasnost.

00:04:13.467 --> 00:04:18.282
Ali algoritamska pristrasnost
može dovesti i do prakse diskriminacije.

00:04:19.257 --> 00:04:20.710
Širom SAD-a,

00:04:20.734 --> 00:04:24.932
policijske uprave počinju da koriste
softver za prepoznavanje lica

00:04:24.956 --> 00:04:27.415
u svom arsenalu za borbu protiv kriminala.

00:04:27.439 --> 00:04:29.452
Zakon Džordžtauna je objavio izveštaj

00:04:29.476 --> 00:04:32.953
koji pokazuje da se jednoj
od dve odrasle osobe u SAD-u -

00:04:32.983 --> 00:04:36.263
to je 117 miliona ljudi -

00:04:36.263 --> 00:04:39.797
lice nalazi u mrežama
za prepoznavanje lica.

00:04:39.821 --> 00:04:44.373
Odeljenja policije trenutno mogu
da neregulisano pregledaju ove mreže,

00:04:44.397 --> 00:04:48.683
pomoću algoritama
kojima nije proverena tačnost.

00:04:48.707 --> 00:04:52.571
Znamo da prepoznavanje lica nije bez mane,

00:04:52.595 --> 00:04:56.774
a naznačavanje lica stalno ostaje izazov.

00:04:56.798 --> 00:04:58.560
Možda ste to videli na Fejsbuku.

00:04:58.584 --> 00:05:01.572
Moji prijatelji i ja se uvek smejemo
kad vidimo druge ljude

00:05:01.596 --> 00:05:04.054
koji su pogrešno označeni
na našim fotografijama.

00:05:04.078 --> 00:05:09.669
Ali pogrešno identifikovanje
osumnjičenog zločinca nije za smejanje,

00:05:09.693 --> 00:05:12.520
kao ni kršenje građanskih sloboda.

00:05:12.544 --> 00:05:15.749
Mašinsko učenje se koristi
za prepoznavanje lica,

00:05:15.773 --> 00:05:20.278
ali se takođe proteže
i van dometa kompjuterskog vida.

00:05:21.086 --> 00:05:25.102
U svojoj knjizi „Oružja
za matematičko uništenje“,

00:05:25.126 --> 00:05:31.807
naučnica u oblasti podataka Keti O'Nil
govori o usponu novih RMD-a,

00:05:31.831 --> 00:05:36.054
rasprostranjenih, misterioznih
i destruktivnih algoritama

00:05:36.078 --> 00:05:39.092
koji se sve više koriste
za donošenje odluka

00:05:39.116 --> 00:05:42.327
koje utiču na sve više aspekata
našeg života.

00:05:42.327 --> 00:05:43.891
Koga će zaposliti ili otpustiti?

00:05:43.891 --> 00:05:46.663
Da li ćete dobiti taj kredit?
Da li ćete dobiti osiguranje?

00:05:46.663 --> 00:05:49.930
Da li ste primljeni na fakultet
u koji ste želeli da upadnete?

00:05:49.954 --> 00:05:53.463
Da li vi i ja plaćamo istu cenu
za isti proizvod

00:05:53.487 --> 00:05:55.929
kupljen na istoj platformi?

00:05:55.953 --> 00:05:59.712
Sprovođenje zakona takođe počinje
da koristi mašinsko učenje

00:05:59.736 --> 00:06:02.025
za prediktivni rad policije.

00:06:02.049 --> 00:06:05.057
Neke sudije koriste
mašinski generisane procene rizika

00:06:05.057 --> 00:06:09.869
da bi odredile koliko vremena
će neki pojedinac provesti u zatvoru.

00:06:09.903 --> 00:06:12.337
Zato zaista treba da razmislimo
o ovim odlukama.

00:06:12.361 --> 00:06:13.543
Jesu li pravedne?

00:06:13.577 --> 00:06:16.467
A videli smo da algoritamske predrasude

00:06:16.501 --> 00:06:19.815
ne dovode nužno uvek do pravednih ishoda.

00:06:19.869 --> 00:06:21.833
Šta možemo da uradimo u vezi sa time?

00:06:21.877 --> 00:06:25.557
Pa, možemo početi da razmišljamo
o tome kako da stvorimo inkluzivniji kod

00:06:25.591 --> 00:06:28.581
i da upotrebimo inkluzivne
postupke kodiranja.

00:06:28.595 --> 00:06:30.904
To zapravo počinje od ljudi.

00:06:31.448 --> 00:06:33.409
Zato je bitno ko kodira.

00:06:33.433 --> 00:06:37.356
Da li kreiramo timove celokupnog spektra
sa različitim pojedincima

00:06:37.386 --> 00:06:40.197
koji mogu da jedno drugome ispitaju
stvari za koje su slepi?

00:06:40.217 --> 00:06:43.556
Sa tehničke strane,
bitno je kako kodiramo.

00:06:43.590 --> 00:06:47.241
Da li uzimamo u obzir pravičnost
dok razvijamo sisteme?

00:06:47.275 --> 00:06:50.478
I najzad, bitno je zašto kodiramo.

00:06:50.525 --> 00:06:55.608
Koristili smo alate računarskog stvaranja
da bismo otključali ogromno bogatstvo.

00:06:55.642 --> 00:07:00.089
Sada imamo priliku da otključamo
još veću ravnopravnost

00:07:00.103 --> 00:07:03.033
ako učinimo društvene promene prioritetom,

00:07:03.077 --> 00:07:05.247
a ne da ih naknadno promišljamo.

00:07:05.828 --> 00:07:10.350
Dakle, ovo su tri principa
koji će sačinjavati pokret „inkodiranja“.

00:07:10.374 --> 00:07:12.026
Bitno je ko kodira,

00:07:12.050 --> 00:07:13.593
kako kodiramo

00:07:13.617 --> 00:07:15.464
i zašto kodiramo.

00:07:15.464 --> 00:07:18.853
Stoga, da bismo išli u pravcu inkodiranja,
možemo početi da razmišljamo

00:07:18.853 --> 00:07:21.951
o izgradnji platforma
koje mogu da identifikuju pristrasnost

00:07:21.975 --> 00:07:25.053
prikupljanjem iskustava ljudi
poput onih koje sam podelila,

00:07:25.077 --> 00:07:28.147
ali i pregledom postojećeg softvera.

00:07:28.171 --> 00:07:31.936
Takođe možemo početi da stvaramo
inkluzivnije komplete za vežbanje.

00:07:31.960 --> 00:07:34.763
Zamislite kampanju „Selfiji za inkluziju“

00:07:34.787 --> 00:07:38.122
u kojoj vi i ja možemo pomoći
programerima da testiraju i naprave

00:07:38.136 --> 00:07:40.229
inkluzivnije komplete za vežbanje.

00:07:41.122 --> 00:07:43.950
Takođe možemo početi
da savesnije razmišljamo

00:07:43.974 --> 00:07:49.365
o društvenom uticaju
tehnologije koju razvijamo.

00:07:49.389 --> 00:07:51.782
Da bih otpočela pokret inkodiranja,

00:07:51.806 --> 00:07:54.653
pokrenula sam Ligu za algoritamsku pravdu,

00:07:54.677 --> 00:07:57.573
gde svako ko se brine o pravičnosti

00:07:57.573 --> 00:08:00.573
može pomoći u borbi
protiv kodiranog pogleda.

00:08:00.573 --> 00:08:03.869
Na codedgaze.com
možete prijaviti pristrasnost,

00:08:03.893 --> 00:08:06.338
zatražiti proveru, postati tester

00:08:06.362 --> 00:08:09.133
i pridružiti se aktuelnom razgovoru,

00:08:09.157 --> 00:08:11.444
#codedgaze.

00:08:12.562 --> 00:08:14.899
Pozivam vas da mi se pridružite

00:08:14.933 --> 00:08:18.652
u stvaranju sveta
u kome tehnologija radi za sve nas,

00:08:18.676 --> 00:08:20.573
a ne samo za neke od nas,

00:08:20.607 --> 00:08:25.235
sveta u kome cenimo inkluziju
i stavljamo u središte društvene promene.

00:08:25.269 --> 00:08:26.444
Hvala.

00:08:26.468 --> 00:08:28.769
(Aplauz)

00:08:32.693 --> 00:08:35.547
Ali imam jedno pitanje.

00:08:35.571 --> 00:08:37.630
Hoćete li mi se pridružiti u borbi?

00:08:37.654 --> 00:08:38.939
(Smeh)

00:08:38.963 --> 00:08:42.650
(Aplauz)

