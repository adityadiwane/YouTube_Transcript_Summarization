WEBVTT
Kind: captions
Language: en

00:00:12.485 --> 00:00:14.707
Ten years ago, I wrote a book which I entitled

00:00:14.707 --> 00:00:17.800
"Our Final Century?" Question mark.

00:00:17.800 --> 00:00:21.377
My publishers cut out the question mark. (Laughter)

00:00:21.377 --> 00:00:23.259
The American publishers changed our title

00:00:23.259 --> 00:00:27.168
to "Our Final Hour."

00:00:27.168 --> 00:00:30.660
Americans like instant gratification and the reverse.

00:00:30.660 --> 00:00:32.368
(Laughter)

00:00:32.368 --> 00:00:34.118
And my theme was this:

00:00:34.118 --> 00:00:38.284
Our Earth has existed for 45 million centuries,

00:00:38.284 --> 00:00:40.297
but this one is special —

00:00:40.297 --> 00:00:43.313
it's the first where one species, ours,

00:00:43.313 --> 00:00:46.115
has the planet's future in its hands.

00:00:46.115 --> 00:00:48.105
Over nearly all of Earth's history,

00:00:48.105 --> 00:00:50.041
threats have come from nature —

00:00:50.041 --> 00:00:53.537
disease, earthquakes, asteroids and so forth —

00:00:53.537 --> 00:00:59.209
but from now on, the worst dangers come from us.

00:00:59.209 --> 00:01:02.480
And it's now not just the nuclear threat;

00:01:02.480 --> 00:01:04.231
in our interconnected world,

00:01:04.231 --> 00:01:07.394
network breakdowns can cascade globally;

00:01:07.394 --> 00:01:11.350
air travel can spread pandemics
worldwide within days;

00:01:11.350 --> 00:01:14.677
and social media can spread panic and rumor

00:01:14.677 --> 00:01:17.894
literally at the speed of light.

00:01:17.894 --> 00:01:21.119
We fret too much about minor hazards —

00:01:21.119 --> 00:01:25.150
improbable air crashes, carcinogens in food,

00:01:25.150 --> 00:01:27.376
low radiation doses, and so forth —

00:01:27.376 --> 00:01:30.201
but we and our political masters

00:01:30.201 --> 00:01:34.404
are in denial about catastrophic scenarios.

00:01:34.404 --> 00:01:37.442
The worst have thankfully not yet happened.

00:01:37.442 --> 00:01:39.638
Indeed, they probably won't.

00:01:39.638 --> 00:01:42.823
But if an event is potentially devastating,

00:01:42.823 --> 00:01:45.691
it's worth paying a substantial premium

00:01:45.691 --> 00:01:49.527
to safeguard against it, even if it's unlikely,

00:01:49.527 --> 00:01:54.040
just as we take out fire insurance on our house.

00:01:54.040 --> 00:01:59.037
And as science offers greater power and promise,

00:01:59.037 --> 00:02:02.903
the downside gets scarier too.

00:02:02.903 --> 00:02:05.142
We get ever more vulnerable.

00:02:05.142 --> 00:02:06.980
Within a few decades,

00:02:06.980 --> 00:02:09.210
millions will have the capability

00:02:09.210 --> 00:02:12.331
to misuse rapidly advancing biotech,

00:02:12.331 --> 00:02:15.884
just as they misuse cybertech today.

00:02:15.884 --> 00:02:19.083
Freeman Dyson, in a TED Talk,

00:02:19.083 --> 00:02:22.679
foresaw that children will design
and create new organisms

00:02:22.679 --> 00:02:27.190
just as routinely as his generation
played with chemistry sets.

00:02:27.190 --> 00:02:29.718
Well, this may be on the science fiction fringe,

00:02:29.718 --> 00:02:32.901
but were even part of his scenario to come about,

00:02:32.901 --> 00:02:35.638
our ecology and even our species

00:02:35.638 --> 00:02:39.627
would surely not survive long unscathed.

00:02:39.627 --> 00:02:43.490
For instance, there are some eco-extremists

00:02:43.490 --> 00:02:45.999
who think that it would be better for the planet,

00:02:45.999 --> 00:02:49.402
for Gaia, if there were far fewer humans.

00:02:49.402 --> 00:02:52.119
What happens when such people have mastered

00:02:52.119 --> 00:02:54.256
synthetic biology techniques

00:02:54.256 --> 00:02:57.108
that will be widespread by 2050?

00:02:57.108 --> 00:03:00.150
And by then, other science fiction nightmares

00:03:00.150 --> 00:03:01.860
may transition to reality:

00:03:01.860 --> 00:03:03.930
dumb robots going rogue,

00:03:03.930 --> 00:03:06.347
or a network that develops a mind of its own

00:03:06.347 --> 00:03:08.936
threatens us all.

00:03:08.936 --> 00:03:12.206
Well, can we guard against such risks by regulation?

00:03:12.206 --> 00:03:14.613
We must surely try, but these enterprises

00:03:14.613 --> 00:03:18.142
are so competitive, so globalized,

00:03:18.142 --> 00:03:20.122
and so driven by commercial pressure,

00:03:20.122 --> 00:03:23.407
that anything that can be done
will be done somewhere,

00:03:23.407 --> 00:03:25.443
whatever the regulations say.

00:03:25.443 --> 00:03:28.930
It's like the drug laws — we try to regulate, but can't.

00:03:28.930 --> 00:03:31.974
And the global village will have its village idiots,

00:03:31.974 --> 00:03:35.470
and they'll have a global range.

00:03:35.470 --> 00:03:37.761
So as I said in my book,

00:03:37.761 --> 00:03:40.650
we'll have a bumpy ride through this century.

00:03:40.650 --> 00:03:44.140
There may be setbacks to our society —

00:03:44.140 --> 00:03:48.255
indeed, a 50 percent chance of a severe setback.

00:03:48.255 --> 00:03:51.169
But are there conceivable events

00:03:51.169 --> 00:03:53.330
that could be even worse,

00:03:53.330 --> 00:03:56.760
events that could snuff out all life?

00:03:56.760 --> 00:03:59.686
When a new particle accelerator came online,

00:03:59.686 --> 00:04:01.475
some people anxiously asked,

00:04:01.475 --> 00:04:03.725
could it destroy the Earth or, even worse,

00:04:03.725 --> 00:04:06.384
rip apart the fabric of space?

00:04:06.384 --> 00:04:09.927
Well luckily, reassurance could be offered.

00:04:09.927 --> 00:04:11.971
I and others pointed out that nature

00:04:11.971 --> 00:04:13.904
has done the same experiments

00:04:13.904 --> 00:04:16.090
zillions of times already,

00:04:16.090 --> 00:04:17.855
via cosmic ray collisions.

00:04:17.855 --> 00:04:20.909
But scientists should surely be precautionary

00:04:20.909 --> 00:04:23.489
about experiments that generate conditions

00:04:23.489 --> 00:04:25.972
without precedent in the natural world.

00:04:25.972 --> 00:04:29.395
Biologists should avoid release
of potentially devastating

00:04:29.395 --> 00:04:32.110
genetically modified pathogens.

00:04:32.110 --> 00:04:35.627
And by the way, our special aversion

00:04:35.627 --> 00:04:39.088
to the risk of truly existential disasters

00:04:39.088 --> 00:04:42.363
depends on a philosophical and ethical question,

00:04:42.363 --> 00:04:44.033
and it's this:

00:04:44.033 --> 00:04:46.341
Consider two scenarios.

00:04:46.341 --> 00:04:51.577
Scenario A wipes out 90 percent of humanity.

00:04:51.577 --> 00:04:55.473
Scenario B wipes out 100 percent.

00:04:55.473 --> 00:04:58.391
How much worse is B than A?

00:04:58.391 --> 00:05:01.414
Some would say 10 percent worse.

00:05:01.414 --> 00:05:04.564
The body count is 10 percent higher.

00:05:04.564 --> 00:05:07.470
But I claim that B is incomparably worse.

00:05:07.470 --> 00:05:10.099
As an astronomer, I can't believe

00:05:10.099 --> 00:05:12.566
that humans are the end of the story.

00:05:12.566 --> 00:05:15.889
It is five billion years before the sun flares up,

00:05:15.889 --> 00:05:18.600
and the universe may go on forever,

00:05:18.600 --> 00:05:20.892
so post-human evolution,

00:05:20.892 --> 00:05:23.082
here on Earth and far beyond,

00:05:23.082 --> 00:05:25.796
could be as prolonged as the Darwinian process

00:05:25.796 --> 00:05:29.077
that's led to us, and even more wonderful.

00:05:29.077 --> 00:05:31.741
And indeed, future evolution
will happen much faster,

00:05:31.741 --> 00:05:33.940
on a technological timescale,

00:05:33.940 --> 00:05:36.239
not a natural selection timescale.

00:05:36.239 --> 00:05:40.434
So we surely, in view of those immense stakes,

00:05:40.434 --> 00:05:43.820
shouldn't accept even a one in a billion risk

00:05:43.820 --> 00:05:46.049
that human extinction would foreclose

00:05:46.049 --> 00:05:48.359
this immense potential.

00:05:48.359 --> 00:05:50.131
Some scenarios that have been envisaged

00:05:50.131 --> 00:05:51.950
may indeed be science fiction,

00:05:51.950 --> 00:05:55.336
but others may be disquietingly real.

00:05:55.336 --> 00:05:58.210
It's an important maxim that the unfamiliar

00:05:58.210 --> 00:06:00.907
is not the same as the improbable,

00:06:00.907 --> 00:06:03.305
and in fact, that's why we at Cambridge University

00:06:03.305 --> 00:06:06.680
are setting up a center to study how to mitigate

00:06:06.680 --> 00:06:08.712
these existential risks.

00:06:08.712 --> 00:06:11.775
It seems it's worthwhile just for a few people

00:06:11.775 --> 00:06:14.091
to think about these potential disasters.

00:06:14.091 --> 00:06:17.104
And we need all the help we can get from others,

00:06:17.104 --> 00:06:19.583
because we are stewards of a precious

00:06:19.583 --> 00:06:23.066
pale blue dot in a vast cosmos,

00:06:23.066 --> 00:06:26.444
a planet with 50 million centuries ahead of it.

00:06:26.444 --> 00:06:29.000
And so let's not jeopardize that future.

00:06:29.000 --> 00:06:30.795
And I'd like to finish with a quote

00:06:30.795 --> 00:06:34.296
from a great scientist called Peter Medawar.

00:06:34.296 --> 00:06:37.569
I quote, "The bells that toll for mankind

00:06:37.569 --> 00:06:40.213
are like the bells of Alpine cattle.

00:06:40.213 --> 00:06:42.499
They are attached to our own necks,

00:06:42.499 --> 00:06:45.174
and it must be our fault if they do not make

00:06:45.174 --> 00:06:47.305
a tuneful and melodious sound."

00:06:47.305 --> 00:06:49.572
Thank you very much.

00:06:49.572 --> 00:06:51.685
(Applause)

