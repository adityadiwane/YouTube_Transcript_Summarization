WEBVTT
Kind: captions
Language: zh-TW

00:00:00.000 --> 00:00:07.000
譯者: Sailin Lu
審譯者: angie chen

00:00:15.394 --> 00:00:18.104
容我為各位呈現一些照片

00:00:18.104 --> 00:00:22.260
(影片)女孩：嗯，這是一隻貓，坐在床上。

00:00:22.260 --> 00:00:26.300
這男孩在拍撫一隻象。

00:00:26.300 --> 00:00:30.654
這些人要去搭飛機。

00:00:30.654 --> 00:00:32.776
好大的飛機。

00:00:32.776 --> 00:00:35.670
主講人：這是由一位三歲的小孩

00:00:35.670 --> 00:00:39.462
所描述她看到的一系列照片

00:00:39.462 --> 00:00:42.194
雖然對於這世界她還有更多要學習的地方，

00:00:42.194 --> 00:00:46.743
但是她已經是其中一項重要技能的專家--

00:00:46.743 --> 00:00:49.669
為所見之聞賦予意義。

00:00:49.669 --> 00:00:54.455
科技在我們的社會已進展到前所未有的程度：

00:00:54.455 --> 00:00:58.084
我們把人送上月球、發明可以與人交談的電話，

00:00:58.084 --> 00:01:03.030
或是客製一個電台，只播放個人喜歡的音樂。

00:01:03.030 --> 00:01:07.085
然而這台無比聰明的機器和電腦

00:01:07.085 --> 00:01:09.988
仍然無法發展這項技能，

00:01:09.988 --> 00:01:13.437
因此今天我來到這裡向各位報告

00:01:13.447 --> 00:01:17.494
我們在電腦視覺的最新研究進展，

00:01:17.494 --> 00:01:20.475
這是現階段在資訊業領域中，

00:01:20.475 --> 00:01:24.861
最先進、最具潛力的革命性技術。

00:01:24.861 --> 00:01:29.412
是的，目前我們已經有自動駕駛的原型車，

00:01:29.412 --> 00:01:33.265
但若不具備視覺辨識技術，
它將無法分辨同樣出現在馬路中，

00:01:33.265 --> 00:01:37.235
一團它其實輾過也無妨的破紙袋，

00:01:37.235 --> 00:01:40.575
以及一個大到它必須閃避的石塊，
兩者有何不同。

00:01:40.585 --> 00:01:44.805
我們製造出畫素極高的相機，

00:01:44.805 --> 00:01:47.970
但我們卻無法賦予盲人視覺；

00:01:47.970 --> 00:01:51.725
無人機可以翻山越嶺，

00:01:51.725 --> 00:01:53.639
卻沒有足夠的視覺技術可以

00:01:53.639 --> 00:01:57.320
讓我們追蹤雨林的變化；

00:01:57.320 --> 00:02:00.270
監視器滿佈在各個角落，

00:02:00.270 --> 00:02:05.437
卻無法在看到一個孩子將溺斃在泳池之際，
對我們發出警訊。

00:02:05.437 --> 00:02:11.762
靜態及動態影像已逐漸與全世界的生活密不可分，

00:02:11.762 --> 00:02:15.845
它們發展的步伐已經遠遠超越人類

00:02:15.845 --> 00:02:18.628
及其群體所相信的，

00:02:18.628 --> 00:02:22.553
在座各位以及我自己
都是TED這個活動裡頭的推手。

00:02:22.553 --> 00:02:26.765
然而，目前最先進的軟體卻仍在其中苦苦掙扎，

00:02:26.765 --> 00:02:31.661
無法理解與應用這龐大的資料體。

00:02:31.661 --> 00:02:36.113
換而言之，在這整個社會裡，

00:02:36.113 --> 00:02:38.679
大家都有如盲人在運作，

00:02:38.679 --> 00:02:43.526
因為連我們最聰明的機器都還看不見。

00:02:43.526 --> 00:02:46.452
或許有人會問：這到底有什麼困難？

00:02:46.452 --> 00:02:49.145
任何相機都可以產生像這樣的照片，

00:02:49.145 --> 00:02:53.139
它是藉由將有色光轉換成2D的數字陣列，

00:02:53.139 --> 00:02:54.789
也就是大家熟知的像素。

00:02:54.789 --> 00:02:57.040
但這些數字是死的，

00:02:57.040 --> 00:03:00.151
並沒有被賦予意義。

00:03:00.151 --> 00:03:04.494
就好像有「聽」，不代表有「到」。

00:03:04.494 --> 00:03:08.534
同樣地，攝取到影像不等於看見，

00:03:08.534 --> 00:03:13.283
我們所認知的看到，應包含著了解其中的意義。

00:03:13.293 --> 00:03:19.470
事實上，這樣的成果，
是大自然花了五億四千萬年的光陰

00:03:19.470 --> 00:03:21.443
才得到的。

00:03:21.443 --> 00:03:23.324
這其中的努力，

00:03:23.324 --> 00:03:28.595
泰半是耗費在發展腦部的視覺處理這個區塊，

00:03:28.595 --> 00:03:31.242
而不是眼睛的部分。

00:03:31.242 --> 00:03:33.989
也就是說，視覺始於眼睛，

00:03:33.989 --> 00:03:38.287
但真正使它有用的，卻是大腦。

00:03:38.287 --> 00:03:43.347
十五年來，從在加州理工學院攻讀博士開始，

00:03:43.347 --> 00:03:46.273
到領導史丹佛的視覺實驗室，

00:03:46.273 --> 00:03:50.669
我和指導教授、同事及學生們，

00:03:50.669 --> 00:03:53.558
試圖讓電腦擁有智能之眼，

00:03:54.658 --> 00:03:57.952
我們研究的領域稱之為電腦視覺與機器學習，

00:03:57.952 --> 00:04:01.830
這是人工智慧其中一環。

00:04:03.000 --> 00:04:08.493
我們的終極目標就是教導機器能夠像人一樣理解所見之物，

00:04:08.493 --> 00:04:13.880
像是識別物品、辨認人臉、
推論物體的幾何形態，

00:04:13.880 --> 00:04:19.568
進而理解其中的關聯、情緒、動作及意圖。

00:04:19.568 --> 00:04:24.451
在座每一位和我，都可以在匆匆一瞥的瞬間，

00:04:24.451 --> 00:04:28.955
理解到人事、地、物所交織而成的網絡，

00:04:28.955 --> 00:04:34.538
要電腦達成這個目標的第一步，就是教導它辨別物品，

00:04:34.538 --> 00:04:37.906
這是視覺的基石。

00:04:37.906 --> 00:04:42.340
簡單來說，我們教導的方法就是

00:04:42.340 --> 00:04:47.235
給電腦看一些特定物體的影像，

00:04:47.235 --> 00:04:48.746
例如貓咪。

00:04:48.746 --> 00:04:53.393
我們設計了一個程式讓電腦利用這些影像來學習

00:04:53.393 --> 00:04:55.437
這有啥困難？

00:04:55.437 --> 00:04:59.489
貓咪不就是由一些幾何圖形和顏色所組成的嘛，

00:04:59.489 --> 00:05:03.575
這就是我們初期所做的物體模型。

00:05:03.575 --> 00:05:07.197
我們用數學語言來告知電腦演繹方法，

00:05:07.197 --> 00:05:10.540
貓就是有圓圓的臉、胖胖的身體，

00:05:10.540 --> 00:05:12.839
兩個尖尖的耳朵和一條長尾巴。

00:05:12.839 --> 00:05:14.859
看起來很好啊，

00:05:14.859 --> 00:05:16.972
但如果貓咪長這樣呢？

00:05:16.972 --> 00:05:18.063
(觀眾笑)

00:05:18.063 --> 00:05:19.689
全身都捲起來了。

00:05:19.689 --> 00:05:24.408
這下子我們又得在原來的模型
加上新的形狀和不同的視野角度。

00:05:24.408 --> 00:05:27.133
又，如果貓咪是躲著的呢？

00:05:27.143 --> 00:05:30.032
像這群傻貓？

00:05:31.112 --> 00:05:33.529
這樣各位了解我的意思嗎？

00:05:33.529 --> 00:05:36.896
即使簡單如貓這樣的家庭寵物，

00:05:36.896 --> 00:05:41.400
也會有相對於原型以外，無數的其他形態表徵，

00:05:41.400 --> 00:05:44.573
而這只是其中一樣。

00:05:44.573 --> 00:05:47.065
因此八年前，

00:05:47.065 --> 00:05:52.095
一項極其簡單和深刻的觀察，改變了我的想法，

00:05:53.425 --> 00:05:56.110
沒有人教導孩子如何去「看」，

00:05:56.110 --> 00:05:58.371
特別是在早期發育階段，

00:05:58.371 --> 00:06:03.371
他們是從真實世界的經驗中學習。

00:06:03.371 --> 00:06:06.111
如果你把孩童的眼睛

00:06:06.111 --> 00:06:08.665
當成生物相機的概念，

00:06:08.665 --> 00:06:12.845
就如同每200毫秒就拍一張照片一樣，

00:06:12.845 --> 00:06:15.979
這是眼球移動的平均時間。

00:06:15.979 --> 00:06:20.509
年紀到了三歲時，
孩子們已經看過了真實世界中

00:06:20.509 --> 00:06:23.363
數以百萬計的照片，

00:06:23.363 --> 00:06:26.363
這樣的訓練範例是很大量的。

00:06:26.383 --> 00:06:32.372
因此，我的直覺告訴我
應該以孩童的學習經驗法則，

00:06:32.372 --> 00:06:35.874
並兼以質與量，

00:06:35.874 --> 00:06:40.963
提供訓練的資料給電腦，

00:06:40.963 --> 00:06:44.841
而非一昧追求更好的程式演算。

00:06:44.841 --> 00:06:46.699
有了上述的洞見，

00:06:46.699 --> 00:06:49.670
我們接下來必須要收集

00:06:49.670 --> 00:06:54.129
前所未有的大量資料群，

00:06:54.129 --> 00:06:56.706
甚至於是千倍以上的。

00:06:56.706 --> 00:07:00.817
於是我與普林斯頓大學的李凱教授

00:07:00.817 --> 00:07:05.569
共同於2007年開始了
我們稱之為 ImageNet 的專案。

00:07:05.569 --> 00:07:09.407
很幸運地，我們不必在頭上綁一個相機，

00:07:09.407 --> 00:07:11.171
然後花費數年收集影像，

00:07:11.171 --> 00:07:12.634
而是轉而由網際網路，

00:07:12.634 --> 00:07:17.070
這個由人類所創造出來 龐大的影像寶窟，

00:07:17.070 --> 00:07:20.111
我們下載了數以百萬計的影像，

00:07:20.111 --> 00:07:25.991
並且使用如Amazon Mechanical Turk 
這樣的群眾外包平台，

00:07:25.991 --> 00:07:28.330
來協助我們處理及分類這些照片。

00:07:28.330 --> 00:07:33.230
在高峰期，ImageNet 甚至是整個亞馬遜平台

00:07:33.230 --> 00:07:36.226
最大的雇主之一，

00:07:36.226 --> 00:07:40.080
我們一共聘請了來自167個國家，

00:07:40.080 --> 00:07:44.120
約5萬個工作者，

00:07:44.120 --> 00:07:48.067
來協助我們分類處理並標示

00:07:48.067 --> 00:07:51.642
將近10億幅影像，

00:07:52.612 --> 00:07:55.265
花費了這麼多的資源，

00:07:55.265 --> 00:07:59.165
就是為了捕捉那一絲絲

00:07:59.165 --> 00:08:04.096
孩童在早期心智發展的浮光掠影。

00:08:04.148 --> 00:08:08.050
用現在眼光看來，使用大量的資料

00:08:08.050 --> 00:08:12.600
來訓練電腦演算是明顯合理的，

00:08:12.600 --> 00:08:16.710
然而在2007年的世界卻非如此。

00:08:16.710 --> 00:08:20.588
有好長一段時間，
我們在這個旅途中孤獨地踽踽而行，

00:08:20.588 --> 00:08:25.591
有些同事好心地建議我，
與其苦苦掙扎於研究經費的募集，

00:08:25.591 --> 00:08:29.933
還不如轉而先做些比較好拿到終身聘的研究，

00:08:29.933 --> 00:08:32.418
我還曾跟我的研究生開玩笑說

00:08:32.418 --> 00:08:36.481
我乾脆再開一間乾洗店來資助ImageNet 好了，

00:08:36.481 --> 00:08:41.242
畢竟那就是我用以支付大學學費的方法。

00:08:41.242 --> 00:08:43.098
就這樣我們還是繼續往前走，

00:08:43.098 --> 00:08:46.813
2009年起，ImageNet 已經是個擁有

00:08:46.813 --> 00:08:50.855
涵蓋了兩萬兩千種不同類別，

00:08:50.855 --> 00:08:55.660
多達150億幅圖像的資料庫，

00:08:55.660 --> 00:08:58.980
並組織以英語日常生活用字為主，

00:08:58.980 --> 00:09:01.906
這樣的規模，不論是「質」或「量」

00:09:01.906 --> 00:09:04.878
都是史無前例的。

00:09:04.878 --> 00:09:08.339
用貓來舉個例子說明，

00:09:08.339 --> 00:09:11.148
我們有超過六萬兩千種

00:09:11.148 --> 00:09:15.258
不同外觀和姿勢的貓咪，

00:09:15.258 --> 00:09:20.481
橫跨不同的種類，有家貓，也有野貓。

00:09:20.481 --> 00:09:23.825
ImageNet 的成果讓我們非常激動，

00:09:23.825 --> 00:09:27.563
我們希望它有助於全世界的研究，

00:09:27.563 --> 00:09:31.604
就如同 TED 的貢獻，我們免費提供整個資料庫

00:09:31.604 --> 00:09:35.196
給全世界的研究單位。

00:09:35.936 --> 00:09:40.636
(觀眾鼓掌)

00:09:41.416 --> 00:09:45.954
有了這些資料，我們可以教育我們的電腦，

00:09:45.954 --> 00:09:49.691
下一步就是回到程式演算的部分了。

00:09:49.691 --> 00:09:54.869
結果我們發現，ImageNet 所提供的豐富資訊

00:09:54.869 --> 00:09:59.675
恰巧與機器學習演算的其中一門特定領域
不謀而合，

00:09:59.675 --> 00:10:02.090
我們稱它為「卷積神經網絡」，

00:10:02.090 --> 00:10:07.338
在七零及八零年代，福島邦彥、Geoff Hinton

00:10:07.338 --> 00:10:10.983
和 Yann LeCun 等學者為該領域的先驅。

00:10:10.983 --> 00:10:16.602
正如同大腦是由無數個緊密連結的神經元所組成，

00:10:16.602 --> 00:10:20.456
神經網絡的基本運作單位

00:10:20.456 --> 00:10:22.852
也是一個類神經元的節點。

00:10:22.872 --> 00:10:25.425
它的運作方式是從別的節點得到資料，

00:10:25.425 --> 00:10:28.143
然後再傳給其他的節點。

00:10:28.143 --> 00:10:32.856
而且這些數不清的節點

00:10:32.856 --> 00:10:36.083
擁有層層的組織架構，

00:10:36.083 --> 00:10:38.637
就好像我們的大腦一樣。

00:10:38.637 --> 00:10:43.420
在一般的神經網絡中，
我們用作訓練的物品辨識模型

00:10:43.420 --> 00:10:46.601
就有兩千四百萬個節點、

00:10:46.601 --> 00:10:49.898
一億四千萬個參數，

00:10:49.898 --> 00:10:52.661
以及一百五十億個連結。

00:10:52.661 --> 00:10:55.076
這是一個大的不得了的模型。

00:10:55.076 --> 00:10:58.977
由ImageNet 提供巨大的資料群、

00:10:58.977 --> 00:11:04.410
並使用先進的核心處理器及圖型處理器來訓練
這個龐然大物，

00:11:04.410 --> 00:11:06.779
卷積神經網絡就在眾人的意料外

00:11:06.779 --> 00:11:10.215
開花結果了。

00:11:10.215 --> 00:11:12.723
在物品辨識領域中，這樣的架構

00:11:12.723 --> 00:11:18.063
以令人興奮的嶄新成果，傲視群雄。

00:11:18.063 --> 00:11:20.873
電腦告訴我們

00:11:20.873 --> 00:11:23.173
這張圖中有隻貓，

00:11:23.173 --> 00:11:25.076
還告訴我們貓在哪裡。

00:11:25.076 --> 00:11:27.188
當然，這世界不會只有貓，

00:11:27.188 --> 00:11:29.626
電腦的演算告訴我們

00:11:29.626 --> 00:11:32.900
這張圖中有一個男孩和一隻泰迪熊；

00:11:32.900 --> 00:11:37.266
有狗，一個人，以及背景中的一支小風箏；

00:11:37.266 --> 00:11:40.401
或這一張令人眼花撩亂的圖，

00:11:40.401 --> 00:11:45.045
有人、滑板、欄杆、路燈，等等。

00:11:45.045 --> 00:11:50.338
有時候，如果電腦不確定自己所見到的東西時，

00:11:51.498 --> 00:11:53.774
我們已經將它教到可以聰明地

00:11:53.774 --> 00:11:57.652
給一個安全的答案，而非莽撞地回答，

00:11:57.652 --> 00:12:00.463
就像一般人會做的。

00:12:00.463 --> 00:12:05.129
更有些時候，電腦的運算竟能夠

00:12:05.129 --> 00:12:07.382
精準地辨別物體品項

00:12:07.382 --> 00:12:10.818
例如製造商、型號、車子的年份。

00:12:10.818 --> 00:12:16.204
Google 將這個演算程式廣泛地運用在

00:12:16.204 --> 00:12:19.339
數百個美國城市的街景裡，

00:12:19.339 --> 00:12:22.265
也因此我們從中得到了一些有趣的概念。

00:12:22.265 --> 00:12:25.585
首先，它證實了一項廣為人知的說法，

00:12:25.585 --> 00:12:28.875
也就是汽車價格和家庭收入

00:12:28.875 --> 00:12:31.220
是息息相關的。

00:12:31.220 --> 00:12:35.747
然而令人驚訝的是，汽車價格也和

00:12:35.747 --> 00:12:38.047
城市中的犯罪率

00:12:39.007 --> 00:12:42.970
以及區域選舉模式，有相當的關係。

00:12:44.060 --> 00:12:46.266
等等，難道說我今天

00:12:46.266 --> 00:12:51.419
就是來告訴各位電腦已經趕上
甚至超越人類了嗎？

00:12:51.419 --> 00:12:53.557
還早得很呢。

00:12:53.557 --> 00:12:58.480
到目前為止，我們只是教導電腦識別物品，

00:12:58.480 --> 00:13:03.124
就像小孩子牙牙學語一樣，

00:13:03.124 --> 00:13:05.794
雖然這是個傲人的進展，

00:13:05.794 --> 00:13:08.254
但它不過是第一步而已，

00:13:08.254 --> 00:13:12.016
很快地，下一波具指標性的後浪就會打上來了，

00:13:12.016 --> 00:13:15.477
小孩子開始進展到用句子來溝通。

00:13:15.477 --> 00:13:19.701
因此，他已經不會用「這是貓」
來描述圖片，

00:13:19.701 --> 00:13:24.903
而是會聽到這個小女孩說「這是躺在床上的貓」。

00:13:24.903 --> 00:13:30.498
因此，要教導電腦看到圖並說出句子，

00:13:30.498 --> 00:13:34.446
必須進一步地仰賴龐大資料群

00:13:34.446 --> 00:13:36.721
以及機器的學習演算。

00:13:36.721 --> 00:13:40.877
現在，電腦不僅要學習圖片識別，

00:13:40.877 --> 00:13:43.733
還要學習人類自然的

00:13:43.733 --> 00:13:47.055
說話方式。

00:13:47.055 --> 00:13:50.908
就如同大腦要結合視覺和語言一樣，

00:13:50.908 --> 00:13:56.109
我們做出了一個模型，
它可以連結不同的可視物體，

00:13:56.109 --> 00:13:58.013
就像視覺片段一樣，

00:13:58.013 --> 00:14:02.216
並附上句子用的字詞和片語。

00:14:02.216 --> 00:14:04.979
約四個月前，

00:14:04.979 --> 00:14:07.626
我們終於把所有的元素全部兜起來了，

00:14:07.626 --> 00:14:11.410
做出了第一個電腦版的模型，

00:14:11.410 --> 00:14:15.404
它有辦法在初次看到照片時

00:14:15.404 --> 00:14:18.910
說出像人類般自然的句子，

00:14:18.910 --> 00:14:23.554
好，現在我要給各位看看電腦

00:14:23.554 --> 00:14:25.529
對於演講一開頭

00:14:25.529 --> 00:14:29.359
那位小女孩所看到的影像，
它又是如何理解的。

00:14:31.519 --> 00:14:34.863
(電腦) 有個人站在大象旁邊。

00:14:36.393 --> 00:14:40.027
一架大飛機停在機場跑道上。

00:14:41.057 --> 00:14:45.269
(主講人) 當然，我們仍戮力於改善這電腦程式，

00:14:45.269 --> 00:14:47.865
它還有很多要學。

00:14:47.865 --> 00:14:50.156
(觀眾鼓掌)

00:14:51.556 --> 00:14:54.877
電腦還是會犯錯。

00:14:54.877 --> 00:14:58.268
(電腦) 一隻貓包著毯子躺在床上。

00:14:58.268 --> 00:15:00.821
(主講人) 因為它看了太多貓了，

00:15:00.821 --> 00:15:03.747
以至於它見到了什麼都像貓咪。

00:15:05.317 --> 00:15:08.181
(電腦) 一位小男孩握著一支球棒。

00:15:08.181 --> 00:15:09.786
(觀眾笑)

00:15:09.786 --> 00:15:14.529
(主講人) 或者，如果電腦是第一次看到牙刷，
會把它與球棒混淆。

00:15:15.309 --> 00:15:18.743
(電腦) 一個人在建築物旁的街道上騎馬。

00:15:18.743 --> 00:15:20.766
(觀眾笑)

00:15:20.766 --> 00:15:24.318
(主講人) 我們還沒讓電腦上基礎美術課。

00:15:25.768 --> 00:15:28.652
(電腦) 一匹斑馬站在原野中。

00:15:28.652 --> 00:15:32.019
(主講人) 電腦還沒辦法像人類一樣，

00:15:32.019 --> 00:15:34.457
學會欣賞大自然的美景。

00:15:34.457 --> 00:15:37.289
這是條漫漫長路，

00:15:37.289 --> 00:15:41.515
要從零歲發展到三歲是很難的，

00:15:41.515 --> 00:15:47.111
更艱深的挑戰在於從三歲發展到十三歲，
甚至到更遠的階段。

00:15:47.111 --> 00:15:51.476
讓我用這張男孩與蛋糕的圖片來進一步說明，

00:15:51.476 --> 00:15:55.540
直到今日，我們已經教會了電腦識別物品，

00:15:55.540 --> 00:15:59.998
甚至於在看到一張圖後，可以簡單地敘述。

00:15:59.998 --> 00:16:03.574
(電腦) 一個人和蛋糕坐在桌旁。

00:16:03.574 --> 00:16:06.204
(主講人) 這張照片其實蘊涵著更多的東西，

00:16:06.204 --> 00:16:08.474
不僅只有人和蛋糕。

00:16:08.474 --> 00:16:12.941
電腦看不出這是種特別的義式蛋糕，

00:16:12.941 --> 00:16:16.158
人們只有在復活節時才會做。

00:16:16.158 --> 00:16:19.363
這個男孩穿著他最心愛的T恤，

00:16:19.363 --> 00:16:23.333
是去雪梨玩的時候，他的父親送的，

00:16:23.333 --> 00:16:27.141
各位和我都可以看得出他有多快樂，

00:16:27.141 --> 00:16:30.344
以及當時他的心裡在想什麼。

00:16:31.214 --> 00:16:34.339
這是我兒子，李奧。

00:16:34.339 --> 00:16:36.963
在探索智能視覺的旅途上，

00:16:36.963 --> 00:16:39.354
我不斷地想到他，

00:16:39.354 --> 00:16:42.257
以及他在將來生活的世界，

00:16:42.257 --> 00:16:44.278
當未來，機器有了視覺，

00:16:44.278 --> 00:16:48.990
醫生和護士就多了雙永不倦怠的眼睛，

00:16:48.990 --> 00:16:53.082
幫助他們診斷及照顧病人；

00:16:53.082 --> 00:16:57.465
行駛在路上的車子可以更聰明、更安全；

00:16:57.465 --> 00:17:00.159
人類與機器人能一起

00:17:00.159 --> 00:17:05.008
共同投入災區的救援工作，拯救受困人員及傷者；

00:17:05.798 --> 00:17:09.594
我們還可以發現新品種
與更好的材料，

00:17:09.594 --> 00:17:14.103
探索未知的疆界，
這一切都可仰賴機器的協助。

00:17:15.113 --> 00:17:19.280
一步一步地，我們賦予機器視覺，

00:17:19.280 --> 00:17:22.078
先教他們識別物品，

00:17:22.078 --> 00:17:24.841
然後它們也讓我們看得更清楚，

00:17:24.841 --> 00:17:29.006
這是第一次人類的眼睛不是唯一

00:17:29.006 --> 00:17:31.940
可以用來思考和探索世界的工具，

00:17:31.940 --> 00:17:35.400
我們不僅可以利用機器的智能，

00:17:35.400 --> 00:17:41.579
更可以運用更多你想像不到的方式攜手合作。

00:17:41.579 --> 00:17:43.740
這是我想追求的目標：

00:17:43.740 --> 00:17:46.452
給予機器智慧之眼，

00:17:46.452 --> 00:17:51.583
為李奧和整個世界創造更美好的未來。

00:17:51.583 --> 00:17:53.394
謝謝各位。

00:17:53.394 --> 00:17:57.179
(觀眾鼓掌)

