WEBVTT
Kind: captions
Language: fa

00:00:00.000 --> 00:00:07.000
Translator: Safoora Yousefi
Reviewer: soheila Jafari

00:00:15.260 --> 00:00:17.260
قدرت.

00:00:17.260 --> 00:00:19.260
این کلمهء است به ذهن خطور می‌کنه.

00:00:19.260 --> 00:00:21.260
ما فن‌آورانِ مدرن هستیم.

00:00:21.260 --> 00:00:24.260
داده‌های زیادی داریم، پس قدرت زیادی داریم.

00:00:24.260 --> 00:00:26.260
ما چه قدر قدرت داریم؟

00:00:26.260 --> 00:00:29.260
صحنه‌ای از یک فیلم: «اینک آخرالزمان»-- فیلم فوق‌العاده‌ایه.

00:00:29.260 --> 00:00:32.260
باید قهرمان‌مون، کاپیتان هیلارد رو به دهانه‌ی رود نانگ برسونیم

00:00:32.260 --> 00:00:34.260
تا بتونه کلنل کرتز رو تعقیب کنه.

00:00:34.260 --> 00:00:36.260
قراره کاپیتان رو از راه هوایی به اون‌جا برسونیم و پایین بندازیمش.

00:00:36.260 --> 00:00:38.260
صحنه این طوریه:

00:00:38.260 --> 00:00:41.260
آسمان مملوء از هلیکوپترهایی که او رو حمل می‌کنن.

00:00:41.260 --> 00:00:43.260
و موسیقی بلند و شورانگیزی در پس‌زمینه وجود داره.

00:00:43.260 --> 00:00:45.260
یک موسیقی طوفانی:

00:00:45.260 --> 00:00:47.260
♫دام دا تا دا دام♫

00:00:47.260 --> 00:00:49.260
♫دام دا تا دا دام♫

00:00:49.260 --> 00:00:52.260
♫دا تا دا دا♫

00:00:52.260 --> 00:00:54.260
یک عالم قدرت.

00:00:54.260 --> 00:00:56.260
این همون قدرتیه که من در این سالن احساس می‌کنم.

00:00:56.260 --> 00:00:58.260
این قدرتیه که ما داریم

00:00:58.260 --> 00:01:00.260
به خاطر همه‌ی داده‌هایی که داریم.

00:01:00.260 --> 00:01:02.260
بذارید یک مثال بزنم.

00:01:02.260 --> 00:01:04.260
با اطلاعاتِ تنها یک فرد

00:01:04.260 --> 00:01:07.260
چه کارهایی می‌تونیم بکنیم؟

00:01:07.260 --> 00:01:09.260
با اطلاعات اون آقا

00:01:09.260 --> 00:01:11.260
چه کار می‌تونیم بکنیم؟

00:01:11.260 --> 00:01:13.260
می‌تونم به سوابق مالی‌ت نگاه کنم.

00:01:13.260 --> 00:01:15.260
اون‌وقت می‌فهمم که قبض‌هاتو به موقع پرداخت می‌کنی یا نه.

00:01:15.260 --> 00:01:17.260
می‌فهمم شخص مناسبی برای وام دادن هستی یا نه.

00:01:17.260 --> 00:01:20.260
می‌تونم به سوابق پزشکی‌ت نگاه کنم، اون‌وقت می‌فهمم قلبِ‌ت هنوز می‌تپه یا نه--

00:01:20.260 --> 00:01:23.260
می‌فهمم پیش‌نهاد بیمه به تو دادن کار مناسبیه یا نه.

00:01:23.260 --> 00:01:25.260
می‌تونم به الگوی کلیک کردنِ‌ت نگاه کنم.

00:01:25.260 --> 00:01:28.260
وقتی به وب‌سایتَ‌م میای، می‌دونم که قصدِ چه کاری رو داری،

00:01:28.260 --> 00:01:30.260
چون می‌دونم میلیون‌ها وب سایتی که قبلا" سرزدی چه سایت‌هایی بودن.

00:01:30.260 --> 00:01:32.260
و متأسفم که اینو می‌گم،

00:01:32.260 --> 00:01:34.260
تو مثل یه بازی‌کنِ پوکری.

00:01:34.260 --> 00:01:36.260
من با تحلیل داده‌ها می‌تونم قبل از این که کاری رو انجام بدی،

00:01:36.260 --> 00:01:38.260
پیش‌بینی کنم که می‌خوای انجامش بدی.

00:01:38.260 --> 00:01:41.260
می‌دونم چی دوست داری. می‌دونم کی هستی.

00:01:41.260 --> 00:01:43.260
و تازه اینا قبل از دیدن ایمیل‌ها

00:01:43.260 --> 00:01:45.260
یا تلفنِته.

00:01:45.260 --> 00:01:47.260
این‌ها کارایی هستن که ما می‌تونیم با

00:01:47.260 --> 00:01:50.260
داده‌هایی که داریم انجام بدیم.

00:01:50.260 --> 00:01:53.260
ولی من این‌جا نیومدم که درباره‌ی کارهایی که می‌تونیم انجام بدیم صحبت کنم.

00:01:56.260 --> 00:01:59.260
اومدم تا درباره‌ی کاری که باید بکنیم صحبت کنیم.

00:02:00.260 --> 00:02:03.260
کارِ درست چیه؟

00:02:04.260 --> 00:02:06.260
می‌بینم که نگاه‌هاتون متحیره، انگار دارید می‌گید:

00:02:06.260 --> 00:02:09.260
«چرا از ما می‌پرسی کار درست چیه؟

00:02:09.260 --> 00:02:12.260
ما صرفا" سیستم رو می‌سازیم، یه نفر دیگه داره ازش استفاده می‌کنه.»

00:02:12.260 --> 00:02:15.260
منطقیه.

00:02:15.260 --> 00:02:17.260
ولی این منو برمی‌گردونه به گذشته.

00:02:17.260 --> 00:02:19.260
به جنگ جهانی دوم فکر می‌کنم.

00:02:19.260 --> 00:02:21.260
بعضی از فن‌آوران ما در اون زمان،

00:02:21.260 --> 00:02:23.260
بعضی از فیزیکدان‌هامون،

00:02:23.260 --> 00:02:25.260
که درباره‌ء شکافت و همجوشی هسته‌ای مطالعه می‌کردند --

00:02:25.260 --> 00:02:27.260
همون مسائل هسته‌ای.

00:02:27.260 --> 00:02:30.260
ما این فیزیکدان‌ها رو در لوس آلاموس دور هم جمع می‌کنیم

00:02:30.260 --> 00:02:33.260
تا ببینیم چه چیزی خواهند ساخت.

00:02:33.260 --> 00:02:36.260
ما از سازنده‌گان تکنولوژی انتظار داریم

00:02:36.260 --> 00:02:39.260
به این که با این تکنولوژی چه کار باید کرد فکر کنند.

00:02:41.260 --> 00:02:44.260
خوب حالا ما با اطلاعات اون شخص چه کار باید بکنیم؟

00:02:44.260 --> 00:02:47.260
باید جمعشون کنیم تا بتونیم

00:02:47.260 --> 00:02:49.260
تجربه‌ء آنلاینِ او رو بهبود ببخشیم؟

00:02:49.260 --> 00:02:51.260
تا پول بیش‌تری به دست بیاریم؟

00:02:51.260 --> 00:02:53.260
تا اگه او خیالاتِ بدی داشت

00:02:53.260 --> 00:02:55.260
بهتر بتونیم از خودمون محافظت کنیم؟

00:02:55.260 --> 00:02:58.260
یا این که به حریم خصوصی‌ش احترام بذاریم،

00:02:58.260 --> 00:03:01.260
شأنِ‌ش رو حفظ کنیم و راحتش بذاریم؟

00:03:02.260 --> 00:03:05.260
کدومش درسته؟

00:03:05.260 --> 00:03:07.260
از کجا بفهمیم؟

00:03:07.260 --> 00:03:10.260
من می‌گم رأی‌گیری کنیم.

00:03:11.260 --> 00:03:14.260
اول بذارید برای دستگرمی،

00:03:14.260 --> 00:03:16.260
با یه سوال آسون شروع کنیم--

00:03:16.260 --> 00:03:19.260
یه چیزی که مطمئنم همه یه نظری درباره‌ش دارن:

00:03:19.260 --> 00:03:21.260
آی-فون در مقابل اندروید.

00:03:21.260 --> 00:03:24.260
دست‌هاتونو بالا ببرید. آی-فون.

00:03:24.260 --> 00:03:26.260
آ-اوه.

00:03:26.260 --> 00:03:29.260
اندروید.

00:03:29.260 --> 00:03:31.260
آدم از افراد باذکاوتی مثل شما انتظار داره

00:03:31.260 --> 00:03:33.260
ولعِ گوشی‌های صرفا" خوشگل رو نداشته‌باشن!

00:03:33.260 --> 00:03:35.260
(خنده‌ی حضار)

00:03:35.260 --> 00:03:37.260
سوال بعدی،

00:03:37.260 --> 00:03:39.260
یه کم سخت تره.

00:03:39.260 --> 00:03:41.260
آیا ما باید همه‌ی داده‌های مربوط به اون شخص رو جمع کنیم

00:03:41.260 --> 00:03:43.260
تا تجربه‌ش رو بهبود ببخشیم

00:03:43.260 --> 00:03:46.260
و در صورتی که او خیالاتِ بدی داره از خودمون محافظت کنیم؟

00:03:46.260 --> 00:03:48.260
یا باید راحتش بذاریم؟

00:03:48.260 --> 00:03:51.260
اطلاعاتِ‌ش رو جمع کنیم.

00:03:53.260 --> 00:03:56.260
راحتش بذاریم.

00:03:56.260 --> 00:03:58.260
کاری‌تون ندارم! نتایج خوب بود.

00:03:58.260 --> 00:04:00.260
(خنده)

00:04:00.260 --> 00:04:02.260
خوب، سوال آخر--

00:04:02.260 --> 00:04:04.260
سوال سخت‌ تریه.

00:04:04.260 --> 00:04:07.260
وقتی می‌خوایم گزینه‌هامونو درباره‌ی سوال قبلی

00:04:07.260 --> 00:04:10.260
ارزیابی کنیم،

00:04:10.260 --> 00:04:14.260
آیا باید از چهارچوب اخلاقیِ وظیفه‌گرایانه‌ی کانت استفاده کنیم،

00:04:14.260 --> 00:04:17.260
یا از چهارچوب نتیجه‌گرایانه‌ی میل؟

00:04:19.260 --> 00:04:22.260
کانت.

00:04:22.260 --> 00:04:25.260
میل.

00:04:25.260 --> 00:04:27.260
چندان نظری ندارید.

00:04:27.260 --> 00:04:30.260
(خنده‌ی حضار)

00:04:30.260 --> 00:04:33.260
بله، این نتیجه وحشت‌ناکه.

00:04:34.260 --> 00:04:38.260
وحشت‌ناکه، چون دیدگاه‌هامون درباره‌ی

00:04:38.260 --> 00:04:40.260
دست‌گاه‌هامون بیش‌تر شکل گرفته

00:04:40.260 --> 00:04:42.260
تا درباره‌ی چهارچوب اخلاقی‌‌ای که

00:04:42.260 --> 00:04:44.260
تصمی‌گیری‌هامونو هدایت می‌کنه.

00:04:44.260 --> 00:04:47.260
اگه چهارچوب اخلاقی مشخصی نداشته‌باشیم

00:04:47.260 --> 00:04:50.260
از کجا بفهمیم که از قدرتِ‌مون چه طور استفاده کنیم؟

00:04:50.260 --> 00:04:53.260
ما درباره‌ی سیستم عامل‌های موبایل بیش‌تر می‌دونیم،

00:04:53.260 --> 00:04:56.260
ولی چیزی که واقعن به‌ش نیاز داریم یک سیستم عامل اخلاق-مداره.

00:04:58.260 --> 00:05:00.260
سیستم عامل اخلاق-مدار چیه؟

00:05:00.260 --> 00:05:02.260
ما همه خوب و بد رو می‌شناسیم. درست.

00:05:02.260 --> 00:05:04.260
وقتی کار خوبی می‌کنیم حس خوبی داریم،

00:05:04.260 --> 00:05:06.260
و وقتی کار بدی می‌کنیم حس بدی داریم.

00:05:06.260 --> 00:05:09.260
پدر و مادرهامون این رو به‌مون یاد می‌دن: تحسین به خاطر کار خوب و سرزنش به خاطر کار بد.

00:05:09.260 --> 00:05:12.260
ولی از کجا بفهمیم چی خوبه و چی بد؟

00:05:12.260 --> 00:05:15.260
در زنده‌گی روزمره این تکنیک‌ها رو استفاده می‌کنیم:

00:05:15.260 --> 00:05:18.260
گاهی به حسّ‌مون تکیه می‌کنیم.

00:05:18.260 --> 00:05:21.260
گاهی رأی‌گیری می‌کنیم.

00:05:21.260 --> 00:05:23.260
یا شاید به مراجع قانونی

00:05:23.260 --> 00:05:26.260
مراجعه کنیم و ببینیم چی می‌گن.

00:05:26.260 --> 00:05:28.260
به عبارت دیگه، روش ما برای پیدا کردنِ

00:05:28.260 --> 00:05:30.260
کاری که باید بکنیم، به نوعی تصادفی

00:05:30.260 --> 00:05:33.260
یا وابسته به شرایطه.

00:05:33.260 --> 00:05:36.260
یا شاید، اگه بخوایم خیالِ‌مون راحت‌تر باشه،

00:05:36.260 --> 00:05:39.260
واقعا" به یک چهارچوب اخلاقی نیاز داریم که راهنمایی‌مون کنه،

00:05:39.260 --> 00:05:42.260
و بهمون بگه که در درجه‌ء اول چه چیزهای درست و غلط هستند،

00:05:42.260 --> 00:05:46.260
و از کجا بفهمیم که در یک وضعیت خاص چی کار کنیم.

00:05:46.260 --> 00:05:48.260
پس بیاید یک چهارچوب اخلاقی پیدا کنیم.

00:05:48.260 --> 00:05:51.260
ما آدم‌های عددی هستیم، با اعداد زندگی می‌کنیم.

00:05:51.260 --> 00:05:53.260
حالا چه طور به عنوان اساس چهارچوب اخلاقی‌مون

00:05:53.260 --> 00:05:56.260
از اعداد استفاده‌ کنیم؟

00:05:56.260 --> 00:05:59.260
من کسی رو می‌شناسم که دقیقا" این کار رو کرد،

00:05:59.260 --> 00:06:02.260
واقعا" مرد باهوشی بود.

00:06:02.260 --> 00:06:05.260
او 2500 ساله که مرده.

00:06:05.260 --> 00:06:07.260
پلاتو، درسته.

00:06:07.260 --> 00:06:09.260
اونو به یاد میارید؟ فیلسوفِ پیر؟

00:06:09.260 --> 00:06:12.260
سر اون کلاس خوابیده‌بودید.

00:06:12.260 --> 00:06:14.260
و پلاتو، دغدغه‌هایی داشت که دغدغه‌های ما هم بودند.

00:06:14.260 --> 00:06:16.260
او دغدغه‌ی درست و غلط رو داشت.

00:06:16.260 --> 00:06:18.260
می‌خواست بدونه عدالت چیه.

00:06:18.260 --> 00:06:20.260
اما نگرانی او این بود که به نظر می‌رسه

00:06:20.260 --> 00:06:22.260
ما داریم نظراتِ مون رو در این زمینه معامله می‌کنیم.

00:06:22.260 --> 00:06:25.260
یه نفر می‌گه این کار عادلانه‌ست، دیگری می‌گه یه کار دیگه عادلانه‌ست.

00:06:25.260 --> 00:06:27.260
و هر دوشون به شکل قانع‌کننده‌ای از خودشون دفاع می‌کنن.

00:06:27.260 --> 00:06:29.260
من فقط عقب و جلو می‌رم و درجا می‌زنم. به جایی نمی‌رسم.

00:06:29.260 --> 00:06:32.260
من نظرات رو نمی‌خوام، من دانش می‌خوام.

00:06:32.260 --> 00:06:35.260
من می‌خوام حقیقتِ عدالت رو بدونم--

00:06:35.260 --> 00:06:38.260
همون طور که حقایقی در ریاضیات داریم.

00:06:38.260 --> 00:06:41.260
در ریاضیات، ما واقعیت‌ها رو می‌دونیم.

00:06:41.260 --> 00:06:43.260
یه عدد در نظر بگیرید، هر عددی-- دو.

00:06:43.260 --> 00:06:45.260
عدد موردعلاقه‌ی من. عاشقِ این عددم.

00:06:45.260 --> 00:06:47.260
حقایقی درباره‌ی دو وجود داره.

00:06:47.260 --> 00:06:49.260
اگه دو تا از یک چیز داشته&lrm;‌ باشین،

00:06:49.260 --> 00:06:51.260
اگه دو تا دیگه بهش اضافه کنید، چهارتا می‌شه.

00:06:51.260 --> 00:06:53.260
این حقیقته و ربطی به چیزی که داری درباره اس می گویی ندارد.

00:06:53.260 --> 00:06:55.260
این یک حقیقت درباره‌ی دو است.

00:06:55.260 --> 00:06:57.260
شکل کلیِ دو.

00:06:57.260 --> 00:06:59.260
وقتی شما دوتا از هرچیزی دارید-- دو چشم، دو گوش، دو بینی،

00:06:59.260 --> 00:07:01.260
دو تا برآمده‌گی--

00:07:01.260 --> 00:07:04.260
این‌ها همه در شکل دهی به دو شرکت دارند.

00:07:04.260 --> 00:07:08.260
همه در حقیقتِ دو شرکت دارند.

00:07:08.260 --> 00:07:10.260
همه‌ی این‌ها، «دو بودن» رو در خودشون دارند.

00:07:10.260 --> 00:07:13.260
بنابراین، این ربطی به نظرات نداره.

00:07:13.260 --> 00:07:15.260
پلاتو فکر کرد، چی‌ می‌شه اگه

00:07:15.260 --> 00:07:17.260
اخلاق هم همین طور باشه؟

00:07:17.260 --> 00:07:20.260
چی می‌شه اگه یک شکل مطلق از عدالت وجود داشته‌باشه؟

00:07:20.260 --> 00:07:22.260
شاید حقایقی درباره‌ی عدالت وجود دارند،

00:07:22.260 --> 00:07:24.260
و ما می‌تونیم به اطرافِ&lrm; مون نگاه کنیم و

00:07:24.260 --> 00:07:26.260
بفهمیم چه چیزهایی در این حقایق شرکت دارند،

00:07:26.260 --> 00:07:29.260
و به اون‌ها شکل می‌دهند؟

00:07:29.260 --> 00:07:32.260
این طوری می‌فهمیدیم که واقعا" چی عادلانه‌ ست و چی نیست.

00:07:32.260 --> 00:07:34.260
اون وقت دیگه ربطی به نظرات یا

00:07:34.260 --> 00:07:37.260
ظواهر امور نداشت.

00:07:37.260 --> 00:07:39.260
دیدگاه شگفت‌انگیزیه.

00:07:39.260 --> 00:07:42.260
منظورم اینه که، بهش فکر کنید. چه قدر بزرگ. چه قدر بلندپروازانه.

00:07:42.260 --> 00:07:44.260
بلندپرواز، مثل ما.

00:07:44.260 --> 00:07:46.260
او می‌خواد راه حلی برای اخلاق پیدا کنه.

00:07:46.260 --> 00:07:48.260
به دنبال حقائق مطلقه.

00:07:48.260 --> 00:07:51.260
اگر این طوری فکر کنیم،

00:07:51.260 --> 00:07:54.260
یک چهارچوبِ اخلاقیِ پلاتویی داریم.

00:07:54.260 --> 00:07:56.260
اگر این طوری فکر نکنیم،

00:07:56.260 --> 00:07:58.260
خوب، در این صورت در فلسفه‌ی غرب خیلی ها با ما موافق هستند.

00:07:58.260 --> 00:08:01.260
چون این ایده خیلی تروتمیزه-- دیگران ازش انتقاد کردند.

00:08:01.260 --> 00:08:04.260
مخصوصا" ارسطو اصلا" از این ایده خوشش نیومد.

00:08:04.260 --> 00:08:07.260
چون فکر می‌کرد عملی نیست.

00:08:07.260 --> 00:08:11.260
ارسطو می&lrm;گفت «در هر موضوع، ما تنها به اندازه‌ای دقت دست می‌یابیم

00:08:11.260 --> 00:08:13.260
که خودِ موضوع بهمون اجازه می‌ده».

00:08:13.260 --> 00:08:16.260
ارسطو فکر نمی‌کرد شباهت زیادی بین اخلاق و ریاضیات وجود داره.

00:08:16.260 --> 00:08:19.260
او فکر می‌کرد موضوعِ اخلاق، تصمیم‌گیری در شرایط فعلی

00:08:19.260 --> 00:08:21.260
با استفاده از بهترین داوری

00:08:21.260 --> 00:08:23.260
برای پیدا کردنِ راه درسته.

00:08:23.260 --> 00:08:25.260
اگر شما این طور فکر می‌کنید، پلاتو طرفِ شما نیست.

00:08:25.260 --> 00:08:27.260
اما هنوز تموم نشده.

00:08:27.260 --> 00:08:29.260
شاید راه دیگه‌ای وجود داره

00:08:29.260 --> 00:08:32.260
که بتونیم اعداد رو مبنای چهارچوب اخلاقی‌مون قرار بدیم.

00:08:33.260 --> 00:08:35.260
این یکی چه طوره:

00:08:35.260 --> 00:08:38.260
شاید بتونیم توی هر وضعیتی حساب و کتاب کنیم،

00:08:38.260 --> 00:08:40.260
به گزینه‌هامون نگاه کنیم،

00:08:40.260 --> 00:08:43.260
بسنجیم که کدوم بهتره و بفهمیم که چی کار بکنیم.

00:08:43.260 --> 00:08:45.260
آشنا به نظرتون میاد؟

00:08:45.260 --> 00:08:48.260
این یک چهارچوب سودگرایانه‌ست.

00:08:48.260 --> 00:08:50.260
جان استوارت میل طرفدار بزرگِ این چهارچوب بود--

00:08:50.260 --> 00:08:52.260
مرد خوبی هم بود--

00:08:52.260 --> 00:08:54.260
و فقط 200 ساله که مُرده.

00:08:54.260 --> 00:08:56.260
پس اساسِ سودگرایی--

00:08:56.260 --> 00:08:58.260
مطمئنم که یه چیزهایی درباره‌ش شنیدید.

00:08:58.260 --> 00:09:00.260
اون سه نفری که به میل رأی دادند حتما" می‌دونند که

00:09:00.260 --> 00:09:02.260
این چهارچوب این طوریه:

00:09:02.260 --> 00:09:05.260
یک کار اخلاقیه اگر

00:09:05.260 --> 00:09:07.260
لذت رو به حداکثر برسونه،

00:09:07.260 --> 00:09:09.260
و رنج رو به حداقل.

00:09:09.260 --> 00:09:12.260
به ذات اون کار بستگی داره.

00:09:12.260 --> 00:09:14.260
نه به ارتباطش با یک مطلقِ خارجی.

00:09:14.260 --> 00:09:16.260
فقط به عواقب اون کار بستگی داره.

00:09:16.260 --> 00:09:18.260
فقط به عواقب توجه می‌کنی

00:09:18.260 --> 00:09:20.260
و تصمیم می‌گیری که جمعا" به نفعه یا به ضرر.

00:09:20.260 --> 00:09:22.260
ساده‌ست. این طوری می‌فهمیم که چی کار کنیم.

00:09:22.260 --> 00:09:24.260
بیاید به یه مثال بپردازیم.

00:09:24.260 --> 00:09:26.260
فرض کنید که من بگم

00:09:26.260 --> 00:09:28.260
«می‌خوام تلفن شما رو بگیرم».

00:09:28.260 --> 00:09:30.260
نه به خاطر این که زنگ زد،

00:09:30.260 --> 00:09:33.260
بلکه چون من کمی حساب و کتاب کردم.

00:09:33.260 --> 00:09:36.260
فکر کردم که این آقا مشکوک به نظر می‌رسه.

00:09:36.260 --> 00:09:39.260
شاید داشته به بن لادن پیغام می‌داده--

00:09:39.260 --> 00:09:41.260
یا به هر کسی که بعد از بن لادن کارها رو دست گرفته--

00:09:41.260 --> 00:09:44.260
و در واقع این آقا تروریست به نظر میاد.

00:09:44.260 --> 00:09:47.260
من باید مطمئن بشم. و اگر حدس من درست باشه،

00:09:47.260 --> 00:09:50.260
جلوی خسارت بزرگی که اون آقا می‌تونست ایجاد کنه گرفته شده.

00:09:50.260 --> 00:09:53.260
جلوگیری از اون خسارت سود خیلی بزرگیه.

00:09:53.260 --> 00:09:55.260
و در مقایسه با رنج کوچیکی که این کار ایجاد می‌کنه--

00:09:55.260 --> 00:09:57.260
چون به هر حال خجالت‌آوره که من تلفن اونو نگاه کنم

00:09:57.260 --> 00:10:00.260
و بفهمم که با فارم ویل درگیره--

00:10:00.260 --> 00:10:03.260
این رنج کوچک، در مقابل سودِ نگاه کردن

00:10:03.260 --> 00:10:05.260
به تلفن، اصلن به چشم نمیاد.

00:10:05.260 --> 00:10:07.260
اگر شما چنین حسی دارید،

00:10:07.260 --> 00:10:10.260
انتخابِ شما سودگرایانه‌ست.

00:10:10.260 --> 00:10:13.260
اما شاید چنین حسی نداشته‌باشید.

00:10:13.260 --> 00:10:15.260
شاید فکر کنید اون صاحب تلفنه.

00:10:15.260 --> 00:10:17.260
گرفتن تلفنش کار نادرستیه،

00:10:17.260 --> 00:10:19.260
چون اون یک انسانه

00:10:19.260 --> 00:10:21.260
و حقوق و حریمی داره،

00:10:21.260 --> 00:10:23.260
و ما نمی‌تونیم این قدر ساده به حریمِ‌ش تجاوز کنیم.

00:10:23.260 --> 00:10:25.260
اون اختیار خودشو داره.

00:10:25.260 --> 00:10:27.260
مهم نیست محاسباتِ ما چی می‌گن.

00:10:27.260 --> 00:10:30.260
بعضی چیزها ذاتن نادرست هستند--

00:10:30.260 --> 00:10:32.260
مثلن دروغ نادرسته،

00:10:32.260 --> 00:10:35.260
مثلن شکنجه‌ی کودکان معصوم نادرسته.

00:10:35.260 --> 00:10:38.260
کانت اینجا حرف خوبی می‌زنه.

00:10:38.260 --> 00:10:40.260
التبه خودش کمی بهتر از من بیانش کرده.

00:10:40.260 --> 00:10:42.260
او می‌گه ما باید با استفاده از استدلال

00:10:42.260 --> 00:10:45.260
قوانینی رو بفهمیم که باهاشون رفتارمون رو هدایت کنیم.

00:10:45.260 --> 00:10:48.260
بعد از اون، وظیفه‌ی ماست که از اون قوانین پیروی کنیم.

00:10:48.260 --> 00:10:51.260
به محاسبات ربطی نداره.

00:10:51.260 --> 00:10:53.260
خوب دیگه کافیه.

00:10:53.260 --> 00:10:56.260
حالا ما در قلب جنگلِ فلسفه هستیم.

00:10:56.260 --> 00:10:59.260
و این تا هزاران سال ادامه داره،

00:10:59.260 --> 00:11:01.260
چون این‌ها سوال‌های سختی هستند،

00:11:01.260 --> 00:11:03.260
و من فقط پانزده دقیقه وقت دارم.

00:11:03.260 --> 00:11:05.260
بریم سر اصل مطلب.

00:11:05.260 --> 00:11:09.260
چه طور تصمیم‌گیری کنیم؟

00:11:09.260 --> 00:11:12.260
پلاتو خوبه، یا ارسطو، یا کانت، یا میل؟

00:11:12.260 --> 00:11:14.260
چه کار کنیم؟ پاسخ چیه؟

00:11:14.260 --> 00:11:17.260
کدوم فرمول می‌تونه در هر موقعیتی،

00:11:17.260 --> 00:11:19.260
کار درست رو مشخص کنه،

00:11:19.260 --> 00:11:21.260
و بگه که آیا ما باید از اطلاعات اون شخص استفاده کنیم یا نه؟

00:11:21.260 --> 00:11:24.260
اون فرمول چیه؟

00:11:25.260 --> 00:11:27.260
هیچ فرمولی وجود نداره.

00:11:29.260 --> 00:11:31.260
پاسخ ساده‌ای به این سوال وجود نداره.

00:11:31.260 --> 00:11:34.260
اخلاق سخته.

00:11:34.260 --> 00:11:37.260
اخلاق مستلزم فکر کردنه.

00:11:38.260 --> 00:11:40.260
و ای کار سخته.

00:11:40.260 --> 00:11:42.260
می‌دونم، بخش عمده‌ی شغل من

00:11:42.260 --> 00:11:44.260
مربوط به هوش مصنوعیه،

00:11:44.260 --> 00:11:47.260
در تلاش برای ساختن ماشین‌هایی که بتونن کمی به جای ما فکر کنند،

00:11:47.260 --> 00:11:49.260
و بتونن جواب‌هایی به ما بدن.

00:11:49.260 --> 00:11:51.260
ولی نمی‌تونن.

00:11:51.260 --> 00:11:53.260
نمی‌شه همین طوری تفکر انسان رو برداشت

00:11:53.260 --> 00:11:55.260
و در یک ماشین قرار داد.

00:11:55.260 --> 00:11:58.260
ما خودمون باید این کار رو بکنیم.

00:11:58.260 --> 00:12:01.260
خوشبختانه ما ماشین نیستیم و می‌تونیم این کار رو بکنیم.

00:12:01.260 --> 00:12:03.260
ما نه تنها می‌تونیم فکر کنیم،

00:12:03.260 --> 00:12:05.260
ما باید( فکر کنیم ).

00:12:05.260 --> 00:12:07.260
هانا آرنت می‌گه:

00:12:07.260 --> 00:12:09.260
«حقیقتِ تلخ اینه که

00:12:09.260 --> 00:12:11.260
بیش‌تر کارهایی بدی که در دنیا انجام می‌شه

00:12:11.260 --> 00:12:13.260
به دست آدم‌هایی اتفاق میفته که

00:12:13.260 --> 00:12:15.260
قصدِ بدی ندارند.

00:12:15.260 --> 00:12:18.260
و این نتیجه‌ی فکر نکردنه.»

00:12:18.260 --> 00:12:22.260
اون اسم این رو «پیش‌پاافتاده‌گیِ بدی» می‌ذاره.

00:12:22.260 --> 00:12:24.260
و پاسخش اینه که

00:12:24.260 --> 00:12:26.260
از هر انسانِ باشعوری انتظار فکر کردن

00:12:26.260 --> 00:12:29.260
داشته‌باشیم.

00:12:29.260 --> 00:12:31.260
خوب بیاید این کارو بکنیم. بیاید فکر کنیم.

00:12:31.260 --> 00:12:34.260
بیاید همین الان شروع کنیم.

00:12:34.260 --> 00:12:37.260
همه‌ی حضار این کار رو بکنند:

00:12:37.260 --> 00:12:40.260
به آخرین باری که می‌خواستید تصمیم‌گیری کنید فکر کنید

00:12:40.260 --> 00:12:42.260
آخرین باری که دغدغه‌ی انجام کار درست رو داشتید،

00:12:42.260 --> 00:12:44.260
و از خودتون پرسیدید که «چی کار باید بکنم؟»

00:12:44.260 --> 00:12:46.260
اون موقعیت رو به یاد بیارید.

00:12:46.260 --> 00:12:48.260
و حالا بهش فکر کنید.

00:12:48.260 --> 00:12:51.260
از خودتون بپرسید: «چه طور اون تصمیم رو گرفتم؟

00:12:51.260 --> 00:12:54.260
چی کار کردم؟ از حسَّ‌م پیروی کردم؟

00:12:54.260 --> 00:12:56.260
رأی‌گیری کردم؟ یا به قانون مراجعه کردم؟»

00:12:56.260 --> 00:12:59.260
البته حالا چندتا گزینه‌ی دیگه هم داریم.

00:12:59.260 --> 00:13:01.260
«آیا محاسبه کردم تا ببینم کدوم کار بیشترین لذت رو داره

00:13:01.260 --> 00:13:03.260
مثل کاری که میل کرد؟

00:13:03.260 --> 00:13:06.260
یا مثل کانت، استدلال کردم تا بفهمم چه کاری ذاتن درسته؟»

00:13:06.260 --> 00:13:09.260
بهش فکر کنید. سعی کنید به یادش بیارید. مهمه.

00:13:09.260 --> 00:13:11.260
اون قدر مهمه که

00:13:11.260 --> 00:13:13.260
می‌خوایم 30 ثانیه از وقت ارزشمند تد تاک رو

00:13:13.260 --> 00:13:15.260
فقط به این فکر کنیم.

00:13:15.260 --> 00:13:17.260
آماده‌اید؟ شروع کنید.

00:13:33.260 --> 00:13:36.260
کافیه. خسته نباشید.

00:13:36.260 --> 00:13:38.260
کاری که الان شما کردید،

00:13:38.260 --> 00:13:40.260
اولین قدمه برای پذیرفتنِ مسئولیتِ

00:13:40.260 --> 00:13:43.260
کاری که می‌تونیم با این همه قدرتِ‌مون انجام بدیم.

00:13:45.260 --> 00:13:48.260
حالا قدمِ بعدی-- اینو امتحان کنید.

00:13:49.260 --> 00:13:51.260
برای یکی از دوست‌هاتون توضیح بدید

00:13:51.260 --> 00:13:53.260
که چه طور اون تصمیم رو گرفتید.

00:13:53.260 --> 00:13:55.260
الان نه. صبر کنید حرف‌های من تموم شه.

00:13:55.260 --> 00:13:57.260
موقع ناهار این کار رو بکنید.

00:13:57.260 --> 00:14:00.260
دوستی که برای این کار انتخاب می‌کنید، مثل خودتون فنّی نباشه.

00:14:00.260 --> 00:14:02.260
کسی رو انتخاب کنید که با شما فرق داشته‌باشه.

00:14:02.260 --> 00:14:04.260
یه هنرمند یا نویسنده--

00:14:04.260 --> 00:14:07.260
یا، خدای ناکرده، یه فیلسوف بهتره.

00:14:07.260 --> 00:14:09.260
در واقع، سعی کنید رشته‌ش علوم انسانی باشه.

00:14:09.260 --> 00:14:11.260
چرا؟ چون اون‌ها نگاه‌شون به مسائل

00:14:11.260 --> 00:14:13.260
با نگاهِ ما فنّی‌ها فرق داره.

00:14:13.260 --> 00:14:16.260
همین چند روز پیش، دقیقا" اون طرف خیابون،

00:14:16.260 --> 00:14:18.260
صدها نفر تجمع کرده‌بودند.

00:14:18.260 --> 00:14:20.260
فنی‌ها و علوم انسانی‌ها برای

00:14:20.260 --> 00:14:22.260
کنفرانس بیبلیوتک اومده‌ بودند.

00:14:22.260 --> 00:14:24.260
اونا دور هم جمع شده‌بودند

00:14:24.260 --> 00:14:26.260
چون فنی‌ها می‌خواستند بدونند

00:14:26.260 --> 00:14:29.260
فکر کردن از چشم‌انداز به علوم انسانی چه طوریه.

00:14:29.260 --> 00:14:31.260
فرض کنید یک نفر از گوگل

00:14:31.260 --> 00:14:33.260
با یک نفر در رشته‌ی ادبیات قیاسی صحبت می‌کنه.

00:14:33.260 --> 00:14:36.260
حتما" دارید فکر می‌کنید که تئاتر قرن ۱۷ فرانسه

00:14:36.260 --> 00:14:38.260
چه ربطی به سرمایه‌گذاری داره؟

00:14:38.260 --> 00:14:41.260
این طوری جالب می‌شه. این یک طرز تفکرِ متفاوته.

00:14:41.260 --> 00:14:43.260
و وقتی این طوری فکر کنید،

00:14:43.260 --> 00:14:46.260
نسبت به ملاحظات انسانی که در تصمیم‌گیری‌های اخلاقی

00:14:46.260 --> 00:14:49.260
نقش مهمی دارند، حساس‌تر می‌شید.

00:14:49.260 --> 00:14:51.260
همین حالا تصور کنید

00:14:51.260 --> 00:14:53.260
که پیش دوستِ موسیقی‌دانتون می‌رید.

00:14:53.260 --> 00:14:56.260
و این حرف‌ها رو براش توضیح می‌دید،

00:14:56.260 --> 00:14:58.260
انقلاب اطلاعات و بقیه‌ی چیزها--

00:14:58.260 --> 00:15:00.260
و شاید کمی هم موسیقی متنمون رو براش بزنید.

00:15:00.260 --> 00:15:03.260
♫ دام تا دا دا دام دام تا دا دا دام ♫

00:15:03.260 --> 00:15:05.260
دوست موسیقی‌دانتون می‌پره وسط آهنگ و می‌گه

00:15:05.260 --> 00:15:07.260
«می‌دونی؟ موسیقی متن

00:15:07.260 --> 00:15:09.260
برای انقلاب اطلاعاتی شما،

00:15:09.260 --> 00:15:11.260
یه اپراست. ونگره.

00:15:11.260 --> 00:15:13.260
اساسش افسانه‌ی نورسه.

00:15:13.260 --> 00:15:15.260
به الهه‌ها و موجودات اسطوره‌ای می‌خوره

00:15:15.260 --> 00:15:18.260
که برای جوهرات جادویی با هم می‌جنگن»

00:15:19.260 --> 00:15:22.260
جالبه.

00:15:22.260 --> 00:15:25.260
حالا قضیه به یه اپرای زیبا هم تبدیل شد.

00:15:25.260 --> 00:15:28.260
و ما با این اپرا انگیزه می‌گیریم.

00:15:28.260 --> 00:15:30.260
انگیزه می‌گیریم،‌ چون داستان،

00:15:30.260 --> 00:15:32.260
داستانِ جنگ بین خوبی و بدی

00:15:32.260 --> 00:15:34.260
و درست و غلطه.

00:15:34.260 --> 00:15:36.260
و ما به درست و غلط اهمیت می‌دیم.

00:15:36.260 --> 00:15:39.260
برامون مهمه که در اون اپرا چه اتفاقی می‌افته.

00:15:39.260 --> 00:15:42.260
برامون مهمه که در «اینک آخرالزمان» چه اتفاقی می‌افته.

00:15:42.260 --> 00:15:44.260
و مخصوصن برامون مهمه که

00:15:44.260 --> 00:15:46.260
در فناوری‌مون چه اتفاقی می‌افته.

00:15:46.260 --> 00:15:48.260
ما امروز خیلی قدرتمندیم.

00:15:48.260 --> 00:15:51.260
تصمیم با خودمونه که چی کار کنیم.

00:15:51.260 --> 00:15:53.260
و این خبر خوبیه.

00:15:53.260 --> 00:15:56.260
ما این اپرا رو می‌نویسیم.

00:15:56.260 --> 00:15:58.260
این فیلمِ ماست.

00:15:58.260 --> 00:16:01.260
ما تعیین می‌کنیم که برای این فناوری چه اتفاقی می‌افته.

00:16:01.260 --> 00:16:04.260
ما تعیین می‌کنیم که آخر قصه چی می‌شه.

00:16:04.260 --> 00:16:06.260
متشکرم.

00:16:06.260 --> 00:16:11.260
(تشویق حضار)

