WEBVTT
Kind: captions
Language: fr

00:00:00.000 --> 00:00:07.000
Traducteur: Hugo Wagner
Relecteur: Timothée Parrique

00:00:15.260 --> 00:00:17.260
Le pouvoir.

00:00:17.260 --> 00:00:19.260
C'est le mot qui vient à l'esprit.

00:00:19.260 --> 00:00:21.260
Nous sommes les nouveaux technologues.

00:00:21.260 --> 00:00:24.260
Nous avons beaucoup de données, donc nous avons beaucoup de pouvoir.

00:00:24.260 --> 00:00:26.260
Quelle quantité de pouvoir avons-nous ?

00:00:26.260 --> 00:00:29.260
Une scène du film "Apocalypse Now" -- un grand film.

00:00:29.260 --> 00:00:32.260
Nous devons amener notre héros, le capitaine Willard, à l'embouchure de la rivière Nung,

00:00:32.260 --> 00:00:34.260
pour qu'il puisse poursuivre le colonel Kurtz.

00:00:34.260 --> 00:00:36.260
Pour cela, nous allons le transporter en hélico et le déposer.

00:00:36.260 --> 00:00:38.260
Dans cette scène :

00:00:38.260 --> 00:00:41.260
le ciel est rempli de cette flotte d'hélicoptères qui le transportent.

00:00:41.260 --> 00:00:43.260
Il y a cette musique bruyante et palpitante en fond,

00:00:43.260 --> 00:00:45.260
cette musique effrénée.

00:00:45.260 --> 00:00:47.260
♫ Dum da ta da dum ♫

00:00:47.260 --> 00:00:49.260
♫ Dum da ta da dum ♫

00:00:49.260 --> 00:00:52.260
♫ Da ta da da ♫

00:00:52.260 --> 00:00:54.260
C'est un grand pouvoir.

00:00:54.260 --> 00:00:56.260
C'est le genre de pouvoir que je ressens dans cette pièce.

00:00:56.260 --> 00:00:58.260
C'est le genre de pouvoir que nous avons

00:00:58.260 --> 00:01:00.260
grâce à toutes les données que nous possédons.

00:01:00.260 --> 00:01:02.260
Prenons un exemple.

00:01:02.260 --> 00:01:04.260
Que pouvons-nous faire

00:01:04.260 --> 00:01:07.260
avec les données d'une seule personne ?

00:01:07.260 --> 00:01:09.260
Que pouvons-nous faire

00:01:09.260 --> 00:01:11.260
avec les données de ce type ?

00:01:11.260 --> 00:01:13.260
Je peux regarder vos registres financiers.

00:01:13.260 --> 00:01:15.260
Je peux vous dire si vous payez vos factures à temps.

00:01:15.260 --> 00:01:17.260
Je sais alors si je peux vous faire un prêt.

00:01:17.260 --> 00:01:20.260
Je peux regarder votre dossier médical, je peux voir si votre cœur fonctionne bien --

00:01:20.260 --> 00:01:23.260
voir si je peux vous proposer une assurance.

00:01:23.260 --> 00:01:25.260
Je peux regarder où vous cliquez sur Internet.

00:01:25.260 --> 00:01:28.260
Quand vous visitez mon site, je sais déjà ce que vous allez faire,

00:01:28.260 --> 00:01:30.260
parce que je vous ai vu visiter des millions de sites auparavant.

00:01:30.260 --> 00:01:32.260
Je suis désolé de vous dire,

00:01:32.260 --> 00:01:34.260
vous êtes comme un joueur de poker, vous avez des manies.

00:01:34.260 --> 00:01:36.260
Je peux dire grâce à l'analyse de données ce que vous allez faire

00:01:36.260 --> 00:01:38.260
avant même que vous ne le fassiez.

00:01:38.260 --> 00:01:41.260
Je sais ce que vous aimez. Je sais qui vous êtes.

00:01:41.260 --> 00:01:43.260
Et cela avant même que je regarde votre courrier

00:01:43.260 --> 00:01:45.260
ou votre téléphone.

00:01:45.260 --> 00:01:47.260
Ce sont le genre de choses que nous pouvons faire

00:01:47.260 --> 00:01:50.260
avec les données que nous avons.

00:01:50.260 --> 00:01:53.260
Mais je ne suis pas là pour parler de ce que nous pouvons faire.

00:01:56.260 --> 00:01:59.260
Je suis ici pour parler de ce que nous devrions faire.

00:02:00.260 --> 00:02:03.260
Quelle est la bonne chose à faire ?

00:02:04.260 --> 00:02:06.260
Je vois des regards perplexes

00:02:06.260 --> 00:02:09.260
comme « Pourquoi nous demandez-vous quelle est la bonne chose à faire ?

00:02:09.260 --> 00:02:12.260
On ne fait que construire ce truc. Quelqu'un d'autre l'utilise. »

00:02:12.260 --> 00:02:15.260
Très bien.

00:02:15.260 --> 00:02:17.260
Mais ça me ramène en arrière.

00:02:17.260 --> 00:02:19.260
Je pense à la deuxième guerre mondiale --

00:02:19.260 --> 00:02:21.260
certains de nos grands technologues de l'époque,

00:02:21.260 --> 00:02:23.260
certains de nos grands physiciens,

00:02:23.260 --> 00:02:25.260
qui étudiaient la fission et la fusion nucléaires --

00:02:25.260 --> 00:02:27.260
juste des trucs nucléaires.

00:02:27.260 --> 00:02:30.260
Nous rassemblons ces physiciens ensemble à Los Alamos

00:02:30.260 --> 00:02:33.260
pour voir ce qu'ils vont construire.

00:02:33.260 --> 00:02:36.260
Nous voulons que les personnes qui construisent la technologie

00:02:36.260 --> 00:02:39.260
pensent à ce que nous devrions faire avec la technologie.

00:02:41.260 --> 00:02:44.260
Que devrions-nous donc faire avec les données de ce type ?

00:02:44.260 --> 00:02:47.260
Devrions-nous les collecter, les rassembler,

00:02:47.260 --> 00:02:49.260
pour optimiser son surf sur Internet ?

00:02:49.260 --> 00:02:51.260
Pour faire de l'argent ?

00:02:51.260 --> 00:02:53.260
Pour que nous puissions nous protéger

00:02:53.260 --> 00:02:55.260
s'il a de mauvaises intentions ?

00:02:55.260 --> 00:02:58.260
Ou devrions-nous respecter son intimité,

00:02:58.260 --> 00:03:01.260
protéger sa dignité et le laisser tranquille ?

00:03:02.260 --> 00:03:05.260
Que fait-on ?

00:03:05.260 --> 00:03:07.260
Comment décidons-nous ?

00:03:07.260 --> 00:03:10.260
Je sais : la participation citoyenne. Faisons du crowdsourcing.

00:03:11.260 --> 00:03:14.260
Pour donner de l'entrain aux gens,

00:03:14.260 --> 00:03:16.260
commençons par une question facile --

00:03:16.260 --> 00:03:19.260
une chose sur laquelle tout le monde ici a une idée, j'en suis sûr :

00:03:19.260 --> 00:03:21.260
iPhone contre Android.

00:03:21.260 --> 00:03:24.260
Levez vos mains pour l'iPhone.

00:03:24.260 --> 00:03:26.260
Uh huh.

00:03:26.260 --> 00:03:29.260
Android.

00:03:29.260 --> 00:03:31.260
Je pensais qu'une assemblée de gens intelligents

00:03:31.260 --> 00:03:33.260
ne succomberait pas si facilement aux jolis téléphones.

00:03:33.260 --> 00:03:35.260
(Rires)

00:03:35.260 --> 00:03:37.260
Question suivante.

00:03:37.260 --> 00:03:39.260
Un peu plus difficile.

00:03:39.260 --> 00:03:41.260
Devrions-nous collecter toutes les données de ce type

00:03:41.260 --> 00:03:43.260
pour optimiser son surf sur Internet

00:03:43.260 --> 00:03:46.260
et pour nous protéger au cas où il aurait de mauvaises intentions ?

00:03:46.260 --> 00:03:48.260
Ou devrions-nous le laisser tranquille ?

00:03:48.260 --> 00:03:51.260
Rassembler ses données.

00:03:53.260 --> 00:03:56.260
Le laisser tranquille.

00:03:56.260 --> 00:03:58.260
Vous êtes hors de danger, c'est bon.

00:03:58.260 --> 00:04:00.260
(Rires)

00:04:00.260 --> 00:04:02.260
Bien, dernière question --

00:04:02.260 --> 00:04:04.260
plus difficile --

00:04:04.260 --> 00:04:07.260
quand on essaye d'estimer

00:04:07.260 --> 00:04:10.260
ce que nous devrions faire dans cette situation,

00:04:10.260 --> 00:04:14.260
devrions-nous utiliser le système moral déontologique de Kant,

00:04:14.260 --> 00:04:17.260
ou bien le système moral conséquentialiste de Mill ?

00:04:19.260 --> 00:04:22.260
Kant.

00:04:22.260 --> 00:04:25.260
Mill.

00:04:25.260 --> 00:04:27.260
Pas autant de voix.

00:04:27.260 --> 00:04:30.260
(Rires)

00:04:30.260 --> 00:04:33.260
Oui, c'est un résultat terrifiant.

00:04:34.260 --> 00:04:38.260
Terrifiant, parce que nous avons des opinions plus fortes

00:04:38.260 --> 00:04:40.260
sur nos appareils téléphones

00:04:40.260 --> 00:04:42.260
que sur les systèmes moraux

00:04:42.260 --> 00:04:44.260
que nous devrions utiliser pour orienter nos décisions.

00:04:44.260 --> 00:04:47.260
Que faire de tout le pouvoir que nous avons

00:04:47.260 --> 00:04:50.260
si nous n'avons pas de système moral ?

00:04:50.260 --> 00:04:53.260
Nous en savons davantage sur les systèmes d'exploitation de nos téléphones,

00:04:53.260 --> 00:04:56.260
alors que ce dont nous avons vraiment besoin est d'un système d'exploitation moral.

00:04:58.260 --> 00:05:00.260
Qu'est-ce qu'un système d'exploitation moral ?

00:05:00.260 --> 00:05:02.260
Nous connaissons tous le bien et le mal.

00:05:02.260 --> 00:05:04.260
Vous vous sentez bien quand vous faites quelque chose de juste,

00:05:04.260 --> 00:05:06.260
vous vous sentez mal quand vous faites le mal.

00:05:06.260 --> 00:05:09.260
Nos parents nous l'apprennent : louer le bien, réprimander le mal.

00:05:09.260 --> 00:05:12.260
Mais comment savoir ce qui est bien et ce qui est mal ?

00:05:12.260 --> 00:05:15.260
De jour en jour, nous utilisons des techniques.

00:05:15.260 --> 00:05:18.260
Peut-être que nous suivons simplement notre instinct.

00:05:18.260 --> 00:05:21.260
Peut-être que nous procédons à un vote -- le crowdsourcing.

00:05:21.260 --> 00:05:23.260
Ou peut-être que nous nous déchargeons --

00:05:23.260 --> 00:05:26.260
nous demandons le service juridique, voir ce qu'ils en pensent.

00:05:26.260 --> 00:05:28.260
En d'autres mots, c'est plutôt aléatoire,

00:05:28.260 --> 00:05:30.260
c'est plutôt ad hoc,

00:05:30.260 --> 00:05:33.260
la façon dont nous décidons de ce que nous devrions faire.

00:05:33.260 --> 00:05:36.260
Peut-être que si nous voulons adopter une position plus sûre,

00:05:36.260 --> 00:05:39.260
ce que nous voulons vraiment est un système moral qui nous aidera à nous y orienter,

00:05:39.260 --> 00:05:42.260
qui nous dira qu'est-ce qui est bien et qu'est-ce qui est mal dès le départ,

00:05:42.260 --> 00:05:46.260
et comment savoir quoi faire dans une situation donnée.

00:05:46.260 --> 00:05:48.260
Prenons donc un cadre moral.

00:05:48.260 --> 00:05:51.260
Nous vivons dans un monde de chiffres.

00:05:51.260 --> 00:05:53.260
Comment pouvons-nous utiliser les chiffres

00:05:53.260 --> 00:05:56.260
comme base d'un système moral ?

00:05:56.260 --> 00:05:59.260
Je connais quelqu'un qui a fait exactement cela,

00:05:59.260 --> 00:06:02.260
quelqu'un de brillant --

00:06:02.260 --> 00:06:05.260
il est mort il y a 2 500 ans.

00:06:05.260 --> 00:06:07.260
Platon, c'est exact.

00:06:07.260 --> 00:06:09.260
Vous vous rappelez de lui -- le vieux philosophe ?

00:06:09.260 --> 00:06:12.260
Vous dormiez pendant les cours.

00:06:12.260 --> 00:06:14.260
Platon partageait beaucoup de nos préoccupations.

00:06:14.260 --> 00:06:16.260
Il se préoccupait du bien et du mal.

00:06:16.260 --> 00:06:18.260
Il voulait savoir ce qui était juste.

00:06:18.260 --> 00:06:20.260
Mais il s'inquiétait que tout ce que nous semblons faire,

00:06:20.260 --> 00:06:22.260
c'est échanger des opinions sur le sujet.

00:06:22.260 --> 00:06:25.260
Il me dit que quelque chose est juste. Elle me dit qu'autre chose est juste.

00:06:25.260 --> 00:06:27.260
Les deux sont plutôt convaincants quand ils parlent.

00:06:27.260 --> 00:06:29.260
Je fais des allers retours ; je n'avance pas.

00:06:29.260 --> 00:06:32.260
Je ne veux pas d'opinions, je veux de la connaissance.

00:06:32.260 --> 00:06:35.260
Je veux connaître la vérité sur la justice --

00:06:35.260 --> 00:06:38.260
comme on connait les vérités mathématiques.

00:06:38.260 --> 00:06:41.260
En maths, on connait les faits concrets.

00:06:41.260 --> 00:06:43.260
Prenez un chiffre, n'importe lequel -- deux.

00:06:43.260 --> 00:06:45.260
Mon chiffre préféré. J'adore ce chiffre.

00:06:45.260 --> 00:06:47.260
Il y a des vérités sur le chiffre deux.

00:06:47.260 --> 00:06:49.260
Si vous avez une chose en deux exemplaires,

00:06:49.260 --> 00:06:51.260
vous en ajoutez deux, vous en obtenez quatre.

00:06:51.260 --> 00:06:53.260
C'est vrai pour n'importe quelle chose.

00:06:53.260 --> 00:06:55.260
c'est une vérité objective sur la forme du chiffre deux,

00:06:55.260 --> 00:06:57.260
la forme abstraite.

00:06:57.260 --> 00:06:59.260
Quand vous avez une chose en deux exemplaires -- deux yeux, deux oreilles, deux nez,

00:06:59.260 --> 00:07:01.260
juste deux éléments --

00:07:01.260 --> 00:07:04.260
ils participent tous à la forme du chiffre deux.

00:07:04.260 --> 00:07:08.260
Ils participent aux vérités intrinsèques du chiffre deux.

00:07:08.260 --> 00:07:10.260
Il y a du chiffre deux en chacun d'eux.

00:07:10.260 --> 00:07:13.260
Par conséquent, ça ne dépend pas de l'opinion.

00:07:13.260 --> 00:07:15.260
Et si, Platon se disait,

00:07:15.260 --> 00:07:17.260
l'étique était comme les maths ?

00:07:17.260 --> 00:07:20.260
Et s'il y avait une forme pure de justice ?

00:07:20.260 --> 00:07:22.260
Et s'il y avait des vérités sur la justice,

00:07:22.260 --> 00:07:24.260
et que vous pouviez simplement regarder le monde

00:07:24.260 --> 00:07:26.260
et voir les choses qui y participent,

00:07:26.260 --> 00:07:29.260
qui prennent part à cette forme de justice ?

00:07:29.260 --> 00:07:32.260
Vous sauriez alors ce qui est vraiment juste et ce qui ne l'est pas.

00:07:32.260 --> 00:07:34.260
Ça ne dépendrait pas

00:07:34.260 --> 00:07:37.260
d'un simple jugement ou d'un simple aspect.

00:07:37.260 --> 00:07:39.260
C'est une vision stupéfiante.

00:07:39.260 --> 00:07:42.260
Je veux dire, pensez-y. C'est magnifique. C'est ambitieux.

00:07:42.260 --> 00:07:44.260
C'est aussi ambitieux que nous.

00:07:44.260 --> 00:07:46.260
Nous voulons résoudre les problèmes d'éthique.

00:07:46.260 --> 00:07:48.260
Nous voulons des vérités objectives.

00:07:48.260 --> 00:07:51.260
Si vous pensez de cette façon,

00:07:51.260 --> 00:07:54.260
vous avez un système moral platonicien.

00:07:54.260 --> 00:07:56.260
Si vous ne pensez pas de cette façon,

00:07:56.260 --> 00:07:58.260
eh bien, vous n'êtes pas seul dans l'histoire de la philosophie occidentale,

00:07:58.260 --> 00:08:01.260
parce que cette jolie idée -- vous savez, les gens l'ont critiquée.

00:08:01.260 --> 00:08:04.260
Aristote, en particulier, n'a pas apprécié.

00:08:04.260 --> 00:08:07.260
Il pensait que c'était infaisable en pratique.

00:08:07.260 --> 00:08:11.260
Aristote disait : « Nous ne devrions chercher qu'autant d'exactitude dans un sujet

00:08:11.260 --> 00:08:13.260
que celui-ci nous le permet. »

00:08:13.260 --> 00:08:16.260
Aristote pensait que l'éthique n'était pas vraiment comme les maths.

00:08:16.260 --> 00:08:19.260
Il pensait que l'éthique consistait à prendre des décision ici et maintenant

00:08:19.260 --> 00:08:21.260
en utilisant notre meilleur jugement

00:08:21.260 --> 00:08:23.260
pour trouver le droit chemin.

00:08:23.260 --> 00:08:25.260
Si vous pensez cela, Platon n'est pas votre homme.

00:08:25.260 --> 00:08:27.260
Mais n'abandonnez pas.

00:08:27.260 --> 00:08:29.260
Peut-être y a-t-il un autre moyen

00:08:29.260 --> 00:08:32.260
pour utiliser les nombres comme base de notre système moral.

00:08:33.260 --> 00:08:35.260
Que diriez-vous de ceci :

00:08:35.260 --> 00:08:38.260
et si dans n'importe quelle situation, vous pouviez simplement calculer,

00:08:38.260 --> 00:08:40.260
examiner les possibilités,

00:08:40.260 --> 00:08:43.260
évaluer laquelle est la meilleure et savoir quoi faire ?

00:08:43.260 --> 00:08:45.260
Cela vous dit quelque chose ?

00:08:45.260 --> 00:08:48.260
C'est un système moral utilitariste.

00:08:48.260 --> 00:08:50.260
John Stuart Mill en était un grand partisan --

00:08:50.260 --> 00:08:52.260
un type bien par ailleurs --

00:08:52.260 --> 00:08:54.260
et il n'est mort que depuis 200 ans.

00:08:54.260 --> 00:08:56.260
Donc les fondements de l'utilitarisme --

00:08:56.260 --> 00:08:58.260
je suis sûr que vous les connaissez.

00:08:58.260 --> 00:09:00.260
Les trois personnes qui ont voté pour Mill tout à l'heure savent ce que c'est.

00:09:00.260 --> 00:09:02.260
Mais voilà comment ça fonctionne.

00:09:02.260 --> 00:09:05.260
Et si la morale, si ce qui rend quelque chose moral,

00:09:05.260 --> 00:09:07.260
n'était qu'un calcul de plaisir maximum

00:09:07.260 --> 00:09:09.260
et de douleur minimum ?

00:09:09.260 --> 00:09:12.260
C'est intrinsèque au fait.

00:09:12.260 --> 00:09:14.260
Ça n'a pas de rapport avec sa forme abstraite.

00:09:14.260 --> 00:09:16.260
C'est juste fonction des conséquences.

00:09:16.260 --> 00:09:18.260
Vous regardez simplement les conséquences,

00:09:18.260 --> 00:09:20.260
et vous voyez si, globalement, c'est pour le meilleur ou pour le pire.

00:09:20.260 --> 00:09:22.260
Ce serait simple. Nous saurions ensuite quoi faire.

00:09:22.260 --> 00:09:24.260
Prenons un exemple.

00:09:24.260 --> 00:09:26.260
Imaginez que je vienne

00:09:26.260 --> 00:09:28.260
et je dise : « Je vais vous prendre votre téléphone. »

00:09:28.260 --> 00:09:30.260
Pas parce qu'il a sonné tout à l'heure,

00:09:30.260 --> 00:09:33.260
mais parce que j'ai fait un petit calcul.

00:09:33.260 --> 00:09:36.260
Je pensais que ce type avait l'air suspect.

00:09:36.260 --> 00:09:39.260
Et s'il était en train d'envoyer des messages à la planque de Ben Laden --

00:09:39.260 --> 00:09:41.260
ou de n'importe qui ayant pris la relève de Ben Laden --

00:09:41.260 --> 00:09:44.260
c'est en fait un terroriste, une cellule dormante.

00:09:44.260 --> 00:09:47.260
Je vais m'en rendre compte, et quand ce sera fait,

00:09:47.260 --> 00:09:50.260
je vais éviter d'énormes dégâts qu'il pourrait causer.

00:09:50.260 --> 00:09:53.260
L'intérêt est très grand d'éviter les dégâts,

00:09:53.260 --> 00:09:55.260
comparé au moindre mal qu'il y aurait

00:09:55.260 --> 00:09:57.260
si je le gêne en regardant dans son téléphone

00:09:57.260 --> 00:10:00.260
pour découvrir qu'il ne faisait que jouer à Farmville --

00:10:00.260 --> 00:10:03.260
c'est écrasé

00:10:03.260 --> 00:10:05.260
par l'utilité d'examiner son téléphone.

00:10:05.260 --> 00:10:07.260
Si vous pensez comme cela,

00:10:07.260 --> 00:10:10.260
c'est un choix utilitariste.

00:10:10.260 --> 00:10:13.260
Mais peut-être que vous ne pensez pas non plus comme ça.

00:10:13.260 --> 00:10:15.260
Peut-être que vous vous dites : c'est son téléphone.

00:10:15.260 --> 00:10:17.260
C'est mal de prendre son téléphone,

00:10:17.260 --> 00:10:19.260
parce que c'est un individu

00:10:19.260 --> 00:10:21.260
et il a des droits et il a une dignité,

00:10:21.260 --> 00:10:23.260
et nous ne pouvons pas interférer avec ça.

00:10:23.260 --> 00:10:25.260
Il est autonome.

00:10:25.260 --> 00:10:27.260
Peut importe les calculs.

00:10:27.260 --> 00:10:30.260
Ces choses sont intrinsèquement mauvaises --

00:10:30.260 --> 00:10:32.260
comme mentir est mal,

00:10:32.260 --> 00:10:35.260
de même que torturer des enfants innocents est mal.

00:10:35.260 --> 00:10:38.260
Kant était vraiment bon sur ce sujet,

00:10:38.260 --> 00:10:40.260
et il le disait un peu mieux que je vais le dire.

00:10:40.260 --> 00:10:42.260
Il disait que nous devrions utiliser notre raison

00:10:42.260 --> 00:10:45.260
pour décider des règles selon lesquelles nous devrions orienter notre conduite.

00:10:45.260 --> 00:10:48.260
Il est ensuite de notre devoir de suivre ces règles.

00:10:48.260 --> 00:10:51.260
Ça n'a rien à voir avec des calculs.

00:10:51.260 --> 00:10:53.260
Arrêtons-nous.

00:10:53.260 --> 00:10:56.260
Nous sommes au cœur de cet enchevêtrement philosophique.

00:10:56.260 --> 00:10:59.260
Et le débat perdure depuis des milliers d'années,

00:10:59.260 --> 00:11:01.260
parce que ce sont des questions difficiles,

00:11:01.260 --> 00:11:03.260
et je n'ai que 15 minutes.

00:11:03.260 --> 00:11:05.260
Alors allons droit au but.

00:11:05.260 --> 00:11:09.260
Comment devrions-nous prendre nos décisions ?

00:11:09.260 --> 00:11:12.260
Selon Platon, en accord avec Aristote, ou bien Kant, ou Mill ?

00:11:12.260 --> 00:11:14.260
Que devrions-nous faire ? Quelle est la réponse ?

00:11:14.260 --> 00:11:17.260
Quelle est la formule que nous pouvons utiliser dans n'importe quelle situation

00:11:17.260 --> 00:11:19.260
pour déterminer ce que nous devrions faire ?

00:11:19.260 --> 00:11:21.260
Si nous devrions utiliser les données de ce type ou pas ?

00:11:21.260 --> 00:11:24.260
Quelle est la formule ?

00:11:25.260 --> 00:11:27.260
Il n'y a pas de formule.

00:11:29.260 --> 00:11:31.260
Il n'y a pas de réponse simple.

00:11:31.260 --> 00:11:34.260
L'éthique, c'est difficile.

00:11:34.260 --> 00:11:37.260
L'éthique exige une réflexion.

00:11:38.260 --> 00:11:40.260
C'est inconfortable.

00:11:40.260 --> 00:11:42.260
Je sais ; j'ai passé une grande partie de ma carrière

00:11:42.260 --> 00:11:44.260
dans l'intelligence artificielle,

00:11:44.260 --> 00:11:47.260
à essayer de construire des machines qui puissent réfléchir là-dessus pour nous,

00:11:47.260 --> 00:11:49.260
qui puissent nous donner des réponses.

00:11:49.260 --> 00:11:51.260
Mais elles ne le peuvent pas.

00:11:51.260 --> 00:11:53.260
Vous ne pouvez pas simplement prendre la pensée humaine

00:11:53.260 --> 00:11:55.260
et la mettre dans une machine.

00:11:55.260 --> 00:11:58.260
Nous devons le faire par nous-mêmes.

00:11:58.260 --> 00:12:01.260
Heureusement, nous ne sommes pas des machines, et nous pouvons le faire.

00:12:01.260 --> 00:12:03.260
Nous pouvons non seulement penser,

00:12:03.260 --> 00:12:05.260
mais nous le devons.

00:12:05.260 --> 00:12:07.260
Hannah Arendt disait :

00:12:07.260 --> 00:12:09.260
« La triste vérité

00:12:09.260 --> 00:12:11.260
est que la plupart du mal fait en ce monde

00:12:11.260 --> 00:12:13.260
n'est pas fait par des gens

00:12:13.260 --> 00:12:15.260
qui ont choisi de faire le mal.

00:12:15.260 --> 00:12:18.260
Il surgit de l'inexistence d'une réflexion. »

00:12:18.260 --> 00:12:22.260
C'est ce qu'elle appelait « la banalité du mal. »

00:12:22.260 --> 00:12:24.260
La réponse à cela

00:12:24.260 --> 00:12:26.260
est que nous réclamons l'exercice de pensée

00:12:26.260 --> 00:12:29.260
à toute personne sensée.

00:12:29.260 --> 00:12:31.260
Faisons-donc cela. Pensons.

00:12:31.260 --> 00:12:34.260
En fait, commençons dès maintenant.

00:12:34.260 --> 00:12:37.260
Tout le monde dans la salle :

00:12:37.260 --> 00:12:40.260
pensez à la dernière fois que vous avez dû prendre une décision

00:12:40.260 --> 00:12:42.260
où vous étiez préoccupés de faire ce qui est juste,

00:12:42.260 --> 00:12:44.260
où vous vous êtes demandés : « Que devrais-je faire ? »

00:12:44.260 --> 00:12:46.260
Pensez à cela.

00:12:46.260 --> 00:12:48.260
Réfléchissez maintenant à cela

00:12:48.260 --> 00:12:51.260
et demandez-vous : « Comment ais-je pris cette décision ?

00:12:51.260 --> 00:12:54.260
Qu'est-ce que j'ai fait ? Est-ce que j'ai suivi mon instinct ?

00:12:54.260 --> 00:12:56.260
Est-ce que j'ai fait procéder à un vote ? Ou est-ce que j'ai fait appel au service juridique ? »

00:12:56.260 --> 00:12:59.260
Ou bien nous avons d'autres choix maintenant.

00:12:59.260 --> 00:13:01.260
« Est-ce que j'ai estimé ce qui procurerait le plus de plaisir,

00:13:01.260 --> 00:13:03.260
comme Mill l'aurait fait ?

00:13:03.260 --> 00:13:06.260
Ou comme Kant, ai-je utilisé ma raison pour décider de ce qui était intrinsèquement juste ?

00:13:06.260 --> 00:13:09.260
Pensez-y. Vraiment. C'est important.

00:13:09.260 --> 00:13:11.260
C'est si important

00:13:11.260 --> 00:13:13.260
que nous allons passer 30 précieuses secondes de mon intervention à TED

00:13:13.260 --> 00:13:15.260
à ne rien faire d'autre que d'y penser.

00:13:15.260 --> 00:13:17.260
Vous êtes prêts ? Allez-y.

00:13:33.260 --> 00:13:36.260
Arrêtez. Bon travail.

00:13:36.260 --> 00:13:38.260
Ce que vous venez de faire,

00:13:38.260 --> 00:13:40.260
c'est le premier pas vers la prise de responsabilité

00:13:40.260 --> 00:13:43.260
concernant ce que nous devrions faire avec tout ce pouvoir.

00:13:45.260 --> 00:13:48.260
La prochaine étape : essayez ceci.

00:13:49.260 --> 00:13:51.260
Allez trouver un ami et expliquez-lui

00:13:51.260 --> 00:13:53.260
comment vous avez pris cette décision.

00:13:53.260 --> 00:13:55.260
Pas tout de suite. Attendez que j'ai terminé de parler.

00:13:55.260 --> 00:13:57.260
Faites-le au déjeuner.

00:13:57.260 --> 00:14:00.260
N'allez pas simplement trouver un autre ami technologue ;

00:14:00.260 --> 00:14:02.260
trouvez quelqu'un de différent.

00:14:02.260 --> 00:14:04.260
Trouvez un artiste ou un écrivain --

00:14:04.260 --> 00:14:07.260
ou, Dieu vous en préserve, trouvez un philosophe et parlez leur.

00:14:07.260 --> 00:14:09.260
En fait, trouvez quelqu'un dans les sciences humaines.

00:14:09.260 --> 00:14:11.260
Pourquoi ? Parce qu'ils pensent aux problèmes

00:14:11.260 --> 00:14:13.260
d'une manière différente à nous les technologues.

00:14:13.260 --> 00:14:16.260
Il y a juste quelques jours, de l'autre côté de la rue ici,

00:14:16.260 --> 00:14:18.260
il y avait un rassemblement de centaines de personnes.

00:14:18.260 --> 00:14:20.260
C'était des technologues et des humanistes

00:14:20.260 --> 00:14:22.260
à la grande BiblioTech Conférence.

00:14:22.260 --> 00:14:24.260
Ils étaient rassemblés

00:14:24.260 --> 00:14:26.260
parce que les technologues voulaient apprendre

00:14:26.260 --> 00:14:29.260
ce que cela faisait de penser du point de vue des sciences sociales.

00:14:29.260 --> 00:14:31.260
Vous avez quelqu'un de chez Google

00:14:31.260 --> 00:14:33.260
qui parle à quelqu'un qui fait de la littérature comparée.

00:14:33.260 --> 00:14:36.260
Vous vous demandez quel est l'intérêt du théâtre français du 17ème siècle --

00:14:36.260 --> 00:14:38.260
quel est le lien avec le capital-risque ?

00:14:38.260 --> 00:14:41.260
Eh bien, c'est intéressant. C'est une manière de penser différente.

00:14:41.260 --> 00:14:43.260
Quand vous pensez de cette façon,

00:14:43.260 --> 00:14:46.260
vous devenez plus sensible aux considérations humaines,

00:14:46.260 --> 00:14:49.260
ce qui est crucial pour prendre des décisions éthiques.

00:14:49.260 --> 00:14:51.260
Imaginez maintenant

00:14:51.260 --> 00:14:53.260
que vous avez trouvé votre ami musicien.

00:14:53.260 --> 00:14:56.260
Vous lui racontez ce dont on parle,

00:14:56.260 --> 00:14:58.260
sur notre révolution des données et tout ça --

00:14:58.260 --> 00:15:00.260
vous fredonnez peut-être quelques notes de notre thème musical.

00:15:00.260 --> 00:15:03.260
♫ Dum ta da da dum dum ta da da dum ♫

00:15:03.260 --> 00:15:05.260
Votre ami musicien va vous interrompre et vous dire :

00:15:05.260 --> 00:15:07.260
« Tu sais, le thème musical

00:15:07.260 --> 00:15:09.260
pour ta révolution des données,

00:15:09.260 --> 00:15:11.260
c'est un opéra, c'est Wagner.

00:15:11.260 --> 00:15:13.260
C'est basé sur une légende nordique.

00:15:13.260 --> 00:15:15.260
Ce sont des dieux et des créatures mythologiques

00:15:15.260 --> 00:15:18.260
qui se battent pour des bijoux magiques. »

00:15:19.260 --> 00:15:22.260
C'est intéressant.

00:15:22.260 --> 00:15:25.260
C'est aussi un magnifique opéra.

00:15:25.260 --> 00:15:28.260
Nous sommes émus par cet opéra.

00:15:28.260 --> 00:15:30.260
Nous sommes émus parce que c'est sur la bataille

00:15:30.260 --> 00:15:32.260
entre le bien et le mal,

00:15:32.260 --> 00:15:34.260
le juste et l'injuste.

00:15:34.260 --> 00:15:36.260
Et nous nous préoccupons du juste et de l'injuste.

00:15:36.260 --> 00:15:39.260
Nous nous soucions de ce qui se passe dans cet opéra.

00:15:39.260 --> 00:15:42.260
Nous nous soucions de ce qui se passe dans "Apocalypse Now".

00:15:42.260 --> 00:15:44.260
Et nous nous préoccupons certainement

00:15:44.260 --> 00:15:46.260
de ce qui se passe avec nos technologies.

00:15:46.260 --> 00:15:48.260
Nous avons tant de pouvoir aujourd'hui,

00:15:48.260 --> 00:15:51.260
ça ne dépend que de nous de savoir ce qu'on en fait.

00:15:51.260 --> 00:15:53.260
C'est la bonne nouvelle.

00:15:53.260 --> 00:15:56.260
Nous sommes ceux qui écrivent cet opéra.

00:15:56.260 --> 00:15:58.260
C'est notre film.

00:15:58.260 --> 00:16:01.260
Nous décidons ce qui va arriver avec cette technologie.

00:16:01.260 --> 00:16:04.260
Nous déterminons comment tout cela va finir.

00:16:04.260 --> 00:16:06.260
Merci.

00:16:06.260 --> 00:16:11.260
(Applaudissements)

