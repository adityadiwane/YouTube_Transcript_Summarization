WEBVTT
Kind: captions
Language: en

00:00:15.260 --> 00:00:17.260
Power.

00:00:17.260 --> 00:00:19.260
That is the word that comes to mind.

00:00:19.260 --> 00:00:21.260
We're the new technologists.

00:00:21.260 --> 00:00:24.260
We have a lot of data, so we have a lot of power.

00:00:24.260 --> 00:00:26.260
How much power do we have?

00:00:26.260 --> 00:00:29.260
Scene from a movie: "Apocalypse Now" -- great movie.

00:00:29.260 --> 00:00:32.260
We've got to get our hero, Captain Willard, to the mouth of the Nung River

00:00:32.260 --> 00:00:34.260
so he can go pursue Colonel Kurtz.

00:00:34.260 --> 00:00:36.260
The way we're going to do this is fly him in and drop him off.

00:00:36.260 --> 00:00:38.260
So the scene:

00:00:38.260 --> 00:00:41.260
the sky is filled with this fleet of helicopters carrying him in.

00:00:41.260 --> 00:00:43.260
And there's this loud, thrilling music in the background,

00:00:43.260 --> 00:00:45.260
this wild music.

00:00:45.260 --> 00:00:47.260
♫ Dum da ta da dum ♫

00:00:47.260 --> 00:00:49.260
♫ Dum da ta da dum ♫

00:00:49.260 --> 00:00:52.260
♫ Da ta da da ♫

00:00:52.260 --> 00:00:54.260
That's a lot of power.

00:00:54.260 --> 00:00:56.260
That's the kind of power I feel in this room.

00:00:56.260 --> 00:00:58.260
That's the kind of power we have

00:00:58.260 --> 00:01:00.260
because of all of the data that we have.

00:01:00.260 --> 00:01:02.260
Let's take an example.

00:01:02.260 --> 00:01:04.260
What can we do

00:01:04.260 --> 00:01:07.260
with just one person's data?

00:01:07.260 --> 00:01:09.260
What can we do

00:01:09.260 --> 00:01:11.260
with that guy's data?

00:01:11.260 --> 00:01:13.260
I can look at your financial records.

00:01:13.260 --> 00:01:15.260
I can tell if you pay your bills on time.

00:01:15.260 --> 00:01:17.260
I know if you're good to give a loan to.

00:01:17.260 --> 00:01:20.260
I can look at your medical records; I can see if your pump is still pumping --

00:01:20.260 --> 00:01:23.260
see if you're good to offer insurance to.

00:01:23.260 --> 00:01:25.260
I can look at your clicking patterns.

00:01:25.260 --> 00:01:28.260
When you come to my website, I actually know what you're going to do already

00:01:28.260 --> 00:01:30.260
because I've seen you visit millions of websites before.

00:01:30.260 --> 00:01:32.260
And I'm sorry to tell you,

00:01:32.260 --> 00:01:34.260
you're like a poker player, you have a tell.

00:01:34.260 --> 00:01:36.260
I can tell with data analysis what you're going to do

00:01:36.260 --> 00:01:38.260
before you even do it.

00:01:38.260 --> 00:01:41.260
I know what you like. I know who you are,

00:01:41.260 --> 00:01:43.260
and that's even before I look at your mail

00:01:43.260 --> 00:01:45.260
or your phone.

00:01:45.260 --> 00:01:47.260
Those are the kinds of things we can do

00:01:47.260 --> 00:01:50.260
with the data that we have.

00:01:50.260 --> 00:01:53.260
But I'm not actually here to talk about what we can do.

00:01:56.260 --> 00:01:59.260
I'm here to talk about what we should do.

00:02:00.260 --> 00:02:03.260
What's the right thing to do?

00:02:04.260 --> 00:02:06.260
Now I see some puzzled looks

00:02:06.260 --> 00:02:09.260
like, "Why are you asking us what's the right thing to do?

00:02:09.260 --> 00:02:12.260
We're just building this stuff. Somebody else is using it."

00:02:12.260 --> 00:02:15.260
Fair enough.

00:02:15.260 --> 00:02:17.260
But it brings me back.

00:02:17.260 --> 00:02:19.260
I think about World War II --

00:02:19.260 --> 00:02:21.260
some of our great technologists then,

00:02:21.260 --> 00:02:23.260
some of our great physicists,

00:02:23.260 --> 00:02:25.260
studying nuclear fission and fusion --

00:02:25.260 --> 00:02:27.260
just nuclear stuff.

00:02:27.260 --> 00:02:30.260
We gather together these physicists in Los Alamos

00:02:30.260 --> 00:02:33.260
to see what they'll build.

00:02:33.260 --> 00:02:36.260
We want the people building the technology

00:02:36.260 --> 00:02:39.260
thinking about what we should be doing with the technology.

00:02:41.260 --> 00:02:44.260
So what should we be doing with that guy's data?

00:02:44.260 --> 00:02:47.260
Should we be collecting it, gathering it,

00:02:47.260 --> 00:02:49.260
so we can make his online experience better?

00:02:49.260 --> 00:02:51.260
So we can make money?

00:02:51.260 --> 00:02:53.260
So we can protect ourselves

00:02:53.260 --> 00:02:55.260
if he was up to no good?

00:02:55.260 --> 00:02:58.260
Or should we respect his privacy,

00:02:58.260 --> 00:03:01.260
protect his dignity and leave him alone?

00:03:02.260 --> 00:03:05.260
Which one is it?

00:03:05.260 --> 00:03:07.260
How should we figure it out?

00:03:07.260 --> 00:03:10.260
I know: crowdsource. Let's crowdsource this.

00:03:11.260 --> 00:03:14.260
So to get people warmed up,

00:03:14.260 --> 00:03:16.260
let's start with an easy question --

00:03:16.260 --> 00:03:19.260
something I'm sure everybody here has an opinion about:

00:03:19.260 --> 00:03:21.260
iPhone versus Android.

00:03:21.260 --> 00:03:24.260
Let's do a show of hands -- iPhone.

00:03:24.260 --> 00:03:26.260
Uh huh.

00:03:26.260 --> 00:03:29.260
Android.

00:03:29.260 --> 00:03:31.260
You'd think with a bunch of smart people

00:03:31.260 --> 00:03:33.260
we wouldn't be such suckers just for the pretty phones.

00:03:33.260 --> 00:03:35.260
(Laughter)

00:03:35.260 --> 00:03:37.260
Next question,

00:03:37.260 --> 00:03:39.260
a little bit harder.

00:03:39.260 --> 00:03:41.260
Should we be collecting all of that guy's data

00:03:41.260 --> 00:03:43.260
to make his experiences better

00:03:43.260 --> 00:03:46.260
and to protect ourselves in case he's up to no good?

00:03:46.260 --> 00:03:48.260
Or should we leave him alone?

00:03:48.260 --> 00:03:51.260
Collect his data.

00:03:53.260 --> 00:03:56.260
Leave him alone.

00:03:56.260 --> 00:03:58.260
You're safe. It's fine.

00:03:58.260 --> 00:04:00.260
(Laughter)

00:04:00.260 --> 00:04:02.260
Okay, last question --

00:04:02.260 --> 00:04:04.260
harder question --

00:04:04.260 --> 00:04:07.260
when trying to evaluate

00:04:07.260 --> 00:04:10.260
what we should do in this case,

00:04:10.260 --> 00:04:14.260
should we use a Kantian deontological moral framework,

00:04:14.260 --> 00:04:17.260
or should we use a Millian consequentialist one?

00:04:19.260 --> 00:04:22.260
Kant.

00:04:22.260 --> 00:04:25.260
Mill.

00:04:25.260 --> 00:04:27.260
Not as many votes.

00:04:27.260 --> 00:04:30.260
(Laughter)

00:04:30.260 --> 00:04:33.260
Yeah, that's a terrifying result.

00:04:34.260 --> 00:04:38.260
Terrifying, because we have stronger opinions

00:04:38.260 --> 00:04:40.260
about our hand-held devices

00:04:40.260 --> 00:04:42.260
than about the moral framework

00:04:42.260 --> 00:04:44.260
we should use to guide our decisions.

00:04:44.260 --> 00:04:47.260
How do we know what to do with all the power we have

00:04:47.260 --> 00:04:50.260
if we don't have a moral framework?

00:04:50.260 --> 00:04:53.260
We know more about mobile operating systems,

00:04:53.260 --> 00:04:56.260
but what we really need is a moral operating system.

00:04:58.260 --> 00:05:00.260
What's a moral operating system?

00:05:00.260 --> 00:05:02.260
We all know right and wrong, right?

00:05:02.260 --> 00:05:04.260
You feel good when you do something right,

00:05:04.260 --> 00:05:06.260
you feel bad when you do something wrong.

00:05:06.260 --> 00:05:09.260
Our parents teach us that: praise with the good, scold with the bad.

00:05:09.260 --> 00:05:12.260
But how do we figure out what's right and wrong?

00:05:12.260 --> 00:05:15.260
And from day to day, we have the techniques that we use.

00:05:15.260 --> 00:05:18.260
Maybe we just follow our gut.

00:05:18.260 --> 00:05:21.260
Maybe we take a vote -- we crowdsource.

00:05:21.260 --> 00:05:23.260
Or maybe we punt --

00:05:23.260 --> 00:05:26.260
ask the legal department, see what they say.

00:05:26.260 --> 00:05:28.260
In other words, it's kind of random,

00:05:28.260 --> 00:05:30.260
kind of ad hoc,

00:05:30.260 --> 00:05:33.260
how we figure out what we should do.

00:05:33.260 --> 00:05:36.260
And maybe, if we want to be on surer footing,

00:05:36.260 --> 00:05:39.260
what we really want is a moral framework that will help guide us there,

00:05:39.260 --> 00:05:42.260
that will tell us what kinds of things are right and wrong in the first place,

00:05:42.260 --> 00:05:46.260
and how would we know in a given situation what to do.

00:05:46.260 --> 00:05:48.260
So let's get a moral framework.

00:05:48.260 --> 00:05:51.260
We're numbers people, living by numbers.

00:05:51.260 --> 00:05:53.260
How can we use numbers

00:05:53.260 --> 00:05:56.260
as the basis for a moral framework?

00:05:56.260 --> 00:05:59.260
I know a guy who did exactly that.

00:05:59.260 --> 00:06:02.260
A brilliant guy --

00:06:02.260 --> 00:06:05.260
he's been dead 2,500 years.

00:06:05.260 --> 00:06:07.260
Plato, that's right.

00:06:07.260 --> 00:06:09.260
Remember him -- old philosopher?

00:06:09.260 --> 00:06:12.260
You were sleeping during that class.

00:06:12.260 --> 00:06:14.260
And Plato, he had a lot of the same concerns that we did.

00:06:14.260 --> 00:06:16.260
He was worried about right and wrong.

00:06:16.260 --> 00:06:18.260
He wanted to know what is just.

00:06:18.260 --> 00:06:20.260
But he was worried that all we seem to be doing

00:06:20.260 --> 00:06:22.260
is trading opinions about this.

00:06:22.260 --> 00:06:25.260
He says something's just. She says something else is just.

00:06:25.260 --> 00:06:27.260
It's kind of convincing when he talks and when she talks too.

00:06:27.260 --> 00:06:29.260
I'm just going back and forth; I'm not getting anywhere.

00:06:29.260 --> 00:06:32.260
I don't want opinions; I want knowledge.

00:06:32.260 --> 00:06:35.260
I want to know the truth about justice --

00:06:35.260 --> 00:06:38.260
like we have truths in math.

00:06:38.260 --> 00:06:41.260
In math, we know the objective facts.

00:06:41.260 --> 00:06:43.260
Take a number, any number -- two.

00:06:43.260 --> 00:06:45.260
Favorite number. I love that number.

00:06:45.260 --> 00:06:47.260
There are truths about two.

00:06:47.260 --> 00:06:49.260
If you've got two of something,

00:06:49.260 --> 00:06:51.260
you add two more, you get four.

00:06:51.260 --> 00:06:53.260
That's true no matter what thing you're talking about.

00:06:53.260 --> 00:06:55.260
It's an objective truth about the form of two,

00:06:55.260 --> 00:06:57.260
the abstract form.

00:06:57.260 --> 00:06:59.260
When you have two of anything -- two eyes, two ears, two noses,

00:06:59.260 --> 00:07:01.260
just two protrusions --

00:07:01.260 --> 00:07:04.260
those all partake of the form of two.

00:07:04.260 --> 00:07:08.260
They all participate in the truths that two has.

00:07:08.260 --> 00:07:10.260
They all have two-ness in them.

00:07:10.260 --> 00:07:13.260
And therefore, it's not a matter of opinion.

00:07:13.260 --> 00:07:15.260
What if, Plato thought,

00:07:15.260 --> 00:07:17.260
ethics was like math?

00:07:17.260 --> 00:07:20.260
What if there were a pure form of justice?

00:07:20.260 --> 00:07:22.260
What if there are truths about justice,

00:07:22.260 --> 00:07:24.260
and you could just look around in this world

00:07:24.260 --> 00:07:26.260
and see which things participated,

00:07:26.260 --> 00:07:29.260
partook of that form of justice?

00:07:29.260 --> 00:07:32.260
Then you would know what was really just and what wasn't.

00:07:32.260 --> 00:07:34.260
It wouldn't be a matter

00:07:34.260 --> 00:07:37.260
of just opinion or just appearances.

00:07:37.260 --> 00:07:39.260
That's a stunning vision.

00:07:39.260 --> 00:07:42.260
I mean, think about that. How grand. How ambitious.

00:07:42.260 --> 00:07:44.260
That's as ambitious as we are.

00:07:44.260 --> 00:07:46.260
He wants to solve ethics.

00:07:46.260 --> 00:07:48.260
He wants objective truths.

00:07:48.260 --> 00:07:51.260
If you think that way,

00:07:51.260 --> 00:07:54.260
you have a Platonist moral framework.

00:07:54.260 --> 00:07:56.260
If you don't think that way,

00:07:56.260 --> 00:07:58.260
well, you have a lot of company in the history of Western philosophy,

00:07:58.260 --> 00:08:01.260
because the tidy idea, you know, people criticized it.

00:08:01.260 --> 00:08:04.260
Aristotle, in particular, he was not amused.

00:08:04.260 --> 00:08:07.260
He thought it was impractical.

00:08:07.260 --> 00:08:11.260
Aristotle said, "We should seek only so much precision in each subject

00:08:11.260 --> 00:08:13.260
as that subject allows."

00:08:13.260 --> 00:08:16.260
Aristotle thought ethics wasn't a lot like math.

00:08:16.260 --> 00:08:19.260
He thought ethics was a matter of making decisions in the here-and-now

00:08:19.260 --> 00:08:21.260
using our best judgment

00:08:21.260 --> 00:08:23.260
to find the right path.

00:08:23.260 --> 00:08:25.260
If you think that, Plato's not your guy.

00:08:25.260 --> 00:08:27.260
But don't give up.

00:08:27.260 --> 00:08:29.260
Maybe there's another way

00:08:29.260 --> 00:08:32.260
that we can use numbers as the basis of our moral framework.

00:08:33.260 --> 00:08:35.260
How about this:

00:08:35.260 --> 00:08:38.260
What if in any situation you could just calculate,

00:08:38.260 --> 00:08:40.260
look at the choices,

00:08:40.260 --> 00:08:43.260
measure out which one's better and know what to do?

00:08:43.260 --> 00:08:45.260
That sound familiar?

00:08:45.260 --> 00:08:48.260
That's a utilitarian moral framework.

00:08:48.260 --> 00:08:50.260
John Stuart Mill was a great advocate of this --

00:08:50.260 --> 00:08:52.260
nice guy besides --

00:08:52.260 --> 00:08:54.260
and only been dead 200 years.

00:08:54.260 --> 00:08:56.260
So basis of utilitarianism --

00:08:56.260 --> 00:08:58.260
I'm sure you're familiar at least.

00:08:58.260 --> 00:09:00.260
The three people who voted for Mill before are familiar with this.

00:09:00.260 --> 00:09:02.260
But here's the way it works.

00:09:02.260 --> 00:09:05.260
What if morals, what if what makes something moral

00:09:05.260 --> 00:09:07.260
is just a matter of if it maximizes pleasure

00:09:07.260 --> 00:09:09.260
and minimizes pain?

00:09:09.260 --> 00:09:12.260
It does something intrinsic to the act.

00:09:12.260 --> 00:09:14.260
It's not like its relation to some abstract form.

00:09:14.260 --> 00:09:16.260
It's just a matter of the consequences.

00:09:16.260 --> 00:09:18.260
You just look at the consequences

00:09:18.260 --> 00:09:20.260
and see if, overall, it's for the good or for the worse.

00:09:20.260 --> 00:09:22.260
That would be simple. Then we know what to do.

00:09:22.260 --> 00:09:24.260
Let's take an example.

00:09:24.260 --> 00:09:26.260
Suppose I go up

00:09:26.260 --> 00:09:28.260
and I say, "I'm going to take your phone."

00:09:28.260 --> 00:09:30.260
Not just because it rang earlier,

00:09:30.260 --> 00:09:33.260
but I'm going to take it because I made a little calculation.

00:09:33.260 --> 00:09:36.260
I thought, that guy looks suspicious.

00:09:36.260 --> 00:09:39.260
And what if he's been sending little messages to Bin Laden's hideout --

00:09:39.260 --> 00:09:41.260
or whoever took over after Bin Laden --

00:09:41.260 --> 00:09:44.260
and he's actually like a terrorist, a sleeper cell.

00:09:44.260 --> 00:09:47.260
I'm going to find that out, and when I find that out,

00:09:47.260 --> 00:09:50.260
I'm going to prevent a huge amount of damage that he could cause.

00:09:50.260 --> 00:09:53.260
That has a very high utility to prevent that damage.

00:09:53.260 --> 00:09:55.260
And compared to the little pain that it's going to cause --

00:09:55.260 --> 00:09:57.260
because it's going to be embarrassing when I'm looking on his phone

00:09:57.260 --> 00:10:00.260
and seeing that he has a Farmville problem and that whole bit --

00:10:00.260 --> 00:10:03.260
that's overwhelmed

00:10:03.260 --> 00:10:05.260
by the value of looking at the phone.

00:10:05.260 --> 00:10:07.260
If you feel that way,

00:10:07.260 --> 00:10:10.260
that's a utilitarian choice.

00:10:10.260 --> 00:10:13.260
But maybe you don't feel that way either.

00:10:13.260 --> 00:10:15.260
Maybe you think, it's his phone.

00:10:15.260 --> 00:10:17.260
It's wrong to take his phone

00:10:17.260 --> 00:10:19.260
because he's a person

00:10:19.260 --> 00:10:21.260
and he has rights and he has dignity,

00:10:21.260 --> 00:10:23.260
and we can't just interfere with that.

00:10:23.260 --> 00:10:25.260
He has autonomy.

00:10:25.260 --> 00:10:27.260
It doesn't matter what the calculations are.

00:10:27.260 --> 00:10:30.260
There are things that are intrinsically wrong --

00:10:30.260 --> 00:10:32.260
like lying is wrong,

00:10:32.260 --> 00:10:35.260
like torturing innocent children is wrong.

00:10:35.260 --> 00:10:38.260
Kant was very good on this point,

00:10:38.260 --> 00:10:40.260
and he said it a little better than I'll say it.

00:10:40.260 --> 00:10:42.260
He said we should use our reason

00:10:42.260 --> 00:10:45.260
to figure out the rules by which we should guide our conduct,

00:10:45.260 --> 00:10:48.260
and then it is our duty to follow those rules.

00:10:48.260 --> 00:10:51.260
It's not a matter of calculation.

00:10:51.260 --> 00:10:53.260
So let's stop.

00:10:53.260 --> 00:10:56.260
We're right in the thick of it, this philosophical thicket.

00:10:56.260 --> 00:10:59.260
And this goes on for thousands of years,

00:10:59.260 --> 00:11:01.260
because these are hard questions,

00:11:01.260 --> 00:11:03.260
and I've only got 15 minutes.

00:11:03.260 --> 00:11:05.260
So let's cut to the chase.

00:11:05.260 --> 00:11:09.260
How should we be making our decisions?

00:11:09.260 --> 00:11:12.260
Is it Plato, is it Aristotle, is it Kant, is it Mill?

00:11:12.260 --> 00:11:14.260
What should we be doing? What's the answer?

00:11:14.260 --> 00:11:17.260
What's the formula that we can use in any situation

00:11:17.260 --> 00:11:19.260
to determine what we should do,

00:11:19.260 --> 00:11:21.260
whether we should use that guy's data or not?

00:11:21.260 --> 00:11:24.260
What's the formula?

00:11:25.260 --> 00:11:27.260
There's not a formula.

00:11:29.260 --> 00:11:31.260
There's not a simple answer.

00:11:31.260 --> 00:11:34.260
Ethics is hard.

00:11:34.260 --> 00:11:37.260
Ethics requires thinking.

00:11:38.260 --> 00:11:40.260
And that's uncomfortable.

00:11:40.260 --> 00:11:42.260
I know; I spent a lot of my career

00:11:42.260 --> 00:11:44.260
in artificial intelligence,

00:11:44.260 --> 00:11:47.260
trying to build machines that could do some of this thinking for us,

00:11:47.260 --> 00:11:49.260
that could give us answers.

00:11:49.260 --> 00:11:51.260
But they can't.

00:11:51.260 --> 00:11:53.260
You can't just take human thinking

00:11:53.260 --> 00:11:55.260
and put it into a machine.

00:11:55.260 --> 00:11:58.260
We're the ones who have to do it.

00:11:58.260 --> 00:12:01.260
Happily, we're not machines, and we can do it.

00:12:01.260 --> 00:12:03.260
Not only can we think,

00:12:03.260 --> 00:12:05.260
we must.

00:12:05.260 --> 00:12:07.260
Hannah Arendt said,

00:12:07.260 --> 00:12:09.260
"The sad truth

00:12:09.260 --> 00:12:11.260
is that most evil done in this world

00:12:11.260 --> 00:12:13.260
is not done by people

00:12:13.260 --> 00:12:15.260
who choose to be evil.

00:12:15.260 --> 00:12:18.260
It arises from not thinking."

00:12:18.260 --> 00:12:22.260
That's what she called the "banality of evil."

00:12:22.260 --> 00:12:24.260
And the response to that

00:12:24.260 --> 00:12:26.260
is that we demand the exercise of thinking

00:12:26.260 --> 00:12:29.260
from every sane person.

00:12:29.260 --> 00:12:31.260
So let's do that. Let's think.

00:12:31.260 --> 00:12:34.260
In fact, let's start right now.

00:12:34.260 --> 00:12:37.260
Every person in this room do this:

00:12:37.260 --> 00:12:40.260
think of the last time you had a decision to make

00:12:40.260 --> 00:12:42.260
where you were worried to do the right thing,

00:12:42.260 --> 00:12:44.260
where you wondered, "What should I be doing?"

00:12:44.260 --> 00:12:46.260
Bring that to mind,

00:12:46.260 --> 00:12:48.260
and now reflect on that

00:12:48.260 --> 00:12:51.260
and say, "How did I come up that decision?

00:12:51.260 --> 00:12:54.260
What did I do? Did I follow my gut?

00:12:54.260 --> 00:12:56.260
Did I have somebody vote on it? Or did I punt to legal?"

00:12:56.260 --> 00:12:59.260
Or now we have a few more choices.

00:12:59.260 --> 00:13:01.260
"Did I evaluate what would be the highest pleasure

00:13:01.260 --> 00:13:03.260
like Mill would?

00:13:03.260 --> 00:13:06.260
Or like Kant, did I use reason to figure out what was intrinsically right?"

00:13:06.260 --> 00:13:09.260
Think about it. Really bring it to mind. This is important.

00:13:09.260 --> 00:13:11.260
It is so important

00:13:11.260 --> 00:13:13.260
we are going to spend 30 seconds of valuable TEDTalk time

00:13:13.260 --> 00:13:15.260
doing nothing but thinking about this.

00:13:15.260 --> 00:13:17.260
Are you ready? Go.

00:13:33.260 --> 00:13:36.260
Stop. Good work.

00:13:36.260 --> 00:13:38.260
What you just did,

00:13:38.260 --> 00:13:40.260
that's the first step towards taking responsibility

00:13:40.260 --> 00:13:43.260
for what we should do with all of our power.

00:13:45.260 --> 00:13:48.260
Now the next step -- try this.

00:13:49.260 --> 00:13:51.260
Go find a friend and explain to them

00:13:51.260 --> 00:13:53.260
how you made that decision.

00:13:53.260 --> 00:13:55.260
Not right now. Wait till I finish talking.

00:13:55.260 --> 00:13:57.260
Do it over lunch.

00:13:57.260 --> 00:14:00.260
And don't just find another technologist friend;

00:14:00.260 --> 00:14:02.260
find somebody different than you.

00:14:02.260 --> 00:14:04.260
Find an artist or a writer --

00:14:04.260 --> 00:14:07.260
or, heaven forbid, find a philosopher and talk to them.

00:14:07.260 --> 00:14:09.260
In fact, find somebody from the humanities.

00:14:09.260 --> 00:14:11.260
Why? Because they think about problems

00:14:11.260 --> 00:14:13.260
differently than we do as technologists.

00:14:13.260 --> 00:14:16.260
Just a few days ago, right across the street from here,

00:14:16.260 --> 00:14:18.260
there was hundreds of people gathered together.

00:14:18.260 --> 00:14:20.260
It was technologists and humanists

00:14:20.260 --> 00:14:22.260
at that big BiblioTech Conference.

00:14:22.260 --> 00:14:24.260
And they gathered together

00:14:24.260 --> 00:14:26.260
because the technologists wanted to learn

00:14:26.260 --> 00:14:29.260
what it would be like to think from a humanities perspective.

00:14:29.260 --> 00:14:31.260
You have someone from Google

00:14:31.260 --> 00:14:33.260
talking to someone who does comparative literature.

00:14:33.260 --> 00:14:36.260
You're thinking about the relevance of 17th century French theater --

00:14:36.260 --> 00:14:38.260
how does that bear upon venture capital?

00:14:38.260 --> 00:14:41.260
Well that's interesting. That's a different way of thinking.

00:14:41.260 --> 00:14:43.260
And when you think in that way,

00:14:43.260 --> 00:14:46.260
you become more sensitive to the human considerations,

00:14:46.260 --> 00:14:49.260
which are crucial to making ethical decisions.

00:14:49.260 --> 00:14:51.260
So imagine that right now

00:14:51.260 --> 00:14:53.260
you went and you found your musician friend.

00:14:53.260 --> 00:14:56.260
And you're telling him what we're talking about,

00:14:56.260 --> 00:14:58.260
about our whole data revolution and all this --

00:14:58.260 --> 00:15:00.260
maybe even hum a few bars of our theme music.

00:15:00.260 --> 00:15:03.260
♫ Dum ta da da dum dum ta da da dum ♫

00:15:03.260 --> 00:15:05.260
Well, your musician friend will stop you and say,

00:15:05.260 --> 00:15:07.260
"You know, the theme music

00:15:07.260 --> 00:15:09.260
for your data revolution,

00:15:09.260 --> 00:15:11.260
that's an opera, that's Wagner.

00:15:11.260 --> 00:15:13.260
It's based on Norse legend.

00:15:13.260 --> 00:15:15.260
It's Gods and mythical creatures

00:15:15.260 --> 00:15:18.260
fighting over magical jewelry."

00:15:19.260 --> 00:15:22.260
That's interesting.

00:15:22.260 --> 00:15:25.260
Now it's also a beautiful opera,

00:15:25.260 --> 00:15:28.260
and we're moved by that opera.

00:15:28.260 --> 00:15:30.260
We're moved because it's about the battle

00:15:30.260 --> 00:15:32.260
between good and evil,

00:15:32.260 --> 00:15:34.260
about right and wrong.

00:15:34.260 --> 00:15:36.260
And we care about right and wrong.

00:15:36.260 --> 00:15:39.260
We care what happens in that opera.

00:15:39.260 --> 00:15:42.260
We care what happens in "Apocalypse Now."

00:15:42.260 --> 00:15:44.260
And we certainly care

00:15:44.260 --> 00:15:46.260
what happens with our technologies.

00:15:46.260 --> 00:15:48.260
We have so much power today,

00:15:48.260 --> 00:15:51.260
it is up to us to figure out what to do,

00:15:51.260 --> 00:15:53.260
and that's the good news.

00:15:53.260 --> 00:15:56.260
We're the ones writing this opera.

00:15:56.260 --> 00:15:58.260
This is our movie.

00:15:58.260 --> 00:16:01.260
We figure out what will happen with this technology.

00:16:01.260 --> 00:16:04.260
We determine how this will all end.

00:16:04.260 --> 00:16:06.260
Thank you.

00:16:06.260 --> 00:16:11.260
(Applause)

