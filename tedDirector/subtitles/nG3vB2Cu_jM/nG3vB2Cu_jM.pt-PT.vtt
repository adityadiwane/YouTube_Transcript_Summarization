WEBVTT
Kind: captions
Language: pt-PT

00:00:00.000 --> 00:00:07.000
Tradutor: Ilona Bastos
Revisora: Nuno Lima

00:00:16.450 --> 00:00:17.640
Poder.

00:00:18.190 --> 00:00:20.220
Esta é a palavra que nos vem à mente.

00:00:20.260 --> 00:00:22.100
Somos os novos tecnólogos.

00:00:22.100 --> 00:00:24.860
Temos muita informação,
por isso temos muito poder.

00:00:25.200 --> 00:00:26.990
Quanto poder temos?

00:00:27.360 --> 00:00:30.080
Cena de um filme: "Apocalypse Now"
— um grande filme.

00:00:30.080 --> 00:00:33.160
Temos que levar o herói, 
Capitão Willard, até à foz do Rio Nung

00:00:33.160 --> 00:00:35.020
para que ele persiga o Coronel Kurtz

00:00:35.020 --> 00:00:37.430
— transportá-lo por via aérea
e largá-lo no local.

00:00:37.430 --> 00:00:38.670
Eis a cena:

00:00:38.670 --> 00:00:42.050
o céu está coberto pela esquadrilha
de helicópteros que o transportam.

00:00:42.050 --> 00:00:44.260
No fundo, uma música
alta e arrebatadora,

00:00:44.260 --> 00:00:45.960
uma música selvagem.

00:00:45.960 --> 00:00:47.460
♫Dum da ta da dum ♫

00:00:47.460 --> 00:00:49.710
♫ Dum da ta da dum ♫

00:00:50.290 --> 00:00:52.200
♫ Da ta da da ♫

00:00:52.530 --> 00:00:54.490
É um enorme poder.

00:00:54.490 --> 00:00:56.610
É o tipo de poder que sinto nesta sala.

00:00:56.610 --> 00:00:58.260
É o tipo de poder que nós temos

00:00:58.260 --> 00:01:00.260
devido a toda a informação que temos.

00:01:01.150 --> 00:01:02.780
Tomemos um exemplo.

00:01:03.110 --> 00:01:04.520
O que é que podemos fazer

00:01:04.520 --> 00:01:07.260
com os dados de apenas uma pessoa?

00:01:07.640 --> 00:01:09.260
O que é que podemos fazer

00:01:09.260 --> 00:01:11.340
com os dados daquele indivíduo?

00:01:12.410 --> 00:01:14.320
Posso ver os seus registos financeiros.

00:01:14.320 --> 00:01:16.170
Posso dizer se paga as contas a horas,

00:01:16.170 --> 00:01:18.240
se tem condições para pedir um empréstimo.

00:01:18.240 --> 00:01:21.130
Posso ver os registos clínicos,
ver se o coração ainda bate,

00:01:21.130 --> 00:01:23.630
ver se está bem,
para lhe oferecer um seguro.

00:01:23.950 --> 00:01:26.070
Posso observar
os seus hábitos na Internet.

00:01:26.070 --> 00:01:29.060
Quando visita o meu sitio na Internet,
já sei o que vai fazer,

00:01:29.060 --> 00:01:31.600
porque o vi visitar milhões
de sítios anteriormente.

00:01:31.600 --> 00:01:32.880
E, lamento dizer-lhe,

00:01:32.880 --> 00:01:34.760
parece um jogador de póquer, descai-se.

00:01:34.760 --> 00:01:36.740
Analisando os dados, sei o que vai fazer

00:01:36.740 --> 00:01:38.880
antes mesmo de você o fazer.

00:01:38.880 --> 00:01:41.490
Eu sei do que você gosta.
Sei quem você é.

00:01:41.490 --> 00:01:45.000
E isso, antes mesmo de ver
o seu correio ou o seu telemóvel.

00:01:46.050 --> 00:01:48.020
Este é o tipo de coisas que podemos fazer

00:01:48.020 --> 00:01:50.400
com as informações que temos.

00:01:50.900 --> 00:01:54.010
Mas eu não estou aqui para falar 
do que podemos fazer.

00:01:56.590 --> 00:01:59.410
Estou aqui para falar
do que devemos fazer.

00:02:01.370 --> 00:02:03.600
Qual é a atuação correta?

00:02:05.080 --> 00:02:07.080
Estou a ver alguns olhares confusos, tipo:

00:02:07.080 --> 00:02:09.960
"Porque é que nos está a perguntar
qual é a atuação correta?

00:02:09.960 --> 00:02:13.170
"Nós só estamos a criar isto.
São outras pessoas que o utilizam."

00:02:13.490 --> 00:02:14.930
É verdade.

00:02:15.270 --> 00:02:17.290
Mas isso leva-me ao passado.

00:02:17.870 --> 00:02:19.620
Penso na II Guerra Mundial.

00:02:19.770 --> 00:02:22.020
Alguns dos nossos grandes
tecnólogos de então,

00:02:22.020 --> 00:02:23.710
alguns dos nossos grandes físicos,

00:02:23.710 --> 00:02:26.110
estudando a fissão e a fusão nuclear,

00:02:26.110 --> 00:02:27.670
apenas estudos nucleares.

00:02:27.670 --> 00:02:30.680
Nós reunimos estes físicos em Los Alamos

00:02:30.680 --> 00:02:33.260
para ver o que criariam.

00:02:33.960 --> 00:02:36.780
Nós queremos que as pessoas
que criam a tecnologia

00:02:36.780 --> 00:02:40.100
pensem no que devemos
fazer com a tecnologia.

00:02:41.640 --> 00:02:44.650
Então, o que devemos fazer
com os dados daquele indivíduo?

00:02:44.940 --> 00:02:47.610
Devemos colher estes dados, reuni-los,

00:02:47.610 --> 00:02:50.030
para melhorar a sua experiência online?

00:02:50.030 --> 00:02:51.930
Para ganharmos dinheiro?

00:02:51.930 --> 00:02:55.010
Para podermos proteger-nos
caso ele proceda mal

00:02:56.010 --> 00:02:59.220
Ou devemos respeitar a sua privacidade,

00:02:59.220 --> 00:03:01.910
proteger a sua dignidade
e deixá-lo em paz?

00:03:02.900 --> 00:03:04.880
O que devemos fazer?

00:03:05.600 --> 00:03:07.480
Como devemos decidir?

00:03:08.180 --> 00:03:11.100
Eu sei: colaboração pública.
Vamos resolver isto juntos.

00:03:11.840 --> 00:03:13.850
Para fazermos o aquecimento,

00:03:13.850 --> 00:03:16.080
vamos começar com uma questão simples,

00:03:16.080 --> 00:03:19.790
uma coisa a respeito da qual estou certo
de que todos aqui têm uma opinião:

00:03:19.790 --> 00:03:21.660
iPhone versus Android.

00:03:21.660 --> 00:03:24.190
Levantem as vossas mãos — iPhone!

00:03:25.610 --> 00:03:26.780
Ha-ha!

00:03:26.930 --> 00:03:28.270
Android!

00:03:29.590 --> 00:03:31.790
Esperava que um grupo de gente inteligente

00:03:31.790 --> 00:03:34.380
não seria tão apanhada
pelos telemóveis bonitinhos.

00:03:34.380 --> 00:03:36.110
(Risos)

00:03:36.290 --> 00:03:37.660
Próxima pergunta,

00:03:37.660 --> 00:03:39.260
um pouco mais difícil.

00:03:39.530 --> 00:03:42.210
Devemos recolher todos
os dados daquele indivíduo

00:03:42.210 --> 00:03:44.260
para melhorar as suas experiências

00:03:44.260 --> 00:03:46.910
e para nos protegermos no caso
de ele não se portar bem?

00:03:46.910 --> 00:03:48.670
Ou devemos deixá-lo em paz?

00:03:49.320 --> 00:03:51.480
Recolher os dados!

00:03:54.160 --> 00:03:55.780
Deixá-lo em paz!

00:03:57.110 --> 00:03:59.060
Estão safos. Está certo.

00:03:59.060 --> 00:04:00.420
(Risos)

00:04:00.420 --> 00:04:02.860
Muito bem, última pergunta,

00:04:02.860 --> 00:04:04.870
esta mais difícil.

00:04:05.170 --> 00:04:07.260
Quando tentamos avaliar

00:04:07.260 --> 00:04:10.260
o que devemos fazer neste caso,

00:04:10.260 --> 00:04:14.550
devemos usar o sistema
da moral deontológica de Kant,

00:04:14.550 --> 00:04:18.390
ou devemos usar
a teoria da causalidade de Mill?

00:04:19.860 --> 00:04:21.370
Kant!

00:04:23.430 --> 00:04:25.000
Mill.

00:04:26.290 --> 00:04:28.060
Já não há tantos votos.

00:04:28.120 --> 00:04:30.120
(Risos)

00:04:31.220 --> 00:04:33.960
Sim, é um resultado assustador.

00:04:35.270 --> 00:04:38.600
Assustador porque temos
opiniões mais firmes

00:04:38.600 --> 00:04:41.010
sobre os nossos aparelhos portáteis

00:04:41.010 --> 00:04:42.480
do que sobre os padrões morais

00:04:42.480 --> 00:04:45.120
que devemos usar para orientar
as nossas decisões.

00:04:45.120 --> 00:04:47.860
Como sabemos o que fazer
com todo o poder que temos

00:04:47.860 --> 00:04:50.260
se não temos um enquadramento moral?

00:04:50.910 --> 00:04:53.740
Sabemos mais
sobre sistemas operativos móveis,

00:04:53.740 --> 00:04:57.030
mas do que realmente necessitamos
é de um sistema operativo moral.

00:04:58.640 --> 00:05:00.660
O que é um sistema operativo moral?

00:05:00.660 --> 00:05:02.580
Todos sabemos o que é certo e errado.

00:05:02.580 --> 00:05:04.750
Sentimo-nos bem quando fazemos
uma coisa boa,

00:05:04.750 --> 00:05:06.800
sentimo-nos mal ao fazer uma coisa errada.

00:05:06.800 --> 00:05:09.970
Os nossos pais ensinam-nos
que se louva o bom, ralha-se ao mau.

00:05:09.970 --> 00:05:12.740
Mas como sabemos
o que é bom e o que é mau?

00:05:12.740 --> 00:05:15.770
No dia a dia, temos
as técnicas que usamos.

00:05:16.170 --> 00:05:19.050
Talvez apenas sigamos o nosso instinto.

00:05:19.050 --> 00:05:21.840
Talvez recorramos à votação
—pedimos a colaboração pública.

00:05:22.130 --> 00:05:23.630
Ou talvez pesquisemos

00:05:23.630 --> 00:05:26.550
— perguntamos ao departamento jurídico,
para ver o que dizem.

00:05:26.830 --> 00:05:29.270
Por outras palavras, é um pouco aleatório,

00:05:29.270 --> 00:05:30.840
um pouco "ad hoc",

00:05:30.840 --> 00:05:33.170
como decidimos o que devemos fazer.

00:05:33.480 --> 00:05:36.480
E talvez, se quisermos
sentir-nos mais seguros,

00:05:36.480 --> 00:05:39.540
o que realmente queiramos
seja um sistema moral que nos oriente,

00:05:39.590 --> 00:05:43.170
que nos diga que tipos de coisas
são, à partida, certas e erradas,

00:05:43.170 --> 00:05:46.570
e como sabemos o que fazer
numa determinada situação.

00:05:47.320 --> 00:05:49.260
Então, vamos arranjar um sistema moral.

00:05:49.260 --> 00:05:52.320
Somos pessoas que lidam com números,
vivemos através de números.

00:05:52.320 --> 00:05:53.890
Como podemos usar números

00:05:53.890 --> 00:05:56.260
como base de um sistema moral?

00:05:56.610 --> 00:05:59.830
Conheço um indivíduo
que fez exatamente isso.

00:06:00.300 --> 00:06:03.060
Era um indivíduo brilhante,

00:06:03.060 --> 00:06:05.470
morreu há 2500 anos.

00:06:06.480 --> 00:06:07.920
Platão, isso mesmo.

00:06:07.920 --> 00:06:09.770
Lembram-se dele, do velho filósofo?

00:06:09.770 --> 00:06:12.260
Vocês estavam a dormir nessa aula?

00:06:12.710 --> 00:06:15.100
Platão tinha grande parte
das nossas preocupações.

00:06:15.100 --> 00:06:17.070
Estava preocupado com o certo e o errado.

00:06:17.070 --> 00:06:18.860
Queria saber o que é justo.

00:06:18.860 --> 00:06:21.650
Mas preocupava-o o facto de que
tudo o que parecemos fazer

00:06:21.650 --> 00:06:23.450
é trocar opiniões sobre o assunto.

00:06:23.450 --> 00:06:26.140
Ele diz que isto é justo.
Ela diz que aquilo é justo.

00:06:26.140 --> 00:06:28.510
Ele parece convincente e ela também.

00:06:28.510 --> 00:06:31.320
Ando para trás e para a frente,
não chego a lugar nenhum.

00:06:31.320 --> 00:06:33.450
Não quero opiniões, quero conhecimento.

00:06:33.450 --> 00:06:35.960
Quero saber a verdade sobre a justiça,

00:06:35.960 --> 00:06:38.520
tal como temos verdades na matemática.

00:06:38.520 --> 00:06:41.880
Em matemática, conhecemos
factos objetivos.

00:06:41.880 --> 00:06:43.940
Escolham um número,
qualquer número — dois.

00:06:43.940 --> 00:06:45.910
O meu número favorito. Adoro-o.

00:06:46.260 --> 00:06:47.900
Há verdades sobre o dois.

00:06:47.900 --> 00:06:49.850
Se vocês tiverem dois de alguma coisa,

00:06:49.850 --> 00:06:51.850
adicionam dois e obtêm quatro.

00:06:51.860 --> 00:06:54.320
Isto é verdade, seja do que for
que estão a falar.

00:06:54.330 --> 00:06:56.450
É uma verdade objetiva
sobre a forma dois,

00:06:56.450 --> 00:06:57.640
a forma abstrata.

00:06:57.640 --> 00:07:00.510
Quando temos dois de alguma coisa
— dois olhos, duas orelhas,

00:07:00.510 --> 00:07:02.550
dois narizes, apenas duas saliências —

00:07:02.550 --> 00:07:04.810
todas elas participam da forma de dois.

00:07:04.810 --> 00:07:08.260
Todas elas participam
nas verdades que o dois tem.

00:07:08.590 --> 00:07:10.600
Todas têm em si o ser "dois".

00:07:10.600 --> 00:07:13.440
E, consequentemente, isto não é
uma questão de opinião.

00:07:13.840 --> 00:07:15.050
Platão pensou:

00:07:15.050 --> 00:07:17.590
"E se a ética fosse como a matemática?

00:07:17.990 --> 00:07:20.860
E se houvesse uma forma pura de justiça?

00:07:20.860 --> 00:07:23.200
E se houvesse verdades sobre a justiça

00:07:23.200 --> 00:07:25.320
e pudéssemos olhar
à nossa volta, neste mundo

00:07:25.320 --> 00:07:29.420
e ver quais as coisas
que participam dessa forma de justiça?

00:07:29.740 --> 00:07:32.540
Assim saberíamos o que é justo
e o que o não é.

00:07:32.540 --> 00:07:36.130
Não seria uma questão
de mera opinião ou aparência.

00:07:37.650 --> 00:07:39.810
É uma visão assombrosa.

00:07:39.810 --> 00:07:42.260
Pensem nisto.
Que grandioso! Que ambicioso!

00:07:42.260 --> 00:07:44.450
Tão ambicioso como nós somos.

00:07:44.450 --> 00:07:46.270
Ele quer resolver a ética.

00:07:46.910 --> 00:07:49.060
Quer verdades objetivas.

00:07:49.060 --> 00:07:51.260
Se pensarmos assim,

00:07:51.260 --> 00:07:54.370
têm um sistema moral platónico.

00:07:54.780 --> 00:07:56.290
Se não pensarem assim,

00:07:56.290 --> 00:07:59.050
terão muita companhia
na história da filosofia ocidental,

00:07:59.050 --> 00:08:01.910
porque as pessoas criticaram a ideia pura

00:08:01.910 --> 00:08:04.710
Aristóteles, em particular,
não a apreciou muito.

00:08:05.210 --> 00:08:07.170
Achou-a impraticável.

00:08:07.520 --> 00:08:10.860
Aristóteles disse: 
"Só devemos procurar em cada assunto

00:08:10.860 --> 00:08:13.610
"o nível de precisão
que esse assunto permita."

00:08:13.610 --> 00:08:16.480
Aristóteles pensou que a ética
não era como a matemática.

00:08:16.480 --> 00:08:19.720
Pensou que a ética tinha a ver
com tomar decisões aqui e agora

00:08:19.720 --> 00:08:21.380
usando o nosso melhor julgamento

00:08:21.380 --> 00:08:23.440
para encontrar o caminho certo.

00:08:23.600 --> 00:08:25.860
Se pensarem assim,
Platão não é o vosso tipo.

00:08:25.860 --> 00:08:27.530
Mas não desistam.

00:08:27.530 --> 00:08:29.260
Talvez haja outra maneira

00:08:29.260 --> 00:08:33.040
de podermos usar números
como base do nosso sistema moral.

00:08:34.110 --> 00:08:35.260
Que tal isto:

00:08:35.260 --> 00:08:38.490
E se em cada situação,
pudéssemos calcular,

00:08:38.490 --> 00:08:40.260
olhar para as opções,

00:08:40.260 --> 00:08:43.750
medir qual delas é a melhor
e saber o que fazer?

00:08:44.060 --> 00:08:45.560
Soa familiar?

00:08:45.700 --> 00:08:48.260
É um sistema moral utilitarista.

00:08:48.540 --> 00:08:50.670
John Stuart Mill foi seu grande defensor

00:08:50.670 --> 00:08:52.260
— um tipo fixe, aliás —

00:08:52.260 --> 00:08:54.260
e só morreu há 200 anos.

00:08:54.720 --> 00:08:56.540
Portanto, as bases do utilitarismo

00:08:56.540 --> 00:08:58.340
— de certeza que vos são familiares,

00:08:58.340 --> 00:09:00.660
pelo menos às três pessoas
que votaram em Mill.

00:09:00.700 --> 00:09:02.730
Mas eis a forma como isto funciona.

00:09:02.730 --> 00:09:05.160
Que tal se o que torna uma coisa moral,

00:09:05.160 --> 00:09:07.820
é só uma questão
de essa coisa maximizar o prazer

00:09:07.820 --> 00:09:09.800
e minimizar a dor?

00:09:10.000 --> 00:09:12.110
Trata-se de uma coisa intrínseca ao ato.

00:09:12.110 --> 00:09:14.810
Não tem que ver com a sua relação
com uma forma abstrata.

00:09:14.810 --> 00:09:16.900
É apenas uma questão de consequências.

00:09:16.900 --> 00:09:18.740
Olhamos apenas para as consequências

00:09:18.740 --> 00:09:21.270
e vemos se, no conjunto,
são para o bem ou para o mal.

00:09:21.270 --> 00:09:23.650
Isso seria simples. 
Assim saberíamos o que fazer.

00:09:23.650 --> 00:09:25.170
Tomemos um exemplo.

00:09:25.170 --> 00:09:26.970
Suponham que eu digo:

00:09:27.430 --> 00:09:29.040
"Vou-lhe tirar o seu telemóvel."

00:09:29.040 --> 00:09:30.840
Não só porque, há bocado, ele tocou,

00:09:30.840 --> 00:09:33.720
mas porque fiz um pequeno cálculo.

00:09:33.900 --> 00:09:36.390
Aquele indivíduo pareceu-me suspeito.

00:09:36.390 --> 00:09:40.010
E se ele tiver estado a enviar mensagens
para o esconderijo do Bin Laden,

00:09:40.010 --> 00:09:42.390
ou para quem quer
que tenha sucedido a Bin Laden?

00:09:42.390 --> 00:09:44.990
Parece mesmo um terrorista,
uma célula de espionagem.

00:09:44.990 --> 00:09:47.520
Vou descobrir isso e, quando descobrir,

00:09:47.520 --> 00:09:50.540
vou impedir os enormes prejuízos
que ele podia causar.

00:09:50.960 --> 00:09:53.750
Será muito útil
para evitar esses prejuízos.

00:09:53.750 --> 00:09:56.110
Em comparação com a pequena dor
que vai causar,

00:09:56.110 --> 00:09:58.590
— porque será embaraçoso
eu estar a ver o telemóvel,

00:09:58.590 --> 00:10:01.300
a ver que ele tem problemas
na Farmville, etc. —

00:10:01.300 --> 00:10:03.260
isso é minimizado

00:10:03.260 --> 00:10:05.880
pela vantagem de ver o telemóvel.

00:10:06.160 --> 00:10:07.590
Se vocês pensarem assim,

00:10:07.590 --> 00:10:10.690
trata-se de uma escolha utilitária.

00:10:11.390 --> 00:10:13.680
Mas talvez também 
não pensem dessa maneira.

00:10:13.680 --> 00:10:15.900
Talvez pensem, o telemóvel é dele.

00:10:15.920 --> 00:10:18.080
É errado tirar-lhe o telemóvel,

00:10:18.080 --> 00:10:21.810
porque é uma pessoa,
tem direitos, e tem dignidade,

00:10:22.160 --> 00:10:24.290
e não podemos simplesmente
interferir neles.

00:10:24.290 --> 00:10:25.510
Ele tem autonomia.

00:10:25.510 --> 00:10:27.720
Não interessam os cálculos.

00:10:27.720 --> 00:10:30.480
Há coisas que são
intrinsecamente erradas

00:10:30.480 --> 00:10:32.430
— mentir é errado,

00:10:32.430 --> 00:10:35.260
como é errado torturar crianças inocentes.

00:10:35.820 --> 00:10:38.450
Kant era muito bom neste ponto,

00:10:38.450 --> 00:10:40.700
e disse isto um pouco melhor
do que eu direi.

00:10:41.250 --> 00:10:43.300
Ele disse que devíamos usar a nossa razão

00:10:43.300 --> 00:10:46.220
para decidir as regras que
deviam orientar a nossa conduta.

00:10:46.220 --> 00:10:48.950
E depois é nosso dever
seguir essas regras.

00:10:49.330 --> 00:10:51.590
Não é uma questão de cálculos.

00:10:52.020 --> 00:10:53.660
Por isso vamos parar.

00:10:53.660 --> 00:10:56.730
Estamos mesmo no cerne
deste emaranhado filosófico.

00:10:57.180 --> 00:10:59.890
E isto continua durante milhares de anos,

00:10:59.890 --> 00:11:01.670
porque estas questões são difíceis,

00:11:01.670 --> 00:11:03.680
e só dispomos de 15 minutos.

00:11:03.680 --> 00:11:05.770
Por isso, vamos diretos ao que interessa.

00:11:05.770 --> 00:11:09.260
Como devemos tomar as nossas decisões?

00:11:09.630 --> 00:11:12.870
De acordo com Platão,
Aristóteles, Kant, Mill?

00:11:12.870 --> 00:11:15.160
O que devemos fazer? Qual é a resposta?

00:11:15.160 --> 00:11:18.380
Qual é a fórmula que podemos usar
em qualquer situação

00:11:18.380 --> 00:11:20.110
para determinar o que devemos fazer,

00:11:20.110 --> 00:11:22.790
se devemos ou não usar as informações
daquele individuo?

00:11:22.790 --> 00:11:24.260
Qual é a fórmula?

00:11:26.650 --> 00:11:28.650
Não há nenhuma fórmula.

00:11:29.630 --> 00:11:31.600
Não há uma resposta simples.

00:11:31.600 --> 00:11:33.440
A ética é difícil.

00:11:34.910 --> 00:11:37.060
A ética requer pensamento.

00:11:38.540 --> 00:11:40.260
E é desconfortável.

00:11:40.510 --> 00:11:43.790
Eu sei, passei grande parte
da minha carreira

00:11:43.790 --> 00:11:46.430
na inteligência artificial,
tentando construir máquinas

00:11:46.430 --> 00:11:48.990
que pudessem pensar
nalgumas destas questões por nós,

00:11:48.990 --> 00:11:50.280
dar-nos respostas.

00:11:50.280 --> 00:11:51.710
Mas não podem.

00:11:51.710 --> 00:11:53.730
Não se consegue pegar no pensamento humano

00:11:53.730 --> 00:11:55.550
e colocá-lo numa máquina.

00:11:55.550 --> 00:11:57.890
Somos nós que temos de pensar.

00:11:58.260 --> 00:12:01.670
Felizmente, não somos máquinas,
podemos pensar.

00:12:01.670 --> 00:12:04.800
Não só podemos pensar,
mas devemos fazê-lo.

00:12:05.800 --> 00:12:07.500
Hannah Arendt disse:

00:12:07.670 --> 00:12:09.520
"A triste verdade

00:12:09.520 --> 00:12:11.860
"é que o maior mal feito neste mundo

00:12:11.860 --> 00:12:14.820
"não é feito por pessoas
que escolhem ser más.

00:12:15.570 --> 00:12:18.260
"Ele surge da falta do pensamento."

00:12:19.250 --> 00:12:22.120
É o que ela denomina
a "banalidade do mal."

00:12:22.720 --> 00:12:24.260
E a resposta a isso

00:12:24.260 --> 00:12:27.290
é que exigimos o exercício do pensamento

00:12:27.290 --> 00:12:29.230
a todas as pessoas sãs.

00:12:29.810 --> 00:12:32.490
Vamos, então, fazer isso. Vamos pensar.

00:12:32.490 --> 00:12:34.760
De facto, vamos começar agora mesmo.

00:12:34.760 --> 00:12:37.440
Cada pessoa, nesta sala, faça o seguinte:

00:12:37.440 --> 00:12:40.600
Pensem na última vez que tiveram 
que tomar uma decisão

00:12:40.600 --> 00:12:43.030
em que se preocuparam
em fazer o que era certo,

00:12:43.030 --> 00:12:45.490
em que se perguntaram:
"O que devo fazer?"

00:12:45.490 --> 00:12:46.960
Recordem esse momento.

00:12:46.960 --> 00:12:49.420
E agora reflitam sobre ele e digam:

00:12:49.420 --> 00:12:52.050
"Como é que cheguei àquela decisão?

00:12:52.050 --> 00:12:54.510
"O que é que eu fiz? 
Segui a minha intuição?

00:12:54.510 --> 00:12:57.980
"Pedi a alguém para votar sobre o assunto?
Ou pedi um parecer jurídico?"

00:12:57.980 --> 00:12:59.880
— agora temos mais algumas opções.

00:12:59.880 --> 00:13:02.700
"Avaliei qual seria o prazer máximo

00:13:02.700 --> 00:13:03.870
como faria Mill?

00:13:03.870 --> 00:13:07.480
Ou, como Kant, usei a razão para decidir
o que era intrinsecamente correto?"

00:13:07.480 --> 00:13:09.660
Pensem nisso. Recordem-se.
Isto é importante.

00:13:09.660 --> 00:13:12.360
É tão importante
que vamos passar 30 segundos

00:13:12.360 --> 00:13:14.030
do valioso tempo do TED Talk

00:13:14.030 --> 00:13:16.070
a não fazer mais nada senão pensar nisto.

00:13:16.070 --> 00:13:17.880
Estão prontos? Comecem!

00:13:33.650 --> 00:13:36.180
Parem. Bom trabalho.

00:13:36.960 --> 00:13:38.620
O que acabaram de fazer

00:13:38.620 --> 00:13:41.500
é o primeiro passo no sentido
de assumir a responsabilidade

00:13:41.500 --> 00:13:44.290
sobre o que devemos fazer
com todo o nosso poder.

00:13:46.370 --> 00:13:48.940
Agora, o passo seguinte — tentem isto.

00:13:49.660 --> 00:13:51.870
Procurem um amigo e expliquem-lhe

00:13:51.870 --> 00:13:53.960
como tomaram aquela decisão.

00:13:54.090 --> 00:13:56.860
Não neste preciso momento.
Esperem que eu acabe de falar.

00:13:56.860 --> 00:13:58.140
Façam-no ao almoço.

00:13:58.140 --> 00:14:01.220
E não se limitem a fazê-lo com outro amigo
da área da tecnologia.

00:14:01.220 --> 00:14:03.070
Encontrem alguém diferente de vocês,

00:14:03.070 --> 00:14:04.470
um artista ou um escritor

00:14:04.470 --> 00:14:07.530
ou, o céu vos proteja,
encontrem um filósofo e falem com ele.

00:14:07.530 --> 00:14:10.180
De facto, encontrem alguém
da área de humanidades.

00:14:10.180 --> 00:14:12.270
Porquê? Porque eles pensam nos problemas

00:14:12.270 --> 00:14:14.470
de uma forma diferente
dos da área tecnológica.

00:14:14.470 --> 00:14:17.260
Há poucos dias, aqui em frente,
mesmo do outro lado da rua,

00:14:17.260 --> 00:14:18.890
reuniram-se centenas de pessoas,

00:14:18.890 --> 00:14:21.000
das áreas humanística e tecnológica

00:14:21.010 --> 00:14:23.480
naquela conferência
sobre Bibliotecas Tecnológicas.

00:14:23.480 --> 00:14:26.650
Juntaram-se todos, porque
os de tecnologia queriam aprender

00:14:26.740 --> 00:14:29.590
como seria pensar a partir
de uma perspetiva humanista.

00:14:29.620 --> 00:14:31.130
Havia pessoas do Google

00:14:31.130 --> 00:14:33.810
a falar com pessoas
que fazem literatura comparativa.

00:14:33.830 --> 00:14:36.690
Estão a pensar na relevância
do teatro francês do séc. XVII

00:14:36.690 --> 00:14:39.300
como é que ele influenciou
o capital de risco?

00:14:39.300 --> 00:14:41.840
Isso é interessante.
É uma forma diferente de pensar.

00:14:41.840 --> 00:14:43.610
E quando pensamos dessa maneira,

00:14:43.610 --> 00:14:46.430
tornamo-nos mais sensíveis
às considerações humanas,

00:14:46.430 --> 00:14:49.570
que são fundamentais
para a tomada de decisões éticas.

00:14:50.110 --> 00:14:52.210
Então, imaginem que agora mesmo

00:14:52.210 --> 00:14:54.410
encontravam o vosso amigo músico.

00:14:54.410 --> 00:14:56.600
E contavam-lhe aquilo
de que estamos a falar,

00:14:56.600 --> 00:14:58.360
sobre a revolução na informação,

00:14:58.360 --> 00:15:00.980
talvez até trauteassem partes
do nosso tema musical:

00:15:00.980 --> 00:15:03.560
♫ Dum ta da da dum dum ta da da dum ♫

00:15:03.560 --> 00:15:05.870
Aí, o vosso amigo músico
interrompe-vos e diz:

00:15:05.870 --> 00:15:08.000
"Sabes, a música de fundo

00:15:08.000 --> 00:15:09.980
"da vossa revolução da informação

00:15:09.980 --> 00:15:12.350
"é uma ópera, é Wagner.

00:15:12.350 --> 00:15:14.040
"É baseada na mitologia nórdica.

00:15:14.040 --> 00:15:16.140
"São deuses e criaturas míticas

00:15:16.140 --> 00:15:18.600
"a lutar por joias mágicas."

00:15:20.340 --> 00:15:22.260
É interessante.

00:15:22.430 --> 00:15:25.550
Mas também é uma ópera maravilhosa.

00:15:25.890 --> 00:15:28.450
Emocionamo-nos com aquela ópera.

00:15:28.640 --> 00:15:31.180
Ficamos emocionados
porque se trata de uma batalha

00:15:31.180 --> 00:15:32.810
entre o bem e o mal,

00:15:32.810 --> 00:15:34.640
sobre o certo e o errado.

00:15:34.640 --> 00:15:37.060
E nós preocupamo-nos
com o certo e o errado.

00:15:37.060 --> 00:15:39.940
Preocupamo-nos com
o que acontece naquela ópera.

00:15:39.940 --> 00:15:42.740
Preocupamo-nos com o que acontece
no "Apocalypse Now."

00:15:42.740 --> 00:15:44.260
E preocupamo-nos certamente

00:15:44.260 --> 00:15:46.700
com o que acontece
com as nossas tecnologias.

00:15:46.700 --> 00:15:49.330
Hoje temos muito poder,

00:15:49.330 --> 00:15:51.730
cabe-nos a nós decidir o que fazer.

00:15:51.730 --> 00:15:53.660
Essa é a boa notícia.

00:15:54.140 --> 00:15:56.470
Somos os únicos a escrever esta ópera.

00:15:56.530 --> 00:15:58.440
É o nosso filme.

00:15:58.440 --> 00:16:01.780
Nós decidimos o que vai acontecer
com esta tecnologia.

00:16:01.780 --> 00:16:04.260
Determinamos como tudo isto acabará.

00:16:05.200 --> 00:16:06.430
Obrigado.

00:16:06.610 --> 00:16:09.680
(Aplausos)

