WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:07.000
翻译人员: Junyi Sha
校对人员: Cindy Ma

00:00:13.000 --> 00:00:15.406
我想谈论一种我们
很多人都经历过的

00:00:15.406 --> 00:00:16.840
来自于直觉上的失误。

00:00:17.480 --> 00:00:20.520
它让人们无法察觉到
一种特定危险的存在。

00:00:21.360 --> 00:00:23.090
我要向大家描述一个情景，

00:00:23.090 --> 00:00:26.416
一个我觉得既令人害怕，

00:00:26.416 --> 00:00:28.160
却又很可能发生的情景。

00:00:28.840 --> 00:00:30.480
这样一个组合的出现，

00:00:30.480 --> 00:00:32.050
显然不是一个好的征兆。

00:00:32.050 --> 00:00:34.490
不过，在座的大部分人都会觉得，

00:00:34.490 --> 00:00:36.640
我要谈论的这件事其实挺酷的。

00:00:37.200 --> 00:00:40.216
我将描述我们从人工智能中

00:00:40.216 --> 00:00:41.940
获得的好处，

00:00:41.940 --> 00:00:43.770
将怎样彻底地毁灭我们。

00:00:43.770 --> 00:00:46.680
事实上，想看到人工智能
最终不摧毁我们是很难的，

00:00:46.680 --> 00:00:48.960
或者说它必将驱使我们自我毁灭。

00:00:49.400 --> 00:00:51.220
如果你和我有共同点，

00:00:51.220 --> 00:00:53.910
你会发现思考这些问题
是相当有趣的。

00:00:53.910 --> 00:00:57.310
而这种反应就是问题的一部分。

00:00:57.310 --> 00:00:59.080
因为这种想法应该使你感到担忧。

00:00:59.920 --> 00:01:02.550
假如我想在这个演讲中让你们相信，

00:01:02.550 --> 00:01:05.980
我们因为气候变化或者其他灾难，

00:01:05.980 --> 00:01:09.070
很可能会遭受全球性的饥荒，

00:01:09.070 --> 00:01:12.500
同时，你们的子孙后辈

00:01:12.500 --> 00:01:14.360
都可能在这样的饥荒中挣扎求生，

00:01:15.200 --> 00:01:16.400
你们就不会觉得

00:01:17.440 --> 00:01:18.730
“真有趣，

00:01:18.730 --> 00:01:20.000
我喜欢这个TED演讲。”

00:01:21.200 --> 00:01:22.720
因为饥荒一点都不有趣。

00:01:23.800 --> 00:01:27.170
不过，科幻小说中的死亡
往往却引人入胜。

00:01:27.170 --> 00:01:30.676
所以我现在最担心的一个问题是，

00:01:30.676 --> 00:01:35.340
人们对人工智能的发展将带来的危险，

00:01:35.340 --> 00:01:37.120
似乎还没有形成一个正确的认识。

00:01:37.120 --> 00:01:40.860
我也同样如此，所以我想
在这个演讲中和大家一起探讨。

00:01:42.120 --> 00:01:44.800
我们就像站在了两扇门前。

00:01:44.800 --> 00:01:46.080
在第一扇门后面，

00:01:46.080 --> 00:01:49.400
我们停下打造智能机器的脚步。

00:01:49.400 --> 00:01:53.420
某些原因也使我们停止了
对电脑软件和硬件的升级。

00:01:53.420 --> 00:01:56.480
现在让我们想一下为什么会这样。

00:01:57.080 --> 00:02:00.720
我的意思是，当我们认识到
智能和自动化不可估量的价值时，

00:02:00.720 --> 00:02:04.280
我们总会竭尽所能的改善这些科技。

00:02:05.200 --> 00:02:06.867
那么，什么会使我们停下脚步呢？

00:02:07.800 --> 00:02:09.600
一场大规模的核战争？

00:02:11.000 --> 00:02:12.560
一次全球性的瘟疫？

00:02:14.320 --> 00:02:15.566
一个小行星撞击了地球？

00:02:17.540 --> 00:02:20.190
或者是贾斯汀·比伯成为了美国总统？

00:02:20.190 --> 00:02:22.520
（笑声）

00:02:24.760 --> 00:02:28.680
重点是，总有一个事物
会摧毁人类现有的文明。

00:02:29.360 --> 00:02:33.640
你需要思考这个灾难究竟有多恐怖，

00:02:33.640 --> 00:02:36.980
才会永久性地阻止我们

00:02:36.980 --> 00:02:38.240
发展科技，

00:02:38.240 --> 00:02:40.270
永久性的。

00:02:40.270 --> 00:02:41.990
光想想它，
就觉得这将是人类历史上

00:02:41.990 --> 00:02:44.470
能发生的最惨绝人寰的事了。

00:02:44.470 --> 00:02:45.780
那么，我们唯一剩下的选择，

00:02:45.780 --> 00:02:48.140
就藏在第二扇门的后面，

00:02:48.140 --> 00:02:51.320
那就是我们持续
改进我们的智能机器，

00:02:51.320 --> 00:02:52.960
永不停歇。

00:02:53.720 --> 00:02:57.360
在将来的某一天，我们会
造出比我们更聪明的机器，

00:02:58.080 --> 00:03:00.150
一旦我们有了
比我们更聪明的机器，

00:03:00.150 --> 00:03:02.650
它们将进行自我改进。

00:03:02.650 --> 00:03:05.516
然后我们就会承担着
数学家IJ Good 所说的

00:03:05.516 --> 00:03:07.220
“智能爆炸”的风险，

00:03:07.220 --> 00:03:09.280
（科技进步的）
进程将不再受我们的控制。

00:03:10.120 --> 00:03:12.986
现在我们时常会看到
这样一些讽刺漫画，

00:03:12.986 --> 00:03:16.150
我们总会担心受到一些不怀好意的

00:03:16.150 --> 00:03:17.440
机器人军队的攻击。

00:03:17.440 --> 00:03:19.630
但这不是最可能出现的事情。

00:03:19.630 --> 00:03:25.040
我们的机器不会自动变得邪恶。

00:03:25.040 --> 00:03:27.826
所以，我们唯一的顾虑就是
我们将会打造

00:03:27.826 --> 00:03:29.726
比我们人类更有竞争力的机器。

00:03:29.726 --> 00:03:33.560
而一旦我们和它们的目标不一致，

00:03:33.560 --> 00:03:34.800
我们将会被摧毁。

00:03:35.960 --> 00:03:38.040
想想我们与蚂蚁的关系吧。

00:03:38.600 --> 00:03:40.230
我们不讨厌它们，

00:03:40.230 --> 00:03:42.330
我们不会去主动去伤害它们。

00:03:42.340 --> 00:03:44.370
实际上，我们经常会尽量
避免伤害蚂蚁。

00:03:44.370 --> 00:03:46.740
我们会选择从它们身边走过。

00:03:46.740 --> 00:03:48.910
但只要它们的存在

00:03:48.910 --> 00:03:51.420
妨碍到了我们达成目标，

00:03:51.420 --> 00:03:53.921
比如说当我们在建造这样一个建筑，

00:03:53.921 --> 00:03:55.941
我们会毫不手软地杀掉它们。

00:03:56.480 --> 00:03:59.350
所以我们的顾虑是，终将有一天
我们打造的机器，

00:03:59.350 --> 00:04:02.246
不管它们是否有意识，
它们终将会以

00:04:02.246 --> 00:04:04.590
我们对待蚂蚁的方式
来对待我们。

00:04:05.760 --> 00:04:08.520
我想很多人会说这很遥远。

00:04:09.360 --> 00:04:15.670
我打赌你们中有些人还会
怀疑超级人工智能是否可能实现，

00:04:15.670 --> 00:04:17.330
认为我是在小题大做。

00:04:17.330 --> 00:04:21.004
但是你很快会发现以下这些
假设中的某一个是有问题的。

00:04:21.004 --> 00:04:22.616
下面是仅有的三种假设：

00:04:23.800 --> 00:04:28.519
第一，智慧可以被看做
物理系统中的信息处理过程。

00:04:29.320 --> 00:04:31.539
事实上，这不仅仅是一个假设。

00:04:31.539 --> 00:04:35.390
我们已经在有些机器中
嵌入了智能系统，

00:04:35.390 --> 00:04:37.400
这些机器中很多已经

00:04:37.400 --> 00:04:40.120
有着超越普通人的智慧了。

00:04:40.840 --> 00:04:43.400
而且，我们也知道任何一点小事

00:04:43.400 --> 00:04:45.840
都可以引发所谓的“普遍智慧”，

00:04:45.840 --> 00:04:49.690
这是一种可以在不同领域间
灵活思考的能力，

00:04:49.690 --> 00:04:52.860
因为我们的大脑已经
成功做到了这些。对吧？

00:04:52.860 --> 00:04:56.780
我的意思是，
大脑里其实都是原子，

00:04:56.780 --> 00:05:01.350
只要我们继续建造这些原子体系，

00:05:01.350 --> 00:05:04.050
我们就能实现越来越多的智慧行为，

00:05:04.050 --> 00:05:06.656
我们最终将会，
当然除非我们被干扰，

00:05:06.660 --> 00:05:10.010
我们最终将会给我们的机器赋予

00:05:10.010 --> 00:05:11.350
广泛意义上的智能。

00:05:11.350 --> 00:05:15.040
我们要知道这个进程的速度并不重要，

00:05:15.040 --> 00:05:18.220
因为任何进程都足够
让我们走进死胡同。

00:05:18.220 --> 00:05:22.040
甚至不需要考虑摩尔定律，
也不需要用指数函数来衡量，

00:05:22.040 --> 00:05:23.680
这一切顺其自然都会发生。

00:05:25.480 --> 00:05:28.400
第二个假设是，我们会一直创新。

00:05:29.000 --> 00:05:31.760
去继续改进我们的智能机器。

00:05:33.000 --> 00:05:37.370
由于智慧的价值就是——

00:05:37.370 --> 00:05:40.930
提供我们所珍爱的事物，

00:05:40.940 --> 00:05:43.710
或是用于保护我们所珍视的一切。

00:05:43.710 --> 00:05:46.010
智慧就是我们最有价值的资源。

00:05:46.010 --> 00:05:47.530
所以我们想继续革新它。

00:05:47.530 --> 00:05:50.910
因为我们有很多需要
迫切解决的问题。

00:05:50.910 --> 00:05:54.160
我们想要治愈像阿兹海默症
和癌症这样的疾病，

00:05:54.960 --> 00:05:58.870
我们想要了解经济系统，
想要改善我们的气候科学，

00:05:58.870 --> 00:06:01.120
所以只要可能，
我们就会将革新继续下去。

00:06:01.120 --> 00:06:04.486
而且革新的列车早已驶出， 
车上却没有刹车。

00:06:05.880 --> 00:06:11.310
第三种假设是：
人类没有登上智慧的巅峰，

00:06:11.310 --> 00:06:13.160
甚至连接近可能都谈不上。

00:06:13.640 --> 00:06:15.240
这个想法十分关键。

00:06:15.240 --> 00:06:17.930
这就是为什么
我们所处的环境是很危险的，

00:06:17.930 --> 00:06:22.040
这也是为什么我们对风险的
直觉是不可靠的。

00:06:23.120 --> 00:06:25.840
现在，请大家想一下
谁是世界上最聪明的人。

00:06:26.640 --> 00:06:30.030
几乎每个人的候选名单里都会
有约翰·冯·诺伊曼。

00:06:30.030 --> 00:06:33.390
冯·诺伊曼留给周围人的印象

00:06:33.390 --> 00:06:37.460
就是他是那个时代当中最杰出的
数学家和物理学家，

00:06:37.460 --> 00:06:39.440
这些都是完好的记录在案的。

00:06:39.440 --> 00:06:43.240
即使他的故事里有一半是假的，

00:06:43.240 --> 00:06:44.390
都没有人会质疑

00:06:44.390 --> 00:06:46.820
他仍然是世界上最聪明的人之一。

00:06:46.820 --> 00:06:49.520
那么，让我们来看看智慧谱线吧。

00:06:50.320 --> 00:06:51.749
现在我们有了约翰·冯·诺伊曼，

00:06:53.560 --> 00:06:54.894
还有我们大家。

00:06:56.120 --> 00:06:57.380
另外还有一只鸡。

00:06:57.380 --> 00:06:59.330
（笑声）

00:06:59.330 --> 00:07:00.590
抱歉，母鸡的位置应该在这。

00:07:00.590 --> 00:07:01.860
（笑声）

00:07:01.860 --> 00:07:05.610
这个演讲已经够严肃了，
开个玩笑轻松一下。

00:07:05.610 --> 00:07:07.280
（笑声）

00:07:08.339 --> 00:07:11.790
然而，很可能的情况是，
智慧谱线上的内容

00:07:11.790 --> 00:07:14.960
已远远超出了我们的认知，

00:07:15.880 --> 00:07:19.050
如果我们建造了比
自身更聪明的机器，

00:07:19.050 --> 00:07:21.400
它们将非常可能
以超乎寻常的方式

00:07:21.400 --> 00:07:23.250
延展这个谱线，

00:07:23.250 --> 00:07:25.840
最终超越人类。

00:07:27.000 --> 00:07:31.300
仅仅从速度方面考虑，
我们就能够意识到这一点。

00:07:31.300 --> 00:07:36.400
那么，现在让我们来想象一下
我们刚建好一个超级人工智能机器，

00:07:36.400 --> 00:07:39.870
大概和斯坦福
或是麻省理工学院的研究员的

00:07:39.870 --> 00:07:42.190
平均水平差不多吧。

00:07:42.190 --> 00:07:44.990
但是，电路板要比生物系统

00:07:44.990 --> 00:07:46.440
运行速度快一百万倍，

00:07:46.440 --> 00:07:49.650
所以这个机器思考起来
会比那些打造它的大脑

00:07:49.650 --> 00:07:51.460
快一百万倍。

00:07:51.460 --> 00:07:53.150
当你让它运行一周后，

00:07:53.150 --> 00:07:57.760
它将能呈现出相当于人类智慧在
20000年间发展出的水平，

00:07:58.400 --> 00:08:00.360
而这个过程将周而复始。

00:08:01.640 --> 00:08:04.720
那么，我们又怎么能理解，
更不用说去制约

00:08:04.720 --> 00:08:07.040
一个以如此速度运行的机器呢？

00:08:08.840 --> 00:08:11.026
坦白讲，另一件令人担心的事就是，

00:08:11.026 --> 00:08:15.950
我们考虑一下最理想的情景。

00:08:15.950 --> 00:08:20.150
想象我们正好做出了
一个没有任何安全隐患的

00:08:20.150 --> 00:08:21.570
超级人工智能。

00:08:21.570 --> 00:08:24.840
我们有了一个前所未有的完美设计。

00:08:24.840 --> 00:08:27.090
就好像我们被赐予了一件神物，

00:08:27.090 --> 00:08:29.120
它能够准确的执行目标动作。

00:08:29.120 --> 00:08:32.880
这个机器将完美的节省人力工作。

00:08:33.680 --> 00:08:36.083
它设计出的机器
能够再生产其他机器，

00:08:36.083 --> 00:08:37.860
去完成所有的人力工作。

00:08:37.860 --> 00:08:39.360
由太阳能供电，

00:08:39.360 --> 00:08:42.050
而成本的多少仅取决于原材料。

00:08:42.050 --> 00:08:45.370
那么，我们正在谈论的
就是人力劳动的终结。

00:08:45.370 --> 00:08:48.200
也关乎脑力劳动的终结。

00:08:49.200 --> 00:08:52.240
那在这种情况下，
像我们这样的"大猩猩"还能有什么用呢？

00:08:52.240 --> 00:08:56.360
我们可以悠闲地玩飞盘，
给彼此做按摩。

00:08:57.840 --> 00:09:00.660
服用一些迷药，
穿一些奇装异服，

00:09:00.660 --> 00:09:02.860
整个世界都沉浸在狂欢节之中。

00:09:02.860 --> 00:09:04.560
（笑声）

00:09:06.320 --> 00:09:08.320
那可能听起来挺棒的，

00:09:09.280 --> 00:09:11.620
不过让我们扪心自问，

00:09:11.620 --> 00:09:14.416
在现有的经济和政治体制下，
这意味着什么？

00:09:14.420 --> 00:09:16.830
我们很可能会目睹

00:09:16.830 --> 00:09:21.010
前所未有的贫富差距

00:09:21.010 --> 00:09:22.400
和失业率。

00:09:22.400 --> 00:09:25.130
有钱人不愿意马上把这笔新的财富

00:09:25.130 --> 00:09:26.680
贡献出来服务社会，

00:09:27.640 --> 00:09:31.220
这时一些千万富翁能够优雅地
登上商业杂志的封面，

00:09:31.220 --> 00:09:33.720
而剩下的人可能都在挨饿。

00:09:34.320 --> 00:09:36.616
如果听说硅谷里的公司

00:09:36.620 --> 00:09:39.250
即将造出超级人工智能，

00:09:39.250 --> 00:09:42.010
俄国人和中国人
会采取怎样的行动呢？

00:09:42.010 --> 00:09:44.850
那个机器将能够
以一种前所未有的能力

00:09:44.850 --> 00:09:46.840
去开展由领土问题和

00:09:46.840 --> 00:09:49.040
网络问题引发的战争。

00:09:50.120 --> 00:09:51.940
这是一个胜者为王的世界。

00:09:51.940 --> 00:09:55.120
机器世界中的半年，

00:09:55.120 --> 00:09:58.126
在现实世界至少会相当于

00:09:58.126 --> 00:09:59.456
50万年。

00:09:59.460 --> 00:10:04.190
所以仅仅是关于这种科技突破的传闻，

00:10:04.190 --> 00:10:06.560
就可以让我们的种族丧失理智。

00:10:06.560 --> 00:10:09.530
在我的观念里，

00:10:09.530 --> 00:10:12.330
当前最可怕的东西

00:10:12.330 --> 00:10:16.630
正是人工智能的研究人员

00:10:16.630 --> 00:10:18.240
安慰我们的那些话。

00:10:19.000 --> 00:10:22.096
最常见的理由就是关于时间。

00:10:22.100 --> 00:10:24.480
他们会说，现在开始担心还为时尚早。

00:10:24.480 --> 00:10:27.000
这很可能是50年或者
100年之后才需要担心的事。

00:10:27.720 --> 00:10:28.950
一个研究人员曾说过，

00:10:28.950 --> 00:10:30.540
“担心人工智能的安全性

00:10:30.540 --> 00:10:32.880
就好比担心火星上人口过多一样。”

00:10:34.116 --> 00:10:35.950
这就是硅谷版本的

00:10:35.950 --> 00:10:38.040
“不要杞人忧天。”

00:10:38.040 --> 00:10:39.490
（笑声）

00:10:39.490 --> 00:10:41.390
似乎没有人注意到

00:10:41.390 --> 00:10:44.030
以时间作为参考系

00:10:44.030 --> 00:10:46.640
是得不出合理的结论的。

00:10:46.640 --> 00:10:49.920
如果说智慧只包括信息处理，

00:10:49.920 --> 00:10:52.600
然后我们继续改善这些机器，

00:10:52.600 --> 00:10:55.520
那么我们终将生产出超级智能。

00:10:56.320 --> 00:10:59.970
但是，我们无法预估将花费多长时间

00:10:59.970 --> 00:11:02.400
来创造实现这一切的安全环境。

00:11:04.200 --> 00:11:05.480
我再重复一遍。

00:11:05.480 --> 00:11:09.310
我们无法预估将花费多长时间

00:11:09.310 --> 00:11:11.600
来创造实现这一切的安全环境。

00:11:12.920 --> 00:11:16.350
你们可能没有注意过，
50年的概念已今非昔比。

00:11:16.350 --> 00:11:18.840
这是用月来衡量50年的样子。
（每个点表示一个月）

00:11:18.840 --> 00:11:20.720
红色的点是代表苹果手机出现的时间。

00:11:21.440 --> 00:11:24.040
这是《辛普森一家》（动画片）
在电视上播出以来的时间。

00:11:24.680 --> 00:11:27.050
要做好准备面对
人类历史上前所未有的挑战，

00:11:27.050 --> 00:11:30.240
50年时间并不是很长。

00:11:31.640 --> 00:11:35.640
就像我刚才说的，
我们对确定会来临的事情

00:11:35.640 --> 00:11:38.340
做出了不合理的回应。

00:11:38.340 --> 00:11:42.350
计算机科学家斯图尔特·罗素
给出了一个极好的类比。

00:11:42.350 --> 00:11:47.250
他说，想象我们从
外太空接收到一条讯息，

00:11:47.250 --> 00:11:48.970
上面写着：

00:11:48.970 --> 00:11:50.560
“地球上的人类，

00:11:50.560 --> 00:11:52.960
我们将在五十年后到达你们的星球，

00:11:53.800 --> 00:11:55.350
做好准备吧。”

00:11:55.350 --> 00:11:59.610
于是我们就开始倒计时，
直到它们的“母舰”着陆吗？

00:11:59.610 --> 00:12:02.680
在这种情况下我们会感到更紧迫。

00:12:04.680 --> 00:12:06.520
另外一个试图安慰我们的理由是，

00:12:06.520 --> 00:12:09.560
那些机器必须
拥有和我们一样的价值观，

00:12:09.560 --> 00:12:12.190
因为它们将会是我们自身的延伸。

00:12:12.190 --> 00:12:14.020
它们将会被嫁接到我们的大脑上，

00:12:14.020 --> 00:12:16.440
我们将会成它们的边缘系统。

00:12:17.120 --> 00:12:18.530
现在我们再思考一下

00:12:18.530 --> 00:12:21.710
最安全的，也是唯一经慎重考虑后

00:12:21.710 --> 00:12:23.060
推荐的发展方向，

00:12:23.060 --> 00:12:25.920
是将这项技术直接植入我们大脑。

00:12:26.600 --> 00:12:29.960
这也许确实是最安全的，
也是唯一慎重的发展方向，

00:12:29.960 --> 00:12:33.030
但通常在我们把它塞进脑袋之前，

00:12:33.030 --> 00:12:36.700
会充分考虑这项技术的安全性。

00:12:36.700 --> 00:12:38.690
（笑声）

00:12:38.690 --> 00:12:44.100
更深一层的问题是：
仅仅制造出超级人工智能机器

00:12:44.100 --> 00:12:45.860
可能要比

00:12:45.860 --> 00:12:47.760
既制造超级人工智能，

00:12:47.760 --> 00:12:49.576
又让其拥有能让
我们的思想和超级人工智能

00:12:49.580 --> 00:12:52.830
无缝对接的完整的
神经科学系统要简单很多。

00:12:52.830 --> 00:12:55.960
而做这些研究的公司或政府，

00:12:55.960 --> 00:12:59.656
很可能将彼此视作竞争对手，

00:12:59.660 --> 00:13:02.930
因为赢得了比赛就意味着称霸了世界，

00:13:02.930 --> 00:13:05.370
前提是不在刚成功后就将其销毁，

00:13:05.370 --> 00:13:08.020
所以结论是：简单的选项

00:13:08.020 --> 00:13:09.280
一定会被先实现。

00:13:10.560 --> 00:13:13.400
但很遗憾，
除了建议更多人去思考这个问题，

00:13:13.400 --> 00:13:15.800
我对此并无解决方案。

00:13:15.800 --> 00:13:18.320
我觉得在人工智能问题上，

00:13:18.320 --> 00:13:20.440
我们需要一个“曼哈顿计划”
（二战核武器研究计划），

00:13:20.440 --> 00:13:23.160
不是用于讨论如何制造人工智能，
因为我们一定会这么做，

00:13:23.160 --> 00:13:26.570
而是去避免军备竞赛，

00:13:26.570 --> 00:13:30.110
最终以一种有利于
我们的方式去打造它。

00:13:30.110 --> 00:13:32.190
当你在谈论一个可以自我改造的

00:13:32.190 --> 00:13:34.510
超级人工智能时，

00:13:34.510 --> 00:13:39.180
我们似乎只有
一次正确搭建初始系统的机会，

00:13:39.180 --> 00:13:41.240
而这个正确的初始系统

00:13:41.240 --> 00:13:44.360
需要我们在经济以及政治上
做出很大的努力。

00:13:45.760 --> 00:13:47.780
但是当我们承认

00:13:47.780 --> 00:13:51.840
信息处理是智慧的源头，

00:13:52.720 --> 00:13:57.520
承认一些电脑系统是智能的基础，

00:13:58.360 --> 00:14:02.120
承认我们会不断改善这些系统，

00:14:03.280 --> 00:14:07.720
承认我们现存的认知远没有达到极限，

00:14:07.720 --> 00:14:08.960
将很可能被超越，

00:14:10.120 --> 00:14:11.310
我们又必须同时承认

00:14:11.310 --> 00:14:14.000
我们在某种意义上
正在创造一个新的“上帝”。

00:14:15.400 --> 00:14:16.960
现在正是思考人类是否

00:14:16.960 --> 00:14:19.623
能与这个“上帝”和睦相处的最佳时机。

00:14:20.120 --> 00:14:21.400
非常感谢！

00:14:21.400 --> 00:14:26.773
（掌声）

