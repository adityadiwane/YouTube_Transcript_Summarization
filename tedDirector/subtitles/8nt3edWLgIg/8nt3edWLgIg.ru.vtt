WEBVTT
Kind: captions
Language: ru

00:00:00.000 --> 00:00:07.000
Переводчик: Effy May
Редактор: Alexander Treshin

00:00:13.000 --> 00:00:15.216
Я хочу поговорить
о неспособности к интуиции,

00:00:15.240 --> 00:00:16.880
от которой страдают многие из нас.

00:00:17.480 --> 00:00:20.870
Фактически, это неспособность
распознавать определённую опасность.

00:00:21.360 --> 00:00:23.096
Я опишу сценарий,

00:00:23.120 --> 00:00:26.376
который назвал бы
одновременно пугающим

00:00:26.400 --> 00:00:28.160
и вполне вероятным,

00:00:28.840 --> 00:00:30.496
и это не хорошее сочетание,

00:00:30.520 --> 00:00:31.466
как выясняется.

00:00:32.060 --> 00:00:34.686
Всё же вместо того, чтобы пугаться,
многие подумают,

00:00:34.710 --> 00:00:36.640
что то, о чём я говорю, здóрово.

00:00:37.200 --> 00:00:40.176
Я расскажу, как наши достижения

00:00:40.200 --> 00:00:41.976
в развитии искусственного интеллекта

00:00:42.000 --> 00:00:43.776
могут в итоге нас уничтожить.

00:00:43.800 --> 00:00:47.256
По-моему, вообще трудно понять,
как они могут нас не уничтожить

00:00:47.280 --> 00:00:48.960
или не побудить к самоуничтожению.

00:00:49.400 --> 00:00:51.256
Всё же, если вы в чём-то схожи со мной,

00:00:51.280 --> 00:00:53.936
то вам нравится думать о таких вещах.

00:00:53.960 --> 00:00:57.336
И эта реакция — часть проблемы.

00:00:57.360 --> 00:00:59.080
Эта реакция должна вас беспокоить.

00:00:59.920 --> 00:01:02.576
И если бы мне надо было убедить вас,

00:01:02.600 --> 00:01:06.016
что нам грозит всемирный голод

00:01:06.040 --> 00:01:09.096
из-за изменения климата
или некой другой катастрофы

00:01:09.120 --> 00:01:12.536
и что ваши внуки или их внуки,

00:01:12.560 --> 00:01:14.360
вероятно, будут жить вот так,

00:01:15.200 --> 00:01:16.400
вы бы не подумали:

00:01:17.440 --> 00:01:18.776
«Интересно.

00:01:18.800 --> 00:01:20.000
Хорошее выступление».

00:01:21.200 --> 00:01:22.720
В голоде нет ничего забавного.

00:01:23.800 --> 00:01:27.176
Смерть от научной фантастики,
с другой стороны, — это круто,

00:01:27.200 --> 00:01:30.816
и меня очень сильно беспокоит
на данном этапе развития ИИ

00:01:30.840 --> 00:01:35.296
наша неспособность к выстраиванию
подобающей эмоциональной реакции

00:01:35.320 --> 00:01:37.136
на ожидающие нас опасности.

00:01:37.160 --> 00:01:40.360
Я сам не могу выстроить эту реакцию,
потому и говорю об этом.

00:01:42.120 --> 00:01:44.816
Как будто мы стои́м перед двумя дверьми.

00:01:44.840 --> 00:01:46.096
За первой дверью

00:01:46.120 --> 00:01:49.416
у нас останавливается прогресс
в создании интеллектуальных устройств.

00:01:49.440 --> 00:01:53.456
Аппаратное и программное обеспечение
почему-то больше не становится лучше.

00:01:53.480 --> 00:01:56.480
Теперь на секунду задумайтесь,
почему это может произойти.

00:01:57.080 --> 00:02:00.736
Ведь учитывая, насколько ценны
интеллект и автоматизация,

00:02:00.760 --> 00:02:04.280
мы продолжим совершенствовать
технологии, пока способны на это.

00:02:05.200 --> 00:02:06.867
Что может нас остановить?

00:02:07.800 --> 00:02:09.600
Полномасштабная ядерная война?

00:02:11.000 --> 00:02:12.560
Глобальная пандемия?

00:02:14.320 --> 00:02:15.640
Падение астероида?

00:02:17.640 --> 00:02:20.216
Избрание Джастина Бибера президентом США?

00:02:20.240 --> 00:02:22.520
(Смех)

00:02:24.760 --> 00:02:28.680
Суть в том, что что-то должно будет
уничтожить цивилизацию в нашем понимании.

00:02:29.360 --> 00:02:33.656
Только представьте себе,
насколько всё должно быть плохо,

00:02:33.680 --> 00:02:37.016
чтобы мы перестали
совершенствовать технологии

00:02:37.040 --> 00:02:38.256
навсегда,

00:02:38.280 --> 00:02:40.056
на все грядущие поколения.

00:02:40.080 --> 00:02:42.136
Практически по определению
это худшее,

00:02:42.160 --> 00:02:44.496
что может случиться с человечеством.

00:02:44.520 --> 00:02:45.816
Единственная альтернатива,

00:02:45.840 --> 00:02:48.176
которая и находится за второй дверью, —

00:02:48.200 --> 00:02:51.336
продолжение совершенствования
интеллектуальных устройств

00:02:51.360 --> 00:02:52.960
из года в год.

00:02:53.720 --> 00:02:57.360
В какой-то момент мы создадим
устройства, которые будут умнее нас,

00:02:58.080 --> 00:03:00.606
и когда такие устройства появятся,

00:03:00.630 --> 00:03:02.666
они начнут улучшать себя сами.

00:03:02.700 --> 00:03:05.456
И тогда появится риск того,
что математик И. Д. Гуд называл

00:03:05.480 --> 00:03:07.256
«взрывом интеллекта» —

00:03:07.280 --> 00:03:09.280
риск утраты нами контроля над процессом.

00:03:10.120 --> 00:03:12.936
Это часто изображают
в виде подобных картинок,

00:03:12.960 --> 00:03:16.176
пугающих армиями злых роботов,

00:03:16.200 --> 00:03:17.456
которые нападут на нас.

00:03:17.480 --> 00:03:20.176
Но это не самый вероятный сценарий.

00:03:20.200 --> 00:03:25.056
Проблема не в том, что наша техника
внезапно рассвирепеет.

00:03:25.080 --> 00:03:27.696
Беспокоит на самом деле то,
что мы создаём устройства,

00:03:27.720 --> 00:03:29.776
которые настолько нас превосходят,

00:03:29.800 --> 00:03:33.576
что малейшее расхождение
между их целями и нашими

00:03:33.600 --> 00:03:34.800
может нас уничтожить.

00:03:35.960 --> 00:03:38.060
Подумайте о том, как мы
относимся к муравьям.

00:03:38.600 --> 00:03:40.256
Мы не ненавидим их.

00:03:40.280 --> 00:03:42.336
Мы не лезем из кожи вон,
чтобы навредить им.

00:03:42.360 --> 00:03:44.736
На самом деле, иногда мы
стараемся не навредить им.

00:03:44.760 --> 00:03:46.776
Мы перешагиваем их на тротуаре.

00:03:46.800 --> 00:03:48.936
Но если вдруг их присутствие

00:03:48.960 --> 00:03:51.456
серьёзно мешает нашим целям,

00:03:51.480 --> 00:03:53.717
например, если мы
строим здание вроде этого,

00:03:53.981 --> 00:03:55.941
мы без колебаний их истребляем.

00:03:56.480 --> 00:03:59.416
Беспокоит то, что однажды
мы создадим устройства,

00:03:59.440 --> 00:04:02.176
которые независимо
от наличия у них сознания

00:04:02.200 --> 00:04:04.200
могут так же пренебречь нами.

00:04:05.760 --> 00:04:08.520
Я подозреваю, многим из вас
это кажется надуманным.

00:04:09.360 --> 00:04:15.476
Уверен, среди вас есть сомневающиеся
в возможности сверхразумного ИИ,

00:04:15.490 --> 00:04:17.376
не говоря уж о его неизбежности.

00:04:17.400 --> 00:04:21.020
Значит, вы не согласны
с одним из следующих допущений.

00:04:21.044 --> 00:04:22.616
Их всего три.

00:04:23.800 --> 00:04:28.519
Интеллект — это результат обработки
информации в физической системе.

00:04:29.320 --> 00:04:31.935
На самом деле это немного больше,
чем допущение.

00:04:31.959 --> 00:04:35.416
Мы заложили ограниченный интеллект
в свои технологии,

00:04:35.440 --> 00:04:37.456
и многие из них уже работают

00:04:37.480 --> 00:04:40.120
на уровне сверхчеловеческого интеллекта.

00:04:40.840 --> 00:04:42.876
И мы знаем, что из одной лишь материи

00:04:42.900 --> 00:04:46.056
возможно развитие так называемого
«общего интеллекта» —

00:04:46.080 --> 00:04:49.736
способности к гибкому мышлению
в разных сферах,

00:04:49.760 --> 00:04:52.896
потому что на это способен наш мозг.
Так?

00:04:52.920 --> 00:04:56.856
Я имею в виду: здесь всего лишь атомы,

00:04:56.880 --> 00:05:01.376
и покуда мы продолжаем
создавать системы из атомов,

00:05:01.400 --> 00:05:04.096
проявляющие всё более разумное поведение,

00:05:04.120 --> 00:05:06.656
в итоге мы — если ничто не помешает —

00:05:06.680 --> 00:05:10.056
в итоге мы наделим общим интеллектом

00:05:10.080 --> 00:05:11.376
свои устройства.

00:05:11.400 --> 00:05:15.056
Очень важно осознавать,
что темп прогресса не важен,

00:05:15.080 --> 00:05:18.256
потому что любой прогресс
может привести нас к конечному итогу.

00:05:18.280 --> 00:05:22.056
Нам не нужно, чтобы закон Мура работал.
Нам не нужен экспоненциальный прогресс.

00:05:22.080 --> 00:05:23.860
Нам просто нужно не останавливаться.

00:05:25.480 --> 00:05:28.400
Второе предположение —
что мы не остановимся.

00:05:29.000 --> 00:05:31.760
Мы продолжим улучшать
свои интеллектуальные устройства.

00:05:33.000 --> 00:05:37.376
И, учитывая ценность интеллекта —

00:05:37.400 --> 00:05:40.936
ведь интеллект либо порождает всё,
что мы ценим,

00:05:40.960 --> 00:05:43.736
либо нужен нам для защиты всего,
что мы ценим, —

00:05:43.760 --> 00:05:46.016
это наш ценнейший ресурс.

00:05:46.040 --> 00:05:47.576
Поэтому мы хотим этим заниматься.

00:05:47.600 --> 00:05:50.936
У нас есть проблемы,
которые непременно нужно решить.

00:05:50.960 --> 00:05:54.160
Мы хотим излечить болезни
вроде Альцгеймера и рака.

00:05:54.960 --> 00:05:58.896
Мы хотим понимать экономические системы.
Мы хотим больше знать о климате.

00:05:58.920 --> 00:06:01.176
Так что мы это сделаем, если сможем.

00:06:01.200 --> 00:06:04.486
Поезд уже тронулся, и тóрмоза нет.

00:06:05.880 --> 00:06:11.336
Наконец, мы не на пике интеллекта

00:06:11.360 --> 00:06:13.160
и даже не близко к нему, вероятно.

00:06:13.640 --> 00:06:15.536
И это действительно важно понимать.

00:06:15.560 --> 00:06:17.976
Именно из-за этого
наша система так уязвима,

00:06:18.000 --> 00:06:22.040
а наши интуитивные оценки риска
так ненадёжны.

00:06:23.020 --> 00:06:26.160
Теперь представьте умнейшего
из когда-либо живших людей.

00:06:26.640 --> 00:06:29.996
Почти все среди прочих называют
Джона фон Неймана.

00:06:30.020 --> 00:06:33.416
Ведь впечатление, которое фон Нейман
производил на окружающих,

00:06:33.440 --> 00:06:37.496
в том числе величайших математиков
и физиков своего времени,

00:06:37.520 --> 00:06:39.456
очень хорошо задокументировано.

00:06:39.480 --> 00:06:43.256
Если половина историй о нём
хотя бы наполовину правдивы,

00:06:43.280 --> 00:06:44.496
сомнений нет:

00:06:44.520 --> 00:06:46.976
он один из умнейших людей,
когда-либо живших.

00:06:47.000 --> 00:06:49.520
Подумайте о диапазоне
интеллектуальных способностей.

00:06:50.320 --> 00:06:51.749
Здесь у нас Джон фон Нейман.

00:06:53.560 --> 00:06:54.894
Здесь мы с вами.

00:06:56.120 --> 00:06:57.416
А здесь курица.

00:06:57.440 --> 00:06:59.376
(Смех)

00:06:59.400 --> 00:07:00.616
Прошу прощения.
Курица.

00:07:00.640 --> 00:07:01.896
(Смех)

00:07:01.920 --> 00:07:05.656
Ни к чему делать эту лекцию
более депрессивной, чем надо.

00:07:05.680 --> 00:07:07.280
(Смех)

00:07:08.339 --> 00:07:11.816
Однако кажется чрезвычайно вероятным,
что интеллект варьируется

00:07:11.840 --> 00:07:14.960
в куда бóльших масштабах,
чем мы себе сейчас представляем,

00:07:15.880 --> 00:07:19.096
и если мы создадим устройства,
которые будут умнее нас,

00:07:19.120 --> 00:07:21.416
они, очень вероятно,
будут осваивать эти масштабы

00:07:21.440 --> 00:07:23.296
невообразимыми для нас способами

00:07:23.320 --> 00:07:25.840
и превзойдут нас невообразимыми способами.

00:07:27.000 --> 00:07:31.336
И важно признавать, что это верно
в силу одной только скорости.

00:07:31.360 --> 00:07:36.416
Так? Представьте, что мы построили
сверхразумный ИИ,

00:07:36.440 --> 00:07:39.896
который не умнее
среднестатистической группы учёных

00:07:39.920 --> 00:07:41.916
из Стэнфорда или МТИ.

00:07:41.940 --> 00:07:45.216
Электронные схемы примерно
в миллион раз быстрее

00:07:45.240 --> 00:07:46.496
биохимических,

00:07:46.520 --> 00:07:49.656
поэтому машина будет думать
в миллион раз быстрее,

00:07:49.680 --> 00:07:51.496
чем создавший её разум.

00:07:51.520 --> 00:07:53.176
Вы запускаете её на неделю,

00:07:53.200 --> 00:07:57.760
и она выполняет 20 000-летнюю
работу интеллекта человеческого уровня

00:07:58.400 --> 00:08:00.360
неделю за неделей.

00:08:01.640 --> 00:08:04.736
Как мы только можем понять,
не говоря о том, чтобы сдержать,

00:08:04.760 --> 00:08:07.040
ум, работающий с подобной скоростью?

00:08:08.840 --> 00:08:10.976
Ещё беспокоит, если честно,

00:08:11.000 --> 00:08:15.976
вот что: представьте себе
наилучший сценарий.

00:08:16.000 --> 00:08:19.696
Представьте, что у нас в распоряжении
проект сверхразумного ИИ,

00:08:19.720 --> 00:08:21.576
не угрожающего безопасности.

00:08:21.600 --> 00:08:24.856
У нас сразу оказывается
идеальная разработка.

00:08:24.880 --> 00:08:27.096
Как будто она дана нам свыше

00:08:27.120 --> 00:08:29.136
и работает точно так, как задумано.

00:08:29.160 --> 00:08:33.020
Это устройство было бы идеально
для снижения трудозатрат.

00:08:33.510 --> 00:08:36.039
Оно может придумать машину,
которая построит машину,

00:08:36.073 --> 00:08:37.976
которая сможет делать физическую работу,

00:08:38.010 --> 00:08:39.376
питаясь энергией солнца

00:08:39.400 --> 00:08:42.096
приблизительно по цене сырья.

00:08:42.120 --> 00:08:45.376
Это означает конец
изматывающей работы для людей.

00:08:45.400 --> 00:08:48.260
Это также означает конец
большей части умственной работы.

00:08:49.200 --> 00:08:52.256
А что такие приматы, как мы,
делают в такой ситуации?

00:08:52.280 --> 00:08:56.360
Ну, мы сможем играть во фрисби
и делать друг другу массаж.

00:08:57.840 --> 00:09:00.696
Добавьте немного ЛСД
и немного смелых нарядов,

00:09:00.720 --> 00:09:02.896
и мир будет похож
на фестиваль «Burning Man».

00:09:02.920 --> 00:09:04.560
(Смех)

00:09:06.320 --> 00:09:08.320
Возможно, звучит неплохо,

00:09:09.280 --> 00:09:11.656
но спроси́те себя, что бы случилось

00:09:11.680 --> 00:09:14.416
при нынешнем
экономико-политическом устройстве?

00:09:14.440 --> 00:09:16.856
Кажется вероятным,
что мы станем свидетелями

00:09:16.880 --> 00:09:21.016
такого уровня имущественного неравенства
и безработицы,

00:09:21.040 --> 00:09:22.536
какого не наблюдали раньше.

00:09:22.560 --> 00:09:25.176
В отсутствие желания
сразу поставить новые блага

00:09:25.200 --> 00:09:26.680
на службу человечеству

00:09:27.640 --> 00:09:31.256
несколько магнатов смогут
украшать обложки деловых журналов,

00:09:31.280 --> 00:09:33.720
пока остальные будут голодать.

00:09:34.320 --> 00:09:36.436
Что бы сделали русские или китайцы,

00:09:36.460 --> 00:09:39.256
узнав, что некая компания
в Силиконовой долине

00:09:39.280 --> 00:09:42.016
на грани внедрения сверхразумного ИИ?

00:09:42.040 --> 00:09:44.896
Такое устройство могло бы
спровоцировать войну,

00:09:44.920 --> 00:09:47.136
наземную или кибер-войну

00:09:47.160 --> 00:09:48.840
беспрецедентного размаха.

00:09:49.870 --> 00:09:52.126
Это сценарий, в котором
победитель получает всё.

00:09:52.140 --> 00:09:55.136
Полугодовое преимущество
в такой конкуренции —

00:09:55.160 --> 00:09:57.936
всё равно что преимущество в 500 000 лет

00:09:57.960 --> 00:09:59.456
как минимум.

00:09:59.480 --> 00:10:04.216
Поэтому представляется,
что даже слухи о такого рода прорыве

00:10:04.240 --> 00:10:06.616
могут привести человечество в неистовство.

00:10:06.640 --> 00:10:09.536
Одна из самых пугающих вещей,

00:10:09.560 --> 00:10:12.336
как мне кажется, на данный момент —

00:10:12.360 --> 00:10:16.516
это те слова, которые твердят
все исследователи ИИ,

00:10:16.540 --> 00:10:18.240
когда хотят звучать обнадёживающе.

00:10:19.000 --> 00:10:22.476
И чаще всего нас просят
не беспокоиться по причине времени.

00:10:22.500 --> 00:10:24.536
До этого ещё далеко, если вы не знали.

00:10:24.560 --> 00:10:27.000
До этого, вероятно, ещё лет 50 или 100.

00:10:27.720 --> 00:10:28.976
Один исследователь сказал:

00:10:29.000 --> 00:10:30.576
«Волноваться о безопасности ИИ —

00:10:30.600 --> 00:10:32.900
всё равно что волноваться
о перенаселении Марса».

00:10:34.116 --> 00:10:35.736
В Силиконовой долине это означает:

00:10:35.760 --> 00:10:38.136
«не морочь этим свою милую головку».

00:10:38.160 --> 00:10:39.496
(Смех)

00:10:39.520 --> 00:10:41.416
Кажется, что никто не замечает,

00:10:41.440 --> 00:10:44.056
что ссылаться на отдалённость во времени —

00:10:44.080 --> 00:10:46.656
нарушение всякой логики.

00:10:46.680 --> 00:10:49.936
Если интеллект — лишь результат
обработки информации

00:10:49.960 --> 00:10:52.616
и мы продолжим совершенствовать технику,

00:10:52.640 --> 00:10:55.520
мы создадим некую форму сверхинтеллекта.

00:10:56.320 --> 00:10:59.976
И мы не имеем понятия,
сколько времени займёт

00:11:00.000 --> 00:11:02.400
создание для этого безопасных условий.

00:11:04.200 --> 00:11:05.496
Позвольте повторить.

00:11:05.520 --> 00:11:09.336
Мы не имеем понятия,
сколько времени займёт

00:11:09.360 --> 00:11:11.600
создание для этого безопасных условий.

00:11:12.920 --> 00:11:16.376
И если вы не заметили,
50 лет — не те 50 лет, что раньше.

00:11:16.400 --> 00:11:18.856
Вот 50 лет по месяцам.

00:11:18.880 --> 00:11:20.720
Вот так давно у нас есть iPhone.

00:11:21.440 --> 00:11:24.040
Так долго «Симпсонов» показывают по ТВ.

00:11:24.680 --> 00:11:27.056
Пятьдесят лет — не так много

00:11:27.080 --> 00:11:30.240
для решения одной из важнейших
для нашего вида задач.

00:11:31.640 --> 00:11:35.656
Опять же, нам не удаётся выстроить
подобающую эмоциональную реакцию

00:11:35.680 --> 00:11:38.376
на то, что по всем признакам
нам предстоит.

00:11:38.400 --> 00:11:42.376
Специалист в области информатики
Стюарт Рассел провёл хорошую аналогию.

00:11:42.400 --> 00:11:47.296
Он предложил представить, что мы получили
послание от внеземной цивилизации,

00:11:47.320 --> 00:11:49.016
в котором говорится:

00:11:49.040 --> 00:11:50.576
«Земляне,

00:11:50.600 --> 00:11:52.960
мы прибудем на вашу планету через 50 лет.

00:11:53.800 --> 00:11:55.376
Готовьтесь».

00:11:55.400 --> 00:11:59.656
И теперь мы просто считаем месяцы
до посадки их корабля-носителя?

00:11:59.680 --> 00:12:02.680
Мы бы немножко больше суетились.

00:12:04.680 --> 00:12:06.656
Ещё мы якобы не должны
волноваться потому,

00:12:06.680 --> 00:12:09.576
что эти машины
не могут не разделять наши ценности,

00:12:09.600 --> 00:12:12.216
так как будут буквально
дополнениями к нам самим.

00:12:12.240 --> 00:12:14.056
Они будут присоединены к нашему мозгу,

00:12:14.080 --> 00:12:16.440
и мы по сути станем
их лимбической системой.

00:12:17.120 --> 00:12:18.536
Теперь на миг задумайтесь,

00:12:18.560 --> 00:12:21.736
что самый безопасный и единственно
благоразумный путь,

00:12:21.760 --> 00:12:23.096
по рекомендациям, —

00:12:23.120 --> 00:12:25.920
имплантация этой технологии прямо в мозг.

00:12:26.600 --> 00:12:29.976
Это действительно может быть самым
безопасным и благоразумным путём,

00:12:30.000 --> 00:12:33.056
но обычно озабоченность
безопасностью технологии

00:12:33.080 --> 00:12:36.736
нужно развеивать до того,
как запихивать её себе в голову.

00:12:36.760 --> 00:12:38.776
(Смех)

00:12:38.800 --> 00:12:44.136
Более глубокая проблема в том, что
создание сверхразумного ИИ как такового

00:12:44.160 --> 00:12:45.896
кажется намного легче,

00:12:45.920 --> 00:12:47.776
чем создание сверхразумного ИИ

00:12:47.800 --> 00:12:49.576
и всех нейротехнологий,

00:12:49.600 --> 00:12:52.320
необходимых для их бесшовной
интеграции с нашим разумом.

00:12:52.800 --> 00:12:55.976
Учитывая, что компании и правительства,
ведущие эту работу,

00:12:56.000 --> 00:12:59.656
могут представлять себя участниками
гонки против всех остальных

00:12:59.680 --> 00:13:02.936
и для них выиграть гонку
значит выиграть весь мир,

00:13:02.960 --> 00:13:05.336
учитывая, что вы не уничтожите его
через секунду,

00:13:05.360 --> 00:13:08.046
тогда вероятным кажется,
что самое простое

00:13:08.070 --> 00:13:09.550
будет сделано в первую очередь.

00:13:10.560 --> 00:13:13.326
К сожалению, у меня нет решения
этой проблемы,

00:13:13.340 --> 00:13:16.056
кроме рекомендации, чтобы больше людей
задумывались о ней.

00:13:16.080 --> 00:13:18.456
Я думаю, нам нужно что-то вроде
проекта «Манхэттен»

00:13:18.480 --> 00:13:20.496
в области искусственного интеллекта.

00:13:20.520 --> 00:13:23.256
Не чтобы его создать —
я думаю, это неизбежно случится, —

00:13:23.280 --> 00:13:26.616
а чтобы понять, как избежать
гонки вооружений

00:13:26.640 --> 00:13:30.136
и создать его так,
чтобы это совпало с нашими интересами.

00:13:30.160 --> 00:13:32.296
Когда говоришь о сверхразумном ИИ,

00:13:32.320 --> 00:13:34.576
который может менять себя сам,

00:13:34.600 --> 00:13:39.216
кажется, что есть только одна попытка
для создания правильных начальных условий,

00:13:39.240 --> 00:13:41.296
и даже тогда нам нужно совладать

00:13:41.320 --> 00:13:44.410
с экономико-политическими 
последствиями их правильного создания.

00:13:45.760 --> 00:13:47.816
Но как только мы признаем,

00:13:47.840 --> 00:13:51.840
что источник интеллекта —
обработка информации,

00:13:52.720 --> 00:13:57.520
что интеллект основан на некой
подходящей вычислительной системе,

00:13:58.360 --> 00:14:02.120
и признаем, что будем совершенствовать
эти системы непрерывно

00:14:03.280 --> 00:14:07.736
и что предел сознания, весьма вероятно,
намного дальше,

00:14:07.760 --> 00:14:09.140
чем мы сейчас представляем,

00:14:10.120 --> 00:14:11.336
тогда мы должны признать,

00:14:11.360 --> 00:14:14.000
что мы в процессе создания
своего рода бога.

00:14:15.400 --> 00:14:16.976
Теперь самое время

00:14:17.000 --> 00:14:19.073
убедиться, что мы сможем с ним ужиться.

00:14:20.120 --> 00:14:21.656
Большое спасибо.

00:14:21.680 --> 00:14:26.773
(Аплодисменты)

