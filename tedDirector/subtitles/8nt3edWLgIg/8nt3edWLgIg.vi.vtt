WEBVTT
Kind: captions
Language: vi

00:00:00.000 --> 00:00:07.000
Translator: Huong Nguyen
Reviewer: Uyên Võ

00:00:12.820 --> 00:00:15.036
Tôi chuẩn bị nói về
sự thất bại của trực giác

00:00:15.060 --> 00:00:17.280
mà nhiều người trong chúng ta
thường gặp phải.

00:00:17.300 --> 00:00:21.180
Nó thật ra là sự thất bại trong việc
phát hiện một mối nguy hiểm nhất định.

00:00:21.180 --> 00:00:22.916
Tôi sẽ mô tả một viễn cảnh

00:00:22.940 --> 00:00:26.196
mà tôi nghĩ vừa kinh khủng

00:00:26.220 --> 00:00:28.620
vừa có thể sẽ xảy ra,

00:00:28.660 --> 00:00:30.316
và đây không phải sự kết hợp tốt

00:00:30.340 --> 00:00:31.876
khi nó xảy ra.

00:00:31.900 --> 00:00:34.356
Thế nhưng thay vì sợ,
hầu hết các bạn sẽ cảm thấy

00:00:34.380 --> 00:00:36.970
rằng điều tôi đang nói thật ra rất tuyệt

00:00:37.020 --> 00:00:39.996
Tôi sẽ mô tả cách mà
những thứ ta gặt hái được

00:00:40.020 --> 00:00:41.796
từ trí thông minh nhân tạo (AI)

00:00:41.820 --> 00:00:43.580
có thể hủy diệt chúng ta.

00:00:43.580 --> 00:00:47.246
Thực ra, tôi nghĩ rất khó để xem làm sao
chúng có thể không tiêu diệt chúng ta

00:00:47.246 --> 00:00:49.240
hoặc làm cho chúng ta tự hủy diệt mình.

00:00:49.240 --> 00:00:51.076
Nhưng nếu các bạn giống tôi,

00:00:51.100 --> 00:00:53.756
bạn sẽ thấy rằng nó rất vui
khi nghĩ về những điều này.

00:00:53.780 --> 00:00:57.156
Và phản ứng đó là một phần của vấn đề.

00:00:57.180 --> 00:00:59.540
OK? Phản ứng đó sẽ làm bạn lo lắng.

00:00:59.540 --> 00:01:02.396
Và nếu tôi thuyết phục được các bạn
trong buổi nói chuyện này

00:01:02.420 --> 00:01:05.836
rằng chúng ta có thể sẽ phải
hứng chịu một nạn đói toàn cầu,

00:01:05.860 --> 00:01:08.916
hoặc do biến đổi khí hậu hoặc
do một số thiên tai khác,

00:01:08.940 --> 00:01:12.356
và rằng cháu chắt của các bạn hay của họ,

00:01:12.380 --> 00:01:14.920
rất có thể sẽ phải sống như thế này,

00:01:15.020 --> 00:01:16.970
thì bạn sẽ không nghĩ,

00:01:17.260 --> 00:01:18.596
"Tuyệt vời.

00:01:18.620 --> 00:01:20.950
Tôi thích buổi nói chuyện này."

00:01:21.020 --> 00:01:23.350
Nạn đói không vui vẻ gì đâu.

00:01:23.620 --> 00:01:26.996
Tuy nhiên cái chết do khoa học
viễn tưởng lại rất hài hước.

00:01:27.020 --> 00:01:30.996
mà một trong số những thứ làm tôi
lo lắng nhất về việc phát triển AI

00:01:31.020 --> 00:01:35.116
là chúng ta dường như không thể sắp xếp
một phản ứng cảm xúc thích hợp nào

00:01:35.140 --> 00:01:36.956
cho những mối nguy hiểm trước mắt.

00:01:36.980 --> 00:01:41.930
Tôi không thể sắp xếp thứ phản ứng này,
nên tôi mới đưa ra buổi nói chuyện này.

00:01:41.940 --> 00:01:44.636
Nó như thể chúng ta
đứng trước hai cánh cửa.

00:01:44.660 --> 00:01:45.916
Sau cánh cửa thứ nhất,

00:01:45.940 --> 00:01:49.236
chúng ta ngưng quá trình xây dựng
các loại máy móc thông minh.

00:01:49.260 --> 00:01:53.276
Phần cứng và phần mềm máy tính
không cải tiến tốt hơn nữa vì vài lí do.

00:01:53.300 --> 00:01:56.770
Giờ hãy dành thời gian suy nghĩ
tại sao điều này có thể xảy ra.

00:01:56.900 --> 00:02:00.556
Ý tôi là, dựa vào giá trị của
trí thông minh và sự tự động hóa,

00:02:00.580 --> 00:02:04.100
chúng ta vẫn sẽ tiếp tục cải tiến
công nghệ nếu ta hoàn toàn có thể.

00:02:04.340 --> 00:02:07.617
Điều gì có thể khiến chúng ta
dừng làm việc này?

00:02:07.620 --> 00:02:10.620
Một cuộc chiến hạt nhân quy mô lớn?

00:02:10.820 --> 00:02:13.960
Một đại dịch toàn cầu?

00:02:14.140 --> 00:02:17.340
Va chạm thiên thạch?

00:02:17.460 --> 00:02:20.036
Hay Justin Bieber trở thành tổng thống Mỹ?

00:02:20.060 --> 00:02:22.340
(Cười)

00:02:24.580 --> 00:02:28.780
Vấn đề là, thứ gì đó sẽ phải hủy diệt
nền văn minh như chúng ta đã biết.

00:02:29.170 --> 00:02:33.476
Bạn phải tưởng tượng nó
sẽ phải tồi tệ như thế nào

00:02:33.500 --> 00:02:36.836
để ngăn chúng ta tiếp tục
cải tiến công nghệ

00:02:36.860 --> 00:02:38.076
vĩnh viễn,

00:02:38.100 --> 00:02:40.110
thế hệ này qua thế hệ khác.

00:02:40.120 --> 00:02:42.306
Gần như theo định nghĩa,
đây là thứ tồi tệ nhất

00:02:42.306 --> 00:02:44.316
xảy ra trong lịch sử nhân loại.

00:02:44.340 --> 00:02:45.636
Vậy nên lựa chọn duy nhất,

00:02:45.660 --> 00:02:47.996
và nó nằm sau cánh cửa số hai,

00:02:48.020 --> 00:02:51.156
đó là chúng ta tiếp tục cải tiến
các máy móc thông minh

00:02:51.180 --> 00:02:52.960
năm này qua năm khác.

00:02:53.130 --> 00:02:57.730
Tới một thời điểm nhất định, chúng ta sẽ
tạo ra những cỗ máy thông minh hơn mình,

00:02:57.730 --> 00:03:00.516
và một khi chúng ta sở hữu
các cỗ máy thông minh hơn mình,

00:03:00.540 --> 00:03:02.500
chúng sẽ bắt đầu tự cải tiến.

00:03:02.500 --> 00:03:05.496
Sau đó, chúng ta mạo hiểm với thứ
mà nhà toán học Ij Good gọi là

00:03:05.496 --> 00:03:07.076
sự "bùng nổ trí tuệ"

00:03:07.100 --> 00:03:09.930
và quá trình đó có thể sẽ
tiến xa ra khỏi chúng ta.

00:03:09.940 --> 00:03:12.756
Và giờ, điều này thường hay bị biếm họa,
như tôi có ở đây,

00:03:12.780 --> 00:03:15.996
nỗi sợ trước một đội quân robot độc ác

00:03:16.020 --> 00:03:17.276
sẽ tấn công chúng ta.

00:03:17.300 --> 00:03:19.996
Nhưng đó chưa phải là viễn cảnh
dễ xảy ra nhất.

00:03:20.020 --> 00:03:24.876
Không phải những cỗ máy của chúng ta 
tự nhiên trở nên ác độc

00:03:24.900 --> 00:03:27.516
Mối lo thật sự là khi 
chúng ta tạo ra những cỗ máy

00:03:27.540 --> 00:03:29.596
tài giỏi hơn chúng ta quá nhiều

00:03:29.620 --> 00:03:33.396
để rồi, dù chỉ một khác biệt nhỏ nhất
giữa mục tiêu của chúng và ta thôi

00:03:33.420 --> 00:03:35.770
cũng có thể hủy diệt chúng ta.

00:03:35.780 --> 00:03:38.400
Hãy nghĩ đến mối quan hệ
giữa chúng ta loài kiến.

00:03:38.420 --> 00:03:40.076
Chúng ta không ghét chúng.

00:03:40.100 --> 00:03:42.140
Chúng ta không cố tình làm hại chúng.

00:03:42.140 --> 00:03:44.616
Thực tế đôi khi chúng ta cố gắng
không làm hại chúng.

00:03:44.616 --> 00:03:46.596
Chúng ta bước qua chúng trên vỉa hè.

00:03:46.620 --> 00:03:48.750
Nhưng mỗi khi sự xuất hiện của chúng

00:03:48.750 --> 00:03:51.366
xung đột cực kì với một
trong các mục đích của chúng ta,

00:03:51.366 --> 00:03:53.847
Ví dụ như khi xây dựng
một tòa nhà như thế này,

00:03:53.847 --> 00:03:56.301
chúng ta tiêu diệt chúng không lo lắng.

00:03:56.301 --> 00:03:59.236
Có nghĩa là một ngày nào đó
khi chúng ta tạo ra các cỗ máy

00:03:59.260 --> 00:04:01.996
cho dù chúng có ý thức hay không,

00:04:02.020 --> 00:04:05.540
cũng sẽ đối xử với chúng ta 
theo sự bất chấp tương tự như thế.

00:04:05.580 --> 00:04:09.170
Bây giờ, tôi nghi ngờ rằng điều này có vẻ
quá xa với nhiều người

00:04:09.180 --> 00:04:15.516
Tôi cá là có một số người đang nghi ngờ
rằng AI siêu thông minh có thể xảy ra,

00:04:15.540 --> 00:04:17.196
ít nhiều không thể tránh khỏi.

00:04:17.220 --> 00:04:20.840
Nhưng sau đó bạn phải tìm ra điều sai
với một trong các giả định sau.

00:04:20.864 --> 00:04:23.386
Và chỉ có 3 giả định.

00:04:23.620 --> 00:04:28.970
Trí thông minh là một vấn đề của quá trình
xử lí thông tin trong các hệ thống vật lí.

00:04:28.980 --> 00:04:31.775
Thực ra, điều này hơn một chút
so với giả định.

00:04:31.779 --> 00:04:35.236
Chúng ta đã tạo ra trí thông minh hạn hẹp
trong các cỗ máy của mình rồi,

00:04:35.260 --> 00:04:37.276
Và rất nhiều máy móc cũng đã hoạt động

00:04:37.300 --> 00:04:40.640
ở mức độ của trí thông minh siêu việt rồi.

00:04:40.660 --> 00:04:43.200
Và chúng ta biết rằng chỉ có vật chất

00:04:43.200 --> 00:04:45.966
mới có thể tạo cơ hội cho
"trí thông minh không giới hạn,"

00:04:45.966 --> 00:04:49.556
một khả năng suy nghĩ linh hoạt
trên nhiều lĩnh vực,

00:04:49.580 --> 00:04:52.716
bởi vì bộ não chúng ta quản lí nó.
Đúng chứ?

00:04:52.740 --> 00:04:56.676
Ý tôi là, chỉ có các hạt nhân trong đây,

00:04:56.700 --> 00:05:01.196
và miễn là chúng ta tiếp tục xây dựng
hệ thống các nguyên tử

00:05:01.220 --> 00:05:03.916
để biểu lộ những hành động
thông minh nhiều hơn nữa,

00:05:03.940 --> 00:05:06.476
cuối cùng chúng ta sẽ,
trừ khi chúng ta bị gián đoạn,

00:05:06.500 --> 00:05:09.876
cuối cùng chúng ta sẽ xây dựng được
trí tuệ không giới hạn

00:05:09.900 --> 00:05:11.196
cho máy móc của mình.

00:05:11.220 --> 00:05:14.670
Cần phải nhận ra rằng
tiến độ không quan trọng.

00:05:14.670 --> 00:05:18.096
bởi vì bất kì quá trình nào cũng đủ
để đưa chúng ta vào khu vực kết thúc.

00:05:18.096 --> 00:05:21.996
Ta không cần định luật Moore để tiếp tục.
Ta không cần phát triển theo cấp số nhân.

00:05:21.996 --> 00:05:25.280
Chúng ta chỉ cần tiếp tục bước đi.

00:05:25.300 --> 00:05:28.780
Giả định thứ hai là
chúng ta sẽ tiếp tục đi.

00:05:28.820 --> 00:05:32.760
Chúng ta tiếp tục cải tiến
những cỗ máy thông minh của mình.

00:05:32.820 --> 00:05:36.910
Và dựa vào giá trị của trí thông minh --

00:05:36.910 --> 00:05:40.666
Ý tôi là, trí thông minh hoặc là nguồn gốc
của tất cả mọi thứ chúng ta đánh giá

00:05:40.666 --> 00:05:43.716
hoặc chúng ta cần nó để bảo vệ
tất cả mọi thứ chúng ta coi trọng.

00:05:43.716 --> 00:05:45.810
Nó là tài nguyên quý giá nhất
của chúng ta.

00:05:45.810 --> 00:05:47.426
Vì thế chúng ta muốn làm điều này.

00:05:47.426 --> 00:05:50.756
Chúng ta có những vần đề nhất định
phải liều mạng giải quyết.

00:05:50.780 --> 00:05:54.740
Chúng ta muốn chữa khỏi các căn bệnh
như Alzheimer hay ung thư.

00:05:54.780 --> 00:05:58.716
Chúng ta muốn nắm vững hệ thống kinh tế.
Chúng ta muốn cải thiện khoa học khí hậu.

00:05:58.740 --> 00:06:00.996
Cho nên chúng ta sẽ làm nó nếu có thể.

00:06:01.020 --> 00:06:05.356
Tàu đã rời khỏi trạm,
và không thể kéo phanh nữa.

00:06:05.700 --> 00:06:11.156
Và điều cuối: chúng ta không đứng trên
đỉnh cao của trí thông minh,

00:06:11.180 --> 00:06:13.070
hay có thể là chỗ nào đó gần nó.

00:06:13.070 --> 00:06:15.446
Và điều này thực sự là
cái nhìn sâu sắc quan trọng.

00:06:15.446 --> 00:06:17.796
Đây là thứ làm tình trạng của
chúng ta bất ổn,

00:06:17.820 --> 00:06:21.860
và nó cũng làm trực giác của chúng ta
về rủi ro trở nên không đáng tin.

00:06:22.940 --> 00:06:26.450
Bây giờ hãy xem xét một người
thông minh nhất trên đời.

00:06:26.460 --> 00:06:29.876
Hầu như trong danh sách của các bạn
đều sẽ có tên John von Neumann.

00:06:29.900 --> 00:06:33.236
Ý tôi là, ấn tượng mà von Neumann tạo ra
đối với mọi người xung quanh,

00:06:33.260 --> 00:06:37.316
và nó cũng bao gồm những nhà toán học
và vật lí vĩ đại ở thời của ông ta,

00:06:37.340 --> 00:06:39.276
được lưu truyền khá tốt.

00:06:39.300 --> 00:06:43.076
Nếu một nửa câu chuyện về ông
chỉ đúng 50%,

00:06:43.100 --> 00:06:44.316
không cần bàn cãi

00:06:44.340 --> 00:06:46.796
ông là người thông minh nhất
từng sống trên đời.

00:06:46.820 --> 00:06:50.090
Vậy hãy xét về chuỗi phân bố
của trí thông minh.

00:06:50.140 --> 00:06:51.569
Ở đây là John von Neumann.

00:06:53.380 --> 00:06:54.714
Tiếp theo là các bạn và tôi.

00:06:55.940 --> 00:06:57.236
Và tiếp là con gà.

00:06:57.260 --> 00:06:59.196
(Cười)

00:06:59.220 --> 00:07:00.436
Xin lỗi, một con gà.

00:07:00.460 --> 00:07:01.716
(Cười)

00:07:01.740 --> 00:07:05.476
Không lí do gì tôi phải làm buổi
nói chuyện này tệ hại hơn cần thiết.

00:07:05.500 --> 00:07:07.100
(Cười)

00:07:08.159 --> 00:07:11.636
Tuy nhiên, có vẻ có khả năng rất lớn là
chuỗi phân bố trí thông minh

00:07:11.660 --> 00:07:15.610
mở rộng ra xa hơn nhiều so với
những gì chúng ta tưởng tượng,

00:07:15.650 --> 00:07:18.916
và nếu chúng ta tạo ra
những cỗ máy thông minh hơn mình,

00:07:18.940 --> 00:07:21.236
chúng có thể sẽ khai phá dải phân bố này

00:07:21.260 --> 00:07:23.116
theo cách ta không thể tưởng tượng,

00:07:23.140 --> 00:07:26.810
và vượt xa chúng ta theo cách
ta không thể tưởng tượng.

00:07:26.820 --> 00:07:31.156
Và quan trọng là nhận ra rằng điều này
đúng với tính hấp dẫn của sự mau lẹ.

00:07:31.180 --> 00:07:36.236
Đúng chứ? Hãy cứ tưởng tượng nếu ta
tạo ra một AI siêu thông minh

00:07:36.260 --> 00:07:39.716
mà không tài giỏi hơn
nhóm các nhà nghiên cứu trung bình

00:07:39.740 --> 00:07:42.036
ở Stanford hay MIT.

00:07:42.060 --> 00:07:45.036
Vâng, các mạch điện tử hoạt động
nhanh hơn khoảng một triệu lần

00:07:45.060 --> 00:07:46.316
so với các mạch sinh hóa,

00:07:46.340 --> 00:07:49.476
nên cỗ máy này có thể suy nghĩ
nhanh hơn khoảng một triệu lần

00:07:49.500 --> 00:07:51.316
so với trí não đã tạo ra nó.

00:07:51.340 --> 00:07:52.996
Rồi bạn cài đặt nó chạy một tuần,

00:07:53.020 --> 00:07:58.200
và nó sẽ thực hiện 20.000 năm làm việc
ở trình độ trí tuệ của con người,

00:07:58.220 --> 00:08:01.460
tuần này qua tuần khác.

00:08:01.460 --> 00:08:04.556
Thậm chí làm sao ta hiểu được,
dù ít nhiều hạn chế,

00:08:04.580 --> 00:08:08.640
một bộ óc thực hiện
kiểu quá trình như vậy?

00:08:08.660 --> 00:08:10.796
Một điều khác thật sự đáng lo lắng,

00:08:10.820 --> 00:08:15.796
đó là, tưởng tượng ra
một viễn cảnh tốt nhất.

00:08:15.820 --> 00:08:19.950
Nào hãy tưởng tượng chúng ta tìm ra
một thiết kế cho AI siêu thông minh

00:08:19.950 --> 00:08:21.426
mà không có các vấn đề an toàn.

00:08:21.426 --> 00:08:24.676
Lần đầu tiên chúng ta có được
một bản thiết kế hoàn hảo.

00:08:24.700 --> 00:08:26.916
Nó như thể chúng ta nhận được
một lời tiên tri

00:08:26.940 --> 00:08:28.956
xảy ra chính xác như trong dự định.

00:08:28.980 --> 00:08:32.700
Vâng, cỗ máy này sẽ là một thiết bị
tuyệt vời giúp tiết kiệm nhân lực.

00:08:32.730 --> 00:08:35.929
Nó có thể thiết kế một cỗ máy
có khả năng tạo ra cỗ máy khác

00:08:35.953 --> 00:08:37.716
thực hiện các công việc thể chất,

00:08:37.740 --> 00:08:39.196
chạy bằng ánh sáng mặt trời,

00:08:39.220 --> 00:08:41.916
nhiều hơn hoặc ít hơn cho
các chi phí nguyên liệu.

00:08:41.940 --> 00:08:45.196
Vậy chúng ta đang nói tới sự kết thúc
cho nỗi cực nhọc của con người.

00:08:45.220 --> 00:08:48.170
Chúng ta cũng nói về sự chấm dứt
cho các công việc trí óc.

00:08:48.170 --> 00:08:52.076
Vậy loài khỉ giống chúng ta sẽ làm gì
trong tình huống này?

00:08:52.100 --> 00:08:57.500
Vâng, chúng ta được tự do chơi Frisbee
và nhắn tin với từng người.

00:08:57.660 --> 00:09:00.516
Thêm vài LSD và vài lựa chọn cho
bộ quần áo đáng ngờ,

00:09:00.540 --> 00:09:02.716
và cả thế giới sẽ như ở Burning Man.

00:09:02.740 --> 00:09:04.380
(Cười)

00:09:06.140 --> 00:09:09.030
Bây giờ, điều đó có lẽ nghe rất tốt,

00:09:09.100 --> 00:09:11.476
nhưng thử hỏi bản thân xem
điều gì sẽ xảy ra

00:09:11.500 --> 00:09:14.236
dưới trật tự kinh tế và chính trị
của chúng ta bây giờ?

00:09:14.260 --> 00:09:16.676
Có vẻ là chúng ta sẽ chứng kiến

00:09:16.700 --> 00:09:20.730
một mức độ phân biệt giàu nghèo
và nạn thất nghiệp

00:09:20.730 --> 00:09:22.356
chưa từng thấy trước đây.

00:09:22.380 --> 00:09:25.326
Sự thiếu tự nguyện trong việc lập tức
đưa sự giàu có mới này

00:09:25.326 --> 00:09:27.460
vào phục vụ cho toàn nhân loại,

00:09:27.460 --> 00:09:31.076
một vài "tỷ tỷ phú" đươc vinh danh 
trên trang bìa của các tờ tạp chí kinh tế

00:09:31.100 --> 00:09:33.930
trong khi phần còn lại của thế giới
thì chịu cảnh chết đói

00:09:33.930 --> 00:09:36.290
Người Nga và Trung Quốc sẽ làm gì

00:09:36.290 --> 00:09:39.096
khi họ nghe được tin một số công ty
trong Thung lũng Silicon

00:09:39.106 --> 00:09:41.836
chuẩn bị triển khai
một AI siêu thông minh?

00:09:41.860 --> 00:09:44.716
Cỗ máy này có khả năng sẽ
tiến hành chiến tranh

00:09:44.740 --> 00:09:46.956
dù cho trên mặt đất hay trên mạng,

00:09:46.980 --> 00:09:49.540
với sức mạnh chưa từng có.

00:09:49.940 --> 00:09:51.796
Đây là viễn cảnh "thắng làm vua".

00:09:51.820 --> 00:09:54.956
Dẫn trước sáu tháng trong cuộc đua này

00:09:54.980 --> 00:09:57.756
tương đương với
bị dẫn trước 500.000 năm,

00:09:57.780 --> 00:09:59.276
ít nhất là vậy.

00:09:59.300 --> 00:10:04.036
Vì vậy, thậm chí nó có vẻ chỉ là
những tin đồn cho kiểu đột phá như thế

00:10:04.060 --> 00:10:06.436
nhưng cũng đủ khiến loài người cáu tiết.

00:10:06.460 --> 00:10:09.356
Và giờ, một trong những điều đáng sợ nhất,

00:10:09.380 --> 00:10:12.156
theo tôi nghĩ ngay tại thời điểm này,

00:10:12.180 --> 00:10:16.476
chính là những thứ mà
các nhà nghiên cứu về AI nói

00:10:16.500 --> 00:10:18.690
khi họ muốn được yên tâm.

00:10:18.690 --> 00:10:22.240
Và thời gian là lí do thường nhất cho việc
chúng ta không cần phải lo lắng.

00:10:22.240 --> 00:10:24.396
Các bạn biết không,
đây là một con đường dài.

00:10:24.396 --> 00:10:27.500
Có thể là 50 hay 100 năm nữa.

00:10:27.540 --> 00:10:28.796
Một nhà nghiên cứu đã nói,

00:10:28.820 --> 00:10:30.396
"lo lắng về độ an toàn của AI

00:10:30.420 --> 00:10:33.426
cũng như lo lắng về
bùng nổ dân số trên sao Hỏa."

00:10:33.426 --> 00:10:35.550
Đây là phiên bản của Thung lũng Silicon

00:10:35.550 --> 00:10:38.036
về việc "đừng làm cái đầu nhỏ
của bạn lo lắng về nó."

00:10:38.036 --> 00:10:39.316
(Cười)

00:10:39.340 --> 00:10:41.236
Có vẻ chả ai chú ý

00:10:41.260 --> 00:10:43.876
rằng xem xét trên phạm vi thời gian

00:10:43.900 --> 00:10:46.476
là hoàn toàn vô căn cứ.

00:10:46.500 --> 00:10:49.756
Nếu trí thông minh chỉ là một vấn đề
về xử lí thông tin,

00:10:49.780 --> 00:10:52.436
và chúng ta tiếp tục cải tiến các cỗ máy,

00:10:52.460 --> 00:10:55.340
chúng ta sẽ tạo ra được
vài dạng của siêu trí tuệ.

00:10:56.140 --> 00:10:59.796
Và chúng ta không biết
sẽ tốn bao nhiêu thời gian

00:10:59.820 --> 00:11:03.830
để tạo ra những điều kiện
thực hiện việc này một cách an toàn.

00:11:04.020 --> 00:11:05.316
Tôi xin nhắc lại.

00:11:05.340 --> 00:11:09.156
Chúng ta không biết
sẽ tốn bao nhiêu thời gian

00:11:09.180 --> 00:11:12.730
để tạo ra những điều kiện
thực hiện việc này một cách an toàn.

00:11:12.740 --> 00:11:16.196
Và nếu bạn chưa nhận ra,
50 năm không còn như xưa

00:11:16.220 --> 00:11:18.676
Đây là 50 năm tính theo nhiều tháng.

00:11:18.700 --> 00:11:21.160
Đây là thời gian chúng ta tạo ra iPhone.

00:11:21.260 --> 00:11:23.860
Đây là thời gian "Gia đình Simpson"
được chiếu trên TV.

00:11:24.500 --> 00:11:26.876
50 năm không quá nhiều

00:11:26.900 --> 00:11:31.390
để thực hiện một trong số những thử thách
lớn nhất mà loài người từng đối mặt.

00:11:31.460 --> 00:11:35.476
Một lần nữa, chúng ta có vẻ không thế
đưa ra một phản ứng cảm xúc phù hợp

00:11:35.500 --> 00:11:38.196
cho thứ mà chúng ta có đủ lí do
để tin rằng nó sẽ xảy ra.

00:11:38.220 --> 00:11:42.196
Nhà khoa học máy tính Stuart Russell
có một phép loại suy rất hay ở đây.

00:11:42.220 --> 00:11:47.116
Ông nói, tưởng tượng chúng ta nhận được
tin nhắn từ nền văn minh ngoài hành tinh,

00:11:47.140 --> 00:11:48.836
ghi rằng:

00:11:48.860 --> 00:11:50.396
"Loài người trên Trái Đất,

00:11:50.420 --> 00:11:53.610
Chúng ta sẽ đáp xuống
hành tinh các người trong 50 năm nữa.

00:11:53.620 --> 00:11:55.196
Chuẩn bị đi."

00:11:55.220 --> 00:11:59.476
Và giờ chúng ta chỉ đếm ngược từng tháng
đến khi phi thuyền mẹ đáp xuống?

00:11:59.500 --> 00:12:03.670
Chúng ta sẽ cảm thấy nhiều cấp bách hơn.

00:12:03.780 --> 00:12:06.010
Một lí do khác bảo
chúng ta không nên lo lắng

00:12:06.020 --> 00:12:09.346
là các cỗ máy này không làm gì khác
ngoài chia sẻ giá trị với chúng ta

00:12:09.346 --> 00:12:12.106
bởi vì chúng thật sự sẽ là
những phần mở rộng của chúng ta.

00:12:12.106 --> 00:12:13.876
Chúng sẽ được cấy vào não chúng ta,

00:12:13.900 --> 00:12:16.910
và chúng ta về cơ bản trở thành
hệ thống cảm xúc của chúng.

00:12:16.940 --> 00:12:18.356
Giờ hãy xem xét

00:12:18.380 --> 00:12:21.556
rằng con đường an toàn và
đầy thận trọng duy nhất phía trước,

00:12:21.580 --> 00:12:22.916
được đề xuất,

00:12:22.940 --> 00:12:25.740
chính là cấy trực tiếp công nghệ này
vào não chúng ta.

00:12:26.420 --> 00:12:29.796
Hiện giờ nó có lẽ thật sự là con đường
an toàn, thận trọng duy nhất,

00:12:29.820 --> 00:12:32.876
nhưng thường thì mối lo lắng an toàn
của một người về công nghệ

00:12:32.900 --> 00:12:36.556
phải được tính toán khá kĩ càng
trước khi bạn đính nó vào đầu mình.

00:12:36.580 --> 00:12:38.596
(Cười)

00:12:38.620 --> 00:12:43.956
Vấn đề sâu hơn là xây dựng
một AI siêu thông minh riêng lẻ

00:12:43.980 --> 00:12:45.716
có vẻ dễ dàng hơn

00:12:45.740 --> 00:12:47.580
là tạo ra một AI siêu thông minh

00:12:47.580 --> 00:12:49.726
và có được ngành
khoa học thần kinh hoàn chỉnh

00:12:49.726 --> 00:12:52.600
mà cho phép chúng ta chúng ta
tích hợp trí óc của mình và nó.

00:12:52.630 --> 00:12:55.796
Và giả sử các công ty và
các chính phủ làm việc này

00:12:55.820 --> 00:12:59.476
có thể sẽ nhận ra bản thân đang trong
cuộc đua chống lại loài người,

00:12:59.500 --> 00:13:02.756
giả sử rằng thắng cuộc đua này
là có cả thế giới,

00:13:02.780 --> 00:13:05.236
miễn là bạn không hủy diệt nó
vào thời điểm kế tiếp,

00:13:05.260 --> 00:13:07.876
thì nó có vẻ như là bất kể cái gì dễ hơn

00:13:07.900 --> 00:13:10.300
sẽ được hoàn thành trước.

00:13:10.380 --> 00:13:13.236
Bây giờ, đáng tiếc là tôi không có
giải pháp cho vấn đề này,

00:13:13.260 --> 00:13:15.800
ngoại trừ đề nghị chúng ta
cần nghĩ về nó nhiều hơn.

00:13:15.800 --> 00:13:18.306
Tôi nghĩ chúng ta cần thứ gì đó
như Manhattan Project

00:13:18.306 --> 00:13:20.250
trong chủ đề về trí thông minh nhân tạo.

00:13:20.250 --> 00:13:23.386
Không vì để tạo ra nó, bởi tôi nghĩ
chúng ta chắc chắn sẽ làm được,

00:13:23.386 --> 00:13:26.436
mà để hiểu cách ngăn ngừa
cuộc chạy đua vũ trang

00:13:26.460 --> 00:13:29.956
và xây dựng nó theo cách phù hợp
với lợi ích của chúng ta.

00:13:29.980 --> 00:13:32.116
Khi chúng ta nói về AI siêu thông minh

00:13:32.140 --> 00:13:34.396
có thể tự thay đổi,

00:13:34.420 --> 00:13:38.960
có vẻ như chúng ta chỉ có một cơ hội
để làm những điều kiện ban đầu đúng đắn,

00:13:38.960 --> 00:13:41.396
và thậm chí sau đó chúng ta
sẽ cần phải ta hứng chịu

00:13:41.396 --> 00:13:45.370
những hậu quả kinh tế chính trị
khi làm nó đúng đắn.

00:13:45.580 --> 00:13:47.636
Nhưng khoảnh khác ta thừa nhận

00:13:47.660 --> 00:13:52.480
rằng xử lí thông tin là
nguồn gốc của trí thông minh,

00:13:52.540 --> 00:13:57.990
rằng một số hệ thống tính toán phù hợp
là nền tảng cho trí thông minh

00:13:58.180 --> 00:14:03.040
và chúng ta thừa nhận rằng chúng ta
tiếp tục cải tiến những hệ thống này,

00:14:03.100 --> 00:14:07.556
và chúng ta thừa nhận rằng
chân trời của nhận thức có thể rất xa

00:14:07.580 --> 00:14:09.740
so với những gì chúng ta đang biết,

00:14:09.740 --> 00:14:11.156
thì chúng ta phải công nhận

00:14:11.180 --> 00:14:15.060
rằng chúng ta đang trong quá trình
tạo ra một loại thánh thần.

00:14:15.220 --> 00:14:16.796
Bây giờ sẽ là thời điểm tốt

00:14:16.820 --> 00:14:19.923
để chắc chắn rằng nó là thánh thần
mà chúng ta có thể sống chung

00:14:19.940 --> 00:14:21.476
Cảm ơn các bạn rất nhiều.

00:14:21.500 --> 00:14:26.593
(Vỗ tay)

