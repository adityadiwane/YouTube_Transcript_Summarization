WEBVTT
Kind: captions
Language: nl

00:00:00.000 --> 00:00:07.000
Vertaald door: Rik Delaet
Nagekeken door: Marleen Laschet

00:00:13.000 --> 00:00:15.216
Ik kom praten over een gebrek aan intuïtie

00:00:15.240 --> 00:00:16.840
waar velen van ons aan lijden.

00:00:17.180 --> 00:00:20.520
Het is eigenlijk een gebrek om 
een bepaald soort gevaar aan te voelen.

00:00:21.360 --> 00:00:23.096
Ik ga een scenario beschrijven

00:00:23.120 --> 00:00:26.376
dat volgens mij zowel angstaanjagend

00:00:26.400 --> 00:00:28.160
als waarschijnlijk is,

00:00:28.840 --> 00:00:30.496
en dat is geen goede combinatie,

00:00:30.520 --> 00:00:32.056
zoals zal blijken.

00:00:32.080 --> 00:00:33.670
Toch, in plaats van bang te zijn,

00:00:33.670 --> 00:00:36.800
vinden de meesten van jullie
dat eerder cool.

00:00:37.270 --> 00:00:40.380
Ik ga uitleggen waarom 
de vooruitgang die we boeken

00:00:40.380 --> 00:00:41.980
in artificiële intelligentie, AI,

00:00:41.980 --> 00:00:43.920
ons uiteindelijk zou kunnen vernietigen.

00:00:43.920 --> 00:00:47.140
Ik vind het erg moeilijk 
om te zien hoe AI ons niet zal vernietigen

00:00:47.140 --> 00:00:49.560
of ons inspireren 
om onszelf te vernietigen.

00:00:49.560 --> 00:00:51.250
Als je een beetje als ik bent,

00:00:51.250 --> 00:00:53.936
vind je het toch leuk
om over deze dingen na te denken.

00:00:53.960 --> 00:00:57.336
Dat antwoord is een deel van het probleem.

00:00:57.360 --> 00:00:59.750
Dat antwoord zou je moeten verontrusten.

00:00:59.920 --> 00:01:02.576
Als ik er jullie in deze talk 
van zou overtuigen

00:01:02.600 --> 00:01:06.016
dat we wellicht op een wereldwijde 
hongersnood afstevenen,

00:01:06.040 --> 00:01:09.096
door klimaatverandering 
of een andere ramp,

00:01:09.120 --> 00:01:12.536
en dat jullie kleinkinderen
of hun kleinkinderen,

00:01:12.560 --> 00:01:14.860
zeer waarschijnlijk 
zo'n leven zouden leiden,

00:01:15.200 --> 00:01:16.400
zou je niet denken,

00:01:17.440 --> 00:01:18.776
"Interessant.

00:01:18.800 --> 00:01:20.000
Leuke TED Talk."

00:01:21.200 --> 00:01:22.720
Hongersnood is niet leuk.

00:01:23.800 --> 00:01:26.800
Daarentegen is doodgaan 
door sciencefiction wel leuk.

00:01:27.060 --> 00:01:29.340
Eén van de dingen 
die me het meest zorgen baren

00:01:29.340 --> 00:01:31.030
over de huidige ontwikkeling van AI

00:01:31.030 --> 00:01:35.296
is dat we niet in staat lijken om 
de juiste emotionele reactie op te wekken

00:01:35.320 --> 00:01:37.136
op de gevaren die op ons afkomen.

00:01:37.160 --> 00:01:40.360
Ik voel deze reactie ook niet, 
en ik geef nog wel deze talk.

00:01:42.120 --> 00:01:44.816
Het is alsof we ​​voor twee deuren staan.

00:01:44.840 --> 00:01:45.960
Achter deur nummer één

00:01:45.960 --> 00:01:48.920
houden we op met het maken 
van steeds intelligentere machines.

00:01:49.190 --> 00:01:50.896
Onze computerhardware en -software

00:01:50.896 --> 00:01:53.666
stopt gewoon met beter worden,
om wat voor reden dan ook.

00:01:53.666 --> 00:01:56.480
Denk er even over na 
waarom dit zou kunnen gebeuren.

00:01:57.080 --> 00:02:00.736
Omdat intelligentie en automatisering 
zo waardevol zijn,

00:02:00.760 --> 00:02:04.820
zullen we doorgaan met onze technologie 
te verbeteren als we enigszins kunnen.

00:02:05.200 --> 00:02:06.867
Wat kan ons tegenhouden?

00:02:07.800 --> 00:02:09.600
Een mondiale nucleaire oorlog?

00:02:11.000 --> 00:02:12.560
Een wereldwijde pandemie?

00:02:14.320 --> 00:02:15.640
Een asteroïde-inslag?

00:02:17.640 --> 00:02:19.950
Justin Bieber president 
van de Verenigde Staten?

00:02:19.950 --> 00:02:22.520
(Gelach)

00:02:24.760 --> 00:02:28.680
Het gaat om iets dat onze huidige 
beschaving zou moeten vernietigen.

00:02:29.360 --> 00:02:33.656
Je moet je voorstellen 
hoe erg het zou moeten zijn

00:02:33.680 --> 00:02:37.016
om ons tegen te houden
onze technologie te verbeteren,

00:02:37.040 --> 00:02:38.256
blijvend,

00:02:38.280 --> 00:02:40.296
generatie na generatie.

00:02:40.320 --> 00:02:42.140
Bijna per definitie is dit het ergste

00:02:42.140 --> 00:02:44.666
dat ooit gebeurd is
in de menselijke geschiedenis.

00:02:44.666 --> 00:02:45.816
Het enige alternatief

00:02:45.840 --> 00:02:48.176
is wat er achter deur nummer twee ligt,

00:02:48.200 --> 00:02:51.336
namelijk dat we doorgaan 
met onze intelligente machines

00:02:51.360 --> 00:02:53.200
jaar na jaar na jaar te verbeteren.

00:02:53.720 --> 00:02:57.360
Op een bepaald moment zullen we 
machines bouwen die slimmer zijn dan wij,

00:02:58.080 --> 00:03:00.600
en zodra we machines 
hebben die slimmer zijn dan wij,

00:03:00.600 --> 00:03:02.696
gaan ze zichzelf verder verbeteren.

00:03:02.720 --> 00:03:05.456
Dan lopen we het risico 
op wat de wiskundige IJ Good

00:03:05.480 --> 00:03:07.256
een 'intelligentie-explosie' noemde:

00:03:07.280 --> 00:03:09.500
dat het proces niet meer 
te beheersen valt.

00:03:10.120 --> 00:03:12.936
Daarvan werd vaak, zoals hier, 
een karikatuur gemaakt:

00:03:12.960 --> 00:03:16.176
de angst dat legers 
van kwaadaardige robots

00:03:16.200 --> 00:03:17.456
ons zouden aanvallen.

00:03:17.480 --> 00:03:20.176
Maar dat is niet 
het meest waarschijnlijke scenario.

00:03:20.200 --> 00:03:25.056
Het is niet zo dat onze machines 
spontaan boosaardig zullen worden.

00:03:25.080 --> 00:03:27.696
Het probleem is eigenlijk 
dat we machines gaan bouwen

00:03:27.720 --> 00:03:29.776
die zo veel competenter zijn dan wij

00:03:29.800 --> 00:03:33.576
dat het geringste verschil 
tussen hun doelen en de onze

00:03:33.600 --> 00:03:34.800
ons einde kan betekenen.

00:03:35.960 --> 00:03:38.340
Denk maar aan hoe we 
ons verhouden tot mieren.

00:03:38.600 --> 00:03:40.256
We haten ze niet.

00:03:40.280 --> 00:03:42.336
We zijn er niet op uit 
om ze kwaad te doen.

00:03:42.360 --> 00:03:44.966
We nemen soms zelfs de moeite 
om ze niet te schaden.

00:03:44.966 --> 00:03:46.776
We stappen over hen op de stoep.

00:03:46.800 --> 00:03:48.936
Maar wanneer hun aanwezigheid

00:03:48.960 --> 00:03:51.456
serieus in strijd is 
met één van onze doeleinden,

00:03:51.480 --> 00:03:53.957
laten we zeggen 
bij de bouw van een gebouw als dit,

00:03:53.981 --> 00:03:56.381
vernietigen we ze 
zonder er bij stil te staan.

00:03:56.480 --> 00:03:59.416
Het probleem is dat we op een dag 
machines gaan bouwen

00:03:59.440 --> 00:04:02.176
die, bewust of niet,

00:04:02.200 --> 00:04:04.980
ons met gelijkaardige 
onverschilligheid gaan behandelen.

00:04:05.760 --> 00:04:08.960
Ik vermoed dat dit voor velen 
van jullie wat vergezocht lijkt.

00:04:09.360 --> 00:04:12.720
Ik wed dat er zijn die er aan twijfelen

00:04:12.720 --> 00:04:15.720
of superintelligente AI 
überhaupt mogelijk is,

00:04:15.720 --> 00:04:17.376
laat staan onvermijdelijk.

00:04:17.400 --> 00:04:21.020
Maar dan moet je de fout vinden 
in één van de volgende veronderstellingen.

00:04:21.044 --> 00:04:22.616
Er zijn er slechts drie.

00:04:23.800 --> 00:04:28.519
Intelligentie is een kwestie van 
informatieverwerking in fysische systemen.

00:04:29.320 --> 00:04:31.935
Eigenlijk is dit iets meer 
dan een veronderstelling.

00:04:31.959 --> 00:04:35.416
We hebben al beperkte intelligentie 
ingebouwd in onze machines

00:04:35.440 --> 00:04:37.456
en veel van die machines werken al

00:04:37.480 --> 00:04:40.120
op een bovenmenselijk 
niveau van intelligentie.

00:04:40.840 --> 00:04:42.820
We weten dat loutere materie

00:04:42.820 --> 00:04:46.056
aanleiding kan geven tot wat 
'algemene intelligentie' wordt genoemd,

00:04:46.080 --> 00:04:49.736
het vermogen om flexibel te denken 
op meerdere domeinen,

00:04:49.760 --> 00:04:52.896
omdat onze hersenen dat kunnen.

00:04:52.920 --> 00:04:56.856
Ik bedoel, er zijn hier alleen maar atomen

00:04:56.880 --> 00:05:01.376
en zolang we systemen 
van atomen blijven bouwen

00:05:01.400 --> 00:05:04.096
die steeds intelligenter
gedrag gaan vertonen,

00:05:04.120 --> 00:05:06.656
zullen we uiteindelijk, 
tenzij iets ons tegenhoudt,

00:05:06.680 --> 00:05:10.056
algemene intelligentie creëren

00:05:10.080 --> 00:05:11.376
in onze machines.

00:05:11.400 --> 00:05:15.056
Het maakt niet uit 
hoe snel we vooruitgaan,

00:05:15.080 --> 00:05:18.256
want elke vooruitgang 
brengt ons dichter bij het eindpunt.

00:05:18.280 --> 00:05:22.056
Vergeet de wet van Moore,
exponentiële vooruitgang is niet nodig.

00:05:22.080 --> 00:05:24.100
We hoeven alleen maar door te gaan.

00:05:25.480 --> 00:05:28.400
De tweede veronderstelling
is dat we zullen blijven doorgaan.

00:05:29.000 --> 00:05:31.760
We zullen onze intelligente 
machines blijven verbeteren.

00:05:33.000 --> 00:05:37.376
En gezien de waarde van intelligentie --

00:05:37.400 --> 00:05:40.936
ik bedoel, intelligentie is ofwel de bron 
van alles wat we waarderen

00:05:40.960 --> 00:05:43.736
of ze moet alles beschermen
wat we waarderen.

00:05:43.760 --> 00:05:46.016
Het is ons waardevolste hulpmiddel.

00:05:46.040 --> 00:05:47.576
Dus we willen dit doen.

00:05:47.600 --> 00:05:50.936
We hebben problemen 
die we nodig moeten oplossen.

00:05:50.960 --> 00:05:54.160
We willen ziektes 
als Alzheimer en kanker genezen.

00:05:55.060 --> 00:05:58.896
We willen economische systemen begrijpen 
en onze klimaatwetenschap verbeteren.

00:05:58.920 --> 00:06:01.176
Dus zullen we dit doen, als we kunnen.

00:06:01.200 --> 00:06:04.486
De trein is al vertrokken 
en er is geen houden aan.

00:06:05.880 --> 00:06:11.336
Tot slot staan we waarschijnlijk niet 
op een piek van intelligentie,

00:06:11.360 --> 00:06:13.160
of ook maar ergens in de buurt ervan.

00:06:13.640 --> 00:06:15.536
En dit is echt het cruciale inzicht.

00:06:15.560 --> 00:06:17.976
Dit maakt onze situatie zo precair

00:06:18.000 --> 00:06:22.040
en dit maakt onze intuïties 
over de risico's zo onbetrouwbaar.

00:06:23.120 --> 00:06:25.840
Beschouw de slimste mens 
die ooit heeft geleefd.

00:06:26.640 --> 00:06:30.056
Op bijna ieders lijstje 
staat hier John von Neumann.

00:06:30.080 --> 00:06:33.416
Ik bedoel, de indruk die von Neumann 
maakte op de mensen om hem heen,

00:06:33.440 --> 00:06:37.496
inclusief de grootste wiskundigen 
en fysici van zijn tijd,

00:06:37.520 --> 00:06:39.456
is redelijk goed gedocumenteerd.

00:06:39.480 --> 00:06:43.256
Als slechts de helft van de verhalen 
over hem half waar zijn,

00:06:43.280 --> 00:06:44.496
dan kan het niet anders

00:06:44.520 --> 00:06:46.976
dan dat hij één 
van de slimste mensen ooit was.

00:06:47.000 --> 00:06:49.520
Bekijk dit spectrum 
van intelligentie eens.

00:06:50.320 --> 00:06:52.289
Hier hebben we John von Neumann.

00:06:53.560 --> 00:06:54.894
En dan hebben we jou en mij.

00:06:56.120 --> 00:06:57.416
En dan hebben we een kip.

00:06:57.440 --> 00:06:59.376
(Gelach)

00:06:59.400 --> 00:07:00.616
Sorry, een kip.

00:07:00.640 --> 00:07:01.896
(Gelach)

00:07:01.920 --> 00:07:05.656
Ik moet dit gesprek niet 
deprimerender maken dan nodig.

00:07:05.680 --> 00:07:07.280
(Gelach)

00:07:08.339 --> 00:07:12.036
Het lijkt echter uiterst waarschijnlijk 
dat het spectrum van intelligentie

00:07:12.036 --> 00:07:14.960
veel verder reikt 
dan we op dit moment beseffen.

00:07:16.040 --> 00:07:19.060
Als we machines bouwen 
die intelligenter zijn dan wij,

00:07:19.060 --> 00:07:21.786
gaan ze hoogstwaarschijnlijk 
dit spectrum verkennen

00:07:21.786 --> 00:07:23.296
op een onvoorstelbare manier

00:07:23.320 --> 00:07:26.310
en ons onvoorstelbaar overtreffen.

00:07:27.000 --> 00:07:31.336
Het is belangrijk te beseffen dat 
dit al waar is voor de snelheid alleen.

00:07:31.360 --> 00:07:36.416
Stel dat we zojuist
een superintelligente AI hadden gemaakt

00:07:36.440 --> 00:07:39.896
die niet slimmer was 
dan een gemiddeld team van onderzoekers

00:07:39.920 --> 00:07:42.216
aan Stanford en MIT.

00:07:42.240 --> 00:07:45.436
Elektronische schakelingen werken 
ongeveer een miljoen keer sneller

00:07:45.436 --> 00:07:46.496
dan biochemische.

00:07:46.520 --> 00:07:49.766
Dus zou deze machine ongeveer 
een miljoen keer sneller moeten denken

00:07:49.766 --> 00:07:51.496
dan de geesten die ze ontwierpen.

00:07:51.520 --> 00:07:53.176
Laat ze één week lopen

00:07:53.200 --> 00:07:57.760
en ze zal voor 20 000 jaar intellectuele 
arbeid op menselijk niveau uitvoeren,

00:07:58.400 --> 00:08:00.360
week na week na week.

00:08:01.640 --> 00:08:04.736
Hoe kunnen we een verstand 
dat dit soort vooruitgang maakt

00:08:04.760 --> 00:08:07.040
nog begrijpen, laat staan ​​beheersen?

00:08:08.840 --> 00:08:11.386
Wat, eerlijk gezegd, 
ook nog verontrustend is:

00:08:12.036 --> 00:08:15.976
stel je even het beste scenario voor.

00:08:16.000 --> 00:08:20.176
Veronderstel dat we 
een superintelligente AI ontwerpen

00:08:20.200 --> 00:08:21.546
die absoluut veilig is.

00:08:21.546 --> 00:08:24.856
We vinden gelijk het perfecte ontwerp.

00:08:24.880 --> 00:08:27.096
Het is alsof we 
een orakel hebben gekregen

00:08:27.120 --> 00:08:29.136
dat ​​zich precies zoals bedoeld gedraagt.

00:08:29.160 --> 00:08:32.880
Deze machine zou het perfecte 
arbeidsbesparende apparaat zijn.

00:08:33.680 --> 00:08:36.329
Het kan de machine ontwerpen 
die de machine kan bouwen

00:08:36.329 --> 00:08:37.896
die elk fysiek werk aankan,

00:08:37.920 --> 00:08:39.376
aangedreven door zonlicht,

00:08:39.400 --> 00:08:42.096
min of meer voor de kosten 
van de grondstoffen.

00:08:42.120 --> 00:08:45.376
Gedaan met alle menselijk geploeter!

00:08:45.400 --> 00:08:48.200
Maar ook het einde 
van de meeste intellectuele arbeid.

00:08:49.200 --> 00:08:52.256
Wat gaan apen zoals wij 
in deze omstandigheden doen?

00:08:52.280 --> 00:08:56.360
Nou ja, we zouden vrij zijn om frisbee 
te spelen en elkaar massages te geven.

00:08:57.840 --> 00:09:00.696
Gooi er nog wat LSD 
en rare kleren tegenaan

00:09:00.720 --> 00:09:03.366
en de hele wereld wordt 
één groot Burning Man-festival.

00:09:03.366 --> 00:09:05.160
(Gelach)

00:09:06.320 --> 00:09:08.320
Nu klinkt dat misschien best goed,

00:09:09.280 --> 00:09:11.656
maar vraag jezelf eens af
wat er zou gebeuren

00:09:11.680 --> 00:09:14.416
onder onze huidige economische 
en politieke orde.

00:09:14.440 --> 00:09:17.096
Het lijkt waarschijnlijk 
dat we getuige zouden zijn

00:09:17.096 --> 00:09:21.016
van een niveau van ongelijke verdeling 
van rijkdom en werkloosheid

00:09:21.040 --> 00:09:22.686
die we nooit eerder hebben gezien.

00:09:22.686 --> 00:09:25.176
Zonder bereidheid om 
deze nieuwe rijkdom

00:09:25.200 --> 00:09:27.140
onmiddellijk met iedereen te delen,

00:09:27.640 --> 00:09:31.256
zullen enkele biljonairs de covers 
van onze zakenmagazines sieren

00:09:31.280 --> 00:09:34.230
terwijl de rest van de wereld 
vrij zou zijn om te verhongeren.

00:09:34.320 --> 00:09:36.616
Wat zouden de Russen of de Chinezen doen

00:09:36.640 --> 00:09:39.256
als ze hoorden 
dat een bedrijf in Silicon Valley

00:09:39.280 --> 00:09:42.016
op het punt stond 
een superintelligente AI te maken?

00:09:42.040 --> 00:09:44.896
Deze machine zou in staat zijn 
tot het voeren van oorlog,

00:09:44.920 --> 00:09:47.136
hetzij op aarde of in cyberspace,

00:09:47.160 --> 00:09:48.840
met ongekende kracht.

00:09:50.120 --> 00:09:51.976
Dit is een winnaar-pakt-alles-scenario.

00:09:52.000 --> 00:09:55.136
Zes maanden voorsprong op de concurrentie

00:09:55.160 --> 00:09:57.936
komt overeen met 500 000 jaar voorsprong,

00:09:57.960 --> 00:09:59.456
op z'n minst.

00:09:59.480 --> 00:10:04.216
Dus het lijkt erop dat zelfs louter 
geruchten van dit soort doorbraak

00:10:04.240 --> 00:10:06.876
ertoe kunnen leiden 
dat het mensdom wild zal worden.

00:10:06.876 --> 00:10:09.536
Eén van de meest angstaanjagende dingen,

00:10:09.560 --> 00:10:12.336
naar mijn mening,

00:10:12.360 --> 00:10:16.656
zijn het soort dingen 
die AI-onderzoekers zeggen

00:10:16.680 --> 00:10:18.630
wanneer ze ons gerust willen stellen.

00:10:19.000 --> 00:10:22.456
Waar we ons vooral 
geen zorgen over moeten maken, is tijd.

00:10:22.480 --> 00:10:24.536
Dit is nog allemaal ver weg, weet je.

00:10:24.560 --> 00:10:27.000
Waarschijnlijk nog 50 of 100 jaar weg.

00:10:27.530 --> 00:10:28.976
Eén onderzoeker heeft gezegd:

00:10:29.000 --> 00:10:30.576
"Piekeren over AI-veiligheid

00:10:30.600 --> 00:10:33.260
is net als je zorgen maken 
over overbevolking op Mars."

00:10:34.116 --> 00:10:35.736
Dit is de Silicon Valley-versie

00:10:35.760 --> 00:10:38.136
van 'breek er je mooie hoofdje niet over'.

00:10:38.160 --> 00:10:39.496
(Gelach)

00:10:39.520 --> 00:10:41.416
Niemand lijkt te merken

00:10:41.440 --> 00:10:43.970
dat verwijzen naar de tijdshorizon

00:10:43.970 --> 00:10:46.260
een totale 'non sequitur' is,
een onjuist gevolg.

00:10:46.260 --> 00:10:49.936
Als intelligentie alleen maar 
een kwestie van informatieverwerking is,

00:10:49.960 --> 00:10:52.616
en we onze machines 
maar blijven verbeteren,

00:10:52.640 --> 00:10:55.840
zullen we een bepaalde vorm 
van superintelligentie produceren.

00:10:56.320 --> 00:10:59.976
We hebben geen idee 
hoe lang het zal duren

00:11:00.000 --> 00:11:02.760
om de voorwaarden te creëren 
om dat veilig te doen.

00:11:04.200 --> 00:11:05.496
Ik zeg het nog eens.

00:11:05.520 --> 00:11:09.336
We hebben geen idee 
hoe lang het zal duren

00:11:09.360 --> 00:11:12.010
om de voorwaarden te creëren 
om dat veilig te doen.

00:11:12.920 --> 00:11:16.696
Mocht het je nog niet zijn opgevallen: 
50 jaar is niet meer wat ooit was.

00:11:16.696 --> 00:11:18.856
Dit is 50 jaar in maanden.

00:11:18.880 --> 00:11:20.720
Zo lang hebben we de iPhone al.

00:11:21.440 --> 00:11:24.040
Dit is hoe lang 'The Simpsons' 
al op televisie zijn.

00:11:24.680 --> 00:11:26.820
Vijftig jaar is niet zo heel veel tijd

00:11:26.820 --> 00:11:29.130
om één van de grootste 
uitdagingen aan te pakken

00:11:29.130 --> 00:11:31.640
waarmee het mensdom 
ooit zal worden geconfronteerd.

00:11:31.640 --> 00:11:35.656
We lijken er niet in te slagen om 
de juiste emotionele reactie te hebben

00:11:35.680 --> 00:11:38.926
op iets waarvan we alle reden hebben 
te geloven dat het komt.

00:11:38.926 --> 00:11:42.376
De computerwetenschapper Stuart Russell 
heeft hier een mooie analogie.

00:11:42.400 --> 00:11:47.296
Hij zei, stel je voor dat we een bericht 
van een buitenaardse beschaving ontvangen,

00:11:47.320 --> 00:11:49.016
dat luidt:

00:11:49.040 --> 00:11:50.576
"Mensen van de Aarde,

00:11:50.600 --> 00:11:52.960
binnen 50 jaar komen 
we naar jullie planeet.

00:11:53.800 --> 00:11:55.376
Hou je gereed."

00:11:55.400 --> 00:11:59.656
En dan gewoon de maanden aftellen 
tot aan de landing van het moederschip?

00:11:59.680 --> 00:12:02.680
We zouden een beetje 
meer urgentie voelen dan we nu doen.

00:12:04.580 --> 00:12:06.926
Waarom we ons ook
geen zorgen zouden hoeven maken,

00:12:06.926 --> 00:12:09.410
is dat deze machines sowieso 
onze waarden delen,

00:12:09.410 --> 00:12:12.216
omdat ze letterlijk uitbreidingen 
van onszelf zullen zijn.

00:12:12.240 --> 00:12:14.276
Ze zullen worden geënt op onze hersenen,

00:12:14.276 --> 00:12:16.700
en wij zullen in wezen 
hun limbisch systeem worden.

00:12:17.120 --> 00:12:18.536
Overweeg even

00:12:18.560 --> 00:12:21.736
dat de veiligste 
en enige verstandige weg vooruit,

00:12:21.760 --> 00:12:23.096
de aanbevolen weg,

00:12:23.120 --> 00:12:26.130
is om deze technologie 
direct in onze hersenen te implanteren.

00:12:26.600 --> 00:12:29.976
Dit kan in feite de veiligste 
en enige voorzichtige weg vooruit zijn,

00:12:30.000 --> 00:12:34.526
maar meestal moet iemand zich toch
volkomen veilig voelen bij een technologie

00:12:34.526 --> 00:12:36.736
voordat hij ze in zijn hoofd steekt.

00:12:36.760 --> 00:12:38.776
(Gelach)

00:12:38.800 --> 00:12:44.136
Het dieperliggende probleem is dat 
superintelligente AI op zich bouwen

00:12:44.160 --> 00:12:45.896
waarschijnlijk gemakkelijker lijkt

00:12:45.920 --> 00:12:47.776
dan superintelligente AI bouwen

00:12:47.800 --> 00:12:49.576
én de neurowetenschappen voltooien

00:12:49.600 --> 00:12:52.770
die ons toelaten onze geest 
er naadloos mee te integreren.

00:12:52.800 --> 00:12:55.976
Gezien het feit dat de bedrijven 
en overheden die dit werk doen

00:12:56.000 --> 00:12:59.656
waarschijnlijk denken 
met een race bezig te zijn,

00:12:59.680 --> 00:13:02.936
en omdat deze race winnen 
neerkomt op de wereld winnen,

00:13:02.960 --> 00:13:05.416
gesteld dat je hem niet direct vernietigt,

00:13:05.440 --> 00:13:08.296
dan lijkt het waarschijnlijk 
dat alles wat gemakkelijker is,

00:13:08.296 --> 00:13:09.740
voorrang zal krijgen.

00:13:10.560 --> 00:13:13.416
Ik heb helaas geen andere oplossing 
voor dit probleem

00:13:13.440 --> 00:13:16.056
dan de aanbeveling dat we er 
over moeten gaan nadenken.

00:13:16.080 --> 00:13:18.456
Ik denk aan iets als een Manhattan Project

00:13:18.480 --> 00:13:20.496
over kunstmatige intelligentie.

00:13:20.520 --> 00:13:23.256
Niet om het te bouwen, 
want dat gaan we sowieso doen,

00:13:23.280 --> 00:13:26.616
maar om te begrijpen 
hoe een wapenwedloop te voorkomen

00:13:26.640 --> 00:13:30.136
en het zo te bouwen 
dat het onze belangen dient.

00:13:30.160 --> 00:13:32.296
Als je over superintelligente AI praat

00:13:32.320 --> 00:13:34.576
die zichzelf kan aanpassen,

00:13:34.600 --> 00:13:37.410
lijkt het erop dat we 
maar één kans hebben

00:13:37.410 --> 00:13:39.240
om de beginvoorwaarden goed te krijgen

00:13:39.240 --> 00:13:41.296
en zelfs dan zullen we moeten leren omgaan

00:13:41.320 --> 00:13:45.130
met de economische en politieke 
gevolgen van die goede beginvoorwaarden.

00:13:45.760 --> 00:13:47.816
Maar zodra we toegeven

00:13:47.840 --> 00:13:51.840
dat informatieverwerking 
de bron van intelligentie is,

00:13:52.720 --> 00:13:57.520
dat een geschikt computersysteem 
aan de basis van intelligentie ligt,

00:13:58.360 --> 00:14:02.120
en we toegeven dat we deze systemen 
onafgebroken zullen blijven verbeteren,

00:14:03.280 --> 00:14:07.736
en dat de kennishorizon 
waarschijnlijk veel verder reikt

00:14:07.760 --> 00:14:09.180
dan we nu weten,

00:14:10.000 --> 00:14:11.336
dan moeten we toegeven

00:14:11.360 --> 00:14:14.000
dat we bezig zijn een soort god te maken.

00:14:15.400 --> 00:14:17.026
Daarom moeten we er nu voor zorgen

00:14:17.026 --> 00:14:19.133
dat het een god is 
waarmee we kunnen leven.

00:14:20.120 --> 00:14:21.656
Dankjewel.

00:14:21.680 --> 00:14:26.773
(Applaus)

