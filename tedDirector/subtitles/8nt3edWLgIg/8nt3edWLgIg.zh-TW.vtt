WEBVTT
Kind: captions
Language: zh-TW

00:00:00.000 --> 00:00:07.000
譯者: Hans Chiang
審譯者: Qiyun Xing

00:00:13.000 --> 00:00:17.166
我要談一種我們很多人遭受的、直覺上的失誤。

00:00:17.480 --> 00:00:20.520
那其實是一種使你無法察覺到特定種類危險的失誤。

00:00:21.360 --> 00:00:23.096
我會描述一個情境

00:00:23.120 --> 00:00:26.376
是我認為很可怕

00:00:26.400 --> 00:00:28.160
而且很有機會發生的，

00:00:28.840 --> 00:00:30.496
這不是很好的組合，

00:00:30.520 --> 00:00:32.056
一如預期。

00:00:32.080 --> 00:00:34.536
然而比起感到害怕，大部分的人會覺得

00:00:34.560 --> 00:00:36.640
我正在說的東西有點酷。

00:00:37.200 --> 00:00:41.966
我將會描述在人工智能領域我們的進展

00:00:42.000 --> 00:00:43.776
如何能最終消滅我們。

00:00:43.800 --> 00:00:46.590
事實上，我認為很難看出他們為何不會消滅我們

00:00:46.590 --> 00:00:48.960
或者驅使我們消滅自己。

00:00:49.400 --> 00:00:51.256
如果你是和我類似的人，

00:00:51.280 --> 00:00:53.936
你會發現思考這類事情很有趣。

00:00:53.960 --> 00:00:57.336
那種反應也是問題的一部分。

00:00:57.360 --> 00:00:59.690
對嗎？那種反應應該讓你感到擔心。

00:00:59.920 --> 00:01:02.576
如果我是打算在這個裡演講說服你，

00:01:02.600 --> 00:01:06.016
我們很可能會遭受全球性的飢荒

00:01:06.040 --> 00:01:09.096
無論是因為氣候變遷或某種大災難

00:01:09.120 --> 00:01:12.536
而你的孫子們或者孫子們的孫子們

00:01:12.560 --> 00:01:14.360
非常可能要這樣生活，

00:01:15.200 --> 00:01:16.400
你不會覺得：

00:01:17.440 --> 00:01:18.776
「有意思，

00:01:18.800 --> 00:01:20.000
我喜歡這個 TED 演講。」

00:01:21.200 --> 00:01:22.720
飢荒並不有趣。

00:01:23.800 --> 00:01:27.176
另一方面來說，科幻式的死亡，是有趣的。

00:01:27.200 --> 00:01:31.176
而現階段人工智能的發展讓最讓我擔心的是

00:01:31.200 --> 00:01:35.296
我們似乎無法組織出一個適當的情緒反應，

00:01:35.320 --> 00:01:37.136
針對眼前的威脅。

00:01:37.160 --> 00:01:40.360
我無法組織出這個回應，所以我在這裡講這個。

00:01:42.120 --> 00:01:44.816
就像我們站在兩扇門前面。

00:01:44.840 --> 00:01:46.096
一號門後面，

00:01:46.120 --> 00:01:49.416
我們停止發展「製造有智能的機器」。

00:01:49.440 --> 00:01:53.456
我們的電腦硬體和軟體就因故停止變得更好。

00:01:53.480 --> 00:01:56.480
現在花一點時間想想為什麼這會發生。

00:01:57.080 --> 00:02:00.736
我的意思是，人工智能和自動化如此有價值，

00:02:00.760 --> 00:02:04.280
我們會持續改善我們的科技，只要我們有能力做。

00:02:05.200 --> 00:02:06.867
有什麼東西能阻止我們這麼做呢？

00:02:07.800 --> 00:02:09.600
一場全面性的核子戰爭？

00:02:11.000 --> 00:02:12.560
一場全球性的流行病？

00:02:14.320 --> 00:02:15.640
一次小行星撞擊地球？

00:02:17.640 --> 00:02:20.216
小賈斯汀成為美國總統？

00:02:20.240 --> 00:02:22.520
（笑聲）

00:02:24.760 --> 00:02:28.680
重點是：必須有什麼我們知道的東西會毀滅我們的文明。

00:02:29.360 --> 00:02:33.656
你必須想像到底能有多糟

00:02:33.680 --> 00:02:37.016
才能阻止我們持續改善我們的科技，

00:02:37.040 --> 00:02:38.256
永久地，

00:02:38.280 --> 00:02:40.296
一代又一代人。

00:02:40.320 --> 00:02:41.650
幾乎從定義上，這就是

00:02:41.650 --> 00:02:44.390
人類歷史上發生過的最糟的事。

00:02:44.390 --> 00:02:45.816
所以唯一的替代選項，

00:02:45.840 --> 00:02:48.070
這是在二號門之後的東西，

00:02:48.070 --> 00:02:51.336
是我們繼續改善我們的智能機器

00:02:51.360 --> 00:02:53.210
年復一年，年復一年。

00:02:53.720 --> 00:02:57.360
到某個時間點我們會造出比我們還聰明的機器，

00:02:58.080 --> 00:03:00.090
而我們一旦造出比我們聰明的機器，

00:03:00.090 --> 00:03:02.696
它們就會開始改善自己。

00:03:02.720 --> 00:03:05.300
然後我們承擔數學家 ij Good 稱為

00:03:05.300 --> 00:03:07.256
「人工智能爆發」的風險，

00:03:07.280 --> 00:03:09.280
那個過程會脫離我們的掌握。

00:03:10.120 --> 00:03:12.936
這時常被漫畫化，如我的這張圖，

00:03:12.960 --> 00:03:16.176
一種恐懼：充滿惡意的機械人軍團

00:03:16.200 --> 00:03:17.456
會攻擊我們。

00:03:17.480 --> 00:03:20.176
但這不是最可能發生的情境。

00:03:20.200 --> 00:03:25.056
並不是說我們的機器會變得自然地帶有敵意。

00:03:25.080 --> 00:03:27.696
問題在於我們將會造出

00:03:27.720 --> 00:03:29.776
遠比我們更有競爭力的機器，

00:03:29.800 --> 00:03:33.576
只要我們和他們的目標些微的歧異

00:03:33.600 --> 00:03:34.800
就會讓我們被毀滅。

00:03:35.960 --> 00:03:38.040
就想想我們和螞蟻的關係。

00:03:38.600 --> 00:03:40.256
我們不討厭牠們。

00:03:40.280 --> 00:03:42.336
我們不會特別去傷害牠們。

00:03:42.360 --> 00:03:44.736
甚至有時我們為了不傷害牠們而承受痛苦。

00:03:44.760 --> 00:03:46.776
我們在人行道跨越他們。

00:03:46.800 --> 00:03:48.936
但當他們的存在

00:03:48.960 --> 00:03:51.456
和我們的目標嚴重衝突，

00:03:51.480 --> 00:03:53.901
譬如當我們要建造一棟和這裡一樣的建築物，

00:03:53.901 --> 00:03:55.941
我們會毫無不安地除滅牠們。

00:03:56.480 --> 00:03:59.416
問題在於有一天我們會造出機器，

00:03:59.440 --> 00:04:02.176
無論他們是有意識或的者沒有意識的，

00:04:02.200 --> 00:04:04.200
會對我們如螞蟻般的不予理會。

00:04:05.760 --> 00:04:08.520
現在，我懷疑這對說法這裡大部分的人來說不著邊際。

00:04:09.360 --> 00:04:15.696
我確信你們有些人懷疑超級人工智能出現的可能，

00:04:15.720 --> 00:04:17.376
更別說它必然出現。

00:04:17.400 --> 00:04:21.020
但接著你一點會發現接下來其中一個假設有點問題。

00:04:21.044 --> 00:04:22.616
以下只有三個假設。

00:04:23.800 --> 00:04:28.519
智能是關於資訊在物質系統裡處理的過程。

00:04:29.320 --> 00:04:31.935
其實這個陳述稍微多於一個假設

00:04:31.959 --> 00:04:35.416
我們已經在我們的機器裡安裝了有限的智能，

00:04:35.440 --> 00:04:37.456
而很多這樣的機器已經表現出

00:04:37.480 --> 00:04:40.120
某種程度的超人類智能。

00:04:40.840 --> 00:04:43.416
而我們知道這個現象

00:04:43.440 --> 00:04:46.056
可能導致被稱為「通用智能」的東西，

00:04:46.080 --> 00:04:49.736
一種能跨多個領域彈性地思考的能力，

00:04:49.760 --> 00:04:52.896
因為我們的腦已經掌握了這個，對吧？

00:04:52.920 --> 00:04:56.856
我的意思是，裡面都只是原子，

00:04:56.880 --> 00:05:01.376
而只要我們繼續製造基於原子的系統

00:05:01.400 --> 00:05:04.096
越來越能表現智能的行為，

00:05:04.120 --> 00:05:06.656
我們終究會，除非我們被打斷，

00:05:06.680 --> 00:05:10.056
我們終究會造出通用智能

00:05:10.080 --> 00:05:11.376
裝進我們的機器裡。

00:05:11.400 --> 00:05:15.056
關鍵是理解到發展的速率無關緊要，

00:05:15.080 --> 00:05:18.256
因為任何進展都足以帶我們到終結之境。

00:05:18.280 --> 00:05:22.056
我們不需要摩爾定律才能繼續。我們不需要指數型的發展。

00:05:22.080 --> 00:05:23.680
我們只需要繼續前進。

00:05:25.480 --> 00:05:28.400
第二個假設是我們會繼續前進。

00:05:29.000 --> 00:05:31.760
我們會持續改善我們的智能機器。

00:05:33.000 --> 00:05:37.376
而因為智能的價值——

00:05:37.400 --> 00:05:40.936
我的意思是，智能是所有我們珍視的事物的源頭

00:05:40.960 --> 00:05:43.736
或者我們需要智能來保護我們珍視的事物。

00:05:43.760 --> 00:05:46.016
智能是我們最珍貴的資源。

00:05:46.040 --> 00:05:47.576
所以我們想要這麼做。

00:05:47.600 --> 00:05:50.936
我們有許多亟需解決的問題。

00:05:50.960 --> 00:05:54.160
我們想要治癒疾病如阿茲海默症和癌症。

00:05:54.960 --> 00:05:58.896
我們想要了解經濟系統。我們想要改進我們的氣候科學。

00:05:58.920 --> 00:06:01.176
所以我們會這麼做，只要我們可以。

00:06:01.200 --> 00:06:04.486
火車已經出站，而沒有煞車可以拉。

00:06:05.880 --> 00:06:11.336
最後一點，我們不站在智能的巔峰，

00:06:11.360 --> 00:06:13.160
或者根本不在那附近。

00:06:13.640 --> 00:06:15.536
而這真的是一種重要的洞察。

00:06:15.560 --> 00:06:17.976
正是這個讓我們的處境如此危險可疑，

00:06:18.000 --> 00:06:22.040
這也讓我們對風險的直覺變得很不可靠。

00:06:23.120 --> 00:06:25.840
現在，想想這世界上活過的最聰明的人。

00:06:26.640 --> 00:06:30.056
每個人的清單上幾乎都會有 約翰·馮·諾伊曼 。

00:06:30.080 --> 00:06:33.416
我是指， 馮·諾伊曼 對他周圍的人造成的印象，

00:06:33.440 --> 00:06:37.496
而這包括和他同時代最棒的數學家和物理學家，

00:06:37.520 --> 00:06:39.456
被好好地記錄了。

00:06:39.480 --> 00:06:43.256
只要有一半關於他的故事的一半是真的，

00:06:43.280 --> 00:06:44.496
那毫無疑問

00:06:44.520 --> 00:06:46.976
他是世界上活過的最聰明的人之一。

00:06:47.000 --> 00:06:49.520
所以考慮智能的頻譜。

00:06:50.320 --> 00:06:51.749
約翰·馮·諾伊曼 在這裡。

00:06:53.560 --> 00:06:54.894
然後你和我在這裡。

00:06:56.120 --> 00:06:57.416
然後雞在這裡。

00:06:57.440 --> 00:06:59.376
（笑聲）

00:06:59.400 --> 00:07:00.616
抱歉，雞應該在那裡。

00:07:00.640 --> 00:07:01.896
（笑聲）

00:07:01.920 --> 00:07:05.656
我實在無意把這個把這個演講弄得比它本身更讓人感到沮喪。

00:07:05.680 --> 00:07:07.280
（笑聲）

00:07:08.339 --> 00:07:11.816
智能的頻譜似乎勢不可擋地

00:07:11.840 --> 00:07:14.960
往比我們能理解的更遠的地方延伸，

00:07:15.880 --> 00:07:19.096
如果我們造出比我們更有智能的機器，

00:07:19.120 --> 00:07:21.416
他們很可能會探索這個頻譜，

00:07:21.440 --> 00:07:23.296
以我們無法想像的方式，

00:07:23.320 --> 00:07:25.840
然後超越我們以我們無法想像的方式。

00:07:27.000 --> 00:07:31.336
重要的是認識到這說法僅因速度的優勢即為真。

00:07:31.360 --> 00:07:36.416
對吧？請想像如果我們造出了一個超級人工智能

00:07:36.440 --> 00:07:42.186
它不比你一般在史丹佛或者 MIT 遇到的研究團隊聰明。

00:07:42.210 --> 00:07:46.486
電子電路作用的速率比起生化作用快一百萬倍，

00:07:46.520 --> 00:07:51.476
所以這個機器思考應該比製造它的心智快一百萬倍。

00:07:51.520 --> 00:07:53.176
如果你設定讓它運行一星期，

00:07:53.200 --> 00:07:57.760
他會執行人類等級的智能要花兩萬年的工作，

00:07:58.400 --> 00:08:00.360
一週接著一週接著一週。

00:08:01.640 --> 00:08:04.736
我們如何可能理解，較不嚴格地說，

00:08:04.760 --> 00:08:07.040
一個達成如此進展的心智？

00:08:08.840 --> 00:08:10.976
另一個另人擔心的事，老實說，

00:08:11.000 --> 00:08:15.976
是想像最好的情況。

00:08:16.000 --> 00:08:21.596
想像我們想到一個沒有安全顧慮的超級人工智能的設計

00:08:21.600 --> 00:08:24.856
我們第一次就做出了完美的設計。

00:08:24.880 --> 00:08:27.096
如同我們被給予了一個神諭，

00:08:27.120 --> 00:08:29.136
完全照我們的預期地動作。

00:08:29.160 --> 00:08:32.880
這個機器會是完美的人力節約裝置。

00:08:33.680 --> 00:08:37.909
它能設計一個機器，那機器能製造出能做任何人工的機器，

00:08:37.920 --> 00:08:39.376
太陽能驅動，

00:08:39.400 --> 00:08:42.096
幾乎只需要原料的成本。

00:08:42.120 --> 00:08:45.376
所以我們是在談人類苦役的終結。

00:08:45.400 --> 00:08:48.200
我們也是在談大部分的智力工作的終結。

00:08:49.200 --> 00:08:52.256
像我們一樣的猩猩在這種情況下會做什麼？

00:08:52.280 --> 00:08:56.360
我們可能可以自由地玩飛盤和互相按摩。

00:08:57.840 --> 00:09:00.696
加上一點迷幻藥和可議的服裝選擇，

00:09:00.720 --> 00:09:02.896
整個世界都可以像在過火人祭典。

00:09:02.920 --> 00:09:04.560
（笑聲）

00:09:06.320 --> 00:09:08.320
那聽起來也許很不錯，

00:09:09.280 --> 00:09:14.406
但請問，在我們目前的經濟和政治秩序下，會發生什麼事情？

00:09:14.440 --> 00:09:16.856
我們很可能會見證

00:09:16.880 --> 00:09:22.216
一種我們從未見過的財富不均和失業程度。

00:09:22.216 --> 00:09:25.176
缺乏一種意願來把這份新財富馬上

00:09:25.200 --> 00:09:26.680
放在服務全人類，

00:09:27.640 --> 00:09:31.256
少數幾個萬億富翁能登上我們的財經雜誌

00:09:31.280 --> 00:09:33.720
而其他人可以自由地選擇挨餓。

00:09:34.320 --> 00:09:36.616
而俄國和中國會怎麼做？

00:09:36.640 --> 00:09:39.256
當他們聽說矽谷的某個公司

00:09:39.280 --> 00:09:42.016
即將部署一個超級人工智能，

00:09:42.040 --> 00:09:44.896
這個機器能夠發動戰爭，

00:09:44.920 --> 00:09:47.136
無論是領土侵略或者網路電子戰，

00:09:47.160 --> 00:09:48.840
以前所未見的威力。

00:09:50.120 --> 00:09:51.976
這是個贏者全拿的劇本。

00:09:52.000 --> 00:09:55.136
在這個競爭領先六個月

00:09:55.160 --> 00:09:57.936
等於領先五十萬年，

00:09:57.960 --> 00:09:59.456
最少。

00:09:59.480 --> 00:10:04.216
所以即使僅僅是這種突破的謠言

00:10:04.240 --> 00:10:06.616
都能使我們這個種族走向狂暴。

00:10:06.640 --> 00:10:09.536
現在，最讓人驚恐的事情，

00:10:09.560 --> 00:10:12.336
在我的看法，在這個時刻，

00:10:12.360 --> 00:10:16.656
是人工智慧研究者說的那類話

00:10:16.680 --> 00:10:18.560
當他們試著表現得讓人安心。

00:10:19.000 --> 00:10:22.456
而最常用來告訴我們現在不要擔心的理由是時間。

00:10:22.480 --> 00:10:24.536
這還有很長的路要走，你不知道嗎，

00:10:24.560 --> 00:10:27.000
起碼還要 50 到 100 年。

00:10:27.720 --> 00:10:28.976
一個研究人員曾說，

00:10:29.000 --> 00:10:30.576
「憂心人工智慧安全

00:10:30.600 --> 00:10:32.880
如同憂心火星人口爆炸。」

00:10:34.116 --> 00:10:35.736
這是矽谷版本的

00:10:35.760 --> 00:10:37.840
「別杞人憂天。」

00:10:37.840 --> 00:10:39.496
（笑聲）

00:10:39.520 --> 00:10:41.416
似乎沒人注意到

00:10:41.440 --> 00:10:44.056
以時間當參考

00:10:44.080 --> 00:10:46.656
是一個不合理的推論。

00:10:46.680 --> 00:10:49.936
如果智能只是關於資訊的處理，

00:10:49.960 --> 00:10:52.616
而我們持續改善我們的機器，

00:10:52.640 --> 00:10:55.520
我們會製作出某種形式的超級智能。

00:10:56.320 --> 00:10:59.976
而且我們不知道要花我們多長的時間

00:11:00.000 --> 00:11:02.400
來創造安全地這麼做的條件。

00:11:04.200 --> 00:11:05.496
讓我再說一次，

00:11:05.520 --> 00:11:09.336
我們不知道要花我們多長的時間

00:11:09.360 --> 00:11:11.600
來創造安全地這麼做的條件。

00:11:12.920 --> 00:11:16.376
而且如果你還沒注意到， 50 年已經不像以前的概念。

00:11:16.400 --> 00:11:18.856
這是 50 年以月來表示

00:11:18.880 --> 00:11:20.720
這是我們有了 iPhone 的時間。

00:11:21.440 --> 00:11:24.040
這是《辛普森家庭》在電視上播映的時間。

00:11:24.680 --> 00:11:27.056
50 年不是那麼長的時間

00:11:27.080 --> 00:11:30.240
來面對對我們這個種族來說最巨大的挑戰之一。

00:11:31.640 --> 00:11:35.656
再一次說，我們似乎無法產生適當的情緒反應

00:11:35.680 --> 00:11:38.376
對應我們有所有的理由相信將發生的事。

00:11:38.400 --> 00:11:42.376
資訊科學家斯圖亞特·羅素有個很好的比喻。

00:11:42.400 --> 00:11:47.296
他說，想像我們收到一則外星文明的訊息，

00:11:47.320 --> 00:11:49.016
寫道：

00:11:49.040 --> 00:11:50.576
「地球的人們，

00:11:50.600 --> 00:11:52.960
我們 50 年內會到達你們的星球。

00:11:53.800 --> 00:11:55.376
作好準備。」

00:11:55.400 --> 00:11:59.656
而現在我們只是在倒數外星母艦還剩幾個月登陸？

00:11:59.680 --> 00:12:02.680
我們會比我們現在稍微感到緊迫。

00:12:04.680 --> 00:12:06.536
另一個我們被告知不用擔心的原因

00:12:06.560 --> 00:12:09.576
是這些機器不得不和我們有一樣的價值觀

00:12:09.600 --> 00:12:12.216
因為他們字面上只是我們的延伸。

00:12:12.240 --> 00:12:14.056
它們會被植入我們的大腦裡，

00:12:14.080 --> 00:12:16.440
而我們基本上變成他們大腦的邊緣系統。

00:12:17.120 --> 00:12:18.536
現在用一點時間想想

00:12:18.560 --> 00:12:21.736
這最安全而且唯一謹慎的往前的路，

00:12:21.760 --> 00:12:23.096
被推薦的，

00:12:23.120 --> 00:12:25.920
是將這個科技植入我們的腦內。

00:12:26.600 --> 00:12:29.976
這也許的確是最安全而且唯一謹慎的往前的路，

00:12:30.000 --> 00:12:33.056
但通常科技的安全性問題對一個人來說

00:12:33.080 --> 00:12:36.736
應該在把東西插到你腦袋裡之前就該大部分解決了。

00:12:36.760 --> 00:12:38.776
（笑聲）

00:12:38.800 --> 00:12:44.136
更深層的問題是，打造超級人工智能本身

00:12:44.160 --> 00:12:45.896
似乎相對容易於

00:12:45.920 --> 00:12:47.776
「打造超級人工智慧

00:12:47.800 --> 00:12:49.576
而且擁有完整的神經科學

00:12:49.600 --> 00:12:52.280
讓我們可以把我們的心智無縫與之整合」。

00:12:52.800 --> 00:12:55.976
而假設正在從事人工智能研發的許多公司和政府

00:12:56.000 --> 00:12:59.656
很可能察覺他們正在和所有其他人競爭，

00:12:59.680 --> 00:13:02.936
假設贏了這個競爭就是贏得世界，

00:13:02.960 --> 00:13:05.416
假設你在下一刻不會毀了世界，

00:13:05.440 --> 00:13:08.056
那麼很可能比較容易做的事

00:13:08.080 --> 00:13:09.280
就會先被做完。

00:13:10.560 --> 00:13:13.220
現在，很不幸地，我沒有這個問題的解決方法，

00:13:13.220 --> 00:13:15.640
除了建議我們更多人思考這個問題。

00:13:15.640 --> 00:13:18.270
我想我們需要類似曼哈頓計畫的東西，

00:13:18.270 --> 00:13:20.410
針對人工智能這個課題。

00:13:20.410 --> 00:13:22.790
不是因為我們不可避免地要這麼做而做，

00:13:22.790 --> 00:13:26.616
而是試著理解如何避免軍備競賽

00:13:26.640 --> 00:13:30.136
而且用一種符合我們利益的方式打造之。

00:13:30.160 --> 00:13:34.356
當你在談論能夠對其本身造成改變的超級人工智能

00:13:34.600 --> 00:13:39.216
這似乎說明我們只有一次機會把初始條件做對，

00:13:39.240 --> 00:13:41.296
而且我們會必須承受

00:13:41.320 --> 00:13:44.360
為了將它們做對的經濟和政治的後果

00:13:45.760 --> 00:13:47.816
但一旦我們承認

00:13:47.840 --> 00:13:51.840
資訊處理是智能的源頭，

00:13:52.720 --> 00:13:57.520
某些適當的電腦系統是智能的基礎，

00:13:58.360 --> 00:14:02.120
而且我們承認我們會持續改進這些系統，

00:14:03.280 --> 00:14:07.736
而且我們承認認知的極限有可能遠遠超越

00:14:07.760 --> 00:14:08.960
我們目前所知，

00:14:10.120 --> 00:14:11.336
然後我們必須承認

00:14:11.360 --> 00:14:14.000
我們正在打造某種神明的過程裡

00:14:15.400 --> 00:14:16.976
現在是個好時機

00:14:17.000 --> 00:14:18.953
來確保那是個我們能夠與之共存的神明。

00:14:20.120 --> 00:14:21.656
謝謝大家。

