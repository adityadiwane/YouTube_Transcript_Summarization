WEBVTT
Kind: captions
Language: uk

00:00:00.000 --> 00:00:07.000
Перекладач: Olga Murphy
Утверджено: Hanna Leliv

00:00:13.103 --> 00:00:15.480
Сьогодні я розповім 
про нездатність до інтуїції,

00:00:15.480 --> 00:00:17.280
від якої страждають багато з нас.

00:00:17.280 --> 00:00:21.380
Це дійсно нездатність визначати
певний вид небезпеки.

00:00:21.410 --> 00:00:22.950
Я опишу сценарій,

00:00:22.960 --> 00:00:26.094
що, на мою думку, є водночас жахливий

00:00:26.314 --> 00:00:28.310
та дуже ймовірний

00:00:28.760 --> 00:00:30.400
і не є гарною комбінацією,

00:00:30.400 --> 00:00:31.720
як виявляється.

00:00:31.990 --> 00:00:34.716
Та все одно, замість того, щоб злякатися,
більшість з вас відчують,

00:00:34.804 --> 00:00:37.120
що те, про що я веду мову,
скоріше прикольне.

00:00:37.550 --> 00:00:39.580
Я розповім, як прибутки,

00:00:39.580 --> 00:00:41.554
що ми отримуємо від штучного інтелекту,

00:00:41.554 --> 00:00:43.460
можуть, врешті-решт, знищити нас.

00:00:43.460 --> 00:00:46.566
Насправді, я думаю, дуже важко
зрозуміти, як вони НЕ знищать нас

00:00:46.566 --> 00:00:48.880
або не надихнуть нас знищити самих себе.

00:00:49.250 --> 00:00:51.050
Та якщо ви такий як я,

00:00:51.050 --> 00:00:53.576
вам здасться, що думати 
про такі речі - весело,

00:00:53.856 --> 00:00:57.186
І така реакція є частиною проблеми.

00:00:57.186 --> 00:00:59.466
Ця реакція має хвилювати вас.

00:01:00.036 --> 00:01:02.720
І якщо б я хотів запевнити 
вас у цій промові,

00:01:02.790 --> 00:01:05.996
що нам, скоріше за все, доведеться 
страждати від глобального голоду

00:01:06.026 --> 00:01:08.556
через кліматичні зміни
або якусь іншу катастрофу,

00:01:09.436 --> 00:01:12.386
та що ваші онуки та їхні онуки

00:01:12.466 --> 00:01:14.446
будуть ймовірно жити саме так,

00:01:15.096 --> 00:01:16.580
ви б не подумали:

00:01:17.410 --> 00:01:18.510
"Цікаво.

00:01:18.664 --> 00:01:20.430
Мені подобається цей виступ на ТED".

00:01:21.510 --> 00:01:23.310
Голод - це не весело.

00:01:23.760 --> 00:01:26.940
А от смерть у науковій фантастиці,
з іншого боку - це весело,

00:01:26.940 --> 00:01:30.606
та одна з речей, що мене найбільше турбує
щодо розвитку штучного інтелекту -

00:01:30.606 --> 00:01:35.260
це наша нездатність мати відповідну 
емоційну реакцію

00:01:35.260 --> 00:01:37.296
щодо небезпек, що чигають на нас попереду.

00:01:37.386 --> 00:01:40.766
Я не здатний осягнути цю реакцію
і саме тому виступаю на цій сцені.

00:01:42.120 --> 00:01:44.816
Це так, наче ми стоїмо 
навпроти двох дверей.

00:01:44.840 --> 00:01:46.096
За дверима номер 1

00:01:46.120 --> 00:01:49.416
ми припиняємо прогрес
у створенні розумних машин.

00:01:49.440 --> 00:01:53.456
Технічне та програмне забезпечення
перестає покращуватись через якісь обставини.

00:01:53.480 --> 00:01:56.480
А тепер поміркуймо,
чому б це могло трапитися.

00:01:57.080 --> 00:02:00.736
Знаючи, якими цінними є
інтелект та автоматизація,

00:02:00.760 --> 00:02:04.280
ми продовжимо удосконалювати
наші технології, якщо матимемо можливість.

00:02:05.200 --> 00:02:06.867
Що нас може зупинити від цього?

00:02:07.800 --> 00:02:09.600
Широкомасштабна ядерна війна?

00:02:11.000 --> 00:02:12.560
Глобальна пандемія?

00:02:14.320 --> 00:02:15.640
Падіння астероїду?

00:02:17.640 --> 00:02:20.216
Джастін Бібер, що став 
президентом Сполучених Штатів?

00:02:20.240 --> 00:02:22.520
(Сміх)

00:02:24.760 --> 00:02:28.680
Суть у тому, що щось має знищити 
цивілізацію у знайомому нас вигляді.

00:02:29.360 --> 00:02:33.656
Уявіть собі, яке страхіття має статися,

00:02:33.680 --> 00:02:37.016
щоб завадити нам удосконалювати
наші технології

00:02:37.040 --> 00:02:38.256
постійно,

00:02:38.280 --> 00:02:40.296
покоління за поколінням.

00:02:40.320 --> 00:02:42.456
Майже за визначенням,
це найгірше,

00:02:42.480 --> 00:02:44.496
що траплялося в історії людства.

00:02:44.520 --> 00:02:45.816
Тому єдиною альтернативою,

00:02:45.840 --> 00:02:48.176
а саме вона знаходиться
за дверима номер два,

00:02:48.200 --> 00:02:51.336
є подальше удосконалення 
розумних машин

00:02:51.360 --> 00:02:52.960
рік за роком, рік за роком.

00:02:53.720 --> 00:02:57.360
Та в певний момент ми збудуємо
машини, розумніші за нас,

00:02:58.080 --> 00:03:00.696
та щойно з'являться машини,
що розумніші за нас,

00:03:00.720 --> 00:03:02.696
вони почнуть самі себе удосконалювати.

00:03:02.720 --> 00:03:05.456
А тоді ми ризикуємо тим, 
що математик І.Дж. Ґуд назвав

00:03:05.480 --> 00:03:07.256
"інтелектуальним вибухом",

00:03:07.540 --> 00:03:09.840
тим, що цей процес може вийти 
з-під нашого контролю.

00:03:10.120 --> 00:03:12.936
Тепер це часто висміюють,
як ось на цьому зображенні,

00:03:12.960 --> 00:03:16.176
як страх, що армії злісних роботів

00:03:16.200 --> 00:03:17.456
нас атакують.

00:03:17.480 --> 00:03:20.176
Та це не найімовірніший сценарій.

00:03:20.200 --> 00:03:25.056
Йдеться не про те, що наші машини
зненацька зробляться злими.

00:03:25.080 --> 00:03:27.696
Викликає тривогу те,
що ми збудуємо машини,

00:03:27.720 --> 00:03:29.776
які будуть настільки компетентніші
за нас,

00:03:29.800 --> 00:03:33.576
що найменші відхилення 
між нашими та їхніми цілями

00:03:33.600 --> 00:03:34.800
зможуть нас знищити.

00:03:35.730 --> 00:03:38.430
Лише поміркуйте, як ми поводимося
з мурахами.

00:03:38.600 --> 00:03:40.256
Ми не ненавидимо їх.

00:03:40.280 --> 00:03:42.336
Ми не звертаємо зі шляху,
щоб нашкодити їм.

00:03:42.360 --> 00:03:44.736
Насправді іноді ми намагаємося 
не нашкодити їм.

00:03:44.760 --> 00:03:46.776
Ми переступаємо через них на стежці.

00:03:46.800 --> 00:03:48.936
Та коли їхня присутність

00:03:48.960 --> 00:03:51.420
відчутно конфліктує 
з будь-якою нашою метою,

00:03:51.420 --> 00:03:53.947
скажімо, коли ми будуємо
таку споруду, як на зображенні,

00:03:53.981 --> 00:03:55.941
ми знищуємо їх без сумнівів.

00:03:56.480 --> 00:03:59.416
Тривога полягає в тому,
що одного дня ми збудуємо машини,

00:03:59.440 --> 00:04:02.176
які, свідомі чи ні,

00:04:02.200 --> 00:04:04.690
можуть поставитися до нас
так само байдуже.

00:04:05.760 --> 00:04:08.720
Гадаю, що це здається
багатьом з вас малоймовірним.

00:04:09.360 --> 00:04:15.696
Я впевнений, серед вас є ті, хто сумнівається,
що надрозумні машини можливі,

00:04:15.720 --> 00:04:17.376
що й казати про неминучість.

00:04:17.400 --> 00:04:21.020
Тоді ви засумніваєтеся у правильності
одного з таких припущень.

00:04:21.044 --> 00:04:22.616
А їх лише три.

00:04:23.800 --> 00:04:28.519
Інтелект - це обробка інформації
у фізичних системах.

00:04:29.320 --> 00:04:31.935
Насправді, це трохи більше,
ніж припущення.

00:04:31.959 --> 00:04:35.416
Ми вже вбудували певний інтелект
в наші машини,

00:04:35.440 --> 00:04:37.456
і багато з цих машин видають результати

00:04:37.480 --> 00:04:40.120
на рівні надлюдського інтелекту 
вже сьогодні.

00:04:40.840 --> 00:04:43.416
Та ми знаємо, що найменший привід

00:04:43.440 --> 00:04:46.056
може призвести до так званого
"інтелектуального вибуху",

00:04:46.080 --> 00:04:49.736
спроможності гнучко мислити
у багатьох сферах,

00:04:49.760 --> 00:04:52.896
адже наш мозок вже зміг це зробити.
Чи не так?

00:04:52.920 --> 00:04:56.856
Я маю на увазі, що тут є лише атоми,

00:04:56.880 --> 00:05:01.376
та допоки ми продовжуємо
будувати системи з атомів,

00:05:01.400 --> 00:05:04.096
що демонструють все більш
інтелектуальну поведінку,

00:05:04.120 --> 00:05:06.656
ми поступово,
якщо нас не зупинять,

00:05:06.680 --> 00:05:10.056
поступово вбудуємо
загальний інтелект

00:05:10.080 --> 00:05:11.376
в наші машини.

00:05:11.400 --> 00:05:14.836
Важливо зрозуміти,
що рівень прогресу не має значення,

00:05:14.870 --> 00:05:18.256
тому що будь-який прогрес здатний
призвести нас до фінального пункту.

00:05:18.280 --> 00:05:22.146
Нам не треба закону Мура, щоб продовжувати.
Нам не потрібний експоненційний прогрес.

00:05:22.146 --> 00:05:23.680
Нам лише треба не зупинятися.

00:05:25.480 --> 00:05:28.400
Друге припущення полягає в тому,
що ми не зупинимося.

00:05:29.000 --> 00:05:31.760
Будемо далі удосконалювати
наші розумні машини.

00:05:33.000 --> 00:05:37.376
Та враховуючи цінність інтелекту--

00:05:37.400 --> 00:05:40.936
Я маю на увазі, інтелект є або
джерелом всього, що ми цінуємо,

00:05:40.960 --> 00:05:43.736
або він нам потрібен, щоб зберегти
все, що ми цінуємо.

00:05:43.760 --> 00:05:46.016
Це наш найцінніший ресурс.

00:05:46.040 --> 00:05:47.576
Тому ми хочемо це робити.

00:05:47.600 --> 00:05:50.936
В нас є проблеми,
що їх нам вкрай необхідно залагодити.

00:05:50.960 --> 00:05:54.160
Ми хочемо лікувати такі захворювання,
як Альцгеймер та рак.

00:05:54.960 --> 00:05:58.896
Ми хочемо зрозуміти економічні системи.
Ми хочемо покращити нашу кліматичну науку.

00:05:58.920 --> 00:06:01.176
Тому ми будемо це робити,
якщо зможемо.

00:06:01.200 --> 00:06:04.486
Потяг вже поїхав зі станції,
і немає гальма, щоб його зупинити.

00:06:05.880 --> 00:06:11.336
Нарешті, ми не стоїмо
на вершині інтелекту,

00:06:11.360 --> 00:06:13.160
і навряд чи десь поблизу.

00:06:13.640 --> 00:06:15.536
І це насправді ключовий факт.

00:06:15.560 --> 00:06:17.976
Це те, що робить нашу ситуацію
такою небезпечною,

00:06:18.000 --> 00:06:22.040
і це те, що робить нашу інтуїцію 
щодо ризику настільки ненадійною.

00:06:23.120 --> 00:06:26.130
Тепер подумайте про найрозумнішу людину,
яка будь-коли існувала.

00:06:26.640 --> 00:06:30.056
Майже у всіх у списку є Джон фон Нейман.

00:06:30.080 --> 00:06:33.416
Я гадаю, що враження, яке фон Нейман
справляв на людей довкола,

00:06:33.440 --> 00:06:37.496
до яких входили найвидатніші математики
та фізики його часу,

00:06:37.520 --> 00:06:39.456
достатньо добре задокументоване.

00:06:39.480 --> 00:06:43.256
Якщо лише половина історій про нього 
бодай наполовину правдива,

00:06:43.280 --> 00:06:44.496
немає сумнівів, що

00:06:44.520 --> 00:06:46.976
він один із найрозумніших людей,
які коли-небудь існували.

00:06:47.000 --> 00:06:49.520
Тож обміркуємо спектр інтелекту.

00:06:50.320 --> 00:06:51.749
Ось тут у нас Джон фон Нейман.

00:06:53.560 --> 00:06:54.894
А ось тут ми з вами.

00:06:56.120 --> 00:06:57.416
А ось тут курка.

00:06:57.440 --> 00:06:59.376
(Сміх)

00:06:59.400 --> 00:07:00.616
Вибачте, курка.

00:07:00.640 --> 00:07:01.896
(Сміх)

00:07:01.920 --> 00:07:05.816
Мені нема чого робити цю промову
гнітючішою, аніж вона має бути.

00:07:05.816 --> 00:07:07.280
(Сміх)

00:07:08.339 --> 00:07:11.816
Здається надзвичайно ймовірним,
що спектр інтелекту

00:07:11.840 --> 00:07:14.960
простягається набагато далі,
ніж ми сьогодні уявляємо,

00:07:15.880 --> 00:07:19.096
і якщо ми збудуємо машини,
що розумніші за нас,

00:07:19.120 --> 00:07:21.416
вони, дуже ймовірно,
розвідають цей спектр

00:07:21.440 --> 00:07:23.296
шляхами, які ми не можемо уявити,

00:07:23.320 --> 00:07:25.840
та перевершать нас так,
як ми не можемо уявити.

00:07:27.000 --> 00:07:31.336
Та важливо визнати, що
це правда вже з огляду на саму силу швидкості.

00:07:31.360 --> 00:07:36.416
Правильно? Тож уявіть, що ми збудували
надрозумний штучний інтелект,

00:07:36.440 --> 00:07:39.896
такий же розумний, як ваша
середньостатистична команда дослідників

00:07:39.920 --> 00:07:42.216
у Стенфорді чи МТІ.

00:07:42.240 --> 00:07:44.950
Електронні ланцюги
функціюють у мільйон разів швидше

00:07:44.990 --> 00:07:46.496
за біохемічні,

00:07:46.520 --> 00:07:49.656
тож ці машини мають думати
в мільйон разів швидше,

00:07:49.680 --> 00:07:51.480
ніж розуми, що їх сконструювали.

00:07:51.480 --> 00:07:53.336
Тож ви запускаєте її на тиждень,

00:07:53.336 --> 00:07:57.760
і вона демонструє 20 000 років
інтелектуальної роботи людського рівня,

00:07:58.400 --> 00:08:00.360
тиждень за тижнем, тиждень за тижнем.

00:08:01.640 --> 00:08:04.736
Як ми можемо зрозуміти,
тим паче стримати

00:08:04.760 --> 00:08:07.610
інтелект, що так прогресує?

00:08:08.840 --> 00:08:10.976
Тривогу викликає ще одна річ:

00:08:11.000 --> 00:08:15.976
це, уявіть собі, найкращий сценарій.

00:08:16.000 --> 00:08:20.176
А тепер уявіть, ми створюємо надрозумний
штучний інтелект,

00:08:20.210 --> 00:08:21.586
не зважаючи на жодні
міркування безпеки.

00:08:21.600 --> 00:08:24.856
В нас уперше з'являється 
бездоганний проект.

00:08:24.880 --> 00:08:27.096
Так наче ми отримали оракула,

00:08:27.120 --> 00:08:29.136
що поводиться саме так, як планувалося.

00:08:29.160 --> 00:08:32.880
Тож така машина була б
ідеальним пристроєм з економії праці.

00:08:33.440 --> 00:08:36.109
Вона може проектувати машини,
що можуть будувати машини,

00:08:36.133 --> 00:08:37.896
що можуть робити будь-яку фізичну роботу,

00:08:37.920 --> 00:08:39.476
працюватимуть на сонячній енергії,

00:08:39.476 --> 00:08:42.096
більш-менш за ціною сировини.

00:08:42.120 --> 00:08:45.376
Тож ми говоримо про кінець
важкої людської праці.

00:08:45.400 --> 00:08:48.580
Ми також говоримо про кінець
найбільш інтелектуальної праці.

00:08:49.200 --> 00:08:52.256
То що ж такі мавпи, як ми, 
будуть робити за цих обставин?

00:08:52.280 --> 00:08:56.360
Ми зможемо вільно грати в Фрізбі
та робити одне одному масаж.

00:08:57.840 --> 00:09:00.696
Додайте трохи ЛСД та трохи
сумнівного гардеробу,

00:09:00.720 --> 00:09:02.896
і весь світ буде
як фестиваль Burning Man.

00:09:02.920 --> 00:09:04.560
(Сміх)

00:09:06.320 --> 00:09:08.320
Непогано звучить,

00:09:09.280 --> 00:09:11.656
але запитайте себе, що станеться

00:09:11.680 --> 00:09:14.416
з нашим теперішнім економічним
та політичним ладом?

00:09:14.440 --> 00:09:16.856
Ймовірно, ми станемо свідками

00:09:16.880 --> 00:09:21.016
такої нерівності розподілу багатства
та безробіття,

00:09:21.040 --> 00:09:22.536
яких ми ніколи не бачили.

00:09:22.560 --> 00:09:25.176
Не бажаючи одразу
використовувати це нове багатство

00:09:25.200 --> 00:09:26.680
на користь всього людства,

00:09:27.640 --> 00:09:31.256
декілька мільярдерів прикрасять 
обкладинки наших бізнес-журналів,

00:09:31.280 --> 00:09:33.720
тоді як решта світу
голодуватиме.

00:09:34.320 --> 00:09:36.616
А що робитимуть росіяни та китайці,

00:09:36.640 --> 00:09:39.256
якщо вони почують, що якась компанія
у Кремнієвій долині

00:09:39.280 --> 00:09:42.016
збирається ввести в дію 
надрозумний штучний інтелект?

00:09:42.040 --> 00:09:44.896
Ця машина зможе вести війну,

00:09:44.920 --> 00:09:47.136
наземну або кібер,

00:09:47.160 --> 00:09:48.840
з безпрецедентною могутністю.

00:09:50.120 --> 00:09:51.976
Це сценарій, де переможець отримує все.

00:09:52.000 --> 00:09:55.136
Бути на шість місяців попереду
у цьому змаганні,

00:09:55.160 --> 00:09:57.936
це бути на 500 000 років попереду,

00:09:57.960 --> 00:09:59.456
як мінімум.

00:09:59.480 --> 00:10:04.216
Тож здається, що навіть незначні чутки
про такі досягнення

00:10:04.240 --> 00:10:06.616
можуть спричинити 
люту ненависть між людьми.

00:10:06.640 --> 00:10:09.536
Але одна з найжахливіших речей,

00:10:09.560 --> 00:10:12.336
на мою думку, на цей час -

00:10:12.360 --> 00:10:16.656
це те, що кажуть розробники
штучного інтелекту,

00:10:16.680 --> 00:10:18.750
коли хочуть заспокоїти нас.

00:10:19.000 --> 00:10:22.456
Найпоширенішою підставою
не турбуватися, кажуть вони, є час.

00:10:22.480 --> 00:10:24.536
Це ще так далеко, 
хіба ви не знаєте.

00:10:24.560 --> 00:10:27.000
Це ще може тривати 50 або 100 років.

00:10:27.720 --> 00:10:28.976
Один із дослідників сказав:

00:10:29.000 --> 00:10:30.576
"Хвилюватися через безпеку 
штучного інтелекту -

00:10:30.600 --> 00:10:32.880
те ж саме, що перейматися
перенаселенням на Марсі".

00:10:34.116 --> 00:10:35.736
Це така версія Кремнієвої долини

00:10:35.760 --> 00:10:38.136
фрази: "Не забивай цим
свою гарненьку голівку".

00:10:38.160 --> 00:10:39.496
(Сміх)

00:10:39.520 --> 00:10:41.416
Здається, ніхто не помічає,

00:10:41.440 --> 00:10:44.056
що посилатися на часову віддаленість -

00:10:44.080 --> 00:10:46.656
хибно.

00:10:46.680 --> 00:10:49.936
Якщо інтелект - це питання
обробки інформації,

00:10:49.960 --> 00:10:52.616
і ми далі удосконалюватимемо наші машини,

00:10:52.640 --> 00:10:55.520
то ми створимо
якусь форму супер-інтелекту.

00:10:56.320 --> 00:10:59.976
І ми не уявляємо,
скільки часу нам потрібно,

00:11:00.000 --> 00:11:02.400
щоб створити умови,
де це буде безпечно.

00:11:03.980 --> 00:11:05.496
Повторюю ще раз.

00:11:05.520 --> 00:11:09.336
Ми не уявляємо, 
скільки часу нам потрібно,

00:11:09.360 --> 00:11:12.250
щоб створити умови,
де це буде безпечно.

00:11:12.920 --> 00:11:16.376
І якщо ви не помітили,
50 років - вже не ті, що раніше.

00:11:16.400 --> 00:11:18.856
Це 50 років у місяцях.

00:11:18.880 --> 00:11:20.870
Стільки часу в нас уже є iPhone.

00:11:21.440 --> 00:11:24.040
Стільки часу "Сімпсони" вже
існують на телебаченні.

00:11:24.550 --> 00:11:27.056
П'ятдесят років -
це не так вже й багато часу,

00:11:27.080 --> 00:11:30.780
щоб прийняти найбільший виклик,
з яким зіткнеться людський рід.

00:11:31.640 --> 00:11:35.656
Повторюю: ми, здається, не маємо
відповідної емоційної реакції

00:11:35.680 --> 00:11:38.376
на те, що точно до нас наближається.

00:11:38.400 --> 00:11:42.376
Комп'ютерний експерт Стюарт Рассел
наводить гарну аналогію.

00:11:42.400 --> 00:11:47.296
Він сказав: "Уявіть, що ми отримали
повідомлення від інопланетної цивілізації,

00:11:47.320 --> 00:11:49.016
в якому йдеться:

00:11:49.040 --> 00:11:50.576
"Земляни,

00:11:50.600 --> 00:11:52.960
ми прибудемо на вашу планету через 50 років.

00:11:53.800 --> 00:11:55.376
Готуйтеся".

00:11:55.400 --> 00:11:59.656
І тепер ми лише рахуємо місяці, що
лишилися до прибуття космічного корабля?

00:11:59.680 --> 00:12:02.680
Ми б відчули тоді трохи більше терміновості,
ніж ми відчуваємо тепер.

00:12:04.470 --> 00:12:06.576
Інша причина, чому нам кажуть
не хвилюватися, полягає в тому,

00:12:06.576 --> 00:12:09.576
що ці машини
муситимуть поділяти наші цінності,

00:12:09.600 --> 00:12:12.216
адже вони будуть фактично
продовженням нас самих.

00:12:12.240 --> 00:12:14.056
Їх трансплантують у наші мізки,

00:12:14.080 --> 00:12:16.440
і ми, по суті,
станемо їхньою лімбічною системою.

00:12:17.120 --> 00:12:18.536
Але задумаймось:

00:12:18.560 --> 00:12:21.736
найбезпечніший
та єдиний розважливий шлях уперед,

00:12:21.760 --> 00:12:23.096
що його нам рекомендують -

00:12:23.120 --> 00:12:25.920
це імплантація цієї технології
прямо у мозок.

00:12:26.600 --> 00:12:29.976
Так, це може бути найбезпечнішим
та єдиним розважливим шляхом уперед,

00:12:30.000 --> 00:12:33.056
але зазвичай усі проблеми з безпекою

00:12:33.080 --> 00:12:36.736
мають бути розв'язані до того,
як технологію вставлять у вашу голову.

00:12:36.760 --> 00:12:38.776
(Сміх)

00:12:38.800 --> 00:12:44.136
Глибшою проблемою є те, що створення
лише надрозумного штучного інтелекту

00:12:44.160 --> 00:12:45.896
здається легшим,

00:12:45.920 --> 00:12:47.776
ніж створення надрозумного
штучного інтелекту

00:12:47.800 --> 00:12:49.576
та володіння повноцінною неврологією,

00:12:49.600 --> 00:12:52.570
яка б дала нам змогу ефективно
інтегрувати наш розум із ним.

00:12:52.800 --> 00:12:55.976
І, знаючи це, компанії та 
уряди, що займаються цією розробкою,

00:12:56.000 --> 00:12:59.656
сприймають себе наче у перегонах
проти всіх інших,

00:12:59.680 --> 00:13:02.936
тому що виграти ці перегони - 
це виграти світ,

00:13:02.960 --> 00:13:05.416
якщо звісно його не буде знищено
наступної хвилини,

00:13:05.440 --> 00:13:08.056
отже, здається, що те, 
що зробити найлегше,

00:13:08.080 --> 00:13:09.280
зроблять у першу чергу.

00:13:10.560 --> 00:13:13.416
На жаль, у мене немає 
розв'язку цієї проблеми,

00:13:13.440 --> 00:13:16.056
окрім як порадити,
щоб більшість із нас думала про це.

00:13:16.080 --> 00:13:18.456
Гадаю, нам потрібно, щось
на кшталт "Проекту Мангеттен"

00:13:18.480 --> 00:13:20.496
на тему штучного інтелекту.

00:13:20.520 --> 00:13:23.256
Не для того, щоб створити його, адже
я думаю, ми неминуче це зробимо,

00:13:23.280 --> 00:13:26.616
а щоб зрозуміти,
як уникнути збройних перегонів

00:13:26.640 --> 00:13:30.136
та створити його так, щоб він
відповідав нашим інтересам.

00:13:30.160 --> 00:13:32.296
Коли ми розмовляємо про
надрозумний штучний інтелект,

00:13:32.320 --> 00:13:34.576
що може самозмінюватися,

00:13:34.600 --> 00:13:39.216
здається, в нас є лише один шанс
отримати правильні початкові умови,

00:13:39.240 --> 00:13:41.296
і навіть тоді нам доведеться пережити

00:13:41.320 --> 00:13:44.360
економічні та політичні наслідки
отримання правильних умов.

00:13:45.760 --> 00:13:47.816
Та в той момент. коли ми визнаємо,

00:13:47.840 --> 00:13:51.840
що оброблення інформації
є джерелом інтелекту,

00:13:52.720 --> 00:13:57.520
що деякі відповідні обчислювальні системи
є базою інтелекту,

00:13:58.360 --> 00:14:02.120
і визнаємо, що ми будемо удосконалювати
ці системи беззупинно,

00:14:03.280 --> 00:14:07.736
а також те, що горизонт пізнання,
скоріш за все, простягається набагато далі,

00:14:07.760 --> 00:14:09.400
ніж ми на цей час розуміємо,

00:14:09.860 --> 00:14:11.336
тоді нам доведеться визнати,

00:14:11.360 --> 00:14:14.000
що ми в процесі створення
своєрідного бога.

00:14:15.400 --> 00:14:16.976
І настав час переконатися,

00:14:17.000 --> 00:14:18.953
що з цим богом ми зможемо жити.

00:14:20.120 --> 00:14:21.656
Дуже вам дякую.

00:14:21.680 --> 00:14:26.773
(Оплески)

