WEBVTT
Kind: captions
Language: ja

00:00:00.000 --> 00:00:07.000
翻訳: Yasushi Aoki
校正: Kaori Nozaki

00:00:12.820 --> 00:00:14.570
我々の多くが抱えている―

00:00:14.570 --> 00:00:17.280
直感の誤りについて
お話しします

00:00:17.300 --> 00:00:21.100
正確には ある種の危険を
察知し損なうということです

00:00:21.110 --> 00:00:22.916
これから説明するシナリオは

00:00:22.940 --> 00:00:26.196
私の考えでは
恐ろしいと同時に

00:00:26.220 --> 00:00:28.480
起こりそうなこと
でもあります

00:00:28.500 --> 00:00:30.466
ありがたくない
組み合わせですね

00:00:30.466 --> 00:00:31.876
考えてみたら

00:00:31.900 --> 00:00:34.496
しかも 多くの人が
それを恐れるよりは

00:00:34.496 --> 00:00:36.960
素敵なことのように
感じています

00:00:36.960 --> 00:00:39.810
人工知能によって
我々がどのように利益を得

00:00:39.810 --> 00:00:43.546
そして最終的には
破滅を招きかねないか お話しします

00:00:43.546 --> 00:00:45.486
人工知能が
人類を破滅させたり

00:00:45.486 --> 00:00:47.510
自滅に追い込んだりしない
シナリオは

00:00:47.510 --> 00:00:49.200
実際 考えにくい気がします

00:00:49.220 --> 00:00:51.076
それでも皆さんが
私と同じなら

00:00:51.100 --> 00:00:53.866
そういったことについて考えるのは
楽しいことでしょう

00:00:53.866 --> 00:00:57.150
その反応自体が問題なのです

00:00:57.150 --> 00:00:59.670
そういう反応は
懸念すべきです

00:00:59.740 --> 00:01:01.850
仮にこの講演で
私の訴えていることが

00:01:01.850 --> 00:01:05.490
地球温暖化や
何かの大異変のため

00:01:05.490 --> 00:01:08.916
世界的な飢饉がやってくる
ということだったとしましょう

00:01:08.940 --> 00:01:12.330
私たちの孫や
その孫の世代は

00:01:12.330 --> 00:01:14.860
この写真のような
有様になるんだと

00:01:14.920 --> 00:01:17.070
その場合
こうは思わないでしょう

00:01:17.120 --> 00:01:18.446
「やあ 面白いな

00:01:18.446 --> 00:01:20.790
このTEDトーク気に入ったよ」

00:01:21.020 --> 00:01:23.560
飢饉は楽しいもの
ではありません

00:01:23.570 --> 00:01:26.996
一方で SFの中の破滅は
楽しいものなのです

00:01:27.020 --> 00:01:30.996
今の時点で AIの発展について
私が最も懸念するのは

00:01:31.020 --> 00:01:33.190
将来に待ち受けている
危険に対して

00:01:33.190 --> 00:01:36.956
我々が感情的に適切な反応を
できずにいることです

00:01:36.980 --> 00:01:41.280
こう話している私自身
そういう反応をできずにいます

00:01:41.750 --> 00:01:44.636
我々は２つの扉を前に
立っているようなものです

00:01:44.660 --> 00:01:45.916
１つ目の扉の先には

00:01:45.940 --> 00:01:49.236
知的な機械の開発をやめるという
道があります

00:01:49.260 --> 00:01:53.276
ハードウェアやソフトウェアの進歩が
何らかの理由で止まるのです

00:01:53.300 --> 00:01:56.840
そういうことが起きうる要因を
ちょっと考えてみましょう

00:01:56.860 --> 00:02:00.330
自動化やインテリジェント化によって
得られる価値を考えれば

00:02:00.330 --> 00:02:04.720
人間は可能な限り
テクノロジーを進歩させ続けるはずです

00:02:05.020 --> 00:02:07.367
それが止まるとしたら
理由は何でしょう？

00:02:07.620 --> 00:02:09.610
全面核戦争とか

00:02:10.720 --> 00:02:12.940
世界的な疫病の大流行とか

00:02:14.140 --> 00:02:15.940
小惑星の衝突とか

00:02:17.320 --> 00:02:20.036
ジャスティン・ビーバーの
大統領就任とか

00:02:20.060 --> 00:02:22.340
(笑)

00:02:24.450 --> 00:02:29.170
要は 何かで現在の文明が
崩壊する必要があるということです

00:02:29.180 --> 00:02:33.120
それがどんなに ひどい事態であるはずか
考えてみてください

00:02:33.120 --> 00:02:35.320
技術の進歩を

00:02:35.320 --> 00:02:37.180
幾世代にもわたり

00:02:37.180 --> 00:02:40.050
半永久的に妨げるようなことです

00:02:40.050 --> 00:02:41.390
ほぼ間違いなく

00:02:41.390 --> 00:02:44.150
人類史上
最悪の出来事でしょう

00:02:44.210 --> 00:02:45.636
もう１つの選択肢

00:02:45.660 --> 00:02:48.066
２番目の扉の
向こうにあるのは

00:02:48.066 --> 00:02:50.460
知的な機械の進歩が

00:02:50.460 --> 00:02:53.420
ずっと続いていく未来です

00:02:53.460 --> 00:02:57.700
するとどこかの時点で 人間よりも
知的な機械を作ることになるでしょう

00:02:57.700 --> 00:03:00.320
そしてひとたび
人間より知的な機械が生まれたなら

00:03:00.320 --> 00:03:02.400
機械は自分で
進化していくでしょう

00:03:02.400 --> 00:03:05.150
そうなると我々は 数学者
I・J・グッドが言うところの

00:03:05.150 --> 00:03:07.076
「知能爆発」の
リスクに直面します

00:03:07.100 --> 00:03:09.900
進歩のプロセスが
人間の手を離れてしまうのです

00:03:09.940 --> 00:03:11.860
それはこの絵のような

00:03:11.860 --> 00:03:15.470
悪意を持つロボットの大群に
襲われる恐怖として

00:03:15.470 --> 00:03:17.180
よく戯画化されています

00:03:17.180 --> 00:03:20.236
でもそれは ごくありそうな
シナリオではないでしょう

00:03:20.236 --> 00:03:24.740
別に機械が悪意に目覚めるという
わけではありません

00:03:24.740 --> 00:03:26.550
本当に 心配すべきなのは

00:03:26.550 --> 00:03:29.586
人間よりもはるかに優れた
機械を作ってしまうと

00:03:29.586 --> 00:03:32.540
人間と機械の目的の
ほんのわずかなズレによって

00:03:32.540 --> 00:03:35.470
人類が破滅しかねないと
いうことです

00:03:35.780 --> 00:03:38.320
人類がアリのような存在になると
考えるといいです

00:03:38.350 --> 00:03:40.076
我々は アリが
嫌いなわけではなく

00:03:40.100 --> 00:03:42.156
わざわざ 潰しに
行きはしません

00:03:42.180 --> 00:03:44.556
潰してしまわないように
気を遣いさえして

00:03:44.580 --> 00:03:46.596
アリを避けて歩きます

00:03:46.620 --> 00:03:48.290
でもアリの存在が

00:03:48.290 --> 00:03:51.190
我々の目的と衝突した時—

00:03:51.190 --> 00:03:53.837
たとえばこんな建造物を
作るという場合には

00:03:53.837 --> 00:03:56.271
良心の呵責なしに
アリを全滅させてしまいます

00:03:56.300 --> 00:03:58.050
私が懸念するのは

00:03:58.050 --> 00:04:00.340
意識的にせよ無意識にせよ

00:04:00.340 --> 00:04:05.320
人類をそのように軽く扱う機械を
我々がいつか作ってしまうことです

00:04:05.500 --> 00:04:09.000
考えすぎだと 多くの方は
思っていることでしょう

00:04:09.180 --> 00:04:12.450
超知的AIの実現可能性を
疑問視し

00:04:12.450 --> 00:04:17.070
ましてや不可避などとは
思わない人もいるでしょう

00:04:17.070 --> 00:04:21.070
でも それなら これから挙げる仮定に
間違いを見つけてほしいものです

00:04:21.070 --> 00:04:22.726
仮定は３つだけです

00:04:23.620 --> 00:04:28.619
知性とは物質的なシステムにおける
情報処理である

00:04:29.140 --> 00:04:31.609
実際これは
仮定以上のものです

00:04:31.609 --> 00:04:35.236
狭い意味での知性なら
もう機械で作り出されており

00:04:35.260 --> 00:04:37.160
そのような機械はすでに

00:04:37.160 --> 00:04:40.580
人間を超える水準に
達しています

00:04:40.600 --> 00:04:42.900
そして様々な分野にわたって

00:04:42.900 --> 00:04:45.940
柔軟に思考できる能力である
「汎用知能」は

00:04:45.940 --> 00:04:49.556
物質で構成可能なことを
我々は知っています

00:04:49.580 --> 00:04:52.716
なぜなら人間の脳が
そうだからです

00:04:52.740 --> 00:04:56.676
ここにあるのは
ただの原子の塊です

00:04:56.700 --> 00:05:00.780
より知的な振る舞いをする

00:05:00.780 --> 00:05:03.790
原子でできたシステムの構築を
続けていくなら

00:05:03.790 --> 00:05:06.656
何かで中断させられない限りは

00:05:06.656 --> 00:05:08.550
いつか汎用知能を

00:05:08.550 --> 00:05:11.256
機械で作り出すことに
なるでしょう

00:05:11.256 --> 00:05:14.876
重要なのは ここで進歩のスピードは
問題ではないことです

00:05:14.900 --> 00:05:17.990
どんな進歩であれ
そこに至るのに十分だからです

00:05:17.990 --> 00:05:21.940
ムーアの法則の持続も
指数爆発的な進歩も必要ありません

00:05:21.940 --> 00:05:24.640
ただ続けていくだけで
いいのです

00:05:25.210 --> 00:05:28.710
第２の仮定は 
人間が進み続けていくこと

00:05:28.740 --> 00:05:32.280
我々が知的な機械の
改良を続けていくことです

00:05:32.820 --> 00:05:37.196
知性の価値を考えてください

00:05:37.220 --> 00:05:40.620
知性は 我々が価値を置くもの
すべての源泉

00:05:40.620 --> 00:05:43.646
価値を置くものを守るために
必要なものです

00:05:43.646 --> 00:05:45.836
我々が持つ
最も貴重なリソースです

00:05:45.860 --> 00:05:47.846
我々は それを必要としています

00:05:47.846 --> 00:05:50.756
我々は解決すべき問題を
抱えています

00:05:50.780 --> 00:05:54.660
アルツハイマーやガンのような病気を
治したいと思っています

00:05:54.690 --> 00:05:58.660
経済の仕組みを理解したい
気象科学をもっと良くしたいと思っています

00:05:58.660 --> 00:06:01.366
だから 可能であれば
やるはずです

00:06:01.366 --> 00:06:05.026
列車はすでに駅を出発していて
ブレーキはありません

00:06:05.700 --> 00:06:10.340
３番目は 人間は知性の
頂点にはおらず

00:06:10.340 --> 00:06:13.420
その近くにすら
いないということ

00:06:13.460 --> 00:06:15.220
これは重要な洞察であり

00:06:15.220 --> 00:06:17.760
我々の状況を
危うくするものです

00:06:17.760 --> 00:06:22.390
これはまた リスクについての我々の勘が
当てにならない理由でもあります

00:06:22.940 --> 00:06:26.120
史上最も頭の良い人間について
考えてみましょう

00:06:26.380 --> 00:06:29.876
ほぼ必ず上がってくる名前に
フォン・ノイマンがあります

00:06:29.900 --> 00:06:31.740
ノイマンの周囲には

00:06:31.740 --> 00:06:34.880
当時の 最も優れた数学者や
物理学者がいたわけですが

00:06:34.880 --> 00:06:39.336
人々がノイマンから受けた印象については
いろいろ記述されています

00:06:39.336 --> 00:06:43.076
ノイマンにまつわる話で 半分でも
正しいものが 半分だけだったとしても

00:06:43.100 --> 00:06:44.156
疑いなく彼は

00:06:44.156 --> 00:06:46.916
これまでに生きた最も頭の良い
人間の１人です

00:06:46.916 --> 00:06:49.740
知能の分布を考えてみましょう

00:06:50.060 --> 00:06:52.389
ここにフォン・ノイマンがいて

00:06:53.170 --> 00:06:55.604
この辺に私や皆さんがいて

00:06:55.810 --> 00:06:57.526
この辺にニワトリがいます

00:06:57.526 --> 00:06:59.120
(笑)

00:06:59.120 --> 00:07:00.876
失敬 ニワトリはこの辺です

00:07:00.876 --> 00:07:01.716
(笑)

00:07:01.740 --> 00:07:05.476
元々滅入る話をさらに滅入るものに
することもないでしょう

00:07:05.500 --> 00:07:07.100
(笑)

00:07:08.159 --> 00:07:11.636
知能の分布には
今の我々に想像できるよりも

00:07:11.660 --> 00:07:15.120
はるかに大きな広がりが
あることでしょう

00:07:15.700 --> 00:07:18.916
そして我々が
人間よりも知的な機械を作ったなら

00:07:18.940 --> 00:07:21.236
それはこの知能の地平を

00:07:21.260 --> 00:07:23.756
我々の想像が
及ばないくらい遠くまで

00:07:23.756 --> 00:07:26.060
進んでいくことでしょう

00:07:26.820 --> 00:07:31.156
重要なのは スピードの点だけを取っても
それは確かだということです

00:07:31.180 --> 00:07:33.670
スタンフォードやMITの

00:07:33.670 --> 00:07:37.120
平均的な研究者並みの
知能を持つ

00:07:37.120 --> 00:07:41.900
超知的AIができたとしましょう

00:07:41.900 --> 00:07:44.380
電子回路は
生化学的回路より

00:07:44.380 --> 00:07:46.476
100万倍も高速に稼働します

00:07:46.476 --> 00:07:49.060
その機械は
それを作り出した頭脳よりも

00:07:49.060 --> 00:07:51.290
100万倍速く
働くということです

00:07:51.290 --> 00:07:53.356
だから１週間稼働させておくと

00:07:53.356 --> 00:07:58.100
人間並みの知的作業を
２万年分こなすことになります

00:07:58.100 --> 00:08:00.320
毎週毎週です

00:08:01.460 --> 00:08:03.740
そのような進歩をする頭脳は

00:08:03.740 --> 00:08:07.600
抑えるどころか理解することすら
おぼつかないでしょう

00:08:08.270 --> 00:08:11.436
正直言って 心配なことが
他にもあります

00:08:11.436 --> 00:08:15.796
それなら最良のシナリオは
どんなものでしょう？

00:08:15.820 --> 00:08:19.850
安全性について心配のない
超知的AIのデザインを

00:08:19.850 --> 00:08:21.396
思い付いたとします

00:08:21.420 --> 00:08:24.676
完璧なデザインを
最初に手にすることができた

00:08:24.700 --> 00:08:26.830
神のお告げで
与えられたかのように

00:08:26.830 --> 00:08:29.266
まったく意図したとおりに
振る舞います

00:08:29.266 --> 00:08:33.490
その機械は人間の労働を不要にする
完璧な装置になります

00:08:33.500 --> 00:08:35.929
どんな肉体労働もこなす
機械を作るための

00:08:35.953 --> 00:08:37.716
機械をデザインでき

00:08:37.740 --> 00:08:39.196
太陽光で稼働し

00:08:39.220 --> 00:08:41.916
原材料費くらいしか
かかりません

00:08:41.940 --> 00:08:45.196
人間は苦役から
解放されます

00:08:45.220 --> 00:08:48.490
知的な仕事の多くすら
不要になります

00:08:48.740 --> 00:08:52.226
そのような状況で 我々のようなサルは
何をするのでしょう？

00:08:52.226 --> 00:08:56.850
気ままにフリスビーしたり
マッサージし合ったりでしょうか

00:08:57.440 --> 00:09:00.610
それにLSDと
奇抜な服を加えるなら

00:09:00.610 --> 00:09:03.406
世界中がバーニングマンの
お祭りのようになるかもしれません

00:09:03.406 --> 00:09:04.900
(笑)

00:09:06.140 --> 00:09:08.760
それも結構かもしれませんが

00:09:09.030 --> 00:09:11.696
現在の政治経済状況の元で

00:09:11.696 --> 00:09:14.346
どんなことが起きるだろうか
考えてみてください

00:09:14.346 --> 00:09:16.956
経済格差や失業が

00:09:16.956 --> 00:09:19.530
かつてない規模で生じる

00:09:19.530 --> 00:09:22.356
可能性が高いでしょう

00:09:22.380 --> 00:09:24.290
この新しい富を即座に

00:09:24.290 --> 00:09:27.310
全人類に進んで
提供する気がなければ

00:09:27.340 --> 00:09:31.076
一握りの兆万長者が
ビジネス誌の表紙を飾る一方で

00:09:31.100 --> 00:09:34.090
残りの人々は
飢えることになるでしょう

00:09:34.140 --> 00:09:36.330
シリコンバレーの会社が

00:09:36.330 --> 00:09:39.656
超知的AIを展開しようと
していると聞いたら

00:09:39.656 --> 00:09:41.836
ロシアや中国は
どうするでしょう？

00:09:41.860 --> 00:09:44.310
その機械は 地上戦であれ
サイバー戦であれ

00:09:44.310 --> 00:09:46.240
かつてない力でもって

00:09:46.240 --> 00:09:49.600
戦争を遂行する
能力があります

00:09:49.750 --> 00:09:52.106
勝者がすべてを
手にするシナリオです

00:09:52.106 --> 00:09:54.560
競争で半年
先んじることで

00:09:54.560 --> 00:09:57.140
少なくとも50万年分

00:09:57.140 --> 00:09:59.276
先を行くことが
できるのです

00:09:59.300 --> 00:10:03.960
そのような技術革新の
噂だけでも

00:10:03.960 --> 00:10:06.436
世界中が大騒ぎに
なるでしょう

00:10:06.460 --> 00:10:09.140
今の時点で私が

00:10:09.140 --> 00:10:12.156
いちばんゾッとするのは

00:10:12.180 --> 00:10:15.680
AI研究者が
安心させようとして

00:10:15.680 --> 00:10:18.740
言うたぐいのことです

00:10:18.770 --> 00:10:22.276
心配すべきでない理由として
よく言われるのは 時間です

00:10:22.300 --> 00:10:24.356
そんなのは ずっと先の話で

00:10:24.380 --> 00:10:27.230
50年とか100年も先の話だと

00:10:27.540 --> 00:10:28.796
ある研究者は

00:10:28.820 --> 00:10:30.606
「AIの安全性を懸念するのは

00:10:30.606 --> 00:10:33.346
火星の人口過剰を心配するようなものだ」と
言っています

00:10:33.346 --> 00:10:36.846
これは「あなたの小さな頭を
そんなことで悩ませないで」というやつの

00:10:36.846 --> 00:10:38.056
シリコンバレー版です

00:10:38.056 --> 00:10:39.220
(笑)

00:10:39.220 --> 00:10:41.596
誰も気付いていない
ように見えるのは

00:10:41.596 --> 00:10:43.876
時間的スケールは

00:10:43.900 --> 00:10:46.476
この際 無関係だと
いうことです

00:10:46.500 --> 00:10:49.756
知性の実体が
情報処理にすぎず

00:10:49.780 --> 00:10:52.360
人類が 機械の改良を
続けていくなら

00:10:52.360 --> 00:10:56.250
いずれ何らかの形の
超知性を生み出すことになるでしょう

00:10:56.250 --> 00:10:59.700
それを安全に行える
条件を作り出すのに

00:10:59.700 --> 00:11:03.160
どれくらい時間がかかるのか
我々には 見当も付きません

00:11:03.820 --> 00:11:05.316
もう一度言いましょう

00:11:05.340 --> 00:11:09.156
それを安全に行える
条件を作り出すのに

00:11:09.180 --> 00:11:12.620
どれくらい時間がかかるのか
我々には 見当も付きません

00:11:12.740 --> 00:11:16.130
今の50年というのは
かつての50年とは違っています

00:11:16.130 --> 00:11:18.806
これは 50年を月の数で
表したものです

00:11:18.806 --> 00:11:21.210
これはiPhoneが
登場してからの時間

00:11:21.210 --> 00:11:24.540
これは『シンプソンズ』の放送が
始まってからの時間

00:11:24.540 --> 00:11:27.316
人類最大の難問に
取り組む時間としては

00:11:27.316 --> 00:11:30.750
50年というのは
そんなに 長いものではありません

00:11:31.460 --> 00:11:35.120
繰り返しになりますが
必ずやってくると思われるものに対して

00:11:35.120 --> 00:11:38.406
我々は感情的に適切な反応を
できずにいるようです

00:11:38.406 --> 00:11:42.406
コンピューター科学者のスチュワート・
ラッセルが うまい例えをしています

00:11:42.406 --> 00:11:44.480
異星の文明から

00:11:44.480 --> 00:11:48.690
こんなメッセージを受け取ったと
想像してください

00:11:48.690 --> 00:11:50.396
「地球の諸君

00:11:50.420 --> 00:11:53.500
あと50年で
我々はそちらに着く

00:11:53.520 --> 00:11:55.130
準備されたし」

00:11:55.130 --> 00:11:59.606
我々は 彼らの母船が着陸する時を
ただ指折り数えて待つのでしょうか？

00:11:59.606 --> 00:12:03.530
もう少し 切迫感を抱くのでは
ないかと思います

00:12:04.290 --> 00:12:06.456
心配無用だとされる
別の理由は

00:12:06.456 --> 00:12:09.396
そういった機械は
人間の延長であり

00:12:09.420 --> 00:12:12.036
人間と同じ価値観を
持つはずだというものです

00:12:12.060 --> 00:12:13.876
我々の脳に接続され

00:12:13.900 --> 00:12:16.880
人間はその辺縁系のような
存在になるのだと

00:12:16.890 --> 00:12:18.856
ちょっと考えてほしいんですが

00:12:18.856 --> 00:12:20.210
最も安全で

00:12:20.210 --> 00:12:22.916
唯一分別のあるやり方として
勧められているのは

00:12:22.940 --> 00:12:26.450
その技術を直接自分の脳に
埋め込むことだと言うのです

00:12:26.450 --> 00:12:30.146
それは実際に 最も安全で
唯一分別のあるやり方なのかもしれませんが

00:12:30.146 --> 00:12:33.086
テクノロジーに対する安全性の懸念は 
ふつうであれば—

00:12:33.086 --> 00:12:36.856
頭に埋め込む前に解消されている
べきではないでしょうか

00:12:36.856 --> 00:12:38.596
(笑)

00:12:38.620 --> 00:12:43.956
より根深い問題点は
超知的AIを作ること自体は

00:12:43.980 --> 00:12:45.716
超知的AIを作り

00:12:45.740 --> 00:12:47.916
かつ それを人間の脳に
統合するための

00:12:47.916 --> 00:12:50.386
神経科学を
完成させることよりは

00:12:50.386 --> 00:12:52.610
簡単だろうことです

00:12:52.620 --> 00:12:55.796
これに取り組む国や企業は

00:12:55.820 --> 00:12:59.140
他の国や企業と競争を
していると認識しており

00:12:59.140 --> 00:13:02.660
勝者が全世界を
手に入れられるのだとすれば—

00:13:02.660 --> 00:13:05.406
次の瞬間に破滅させて
いたら別ですが—

00:13:05.406 --> 00:13:06.810
そのような状況においては

00:13:06.810 --> 00:13:10.270
何にせよ簡単な方が
先に成し遂げられることでしょう

00:13:10.290 --> 00:13:13.100
あいにく 私はこの問題への答えを
持ち合わせてはおらず

00:13:13.100 --> 00:13:15.780
ただもっと考えてみるように
お勧めするだけです

00:13:15.780 --> 00:13:18.416
人工知能のための
「マンハッタン計画」のようなものが

00:13:18.416 --> 00:13:20.300
必要なのでは
ないかと思います

00:13:20.300 --> 00:13:23.006
作るためではありません
それは不可避でしょうから

00:13:23.006 --> 00:13:26.090
軍拡競争のようになるのを
いかに避けられるか探り

00:13:26.090 --> 00:13:30.076
人類の利益にかなうように
それが作られるようにするためです

00:13:30.076 --> 00:13:31.590
超知的AIが

00:13:31.590 --> 00:13:34.396
自身を変えていけることを
考えると

00:13:34.420 --> 00:13:39.010
それを正しく作れるチャンスは
１度しかないでしょう

00:13:39.010 --> 00:13:41.656
しかもその上で
それを正しくやるための

00:13:41.656 --> 00:13:45.130
政治的・経済的な帰結を
受け入れる必要があります

00:13:45.450 --> 00:13:48.726
知性の源は情報処理だと—

00:13:48.726 --> 00:13:52.390
知性の基礎をなしているのは

00:13:52.420 --> 00:13:58.010
ある種の計算システムである
ということを認め

00:13:58.180 --> 00:14:02.960
人類がそういうシステムを継続的に
改良していくだろうことを認め

00:14:03.100 --> 00:14:06.460
そして認知の地平は
我々に今分かっているよりも

00:14:06.460 --> 00:14:09.770
はるか遠くまで
続いているであろうことを認めるとき

00:14:09.770 --> 00:14:12.646
我々はある種の神を
作ろうとしていることを

00:14:12.646 --> 00:14:14.900
認めないわけには
いかないでしょう

00:14:14.910 --> 00:14:17.986
その神が一緒にやっていける相手か
確認したいなら

00:14:17.986 --> 00:14:19.673
今がその時です

00:14:19.940 --> 00:14:21.476
ありがとうございました

00:14:21.500 --> 00:14:26.593
(拍手)

