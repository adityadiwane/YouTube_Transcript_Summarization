WEBVTT
Kind: captions
Language: hr

00:00:00.000 --> 00:00:07.000
Prevoditelj: Stjepan Mateljan
Recezent: Ivan Stamenković

00:00:12.820 --> 00:00:15.036
Pričat ću vam
o neuspjehu intuicije

00:00:15.060 --> 00:00:16.660
koji pogađa brojne među nama.

00:00:17.300 --> 00:00:20.340
To je zapravo neuspjeh
uočavanja određene vrste opasnosti.

00:00:21.180 --> 00:00:22.916
Opisat ću scenarij

00:00:22.940 --> 00:00:26.196
za koji smatram kako je jednako zastrašujuć

00:00:26.220 --> 00:00:27.980
koliko i vjerojatan da se dogodi,

00:00:28.660 --> 00:00:30.316
a to nije dobra kombinacija,

00:00:30.340 --> 00:00:31.876
kako se ispostavlja.

00:00:31.900 --> 00:00:34.356
A ipak umjesto da budete prestrašeni,
većina će vas osjećati

00:00:34.380 --> 00:00:36.460
kako je ono o čemu govorim
nekako fora.

00:00:37.020 --> 00:00:39.996
Opisat ću kako bi nas
dobici koje stvaramo

00:00:40.020 --> 00:00:41.796
u umjetnoj inteligenciji (dalje: AI)

00:00:41.820 --> 00:00:43.596
na kraju mogli uništiti.

00:00:43.620 --> 00:00:47.076
I zapravo, mislim ako je jako teško
vidjeti kako nas oni neće uništiti

00:00:47.100 --> 00:00:48.780
ili nas nadahnuti da se sami uništimo.

00:00:49.220 --> 00:00:51.076
Pa ipak ako ste imalo poput mene,

00:00:51.100 --> 00:00:53.756
naći ćete kako je zabavno
razmišljati o ovim stvarima.

00:00:53.780 --> 00:00:57.156
A taj je odgovor dio problema.

00:00:57.180 --> 00:00:58.900
Ok? Taj bi vas odgovor trebao brinuti.

00:00:59.740 --> 00:01:02.396
A ako bih vas uvjeravao
ovim govorom

00:01:02.420 --> 00:01:05.836
kako ćemo vjerojatno
doživjeti globalnu glad,

00:01:05.860 --> 00:01:08.916
bilo zbog klimatskih promjena
ili neke druge katastrofe,

00:01:08.940 --> 00:01:12.356
te kako će vaši unuci,
ili njihovi unuci,

00:01:12.380 --> 00:01:14.180
vrlo vjerojatno živjeti ovako,

00:01:15.020 --> 00:01:16.220
ne biste mislili:

00:01:17.260 --> 00:01:18.596
"Zanimljivo.

00:01:18.620 --> 00:01:19.820
Sviđa mi se ovaj TED govor."

00:01:21.020 --> 00:01:22.540
Glad nije zabavna.

00:01:23.620 --> 00:01:26.996
Smrt znanstvenom fantastikom,
s druge strane, jest zabavna,

00:01:27.020 --> 00:01:30.996
a jedna od stvari koje me najviše brinu
oko razvoja AI u ovom trenutku

00:01:31.020 --> 00:01:35.116
je kako se činimo nesposobnim
stvoriti prikladan emotivni odgovor

00:01:35.140 --> 00:01:36.956
na opasnosti pred kojima se nalazimo.

00:01:36.980 --> 00:01:40.180
Ja sam nesposoban stvorti taj odgovor,
a držim ovaj govor.

00:01:41.940 --> 00:01:44.636
Ovo je kao da stojimo
pred dvama vratima.

00:01:44.660 --> 00:01:45.916
Iza vrata broj jedan,

00:01:45.940 --> 00:01:49.236
prestanemo stvarati napredak
u izgradnji inteligentnih strojeva.

00:01:49.260 --> 00:01:53.276
Hardver se i softver naših računala
samo iz nekog razloga prestane poboljšavati.

00:01:53.300 --> 00:01:56.300
Sad uzmite trenutak,
razmislite zašto bi se to moglo dogoditi.

00:01:56.900 --> 00:02:00.556
MIslim, uzevši kako su vrijedne
inteligencija i automatizacija,

00:02:00.580 --> 00:02:04.100
nastavit ćemo unaprijeđivati tehnologiju
ako smo to ikako sposobni.

00:02:05.020 --> 00:02:06.687
Što bi nas moglo zaustaviti u tome?

00:02:07.620 --> 00:02:09.420
Potpuni nuklearni rat?

00:02:10.820 --> 00:02:12.380
Globalna pandemija?

00:02:14.140 --> 00:02:15.460
Udar asteroida?

00:02:17.460 --> 00:02:20.036
Da Justin Bieber postane
predsjednik Sjedinjenih država?

00:02:20.060 --> 00:02:22.340
(Smijeh)

00:02:24.580 --> 00:02:28.500
Poanta je, nešto bi moralo
uništiti civilizaciju kakvu znamo.

00:02:29.180 --> 00:02:33.476
Morate zamisliti
kako bi loše to moralo biti

00:02:33.500 --> 00:02:36.836
da nas spriječi od pravljenja
poboljšanja u našoj tehnologiji

00:02:36.860 --> 00:02:38.076
trajno,

00:02:38.100 --> 00:02:40.116
generaciju za generacijom.

00:02:40.140 --> 00:02:42.276
Skoro po definiciji,
to je najgora stvar

00:02:42.300 --> 00:02:44.316
koja se ikad dogodila u ljudskoj povijesti.

00:02:44.340 --> 00:02:45.636
Dakle jedina alternativa,

00:02:45.660 --> 00:02:47.996
a to je ono što leži
iza vrata broj dva,

00:02:48.020 --> 00:02:51.156
je da nastavimo poboljšavati
naše inteligentne strojeve

00:02:51.180 --> 00:02:52.780
godinu za godinom za godinom.

00:02:53.540 --> 00:02:57.180
U nekom trenutku, sagradit ćemo
strojeve koji su pametniji od nas,

00:02:57.900 --> 00:03:00.516
a jednom kad imamo strojeve
koji su pametniji od nas,

00:03:00.540 --> 00:03:02.516
oni će početi poboljšavati sami sebe.

00:03:02.540 --> 00:03:05.276
A tada risikiramo što je
matematičar I.J. Good nazvao

00:03:05.300 --> 00:03:07.076
"eksplozijom inteligencije,"

00:03:07.100 --> 00:03:09.100
da bi nam se proces mogao izmaknuti.

00:03:09.940 --> 00:03:12.756
Sad, to je često karikirano,
kako imam ovdje,

00:03:12.780 --> 00:03:15.996
kao strah da će
vojske zloćudnih robota

00:03:16.020 --> 00:03:17.276
udariti na nas.

00:03:17.300 --> 00:03:19.996
Ali to nije najvjerojatniji scenarij.

00:03:20.020 --> 00:03:24.876
Nije da će naši strojevi
spontano postati zloćudnim.

00:03:24.900 --> 00:03:27.516
Zapravo je briga
kako ćemo izraditi strojeve

00:03:27.540 --> 00:03:29.596
koji su toliko
sposobniji od nas

00:03:29.620 --> 00:03:33.396
da bi nas najmanje razmimoilaženje
između njihovih i naših ciljeva

00:03:33.420 --> 00:03:34.620
moglo uništiti.

00:03:35.780 --> 00:03:37.860
Samo pomislite kako se odnosimo prema mravima.

00:03:38.420 --> 00:03:40.076
Ne mrzimo ih.

00:03:40.100 --> 00:03:42.156
Ne silazimo sa svog puta
kako bismo im naudili.

00:03:42.180 --> 00:03:44.556
Zapravo, ponekad
se baš i trudimo kako im ne bismo naudili.

00:03:44.580 --> 00:03:46.596
Prekoračimo ih na pločniku.

00:03:46.620 --> 00:03:48.756
Ali kad god sje njihova prisutnost

00:03:48.780 --> 00:03:51.276
ozbiljno sukobi s nekim od naših ciljeva,

00:03:51.300 --> 00:03:53.777
recimo kad dižemo
građevinu poput ove,

00:03:53.801 --> 00:03:55.761
uništavamo ih bez pol muke.

00:03:56.300 --> 00:03:59.236
Briga je kako ćemo
jednog dana sagraditi strojeve

00:03:59.260 --> 00:04:01.996
koji, bili oni svjesni ili ne,

00:04:02.020 --> 00:04:04.020
bi se mogli odnositi spram nas
sa sličnim nehajem.

00:04:05.580 --> 00:04:08.340
Sad, pretpostavljam kako
vam se ovo doima nategnutim.

00:04:09.180 --> 00:04:15.516
Kladim se kako vas ima koji sumnjate
kako je superinteligentan AI moguć,

00:04:15.540 --> 00:04:17.196
a još manje neizbježan.

00:04:17.220 --> 00:04:20.840
Ali onda morate naći nešto krivo
s nekom od slijedeći pretpostavki.

00:04:20.864 --> 00:04:22.436
A samo ih je tri.

00:04:23.620 --> 00:04:28.339
Inteligencija je stvar obrade
informacija u fizičkim sustavima.

00:04:29.140 --> 00:04:31.755
Zapravo, ovo je 
malo više od pretpostavke.

00:04:31.779 --> 00:04:35.236
Već smo ugradili
usku inteligenciju u naše strojeve,

00:04:35.260 --> 00:04:37.276
i puno tih strojeva imaju izvedbu

00:04:37.300 --> 00:04:39.940
na razini nadljudske 
inteligencije već sada.

00:04:40.660 --> 00:04:43.236
I znamo kako puka materija

00:04:43.260 --> 00:04:45.876
može omogućiti nastanak 
onog što nazivamo "opća inteligencija,"

00:04:45.900 --> 00:04:49.556
sposobnost razmišljanja
rastezljivog na višestruka područja,

00:04:49.580 --> 00:04:52.716
jer to mogu naši mozgovi. U redu?

00:04:52.740 --> 00:04:56.676
Mislim, ovdje su samo atomi,

00:04:56.700 --> 00:05:01.196
i sve dok nastavimo
graditi sustave atoma

00:05:01.220 --> 00:05:03.916
koji pokazuju sve više
inteligentnog ponašanja

00:05:03.940 --> 00:05:06.476
s vremenom ćemo,
osim ako ne budemo ometeni,

00:05:06.500 --> 00:05:09.876
s vremenom ćemo
ugraditi opću inteligenciju

00:05:09.900 --> 00:05:11.196
u naše strojeve.

00:05:11.220 --> 00:05:14.876
Ključno je shvatiti
kako stopa napretka nije bitna,

00:05:14.900 --> 00:05:18.076
jer će nas bilo kakav
napredak dovesti u krajnju zonu.

00:05:18.100 --> 00:05:21.876
Ne trebamo nastavak Mooreovog zakona.
Ne trebamo eksponencijalni rast.

00:05:21.900 --> 00:05:23.500
Samo trebamo nastaviti napredovati.

00:05:25.300 --> 00:05:28.220
Druga je pretpostavka
kako ćemo nastaviti napredovati.

00:05:28.820 --> 00:05:31.580
Nastavit ćemo unapređivati
naše inteligentne strojeve.

00:05:32.820 --> 00:05:37.196
A uzevši vrijednost inteligencije --

00:05:37.220 --> 00:05:40.756
Mislim, inteligencija je ili
izvor svega što držimo vrijednim

00:05:40.780 --> 00:05:43.556
ili je trebamo kako bi zaštitili
sve što smatramo vrijednim.

00:05:43.580 --> 00:05:45.836
Ona je naš najvredniji resurs.

00:05:45.860 --> 00:05:47.396
Pa želimo to raditi.

00:05:47.420 --> 00:05:50.756
Imamo probleme
koje očajnički trebamo riješiti.

00:05:50.780 --> 00:05:53.980
Želimo liječiti bolesti
poput Alzheimera ili raka.

00:05:54.780 --> 00:05:58.716
Želimo razumjeti ekonomske sustave.
Želimo unaprijediti naše klimatske znanosti.

00:05:58.740 --> 00:06:00.996
Pa ćemo to učiniti, ako možemo.

00:06:01.020 --> 00:06:04.306
Vlak je već napustio postaju
a nema kočnice za povući.

00:06:05.700 --> 00:06:11.156
Konačno, ne stojimo
na vrhuncu inteligencije,

00:06:11.180 --> 00:06:12.980
ili igdje blizu njega, vjerojatno.

00:06:13.460 --> 00:06:15.356
A to je zbilja ključan uvid.

00:06:15.380 --> 00:06:17.796
To je ono što čini
našu situaciju tako neizvjesnom,

00:06:17.820 --> 00:06:21.860
i to je ono što čini
naše intuicije o riziku tako nepouzdanim.

00:06:22.940 --> 00:06:25.660
Sad, samo razmotrite najpametniju osobu
koja je ikad živjela.

00:06:26.460 --> 00:06:29.876
U gotovo svačijem užem izboru
ovdje je John von Neumann.

00:06:29.900 --> 00:06:33.236
Mislim, utisak koji je von Neumann
ostavljao na ljude oko sebe,

00:06:33.260 --> 00:06:37.316
a ti uključuju najveće matematičare
i fizičare njegovog doba,

00:06:37.340 --> 00:06:39.276
je dobro dokumentirana.

00:06:39.300 --> 00:06:43.076
Ako je samo pola priča
o njemu na pola istinito,

00:06:43.100 --> 00:06:44.316
nema pitanja

00:06:44.340 --> 00:06:46.796
on je jedna od najpametnijih
osoba koje su ikad živjele.

00:06:46.820 --> 00:06:49.340
Pa razmotrite spektar inteligencije.

00:06:50.140 --> 00:06:51.569
Ovdje imamo John von Neumanna.

00:06:53.380 --> 00:06:54.714
A onda imamo vas i mene.

00:06:55.940 --> 00:06:57.236
A onda imamo piceka.

00:06:57.260 --> 00:06:59.196
(Smijeh)

00:06:59.220 --> 00:07:00.436
Pardon, piceka.

00:07:00.460 --> 00:07:01.716
(Smijeh)

00:07:01.740 --> 00:07:05.476
Nema razloga da učinim ovaj govor
depresivnijem nego što mora biti.

00:07:05.500 --> 00:07:07.100
(Smijeh)

00:07:08.159 --> 00:07:11.636
Izgleda premoćno vjerojatno,
međutim, kako se spektar inteligencije

00:07:11.660 --> 00:07:14.780
proteže puno dalje
nego što trenutno poimamo,

00:07:15.700 --> 00:07:18.916
i ako sagradimo strojeve
koji su inteligentniji od nas,

00:07:18.940 --> 00:07:21.236
oni će vrlo vjerojatno
istražiti ovaj spektar

00:07:21.260 --> 00:07:23.116
na načine koje ne možemo zamisliti,

00:07:23.140 --> 00:07:25.660
te nas nadmašiti na načine
koje ne možemo zamisliti.

00:07:26.820 --> 00:07:31.156
A bitno je prepoznati kako je
to istinito već po samom svojstvu brzine.

00:07:31.180 --> 00:07:36.236
U redu? Zamislite kako smo
upravo sagradili superinteligentan AI

00:07:36.260 --> 00:07:39.716
koji nije pametniji od
vašeg prosječnog tima istraživača

00:07:39.740 --> 00:07:42.036
na Stanfordu ili MIT-u.

00:07:42.060 --> 00:07:45.036
Dobro, elektronički krugovi
rade oko milijun puta brže

00:07:45.060 --> 00:07:46.316
od onih biokemijskih,

00:07:46.340 --> 00:07:49.476
pa bi ovaj stroj trebao
razmišljati oko milijun puta brže

00:07:49.500 --> 00:07:51.316
od umova koji su ga sagradili.

00:07:51.340 --> 00:07:52.996
Pa ga pustite raditi tjedan dana,

00:07:53.020 --> 00:07:57.580
a on će izvesti 20.000 godina
intelektualnog rada ljudske razine,

00:07:58.220 --> 00:08:00.180
tjedan za tjednom za tjednom.

00:08:01.460 --> 00:08:04.556
Kako bi uopće mogli razumjeti,
puno manje ograničiti,

00:08:04.580 --> 00:08:06.860
um koji stvara ovu vrst napretka?

00:08:08.660 --> 00:08:10.796
Druga stvar koja je
zabrinjavajuća, iskreno,

00:08:10.820 --> 00:08:15.796
je kako, zamislite najbolji scenarij.

00:08:15.820 --> 00:08:19.996
dakle zamislite kako pogodimo
dizajn superinteligentne AI

00:08:20.020 --> 00:08:21.396
koji nema sigurnosnih problema.

00:08:21.420 --> 00:08:24.676
Imamo savršen dizajn
iz prve.

00:08:24.700 --> 00:08:26.916
Kao da nam je dano proročanstvo

00:08:26.940 --> 00:08:28.956
koje se ponaša posve kako bi i trebalo.

00:08:28.980 --> 00:08:32.700
Dobro, taj bi stroj bio
savršena naprava za uštedu rada.

00:08:33.500 --> 00:08:35.929
Može dizajnirati stroj
koji može izraditi stroj

00:08:35.953 --> 00:08:37.716
koji može obavljati
svaki fizički rad,

00:08:37.740 --> 00:08:39.196
pogonjen sunčanom svjetlošću,

00:08:39.220 --> 00:08:41.916
manje ili više po cijeni
sirovog materijala.

00:08:41.940 --> 00:08:45.196
Dakle pričamo o
kraju ljudske tlake.

00:08:45.220 --> 00:08:48.020
Također pričamo o kraju
većine intelektualnog rada.

00:08:49.020 --> 00:08:52.076
Pa što bi majmuni poput nas
činili u takvim okolnostima?

00:08:52.100 --> 00:08:56.180
Ok, bili bi slobodni
igrati frizbi i masirati se međusobno.

00:08:57.660 --> 00:09:00.516
Dodajte nešto LSD i nešto
upitnih odluka oko odjeće,

00:09:00.540 --> 00:09:02.716
i cijeli bi svijet mogao
biti poput Burning Mana.

00:09:02.740 --> 00:09:04.380
(Smijeh)

00:09:06.140 --> 00:09:08.140
Sad, to može zvučati prilično dobro,

00:09:09.100 --> 00:09:11.476
ali pitajte se što bi se dogodilo

00:09:11.500 --> 00:09:14.236
pod našim trenutnim
ekonomskim i političkim poretkom?

00:09:14.260 --> 00:09:16.676
Čini se vjerojatnim
kako bismo svjedočili

00:09:16.700 --> 00:09:20.836
razini nejednakosti u bogatstvu
te nezaposlenosti

00:09:20.860 --> 00:09:22.356
kakvu nismo vidjeli nikad prije.

00:09:22.380 --> 00:09:24.996
Bez volje za trenutnim
stavljanjem tog novog bogatstva

00:09:25.020 --> 00:09:26.500
u službu svog čovječanstva,

00:09:27.460 --> 00:09:31.076
nekoliko bi trilijunaša moglo
resiti naslovnice naših poslovni časopisa

00:09:31.100 --> 00:09:33.540
dok bi ostatak svijeta
bio slobodan gladovati.

00:09:34.140 --> 00:09:36.436
A što bi Rusi
ili Kinezi učinili

00:09:36.460 --> 00:09:39.076
ako bi čuli kako se
neka kompanija u Silicijskoj dolini

00:09:39.100 --> 00:09:41.836
upravo sprema izbaciti
superinteligentnu AI?

00:09:41.860 --> 00:09:44.716
Taj bi stroj bio
sposoban voditi rat,

00:09:44.740 --> 00:09:46.956
bilo zemaljski ili cyber,

00:09:46.980 --> 00:09:48.660
sa neviđenom moći.

00:09:49.940 --> 00:09:51.796
Ovo je scenarij gdje pobjednik dobija sve.

00:09:51.820 --> 00:09:54.956
Biti šest mjeseci ispred
konkurencije ovdje

00:09:54.980 --> 00:09:57.756
je biti 500.000 godina ispred,

00:09:57.780 --> 00:09:59.276
minimalno.

00:09:59.300 --> 00:10:04.036
Pa se čini kako bi čak i puke glasine
o ovakvoj vrsti probitka

00:10:04.060 --> 00:10:06.436
mogle prouzročiti našu vrstu da poludi.

00:10:06.460 --> 00:10:09.356
Sad, jedna od najviše
zastrašujućih stvari,

00:10:09.380 --> 00:10:12.156
kako ja to vidim, u ovom trenutku,

00:10:12.180 --> 00:10:16.476
su stvari koje istraživači AI govore

00:10:16.500 --> 00:10:18.060
kada žele biti ohrabrujući.

00:10:18.820 --> 00:10:22.276
A najčešći razlog za bezbrižnost
koji navode je vrijeme.

00:10:22.300 --> 00:10:24.356
To je sve daleko,
ne znaš li?

00:10:24.380 --> 00:10:26.820
To je vjerojatno 50 ili 100 godina od nas.

00:10:27.540 --> 00:10:28.796
Jedan je istraživač rekao:

00:10:28.820 --> 00:10:30.396
"Brinuti se oko sigurnosti AI

00:10:30.420 --> 00:10:32.700
je poput brige
o prekomjernoj populaciji na Marsu."

00:10:33.936 --> 00:10:35.556
To je verzija iz Silicijske doline

00:10:35.580 --> 00:10:37.956
one: "nemoj brinuti
svoju lijepu malu glavu sa time."

00:10:37.980 --> 00:10:39.316
(Smijeh)

00:10:39.340 --> 00:10:41.236
Izgleda kako nitko ne primjećuje

00:10:41.260 --> 00:10:43.876
kako je oslanjanje na vremenski horizont

00:10:43.900 --> 00:10:46.476
potpuni non sequitur.

00:10:46.500 --> 00:10:49.756
Ako je inteligencija samo pitanje
procesiranja informacija,

00:10:49.780 --> 00:10:52.436
te ako nastavimo
unapređivati naše strojeve,

00:10:52.460 --> 00:10:55.340
stvorit ćemo
neki oblik superinteligencije.

00:10:56.140 --> 00:10:59.796
A nemamo pojma
koliko će nam dugo trebati

00:10:59.820 --> 00:11:02.220
stvoriti uvjete
kako bismo to učinili sigurno.

00:11:04.020 --> 00:11:05.316
Dajte da to kažem ponovo.

00:11:05.340 --> 00:11:09.156
Nemamo pojma
koliko će nam dugo trebati

00:11:09.180 --> 00:11:11.420
stvoriti uvjete
kako bismo to učinili sigurno.

00:11:12.740 --> 00:11:16.196
A ako niste primijetili,
50 godina nije što je nekad bilo.

00:11:16.220 --> 00:11:18.676
Ovo je 50 godina u mjesecima.

00:11:18.700 --> 00:11:20.540
Ovo je koliko dugo imamo iPhone.

00:11:21.260 --> 00:11:23.860
Ovo je koliko dugo su
"Simpsoni" na televiziji.

00:11:24.500 --> 00:11:26.876
Pedeset godina nije tako puno vremena

00:11:26.900 --> 00:11:30.060
za sučeliti se sa jednim od
najvećih izazova koje će naša vrsta ikad imati.

00:11:31.460 --> 00:11:35.476
Još jednom, izgleda da ne uspijevamo
imati prikladan emotivni odgovor

00:11:35.500 --> 00:11:38.196
za što imamo
svaki razlog vjerovati kako dolazi.

00:11:38.220 --> 00:11:42.196
Računalni znanstvenik Stuart Russell
ovdje pravi zgodnu analogiju.

00:11:42.220 --> 00:11:47.116
On kaže, zamislite kako smo primili
poruku od vanzemaljske civilizacije,

00:11:47.140 --> 00:11:48.836
koja piše:

00:11:48.860 --> 00:11:50.396
"Ljudi Zemlje,

00:11:50.420 --> 00:11:52.780
stići ćemo na vaš planet za 50 godina.

00:11:53.620 --> 00:11:55.196
Pripremite se."

00:11:55.220 --> 00:11:59.476
I sad mi samo brojimo
mjesece dok matični brod ne sleti?

00:11:59.500 --> 00:12:02.500
Osjećali bismo malo veću
hitnost nego što osjećamo.

00:12:04.500 --> 00:12:06.356
Još jedan razlog
što nam govore da ne brinemo

00:12:06.380 --> 00:12:09.396
je kako si ti strojevi
ne mogu pomoći da ne dijele naše vrijednosti

00:12:09.420 --> 00:12:12.036
jer će doslovno biti
produžeci nas samih.

00:12:12.060 --> 00:12:13.876
Bit će usađeni u naše mozgove,

00:12:13.900 --> 00:12:16.260
a mi ćemo u biti
postati njihov limbički sustav.

00:12:16.940 --> 00:12:18.356
sad uzmite trenutak pa razmislite

00:12:18.380 --> 00:12:21.556
kako je najsigurniji
i jedini razborit put naprijed,

00:12:21.580 --> 00:12:22.916
preporučeno,

00:12:22.940 --> 00:12:25.740
ugraditi tu tehnologiju
izravno u naše mozgove.

00:12:26.420 --> 00:12:29.796
Sad, to može zapravo biti najsigurniji
i jedini razborit put nparijed,

00:12:29.820 --> 00:12:32.876
ali obično sigurnosni problemi
oko tehnologije

00:12:32.900 --> 00:12:36.556
moraju biti manje više izglađeni
prije nego ju zabijete u vlastitu glavu.

00:12:36.580 --> 00:12:38.596
(Smijeh)

00:12:38.620 --> 00:12:43.956
Dublji je problem što se
izgradnja superinteligentne AI

00:12:43.980 --> 00:12:45.716
čini vjerojatno lakšim

00:12:45.740 --> 00:12:47.596
nego izgradnja superinteligentne AI

00:12:47.620 --> 00:12:49.396
i imanje zaokružene neuroznanosti

00:12:49.420 --> 00:12:52.100
koja nam omogućuje da ju glatko
integriramo s našim umovima.

00:12:52.620 --> 00:12:55.796
A uzevši kako će kompanije
i vlade koje se bave ovim poslom

00:12:55.820 --> 00:12:59.476
vjerojatno percipirati
kako su u utrci protiv svih ostalih,

00:12:59.500 --> 00:13:02.756
uzevši kako je osvojiti ovu utrku
isto što i osvojiti svijet,

00:13:02.780 --> 00:13:05.236
pod pretpostavkom kako ga
nećete uništiti u slijedećem trenutku,

00:13:05.260 --> 00:13:07.876
onda se čini vjerojatnim
kako će što god je lakše učiniti

00:13:07.900 --> 00:13:09.100
biti učinjeno prvo.

00:13:10.380 --> 00:13:13.236
Sad, na nesreću,
nemam rješenje za ovaj problem,

00:13:13.260 --> 00:13:15.876
osim preporučiti kako bi više nas
trebalo razmišljati o njemu.

00:13:15.900 --> 00:13:18.276
Mislim kako trebamo nešto
poput Manhattan Projecta

00:13:18.300 --> 00:13:20.316
po pitanju umjetne inteligencije.

00:13:20.340 --> 00:13:23.076
Ne kako bismo je stvorili, jer
ćemo to neizbježno napraviti,

00:13:23.100 --> 00:13:26.436
već kako bismo razumjeli
kako izbjeći utrku u naoružanju

00:13:26.460 --> 00:13:29.956
te kako ju sagraditi na način
koji se podudara s našim interesima.

00:13:29.980 --> 00:13:32.116
Kad pričate o 
superinteligentnoj AI

00:13:32.140 --> 00:13:34.396
koja može raditi preinake sebi samoj,

00:13:34.420 --> 00:13:39.036
čini se kako imamo samo jednu priliku
za posložiti ispravne početne uvjete,

00:13:39.060 --> 00:13:41.116
a čak i tada ćemo trebati upiti

00:13:41.140 --> 00:13:44.180
ekonomske i političke 
posljedice slaganja takvih uvjeta.

00:13:45.580 --> 00:13:47.636
Ali trenutak u kojem priznamo

00:13:47.660 --> 00:13:51.660
kako je obrada informacija
izvor inteligencije,

00:13:52.540 --> 00:13:57.340
kako je neki prikladan račualni sustav
ono na čemu se inteligencija temelji,

00:13:58.180 --> 00:14:01.940
te priznamo kako ćemo
trajno unaprijeđivati te sustave,

00:14:03.100 --> 00:14:07.556
te priznamo kako horizont
spoznaje vrlo vjerojatno daleko premašuje

00:14:07.580 --> 00:14:08.780
što trenutno znamo,

00:14:09.940 --> 00:14:11.156
onda moramo priznati

00:14:11.180 --> 00:14:13.820
kako smo u postupku
izgradnje neke vrste boga.

00:14:15.220 --> 00:14:16.796
Sad bi bilo dobro vrijeme

00:14:16.820 --> 00:14:18.773
osigurati da to bude bog
s kakvim možemo živjeti.

00:14:19.940 --> 00:14:21.476
Hvala Vam puno.

00:14:21.500 --> 00:14:26.593
(Pljesak)

