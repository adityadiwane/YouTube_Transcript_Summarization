WEBVTT
Kind: captions
Language: ko

00:00:00.000 --> 00:00:07.000
번역: Jihyeon J. Kim
검토: JY Kang

00:00:13.000 --> 00:00:15.250
직감이 틀리는 경우에 
대해서 얘기할까 합니다.

00:00:15.250 --> 00:00:17.480
우리가 흔히 겪는 일이죠.

00:00:17.480 --> 00:00:20.520
이것은 실제로 어떤 종류의 
위험을 감지하지 못하는 겁니다.

00:00:21.360 --> 00:00:23.096
가능한 시나리오를 말씀드리겠습니다.

00:00:23.120 --> 00:00:26.376
아주 무섭기도 하고

00:00:26.400 --> 00:00:28.160
일어날 가능성도 높죠.

00:00:28.840 --> 00:00:32.086
이 둘의 조합은 별로 
반길 상황은 아닐 겁니다.

00:00:32.086 --> 00:00:34.536
하지만 여러분 대부분은 
제가 말씀드릴 내용에 대해서

00:00:34.560 --> 00:00:36.640
무서워하기 보다는 
멋지다고 생각할 겁니다.

00:00:37.200 --> 00:00:41.936
제가 말씀드리고 싶은 것은
인공지능을 만들어서 얻게 될 이점이

00:00:41.936 --> 00:00:43.836
결국 우리를 파멸시킬 수도 
있다는 사실입니다.

00:00:43.836 --> 00:00:46.370
사실 인공지능이 우리를
파멸시키지 않을지

00:00:46.370 --> 00:00:48.960
우리 스스로 자멸하도록 
만들지는 전혀 알 수 없죠.

00:00:49.400 --> 00:00:51.256
하지만 여러분이 저와 같다면

00:00:51.280 --> 00:00:53.936
이 문제를 생각해 보는 것도
재미있다고 느끼실 겁니다.

00:00:53.960 --> 00:00:57.336
그런 반응도 문제입니다.

00:00:57.360 --> 00:00:59.290
걱정스런 반응이어야죠.

00:00:59.920 --> 00:01:02.576
이 강연에서 여러분들에게

00:01:02.600 --> 00:01:09.126
기후변화나 다른 재앙으로 인해
세계적 기근으로 고통받을 거라든지

00:01:09.126 --> 00:01:12.536
여러분의 손자나 그들의 손자들이

00:01:12.560 --> 00:01:14.580
이렇게 살게 될 거라고 설명하면

00:01:15.200 --> 00:01:16.700
이런 생각은 안드시겠죠.

00:01:17.440 --> 00:01:18.776
"재미있군.

00:01:18.800 --> 00:01:20.000
이 강연 맘에 들어"라고요.

00:01:21.200 --> 00:01:22.720
기근은 재미있는 일이 아닙니다.

00:01:23.800 --> 00:01:27.176
그러나 공상과학에서 
벌어지는 죽음은 재미있죠.

00:01:27.200 --> 00:01:31.176
지금 이 시점에서 인공지능
개발에 대해 가장 걱정되는 것은

00:01:31.200 --> 00:01:35.296
앞으로 있을 위험에 대한 
적절한 감정적 반응을

00:01:35.320 --> 00:01:37.136
못하는 것 같다는 겁니다.

00:01:37.160 --> 00:01:40.360
저도 잘 못하면서 
이 강연을 하고 있습니다.

00:01:42.120 --> 00:01:44.816
마치 두 개의 문 앞에 
서 있는 것과 같습니다.

00:01:44.840 --> 00:01:46.096
1번 문으로 들어서면

00:01:46.120 --> 00:01:49.416
인공지능 기계의 구축은
더이상 진보하지 않습니다.

00:01:49.440 --> 00:01:53.456
컴퓨터 하드웨어와 소프트웨어가 
더 이상 발전하지 않습니다.

00:01:53.480 --> 00:01:56.480
왜 그런 일이 일어날지 
한번 생각해보죠.

00:01:57.080 --> 00:02:00.736
인공지능과 자동화가 
얼마나 가치있는지 안다면

00:02:00.760 --> 00:02:04.280
할 수 있는 한 계속해서 
기술을 향상시키려 하겠죠.

00:02:05.200 --> 00:02:06.867
그걸 멈춘다면 이유가 뭘까요?

00:02:07.800 --> 00:02:09.600
전면적 핵 전쟁일까요?

00:02:11.000 --> 00:02:12.560
세계적 전염병 재앙일까요?

00:02:14.320 --> 00:02:15.640
소행성 충돌일까요?

00:02:17.640 --> 00:02:20.216
저스틴 비버가 미국 대통령이
된다면 이유가 될까요?

00:02:20.240 --> 00:02:22.520
(웃음)

00:02:24.760 --> 00:02:28.680
핵심은 뭔가가 우리가 아는 
문명을 파괴한다는 겁니다.

00:02:29.360 --> 00:02:33.656
이게 얼마나 나쁠지 
생각해 봐야 합니다.

00:02:33.680 --> 00:02:37.016
우리의 기술향상을 막을 정도로 말이죠.

00:02:37.040 --> 00:02:38.256
영원히

00:02:38.280 --> 00:02:40.296
앞으로 계속이요.

00:02:40.320 --> 00:02:44.046
말 그대로 그것은 
인류 사상 최악의 일입니다.

00:02:44.520 --> 00:02:45.816
그래서 유일한 대안은

00:02:45.840 --> 00:02:48.176
2번 문 뒤에 있는데

00:02:48.200 --> 00:02:51.336
인공지능을 계속 발전시키는 겁니다.

00:02:51.360 --> 00:02:53.240
앞으로 수년 동안 계속이요.

00:02:53.720 --> 00:02:57.360
언젠가는 우리보다 
똑똑한 기계를 만들겠죠.

00:02:58.080 --> 00:03:00.100
더 똑똑한 기계가 생기면

00:03:00.100 --> 00:03:02.696
기계 스스로 발전할 겁니다.

00:03:02.720 --> 00:03:05.456
그러면 수학자 아이 제이 굿의 말대로

00:03:05.480 --> 00:03:07.256
"지능 폭발"이라는 
위험성을 안게 되는데

00:03:07.280 --> 00:03:09.280
우리 통제권을 벗어나는 
일이 생기는 거죠.

00:03:10.120 --> 00:03:12.936
여기 보시는 것처럼 
이런 풍자만화도 있는데요.

00:03:12.960 --> 00:03:17.476
위험한 로봇 군대가 우리를 
공격하리라는 두려움이죠.

00:03:17.480 --> 00:03:20.176
하지만 그건 있을만한 일은 아닙니다.

00:03:20.200 --> 00:03:25.056
기계가 스스로 악해질 리가 없거든요.

00:03:25.080 --> 00:03:27.696
문제는 우리가 기계를 만드는데

00:03:27.720 --> 00:03:29.776
우리보다 너무 능력이 뛰어나서

00:03:29.800 --> 00:03:34.866
기계와 우리의 목표가 살짝만 어긋나도
우리를 파멸시킬 수 있는 겁니다.

00:03:35.960 --> 00:03:38.040
개미와 관련지어 생각해 보세요.

00:03:38.600 --> 00:03:40.256
우리는 개미를 싫어하진 않아요.

00:03:40.280 --> 00:03:42.336
개미를 해치지도 않죠.

00:03:42.360 --> 00:03:44.736
사실 때로는 해치지 않으려고 노력하죠.

00:03:44.760 --> 00:03:46.776
길에서 개미들을 넘어서 
걸음을 딛잖아요.

00:03:46.800 --> 00:03:48.936
하지만 개미의 존재가

00:03:48.960 --> 00:03:51.456
우리의 목표와 심각하게 부딪힐 때

00:03:51.480 --> 00:03:53.957
가령, 이런 건물을 지을 때

00:03:53.981 --> 00:03:55.941
거리낌없이 전멸시킵니다.

00:03:56.480 --> 00:03:59.416
문제는 우리가 언젠가 기계를 만들텐데

00:03:59.440 --> 00:04:02.176
그 기계가 의식이 있든 없든

00:04:02.200 --> 00:04:04.570
비슷하게 무시하는 태도로 
우리를 대할 수 있다는 거죠.

00:04:05.760 --> 00:04:08.520
지나친 억측이라고 
생각하는 분이 대다수겠지만

00:04:09.360 --> 00:04:15.696
초지능 AI가 가능하기나 한지 
의심스러운 분도 분명 계시겠죠.

00:04:15.720 --> 00:04:17.376
물론 필연적이라고 보지도 않고요.

00:04:17.400 --> 00:04:21.020
그럼 다음 가정에서 잘못된
점을 찾으셔야 할 겁니다.

00:04:21.044 --> 00:04:22.616
단 세 가지입니다.

00:04:23.800 --> 00:04:28.519
지능이란 물리적 시스템에서 
정보를 처리하는 것입니다.

00:04:29.320 --> 00:04:31.935
사실 이것은 약간은 
가정 그 이상입니다.

00:04:31.959 --> 00:04:35.416
우린 이미 기계에 
제한된 지능을 구축했고

00:04:35.440 --> 00:04:37.456
이 기계들은 이미

00:04:37.480 --> 00:04:40.120
인간을 뛰어넘는 지능
수준으로 구동합니다.

00:04:40.840 --> 00:04:43.416
우리가 알기로 단순한 문제가

00:04:43.440 --> 00:04:46.056
"일반 지능"이란 것을 
만들어 낼 수도 있죠.

00:04:46.080 --> 00:04:49.736
다양한 분야에서 융통성 있게 
사고하는 능력인데

00:04:49.760 --> 00:04:52.896
우리 뇌는 그렇게 해왔잖아요. 그렇죠?

00:04:52.920 --> 00:04:56.856
여기 원자들이 있고

00:04:56.880 --> 00:05:01.376
원자 시스템을 계속 구축해서

00:05:01.400 --> 00:05:04.096
점점 더 지능적인 행동을 하게 하면

00:05:04.120 --> 00:05:06.656
중간에 멈추지만 않는다면

00:05:06.680 --> 00:05:10.056
우린 마침내 일반지능을

00:05:10.080 --> 00:05:11.376
기계에 구축할 겁니다.

00:05:11.400 --> 00:05:15.056
진보의 속도가 문제되지 
않음을 아는게 중요합니다.

00:05:15.080 --> 00:05:18.256
어떻게든 발전하면 끝까지
갈 수 있기 때문이죠.

00:05:18.280 --> 00:05:22.056
계속 하기 위해 무어의 법칙이나
기하급수적인 진보가 필요없습니다.

00:05:22.080 --> 00:05:23.680
그냥 계속 하면 되죠.

00:05:25.480 --> 00:05:28.400
두 번째 가정은 우리가 
계속 할 것이라는 겁니다.

00:05:29.000 --> 00:05:31.760
우리는 지능적인 기계를 
계속 향상시킬 겁니다.

00:05:33.000 --> 00:05:37.376
지능의 가치를 생각해 볼 때

00:05:37.400 --> 00:05:40.936
지능이란 우리가 가치있게
여기는 모든 것의 원천이거나

00:05:40.960 --> 00:05:43.736
가치있는 것을 지키기 위해 필요하죠.

00:05:43.760 --> 00:05:46.016
우리의 가장 귀한 자원입니다.

00:05:46.040 --> 00:05:47.576
그래서 개발을 하고 싶은 겁니다.

00:05:47.600 --> 00:05:50.936
우리는 간절히 해결하고 
싶은 문제들이 있죠.

00:05:50.960 --> 00:05:54.160
알츠하이머 병이나 암같은 
질병을 치료하고 싶어 합니다.

00:05:54.960 --> 00:05:58.896
경제 체계를 이해하고 기후 과학을
발전시키고 싶어하죠.

00:05:58.920 --> 00:06:01.176
그래서 할 수만 있다면 할 겁니다.

00:06:01.200 --> 00:06:04.486
기차는 이미 출발했고 
멈출 브레이크가 없습니다.

00:06:05.880 --> 00:06:11.336
마지막 가정으로 우리는 지능의 
정점에 서있지 않다는 겁니다.

00:06:11.360 --> 00:06:13.160
그 근처에 가지도 못했을 거예요.

00:06:13.640 --> 00:06:15.536
사실 이게 중요한 생각입니다.

00:06:15.560 --> 00:06:17.976
바로 그래서 우리 상황이 
매우 위험한 겁니다.

00:06:18.000 --> 00:06:22.040
그래서 우리 위험을 인지하는 
우리 직감을 믿을 수가 없는 것이죠.

00:06:23.120 --> 00:06:25.840
이제 지구상에서 가장 
똑똑한 사람이 있다고 하죠.

00:06:26.640 --> 00:06:30.056
거의 모든 사람들을 추려내면
존 폰 노이만이 나옵니다.

00:06:30.080 --> 00:06:33.416
주변 사람들이 폰 노이만을 
어떻게 생각했는지와

00:06:33.440 --> 00:06:37.496
그가 당대의 위대한 수학자이자 
물리학자에 포함된다는 사실은

00:06:37.520 --> 00:06:39.456
잘 알려져 있죠.

00:06:39.480 --> 00:06:43.256
그에 대한 일화의 절반 중에서
반만 사실이라 하더라도

00:06:43.280 --> 00:06:44.496
의심의 여지 없이

00:06:44.520 --> 00:06:46.976
그는 역사상 가장 똑똑한 사람입니다.

00:06:47.000 --> 00:06:49.520
지능의 범위를 생각해 보세요.

00:06:50.320 --> 00:06:51.749
폰 노이만은 여기 있습니다.

00:06:53.560 --> 00:06:54.894
그리고 여기 여러분과 제가 있죠.

00:06:56.120 --> 00:06:57.416
그리고 닭은 여기 있습니다.

00:06:57.440 --> 00:06:59.376
(웃음)

00:06:59.400 --> 00:07:00.616
닭아 미안하다.

00:07:00.640 --> 00:07:01.896
(웃음)

00:07:01.920 --> 00:07:05.656
이 강연을 꼭 우울하게 
할 필요는 없죠.

00:07:05.680 --> 00:07:07.280
(웃음)

00:07:08.339 --> 00:07:11.816
하지만 놀랍게도 지능의 범위는

00:07:11.840 --> 00:07:14.960
우리가 현재 가질 수 있는 
것보다 훨씬 넓게 확장되고

00:07:15.880 --> 00:07:19.096
우리보다 지능적인 기계를 구축한다면

00:07:19.120 --> 00:07:21.416
이 범위를 아우를 
가능성이 매우 높습니다.

00:07:21.440 --> 00:07:23.296
상상할 수 없을 정도로요.

00:07:23.320 --> 00:07:25.840
또한 상상할 수 없을 정도로 
우리를 능가할 겁니다.

00:07:27.000 --> 00:07:31.336
속도 하나만 봐도 
그렇다는 걸 알아야 합니다.

00:07:31.360 --> 00:07:36.416
그렇죠? 초지능 AI를 
구축한다고 해봅시다.

00:07:36.440 --> 00:07:42.226
스탠포드 대학이나 MIT의 연구팀의 
평균 지능 그 이상은 아닌 정도로요.

00:07:42.240 --> 00:07:46.516
생화학적인 회로보다 전자 회로가 
백만 배는 빠르게 작동합니다.

00:07:46.520 --> 00:07:51.536
그러니 이 기계는 그걸 구축한 사람보다 
백만 배의 속도로 빠르게 생각하겠죠.

00:07:51.536 --> 00:07:53.176
일주일 동안 돌리면

00:07:53.200 --> 00:07:57.760
인간 수준 지능의 2만 년 
분량을 해치울 겁니다.

00:07:58.400 --> 00:08:00.360
몇 주 동안 계속 말이죠.

00:08:01.640 --> 00:08:03.490
별 제한 없이

00:08:03.490 --> 00:08:07.040
이런 진보를 이루어내는 인공지능을
어떻게 받아들여야 할까요?

00:08:08.840 --> 00:08:10.976
솔직히 걱정되는 다른 것은

00:08:11.000 --> 00:08:15.976
최고의 시나리오를 생각해 보는 겁니다.

00:08:16.000 --> 00:08:21.606
안전성에 대해 고려하지 않고
초지능 AI를 설계한다고 상상해 보세요.

00:08:21.606 --> 00:08:24.856
처음에는 설계가 완벽하죠.

00:08:24.880 --> 00:08:29.156
의도한 대로 정확히 행동하는 
조력자를 얻은 것 같습니다.

00:08:29.160 --> 00:08:32.880
노동력을 절감해주는
완벽한 기계가 될 겁니다.

00:08:33.680 --> 00:08:36.109
이걸로 물리적인 작업을 할
기계를 만들어 내는

00:08:36.133 --> 00:08:37.896
또 다른 기계를 고안할 수 있습니다.

00:08:37.920 --> 00:08:39.376
태양에너지로 돌아가고

00:08:39.400 --> 00:08:42.096
원자재 정도의 비용만 들어가죠.

00:08:42.120 --> 00:08:45.376
인간의 고된 노동이 끝나게 
된다는 걸 의미하는 거예요.

00:08:45.400 --> 00:08:48.200
그건 대부분의 지적 노동도 
끝나게 된다는 말이기도 합니다.

00:08:49.200 --> 00:08:52.256
우리 같은 유인원은 
이런 상황에서 뭘 할까요?

00:08:52.280 --> 00:08:56.360
한가로이 원반 던지기 놀이하면서 
서로 마사지나 해주겠죠.

00:08:57.840 --> 00:09:00.696
환각제 조금과 이상한 옷만 더하면

00:09:00.720 --> 00:09:02.896
전 세계가 마치 버닝맨 
축제 같을 겁니다.

00:09:02.920 --> 00:09:04.560
(웃음)

00:09:06.320 --> 00:09:08.320
참 괜찮을 것 같지만

00:09:09.280 --> 00:09:14.436
현재의 정치 경제 상황에서 
무슨 일이 생길지 자문해 보세요.

00:09:14.440 --> 00:09:16.856
다분히 우리들은

00:09:16.880 --> 00:09:22.556
이전에 없던 부의 불평등과 
실업문제를 직면하게 될 겁니다.

00:09:22.566 --> 00:09:26.746
새로운 부를 즉시 모든 
인류와 나누려 하지 않고

00:09:27.640 --> 00:09:31.256
소수의 백만장자들만이
경제지 표지를 장식하겠죠.

00:09:31.280 --> 00:09:33.720
세계의 나머지 인류는 
굶어 죽는 동안에요.

00:09:34.320 --> 00:09:36.616
러시아나 중국인은 뭘 할까요?

00:09:36.640 --> 00:09:39.256
실리콘 밸리의 어떤 회사들이

00:09:39.280 --> 00:09:42.016
초지능 AI를 곧 사용하려고 
한다는 걸 들으면요.

00:09:42.040 --> 00:09:44.530
이 기계는 전쟁도 
일으킬 수 있을 겁니다.

00:09:44.920 --> 00:09:47.136
지역분쟁이건 사이버 전쟁이건

00:09:47.160 --> 00:09:48.840
전례없는 힘을 가지고 말입니다.

00:09:50.120 --> 00:09:51.976
이건 승자독식의 시나리오입니다.

00:09:52.000 --> 00:09:55.136
이 경쟁에서 6개월 앞선다는 것은

00:09:55.160 --> 00:09:59.426
최소한 50만 년 앞서는 것입니다.

00:09:59.480 --> 00:10:02.710
그래서 이런 혁신적인 일이

00:10:02.710 --> 00:10:06.616
우리 인류를 미쳐 날뛰게 만들 
거라는 소문이 더욱 떠도는 거죠.

00:10:06.640 --> 00:10:09.536
이제 가장 두려운 것은

00:10:09.560 --> 00:10:12.336
지금 현재, 제 생각에는

00:10:12.360 --> 00:10:16.656
AI연구자가 이런 말을 하는 겁니다.

00:10:16.680 --> 00:10:18.240
안심시키려고 말이죠.

00:10:19.000 --> 00:10:22.456
걱정하지 않아도 되는 
이유는 시기 때문이라고 하죠.

00:10:22.480 --> 00:10:24.536
이것은 완전히 빗나간 말입니다.

00:10:24.560 --> 00:10:27.000
아마도 50에서 100년 뒤겠죠.

00:10:27.720 --> 00:10:28.976
한 연구원이 말했습니다.

00:10:29.000 --> 00:10:30.576
"AI의 안전성에 대한 우려는

00:10:30.600 --> 00:10:33.200
마치 화성의 과잉 인구를 
걱정하는 것과 같다."

00:10:34.116 --> 00:10:35.736
실리콘 밸리 식으로
표현하면 이런 뜻이죠.

00:10:35.760 --> 00:10:38.136
"그 쬐그만 머리로 걱정하지 마세요."

00:10:38.160 --> 00:10:39.496
(웃음)

00:10:39.520 --> 00:10:41.416
아무도 모르는 것 같습니다.

00:10:41.440 --> 00:10:46.490
시간의 축을 말하는 게
완전히 잘못된 결론인 것을

00:10:46.490 --> 00:10:49.936
지능이 그저 정보처리하는 문제라면

00:10:49.960 --> 00:10:52.616
우리는 기계를 계속 향상시킬 것이고

00:10:52.640 --> 00:10:55.520
초지능의 형태를 만들게 될 겁니다.

00:10:56.320 --> 00:11:02.726
그것을 안전하게 할 조건을 만들려면
얼마나 걸릴지 우린 모릅니다.

00:11:04.200 --> 00:11:05.496
다시 말씀드리죠.

00:11:05.520 --> 00:11:12.046
그걸 안전하게 할 조건을 만드는 게
얼마나 걸릴지 우리는 모른다고요.

00:11:12.920 --> 00:11:16.376
잘 모르신다면, 지금의 50년은 
과거와 같은 시간이 아닙니다.

00:11:16.400 --> 00:11:18.856
몇 개월이 50년과 마찬가지죠.

00:11:18.880 --> 00:11:20.720
아이폰을 갖기까지 그만큼 걸렸습니다.

00:11:21.440 --> 00:11:24.040
심슨가족이 TV로 
방영되는 데 걸린 시간입니다.

00:11:24.680 --> 00:11:27.056
50년이란 그리 많은 시간이 아닙니다.

00:11:27.080 --> 00:11:30.240
우리 인류가 당면한 
최고의 난제를 만나는데요.

00:11:31.640 --> 00:11:35.656
다시, 우리는 제대로 된 감정적 반응을 
못하고 있는 것 같습니다.

00:11:35.680 --> 00:11:38.376
앞으로 벌어질 거라고 
생각하는 것들에 대해서요.

00:11:38.400 --> 00:11:42.376
컴퓨터공학자 스튜어트 러셀이 
멋진 비유를 들었습니다.

00:11:42.400 --> 00:11:47.296
외계문명이 보낸 메세지를 
받았다고 상상해 보는 겁니다.

00:11:47.320 --> 00:11:49.016
이런 내용의 메세지죠.

00:11:49.040 --> 00:11:50.576
"지구인들이여

00:11:50.600 --> 00:11:52.960
우리는 50년 후에 
지구에 도착할 것이다.

00:11:53.800 --> 00:11:55.376
이에 준비하라."

00:11:55.400 --> 00:11:59.656
그럼 외계 우주선이 도착할 
때까지 날짜만 세는 겁니까?

00:11:59.680 --> 00:12:02.680
그것보다는 좀더 위급함을 느껴야죠.

00:12:04.680 --> 00:12:06.536
우리에게 걱정말라는 다른 이유는

00:12:06.560 --> 00:12:09.576
이런 기계들은 우리의 가치를 
공유할 수밖에 없다는 겁니다.

00:12:09.600 --> 00:12:12.216
말 그대로 우리의 연장선이니까요.

00:12:12.240 --> 00:12:14.056
기계들은 우리 뇌에 이식될테고

00:12:14.080 --> 00:12:16.440
우리는 본질적으로 그들의 
감정 표현 시스템으로 전락하겠죠.

00:12:17.120 --> 00:12:18.536
한 번 생각해 보세요.

00:12:18.560 --> 00:12:23.096
가장 안전하고 현명한 방법을 추천하자면

00:12:23.120 --> 00:12:25.920
이 기술을 우리 뇌에 
직접 이식하는 겁니다.

00:12:26.600 --> 00:12:29.976
앞으로 할 수 있는 가장 
안전하고 현명한 방법이겠지만

00:12:30.000 --> 00:12:33.056
기술의 안전성에 대한 우려는 보통

00:12:33.080 --> 00:12:36.736
머릿속에서 꽂아 보기 
전에 해야 할 일이죠.

00:12:36.760 --> 00:12:38.776
(웃음)

00:12:38.800 --> 00:12:40.550
좀 더 심오한 문제는 

00:12:41.140 --> 00:12:44.160
초지능의 AI를 만드는 것 자체가

00:12:44.160 --> 00:12:45.896
오히려 더 쉽다는 거에요.

00:12:45.920 --> 00:12:47.776
초지능 AI를 만들고

00:12:47.800 --> 00:12:49.576
완전한 신경과학이 있어서

00:12:49.600 --> 00:12:52.280
우리의 정신을 AI와 매끄럽게 
접목시키는 것에 비해서 말이죠.

00:12:52.800 --> 00:12:55.976
기업과 정부가 이 일을 하고 있음을 보면

00:12:56.000 --> 00:12:59.656
다른 종에 대항하는 존재임을 
스스로 인지할 가능성이 있습니다.

00:12:59.680 --> 00:13:02.936
이 경쟁에서 이기는 것이 세상을 
이기는 것이라고 한다면요.

00:13:02.960 --> 00:13:05.416
그것을 바로 파괴하지 않는다면

00:13:05.440 --> 00:13:09.526
가장 쉬운 일을 가장 먼저 
할 가능성이 있을 겁니다.

00:13:10.560 --> 00:13:13.416
안타깝게도 저는 이 문제에 
대한 해법은 없습니다.

00:13:13.440 --> 00:13:16.056
좀 더 생각해 보자고 
하는 것 빼고는요.

00:13:16.080 --> 00:13:18.456
맨하탄 프로젝트 같은 것이 있어서

00:13:18.480 --> 00:13:20.496
AI에 대한 연구를 해야
되지 않을까 싶습니다.

00:13:20.520 --> 00:13:23.256
만들기 위해서가 아니라, 
필연적으로 하게 될테니까

00:13:23.280 --> 00:13:26.616
군비경쟁을 피할 방법을 알고

00:13:26.640 --> 00:13:30.136
우리의 관심과 맞출 수 
있는 방법을 찾기 위해서죠.

00:13:30.160 --> 00:13:34.656
스스로 변할 수 있는 
초지능 AI를 말할 때는

00:13:34.656 --> 00:13:39.216
첫 출발을 제대로 할 수 있는 
기회는 단 한 번뿐인 것 같습니다.

00:13:39.240 --> 00:13:41.296
그때도 우리는

00:13:41.320 --> 00:13:44.360
제대로 하기 위한 정치 경제적 
결과를 감내해야 할 겁니다.

00:13:45.760 --> 00:13:47.816
하지만 우리가 인정하는 순간

00:13:47.840 --> 00:13:51.840
정보처리가 지능의 원천이고

00:13:52.720 --> 00:13:57.520
적정 연산 체계가 지능의 
기본이라고 하는 순간

00:13:58.360 --> 00:14:02.120
이 체계를 계속 발전시켜 
나갈 것임을 인정하는 것이며

00:14:03.280 --> 00:14:09.206
우리가 현재 아는 인지의 한계를 훨씬 
뛰어넘게 될 것임을 인정하는 겁니다.

00:14:10.120 --> 00:14:11.336
그럼 우리는

00:14:11.360 --> 00:14:14.000
어떤 신적 존재를 구축하고 
있음을 인정해야 합니다.

00:14:15.400 --> 00:14:19.236
지금이야말로 신이 우리와 함께 할 수 
있음을 확신하기 좋은 때이겠네요.

00:14:20.120 --> 00:14:21.656
대단히 감사합니다.

00:14:21.680 --> 00:14:25.353
(박수)

