WEBVTT
Kind: captions
Language: it

00:00:00.000 --> 00:00:07.000
Traduttore: Giulia Claudi

00:00:12.820 --> 00:00:15.036
Vi parlerò della mancanza d'intuito

00:00:15.060 --> 00:00:16.660
di cui molti di noi soffrono

00:00:17.300 --> 00:00:20.340
è davvero un insuccesso non notare un certo tipo di pericolo

00:00:21.180 --> 00:00:22.916
Vi descriverò uno scenario

00:00:22.940 --> 00:00:26.196
che penso sia terrificante

00:00:26.220 --> 00:00:27.980
e allo stesso tempo molto probabile che accada,

00:00:28.660 --> 00:00:30.316
una combinazione poco felice,

00:00:30.340 --> 00:00:31.876
a quanto pare.

00:00:31.900 --> 00:00:34.466
Ma invece di essere spaventati,
molti di voi penseranno

00:00:34.466 --> 00:00:36.710
che quello di cui sto parlando
è piuttosto figo.

00:00:37.020 --> 00:00:39.996
Vi mostrerò che i progressi
che stiamo facendo

00:00:40.020 --> 00:00:41.796
nell'intelligenza artificiale

00:00:41.820 --> 00:00:43.596
potrebbero distruggerci.

00:00:43.620 --> 00:00:47.076
Penso sia difficile immaginare
come non possano distruggerci

00:00:47.100 --> 00:00:48.780
o indurci a distruggerci tra noi.

00:00:49.220 --> 00:00:51.076
E tuttavia, se siete come me,

00:00:51.100 --> 00:00:53.756
penserete che sia divertente
pensare a queste cose.

00:00:53.780 --> 00:00:57.156
Questa reazione è parte del problema.

00:00:57.180 --> 00:00:59.350
Ok? Questa reazione dovrebbe preoccuparvi.

00:00:59.740 --> 00:01:02.396
Se in questa conferenza
vi dovessi convincere

00:01:02.420 --> 00:01:05.836
che è molto probabile che si verifichi
una carestia globale,

00:01:05.860 --> 00:01:08.916
o per i cambiamenti climatici
o per altre catastrofi,

00:01:08.940 --> 00:01:12.356
e che i vostri nipoti, o i loro nipoti,

00:01:12.380 --> 00:01:14.180
vivranno in queste condizioni,

00:01:15.020 --> 00:01:16.220
non pensereste:

00:01:17.260 --> 00:01:18.596
"Interessante.

00:01:18.620 --> 00:01:19.820
Bello questo TED Talk."

00:01:21.020 --> 00:01:22.540
La carestia non è divertente.

00:01:23.620 --> 00:01:26.996
Una morte fantascientifica,
d'altro canto, è divertente,

00:01:27.020 --> 00:01:30.996
e una delle cose che mi preoccupano di più
dello sviluppo dell'AI

00:01:31.020 --> 00:01:35.116
è che non sembriamo in grado
di organizzare la giusta risposta emotiva

00:01:35.140 --> 00:01:36.956
rispetto al pericolo che corriamo.

00:01:36.980 --> 00:01:40.180
Nemmeno io ne sono in grado,
e sto facendo questo intervento.

00:01:41.940 --> 00:01:44.600
È come se ci trovassimo
davanti a due porte.

00:01:44.600 --> 00:01:45.916
Dietro la prima c'è questo:

00:01:45.940 --> 00:01:49.236
smettiamo di fare progressi
nel costruire macchine intelligenti.

00:01:49.260 --> 00:01:53.276
Per qualche motivo gli hardware e software
dei nostri computer non migliorano più.

00:01:53.300 --> 00:01:56.300
Pensate un attimo per quale motivo
potrebbe succedere.

00:01:56.900 --> 00:02:00.556
Insomma, visto quanto sono importanti
l'intelligenza e l'automazione,

00:02:00.580 --> 00:02:04.100
continueremo a migliorare
la nostra tecnologia, se possiamo.

00:02:05.020 --> 00:02:06.687
Cosa potrebbe impedircelo?

00:02:07.620 --> 00:02:09.420
Una guerra atomica su scala mondiale?

00:02:10.820 --> 00:02:12.380
Una pandemia globale?

00:02:14.140 --> 00:02:15.460
L'impatto di un asteroide?

00:02:17.460 --> 00:02:20.086
Justin Bieber che diventa
il primo presidente degli USA?

00:02:20.086 --> 00:02:22.340
(Risate)

00:02:24.580 --> 00:02:28.500
Qualcosa dovrebbe distruggere
la nostra civiltà, per come la conosciamo.

00:02:29.180 --> 00:02:33.476
Dovete immaginare quanto
dovrebbe essere catastrofico

00:02:33.500 --> 00:02:36.836
per farci smettere di sviluppare
la nostra tecnologia,

00:02:36.860 --> 00:02:38.076
per sempre,

00:02:38.100 --> 00:02:40.116
generazione dopo generazione.

00:02:40.140 --> 00:02:42.276
Sarebbe per definizione la cosa peggiore

00:02:42.300 --> 00:02:44.316
mai successa nella storia dell'uomo.

00:02:44.340 --> 00:02:45.636
Perciò l'unica alternativa,

00:02:45.660 --> 00:02:47.996
ovvero ciò che c'è
dietro la seconda porta,

00:02:48.020 --> 00:02:51.156
è che continueremo a sviluppare
l'intelligenza artificiale,

00:02:51.180 --> 00:02:52.780
anno dopo anno.

00:02:53.540 --> 00:02:57.180
Arriverà un momento in cui costruiremo
macchine più intelligenti di noi,

00:02:57.900 --> 00:03:00.516
e quando le macchine saranno
più intelligenti di noi

00:03:00.540 --> 00:03:02.516
inizieranno a migliorarsi da sole.

00:03:02.540 --> 00:03:05.276
Allora rischieremo quello che
il matematico IJ Good chiama

00:03:05.300 --> 00:03:07.076
"esplosione dell'intelligenza",

00:03:07.100 --> 00:03:09.100
ovvero che potremmo perdere il controllo.

00:03:09.940 --> 00:03:12.756
Come vedete, spesso questo viene reso
con una caricatura,

00:03:12.780 --> 00:03:15.996
ovvero la paura che un esercito
di robot malvagi

00:03:16.020 --> 00:03:17.276
ci attacchi.

00:03:17.300 --> 00:03:19.996
Ma questo non è lo scenario più probabile.

00:03:20.020 --> 00:03:24.876
Le nostre macchine non diventeranno
malvagie spontaneamente.

00:03:24.900 --> 00:03:27.516
Il problema è che costruiremo macchine

00:03:27.540 --> 00:03:29.596
che saranno tanto più capaci di noi

00:03:29.620 --> 00:03:33.396
che la più piccola divergenza
tra i loro obiettivi e i nostri

00:03:33.420 --> 00:03:34.620
ci potrebbe distruggere.

00:03:35.780 --> 00:03:38.000
Pensate a come ci comportiamo
con le formiche.

00:03:38.420 --> 00:03:40.076
Non le odiamo.

00:03:40.100 --> 00:03:42.156
Non facciamo loro del male apposta.

00:03:42.180 --> 00:03:44.556
Alle volte facciamo attenzione
a non danneggiarle.

00:03:44.580 --> 00:03:46.596
Le saltiamo, quando sono sul marciapiede.

00:03:46.620 --> 00:03:48.756
Ma se la loro presenza

00:03:48.780 --> 00:03:51.230
ci intralcia,

00:03:51.230 --> 00:03:53.897
ad esempio se dobbiamo costruire
un edificio come questo,

00:03:53.897 --> 00:03:55.901
le distruggiamo senza battere ciglio.

00:03:56.300 --> 00:03:59.296
La mia preoccupazione è che un giorno
costruiremo delle macchine

00:03:59.296 --> 00:04:01.996
che, consapevolmente o meno,

00:04:02.020 --> 00:04:04.020
possano trattarci
con la stessa noncuranza.

00:04:05.580 --> 00:04:08.340
Penserete che tutto questo
sia inverosimile

00:04:09.180 --> 00:04:15.516
Scommetto che alcuni di voi dubitano
che una IA del genere sia possibile,

00:04:15.540 --> 00:04:17.196
e ancor meno, che sia inevitabile.

00:04:17.220 --> 00:04:20.840
Ma per credere questo dovete trovare
qualcosa di sbagliato in questi assunti.

00:04:20.864 --> 00:04:22.436
Sono solo tre.

00:04:23.620 --> 00:04:28.339
L'intelligenza dipende dalle informazioni
elaborate in un sistema fisico.

00:04:29.140 --> 00:04:31.755
In realtà questo è più di un assunto.

00:04:31.779 --> 00:04:35.236
Abbiamo già inserito delle "IA deboli"
nelle nostre macchine,

00:04:35.260 --> 00:04:37.276
e molte di queste hanno già dato risultati

00:04:37.300 --> 00:04:39.940
a livelli di una super intelligenza.

00:04:40.660 --> 00:04:43.236
Sappiamo che la sola materia

00:04:43.260 --> 00:04:45.876
può dar vita alla "intelligenza generale",

00:04:45.900 --> 00:04:49.556
l'abilità di pensare in modo flessibile
attraverso settori diversi,

00:04:49.580 --> 00:04:52.716
perché il nostro cervello
lo sa fare. Giusto?

00:04:52.740 --> 00:04:56.676
Alla fine, ci sono solo atomi qua dentro,

00:04:56.700 --> 00:05:01.196
e finché continuiamo a costruire
dei sistemi fatti di atomi

00:05:01.220 --> 00:05:03.916
che manifestano comportamenti
sempre più intelligenti,

00:05:03.940 --> 00:05:06.476
a meno che qualcosa non ce lo impedisca,

00:05:06.500 --> 00:05:09.876
arriveremo a immettere
l'intelligenza generale

00:05:09.900 --> 00:05:11.196
nelle nostre macchine.

00:05:11.220 --> 00:05:14.876
È fondamentale capire
che il tasso di progresso non conta,

00:05:14.900 --> 00:05:18.076
perché ogni progresso basta
ad avvicinarci al suo compimento.

00:05:18.100 --> 00:05:21.876
Non c'è bisogno della legge di Moore,
o della crescita esponenziale.

00:05:21.900 --> 00:05:23.500
Basta proseguire.

00:05:25.300 --> 00:05:28.220
Il secondo assunto è che proseguiremo.

00:05:28.820 --> 00:05:31.580
Continueremo a migliorare
le nostre macchine intelligenti.

00:05:32.820 --> 00:05:37.196
E data l'importanza dell'intelligenza...

00:05:37.220 --> 00:05:40.756
O l'intelligenza è la fonte di tutto ciò
che per noi ha valore,

00:05:40.780 --> 00:05:43.586
o ne abbiamo bisogno per preservare
tutto ciò che ha valore.

00:05:43.606 --> 00:05:45.836
È la nostra risorsa più importante.

00:05:45.860 --> 00:05:47.396
Per questo lo facciamo.

00:05:47.420 --> 00:05:50.756
Ci sono problemi che dobbiamo
risolvere disperatamente.

00:05:50.780 --> 00:05:53.980
Vogliamo curare malattie
come l'Alzheimer e il cancro.

00:05:54.780 --> 00:05:58.716
Vogliamo comprendere i sistemi economici,
migliorare le scienze del clima.

00:05:58.740 --> 00:06:00.996
Perciò lo faremo, se possiamo.

00:06:01.020 --> 00:06:04.306
Il treno ha già lasciato la stazione
e non c'è nessun freno da tirare.

00:06:05.700 --> 00:06:11.156
Infine, non abbiamo ancora raggiunto
il culmine della nostra intelligenza,

00:06:11.180 --> 00:06:12.980
nemmeno lontanamente.

00:06:13.460 --> 00:06:15.356
Questa è l'intuizione fondamentale,

00:06:15.380 --> 00:06:17.796
ciò che rende la nostra situazione
così precaria

00:06:17.820 --> 00:06:21.860
e la nostra percezione del rischio
così inaffidabile.

00:06:22.940 --> 00:06:25.660
Pensate alla persona più intelligente
che sia mai vissuta.

00:06:26.460 --> 00:06:29.876
Quasi tutti inserirebbero
John von Neumann tra i primi posti.

00:06:29.900 --> 00:06:33.236
L'ottima impressione che von Neumann
fece sulle persone intorno a lui,

00:06:33.260 --> 00:06:37.316
tra cui c'erano i più grandi fisici
e matematici del suo tempo,

00:06:37.340 --> 00:06:39.276
è ben documentata.

00:06:39.300 --> 00:06:43.070
Se solo metà delle storie su di lui
sono vere a metà,

00:06:43.070 --> 00:06:44.270
non ci sono dubbi

00:06:44.270 --> 00:06:46.846
che sia una delle persone
più intelligenti mai vissute.

00:06:46.846 --> 00:06:49.340
Considerate una gamma di intelligenze.

00:06:50.140 --> 00:06:51.569
Qui abbiamo John von Neumann.

00:06:53.380 --> 00:06:54.714
Qui ci siamo io e voi.

00:06:55.940 --> 00:06:57.236
E qui una gallina.

00:06:57.260 --> 00:06:59.196
(Risate)

00:06:59.220 --> 00:07:00.436
Scusate, una gallina.

00:07:00.460 --> 00:07:01.716
(Risate)

00:07:01.740 --> 00:07:05.476
Non rendiamo questo incontro
più deprimente di quanto non sia già.

00:07:05.500 --> 00:07:07.100
(Risate)

00:07:08.159 --> 00:07:11.636
È molto probabile, però,
che la gamma delle intelligenze

00:07:11.660 --> 00:07:14.780
sia molto più estesa di quando si pensi,

00:07:15.700 --> 00:07:18.916
e se costruiamo delle macchine
molto più intelligenti di noi,

00:07:18.940 --> 00:07:21.236
finiranno con l'esplorare questa gamma

00:07:21.260 --> 00:07:23.116
in modi che non possiamo prevedere,

00:07:23.140 --> 00:07:25.660
e ci supereranno in modi
che non possiamo immaginare.

00:07:26.820 --> 00:07:31.156
È importante riconoscere che questo
sia vero anche in virtù della velocità.

00:07:31.180 --> 00:07:36.236
Giusto? Immaginate che sia stata inventata
una IA super intelligente,

00:07:36.260 --> 00:07:39.716
che non sia più brillante
di un normale gruppo di ricerca

00:07:39.740 --> 00:07:42.036
a Stanford o al MIT.

00:07:42.060 --> 00:07:45.056
I circuiti elettronici funzionano
un milione di volte più veloci

00:07:45.060 --> 00:07:46.316
di quelli biochimici,

00:07:46.340 --> 00:07:49.586
perciò questa macchina funzionerà
un milione di volte più velocemente

00:07:49.586 --> 00:07:51.316
delle menti che l'hanno costruita.

00:07:51.340 --> 00:07:52.996
In una settimana

00:07:53.020 --> 00:07:57.580
avrà fatto il lavoro di 20 mila anni
di lavoro intellettuale umano,

00:07:58.220 --> 00:08:00.180
settimana, dopo settimana.

00:08:01.460 --> 00:08:04.556
Come possiamo comprendere,
e figuriamoci contenere,

00:08:04.580 --> 00:08:06.860
una mente che progredisce
così velocemente?

00:08:08.660 --> 00:08:10.796
C'è un altro aspetto preoccupante,

00:08:10.820 --> 00:08:15.796
anche immaginando lo scenario
più ottimista.

00:08:15.820 --> 00:08:19.996
Mettiamo che abbiamo realizzato
una IA super intelligente

00:08:20.020 --> 00:08:21.396
senza problemi di sicurezza.

00:08:21.420 --> 00:08:24.676
Abbiamo trovato alla prima
il design perfetto.

00:08:24.700 --> 00:08:26.916
Come se ci avessero dato un oracolo

00:08:26.940 --> 00:08:28.956
che si comporta proprio come dovrebbe.

00:08:28.980 --> 00:08:32.700
Sarebbe lo strumento perfetto
per risparmiare fatica.

00:08:33.500 --> 00:08:35.989
Può progettare la macchina
che ne costruisce un'altra

00:08:35.989 --> 00:08:37.716
che faccia qualsiasi lavoro fisico,

00:08:37.740 --> 00:08:39.196
azionata dalla luce solare,

00:08:39.220 --> 00:08:41.916
più o meno al costo della materia prima.

00:08:41.940 --> 00:08:45.196
Per l'uomo, sarebbe la fine
del lavoro duro,

00:08:45.220 --> 00:08:48.020
ma anche della maggior parte
dei lavori intellettuali.

00:08:49.020 --> 00:08:52.076
Cosa farebbero degli scimmioni come siamo
in queste circostanze?

00:08:52.100 --> 00:08:56.180
Saremmo liberi di giocare a frisbee
e farci i massaggi a vicenda.

00:08:57.660 --> 00:09:00.516
Aggiungete un po' di LSD, dei vestiti
di dubbio gusto,

00:09:00.540 --> 00:09:02.716
e il mondo sarebbe come il Burning Man.

00:09:02.740 --> 00:09:04.380
(Risate)

00:09:06.140 --> 00:09:08.140
Tutto questo non suona male,

00:09:09.100 --> 00:09:11.476
ma pensate a come sarebbe se accadesse

00:09:11.500 --> 00:09:14.236
in un ordine economico
e politico come il nostro.

00:09:14.260 --> 00:09:16.676
Assisteremmo

00:09:16.700 --> 00:09:20.836
ad una disparità delle ricchezze
e ad una disoccupazione

00:09:20.860 --> 00:09:22.356
senza precedenti.

00:09:22.380 --> 00:09:25.296
Non ci sarebbe la volontà 
di mettere questo nuovo benessere

00:09:25.296 --> 00:09:26.610
al servizio di tutti,

00:09:27.460 --> 00:09:31.076
Pochi miliardari adornerebbero
le copertine di riviste di business,

00:09:31.100 --> 00:09:33.540
mentre il resto del mondo
morirebbe di fame.

00:09:34.140 --> 00:09:36.436
Cosa farebbero, allora,
la Russia e la Cina

00:09:36.460 --> 00:09:39.076
se sapessero che nella Silicon Valley
delle compagnie

00:09:39.100 --> 00:09:41.836
fossero sul punto di rilasciare una IA
super intelligente?

00:09:41.860 --> 00:09:44.716
Questa macchina
potrebbe scatenare una guerra,

00:09:44.740 --> 00:09:46.956
terrestre o cibernetica,

00:09:46.980 --> 00:09:48.660
disponendo di un potere senza pari.

00:09:49.940 --> 00:09:51.796
Uno scenario di vittoria assoluta.

00:09:51.820 --> 00:09:54.956
Essere in vantaggio sulla competizione
anche solo di sei mesi

00:09:54.980 --> 00:09:57.756
è come essere avanti di 500 mila,

00:09:57.780 --> 00:09:59.276
almeno.

00:09:59.300 --> 00:10:04.036
Perciò anche solo la voce
di una svolta simile

00:10:04.060 --> 00:10:06.436
potrebbe far impazzire la nostra specie.

00:10:06.460 --> 00:10:09.356
Tra gli aspetti più inquietanti,

00:10:09.380 --> 00:10:12.156
a mio parere, che ci sono al momento,

00:10:12.180 --> 00:10:16.476
sono le risposte che danno
i ricercatori di IA

00:10:16.500 --> 00:10:18.060
quando vogliono rassicurarci.

00:10:18.820 --> 00:10:22.276
Una delle rassicurazioni più comuni
è che abbiamo tempo.

00:10:22.300 --> 00:10:24.356
Siamo ancora molto lontani, sapete.

00:10:24.380 --> 00:10:26.820
Ci vorranno ancora 50 o 100 anni.

00:10:27.540 --> 00:10:28.790
Un ricercatore ha detto:

00:10:28.790 --> 00:10:30.586
"Preoccuparsi della sicurezza dell'IA

00:10:30.586 --> 00:10:33.130
è come preoccuparsi
della sovrappopolazione su Marte."

00:10:33.906 --> 00:10:35.626
La versione della Silicon Valley di:

00:10:35.626 --> 00:10:38.356
"non turbare la tua bella testolina
con questi problemi".

00:10:38.356 --> 00:10:39.316
(Risate)

00:10:39.340 --> 00:10:41.236
Nessuno sembra notare

00:10:41.260 --> 00:10:43.876
che far riferimento
all'orizzonte temporale

00:10:43.900 --> 00:10:46.476
è del tutto illogico.

00:10:46.500 --> 00:10:49.756
Se l'intelligenza è questione
di processare le informazioni

00:10:49.780 --> 00:10:52.436
e continueremo a migliorare
le nostre macchine,

00:10:52.460 --> 00:10:55.340
arriveremo a produrre una forma
di intelligenza superiore.

00:10:56.140 --> 00:10:59.796
E non abbiamo idea di quanto ci voglia

00:10:59.820 --> 00:11:02.220
per crearla in condizioni di sicurezza.

00:11:04.020 --> 00:11:05.316
Lo ripeto.

00:11:05.340 --> 00:11:09.156
Non abbiamo idea di quanto ci voglia

00:11:09.180 --> 00:11:11.420
per crearla in condizioni di sicurezza.

00:11:12.740 --> 00:11:16.196
E 50 di progresso, oggi,
non sono più come una volta.

00:11:16.220 --> 00:11:18.676
Questi sono 50 anni riportati in mesi.

00:11:18.700 --> 00:11:20.540
Questo è da quanto abbiamo l'iPhone.

00:11:21.260 --> 00:11:23.860
Questo è da quanto "I Simpson" sono in tv.

00:11:24.500 --> 00:11:26.876
50 anni non sono poi così tanti

00:11:26.900 --> 00:11:30.090
per affrontare una delle sfide maggiori
che l'uomo abbia affrontato.

00:11:31.460 --> 00:11:35.476
Ancora una volta, sembra che ci manchi
la giusta risposta emotiva

00:11:35.500 --> 00:11:38.196
a quello che, per vari motivi,
sappiamo che accadrà.

00:11:38.220 --> 00:11:42.196
L'informatico Stuart Russel
ha fatto una giusta analogia.

00:11:42.220 --> 00:11:47.116
Ha detto di immaginare di ricevere
un messaggio dagli alieni

00:11:47.140 --> 00:11:48.836
che dice:

00:11:48.860 --> 00:11:50.396
"Popolo della Terra,

00:11:50.420 --> 00:11:52.780
arriveremo sul vostro pianeta fra 50 anni.

00:11:53.620 --> 00:11:55.196
Preparatevi".

00:11:55.220 --> 00:11:59.476
Staremmo qui a fare il conto alla rovescia
finché non atterri la nave ammiraglia?

00:11:59.500 --> 00:12:02.500
O forse sentiremmo
un pochino più di urgenza?

00:12:04.500 --> 00:12:06.356
In più, ci dicono di non preoccuparci

00:12:06.380 --> 00:12:09.396
perché queste macchine
avrebbero i nostri stessi valori,

00:12:09.420 --> 00:12:12.036
essendo una vera e propria
estensione di noi stessi.

00:12:12.060 --> 00:12:13.876
Ce le impianteremo nel cervello,

00:12:13.900 --> 00:12:16.260
e diventeremo il loro sistema limbico.

00:12:16.940 --> 00:12:18.356
Pensate un attimo

00:12:18.380 --> 00:12:21.556
che la via più sicura e prudente,

00:12:21.580 --> 00:12:22.916
quella raccomandata,

00:12:22.940 --> 00:12:25.800
è di impiantarci questa tecnologia
direttamente nel cervello.

00:12:26.420 --> 00:12:29.796
Ora, questa potrebbe essere
l'unica via più sicura e prudente,

00:12:29.820 --> 00:12:32.876
ma di solito uno dovrebbe
aver chiarito tutti i dubbi

00:12:32.900 --> 00:12:36.556
su una tecnologia prima
di infilarsela in testa.

00:12:36.580 --> 00:12:38.596
(Risate)

00:12:38.620 --> 00:12:43.956
Il problema principale è che costruire
una IA super intelligente, a parte,

00:12:43.980 --> 00:12:45.716
sembra molto più semplice

00:12:45.740 --> 00:12:47.490
di costruirla

00:12:47.490 --> 00:12:49.426
ed avere le conoscenze neuroscientifiche

00:12:49.426 --> 00:12:52.290
che consentano di integrarla
completamente alla nostra mente.

00:12:52.620 --> 00:12:55.796
Dato che le aziende e i governi
che ci lavorano

00:12:55.820 --> 00:12:59.476
sono in competizione
gli uni con gli altri,

00:12:59.500 --> 00:13:02.756
e dato che vincere questa gara
equivale a dominare il mondo,

00:13:02.780 --> 00:13:05.236
sempre che non venga distrutto
subito dopo,

00:13:05.260 --> 00:13:07.876
è probabile che verrà intrapresa

00:13:07.900 --> 00:13:09.100
la strada più facile.

00:13:10.380 --> 00:13:13.236
Purtroppo non ho soluzione
a questo problema,

00:13:13.260 --> 00:13:15.510
a parte raccomandare
che più persone ci pensino.

00:13:15.510 --> 00:13:18.136
Credo che ci servirebbe una specie
di Progetto Manhattan

00:13:18.136 --> 00:13:20.160
sull'intelligenza artificiale.

00:13:20.160 --> 00:13:23.186
Non per costruirla, perché penso
che ci arriveremo comunque,

00:13:23.186 --> 00:13:26.436
ma per capire come evitare
una corsa alle armi

00:13:26.460 --> 00:13:29.956
e per svilupparla in modo che sia in linea
con i nostri interessi.

00:13:29.980 --> 00:13:32.116
Quando si parla di una IA
super intelligente

00:13:32.140 --> 00:13:34.396
che possa migliorarsi da sola,

00:13:34.420 --> 00:13:39.036
la nostra unica opzione è quella
di darle le giuste impostazioni iniziali,

00:13:39.060 --> 00:13:41.116
e anche allora dovremo comprendere

00:13:41.140 --> 00:13:44.180
quali saranno le conseguenze
economiche e politiche.

00:13:45.580 --> 00:13:47.636
Ma se ammettiamo che

00:13:47.660 --> 00:13:51.660
l'elaborazione delle informazioni
è la fonte dell'intelligenza,

00:13:52.540 --> 00:13:57.340
che un sistema computazionale idoneo
è alla base dell'intelligenza,

00:13:58.180 --> 00:14:01.940
se ammettiamo che svilupperemo
continuamente tali sistemi,

00:14:03.100 --> 00:14:07.556
e che l'orizzonte cognitivo
è di gran lunga maggiore

00:14:07.580 --> 00:14:08.780
di ciò che sappiamo ora,

00:14:09.940 --> 00:14:11.156
allora dobbiamo ammettere

00:14:11.180 --> 00:14:13.820
che stiamo costruendo una sorta di dio.

00:14:15.220 --> 00:14:16.796
Ora sarebbe il momento giusto

00:14:16.820 --> 00:14:19.453
per assicurarci che sia un dio
con cui possiamo vivere.

00:14:19.940 --> 00:14:21.476
Grazie mille.

00:14:21.500 --> 00:14:26.593
(Applausi)

