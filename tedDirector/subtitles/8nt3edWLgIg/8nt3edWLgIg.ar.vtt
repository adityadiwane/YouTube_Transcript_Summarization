WEBVTT
Kind: captions
Language: ar

00:00:00.000 --> 00:00:07.000
المترجم: Hani Eldalees
المدقّق: Hussain Laghabi

00:00:12.820 --> 00:00:15.036
سأقوم بالتحدث عن فشل الحدس

00:00:15.060 --> 00:00:16.660
الذي يعاني معظمنا منه.

00:00:17.300 --> 00:00:20.340
هو في الواقع الفشل في تحرِّي نوع محدد
من الخطر.

00:00:21.180 --> 00:00:22.916
سوف أقوم بوصف سيناريو

00:00:22.940 --> 00:00:26.196
أظنه مخيفاً

00:00:26.220 --> 00:00:27.980
ومن المرجح أن يحدث،

00:00:28.660 --> 00:00:30.316
وهذا ليس مزيجاً جيداً،

00:00:30.340 --> 00:00:31.876
كما سيبدو.

00:00:31.900 --> 00:00:34.356
ومع ذلك، بدلاً من الخوف، سيشعر معظمكم

00:00:34.380 --> 00:00:36.460
بأن ما أتحدُّث عنه مثير نوعاً ما.

00:00:37.020 --> 00:00:39.996
سأقوم بوصف كيف أن المكاسب التي نحصل عليها

00:00:40.020 --> 00:00:41.796
من الذكاء الاصطناعي

00:00:41.820 --> 00:00:43.596
يمكن في النهاية أن تدمرنا.

00:00:43.620 --> 00:00:47.076
وفي الواقع، أظنه من الصعب جداً أن نرى
كيف أنها لن تدمرنا

00:00:47.100 --> 00:00:48.780
أو تلهمنا أن ندمر أنفسنا.

00:00:49.220 --> 00:00:51.076
ومع ذلك إذا كنتم تشبهونني في شيء

00:00:51.100 --> 00:00:53.756
فستجدون أنه من الممتع التفكير
بهذه الأشياء.

00:00:53.780 --> 00:00:57.156
وتلك الإجابة هي جزء من المشكلة.

00:00:57.180 --> 00:00:58.900
حسناً؟ تلك الإجابة يجب أن تُقلقكم.

00:00:59.740 --> 00:01:02.396
وإذا كان عليَّ أن أقنعكم في هذه المحادثة

00:01:02.420 --> 00:01:05.836
أنه من المرجح أننا سوف نعاني من
مجاعة عالمية

00:01:05.860 --> 00:01:08.916
إما بسبب تغيُّر المناخ، أو بسبب
كارثة أخرى،

00:01:08.940 --> 00:01:12.356
وأن أحفادكم، أو أحفادهم،

00:01:12.380 --> 00:01:14.180
من المرجح جداً أن يعيشوا، هكذا

00:01:15.020 --> 00:01:16.220
فلن يخطر ببالكم.

00:01:17.260 --> 00:01:18.596
"مثير للاهتمام".

00:01:18.620 --> 00:01:19.820
تعجبني محادثة TED هذه.

00:01:21.020 --> 00:01:22.540
المجاعة ليست مسلية.

00:01:23.620 --> 00:01:26.996
الموت عن طريق الخيال العلمي، من ناحية
أخرى، مسلٍّ،

00:01:27.020 --> 00:01:30.996
وأحد الأشياء التي تقلقني للغاية بسبب تطور
الذكاء الاصطناعي عند هذه النقطة

00:01:31.020 --> 00:01:35.116
هي أننا نبدو وكأننا غير قادرين على إبداء
استجابة عاطفية ملائمة

00:01:35.140 --> 00:01:36.956
للأخطار التي تنتظرنا.

00:01:36.980 --> 00:01:40.180
أنا غير قادر على إبداء هذه الاستجابة،
وأنا أقوم بهذه المحادثة.

00:01:41.940 --> 00:01:44.636
يبدو الأمر وكأننا نقف أمام بابين.

00:01:44.660 --> 00:01:45.916
خلف الباب رقم واحد،

00:01:45.940 --> 00:01:49.236
نتوقف عن تحقيق التقدُّم في بناء الآليات
الذكية.

00:01:49.260 --> 00:01:53.276
معدات حاسوبنا وبرمجياته تتوقف عن التحسن
لسببٍ ما وحسب.

00:01:53.300 --> 00:01:56.300
خذوا الآن لحظة للتفكير لماذا قد يحدث ذلك.

00:01:56.900 --> 00:02:00.556
أعني، بافتراض مدى نفع الذكاء والأتمتة،

00:02:00.580 --> 00:02:04.100
فإننا سنستمر بتحسين التكنولوجيا الخاصة بنا
إذا كان بإمكاننا أصلاً.

00:02:05.020 --> 00:02:06.687
ما الذي قد يمنعنا من فعل هذا؟

00:02:07.620 --> 00:02:09.420
حربٌ نووية على جميع المقاييس؟

00:02:10.820 --> 00:02:12.380
وباء عالمي؟

00:02:14.140 --> 00:02:15.460
اصطدام كويكب؟

00:02:17.460 --> 00:02:20.036
أن يصبح جاستن بيبر رئيساً
للولايات المتحدة؟

00:02:20.060 --> 00:02:22.340
(ضحك)

00:02:24.580 --> 00:02:28.500
الخلاصة هي، شيءٌ ما عليه تدمير
الحضارة كما نعرفها.

00:02:29.180 --> 00:02:33.476
عليكم تخيُّل كم عليه أن يكون سيئاً

00:02:33.500 --> 00:02:36.836
لكي يمنعنا من تحقيق تحسينات في
التكنولوجيا الخاصة بنا

00:02:36.860 --> 00:02:38.076
بشكل دائم،

00:02:38.100 --> 00:02:40.116
جيلاً بعد جيل.

00:02:40.140 --> 00:02:42.276
تقريباً بالتعريف، هذا أسوأ شيء

00:02:42.300 --> 00:02:44.316
يمكن أن يحصل في تاريخ البشرية.

00:02:44.340 --> 00:02:45.636
لذا البديل الوحيد،

00:02:45.660 --> 00:02:47.996
وهو الذي يوجد خلف الباب رقم اثنين،

00:02:48.020 --> 00:02:51.156
هو أن نستمر بتحسين آلاتنا الذكية

00:02:51.180 --> 00:02:52.780
سنة بعد سنة بعد سنة.

00:02:53.540 --> 00:02:57.180
وعند نقطة معينة، سنبني آلياتٍ أذكى
مما نحن عليه،

00:02:57.900 --> 00:03:00.516
وحالما يصبح لدينا آليات أذكى مننا،

00:03:00.540 --> 00:03:02.516
سوف تبدأ بتحسين نفسها.

00:03:02.540 --> 00:03:05.276
وعندها سنواجه ما دعاه عالم 
الرياضيات IJ Good

00:03:05.300 --> 00:03:07.076
"انفجاراً ذكائياً"

00:03:07.100 --> 00:03:09.100
وأن العملية يمكن أن تفلت من بين أيدينا.

00:03:09.940 --> 00:03:12.756
الآن، هذا يتم عمله كرسم كاريكاتيري، 
كما أفعل هنا

00:03:12.780 --> 00:03:15.996
كخوفٍ من جيوش ميليشيا من الرجال الآليين

00:03:16.020 --> 00:03:17.276
التي ستهاجمنا.

00:03:17.300 --> 00:03:19.996
ولكن هذا ليس السيناريو الأكثر احتمالاً.

00:03:20.020 --> 00:03:24.876
الأمر ليس أن آلياتنا ستصبح حاقدة بشكل عفوي

00:03:24.900 --> 00:03:27.516
المقلق حقاً هو أننا نبني آليات

00:03:27.540 --> 00:03:29.596
أكثر كفاءة مما نحن عليه

00:03:29.620 --> 00:03:33.396
وأن أقل اختلاف بين أهدافها وأهدافنا

00:03:33.420 --> 00:03:34.620
يمكن أن يدمرنا.

00:03:35.780 --> 00:03:37.860
فقط فكروا بكيفية علاقتنا بالنمل

00:03:38.420 --> 00:03:40.076
نحن لا نكرههم.

00:03:40.100 --> 00:03:42.156
ولا نخرج عن طريقنا لنؤذيهم.

00:03:42.180 --> 00:03:44.556
في الواقع، أحياناً نتحمل آلاماً لكي
لا نؤذيهم.

00:03:44.580 --> 00:03:46.596
نحن نمشي من فوقهم على الرصيف.

00:03:46.620 --> 00:03:48.756
ولكن عندما يكون لوجودهم

00:03:48.780 --> 00:03:51.276
تعارض جدِّي مع أحد أهدافنا،

00:03:51.300 --> 00:03:53.777
لنقل مثلاً عند إنشاء بناء كهذا،

00:03:53.801 --> 00:03:55.761
نقوم بإبادتهم بدون أي تأنيب ضمير.

00:03:56.300 --> 00:03:59.236
والقلق من أننا يوماً ما سوف نبني آليات

00:03:59.260 --> 00:04:01.996
والتي، سواءٌ كانت مدركة أم لا،

00:04:02.020 --> 00:04:04.020
قد تعاملنا بمثل هذا الاستخفاف.

00:04:05.580 --> 00:04:08.340
والآن، أنا أشك أن هذا أبعد ما يكون
عن العديد منكم.

00:04:09.180 --> 00:04:15.516
وأراهن على أن هنالك منكم من يشك أن الذكاء
الاصطناعي الخارق الذكاء ممكن،

00:04:15.540 --> 00:04:17.196
أو حتى محتوم.

00:04:17.220 --> 00:04:20.840
ولكن مع ذلك يجب عليكم أن تجدوا شيئاً ما 
خاطئاً في أحد الافتراضات التالية.

00:04:20.864 --> 00:04:22.436
وهنالك ثلاثة منهم.

00:04:23.620 --> 00:04:28.339
الذكاء هو مسألة معالجة للمعلومات في
الأنظمة الفيزيائية.

00:04:29.140 --> 00:04:31.755
في الواقع، هذا أكثر بقليل من مجرد افتراض.

00:04:31.779 --> 00:04:35.236
نحن قمنا مسبقاً ببناء ذكاء محدود
في آلياتنا،

00:04:35.260 --> 00:04:37.276
والعديد من هذه الآليات تقوم بالأداء

00:04:37.300 --> 00:04:39.940
في مستوى الذكاء البشري الخارق أصلاً.

00:04:40.660 --> 00:04:43.236
ونعلم أن المادة المجردة

00:04:43.260 --> 00:04:45.876
بإمكانها أن تعطي زيادة لما يدعى
"بالذكاء العام"،

00:04:45.900 --> 00:04:49.556
القدرة على التفكير بمرونة في عدة مجالات،

00:04:49.580 --> 00:04:52.716
لأن دماغنا قام بتدبر الأمر. صحيح؟

00:04:52.740 --> 00:04:56.676
أنا أعني، هناك ذرات فقط هنا،

00:04:56.700 --> 00:05:01.196
وطالما نستمر ببناء أنظمة من الذرات

00:05:01.220 --> 00:05:03.916
والتي تُبدي سلوكاً متزايداً أكثر
وأكثر من الذكاء

00:05:03.940 --> 00:05:06.476
فإننا في النهاية، إلا إذا تمَّت مقاطعتنا،

00:05:06.500 --> 00:05:09.876
في النهاية سوف نبني ذكاءً عاماً

00:05:09.900 --> 00:05:11.196
في آلياتنا.

00:05:11.220 --> 00:05:14.876
من المصيري أن ندرك أن معدل التطور لا يهم،

00:05:14.900 --> 00:05:18.076
لأن أي تطور هو كافٍ لإيصالنا إلى 
منطقة النهاية.

00:05:18.100 --> 00:05:21.876
لا نحتاج قانون Moore للاستمرار.
لا نحتاج التطور الأسي.

00:05:21.900 --> 00:05:23.500
نحتاج فقط لأن نستمر بالمضي قدماً

00:05:25.300 --> 00:05:28.220
الافتراض الثاني هو أننا سنستمر
بالمضي قدماً.

00:05:28.820 --> 00:05:31.580
سنستمر بتحسين آلياتنا الذكية.

00:05:32.820 --> 00:05:37.196
وبافتراض أهمية الذكاء --

00:05:37.220 --> 00:05:40.756
أنا أعني، أن الذكاء هو إما مصدر 
لكل شيءٍ نقدِّره

00:05:40.780 --> 00:05:43.556
أو أننا نحتاجه لحماية كل شيءٍ نقدُّره.

00:05:43.580 --> 00:05:45.836
إنه المنبع الأكثر أهمية لدينا.

00:05:45.860 --> 00:05:47.396
لذلك نحتاج لفعل التالي.

00:05:47.420 --> 00:05:50.756
لدينا مشاكل نحن بحاجة ماسِّة لحلها.

00:05:50.780 --> 00:05:53.980
نريد شفاء الأمراض كالزهايمر والسرطان.

00:05:54.780 --> 00:05:58.716
نريد أن نفهم الأنظمة الاقتصادية.
نريد تحسين علم المناخ الخاص بنا.

00:05:58.740 --> 00:06:00.996
لذلك سنفعل ذلك، إذا استطعنا.

00:06:01.020 --> 00:06:04.306
القطار غادر المحطة بالفعل، ولا
يوجد مكابح لشدها.

00:06:05.700 --> 00:06:11.156
وأخيراً، نحن لا نقف عند قمة الذكاء،

00:06:11.180 --> 00:06:12.980
أو في أي مكان قريب منها، على ما يبدو،

00:06:13.460 --> 00:06:15.356
وهذا حقاً هو البصيرة المصيرية.

00:06:15.380 --> 00:06:17.796
هذا ما يجعل موقفنا متزعزعاً للغاية،

00:06:17.820 --> 00:06:21.860
وهذا ما يجعل حدسنا بما يخص
الخطورة لا يُعتمد عليه أبداً.

00:06:22.940 --> 00:06:25.660
والآن، فكروا فقط بأذكى شخص
عاش في أي وقتٍ مضى.

00:06:26.460 --> 00:06:29.876
تقريباً على قائمة الجميع القصيرة 
يوجد John von Neumann.

00:06:29.900 --> 00:06:33.236
أعني، الانطباع الذي تركه von Neumann 
على الناس المحيطين به،

00:06:33.260 --> 00:06:37.316
وهذا يشمل أعظم علماء الرياضيات 
والفيزياء في عصره،

00:06:37.340 --> 00:06:39.276
بالكاد تم توثيقه بشكل جيد.

00:06:39.300 --> 00:06:43.076
إذا كانت نصف القصص فقط عنه نصف صحيحة،

00:06:43.100 --> 00:06:44.316
فلا يوجد شك

00:06:44.340 --> 00:06:46.796
أنه أحد أذكى الأشخاص الذين عاشوا
في أي وقتٍ مضى.

00:06:46.820 --> 00:06:49.340
لذلك باعتبار طيف الذكاء.

00:06:50.140 --> 00:06:51.569
هنا لدينا John von Neumann.

00:06:53.380 --> 00:06:54.714
وهنا لدينا أنتم وأنا.

00:06:55.940 --> 00:06:57.236
وثم هنا لدينا دجاجة.

00:06:57.260 --> 00:06:59.196
(ضحك)

00:06:59.220 --> 00:07:00.436
آسف، دجاجة.

00:07:00.460 --> 00:07:01.716
(ضحك)

00:07:01.740 --> 00:07:05.476
لا يوجد سبب يجعلني أجعل هذه المحادثة أكثر
إحباطاً مما يحتاجه الأمر.

00:07:05.500 --> 00:07:07.100
(ضحك)

00:07:08.159 --> 00:07:11.636
يبدو أنه ساحق على الأرجح، ومع ذلك، يمتد
طيف الذكاء

00:07:11.660 --> 00:07:14.780
أبعد بكثير مما نتصور حالياً،

00:07:15.700 --> 00:07:18.916
وإذا قمنا ببناء آلات أذكى مما نحن عليه،

00:07:18.940 --> 00:07:21.236
فهي على الأرجح ستستكشف الطيف

00:07:21.260 --> 00:07:23.116
بطرقٍ لا نستطيع تخيلها،

00:07:23.140 --> 00:07:25.660
وسف تتعدانا بطرقٍ لا يمكننا تخيلها.

00:07:26.820 --> 00:07:31.156
ومن المهم أن ندرك أن هذا صحيح بخصوص 
تأثير السرعة وحدها.

00:07:31.180 --> 00:07:36.236
صحيح؟ إذاً تخيلوا إذا بنينا ذكاءً صنعياً 
خارق الذكاء

00:07:36.260 --> 00:07:39.716
ولم يكن أذكى من فريق الباحثين العادي
الخاص بكم

00:07:39.740 --> 00:07:42.036
في ستانفورد أو معهد ماساتشوستس
للتكنولوجيا.

00:07:42.060 --> 00:07:45.036
حسناً، الدارات الالكترونية تعمل بشكل أسرع 
بحوالي ملايين المرات

00:07:45.060 --> 00:07:46.316
من تلك الكيميائية الحيوية،

00:07:46.340 --> 00:07:49.476
لذا هذه الآلة يجب أن تفكر أسرع بملايين
المرات

00:07:49.500 --> 00:07:51.316
من العقول التي قامت ببنائها.

00:07:51.340 --> 00:07:52.996
لذا إذا تركتها تعمل لمدة أسبوع،

00:07:53.020 --> 00:07:57.580
فسوف تؤدي ما يعادل 20,000 سنة من
العمل بمستوى الذكاء البشري،

00:07:58.220 --> 00:08:00.180
أسبوع بعد أسبوع بعد أسبوع.

00:08:01.460 --> 00:08:04.556
كيف بإمكاننا حتى أن نفهم،
أو حتى أن نُقيّد،

00:08:04.580 --> 00:08:06.860
عقلاً يصنع هذا النوع من التطور؟

00:08:08.660 --> 00:08:10.796
والأمر الآخر المقلق، بصراحة،

00:08:10.820 --> 00:08:15.796
هو أن، تخيل أفضل سيناريو.

00:08:15.820 --> 00:08:19.996
إذاً تخيلوا أننا اكتشفنا تصميماً للذكاء
الصنعي فائق الذكاء

00:08:20.020 --> 00:08:21.396
وليس لديه مخاوف سلامة.

00:08:21.420 --> 00:08:24.676
لدينا التصميم المثالي للمرة الأولى.

00:08:24.700 --> 00:08:26.916
وكأنه تم تسليمنا نبوءة

00:08:26.940 --> 00:08:28.956
تتصرَّف تماماً كما ينبغي عليها.

00:08:28.980 --> 00:08:32.700
هذه الآلة ستكون المثلى لتوفير العمالة.

00:08:33.500 --> 00:08:35.929
بإمكانها تصميم آلية ستقوم ببناء الآلة

00:08:35.953 --> 00:08:37.716
التي بإمكانها القيام بأي عمل فيزيائي،

00:08:37.740 --> 00:08:39.196
تعمل بضوء الشمس،

00:08:39.220 --> 00:08:41.916
ومهما يكن من أجل كلفة المواد الخام.

00:08:41.940 --> 00:08:45.196
إذاً نحن نتكلم عن نهاية الكدح البشري.

00:08:45.220 --> 00:08:48.020
نحن أيضا نتكلم عن نهاية العمل الذكائي.

00:08:49.020 --> 00:08:52.076
فماذا قد تفعل القرود أمثالنا في هذا الظرف؟

00:08:52.100 --> 00:08:56.180
حسناً، سنكون متفرغين للعب بالصحن الطائر 
وتبادل الرسائل مع بعضنا.

00:08:57.660 --> 00:09:00.516
أضف بعض الـLSD
مع اختيارات اللباس المشكوك بأمرها،

00:09:00.540 --> 00:09:02.716
وبإمكان العالم كله أن يصبح
كفيلم Burning Man.

00:09:02.740 --> 00:09:04.380
(ضحك)

00:09:06.140 --> 00:09:08.140
حسناً، هذا قد يبدو جيداً جداً،

00:09:09.100 --> 00:09:11.476
ولكن اسألوا أنفسكم ماذا قد يسبب هذا

00:09:11.500 --> 00:09:14.236
لاقتصادنا الحالي ووضعنا السياسي؟

00:09:14.260 --> 00:09:16.676
يبدو أننا سنشهد على الأرجح

00:09:16.700 --> 00:09:20.836
مستوى من تفاوت الثروات والبطالة

00:09:20.860 --> 00:09:22.356
الذي لم نشهده أبداً من قبل

00:09:22.380 --> 00:09:24.996
غياب الرغبة لوضع هذه الثروة الجديدة فوراً

00:09:25.020 --> 00:09:26.500
في خدمة جميع البشرية،

00:09:27.460 --> 00:09:31.076
بعض أصحاب التريليونات سوف يُشرِّفون
أغلفة مجلات أعمالنا

00:09:31.100 --> 00:09:33.540
بينما يكون باقي العالم متفرغاً لكي 
يتضور جوعاً.

00:09:34.140 --> 00:09:36.436
وماذا سيفعل الروس أو الصينيون

00:09:36.460 --> 00:09:39.076
إذا سمعوا أن شركةً ما في Silicon Valley

00:09:39.100 --> 00:09:41.836
كانت على وشك نشر الذكاء الا 
فائق الذكاء؟

00:09:41.860 --> 00:09:44.716
هذه الآلة ستكون قادرة على شن حرب،

00:09:44.740 --> 00:09:46.956
سواء كانت على الأرض أو على الإنترنت،

00:09:46.980 --> 00:09:48.660
بقوة غير مسبوقة.

00:09:49.940 --> 00:09:51.796
هذا السيناريو هو الذي يربح كل شيء

00:09:51.820 --> 00:09:54.956
أن تكون متقدماً بستة أشهر على
المنافسة في هذه الحالة

00:09:54.980 --> 00:09:57.756
تعني أن تكون متقدماً بـ500,000 سنة،

00:09:57.780 --> 00:09:59.276
بأقل حد.

00:09:59.300 --> 00:10:04.036
وبالتالي يبدو أنه حتى الإشاعات المحضة 
بما يخص النوع من الاكتشافات

00:10:04.060 --> 00:10:06.436
يمكن أن تجعل فصائلنا تهتاج.

00:10:06.460 --> 00:10:09.356
والآن، أكثر الأشياء إخافة،

00:10:09.380 --> 00:10:12.156
من وجهة نظري، في هذه اللحظة،

00:10:12.180 --> 00:10:16.476
هي النوع من الأشياء التي يقولها باحثو
الذكاء الاصطناعي عندما

00:10:16.500 --> 00:10:18.060
يريدون أن يكونوا مُطمئِنين.

00:10:18.820 --> 00:10:22.276
والسبب الأكثر شيوعاً الذي يُقال لنا
ألا نقلق بسببه هو الوقت

00:10:22.300 --> 00:10:24.356
هذا كله سيمر، ألا تعلم.

00:10:24.380 --> 00:10:26.820
هذا من المحتمل بعد 50 أو 100 سنة.

00:10:27.540 --> 00:10:28.796
قال أحد الباحثين،

00:10:28.820 --> 00:10:30.396
"القلق بشأن سلامة
الذكاء الاصطناعي

00:10:30.420 --> 00:10:32.700
هو كالقلق بشأن زيادة
التعداد السكاني على المريخ"

00:10:33.936 --> 00:10:35.556
هذه نسخة Silicon Valley

00:10:35.580 --> 00:10:37.956
من "لا تُقلق رأسك الجميل الصغير بهذا".

00:10:37.980 --> 00:10:39.316
(ضحك)

00:10:39.340 --> 00:10:41.236
لا يبدو أن أحداً يلاحظ

00:10:41.260 --> 00:10:43.876
أن الإشارة للحد الزمني

00:10:43.900 --> 00:10:46.476
ليس له عواقب بتاتاً.

00:10:46.500 --> 00:10:49.756
إذا كان الذكاء هو مجرد مسألة معالجة
معلومات،

00:10:49.780 --> 00:10:52.436
ونحن نستمر بتحسين آلياتنا،

00:10:52.460 --> 00:10:55.340
فسنقوم بإنتاج شكلٍ ما من الذكاء الفائق.

00:10:56.140 --> 00:10:59.796
وليس لدينا فكرة كم سيستغرق هذا مننا

00:10:59.820 --> 00:11:02.220
لخلق الشروط للقيام بذلك بأمان.

00:11:04.020 --> 00:11:05.316
دعوني أقل ذلك مرة أخرى.

00:11:05.340 --> 00:11:09.156
ليس لدينا فكرة كم سيستغرق الأمر مننا

00:11:09.180 --> 00:11:11.420
لخلق الشروط للقيام بذلك بأمان.

00:11:12.740 --> 00:11:16.196
وإذا لم تلاحظوا،
لم تعد الخمسون سنة كما كانت عليه.

00:11:16.220 --> 00:11:18.676
هذه 50 سنة في أشهر.

00:11:18.700 --> 00:11:20.540
هذه المدة التي امتلكنا فيها الـiPhone

00:11:21.260 --> 00:11:23.860
هذه المدة التي انعرض فيها
The Simpsons على التلفاز

00:11:24.500 --> 00:11:26.876
خمسون سنة هي ليست بالوقت الكثير

00:11:26.900 --> 00:11:30.060
لمواجهة أحد أكبر التحديات الذي سيواجه
لفصيلتنا.

00:11:31.460 --> 00:11:35.476
مرة أخرى، يبدو أننا نفشل في امتلاك
الاستجابة العاطفية الملائمة

00:11:35.500 --> 00:11:38.196
لما يوجد لدينا أكثر من سبب للإيمكان
بأنه قادم

00:11:38.220 --> 00:11:42.196
عالم الحاسوب Stuart Russell
لديه تحليل لطيف هنا.

00:11:42.220 --> 00:11:47.116
هو يقول، تخيلوا أننا استلمنا رسالة من
حضارة الفضائيين،

00:11:47.140 --> 00:11:48.836
والتي كانت:

00:11:48.860 --> 00:11:50.396
"سكان الأرض،

00:11:50.420 --> 00:11:52.780
سوف نصل إلى كوكبكم خلال 50 سنة.

00:11:53.620 --> 00:11:55.196
استعدوا."

00:11:55.220 --> 00:11:59.476
والآن نقوم بالعد التنازلي للأشهر حتى تهبط
السفينة الأم؟

00:11:59.500 --> 00:12:02.500
سنشعر بالعجلة أكثر مما نفعل الآن بقليل.

00:12:04.500 --> 00:12:06.356
يُقال لنا ألا نقلق لسببٍ آخر

00:12:06.380 --> 00:12:09.396
هو أن هذه الآليات ليس بوسعها
إلا مشاركتنا منافعها

00:12:09.420 --> 00:12:12.036
لأنها ستكون حرفياً امتداداً لأنفسنا.

00:12:12.060 --> 00:12:13.876
ستكون مُطعَّمة مع أدمغتنا،

00:12:13.900 --> 00:12:16.260
ونحن سنصبح جوهرياً أنظمتها الحوفية.

00:12:16.940 --> 00:12:18.356
الآن خذوا لحظة للتفكير

00:12:18.380 --> 00:12:21.556
أن الطريق الوحيد الآمن والحكيم للأمام،

00:12:21.580 --> 00:12:22.916
الذي يُنصح به،

00:12:22.940 --> 00:12:25.740
هو زرع هذه التكنولوجيا مباشرةً في أدمغتنا.

00:12:26.420 --> 00:12:29.796
قد يكون الطريق الوحيد
الآمن والحكيم للأمام،

00:12:29.820 --> 00:12:32.876
ولكن عادةً يكون للمرء مخاوف
عن كون التكنولوجيا

00:12:32.900 --> 00:12:36.556
مفهومة لحدٍّ ما قبل أن تدخلها داخل رأسك.

00:12:36.580 --> 00:12:38.596
(ضحك)

00:12:38.620 --> 00:12:43.956
المشكلة الأعمق أن بناء ذكاء اصطناعي فائق
الذكاء بمفرده

00:12:43.980 --> 00:12:45.716
يبدو على الأرجح أنه أسهل

00:12:45.740 --> 00:12:47.596
من بناء ذكاء اصطناعي فائق
الذكاء

00:12:47.620 --> 00:12:49.396
وامتلاك علم الأعصاب الكامل

00:12:49.420 --> 00:12:52.100
الذي يسمح لنا بمكاملة عقولنا معه بسلاسة.

00:12:52.620 --> 00:12:55.796
وبافتراض أن الشركات والحكومات
التي تقوم بهذا العمل

00:12:55.820 --> 00:12:59.476
على الأرجح ستعي كونها في سباق ضد 
الآخرين جميعاً،

00:12:59.500 --> 00:13:02.756
وبافتراض أن الفوز في هذا السباق
هو الفوز بالعالم،

00:13:02.780 --> 00:13:05.236
بفرض أنك لن تدمره في الشهر التالي،

00:13:05.260 --> 00:13:07.876
فبالتالي يبدو على الأرجح أنه مهما يكن
الأسهل فعله

00:13:07.900 --> 00:13:09.100
سيتم الانتهاء منه أولاً.

00:13:10.380 --> 00:13:13.236
الآن، لسوء الحظ، لا أمتلك حلاً لهذه
المشكلة،

00:13:13.260 --> 00:13:15.876
بمعزل عن توصية أن يفكِّر المزيد مننا به.

00:13:15.900 --> 00:13:18.276
أظن أننا بحاجة شيء ما مثل "مشروع مانهاتن"

00:13:18.300 --> 00:13:20.316
لموضوع الذكاء الاصطناعي.

00:13:20.340 --> 00:13:23.076
ليس لبنائه، لأنني أظننا سنفعل ذلك حتماً،

00:13:23.100 --> 00:13:26.436
ولكن لفهم كيفية تجنب سباق الأذرع

00:13:26.460 --> 00:13:29.956
ولبنائه بطريقة منحازة مع اهتماماتنا.

00:13:29.980 --> 00:13:32.116
عندما تتحدث عن ذكاء اصطناعي فائق
الذكاء

00:13:32.140 --> 00:13:34.396
يمكنه إحداث تغيير لنفسه،

00:13:34.420 --> 00:13:39.036
يبدو أن لدينا فرصة واحدة للقيام بالشروط 
الابتدائية بشكل صحيح،

00:13:39.060 --> 00:13:41.116
وحتى حينها سنكون بحاجة امتصاص

00:13:41.140 --> 00:13:44.180
العواقب الاقتصادية والسياسية من القيام
بها بشكل صحيح.

00:13:45.580 --> 00:13:47.636
ولكن في اللحظة التي نعترف فيها

00:13:47.660 --> 00:13:51.660
أن معالجة المعلومات هي مصدر الذكاء،

00:13:52.540 --> 00:13:57.340
وأن بعض أنظمة الحاسوب الملائمة
تُعبِّر عن أساس الذكاء،

00:13:58.180 --> 00:14:01.940
ونعترف أننا سوف نحسِّن هذه الأنظمة 
باستمرار،

00:14:03.100 --> 00:14:07.556
ونعترف أن أفق الإدراك على الأرجح يتجاوز

00:14:07.580 --> 00:14:08.780
الذي نعرفه حالياً بكثير،

00:14:09.940 --> 00:14:11.156
وبالتالي علينا الاعتراف

00:14:11.180 --> 00:14:13.820
أننا في مرحلة بناء نوع ما من الآلهة.

00:14:15.220 --> 00:14:16.796
الآن سيكون وقتاً جيداً

00:14:16.820 --> 00:14:18.773
للتأكد من أنه إله نستطيع العيش معه.

00:14:19.940 --> 00:14:21.476
شكراً جزيلاً لكم.

00:14:21.500 --> 00:14:26.593
(تصفيق)

