WEBVTT
Kind: captions
Language: sr

00:00:00.000 --> 00:00:07.000
Prevodilac: Vesna Radovic
Lektor: Mile Živković

00:00:13.000 --> 00:00:15.216
Pričaću vam o porazu intuicije

00:00:15.240 --> 00:00:16.840
od koga mnogi patimo.

00:00:17.480 --> 00:00:20.520
Zista je porazno opaziti
određenu vrstu opasnosti.

00:00:21.360 --> 00:00:23.096
Opisaću vam jedan scenario

00:00:23.120 --> 00:00:26.376
za koji mislim da je i zastrašujuć

00:00:26.400 --> 00:00:28.160
i koji se realno može dogoditi,

00:00:28.840 --> 00:00:30.496
a to, kako se čini,

00:00:30.520 --> 00:00:32.056
nije dobra kombinacija.

00:00:32.080 --> 00:00:34.536
A opet umesto da se uplaše,
mnogi od vas će misliti

00:00:34.560 --> 00:00:36.640
da je ono o čemu pričam nešto kul.

00:00:37.200 --> 00:00:40.176
Opisaću vam kako bi
napredak koji pravimo

00:00:40.200 --> 00:00:41.976
u veštačkoj inteligenciji

00:00:42.000 --> 00:00:43.776
mogao na kraju da nas uništi.

00:00:43.800 --> 00:00:47.256
Zapravo, mislim da je jako teško
zamisliti kako nas neće uništiti,

00:00:47.280 --> 00:00:48.960
ili navesti da se samouništimo.

00:00:49.400 --> 00:00:51.256
A opet ako ste imalo kao ja,

00:00:51.280 --> 00:00:53.936
shvatićete da je zabavno
razmišljati o ovim stvarima.

00:00:53.960 --> 00:00:57.336
I taj odgovor je deo ovog problema.

00:00:57.360 --> 00:00:59.080
OK? Taj odgovor treba da vas brine.

00:00:59.920 --> 00:01:02.576
Kad bih vas u ovom govoru ubedio

00:01:02.600 --> 00:01:06.016
da ćemo vrlo verovatno
globalno gladovati,

00:01:06.040 --> 00:01:09.096
bilo zbog klimatskih promena
ili neke druge katastrofe,

00:01:09.120 --> 00:01:12.536
i da će vaši unuci, ili njihovi unuci,

00:01:12.560 --> 00:01:14.360
vrlo verovatno živeti ovako,

00:01:15.200 --> 00:01:16.400
ne biste mislili:

00:01:17.440 --> 00:01:18.776
"Zanimljivo.

00:01:18.800 --> 00:01:20.251
Dobar ovaj TED Talk."

00:01:21.200 --> 00:01:22.720
Glad nije zabavna.

00:01:23.800 --> 00:01:27.176
Smrt u naučnoj fantastici, 
sa druge strane, jeste zabavna,

00:01:27.200 --> 00:01:31.176
i nešto što me najviše brine
u ovom trenutku u vezi sa razvojem VI

00:01:31.200 --> 00:01:35.296
je što izgleda da ne možemo da pružimo
odgovarajući emocionalni odgovor

00:01:35.320 --> 00:01:37.136
na opasnosti koje dolaze.

00:01:37.160 --> 00:01:40.360
Ni ja ne mogu da pružim ovaj odgovor,
a držim ovaj govor.

00:01:42.120 --> 00:01:44.816
To je kao da stojimo ispred dvoja vrata.

00:01:44.840 --> 00:01:46.096
Iza vrata broj jedan,

00:01:46.120 --> 00:01:49.416
zaustavljamo napredak
u pravljenju inteligentnih mašina.

00:01:49.440 --> 00:01:53.456
Kompjuterski hardver i softver prosto
prestaju da budu bolji iz nekog razloga.

00:01:53.480 --> 00:01:56.480
Sada na trenutak razmislite
zašto bi se ovo moglo desiti.

00:01:57.080 --> 00:02:00.736
Mislim, s obzirom koliku vrednost
imaju inteligencija i automatizacija,

00:02:00.760 --> 00:02:04.280
nastavićemo da unapređujemo tehnologiju
ako smo uopšte kadri za to.

00:02:05.200 --> 00:02:06.867
Šta bi nas moglo sprečiti u tome?

00:02:07.800 --> 00:02:09.600
Svetski nuklearni rat?

00:02:11.000 --> 00:02:12.560
Globalna pandemija?

00:02:14.320 --> 00:02:15.640
Pad asteroida?

00:02:17.640 --> 00:02:20.216
Ako Džastin Biber postane
predsednik Amerike?

00:02:20.240 --> 00:02:22.520
(Smeh)

00:02:24.760 --> 00:02:28.680
Poenta je u tome, da bi nešto moralo
da uništi civilizaciju ovakvu kakva je.

00:02:29.360 --> 00:02:33.656
Morate zamisliti kako
bi zaista loše moralo biti to

00:02:33.680 --> 00:02:37.016
što bi nas trajno sprečilo
da unapređujemo,

00:02:37.040 --> 00:02:38.256
tehnologiju,

00:02:38.280 --> 00:02:40.296
generaciju za generacijom.

00:02:40.320 --> 00:02:42.456
Skoro po definiciji, ovo je najgora stvar

00:02:42.480 --> 00:02:44.496
koja se ikada desila u ljudskoj istoriji.

00:02:44.520 --> 00:02:45.816
Pa je jedina alternativa,

00:02:45.840 --> 00:02:48.176
a ovo se nalazi iza vrata broj dva,

00:02:48.200 --> 00:02:51.336
da nastavimo da unapređujemo
naše inteligentne mašine

00:02:51.360 --> 00:02:52.960
bez prestanka.

00:02:53.720 --> 00:02:57.360
U jednom trenutku, napravićemo
mašine koje su pametnije od nas,

00:02:58.080 --> 00:03:00.696
i tad kad budemo imali mašine
koje su pametnije od nas,

00:03:00.720 --> 00:03:02.696
one će same početi da se razvijaju.

00:03:02.720 --> 00:03:05.456
I onda rizikujemo ono
što je matematičar I.Dž. Gud nazvao

00:03:05.480 --> 00:03:07.256
"eksplozija inteligencije",

00:03:07.280 --> 00:03:09.280
da proces može da nam pobegne.

00:03:10.120 --> 00:03:12.936
E, sad, ovo se često karikira,
kao što ću i ja uraditi,

00:03:12.960 --> 00:03:16.176
kao strah od napada armije

00:03:16.200 --> 00:03:17.456
zlih robota.

00:03:17.480 --> 00:03:20.176
Ali to nije najverovatniji scenario.

00:03:20.200 --> 00:03:25.056
Nije kao da će naše mašine
spontano postati zle.

00:03:25.080 --> 00:03:27.696
Brine nas, zapravo,
da ćemo mi napraviti mašine

00:03:27.720 --> 00:03:29.776
koje su mnogo, mnogo
kompetentnije od nas,

00:03:29.800 --> 00:03:33.576
brinemo da će nas uništiti
i najmanja razlika između

00:03:33.600 --> 00:03:34.800
naših i njihovih ciljeva.

00:03:35.960 --> 00:03:38.040
Zamislite kakvi smo mi prema mravima.

00:03:38.600 --> 00:03:40.256
Ne mrzimo ih.

00:03:40.280 --> 00:03:42.336
Ne povređujemo ih namerno.

00:03:42.360 --> 00:03:44.736
Zapravo, ponekad se
trudimo se da ih ne povredimo.

00:03:44.760 --> 00:03:46.776
Preskačemo ih na trotoaru.

00:03:46.800 --> 00:03:48.936
Ali kada god njihovo prisustvo

00:03:48.960 --> 00:03:51.456
ozbiljno ugrozi neke naše ciljeve,

00:03:51.480 --> 00:03:53.957
recimo kada konstruišemo
neku zgradu poput ove,

00:03:53.981 --> 00:03:55.941
bez po muke ih istrebimo.

00:03:56.480 --> 00:03:59.416
Brine nas da ćemo
jednog dana napraviti mašine

00:03:59.440 --> 00:04:02.176
koje će, bilo da su svesne ili ne,

00:04:02.200 --> 00:04:04.200
nas tretirati na isti način.

00:04:05.760 --> 00:04:08.520
E, sad, cenim da većini vas
ovo zvuči kao daleka budućnost.

00:04:09.360 --> 00:04:15.696
Kladim se da ima onih koji ne veruju
da je superinteligentna VI moguća,

00:04:15.720 --> 00:04:17.376
a kamoli neizbežna.

00:04:17.400 --> 00:04:21.020
Ali onda morate naći neku grešku
u sledećim pretpostavkama.

00:04:21.044 --> 00:04:22.616
A samo ih je tri.

00:04:23.800 --> 00:04:28.519
Inteligencija je stvar obrade
informacija u fizičkim sistemima.

00:04:29.320 --> 00:04:31.935
U stvari, ovo je malo više
od neke pretpostavke.

00:04:31.959 --> 00:04:35.416
Već smo ugradili specifičnu
inteligenciju u naše mašine,

00:04:35.440 --> 00:04:37.456
i mnoge od ovih mašina već

00:04:37.480 --> 00:04:40.120
imaju performanse na nivou
superljudske inteligencije.

00:04:40.840 --> 00:04:43.416
I znamo da sama materija

00:04:43.440 --> 00:04:46.056
može da omogući takozvanu
"uopštenu inteligenciju",

00:04:46.080 --> 00:04:49.736
sposobnost da se fleksibilno
razmišlja u različitim domenima,

00:04:49.760 --> 00:04:52.896
jer to može i naš mozak. Je l' tako?

00:04:52.920 --> 00:04:56.856
Mislim, ovde postoje samo atomi,

00:04:56.880 --> 00:05:01.376
i sve dok mi nastavljamo
da gradimo sisteme atoma

00:05:01.400 --> 00:05:04.096
koji su sve inteligentniji
i inteligentniji,

00:05:04.120 --> 00:05:06.656
mi ćemo na kraju, osim ako nas prekinu,

00:05:06.680 --> 00:05:10.056
mi ćemo na kraju
ugraditi uopštenu inteligenciju

00:05:10.080 --> 00:05:11.376
u naše mašine.

00:05:11.400 --> 00:05:15.056
Od suštinske je važnosti da shvatimo
da stopa napretka nije važna,

00:05:15.080 --> 00:05:18.256
jer bilo kakav napredak
je dovoljan da nas dovede do cilja.

00:05:18.280 --> 00:05:22.056
Ne treba nam Murov zakon.
Ne treba nam eksponencijalni napredak.

00:05:22.080 --> 00:05:23.680
Samo treba da nastavimo dalje.

00:05:25.480 --> 00:05:28.400
Druga pretpostavka je
da ćemo nastaviti dalje.

00:05:29.000 --> 00:05:31.760
Nastavićemo da poboljšavamo
naše inteligentne mašine.

00:05:33.000 --> 00:05:37.376
I uz postojeću vrednost inteligencije -

00:05:37.400 --> 00:05:40.936
mislim, inteligencija je
ili izvor svega što je vredno

00:05:40.960 --> 00:05:43.736
ili nam treba da sačuvamo
sve ono što nam je vredno.

00:05:43.760 --> 00:05:46.016
To je naš najvredniji resurs.

00:05:46.040 --> 00:05:47.576
I zato želimo da uradimo ovo.

00:05:47.600 --> 00:05:50.936
Imamo probleme koje
očajnički moramo da rešimo.

00:05:50.960 --> 00:05:54.160
Želimo da izlečimo bolesti poput
Alchajmera i raka.

00:05:54.960 --> 00:05:58.896
Želimo da shvatimo ekonomske sisteme.
Želimo da poboljšamo nauku o klimi.

00:05:58.920 --> 00:06:01.176
Tako da, ako možemo, to ćemo i uraditi.

00:06:01.200 --> 00:06:04.486
Voz je već napustio stanicu,
i nema kočnice.

00:06:05.880 --> 00:06:11.336
Konačno, mi ne stojimo
na vrhuncu inteligencije,

00:06:11.360 --> 00:06:13.160
a verovatno ni blizu toga.

00:06:13.640 --> 00:06:15.536
I ovo je zaista suštinski uvid.

00:06:15.560 --> 00:06:17.976
Zbog ovoga je naša
situacija tako nepouzdana,

00:06:18.000 --> 00:06:22.040
i zbog toga je naša intuicija
o riziku tako nepouzdana.

00:06:23.120 --> 00:06:25.840
E, sad, samo razmislite
o najpametnijoj osobi ikada.

00:06:26.640 --> 00:06:30.056
Na skoro svačijem užem spisku ovde
je Džon fon Nojman.

00:06:30.080 --> 00:06:33.416
Mislim, utisak koji je Nojman
ostavio na ljude oko sebe,

00:06:33.440 --> 00:06:37.496
uključujući tu najveće
matematičare i fizičare tog vremena,

00:06:37.520 --> 00:06:39.456
je prilično dobro dokumentovan.

00:06:39.480 --> 00:06:43.256
I ako je samo pola onih
priča o njemu polutačna,

00:06:43.280 --> 00:06:44.496
nema dileme da je on

00:06:44.520 --> 00:06:46.976
jedan od najpametnijih ljudi
koji su ikada živeli.

00:06:47.000 --> 00:06:49.520
Samo zamislite opseg inteligencije.

00:06:50.320 --> 00:06:51.749
Ovde imamo Džona fon Nojmana.

00:06:53.560 --> 00:06:54.894
Onda smo tu vi i ja.

00:06:56.120 --> 00:06:57.416
I onda imamo neko pile.

00:06:57.440 --> 00:06:59.376
(Smeh)

00:06:59.400 --> 00:07:00.616
Pardon, neko pile.

00:07:00.640 --> 00:07:01.896
(Smeh)

00:07:01.920 --> 00:07:05.656
Nema potrebe da idem dalje u dubiozu
u ovom govoru nego što treba.

00:07:05.680 --> 00:07:07.280
(Smeh)

00:07:08.339 --> 00:07:11.816
Međutim, čini se da je više nego verovatno
da se spektar inteligencije

00:07:11.840 --> 00:07:14.960
proteže mnogo više
nego što trenutno možemo da shvatimo,

00:07:15.880 --> 00:07:19.096
i ako napravimo mašine
koje su mnogo inteligentnije od nas,

00:07:19.120 --> 00:07:21.416
one će skoro sigurno
istražiti ovaj spektar

00:07:21.440 --> 00:07:23.296
onako kako to ne možemo ni zamisliti,

00:07:23.320 --> 00:07:25.840
i preteći nas na načine
koje ne možemo ni zamisliti.

00:07:27.000 --> 00:07:31.336
I važno je shvatiti da je ovo
moguće samo putem brzine.

00:07:31.360 --> 00:07:36.416
U redu? Zamislite samo da napravimo
jednu superinteligentnu VI

00:07:36.440 --> 00:07:39.896
koja nije ništa pametnija
od vašeg prosečnog tima istraživača

00:07:39.920 --> 00:07:42.216
na Stanfordu ili MIT-u.

00:07:42.240 --> 00:07:45.216
Pa, električna kola
funkcionišu oko milion puta brže

00:07:45.240 --> 00:07:46.496
od biohemijskih,

00:07:46.520 --> 00:07:49.656
tako da bi ove mašine razmišljale
oko milion puta brže

00:07:49.680 --> 00:07:51.496
od umova koje su ih sastavile.

00:07:51.520 --> 00:07:53.176
Pustite ih da rade nedelju dana,

00:07:53.200 --> 00:07:57.760
i izvešće 20.000 godina
ljudskog intelektualnog rada,

00:07:58.400 --> 00:08:00.360
nedelju za nedeljom.

00:08:01.640 --> 00:08:04.736
Kako čak možemo i razumeti,
a kamoli ograničiti

00:08:04.760 --> 00:08:07.040
um koji bi mogao ovo da uradi?

00:08:08.840 --> 00:08:10.976
Iskreno, ono drugo što je zabrinjavajuće

00:08:11.000 --> 00:08:15.976
je da, zamislite najbolji mogući scenario.

00:08:16.000 --> 00:08:20.176
Zamislite da nabasamo na neki dizajn
superinteligentne VI

00:08:20.200 --> 00:08:21.576
čija nas sigurnost ne brine.

00:08:21.600 --> 00:08:24.856
Po prvi put imamo savršeni dizajn.

00:08:24.880 --> 00:08:27.096
To je kao da nam je dato proročanstvo

00:08:27.120 --> 00:08:29.136
koje se odvija baš kako je zapisano.

00:08:29.160 --> 00:08:32.880
Ta mašina bi bila savršeni
uređaj za uštedu radne snage.

00:08:33.680 --> 00:08:36.109
Dizajnirala bi mašinu
koja može napraviti mašine

00:08:36.133 --> 00:08:37.896
za bilo kakav fizički posao,

00:08:37.920 --> 00:08:39.376
a koje bi napajalo sunce,

00:08:39.400 --> 00:08:42.096
manje-više po ceni sirovina.

00:08:42.120 --> 00:08:45.376
Pričamo o kraju teškog ljudskog rada.

00:08:45.400 --> 00:08:48.200
Takođe pričamo o kraju
većeg dela intelektualnog rada.

00:08:49.200 --> 00:08:52.256
Pa šta će onda majmuni poput nas
da rade u takvim okolnostima?

00:08:52.280 --> 00:08:56.360
Pa, imali bismo vremena za frizbi
i da masiramo jedni druga.

00:08:57.840 --> 00:09:00.696
Dodajmo malo LSD-a i neke
diskutabilne garderobe,

00:09:00.720 --> 00:09:03.366
i ceo svet bi mogao
biti poput festivala Burning Man.

00:09:03.390 --> 00:09:05.030
(Smeh)

00:09:06.320 --> 00:09:08.320
E, sad, to možda zvuči kao dobra ideja,

00:09:09.280 --> 00:09:11.656
ali zapitajte se šta bi se desilo

00:09:11.680 --> 00:09:14.416
pod našim trenutnim ekonomskim
i političkim poretkom?

00:09:14.440 --> 00:09:16.856
Čini se verovatnim da bismo bili svedoci

00:09:16.880 --> 00:09:21.016
određenog nivoa novčane nejednakosti
i nezaposlenosti

00:09:21.040 --> 00:09:22.536
koji ranije nismo viđali.

00:09:22.560 --> 00:09:25.176
Uz odsustvo volje da
se odmah ovo novo bogatstvo stavi

00:09:25.200 --> 00:09:26.680
na raspolaganje čovečanstvu,

00:09:27.640 --> 00:09:31.256
nekoliko bilionera bi poziralo
na omotima biznis magazina

00:09:31.280 --> 00:09:33.720
dok bi ostatak sveta slobodno
mogao umreti od gladi.

00:09:34.320 --> 00:09:36.616
I šta bi radili Rusi ili Kinezi

00:09:36.640 --> 00:09:39.256
kada bi čuli da tamo neka firma
iz Silicijumske doline

00:09:39.280 --> 00:09:42.016
treba da izbaci superinteligentnu VI?

00:09:42.040 --> 00:09:44.896
Ova mašina bi bila sposobna da vodi rat,

00:09:44.920 --> 00:09:47.136
bilo klasični, bilo sajber rat,

00:09:47.160 --> 00:09:48.840
nadmoćna bez presedana.

00:09:50.120 --> 00:09:51.976
Ovo je scenario pobednik nosi sve.

00:09:52.000 --> 00:09:55.136
Biti šest meseci ispred konkurencije ovde

00:09:55.160 --> 00:09:57.936
je isto biti 500 000 godina ispred,

00:09:57.960 --> 00:09:59.456
u najmanju ruku.

00:09:59.480 --> 00:10:04.216
Tako se čini da bi čak i samo glasina
o proboju ovog tipa

00:10:04.240 --> 00:10:06.616
izazvala ludnicu među ljudima.

00:10:06.640 --> 00:10:09.536
I, sad, jedna od najstarašnijih stvari,

00:10:09.560 --> 00:10:12.336
prema mom mišljenju, trenutno,

00:10:12.360 --> 00:10:16.656
su one stvari koje istraživači VI kažu

00:10:16.680 --> 00:10:18.240
kada žele da vas ohrabre.

00:10:19.000 --> 00:10:22.456
I najuobičajeniji razlog zbog koga
nam kažu da ne brinemo jeste vreme.

00:10:22.480 --> 00:10:24.536
Sve je to daleko, znate.

00:10:24.560 --> 00:10:27.000
To je verovatno 50 ili 100 godina daleko.

00:10:27.720 --> 00:10:28.976
Jedan istraživač je rekao:

00:10:29.000 --> 00:10:30.576
"Briga o bezbednosti VI

00:10:30.600 --> 00:10:32.880
je kao briga o prenaseljenosti Marsa."

00:10:33.486 --> 00:10:35.736
Ovo je kao kada vam
iz Silicijumske doline kažu:

00:10:35.760 --> 00:10:38.136
"Nemojte vi zamarati vaše glavice time."

00:10:38.160 --> 00:10:39.496
(Smeh)

00:10:39.520 --> 00:10:41.416
Izgleda da niko ne primećuje

00:10:41.440 --> 00:10:44.056
da je odnos prema vremenu

00:10:44.080 --> 00:10:46.656
potpuni non sequitur.

00:10:46.680 --> 00:10:49.936
Ako je inteligencija samo stvar
procesuiranja informacija,

00:10:49.960 --> 00:10:52.616
i mi nastavimo da poboljšavamo mašine,

00:10:52.640 --> 00:10:55.520
proizvešćemo neki
oblik superinteligencije.

00:10:56.320 --> 00:10:59.976
I nemamo pojma
koliko će nam trebati vremena

00:11:00.000 --> 00:11:02.400
da stvorimo uslove da to uradimo bezbedno.

00:11:04.200 --> 00:11:05.496
Dozvolite mi da ponovim.

00:11:05.520 --> 00:11:09.336
Nemamo pojma koliko vremena će nam trebati

00:11:09.360 --> 00:11:11.600
da stvorimo uslove da to uradimo bezbedno.

00:11:12.920 --> 00:11:16.376
Ako niste primetili,
50 godina nije kao što je bilo ranije.

00:11:16.400 --> 00:11:18.856
Ovo je 50 godina u mesecima.

00:11:18.880 --> 00:11:20.720
Ovoliko dugo već imamo iPhone.

00:11:21.440 --> 00:11:24.040
Ovoliko se već prikazuju "Simpsonovi".

00:11:24.680 --> 00:11:27.056
Pedeset godina nije mnogo vremena

00:11:27.080 --> 00:11:30.240
da se ostvari jedan od najvećih
izazova naše vrste.

00:11:31.640 --> 00:11:35.656
Još jedanput, izgleda da nam nedostaje
odgovarajući emocionalni odgovor

00:11:35.680 --> 00:11:38.376
na ono na šta imamo svako pravo
da verujemo da dolazi.

00:11:38.400 --> 00:11:42.376
Kompjuterski naučnik Stjuart Rasel
ima lepu analogiju za ovo.

00:11:42.400 --> 00:11:47.296
On kaže, zamislite da smo primili
neku poruku od vanzemaljaca,

00:11:47.320 --> 00:11:49.016
u kojoj stoji:

00:11:49.040 --> 00:11:50.576
"Zemljani,

00:11:50.600 --> 00:11:52.960
stižemo na vašu planetu za 50 godina.

00:11:53.800 --> 00:11:55.376
Spremite se."

00:11:55.400 --> 00:11:59.656
I mi ćemo samo da odbrojavamo mesece
dok se njihov brod ne spusti?

00:11:59.680 --> 00:12:02.680
Osetili bismo malo
više straha nego obično.

00:12:04.680 --> 00:12:06.536
Drugo zbog čega kažu da ne brinemo

00:12:06.560 --> 00:12:09.576
je da ove mašine ne mogu drugačije
nego da dele naše vrednosti

00:12:09.600 --> 00:12:12.216
zato što će one bukvalno biti
produžeci nas samih.

00:12:12.240 --> 00:12:14.056
One će biti nakalamljene na naš mozak

00:12:14.080 --> 00:12:16.440
i mi ćemo suštinski postati
limbički sistem.

00:12:17.120 --> 00:12:18.536
Sad na trenutak razmislite

00:12:18.560 --> 00:12:21.736
da je najbezbednija i jedina
razborita i preporučena,

00:12:21.760 --> 00:12:23.096
putanja napred,

00:12:23.120 --> 00:12:25.920
da ovu tehnologiju implementiramo
direktno u naš mozak.

00:12:26.600 --> 00:12:29.976
Sad, ovo može biti najbezbednija i jedina
razborita putanja napred,

00:12:30.000 --> 00:12:33.056
ali obično se bezbednost neke tehnologije

00:12:33.080 --> 00:12:36.736
mora baš dobro razmotriti
pre nego što vam se zabode u mozak.

00:12:36.760 --> 00:12:38.776
(Smeh)

00:12:38.800 --> 00:12:44.136
Veći problem je što pravljenje
superinteligentne VI same od sebe

00:12:44.160 --> 00:12:45.896
izgleda lakše

00:12:45.920 --> 00:12:47.776
od pravljenja superinteligentne VI

00:12:47.800 --> 00:12:49.576
i kompletiranja neuronauke

00:12:49.600 --> 00:12:52.280
koja nam omogućava da
integrišemo naše umove sa njom.

00:12:52.800 --> 00:12:55.976
I ako imamo to da kompanije
i vlade rade ovaj posao,

00:12:56.000 --> 00:12:59.656
one će verovatno to shvatiti
kao da se takmiče protiv svih drugih,

00:12:59.680 --> 00:13:02.936
jer pobediti u ovoj trci
znači osvojiti svet,

00:13:02.960 --> 00:13:05.416
uz uslov da ga sledećeg 
trenutka ne uništite,

00:13:05.440 --> 00:13:08.056
a onda će verovatno prvo biti urađeno

00:13:08.080 --> 00:13:09.280
ono što je najlakše.

00:13:10.560 --> 00:13:13.416
E, sad, nažalost,
ja nemam rešenje za ovaj problem,

00:13:13.440 --> 00:13:16.056
osim toga da preporučim
da više nas promisli o tome.

00:13:16.080 --> 00:13:18.696
Smatram da nam je potrebno
nešto poput Projekta Menhetn

00:13:18.696 --> 00:13:20.496
na temu veštačke inteligencije.

00:13:20.520 --> 00:13:23.256
Ne da bismo je izgradili, jer mislim
da je to neizbežno,

00:13:23.280 --> 00:13:26.616
već da razumemo
kako da izbegnemo trku naoružanja

00:13:26.640 --> 00:13:30.136
i da je napravimo na način
koji je u skladu sa našim interesima.

00:13:30.160 --> 00:13:32.296
Kada se razgovara o superinteligentnoj VI

00:13:32.320 --> 00:13:34.576
koja sama sebe može da menja,

00:13:34.600 --> 00:13:39.216
čini se da imamo samo jednu šansu
da početne uslove postavimo kako treba,

00:13:39.240 --> 00:13:41.296
a čak i onda ćemo morati da prihvatimo

00:13:41.320 --> 00:13:44.360
ekonomske i političke
posledice pravilne upotrebe.

00:13:45.760 --> 00:13:47.816
Ali trenutak kada priznamo

00:13:47.840 --> 00:13:51.840
da je obrada informacija
izvor inteligencije,

00:13:52.720 --> 00:13:57.520
da je neki odgovarajući računarski sistem
osnova onoga što je inteligencija,

00:13:58.360 --> 00:14:02.120
i prihvatimo da ćemo stalno
unapređivati ove sisteme,

00:14:03.280 --> 00:14:07.736
i prihvatimo da će horizont
shvatanja vrlo verovatno daleko nadmašiti

00:14:07.760 --> 00:14:08.960
ono što trenutno znamo,

00:14:10.120 --> 00:14:11.336
onda moramo da prihvatimo

00:14:11.360 --> 00:14:14.000
da smo u procesu
pravljenja neke vrste božanstva.

00:14:15.400 --> 00:14:16.976
Sada bi bilo dobro vreme

00:14:17.000 --> 00:14:20.113
da se postaramo da je to božanstvo
sa kojim bismo mogli živeti.

00:14:20.120 --> 00:14:21.656
Hvala vam mnogo.

00:14:21.680 --> 00:14:25.779
(Aplauz)

