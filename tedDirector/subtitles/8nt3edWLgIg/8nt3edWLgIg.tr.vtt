WEBVTT
Kind: captions
Language: tr

00:00:00.000 --> 00:00:07.000
Çeviri: pinar sadi
Gözden geçirme: Yunus ASIK

00:00:13.000 --> 00:00:15.236
Birçoğumuzun muzdarip olduğu

00:00:15.256 --> 00:00:17.330
bir önsezi yetersizliğinden bahsedeceğim.

00:00:17.480 --> 00:00:20.520
Belirli tür bir tehlikeyi
sezmek gerçekten bir yetersizliktir.

00:00:21.360 --> 00:00:23.096
Bir senaryo anlatacağım,

00:00:23.120 --> 00:00:26.376
korkunç olduğunu düşündüğüm

00:00:26.400 --> 00:00:28.160
ve gerçekleşmesi muhtemel olan

00:00:28.840 --> 00:00:30.496
ve görünen o ki

00:00:30.520 --> 00:00:32.056
iyi bir kombinasyon da değil.

00:00:32.080 --> 00:00:34.536
Yine de korkmak yerine
çoğunuz, bahsettiklerimin

00:00:34.560 --> 00:00:36.640
havalı olduğunu düşüneceksiniz.

00:00:37.200 --> 00:00:40.176
Yapay zekâdaki kazanımlarımızın

00:00:40.200 --> 00:00:41.976
nihayetinde bizi nasıl 

00:00:42.000 --> 00:00:43.776
mahvedeceğinden bahsedeceğim.

00:00:43.800 --> 00:00:47.256
Hatta bence, bizi mahvetmeyeceğini
veya kendimizi mahvetmeyi bize

00:00:47.280 --> 00:00:48.960
aşılamayacağını düşünmek çok güç.

00:00:49.400 --> 00:00:51.256
Fakat sizler de benim gibiyseniz,

00:00:51.280 --> 00:00:53.936
bunları düşünmenin zevkli
olacağını keşfedeceksiniz.

00:00:53.960 --> 00:00:57.336
Ve bu yanıtta sorunun bir parçası.

00:00:57.360 --> 00:00:59.080
Bu yanıt sizi endişelendirmeli.

00:00:59.920 --> 00:01:02.576
Bu konuşmada sizi, olası bir
küresel bir kıtlığa,

00:01:02.600 --> 00:01:06.016
iklim değişikliği veya 
başka bir afetten dolayı,

00:01:06.040 --> 00:01:09.096
torunlarınız veya 
onların torunlarının muhtemelen

00:01:09.120 --> 00:01:12.536
bu şekilde yaşama durumuna

00:01:12.560 --> 00:01:14.360
yakın olduğuna ikna ediyor olsaydım,

00:01:15.200 --> 00:01:16.400
"İlginç.

00:01:17.440 --> 00:01:18.776
Bu TED konuşmasını beğendim."

00:01:18.800 --> 00:01:20.000
diye düşünmezdiniz.

00:01:21.200 --> 00:01:22.720
Kıtlık eğlenceli değildir.

00:01:23.800 --> 00:01:26.280
Diğer taraftan bilim 
kurgudaki ölüm ise eğlencelidir

00:01:26.280 --> 00:01:28.850
ve bu noktada beni yapay zekâdaki

00:01:28.850 --> 00:01:31.200
en çok endişelendiren gelişmelerden biri,

00:01:31.200 --> 00:01:35.296
önümüzde uzanan tehlikelere,
uygun duyarlı bir yanıtı

00:01:35.320 --> 00:01:37.136
sıralayamıyor gibi görünüyoruz.

00:01:37.160 --> 00:01:40.360
Bu yanıtı sıralayamıyorum ve 
bu konuşmayı yapıyorum.

00:01:42.120 --> 00:01:44.816
İki kapının önünde duruyoruz gibi.

00:01:44.840 --> 00:01:46.096
1 numaralı kapının ardında

00:01:46.120 --> 00:01:49.416
zeki makine üretmedeki
gelişmeleri durduruyoruz.

00:01:49.440 --> 00:01:51.360
Her nedense bilgisayarlarımızın donanım

00:01:51.360 --> 00:01:53.480
ve yazılımı daha iyi olmayı durduruyor.

00:01:53.480 --> 00:01:55.300
Şimdi bir dakikanızı ayırın ve bunun

00:01:55.300 --> 00:01:57.080
neden olabileceğini değerlendirin.

00:01:57.080 --> 00:01:58.770
Yani, zekâ ve otomasyonun ne kadar

00:01:58.770 --> 00:02:00.760
değerli olduğu göz önüne alındığında,

00:02:00.760 --> 00:02:04.280
yapabilirsek, teknolojimizi geliştirmeye
devam edeceğiz.

00:02:05.200 --> 00:02:06.867
Bunu yapmamızı ne engelleyebilir?

00:02:07.800 --> 00:02:09.600
Büyük çaplı bir nükleer savaş mı?

00:02:11.000 --> 00:02:12.560
Küresel bir salgın mı?

00:02:14.320 --> 00:02:15.640
Asteroit çarpması mı?

00:02:17.640 --> 00:02:20.140
Justin Bieber'in Amerika Başkanı 
olması mı?

00:02:20.140 --> 00:02:22.520
(Kahkahalar)

00:02:24.760 --> 00:02:26.360
Mesele şu ki, bildiğimiz üzere

00:02:26.360 --> 00:02:29.360
bir şeyler medeniyeti yok etmeli.

00:02:29.360 --> 00:02:33.656
Ne kadar kötü
olabileceğini hayal etmelisiniz,

00:02:33.680 --> 00:02:37.016
teknolojimizde iyileştirme 
yapmamızı engellemenin,

00:02:37.040 --> 00:02:38.256
daimi olarak,

00:02:38.280 --> 00:02:40.296
nesilden nesile.

00:02:40.320 --> 00:02:42.410
Neredeyse, doğası gereği

00:02:42.410 --> 00:02:44.496
bu, insan tarihinde gerçekleşen
en kötü şey.

00:02:44.520 --> 00:02:45.816
Bu yüzden tek alternatif,

00:02:45.840 --> 00:02:48.176
ikinci kapının ardındaki,

00:02:48.200 --> 00:02:51.336
akıllı makinelerimizi yıllar geçtikçe

00:02:51.360 --> 00:02:52.960
geliştirmeye devam etmemiz.

00:02:53.720 --> 00:02:57.360
Bir noktada da bizden daha 
akıllı makineler inşa edeceğiz

00:02:58.080 --> 00:03:00.696
ve bizden akıllı makinelerimiz
olduğunda onlar,

00:03:00.720 --> 00:03:02.696
kendilerini geliştirmeye başlayacaklar.

00:03:02.720 --> 00:03:05.456
Ve sonra, matematikçi IJ Good'un
"zekâ patlaması" olarak

00:03:05.480 --> 00:03:07.256
adlandırdığı bu süreçle,

00:03:07.280 --> 00:03:09.280
bizden kurtulabilecekler.

00:03:10.120 --> 00:03:12.936
Sıklıkla karikatürize edilir,
benim de burada yaptığım gibi,

00:03:12.960 --> 00:03:16.176
korku kötücül robot ordularının

00:03:16.200 --> 00:03:17.456
bize saldırmaları.

00:03:17.480 --> 00:03:20.176
Ama bu çok da olası bir senaryo değil.

00:03:20.200 --> 00:03:25.056
Kendiliğinden makinelerimiz
kötücül olacak değil.

00:03:25.080 --> 00:03:27.696
Asıl kaygılandıran ise;
bizden o kadar çok daha yetkin

00:03:27.720 --> 00:03:29.776
makineler üreteceğiz ki bizim ve onların

00:03:29.800 --> 00:03:33.576
amaçları arasındaki 
en küçük bir fark, bizi

00:03:33.600 --> 00:03:34.800
ortadan kaldırabilecek.

00:03:35.960 --> 00:03:38.260
Nasıl da karıncalar gibi
olacağımızı bir düşünün.

00:03:38.600 --> 00:03:40.030
Onlardan nefret etmiyoruz.

00:03:40.030 --> 00:03:42.406
Onlara zarar vermek için
yolumuzu değiştirmiyoruz.

00:03:42.406 --> 00:03:44.736
Üstelik bazen onlara zarar
vermemeye çalışıyoruz.

00:03:44.760 --> 00:03:46.776
Kaldırımlarda üzerlerinden geçiyoruz.

00:03:46.800 --> 00:03:48.936
Ancak ne zaman onların varlıkları

00:03:48.960 --> 00:03:51.456
bizim hedeflerimizle çakışsa,

00:03:51.480 --> 00:03:53.957
diyelim ki bu gibi bir bina inşa ederken,

00:03:53.981 --> 00:03:55.941
tereddütsüz imha edebiliyoruz onları.

00:03:56.480 --> 00:03:59.416
Endişe uyandıran ise bir gün,
bilinçli olsun veya olmasın

00:03:59.440 --> 00:04:02.176
bize de benzer bir umursamazlıkla
muamele edebilecek

00:04:02.200 --> 00:04:04.200
makineler üreteceğiz.

00:04:05.760 --> 00:04:08.520
Zannederim ki bir çoğunuza ihtimal
dışı görünüyor bu durum.

00:04:09.360 --> 00:04:12.720
Eminim ki bazılarınız süper zeki

00:04:12.720 --> 00:04:15.480
Yapay Zekâ'nın kaçınılmaz olması
şöyle dursun,

00:04:15.480 --> 00:04:17.376
mümkünlüğünden dahi şüphe duyuyor.

00:04:17.400 --> 00:04:19.120
Ancak sonra ileriki varsayımlardan

00:04:19.120 --> 00:04:21.044
bazılarında bir tuhaflık bulmalısınız.

00:04:21.044 --> 00:04:22.616
Ve sadece üç tanesi var.

00:04:23.800 --> 00:04:28.519
Zekâ, fiziksel sistemlerde
bilgi işleme ile ilgilidir.

00:04:29.320 --> 00:04:31.935
Aslında bu bir varsayımdan biraz fazla.

00:04:31.959 --> 00:04:35.416
Zaten makinelerimizde kısıtlı
zekâyı kurduk ve

00:04:35.440 --> 00:04:37.456
bu makinelerin çoğu çoktan

00:04:37.480 --> 00:04:40.120
üstün insan seviyesinde bir
performansta.

00:04:40.840 --> 00:04:43.416
Ve biliyoruz ki bu mutlak mesele,

00:04:43.440 --> 00:04:46.056
"genel zekâ" denilen birçok
alanda esneklikle

00:04:46.080 --> 00:04:49.736
düşünebilme yeteneğine sebep olabilir,

00:04:49.760 --> 00:04:52.896
çünkü beyinlerimiz bunu başardı değil mi?

00:04:52.920 --> 00:04:56.856
Yani sadece atomlar var burada

00:04:56.880 --> 00:05:01.376
ve biz gitgide daha zeki 
davranışlar sergileyen atomlar

00:05:01.400 --> 00:05:04.096
sistemi kurmaya devam ettikçe,

00:05:04.120 --> 00:05:06.656
nihayetinde makinelerimize

00:05:06.680 --> 00:05:10.056
genel zekâyı kurabileceğiz,

00:05:10.080 --> 00:05:11.376
engellenmedikçe.

00:05:11.400 --> 00:05:15.056
İlerlemenin seviyesinin önemli
olmadığının farkına varmak çok mühim,

00:05:15.080 --> 00:05:18.240
çünkü herhangi bir gelişme
bizi sayı çizgisine getirmeye yetebilir.

00:05:18.240 --> 00:05:20.390
Devam etmek için Moore
kanununa gerek yok.

00:05:20.390 --> 00:05:22.080
Üstel bir gelişime ihtiyacımız yok.

00:05:22.080 --> 00:05:23.680
Sadece devam etmeliyiz.

00:05:25.480 --> 00:05:28.400
İkinci varsayım ise devam etmemiz.

00:05:29.000 --> 00:05:31.760
Akıllı makinelerimizi geliştirmeye
devam edeceğiz.

00:05:33.000 --> 00:05:37.376
Ve zekânın değeri göz önüne alınırsa,

00:05:37.400 --> 00:05:40.770
yani zekâ ya bütün değer
verdiklerimizin kaynağıdır ya da

00:05:40.770 --> 00:05:42.000
değer verdiğimiz her şeyi

00:05:42.000 --> 00:05:43.760
korumak için ona ihtiyacımız vardır.

00:05:43.760 --> 00:05:46.016
Bizim en değerli kaynağımızdır.

00:05:46.040 --> 00:05:47.576
O yüzden bunu yapmak isteriz.

00:05:47.600 --> 00:05:50.936
Umutsuzca çözmek istediğimiz
problemlerimiz vardır.

00:05:50.960 --> 00:05:54.160
Alzheimer ve kanser gibi
hastalıkları tedavi etmek isteriz.

00:05:54.960 --> 00:05:56.920
Ekonomi sistemlerini anlamak isteriz.

00:05:56.920 --> 00:05:58.920
İklim bilimini ilerletmek isteriz.

00:05:58.920 --> 00:06:01.176
Yani yapabilirsek bunu yapacağız.

00:06:01.200 --> 00:06:04.486
Tren çoktan istasyondan çıktı ve
basılabilecek bir fren yok.

00:06:05.880 --> 00:06:11.336
Velhasıl, zekânın zirvesinde duramayız

00:06:11.360 --> 00:06:13.160
ya da yakınında herhangi bir yerde.

00:06:13.640 --> 00:06:15.536
Ve bu gerçekten mühim bir öngörüdür.

00:06:15.560 --> 00:06:17.976
Durumumuzu istikrarsız yapan ve

00:06:18.000 --> 00:06:22.040
risk hakkındaki sezgilerimizi güvenilmez
kılan bu durum.

00:06:23.120 --> 00:06:25.840
Şimdi yaşamış en zeki insanı düşünün.

00:06:26.640 --> 00:06:30.056
Buradaki hemen hemen herkesin
son listesinde John von Neumann vardır.

00:06:30.080 --> 00:06:33.416
Yani, John von Neumann' nın çevresindeki
insanlardaki etkisi,

00:06:33.440 --> 00:06:37.496
zamanının en iyi matematikçileri
ve fizikçileri de dâhil olmak üzere,

00:06:37.520 --> 00:06:39.456
oldukça detaylı olarak hazırlanmış.

00:06:39.480 --> 00:06:43.256
Hakkında anlatılanların yarısı
yarı gerçek ise

00:06:43.280 --> 00:06:44.496
şimdiye dek yaşamış

00:06:44.520 --> 00:06:46.976
en zeki insan olduğuna şüphe yok.

00:06:47.000 --> 00:06:49.520
Öyleyse zekâ spektrumunu bir düşünün.

00:06:50.320 --> 00:06:51.749
Bir tarafta John von Neumann

00:06:53.560 --> 00:06:54.894
ve diğer tarafta siz ve ben.

00:06:56.120 --> 00:06:57.416
Ve bir de tavuk.

00:06:57.440 --> 00:06:59.376
(Gülüşmeler)

00:06:59.400 --> 00:07:00.616
Pardon, bir tavuk.

00:07:00.640 --> 00:07:01.896
(Gülüşmeler)

00:07:01.920 --> 00:07:05.656
Bu konuşmayı gereğinden daha
depresif yapmama gerek yok.

00:07:05.680 --> 00:07:07.280
(Gülüşmeler)

00:07:08.339 --> 00:07:11.816
Ezici bir üstünlükle olası görünüyor
ancak zekâ spektrumu

00:07:11.840 --> 00:07:14.960
şu anda tasarlayabileceğimizden
daha da ileriye uzanıyor

00:07:15.880 --> 00:07:19.096
ve kendimizden daha zeki
makineler üretirsek

00:07:19.120 --> 00:07:21.416
büyük ihtimalle bu spektrumu
hayal edemeyeceğimiz

00:07:21.440 --> 00:07:23.296
ölçüde keşfedecekler

00:07:23.320 --> 00:07:25.840
ve o ölçüde bizi aşacaklar.

00:07:27.000 --> 00:07:31.336
Ve hızın fazileti gereği bunun
doğru olduğunu anlamak önemli.

00:07:31.360 --> 00:07:36.416
Değil mi? Stanford veya MIT'deki
ortalama araştırmacı takımınızdan

00:07:36.440 --> 00:07:39.896
daha zeki olmayan süper zeki 
Yapay Zekâ

00:07:39.920 --> 00:07:42.216
geliştirdiğimizi düşünün.

00:07:42.240 --> 00:07:45.216
Elektronik devreler, 
biyokimyasallardan milyon kez

00:07:45.240 --> 00:07:46.496
daha hızlı çalışır, yani

00:07:46.520 --> 00:07:49.656
bu makine onu yapan beyinlerden
milyon kez daha

00:07:49.680 --> 00:07:51.496
hızlı çalışmalıdır.

00:07:51.520 --> 00:07:53.406
Bir haftalığına çalışmaya ayarlasanız

00:07:53.406 --> 00:07:57.760
ve insan seviyesinde 20.000 yıl 
sürecek zihinsel

00:07:58.400 --> 00:08:00.360
işleri yapacak, haftalarca.

00:08:01.640 --> 00:08:04.736
Engellemek şöyle dursun,
bu tarz bir zihinsel

00:08:04.760 --> 00:08:07.040
ilerlemeyi nasıl durdurabiliriz ki?

00:08:08.840 --> 00:08:10.976
Endişe uyandıran diğer bir şey ise,

00:08:11.000 --> 00:08:15.976
açıkcası, olabilecek en iyi
senaryoyu düşünün.

00:08:16.000 --> 00:08:20.080
Hiçbir kaygı barındırmayan
bir süper zeki Yapay Zekâ

00:08:20.080 --> 00:08:21.576
tasarımı keşfettiğimizi düşünün.

00:08:21.600 --> 00:08:24.856
İlk seferde en iyi tasarımı bulduk.

00:08:24.880 --> 00:08:27.096
Tam da istenilen şekilde davranan bir

00:08:27.120 --> 00:08:29.136
kâhin bize verilmiş gibi.

00:08:29.160 --> 00:08:32.880
Bu makine müthiş bir işten
tasarruf ettiren bir aygıt olurdu.

00:08:33.680 --> 00:08:36.299
Güneş ışığıyla çalışan, herhangi
bir fiziksel işi yapabilen,

00:08:36.299 --> 00:08:37.896
aşağı yukarı ham maddeleri

00:08:37.920 --> 00:08:39.376
maliyetinde olan makineyi

00:08:39.400 --> 00:08:42.096
yapabilecek makineyi tasarlayabilir.

00:08:42.120 --> 00:08:45.376
Yani insanların yaptığı angarya 
işlerin sonundan bahsediyoruz.

00:08:45.400 --> 00:08:48.200
Aynı zamanda çoğu zihinsel işlerin de
sonundan bahsediyoruz.

00:08:49.200 --> 00:08:52.256
Böyle bir durumda biz
maymunlar ne yapardık?

00:08:52.280 --> 00:08:56.360
Birbirimize masaj yapabilir ve
frizbi oynayabiliriz.

00:08:57.840 --> 00:09:00.550
Biraz LSD ve tartışmaya açık
giysi seçimlerini de ekleyin

00:09:00.550 --> 00:09:03.006
ve bütün dünya Burning
Man (festival) gibi olabilir.

00:09:03.006 --> 00:09:04.560
(Gülüşmeler)

00:09:06.320 --> 00:09:08.320
Şimdi kulağa hoş gelebilir ama

00:09:09.280 --> 00:09:11.656
kendinize şu anki ekonomik
ve siyasi düzenimizde

00:09:11.680 --> 00:09:14.416
ne olabileceğini sorun.

00:09:14.440 --> 00:09:16.856
Daha önce görmediğimiz düzeyde

00:09:16.880 --> 00:09:21.016
eşitsiz bir zenginlik ve işsizliğe

00:09:21.040 --> 00:09:22.536
tanık olurduk gibi görünüyor.

00:09:22.560 --> 00:09:25.176
Bu yeni zenginliği derhal tüm 
insanlığın hizmetine

00:09:25.200 --> 00:09:26.680
sunmadaki isteksizlikle,

00:09:27.640 --> 00:09:31.256
dünyanın geri kalanı açlıktan
ölürken birkaç trilyoner

00:09:31.280 --> 00:09:33.780
ekonomi dergilerimizin
kapaklarını süslüyor olabilir.

00:09:34.260 --> 00:09:36.616
Peki ya Ruslar ya da Çinliler,
Silikon Vadisi'ndeki

00:09:36.640 --> 00:09:39.256
bazı şirketlerin üstün zeki 
Yapay Zekâ'yı dağıtmak

00:09:39.280 --> 00:09:42.016
üzere olduğunu duysa ne yaparlardı?

00:09:42.040 --> 00:09:44.896
Karasal ya da siber, bu makine

00:09:44.920 --> 00:09:47.136
tahmin edilemez bir güçle savaş

00:09:47.160 --> 00:09:48.840
çıkarabilecek potansiyelde olurdu.

00:09:50.120 --> 00:09:52.076
Bu kazananın her şeyi
aldığı bir senaryo.

00:09:52.076 --> 00:09:55.136
Buradaki rekabetten 6 ay
ilerde olmak

00:09:55.160 --> 00:09:57.936
minimum 500.000 yıl

00:09:57.960 --> 00:09:59.456
ileride olmaktır.

00:09:59.480 --> 00:10:04.216
Demek ki bu tür bir atılımın
sadece söylentisi bile

00:10:04.240 --> 00:10:06.616
türümüzün çıldırmasına yol açabiliyor.

00:10:06.640 --> 00:10:09.536
Şimdi en korkutucu şeylerden bir tanesi

00:10:09.560 --> 00:10:12.336
bana göre, şu anda,

00:10:12.360 --> 00:10:16.656
Yapay Zekâ araştırmacılarının
güven verici olmak istediklerinde

00:10:16.680 --> 00:10:18.240
söyledikleri şeyler.

00:10:18.920 --> 00:10:20.490
Ve en sık bize endişe etmememiz

00:10:20.490 --> 00:10:22.480
için söylenen neden ise zaman.

00:10:22.480 --> 00:10:24.536
Çok çok uzakta, haberin yok mu?

00:10:24.560 --> 00:10:27.000
Muhtemelen 50 ya da 100 yıl kadar uzakta.

00:10:27.720 --> 00:10:28.930
Bir araştırmacı dedi ki;

00:10:28.930 --> 00:10:30.756
"Yapay Zekâ güvenliği için endişelenmek

00:10:30.756 --> 00:10:32.990
Mars'ta aşırı nüfus artışına
endişelenmek gibi."

00:10:34.116 --> 00:10:35.736
Bu Silikon Vadisi'nin

00:10:35.760 --> 00:10:38.136
"Sıkmayın tatlı canınızı" versiyonu.

00:10:38.160 --> 00:10:39.496
(Gülüşmeler)

00:10:39.520 --> 00:10:41.416
Zamana referanslamanın

00:10:41.440 --> 00:10:44.056
tamamen yersiz bir deyim

00:10:44.080 --> 00:10:46.656
olduğunu kimse fark etmiyor.

00:10:46.680 --> 00:10:49.936
Zekâ eğer bilgi işlemeden ibaretse

00:10:49.960 --> 00:10:52.616
ve biz makinelerimizi geliştirmeye
devam edersek

00:10:52.640 --> 00:10:55.520
bir tür süper zekâ üreteceğiz.

00:10:56.320 --> 00:10:59.976
Ve güvenli bir biçimde yapmak için
gerekli şartları yaratmanın

00:11:00.000 --> 00:11:02.400
ne kadar süreceği hakkında 
hiçbir fikrimiz yok.

00:11:04.200 --> 00:11:05.496
Bir daha söyleyeyim.

00:11:05.520 --> 00:11:09.336
Güvenli bir biçimde yapmak için
gerekli şartları yaratmanın

00:11:09.360 --> 00:11:11.600
ne kadar süreceği hakkında 
hiçbir fikrimiz yok.

00:11:12.920 --> 00:11:16.376
Ve eğer fark etmediyseniz,
50 yıl eskisi gibi değil artık.

00:11:16.400 --> 00:11:18.856
Bu ay olarak 50 yıl.

00:11:18.880 --> 00:11:20.720
Bu kadar zamandır iPhone var.

00:11:21.440 --> 00:11:24.040
Bu kadar zamandır "The Simpsons" 
(Simpsonlar) televizyonda.

00:11:24.680 --> 00:11:27.056
50 yıl, türümüzün karşılaşabileceği

00:11:27.080 --> 00:11:30.240
en büyük zorluklar için
fazla bir zaman değil.

00:11:31.640 --> 00:11:35.656
Bir defa daha uygun duygusal karşılık
vermede başarısız gibiyiz,

00:11:35.680 --> 00:11:38.376
yaklaştığına inanmamız için 
tüm gerekçelere sahipken.

00:11:38.400 --> 00:11:42.376
Bilgisayar bilimcisi Stuart Russell'ın 
güzel bir benzetimi var.

00:11:42.400 --> 00:11:47.296
Diyor ki; uzaylı bir medeniyetten
bir mektup aldığımızı düşünün,

00:11:47.320 --> 00:11:49.016
şöyle diyor:

00:11:49.040 --> 00:11:50.576
"Dünya Halkı,

00:11:50.600 --> 00:11:52.960
50 yıl içinde gezegeninize ulaşacağız.

00:11:53.800 --> 00:11:55.376
Hazırlanın."

00:11:55.400 --> 00:11:59.656
Ve şimdi, ana gemi varana kadar 
ay ay geri mi sayacağız?

00:11:59.680 --> 00:12:02.680
Normalden biraz daha fazla
aciliyet hissederdik.

00:12:04.680 --> 00:12:06.312
Endişelenmememiz söylenen

00:12:06.312 --> 00:12:07.394
bir diğer gerekçe ise

00:12:07.394 --> 00:12:10.046
bu makinelerin istemeden bizim
değerlerimizi paylaşacak

00:12:10.046 --> 00:12:12.216
olmaları, çünkü
bizim devamımız gibi olacaklar.

00:12:12.240 --> 00:12:14.056
Beyinlerimize aşılanacaklar

00:12:14.080 --> 00:12:16.440
ve temelde onların limbik
sistemi olacağız.

00:12:17.120 --> 00:12:18.696
Şimdi bir dakikanızı ayırın ve

00:12:18.696 --> 00:12:21.736
önümüzdeki en sağlam ve ihtiyatlı yolun,

00:12:21.760 --> 00:12:23.096
önerilenin, bu teknolojiyi

00:12:23.120 --> 00:12:24.400
direkt olarak beyinlerimize

00:12:24.400 --> 00:12:26.600
yerleştirmek olduğunu değerlendirin.

00:12:26.600 --> 00:12:29.976
Belki de bu, önümüzdeki en güvenli
ve özenli yol olabilir

00:12:30.000 --> 00:12:33.056
ama genelde kişinin teknoloji hakkındaki 
güvenlik endişelerinin

00:12:33.080 --> 00:12:36.736
kafasının içine bir şey yerleştirmeden
çözülmesi gerekir.

00:12:36.760 --> 00:12:38.776
(Kahkahalar)

00:12:38.800 --> 00:12:44.136
Daha derindeki problem ise süper zeki
YZ geliştirmek başlı başına

00:12:44.160 --> 00:12:45.896
daha kolay gibi görünüyor,

00:12:45.920 --> 00:12:47.776
sorunsuzca zihinlerimizle entegre

00:12:47.800 --> 00:12:49.576
olabilen tamamlanmış sinirbilimli

00:12:49.600 --> 00:12:52.280
bir yapay zekâ geliştirmektense.

00:12:52.800 --> 00:12:55.976
Ve farz edelim ki devletlerin ve 
şirketlerin bunu yapıyorken

00:12:56.000 --> 00:12:59.656
birbirleriyle yarış
içerisindeymişçe algılamaları

00:12:59.680 --> 00:13:02.936
ve bu yarışı kazanmalarının da
dünyayı kazanmak sanmaları,

00:13:02.960 --> 00:13:05.416
peşi sıra mahvetmemek şartıyla,

00:13:05.440 --> 00:13:08.056
sonrasında hangisi en kolaysa

00:13:08.080 --> 00:13:09.830
ilk o yapılacakmış gibi görünüyor.

00:13:10.560 --> 00:13:13.416
Şu an ne yazık ki bu soruna bir
çözümüm yok,

00:13:13.440 --> 00:13:16.056
çoğumuza bunu düşünmeyi önermek dışında.

00:13:16.080 --> 00:13:18.456
Yapay Zekâ konusunda Manhattan 
Projesi gibi

00:13:18.480 --> 00:13:20.496
bir şeye ihtiyacımız var.

00:13:20.520 --> 00:13:21.840
Geliştirmek için değil, çünkü

00:13:21.840 --> 00:13:23.870
kaçınılamaz bir şekilde
yapacağız bana göre,

00:13:23.870 --> 00:13:26.616
ama silahlanma yarışından
nasıl kaçıncağımızı anlamak

00:13:26.640 --> 00:13:30.136
ve menfaatlerimiz doğrultusunda 
inşa etmek için.

00:13:30.160 --> 00:13:32.296
Süper zeki YZ 'nın kendi
başına değişiklikler

00:13:32.320 --> 00:13:34.576
yapabileceğinden bahsederken,

00:13:34.600 --> 00:13:39.216
ana koşulları doğru anlamada tek
şansımız varmış gibi görünüyor

00:13:39.240 --> 00:13:41.296
ve o zaman dahi bunları doğru anlamanın

00:13:41.320 --> 00:13:44.360
siyasi ve ekonomik sonuçlarını
özümsememiz gerekecektir.

00:13:45.760 --> 00:13:47.816
Bilgi işlemenin yapay zekânın

00:13:47.840 --> 00:13:51.840
kaynağı olduğunu kabul ettiğimiz an,

00:13:52.720 --> 00:13:57.520
bu bazı uygun hesaplama sistemlerinin
zekânın temeli olduğunu

00:13:58.360 --> 00:14:02.120
ve bu sistemleri aralıksız
geliştireceğimizi kabul ettiğimizde

00:14:03.280 --> 00:14:07.736
ve bilişin ufkunun bildiğimizden 
daha uzaklara uzandığını

00:14:07.760 --> 00:14:08.960
kabul ettiğimizde,

00:14:10.120 --> 00:14:11.336
bir çeşit yaratıcı

00:14:11.360 --> 00:14:13.070
geliştirme sürecinde olduğumuzu

00:14:13.070 --> 00:14:15.310
kabul etmek durumunda olacağız.

00:14:15.310 --> 00:14:17.506
Şu an ise bu yaratıcıyla
yaşayabilme durumumuza

00:14:17.506 --> 00:14:19.423
karar vermek için doğru zaman olabilir.

00:14:20.120 --> 00:14:21.656
Çok teşekkürler.

00:14:21.680 --> 00:14:26.773
(Alkışlar)

