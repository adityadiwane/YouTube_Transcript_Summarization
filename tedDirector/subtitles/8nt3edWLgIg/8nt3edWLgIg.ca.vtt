WEBVTT
Kind: captions
Language: ca

00:00:00.000 --> 00:00:07.000
Translator: Marcos Morales Pallares
Reviewer: Elena Pérez Gómez

00:00:12.820 --> 00:00:15.036
Tot seguit parlaré
sobre una manca d'intuïció

00:00:15.060 --> 00:00:16.660
que molts de nosaltres patim.

00:00:17.300 --> 00:00:20.340
És una dificultat per detectar
un tipus de perill específic.

00:00:21.180 --> 00:00:22.916
Imagineu-vos el següent escenari,

00:00:22.940 --> 00:00:26.196
que jo crec que és esgarrifós

00:00:26.220 --> 00:00:27.980
i alhora ben probable.

00:00:28.660 --> 00:00:30.316
Això no és pas una bona combinació

00:00:30.340 --> 00:00:31.876
pel que sembla.

00:00:31.900 --> 00:00:34.356
I enlloc d'espantar-vos,
molts creureu

00:00:34.380 --> 00:00:36.460
que allò del que parlo
és divertit.

00:00:37.020 --> 00:00:39.996
M'agradaria explicar
que els avenços que fem

00:00:40.020 --> 00:00:41.796
en intel·ligència artificial

00:00:41.820 --> 00:00:43.596
poden acabar destruint-nos.

00:00:43.620 --> 00:00:47.076
I de fet, crec que és ben difícil
no veure com no ens poden destruir

00:00:47.100 --> 00:00:48.800
o fer que ens autodestruïm.

00:00:49.220 --> 00:00:51.076
Tot i així, si penseu com jo,

00:00:51.100 --> 00:00:53.756
veureu que és distret
pensar en aquestes coses.

00:00:53.780 --> 00:00:57.156
I aquesta és la resposta
que és part del problema.

00:00:57.180 --> 00:00:58.900
És la que hauria d'amoïnar-vos.

00:00:59.740 --> 00:01:02.396
I si us hagués de convèncer
amb aquesta xerrada

00:01:02.420 --> 00:01:05.836
de que és ben possible
que arribem a patir una fam mundial,

00:01:05.860 --> 00:01:08.916
ja sigui pel canvi climàtic
o per qualsevol altra catàstrofe,

00:01:08.940 --> 00:01:12.356
i que els vostres néts,
o els seus néts,

00:01:12.380 --> 00:01:14.180
probablement viuran així,

00:01:15.020 --> 00:01:16.220
no pensaríeu:

00:01:17.260 --> 00:01:18.596
"Interessant.

00:01:18.620 --> 00:01:20.400
M'agrada aquesta xerrada TED."

00:01:21.020 --> 00:01:22.540
La fam no és divertida.

00:01:23.620 --> 00:01:26.996
La mort en la ciència ficció,
en canvi, sí que ho és,

00:01:27.020 --> 00:01:30.996
i el que més em preocupa
sobre el progrés de la IA ara mateix

00:01:31.020 --> 00:01:35.116
és que no sembla que siguem capaços
de respondre com cal a nivell emocional

00:01:35.140 --> 00:01:36.956
als perills que ens esperen.

00:01:36.980 --> 00:01:40.180
No sóc capaç de respondre-hi
i ara em trobo donant aquesta xerrada.

00:01:41.940 --> 00:01:44.636
És com si ens trobéssim
davant dues portes.

00:01:44.660 --> 00:01:45.916
Rera la porta número u,

00:01:45.940 --> 00:01:49.236
deixem de progressar
i de construir màquines intel·ligents.

00:01:49.260 --> 00:01:53.276
El nostre maquinari i programari
deixa de millorar per alguna raó.

00:01:53.300 --> 00:01:56.300
Penseu durant uns instants
per què podria passar això.

00:01:56.900 --> 00:02:00.556
Vull dir, ja que la intel·ligència
i la automatització són tan valuoses,

00:02:00.580 --> 00:02:04.100
seguirem millorant la tecnologia
sempre que siguem sent capaços.

00:02:05.020 --> 00:02:06.687
Què podria impedir-nos-ho?

00:02:07.620 --> 00:02:09.630
Potser una guerra nuclear mundial?

00:02:10.820 --> 00:02:12.540
O una gran epidèmia?

00:02:14.140 --> 00:02:15.590
L'impacte d'un asteroide?

00:02:17.460 --> 00:02:20.036
Justin Bieber com
president dels Estats Units?

00:02:20.060 --> 00:02:22.340
(Riures)

00:02:24.580 --> 00:02:28.500
Alguna cosa hauria de destruir
la civilització tal i com la coneixem.

00:02:29.180 --> 00:02:33.476
Imagineu-vos com de greu hauria de ser

00:02:33.500 --> 00:02:36.836
per impedir-nos seguir
millorant la tecnologia

00:02:36.860 --> 00:02:38.076
de forma permanent,

00:02:38.100 --> 00:02:40.116
generació rera generació.

00:02:40.140 --> 00:02:42.276
Quasi per definició,
es tracta del pitjor

00:02:42.300 --> 00:02:44.316
que podria arribar a patir la humanitat.

00:02:44.340 --> 00:02:45.636
Però l'única alternativa,

00:02:45.660 --> 00:02:47.996
que es troba darrera la porta número dos,

00:02:48.020 --> 00:02:51.156
és seguir millorant
la intel·ligència de les màquines

00:02:51.180 --> 00:02:52.780
any rere any.

00:02:53.540 --> 00:02:57.180
En algun moment, construirem màquines
més intel·ligents que nosaltres,

00:02:57.900 --> 00:03:00.516
i un cop les màquines ens superin,

00:03:00.540 --> 00:03:02.516
es milloraran a elles mateixes.

00:03:02.540 --> 00:03:05.276
I el que el matemàtic
IJ Good anomenava com

00:03:05.300 --> 00:03:07.076
"l'explosió de la intel·ligència",

00:03:07.100 --> 00:03:09.910
pot succeir fent que les màquines
ens superin amb escreix.

00:03:09.940 --> 00:03:12.756
Aquest fet sovint es representa
com jo ho he fet aquí,

00:03:12.780 --> 00:03:15.996
imaginant que exèrcits de robots malèvols

00:03:16.020 --> 00:03:17.276
ens ataquen.

00:03:17.300 --> 00:03:19.996
Però aquest no és l'escenari més probable.

00:03:20.020 --> 00:03:24.876
No crec que les màquines
de cop i volta es tornin malèfiques.

00:03:24.900 --> 00:03:27.516
Em preocupa més que arribem
a construir màquines

00:03:27.540 --> 00:03:29.596
que són molt més
competents que nosaltres

00:03:29.620 --> 00:03:33.396
i que la més subtil de les divergències
entre els seus objectius i els nostres

00:03:33.420 --> 00:03:35.210
puguin arribar a destruir-nos.

00:03:35.780 --> 00:03:38.130
Penseu només en com
tractem a les formigues.

00:03:38.420 --> 00:03:40.076
No les odiem pas.

00:03:40.100 --> 00:03:42.156
No sortim del camí per fer-les-hi mal.

00:03:42.180 --> 00:03:44.556
Fins i tot patim mals de cap
per no esclafar-les

00:03:44.580 --> 00:03:46.596
i les saltem quan caminem per la vorera.

00:03:46.620 --> 00:03:48.756
Però quan la seva presència

00:03:48.780 --> 00:03:51.276
entra en conflicte
amb els nostres objectius,

00:03:51.300 --> 00:03:53.777
per exemple si volem construir un edifici,

00:03:53.801 --> 00:03:55.761
les exterminem sense cap prejudici.

00:03:56.300 --> 00:03:59.236
El que em preocupa és que un dia
construirem màquines

00:03:59.260 --> 00:04:01.996
que tant si són conscients
com si no ho són,

00:04:02.020 --> 00:04:04.020
podrien arribar a menysprear-nos així.

00:04:05.580 --> 00:04:08.340
Crec que això queda molt lluny
per molts de vosaltres.

00:04:09.180 --> 00:04:15.516
Inclús crec que molts de vosaltres dubteu
que la superintel·ligència sigui possible,

00:04:15.540 --> 00:04:17.196
i menys encara inevitable.

00:04:17.220 --> 00:04:21.120
Però en aquest cas heu de trobar quelcom
erroni en una d'aquestes suposicions:

00:04:21.150 --> 00:04:22.786
I només n'hi ha tres:

00:04:23.620 --> 00:04:28.339
La intel·ligència consisteix en processar
informació en sistemes físics.

00:04:29.140 --> 00:04:31.755
De fet, això és quelcom
més que una suposició.

00:04:31.779 --> 00:04:35.236
Ja hem incorporat petites intel·ligències
en les nostres màquines,

00:04:35.260 --> 00:04:37.536
i moltes d'elles tenen
un rendiment superior

00:04:37.536 --> 00:04:40.130
a un nivell d'intel·ligència suprahumana.

00:04:40.660 --> 00:04:43.236
I ja sabem que la mera matèria

00:04:43.260 --> 00:04:45.876
pot donar lloc
a la "intel·ligència general",

00:04:45.900 --> 00:04:49.556
és a dir l'habilitat de pensar
amb flexibilitat en diversos camps.

00:04:49.580 --> 00:04:52.716
Al cap i a la fi els nostres cervells
ho han aconseguit, cert?

00:04:52.740 --> 00:04:56.676
És a dir, només hi ha àtoms aquí,

00:04:56.700 --> 00:05:01.196
i sempre i quan seguim construint
sistemes a base d'àtoms

00:05:01.220 --> 00:05:03.916
que es comporten de forma
cada cop més intel·ligent,

00:05:03.940 --> 00:05:06.476
en algun moment, si res no ho impedeix,

00:05:06.500 --> 00:05:09.876
en algun moment construirem
intel·ligència artificial

00:05:09.900 --> 00:05:11.196
en les nostres màquines.

00:05:11.220 --> 00:05:14.876
És de vital importància adonar-se'n
que el ritme de progrés no importa,

00:05:14.900 --> 00:05:18.076
perquè qualsevol progrés ens acabarà
portant a l'altre extrem.

00:05:18.100 --> 00:05:21.876
No ens cal la llei de Moore per continuar.
No ens cal un progrés exponencial.

00:05:21.900 --> 00:05:23.500
Només ens cal seguir avançant.

00:05:25.300 --> 00:05:28.220
La segona suposició
és que seguirem avançant.

00:05:28.820 --> 00:05:31.580
Seguirem millorant
les màquines intel·ligents.

00:05:32.820 --> 00:05:37.196
I donat el valor que té la intel·ligència,

00:05:37.220 --> 00:05:40.756
és a dir, ja que la intel·ligència
és la font de tot el que valorem

00:05:40.780 --> 00:05:43.556
i la necessitem per salvaguardar
tot el que ens importa.

00:05:43.580 --> 00:05:45.836
És el recurs més valuós de que disposem.

00:05:45.860 --> 00:05:47.396
Així doncs, volem que avanci.

00:05:47.420 --> 00:05:50.756
Tenim problemes que
hem de solucionar desesperadament.

00:05:50.780 --> 00:05:53.980
Volem guarir malalties
com l'Alzheimer i el càncer.

00:05:54.780 --> 00:05:58.716
Volem entendre sistemes econòmics.
Volem millorar la ciència del clima.

00:05:58.740 --> 00:06:00.996
Així doncs ho farem si podem.

00:06:01.020 --> 00:06:04.306
El tren ja ha sortit de l'estació
i no hi ha manera de que pari.

00:06:05.700 --> 00:06:11.156
I per últim, no ens trobem
al cim de la nostra intel·ligència,

00:06:11.180 --> 00:06:12.980
o en qualsevol punt que s'hi acosti.

00:06:13.460 --> 00:06:15.356
I això és la clau de la qüestió.

00:06:15.380 --> 00:06:18.166
És el que fa que la nostra
situació sigui tan precària,

00:06:18.166 --> 00:06:22.340
i és el que fa que les nostres intuïcions
sobre aquest risc siguin tan poc fiables.

00:06:22.940 --> 00:06:26.140
Penseu en la persona
més intel·ligent que mai ha viscut.

00:06:26.460 --> 00:06:29.876
Molts de vosaltres haureu considerat
a en John von Neumann.

00:06:29.900 --> 00:06:33.236
La impressió que va causar
en von Neumann al seu voltant,

00:06:33.260 --> 00:06:37.316
i això inclou als més grans matemàtics
i físics de la seva època,

00:06:37.340 --> 00:06:39.276
està ben documentada.

00:06:39.300 --> 00:06:43.076
Si només la meitat de
les històries sobre ell són certes,

00:06:43.100 --> 00:06:44.316
no hi ha cap dubte

00:06:44.340 --> 00:06:46.796
que és una de les persones més brillants.

00:06:46.820 --> 00:06:49.340
Considerem l'espectre d'intel·ligència.

00:06:50.140 --> 00:06:51.569
Tenim en John von Neumann.

00:06:53.380 --> 00:06:54.714
I aquí estem nosaltres.

00:06:55.940 --> 00:06:57.236
I aquí un pollastre.

00:06:57.260 --> 00:06:59.196
(Riures)

00:06:59.220 --> 00:07:00.436
Perdó, un pollastre.

00:07:00.460 --> 00:07:01.716
(Riures)

00:07:01.740 --> 00:07:05.716
No hi ha cap necessitat de fer aquesta
xerrada més depriment del que pot ser.

00:07:05.716 --> 00:07:07.100
(Riures)

00:07:08.159 --> 00:07:11.636
Sembla que està ben clar
que l'espectre de la intel·ligència

00:07:11.660 --> 00:07:14.780
s'estén molt més enllà
del que considerem actualment,

00:07:15.700 --> 00:07:18.916
i si construïm màquines que
són més intel·ligents que nosaltres,

00:07:18.940 --> 00:07:21.236
molt probablement exploraran
aquest espectre

00:07:21.260 --> 00:07:23.116
de maneres que no podem imaginar,

00:07:23.140 --> 00:07:25.660
i ens superaran de maneres
que tampoc podem imaginar.

00:07:26.820 --> 00:07:31.156
I hem de reconèixer que això és així
només en base a la velocitat.

00:07:31.180 --> 00:07:36.236
D'acord? Doncs imagineu que
hem construit una IA superintel·ligent

00:07:36.260 --> 00:07:39.716
que no és més brillant que
la majoria d'equips de recerca

00:07:39.740 --> 00:07:42.036
de Stanford o del MIT.

00:07:42.060 --> 00:07:45.036
Els circuits electrònics són
un milió de vegades més ràpids

00:07:45.060 --> 00:07:46.316
que les ones bioquímiques,

00:07:46.340 --> 00:07:49.476
així, aquesta màquina pensaria
un milió de cops més depressa

00:07:49.500 --> 00:07:51.316
que les ments que l'han dissenyat.

00:07:51.340 --> 00:07:52.996
Si la deixem encesa una setmana

00:07:53.020 --> 00:07:57.580
haurà complert 20.000 anys de
feina intel·lectual a nivell humà,

00:07:58.220 --> 00:08:00.180
setmana rere setmana rere setmana.

00:08:01.460 --> 00:08:04.556
Com podem arribar a comprendre,
i menys encara limitar,

00:08:04.580 --> 00:08:06.860
una ment que progressa així?

00:08:08.660 --> 00:08:10.796
L'altra cosa que em preocupa,
sent honestos,

00:08:10.820 --> 00:08:15.796
es que... imagineu-vos
el millor escenari possible.

00:08:15.820 --> 00:08:19.996
Imagineu que tombem amb un disseny
de IA superintel·ligent

00:08:20.020 --> 00:08:21.646
sense problemes de seguretat.

00:08:21.646 --> 00:08:24.676
És el primer cop que
aconseguim el disseny perfecte.

00:08:24.700 --> 00:08:26.916
És com si ens hagués arribat un oracle

00:08:26.940 --> 00:08:28.956
que es comporta tal i com preteníem.

00:08:28.980 --> 00:08:32.700
Doncs bé, aquesta màquina seria
l'eina perfecta per reduir-nos feina.

00:08:33.350 --> 00:08:36.129
Podria dissenyar màquines
que podrien construir màquines

00:08:36.129 --> 00:08:37.716
per fer qualsevol feina física,

00:08:37.740 --> 00:08:39.196
alimentades per energia solar,

00:08:39.220 --> 00:08:41.916
i més o menys pel que
costa la matèria prima.

00:08:41.940 --> 00:08:45.196
Estem parlant de la fi
de les feines feixugues.

00:08:45.220 --> 00:08:48.020
També seria la fi de la majoria
de feines intel·lectuals.

00:08:49.020 --> 00:08:52.076
Què farien uns simis com nosaltres
en aquestes circumstàncies?

00:08:52.100 --> 00:08:56.650
Tindríem temps per jugar al disc volador
i de fer-nos massatges els uns als altres.

00:08:57.660 --> 00:09:00.516
Afegiu una mica de LSD
i un vestuari de gust qüestionable

00:09:00.540 --> 00:09:02.716
i el món sencer podria
ser com el Burning Man.

00:09:02.740 --> 00:09:04.380
(Riures)

00:09:06.140 --> 00:09:08.140
Tot això pot sonar força atractiu,

00:09:09.100 --> 00:09:11.476
però penseu que passaria

00:09:11.500 --> 00:09:14.236
si succeís sota
l'ordre econòmic i polític actual?

00:09:14.260 --> 00:09:16.676
És ben probable que experimentéssim

00:09:16.700 --> 00:09:20.836
una desigualtat econòmica
i un nivell d'atur

00:09:20.860 --> 00:09:22.356
que mai havíem experimentat.

00:09:22.380 --> 00:09:24.996
I si no hi ha la voluntat
de posar tota aquesta riquesa

00:09:25.020 --> 00:09:26.500
al servei de la humanitat,

00:09:27.460 --> 00:09:31.076
uns quants bilionaris arribarien
a les portades de les revistes

00:09:31.100 --> 00:09:33.540
mentre la resta
ens moriríem de gana.

00:09:34.140 --> 00:09:36.436
I què farien els russos o els xinesos

00:09:36.460 --> 00:09:39.076
si sapiguessin que
una empresa de Silicon Valley

00:09:39.100 --> 00:09:41.836
estigués a punt d'aconseguir
una IA superintel·ligent?

00:09:41.860 --> 00:09:44.716
Aquesta màquina estaria
preparada per entrar en guerra,

00:09:44.740 --> 00:09:46.956
ja sigui per terra o cibernètica,

00:09:46.980 --> 00:09:48.660
amb un poder sense precedents.

00:09:49.940 --> 00:09:51.796
I aquí el guanyador s'ho emporta tot.

00:09:51.820 --> 00:09:54.956
Aquí, trobar-se sis mesos
al capdavant de la competició

00:09:54.980 --> 00:09:57.756
vol dir trobar-se
500.000 anys al davant

00:09:57.780 --> 00:09:59.276
com a mínim.

00:09:59.300 --> 00:10:04.036
Només que hi hagi un rumor
sobre algun descobriment d'aquest tipus

00:10:04.060 --> 00:10:06.436
pot fer que la nostra espècie embogeixi.

00:10:06.460 --> 00:10:09.356
Ara bé, una de les coses més paoroses,

00:10:09.380 --> 00:10:12.156
segons el meu punt de vista, ara per ara,

00:10:12.180 --> 00:10:16.476
són el tipus de coses
que els investigadors en IA diuen

00:10:16.500 --> 00:10:18.060
quan volen tranquil·litzar-nos.

00:10:18.820 --> 00:10:22.276
I la principal raó que ens donen
per no preocupar-no és el temps.

00:10:22.300 --> 00:10:24.356
"Això encara està lluny,
no ho sabíeu?"

00:10:24.380 --> 00:10:26.820
"Probablement encara queden
uns 50 o 100 anys."

00:10:27.540 --> 00:10:28.796
Un investigador va dir:

00:10:28.820 --> 00:10:30.396
"Preocuparse per una IA insegura

00:10:30.420 --> 00:10:33.260
és com preocupar-se per la
sobre-població a Mart."

00:10:33.936 --> 00:10:35.556
Això és la versió
de Silicon Valley

00:10:35.580 --> 00:10:37.956
de "no et preocupis, nena!"

00:10:37.980 --> 00:10:39.316
(Riures)

00:10:39.340 --> 00:10:41.236
Ningú sembla adonar-se'n

00:10:41.260 --> 00:10:43.876
que basar-se en un horitzó cronològic

00:10:43.900 --> 00:10:46.476
és un 'non sequitur' absolut.

00:10:46.500 --> 00:10:49.756
Si la intel·ligència consisteix
simplement en processar informació,

00:10:49.780 --> 00:10:52.436
i seguim perfeccionant les màquines,

00:10:52.460 --> 00:10:55.340
acabarem per generar alguna
forma de superintel·ligència.

00:10:56.140 --> 00:10:59.796
I no tenim ni idea de quant
de temps ens portarà

00:10:59.820 --> 00:11:02.220
crear les condicions
per fer-ho de forma segura.

00:11:04.020 --> 00:11:05.316
Deixeu-me que ho repeteixi.

00:11:05.340 --> 00:11:09.156
No tenim ni idea de quant
de temps ens portarà

00:11:09.180 --> 00:11:11.420
crear les condicions per
fer-ho de forma segura.

00:11:12.740 --> 00:11:16.196
I per si no us n'heu adonat,
50 anys no són el que eren abans.

00:11:16.220 --> 00:11:18.676
Això són 50 anys en mesos.

00:11:18.700 --> 00:11:21.170
Això són els mesos
que hem tingut l'iPhone.

00:11:21.260 --> 00:11:24.460
Això són els mesos que 
"Els Simpsons" han estat en pantalla.

00:11:24.500 --> 00:11:26.876
Cinquanta anys no és gaire temps

00:11:26.900 --> 00:11:30.060
per assolir un dels grans reptes
de la nostra espècie.

00:11:31.460 --> 00:11:35.476
Torno a dir, sembla que no estem
reaccionant com cal a nivell emocional

00:11:35.500 --> 00:11:38.196
envers allò que cada cop n'estem
més segurs que arribarà.

00:11:38.220 --> 00:11:42.196
El científic informàtic Stuart Russell
fa una bonica analogia.

00:11:42.220 --> 00:11:47.116
Va dir que imaginéssim que havíem rebut
un missatge d'una civilització alienígena,

00:11:47.140 --> 00:11:48.836
que deia:

00:11:48.860 --> 00:11:50.396
"Gent de la Terra,

00:11:50.420 --> 00:11:52.780
arribarem al vostre planeta
d'aquí 50 anys.

00:11:53.620 --> 00:11:55.196
Prepareu-vos."

00:11:55.220 --> 00:11:59.476
I llavors comencem a comptar
els mesos que queden abans que arribin.

00:11:59.500 --> 00:12:02.500
Segurament sentiríem
una mica més de pressió que ara.

00:12:04.300 --> 00:12:06.486
Una altra raó que donen
per no preocupar-nos

00:12:06.486 --> 00:12:09.396
és que les màquines només podran
compartir els nostres valors

00:12:09.420 --> 00:12:12.036
perquè seran literalment
extensions de nosaltres.

00:12:12.060 --> 00:12:14.076
Seran uns empelts
en els nostres cervells,

00:12:14.076 --> 00:12:16.530
i esdevindrem els seus sistemes límbics.

00:12:16.940 --> 00:12:18.356
Considereu per un moment

00:12:18.380 --> 00:12:21.556
que la forma més segura
i més prudent de seguir avançant,

00:12:21.580 --> 00:12:22.916
la forma més recomanada,

00:12:22.940 --> 00:12:25.740
és implantar-los en els nostres
cervells directament.

00:12:26.420 --> 00:12:29.796
És possible que sigui l'única manera
segura i prudent de continuar,

00:12:29.820 --> 00:12:32.876
tot i que normalment si una tecnologia
ens preocupa una micona

00:12:32.900 --> 00:12:37.056
primer cal que estigui lliure d'errors
si després ens la volem enganxar al cap.

00:12:37.056 --> 00:12:38.596
(Riures)

00:12:38.620 --> 00:12:43.956
El problema de fons és que desenvolupar
una IA superintel·ligent per si mateixa

00:12:43.980 --> 00:12:45.716
sembla ser més fàcil

00:12:45.740 --> 00:12:47.596
que construir una IA superintel·ligent

00:12:47.620 --> 00:12:49.396
i tenir alhora la neurociència llesta

00:12:49.420 --> 00:12:52.100
per permetre'ns connectar-hi
la ment sense interrupcions.

00:12:52.620 --> 00:12:55.796
I donat que les empreses i els governs
que estan fent aquesta feina

00:12:55.820 --> 00:12:59.476
és probable que es considerin
en una cursa contra la resta,

00:12:59.500 --> 00:13:02.756
i donat que guanyar la cursa
és guanyar el món,

00:13:02.780 --> 00:13:05.236
sempre i quan no
se'l destrueixi tot seguit,

00:13:05.260 --> 00:13:08.076
aleshores és probable que allò
que sembli més fàcil de fer

00:13:08.106 --> 00:13:09.440
sigui el que primer es faci.

00:13:10.380 --> 00:13:13.236
Malauradament, no tinc cap
solució a aquest problema,

00:13:13.260 --> 00:13:15.876
apart de recomanar que
tots hauríem de pensar-hi més.

00:13:15.900 --> 00:13:18.276
Crec que necessitem
una mena de projecte Manhattan

00:13:18.300 --> 00:13:20.316
però per la intel·ligència artificial.

00:13:20.340 --> 00:13:23.076
No per desenvolupar-la,
ja que és inevitable,

00:13:23.100 --> 00:13:26.436
sinó per comprendre com podem
evitar una cursa armementística

00:13:26.460 --> 00:13:29.956
i per desenvolupar-la de manera
que vagi de la mà dels nostres interessos.

00:13:29.980 --> 00:13:32.116
Quan parlem d'una IA superintel·ligent

00:13:32.140 --> 00:13:34.396
que es pot canviar a si mateixa,

00:13:34.420 --> 00:13:39.036
sembla que només tenim una oportunitat
per crear les bones condicions inicials,

00:13:39.060 --> 00:13:41.116
però inclús així haurem d'absorbir

00:13:41.140 --> 00:13:45.020
les conseqüències econòmiques i polítiques
d'haver-les establert correctament.

00:13:45.580 --> 00:13:47.636
Però en el moment que admetem

00:13:47.660 --> 00:13:51.660
que el processat d'informació
és l'origen de la intel·ligència,

00:13:52.540 --> 00:13:57.340
que un sistema de computació apropiat
és la base de tota intel·ligència,

00:13:58.180 --> 00:14:01.940
i que seguirem millorant
aquests sistemes indefinidament,

00:14:03.100 --> 00:14:07.556
i admetem que l'horitzó de la cognició
molt probablement excedeix

00:14:07.580 --> 00:14:08.780
el que sabem fins ara,

00:14:09.940 --> 00:14:11.156
aleshores podrem admetre

00:14:11.180 --> 00:14:13.820
que estem en un procés
de crear una mena de déu.

00:14:15.220 --> 00:14:16.796
Ara és el moment

00:14:16.820 --> 00:14:19.443
d'assegurar-nos que és un déu
amb qui podem viure.

00:14:19.940 --> 00:14:21.476
Moltes gràcies.

00:14:21.500 --> 00:14:23.123
(Aplaudiments)

00:14:23.153 --> 00:14:25.593
Traduït per Marcos Morales Pallarés

