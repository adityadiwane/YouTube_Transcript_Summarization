WEBVTT
Kind: captions
Language: fa

00:00:00.000 --> 00:00:07.000
Translator: Leila Ataei
Reviewer: Bidel Akbari

00:00:13.000 --> 00:00:15.216
میخواهم درباره فقدانِ بصیرتی صحبت کنم

00:00:15.240 --> 00:00:16.840
که بسیاری از ما از آن زیان می‌بینیم.

00:00:17.480 --> 00:00:20.520
این، در واقع ناکامی در 
پیدا کردنِ نوع خاصی ازخطر است.

00:00:21.360 --> 00:00:23.096
میخواهم سناریویی را شرح دهم

00:00:23.120 --> 00:00:26.376
که فکر می‌کنم هم ترسناک است

00:00:26.400 --> 00:00:28.160
و هم وقوع آن محتمل،

00:00:28.840 --> 00:00:30.496
و آنطوری که معلوم می‌شود ترکیب

00:00:30.520 --> 00:00:32.056
خوبی نیست.

00:00:32.080 --> 00:00:34.536
و بجای این که وحشت کنید،
اغلب شما احساس خواهید کرد

00:00:34.560 --> 00:00:36.640
که موضوع مورد صحبت 
یکجورهایی باحال است.

00:00:37.200 --> 00:00:40.176
می‌خواهم شرح دهم که چطور یافته‌های ما

00:00:40.200 --> 00:00:41.976
در هوش مصنوعی می‌تواند

00:00:42.000 --> 00:00:43.776
درنهایت به نابودی ما بیانجامد.

00:00:43.800 --> 00:00:47.256
و در واقع، فکر می‌کنم خیلی سخت است
که ببینیم چطور نابودمان نخواهند کرد

00:00:47.280 --> 00:00:48.960
یا ما را به نابود کردن خودمان سوق ندهند.

00:00:49.400 --> 00:00:51.256
و در عین حال اگر مثل من باشید

00:00:51.280 --> 00:00:53.936
خواهید دید فکر کردن 
به این چیزها جالب است.

00:00:53.960 --> 00:00:57.336
و آن واکنش بخشی از مشکل است.

00:00:57.360 --> 00:00:59.080
درسته؟ آن واکنش باید نگرانتان کند.

00:00:59.920 --> 00:01:02.576
و اگر قرار بود در این سخنرانی متقاعدتان کنم

00:01:02.600 --> 00:01:06.016
که احتمالا از یک قحطی جهانی بخاطر

00:01:06.040 --> 00:01:09.096
تغییرات جوی یا هر فاجعه دیگری رنج ببریم

00:01:09.120 --> 00:01:12.536
و این که نوه‌های ما یا نوه‌های آنها

00:01:12.560 --> 00:01:14.360
هم احتمال زندگی این 
شکلی را دشته باشند،

00:01:15.200 --> 00:01:16.400
با خودتان فکر نمی‌کردید،

00:01:17.440 --> 00:01:18.776
«جالبه.

00:01:18.800 --> 00:01:20.000
از این سخنرانی TED 
خوشم می‌آید.»

00:01:21.200 --> 00:01:22.720
قحطی جالب نیست.

00:01:23.800 --> 00:01:27.176
از سوی دیگر مرگ ناشی از 
ژانر علمی تخیلی جالب است،

00:01:27.200 --> 00:01:31.176
و یکی از چیزهایی که بیش از همه نسبت به 
توسعه هوش صنوعی در این مرحله نگرانم می‌کند

00:01:31.200 --> 00:01:35.296
این است که به نظر می‌رسد ما قادر به ایجاد 
واکنش عاطفی مناسبی

00:01:35.320 --> 00:01:37.136
به خطرات در پیشِ روی خود نبوده‌ایم.

00:01:37.160 --> 00:01:40.360
حتی من که این سخنرانی را انجام می‌دهم،
قادر نیستم این واکنش را ایجاد کنم.

00:01:42.120 --> 00:01:44.816
مثل ایستادن در برابر دو در میماند.

00:01:44.840 --> 00:01:46.096
پشت درب شماره یک

00:01:46.120 --> 00:01:49.416
ما دست از پیشرفت در ساخت 
ماشینهای هوشمند می‌کشیم.

00:01:49.440 --> 00:01:53.456
سخت افزار و نرم افزار کامپیوترمان 
به دلیلی دست از بهتر شدن بر می‌دارد.

00:01:53.480 --> 00:01:56.480
چند لحظه در مورد چرایی این اتفاق تامل کنید.

00:01:57.080 --> 00:02:00.736
منظورم این است که با فرض به دانستن این که
هوش و ماشینی شدن چقدر با ارزشند،

00:02:00.760 --> 00:02:04.280
ما به بهبود فناوری خود ادمه خواهیم داد اگر
امکانش باشد.

00:02:05.200 --> 00:02:06.867
چه چیزی قادر است سد راهمان شود؟

00:02:07.800 --> 00:02:09.600
یک جنگ هسته‌ای تمام عیار؟

00:02:11.000 --> 00:02:12.560
یک بیماری فراگیر جهانی؟

00:02:14.320 --> 00:02:15.640
اصابت یک اخترواره؟

00:02:17.640 --> 00:02:20.216
جاستین بیبر رییس جمهور آمریکا شود؟

00:02:20.240 --> 00:02:22.520
(خنده)

00:02:24.760 --> 00:02:28.680
نکته این است که باید چیزی باشد که 
تمدن را نابود کند.

00:02:29.360 --> 00:02:33.656
باید تصور کنید چقدر بد باید باشد

00:02:33.680 --> 00:02:37.016
تا برای همیشه مانع ما در انجام اصلاحات 
در این فناوری

00:02:37.040 --> 00:02:38.256
گردد،

00:02:38.280 --> 00:02:40.296
در همه نسلها.

00:02:40.320 --> 00:02:42.456
تقریبا طبق تعریف
این بدترین چیزی است

00:02:42.480 --> 00:02:44.496
که تابحال در تاریخ بشر اتفاق افتاده.

00:02:44.520 --> 00:02:45.816
بنابراین تنها جایگزین

00:02:45.840 --> 00:02:48.176
که خب پشت درب شماره دو خوابیده

00:02:48.200 --> 00:02:51.336
این است که به بهسازی ماشینهای 
هوشمند هر سالی که بگذرد

00:02:51.360 --> 00:02:52.960
ادامه دهیم.

00:02:53.720 --> 00:02:57.360
در یک نقطه خاص، ماشینهایی را خواهیم است 
که از ما باهوش‌تر هستند،

00:02:58.080 --> 00:03:00.696
و وقتی ماشینهایی بسازیم
که از ما باهوش‌تر باشند،

00:03:00.720 --> 00:03:02.696
آنها شروع به اصلاح خود می کنند.

00:03:02.720 --> 00:03:05.456
و بعد خطر چیزی را خواهیم داشت 
که ای جی گود ریاضیدان

00:03:05.480 --> 00:03:07.256
«انفجار هوش» نامید،

00:03:07.280 --> 00:03:09.280
این که این پروسه از کنترل ما خارج شود.

00:03:10.120 --> 00:03:12.936
این مساله بیشتر به شکلی که 
من اینجا نشان می‌دهم

00:03:12.960 --> 00:03:16.176
مثل ترس از این که ارتش‌هایی 
از رباتهای بدطینت

00:03:16.200 --> 00:03:17.456
به ما حمله کنند، ترسیم شده.

00:03:17.480 --> 00:03:20.176
اما این، سناریوی چندان محتملی نیست.

00:03:20.200 --> 00:03:25.056
اینطور نیست که ماشینهای ساخت دست ما به یکباره هیولا شوند.

00:03:25.080 --> 00:03:27.696
نگرانی اینجاست که ماشین‌هایی خواهیم ساخت

00:03:27.720 --> 00:03:29.776
که بسیار توانمندتر از ما هستند

00:03:29.800 --> 00:03:33.576
جوریکه کمترین انحراف در بین 
اهداف ما و آنها

00:03:33.600 --> 00:03:34.800
می تواند ما را نابود کند.

00:03:35.960 --> 00:03:38.040
فقط به نحوه ارتباط ما با مورچه‌ فکر کنید.

00:03:38.600 --> 00:03:40.256
از آنها متنفر نیستیم.

00:03:40.280 --> 00:03:42.336
راه مان را برای کشتنشان کج نمی‌کنیم.

00:03:42.360 --> 00:03:44.736
در واقع،‌ گاهی برای آسیب نرساندن 
به آنها خود را به زحمت می‌اندازیم.

00:03:44.760 --> 00:03:46.776
در پیاده رو از رویشان می‌پریم.

00:03:46.800 --> 00:03:48.936
اما هر وقت در جایی حضورشان

00:03:48.960 --> 00:03:51.456
بطور جدی با یکی از اهداف 
ما منافعات داشته باشد،

00:03:51.480 --> 00:03:53.957
برای مثال وقتی ساختمانی مثل این را بسازیم،

00:03:53.981 --> 00:03:55.941
آنها را بی‌درنگ قلع و قمع می‌کنیم.

00:03:56.480 --> 00:03:59.416
نگرانی این است که یک روز
ماشین‌هایی را خواهیم ساخت

00:03:59.440 --> 00:04:02.176
که چه آگاه باشند یا نباشند

00:04:02.200 --> 00:04:04.200
می‌توانند این چنین بی‌ملاحظه
با ما رفتار کنند.

00:04:05.760 --> 00:04:08.520
البته خیلی از شما آن را خطری
در دراز مدت می‌ببینید.

00:04:09.360 --> 00:04:15.696
شرط می‌بندم کسانی در بین شما هستند 
که به احتمال این ماشین ابرهوشمند شک داشته

00:04:15.720 --> 00:04:17.376
و آن را کمابیش بعید می‌دانند.

00:04:17.400 --> 00:04:21.020
اما بعد باید به غلط بودن
یکی از فرضیه‌های زیر برسید.

00:04:21.044 --> 00:04:22.616
و آنها سه تا بیشتر نیستند.

00:04:23.800 --> 00:04:28.519
هوشمندی موضوع پردازش اطلاعات
در سیستمهای فیزیکی است.

00:04:29.320 --> 00:04:31.935
راستش کمی بیشتر از یک فرض است.

00:04:31.959 --> 00:04:35.416
درحال حاضر میزان اندکی از هوشمندی
را در داخل ماشینهایمان بکار بردیم،

00:04:35.440 --> 00:04:37.456
و خیلی از این ماشینها همین الان نیز

00:04:37.480 --> 00:04:40.120
عملکردی در سطح هوش
یک انسان فوق العاده باهوش دارند.

00:04:40.840 --> 00:04:43.416
و می‌دانیم که خود این مسله صرفا می‌تواند

00:04:43.440 --> 00:04:46.056
منجر به پیداش به اصطلاح
« هوشمندی عمومی» شود،

00:04:46.080 --> 00:04:49.736
توانایی اندیشیدنِ منعطف
در سرتاسر حوزه‌های مختلف،

00:04:49.760 --> 00:04:52.896
چون که مغزهایمان در انجامش موفق بودند، 
اینطور نیست؟

00:04:52.920 --> 00:04:56.856
منظورم این است که در اینجا فقط اتم است،

00:04:56.880 --> 00:05:01.376
و تا مادامی که سیستمی اتمی را می‌سازیم

00:05:01.400 --> 00:05:04.096
که رفتار هوشمندانه بیشتر و بیشتری نشان می‌دهند،

00:05:04.120 --> 00:05:06.656
مگر این که دچار وقفه شویم، وگرنه

00:05:06.680 --> 00:05:10.056
ما نهایتا هومندیش عمومی را در ماشینهایمان

00:05:10.080 --> 00:05:11.376
بکار خواهیم برد.

00:05:11.400 --> 00:05:15.056
مهم است که تشخیص دهیم میزان 
پیشرفت اهمیت ندارد،

00:05:15.080 --> 00:05:18.256
زیرا هر پیشرفتی برای رساندن ما به 
منطقه نهایی کافی است.

00:05:18.280 --> 00:05:22.056
برای ادامه نیاز به قانون مورفی نداریم.
نیاز به پیشرفت تصاعدی نداریم.

00:05:22.080 --> 00:05:23.680
فقط باید ادامه دهیم.

00:05:25.480 --> 00:05:28.400
فرض دوم این است که 
ادامه خواهیم داد.

00:05:29.000 --> 00:05:31.760
به بهسازی ماشینهای هوشمند
خود ادامه می‌دهیم.

00:05:33.000 --> 00:05:37.376
و با فرض ارزش هوشمندی--

00:05:37.400 --> 00:05:40.936
منظورم این است که خواه هوشمندی منبع 
همه چیزهایی است که برایمان ارزش دارد

00:05:40.960 --> 00:05:43.736
یا که برای محافظت از چیزهای باارزشمان
به آن نیاز داریم.

00:05:43.760 --> 00:05:46.016
مهمترین منبع باارزش ما است.

00:05:46.040 --> 00:05:47.576
بنابراین می‌خواهیم
این را انجام دهیم.

00:05:47.600 --> 00:05:50.936
مشکلاتی که مایوسانه نیاز به حل شان داریم.

00:05:50.960 --> 00:05:54.160
میخواهیم امراضی چون آلزایمر 
و سرطان را درمان کنیم.

00:05:54.960 --> 00:05:58.896
می‌خواهیم سیستمهای اقتصادی را بفهمیم.
می‌خواهیم دانش اقلیمی خویش را بهبود ببخشیم.

00:05:58.920 --> 00:06:01.176
پس اگر بتوانیم این کار را می‌کنیم.

00:06:01.200 --> 00:06:04.486
قطار دیگر از ایستگاه خارج شده،
و ترمزی برای کشیدن نیست.

00:06:05.880 --> 00:06:11.336
سرآخر این که احتمالا ما در راس هوش 
یا جایی نزدیک آن

00:06:11.360 --> 00:06:13.160
نخواهیم ایستاد.

00:06:13.640 --> 00:06:15.536
و این یک بینش واقعا ضروری است.

00:06:15.560 --> 00:06:17.976
این چیزی است که شرایط 
ما را پرمخاطره می‌کند،

00:06:18.000 --> 00:06:22.040
و این چیزی است که درک ما از
ریسک را شدیدا غیرقابل اتکا می‌کند.

00:06:23.120 --> 00:06:25.840
الان باهوش‌ترین فردی که تا به امروز زیسته
را در نظر بگیرید.

00:06:26.640 --> 00:06:30.056
اینجا تقریبا در فهرست کوتاه همه 
جان فان نیومن وجود دارد.

00:06:30.080 --> 00:06:33.416
منظورم تاثیری است
که فان نیومن بر اطرافیانش داشته

00:06:33.440 --> 00:06:37.496
و این دربرگیرنده بزرگترین ریاضیدانها 
و فیزیکدانهای هم دوره‌اش می‌شود،

00:06:37.520 --> 00:06:39.456
که نسبتا بخوبی ثبت شده است.

00:06:39.480 --> 00:06:43.256
اگر فقط بخشی از نیمی از داستانهای
درباره او درست باشد

00:06:43.280 --> 00:06:44.496
شکی نیست

00:06:44.520 --> 00:06:46.976
که او از باهوشترین آدمهایی است 
که تابحال زیسته.

00:06:47.000 --> 00:06:49.520
بنابراین گستره هوش را در نظر بگیرید.

00:06:50.320 --> 00:06:51.749
در اینجا جان فان نیومن را داریم.

00:06:53.560 --> 00:06:54.894
و بعد نیز امثال من و شما.

00:06:56.120 --> 00:06:57.416
و البته مرغ را هم داریم.

00:06:57.440 --> 00:06:59.376
(خنده)

00:06:59.400 --> 00:07:00.616
متاسفم، یک مرغ.

00:07:00.640 --> 00:07:01.896
(خنده)

00:07:01.920 --> 00:07:05.656
دلیلی ندارم که این سخنرانی
را نامیدکننده‌تر از حد نیاز کنم.

00:07:05.680 --> 00:07:07.280
(خنده)

00:07:08.339 --> 00:07:11.816
بحد قابل توجهی همینطور است،‌هر چند 
گستره هوش

00:07:11.840 --> 00:07:14.960
خیلی وسیعتر از برداشت کنونی ما است،

00:07:15.880 --> 00:07:19.096
و اگر ماشینهایی را بسازیم
که از ما هوشمندترند،

00:07:19.120 --> 00:07:21.416
آنها احتمالا این گستره را بررسی
خواهند کرد

00:07:21.440 --> 00:07:23.296
به ترتیبی که ما قادر به تصورش نیستیم،

00:07:23.320 --> 00:07:25.840
و طوری از ما پیشی خواهند گرفت
که تصورش را هم نمی‌کنیم.

00:07:27.000 --> 00:07:31.336
و تشخیص این نکته مهم است که این مساله
بواسطه خصیصه سرعت معتبر است.

00:07:31.360 --> 00:07:36.416
درسته؟ پس ساخت یک هوش مصنوعی
ابرهوشمندی را تصور کنید

00:07:36.440 --> 00:07:39.896
که باهوشتر از یک 
تیم معمولی تحقیقاتی شما

00:07:39.920 --> 00:07:42.216
در استانفورد یا MIT نباشد.

00:07:42.240 --> 00:07:45.216
خب، مدارهای الکترونیکی کارکردی
حدود یک میلیون بار سریعتر

00:07:45.240 --> 00:07:46.496
از انواع زیست شیمی دارند

00:07:46.520 --> 00:07:49.656
بنابراین این ماشین شاید یک میلیون بار
سریعتر از ذهنهایی

00:07:49.680 --> 00:07:51.496
که آن را ساختند فکر کند.

00:07:51.520 --> 00:07:53.176
پس یک هفته‌ای که از 
راه‌اندازیش بگذرد،

00:07:53.200 --> 00:07:57.760
هر هفته‌ای که سپری بگذرد
عملکردی برابر ۲۰٫۰۰۰ سال 

00:07:58.400 --> 00:08:00.360
کار اندیشمدانه در سطح بشر خواهد داشت.

00:08:01.640 --> 00:08:04.736
ما چطور قادر به درک ذهنی باشیم که چنین 
نوعی از پیشرفت را ایجاد می‌کند

00:08:04.760 --> 00:08:07.040
چه برسد به محدود کردن آن؟

00:08:08.840 --> 00:08:10.976
صادقانه بگویم، یک از چیزهای دیگری 
که نگران کننده است،

00:08:11.000 --> 00:08:15.976
این است که - 
بهترین سناریوی ممکن را تصور کنید.

00:08:16.000 --> 00:08:20.176
تصور کنید به طراحی از یک هوش مصنوعی
ابرهوشمند رسیده‌ایم

00:08:20.200 --> 00:08:21.576
که هیچ مشکل امنیتی ندارد.

00:08:21.600 --> 00:08:24.856
برای اولین بار بهترین طراحی را داریم.

00:08:24.880 --> 00:08:27.096
مثل این میماند که ما به دانشمندی
دست پیدا کرده‌ایم

00:08:27.120 --> 00:08:29.136
که عینا طبق خواسته رفتار می‌کند.

00:08:29.160 --> 00:08:32.880
خب این ماشین می‌تواند بهترین 
وسیله در صرفه‌جویی نیروی کار باشد.

00:08:33.680 --> 00:08:36.109
می‌تواند ماشینی را طراحی کند
که خود ماشینی را بسازد

00:08:36.133 --> 00:08:37.896
که توان انجام هر کار فیزیکی را دارد،

00:08:37.920 --> 00:08:39.376
با استفاده از نور خورشید،

00:08:39.400 --> 00:08:42.096
کمابیش بخاطر هزینه مصالح خام.

00:08:42.120 --> 00:08:45.376
پس درباره به پایان رسیدن 
جان کندن بشر صحبت می‌کنیم.

00:08:45.400 --> 00:08:48.200
و همچنین به پایان رسیدن اغلب کارهایی 
که با عقل و خِرَد سروکار دارند.

00:08:49.200 --> 00:08:52.256
پس در شرایط این چنینی چه 
بر سر میمون‌هایی مثل ما می‌آید؟

00:08:52.280 --> 00:08:56.360
خب، ما در فریزبی بازی کردن 
و ماساژ دادن یکدیگر آزاد خواهیم بود.

00:08:57.840 --> 00:09:00.696
و همینطور یکسری داروهای روانگردان و 
انتخابهای سوال‌برانگیز توی کمد لباس،

00:09:00.720 --> 00:09:02.896
و کل دنیا چیزی شبیه به جشنواره 
مرد سوزان می‌شود.

00:09:02.920 --> 00:09:04.560
(خنده)

00:09:06.320 --> 00:09:08.320
الان شاید خوشایند به گوش برسد

00:09:09.280 --> 00:09:11.656
اما از خود سوال کنید چه اتفاقی برای

00:09:11.680 --> 00:09:14.416
نظام سیاسی و اقتصادی کنونی ما خواهد افتاد؟

00:09:14.440 --> 00:09:16.856
محتمل است که ما شاهد سطحی از نابرابری

00:09:16.880 --> 00:09:21.016
ثروت و بیکاری باشیم

00:09:21.040 --> 00:09:22.536
که بی‌سابقه است.

00:09:22.560 --> 00:09:25.176
فقدان اراده در بکارگیری فوری این ثروت تازه

00:09:25.200 --> 00:09:26.680
در خدمت کل بشریت.

00:09:27.640 --> 00:09:31.256
اندک تریلیونرها قادر به عرض اندام
در روی جلد مجلات تجاری هستند

00:09:31.280 --> 00:09:33.720
در حالیکه بقیه دنیا
از گرسنگی رنج خواهد بردد.

00:09:34.320 --> 00:09:36.616
روسها یا چینی‌ها 
چه کار می‌کنند

00:09:36.640 --> 00:09:39.256
اگر بشنوند که یک کمپانی 
در سیلیکون ولی

00:09:39.280 --> 00:09:42.016
در آستانه بکارگیری هوش مصنوعی
اَبَرهوشمند است؟

00:09:42.040 --> 00:09:44.896
ماشینی که قادر به ادامه جنگ است،

00:09:44.920 --> 00:09:47.136
خواه زمینی یا سایبری،

00:09:47.160 --> 00:09:48.840
با قدرتی بی‌سابقه.

00:09:50.120 --> 00:09:51.976
این یک سناریویی‌ست که در آن
برنده همه چیز را بدست می‌آورد.

00:09:52.000 --> 00:09:55.136
در این رقابت شش ماه پیش بودن

00:09:55.160 --> 00:09:57.936
یعنی حداقل ۵۰۰٫۰۰۰ سال

00:09:57.960 --> 00:09:59.456
جلوتر بودن.

00:09:59.480 --> 00:10:04.216
پس به نظر می‌رسد که حتی شایعات بیشتر از 
این پیشرفتها

00:10:04.240 --> 00:10:06.616
می‌تواند منجر به از خود بیخود شدن 
نوع بشر شود.

00:10:06.640 --> 00:10:09.536
الان یکی از وحشتناکترین چیزها از دیدگاه من

00:10:09.560 --> 00:10:12.336
در این لحظه همه آن چیزهایی هست

00:10:12.360 --> 00:10:16.656
که محققان هوش مصنوعی

00:10:16.680 --> 00:10:18.240
به ما می‌گویند، وقتی که 
می‌خواهند اطمینان بخش باشند.

00:10:19.000 --> 00:10:22.456
و شایع‌ترین دلیلی که برای
نگران نبودن به ما می‌گویند زمان است.

00:10:22.480 --> 00:10:24.536
کلی زمان‌بر است،‌ مگر خبر ندارید.

00:10:24.560 --> 00:10:27.000
شاید ۵۰ یا ۱۰۰ سال بعد.

00:10:27.720 --> 00:10:28.976
یک محققی می‌گفت،

00:10:29.000 --> 00:10:30.576
«نگران بودن درباره امنیت هوش مصنوعی

00:10:30.600 --> 00:10:32.880
مثل نگران بودن درباره
رشد بیحد جمعیت در مریخ است.»

00:10:34.116 --> 00:10:35.736
در واقع آنها می‌گویند:

00:10:35.760 --> 00:10:38.136
« فکرهای کوچک تان را در این مورد نگران نکنید.»

00:10:38.160 --> 00:10:39.496
(خنده)

00:10:39.520 --> 00:10:41.416
به نظر نمیاید کسی متوجه باشد

00:10:41.440 --> 00:10:44.056
که ارجاع دادن به افق زمانی

00:10:44.080 --> 00:10:46.656
کلا نامربوط است.

00:10:46.680 --> 00:10:49.936
اگر هوش صرفا مساله پردازش اطلاعات باشد

00:10:49.960 --> 00:10:52.616
و ما به اصلاح ماشینهای خود ادامه دهیم،

00:10:52.640 --> 00:10:55.520
شکلی از هوش ماورایی را تولید خواهیم کرد.

00:10:56.320 --> 00:10:59.976
و ایده‌ای نداریم که چقدر زمان می‌برد

00:11:00.000 --> 00:11:02.400
تا شرایط انجام این کار
را به شکل ایمن فراهم کنیم.

00:11:04.200 --> 00:11:05.496
بگذارید باز هم بگویم.

00:11:05.520 --> 00:11:09.336
ایده‌ای نداریم که چقدر زمان می‌برد

00:11:09.360 --> 00:11:11.600
تا شرایط انجام این کار
را به شکل ایمن فراهم کنیم.

00:11:12.920 --> 00:11:16.376
و اگر متوجه نشده باشید
۵۰ سال، دیگر مثل سابق نیست.

00:11:16.400 --> 00:11:18.856
این، ماه‌های تشکیل دهنده‌ی ۵۰ سال است.

00:11:18.880 --> 00:11:20.720
این، مدتی‌ست که ما 
صاحب آیفون هستیم.

00:11:21.440 --> 00:11:24.040
در این مدت سریال 
خانواده سیمپسون پخش شده.

00:11:24.680 --> 00:11:27.056
پنجاه سال زمان چندان زیادی

00:11:27.080 --> 00:11:30.240
برای روبرو شدن با یکی از بزرگترین 
چالشهایی که نوع بشر با آن روبروست، نیست.

00:11:31.640 --> 00:11:35.656
یکبار دیگر به نظر میاید که ما در 
داشتن جواب عاطفی مناسب نسبت به آنچه

00:11:35.680 --> 00:11:38.376
که دلایل کافی برای ظهورش را داریم 
ناموفق عمل کرده‌ایم.

00:11:38.400 --> 00:11:42.376
دانشمند علوم رایانه‌ استورات راسل 
قیاس جالبی دارد.

00:11:42.400 --> 00:11:47.296
او گفت، تصور کنید
پیغامی را از یک تمدن فضایی دریافت کنیم

00:11:47.320 --> 00:11:49.016
که می‌گوید،

00:11:49.040 --> 00:11:50.576
«مردم زمین،

00:11:50.600 --> 00:11:52.960
ما ۵۰ سال دیگر به سیاره شما می‌رسیم.

00:11:53.800 --> 00:11:55.376
آماده باشید.»

00:11:55.400 --> 00:11:59.656
و اکنون ما فقط به شمارش معکوس 
ماه‌ها مشغولیم تا ناوِ فضایی فرود بیاید؟

00:11:59.680 --> 00:12:02.680
لازم است که کمی بیشتر در این باره 
احساس ضرورت کنیم.

00:12:04.680 --> 00:12:06.536
دلیل دیگری که به ما می‌گویند نگران نباشید

00:12:06.560 --> 00:12:09.576
این است که این ماشین‌ها راهی جز 
سهیم شدن در ارزشهای ما ندارند

00:12:09.600 --> 00:12:12.216
چون که آنها به معنای واقعی کلمه، 
ما را بسط می‌دهند.

00:12:12.240 --> 00:12:14.056
آنها به مغزهای ما پیوند زده می‌شوند

00:12:14.080 --> 00:12:16.440
و ما در اصل دستگاه‌های کناره‌ای
(Limbic system) آنها خواهیم شد.

00:12:17.120 --> 00:12:18.536
الان لحظه‌ای به این فکر کنید

00:12:18.560 --> 00:12:21.736
که ایمن‌ترین و تنها مسیر محتاط رو به جلوی

00:12:21.760 --> 00:12:23.096
توصیه شده

00:12:23.120 --> 00:12:25.920
تعبیه مستقیم این فناوری در مغزهای ما باشد.

00:12:26.600 --> 00:12:29.976
اکنون، شاید این ایمن‌ترین و 
تنها مسیر محتاط رو به جلوست،

00:12:30.000 --> 00:12:33.056
اما معمولا دغدغه‌های ایمنی 
یک نفر درباره فناوری

00:12:33.080 --> 00:12:36.736
باید حسابی مورد بررسی قرار گرفته باشد 
پیش از این بخواهید که آن را تو کله‌ خود فرو کنید.

00:12:36.760 --> 00:12:38.776
(خنده)

00:12:38.800 --> 00:12:44.136
مشکل عمیق‌تر این است 
که ساختن هوش مصنوعی ابرهوشمند مستقل

00:12:44.160 --> 00:12:45.896
به نظر آسانتر از ساختن

00:12:45.920 --> 00:12:47.776
هوش مصنوعی ابرهوشمند

00:12:47.800 --> 00:12:49.576
و داشتن علم عصب شناسی کاملی میاید

00:12:49.600 --> 00:12:52.280
که به ما اجازه یکپارچه کردن بی‌درز
ذهن‌های خویش با آن را می‌دهد.

00:12:52.800 --> 00:12:55.976
و با فرض این که کمپانی‌ها 
و دولت‌هایی این کار را می‌کنند

00:12:56.000 --> 00:12:59.656
محتمل به پی بردن به موجودیت خود 
در این مسابقه علیه دیگران هستند،

00:12:59.680 --> 00:13:02.936
با این فرض که بردن این مسابقه
به معنی بردن جهان است،

00:13:02.960 --> 00:13:05.416
به شرطی که در لحظه بعدی آن را نابود نکنید،

00:13:05.440 --> 00:13:08.056
که احتمال وقوع هر چیزی که آسانتر باشد

00:13:08.080 --> 00:13:09.280
اول از همه می‌رود.

00:13:10.560 --> 00:13:13.416
شوربختانه الان راه‌حلی
برای این مشکل ندارم

00:13:13.440 --> 00:13:16.056
بجز توصیه‌ام برای بیشتر فکردن کردن 
درباره آن.

00:13:16.080 --> 00:13:18.456
فکر می‌کنم بیشتر به چیزی مثل 
پروژه منهتن در موضوع

00:13:18.480 --> 00:13:20.496
هوش مصنوعی نیاز داریم.

00:13:20.520 --> 00:13:23.256
نه درباره ساختن آن، چون به نظرم 
بالاخره این کار را خواهیم کرد،

00:13:23.280 --> 00:13:26.616
بلکه درک نحوه اجتناب
از یک مسابقه تسلیحاتی

00:13:26.640 --> 00:13:30.136
و ساخت آن به نحوی که 
هم راستا با منافع ما باشد.

00:13:30.160 --> 00:13:32.296
زمانی که راجع به هوش مصنوعی
ابرهوشمند صحبت می‌کنید

00:13:32.320 --> 00:13:34.576
که قادر به انجام تغییرات در خویش است،

00:13:34.600 --> 00:13:39.216
به نظر میاید که ما تنها از یک شانس برای
رسیدن به شرایط آغازین صحیح برخورداریم

00:13:39.240 --> 00:13:41.296
و حتی بعد لازم است به جذب

00:13:41.320 --> 00:13:44.360
پیامدهای سیاسی و اقتصادی
درست بودن آنها بپردازیم.

00:13:45.760 --> 00:13:47.816
اما لحظه‌ای که می‌پذیریم

00:13:47.840 --> 00:13:51.840
پردازش اطلاعات منبع هوشمندی است،

00:13:52.720 --> 00:13:57.520
که یک نظام محاسباتی مناسب
مبنای هوشمندی است،

00:13:58.360 --> 00:14:02.120
و بپذیریم که این نظامها
را در ادامه اصلاح خواهیم کرد،

00:14:03.280 --> 00:14:07.736
و بپذیریم که افق فرایند یادگیری و شناخت
به احتمال زیاد فراتر

00:14:07.760 --> 00:14:08.960
از دانش فعلی ما می‌رود

00:14:10.120 --> 00:14:11.336
در ادامه باید بپذیریم

00:14:11.360 --> 00:14:14.000
که ما در فرایند ساخت نوعی از خدا هستیم.

00:14:15.400 --> 00:14:16.976
الان زمان خوبی است

00:14:17.000 --> 00:14:18.953
مطمئن شویم خدایی است
که می‌توان با آن زندگی کرد.

00:14:20.120 --> 00:14:21.656
از شما خیلی ممنونم.

00:14:21.680 --> 00:14:26.773
(تشویق)

