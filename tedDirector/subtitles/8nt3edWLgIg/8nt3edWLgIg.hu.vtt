WEBVTT
Kind: captions
Language: hu

00:00:00.000 --> 00:00:07.000
Fordító: Rita Hajnal
Lektor: Reka Lorinczy

00:00:13.000 --> 00:00:15.216
A hibás előérzetekről fogok beszélni,

00:00:15.240 --> 00:00:16.840
amitől oly sokan szenvedünk.

00:00:17.480 --> 00:00:20.520
Valóban hibás egy bizonyos fajta 
veszély felismerése.

00:00:21.360 --> 00:00:23.096
Felvázolok egy forgatókönyvet,

00:00:23.120 --> 00:00:26.376
amelyről úgy gondolom egyszerre ijesztő,

00:00:26.400 --> 00:00:28.420
és bekövetkezése egyben valószínűsíthető,

00:00:28.680 --> 00:00:30.536
és ez a kettő együtt nem jó kombináció,

00:00:30.536 --> 00:00:32.420
ahogy az bebizonyosodik majd.

00:00:32.420 --> 00:00:35.390
Ennek ellenére a többségetek
az ijedtség helyett úgy érzi majd,

00:00:35.390 --> 00:00:37.200
hogy amiről beszélek, egy menő dolog.

00:00:37.200 --> 00:00:39.820
El fogom magyarázni,

00:00:39.820 --> 00:00:41.976
hogy a mesterséges intelligencia fejlődése

00:00:42.000 --> 00:00:43.776
miként semmisíthet meg bennünket.

00:00:43.800 --> 00:00:47.256
Úgy vélem, nehezebb felismerni,
hogyan nem pusztítanak el minket,

00:00:47.280 --> 00:00:49.660
vagy buzdítanak minket 
önmagunk elpusztítására.

00:00:49.660 --> 00:00:51.256
De ha kicsit is hasonlítasz rám,

00:00:51.280 --> 00:00:53.936
rájössz majd, mennyire vicces dolog 
ezeken elmélkedni.

00:00:53.960 --> 00:00:57.336
És ez a fajta reakció 
szintén a probléma része.

00:00:57.360 --> 00:00:59.860
Értitek ugye? A reakció miatt 
aggódnotok kellene.

00:00:59.920 --> 00:01:02.576
Ha az előadással meg tudlak 
győzni benneteket,

00:01:02.600 --> 00:01:06.016
hogy a globális éhínség várhatóan 
bekövetkezik majd,

00:01:06.040 --> 00:01:09.096
klímaváltozás vagy 
más katasztrófa hatására,

00:01:09.120 --> 00:01:12.536
és hogy az unokátokra, vagy az ő unokáikra

00:01:12.560 --> 00:01:14.550
nagy valószínűséggel hasonló élet vár,

00:01:15.200 --> 00:01:16.580
magatokban nem gondolnátok:

00:01:17.440 --> 00:01:18.776
"Érdekes.

00:01:18.800 --> 00:01:20.200
Tetszik ez a TED előadás."

00:01:21.200 --> 00:01:22.720
Az éhínség nem vicc.

00:01:23.800 --> 00:01:27.176
A tudományos-fantasztikum általi halál
ezzel ellentétben vicces,

00:01:27.200 --> 00:01:31.176
de ami engem aggaszt leginkább,
a MI jelenlegi állapotát illetően,

00:01:31.200 --> 00:01:35.296
hogy képtelenek vagyunk 
az előttünk álló veszélyekhez

00:01:35.320 --> 00:01:37.136
megfelelő érzelmi reakciót párosítani.

00:01:37.160 --> 00:01:40.360
Képtelen vagyok irányítani a reakciót,
pedig én tartom az előadást.

00:01:42.120 --> 00:01:44.816
Olyan, mintha két ajtó előtt állnánk.

00:01:44.840 --> 00:01:46.286
Az egyes számú ajtó mögött

00:01:46.286 --> 00:01:48.456
leállunk az intelligens gépek 
fejlesztésével.

00:01:49.436 --> 00:01:53.456
Valamilyen okból a hardverek és 
szoftverek fejlesztése leáll.

00:01:53.480 --> 00:01:56.480
Egy percre kérlek, gondoljatok bele,
miért történne ilyesmi.

00:01:57.080 --> 00:02:00.736
Tudván, mekkora érték rejlik
az intelligenciában és automatizálásban,

00:02:00.760 --> 00:02:04.280
ha mód van rá, folytatni fogjuk 
a technológiai fejlesztéseket.

00:02:05.200 --> 00:02:06.867
Mi állhatna az utunkba?

00:02:07.800 --> 00:02:09.600
Egy mindent elsöprő nukleáris háború?

00:02:11.000 --> 00:02:12.560
Egy globális járvány?

00:02:14.320 --> 00:02:15.640
Egy aszteroida becsapódása?

00:02:17.640 --> 00:02:20.216
Justin Biebert megválasztják elnöknek?

00:02:20.240 --> 00:02:22.520
(Nevetés)

00:02:24.760 --> 00:02:28.680
Bármi legyen is, az általunk ismert 
civilizációt kellene megsemmisítse.

00:02:29.360 --> 00:02:33.656
Képzeld el, mennyire borzasztónak
kellene lennie ahhoz,

00:02:33.680 --> 00:02:37.016
hogy leálljunk a technológiai 
fejlesztésekkel,

00:02:37.040 --> 00:02:38.406
egyik nemzedék a másik után,

00:02:38.406 --> 00:02:40.296
mindörökre.

00:02:40.320 --> 00:02:42.536
Fogalmazhatnánk úgy is, 
hogy ez a legrosszabb,

00:02:42.536 --> 00:02:44.806
ami valaha történhet
az emberiség történelmében.

00:02:44.806 --> 00:02:46.336
Így az egyetlen alternatíva,

00:02:46.336 --> 00:02:48.276
és ez rejlik a kettes számú ajtó mögött,

00:02:48.276 --> 00:02:51.336
hogy folytatjuk 
az intelligens gépek fejlesztését,

00:02:51.360 --> 00:02:52.960
egyik évről a másikra.

00:02:53.720 --> 00:02:57.360
És elérkezünk a ponthoz, amikor nálunk 
okosabb gépeket hozunk létre.

00:02:58.080 --> 00:03:00.696
És amint létrehoztuk 
a minket meghaladó okos gépeket,

00:03:00.720 --> 00:03:02.696
megkezdik majd önmaguk fejlesztését.

00:03:02.720 --> 00:03:05.456
És ezzel megkockáztatjuk azt,
amit IJ Good matematikus

00:03:05.480 --> 00:03:07.256
úgy hívott: "intelligencia-robbanás".

00:03:07.280 --> 00:03:09.770
Amikor a fejlődésük 
kicsúszik a kezünk közül.

00:03:10.120 --> 00:03:12.936
Sokszor kifigurázzák, mint én is itt,

00:03:12.960 --> 00:03:16.176
miként rettegünk a rosszindulatú robotok

00:03:16.200 --> 00:03:17.456
támadásától.

00:03:17.480 --> 00:03:20.176
De nem ez a legvalószínűbb forgatókönyv.

00:03:20.200 --> 00:03:25.056
A gépek nem fognak egyik pillanatról 
a másikra gonosszá válni.

00:03:25.080 --> 00:03:28.036
A valós aggály abban rejlik, 
hogy olyan gépeket hozunk létre,

00:03:28.036 --> 00:03:30.206
melyek meghaladják a mi képességeinket,

00:03:30.316 --> 00:03:33.576
és a legapróbb összeférhetetlenség
az ő és saját céljaink között

00:03:33.600 --> 00:03:34.800
a végünket jelentheti.

00:03:35.960 --> 00:03:38.870
Csak gondoljatok bele, mi hogyan 
viszonyulunk a hangyákhoz.

00:03:38.870 --> 00:03:40.180
Nem utáljuk őket.

00:03:40.180 --> 00:03:42.436
Nem akarjuk szándékosan bántalmazni őket.

00:03:42.436 --> 00:03:44.846
Sőt, néha megóvásukra 
áldozatokat is hozunk értük.

00:03:44.846 --> 00:03:46.776
Kikerüljük őket a járdán.

00:03:46.800 --> 00:03:48.936
De amint összeütközik a jelenlétük

00:03:48.960 --> 00:03:51.456
a céljaink megvalósításával,

00:03:51.480 --> 00:03:53.957
mint például egy ilyen
épület felhúzásakor,

00:03:53.981 --> 00:03:55.941
szemrebbenés nélkül kiírtjuk valamennyit.

00:03:56.480 --> 00:03:59.416
A gond az, hogy egy napon 
olyan gépeket hozunk létre,

00:03:59.440 --> 00:04:02.176
amelyek, akár tudatosak akár nem,

00:04:02.200 --> 00:04:04.200
hasonlóképpen viszonyulnak majd hozzánk.

00:04:05.760 --> 00:04:08.520
Gondolom, ez sokatok számára, 
elképzelhetetlennek tűnik.

00:04:09.360 --> 00:04:15.696
Fogadni mernék, hogy egyesek a MI 
lehetőségét is megkérdőjelezik,

00:04:15.720 --> 00:04:17.376
mintsem elkerülhetetlennek véljék.

00:04:17.400 --> 00:04:21.020
De akkor valami mégsem stimmel 
a következő feltevésekkel.

00:04:21.044 --> 00:04:22.616
Amiből mindössze csak három van.

00:04:23.800 --> 00:04:28.519
Az intelligencia a fizikai rendszerekben
feldolgozott információból származik.

00:04:29.320 --> 00:04:31.935
Tulajdonképpen, ez kicsivel 
több puszta feltevésnél.

00:04:31.959 --> 00:04:35.416
A közvetlen intelligenciát 
már beépítettük a gépeinkbe,

00:04:35.440 --> 00:04:37.456
mely gépek többsége már most

00:04:37.480 --> 00:04:40.120
emberfeletti teljesítményekre képes.

00:04:40.840 --> 00:04:43.416
Azt is tudjuk, hogy az anyag önmagában

00:04:43.440 --> 00:04:46.326
életre tudja hívni az úgynevezett
"alap intelligenciát,"

00:04:46.326 --> 00:04:49.736
a különböző tartományokon belüli 
rugalmas gondolkodás képességét,

00:04:49.760 --> 00:04:52.896
hiszen saját agyunk is 
képes volt rá, nem igaz?

00:04:52.920 --> 00:04:56.856
Mivel itt legbelül csak atomok vannak,

00:04:56.880 --> 00:05:01.376
és amíg atomokból álló 
rendszereket építünk,

00:05:01.400 --> 00:05:04.096
melyek egyre intelligensebb 
viselkedést mutatnak,

00:05:04.120 --> 00:05:06.656
akkor egyszer, hacsak nem lépünk közbe,

00:05:06.680 --> 00:05:10.056
akkor egyszer megépítjük 
az alap intelligenciával

00:05:10.080 --> 00:05:11.376
rendelkező gépeket.

00:05:11.400 --> 00:05:15.056
Mindennél fontosabb azonban felismerni, 
hogy a fejlődés üteme lényegtelen,

00:05:15.080 --> 00:05:18.256
mert a fejlődés önmagában 
elegendő ahhoz, hogy célba érjünk.

00:05:18.280 --> 00:05:22.056
Nincs szükségünk Moore-törvényre, 
sem exponenciális növekedésre.

00:05:22.080 --> 00:05:23.680
Csak haladnunk kell tovább.

00:05:25.480 --> 00:05:28.400
A második feltevés, hogy haladunk tovább.

00:05:29.000 --> 00:05:31.760
Egyre intelligensebb gépeket hozunk létre.

00:05:33.000 --> 00:05:37.376
És ami az intelligencia értékét illeti --

00:05:37.400 --> 00:05:40.936
az intelligencia, vagy a forrása
a számunkra értékesnek,

00:05:40.960 --> 00:05:43.736
vagy megóvja számunkra mindazt,
amit értékesnek tartunk.

00:05:43.760 --> 00:05:46.016
Az egyik legértékesebb erőforrásunk.

00:05:46.040 --> 00:05:47.576
Szóval megakarjuk alkotni.

00:05:47.600 --> 00:05:50.936
Vannak problémák, melyeket 
haladéktalanul meg kell oldanunk.

00:05:50.960 --> 00:05:54.310
Meg akarjuk gyógyítani az Alzheimert,
a rákot és hasonló betegségeket.

00:05:54.960 --> 00:05:58.896
Érteni akarjuk a gazdasági rendszereket.
Javítani akarjuk a klímaismereteinket.

00:05:58.920 --> 00:06:01.176
Amint mód van rá, megtesszük ezeket.

00:06:01.200 --> 00:06:04.486
A vonat már elhagyta az állomást,
és nincs fék, amit behúzhatnánk.

00:06:05.880 --> 00:06:11.336
Végül is nem vagyunk
az intelligencia tetőfokán,

00:06:11.360 --> 00:06:13.160
de még csak a közelében sem járunk.

00:06:13.640 --> 00:06:15.536
És ez itt a lényeges momentum.

00:06:15.560 --> 00:06:17.976
Ez teszi a helyzetünket
különösen bizonytalanná,

00:06:18.000 --> 00:06:22.040
és pont ezért megbízhatatlanok 
a veszélyhez fűződő előérzeteink.

00:06:23.120 --> 00:06:25.840
Képzeljük el a valaha élt 
legokosabb embert.

00:06:26.640 --> 00:06:30.056
Minden bizonnyal Neumann János
szerepel mindenki listáján.

00:06:30.080 --> 00:06:33.416
Ahogy Neumann hatni tudott 
a körülötte lévő emberekre,

00:06:33.440 --> 00:06:37.496
többek között minden idők legjobb 
matematikusaira és fizikusaira,

00:06:37.520 --> 00:06:39.456
annak bárki utánajárhat.

00:06:39.480 --> 00:06:43.256
Ha a róla szóló történeteknek
csak a fele is igaz,

00:06:43.280 --> 00:06:44.496
akkor kétségkívül

00:06:44.520 --> 00:06:46.976
ő a világon valaha élt legokosabb ember.

00:06:47.000 --> 00:06:49.520
Képzeljük el az intelligencia tartományát.

00:06:50.320 --> 00:06:51.749
Itt van Neumann János.

00:06:53.560 --> 00:06:54.894
És itt vagytok ti és én.

00:06:56.120 --> 00:06:57.416
Itt pedig egy csirke.

00:06:57.440 --> 00:06:59.376
(Nevetés)

00:06:59.400 --> 00:07:00.616
Elnézést, egy kiscsirke.

00:07:00.640 --> 00:07:01.896
(Nevetés)

00:07:01.920 --> 00:07:05.656
Nem akarom ezt a beszédet 
még lesújtóbbá tenni.

00:07:05.680 --> 00:07:07.280
(Nevetés)

00:07:08.339 --> 00:07:11.816
Valószínűsíthető, 
hogy az intelligencia tartománya

00:07:11.840 --> 00:07:14.960
sokkalta tágabb, 
mint ahogy azt gondolnánk,

00:07:15.880 --> 00:07:18.910
és ha magunknál intelligensebb 
gépeket építünk,

00:07:18.910 --> 00:07:21.706
akkor azok valószínűleg olyan 
területeket tárnak fel,

00:07:21.706 --> 00:07:23.296
melyeket elképzelni sem tudunk,

00:07:23.320 --> 00:07:25.840
és ott múlnak felül minket, 
ahol nem is sejtjük.

00:07:27.000 --> 00:07:31.336
Fontos belátni, hogy ez pusztán
a sebességet tekintve már igaz.

00:07:31.360 --> 00:07:36.416
Ugye? Mi lenne, ha olyan 
szuperintelligens MI-t hoznánk létre,

00:07:36.440 --> 00:07:39.896
ami semmivel sem okosabb 
egy hétköznapi kutatócsoportnál

00:07:39.920 --> 00:07:42.216
az MIT-n vagy a Stanford Egyetemen.

00:07:42.240 --> 00:07:45.216
Az elektronikus áramkörök 
milliószor gyorsabban működnek,

00:07:45.240 --> 00:07:46.496
mint a biokémiaiak,

00:07:46.520 --> 00:07:49.656
ezért ez a gép milliószor gyorsabb 
gondolkodásra lesz képes,

00:07:49.680 --> 00:07:51.656
mind azok az elmék, akik ezt létrehozták.

00:07:51.656 --> 00:07:53.906
Szóval életre kelted, 
és alig egy hét alatt

00:07:53.906 --> 00:07:57.760
20 000 évnyi emberi 
szellemi munkát végez el.

00:07:58.400 --> 00:08:00.360
És ezt hétről hétre megismétli.

00:08:01.640 --> 00:08:04.736
Képtelenség felfogni, 
nemhogy kordában tartani

00:08:04.760 --> 00:08:07.040
egy elmét, ami ilyen mértékű 
fejlődésre képes.

00:08:08.840 --> 00:08:11.266
A másik, ami aggodalomra ad okot, 
őszintén szólva,

00:08:11.266 --> 00:08:15.976
ez a legjobb, ami megtörténhet velünk.

00:08:16.000 --> 00:08:20.176
Képzeljük el, hogy olyan 
szuperintelligens MI-t alkotunk,

00:08:20.200 --> 00:08:21.966
ami nem rejt biztonsági kockázatokat.

00:08:21.966 --> 00:08:24.856
Rögtön, elsőre létrehozzuk
a legtökéletesebb változatát.

00:08:24.880 --> 00:08:27.096
Mintha kezünkbe kapnánk a mindentudást,

00:08:27.120 --> 00:08:29.136
ami pont úgy működik, ahogy kell.

00:08:29.160 --> 00:08:32.880
Ez a gép lenne a leghatékonyabb 
munkaerő-helyettesítő berendezés.

00:08:33.350 --> 00:08:35.930
Létrehozná a gépet, 
amely létrehozná azt a gépet,

00:08:35.953 --> 00:08:37.896
amely bármilyen fizikai munkát elvégez,

00:08:37.920 --> 00:08:39.376
napenergia hajtja,

00:08:39.400 --> 00:08:42.096
és költségét csak az anyagköltség okozza.

00:08:42.120 --> 00:08:45.376
Az emberi robotolás végéről beszélünk.

00:08:45.400 --> 00:08:48.200
De ez egyben a szellemi 
munka végét is jelenti.

00:08:49.200 --> 00:08:52.416
Vajon ilyen esetben mit tennének
a hozzánk hasonló emberszabásúak?

00:08:52.416 --> 00:08:56.360
Nos, bármikor frizbizhetnénk, 
vagy masszírozhatnánk egymást.

00:08:57.840 --> 00:09:00.696
Jöhet egy kis LSD, 
pár extravagáns ruhadarab,

00:09:00.720 --> 00:09:02.896
és az egész világ 
Burning Manné változhatna.

00:09:02.920 --> 00:09:04.560
(Nevetés)

00:09:06.320 --> 00:09:08.320
Ez tényleg jó mókának hangzik,

00:09:09.280 --> 00:09:11.500
de tegyétek fel magatoknak a kérdést,

00:09:11.500 --> 00:09:14.616
hogy mi történne a jelenlegi 
gazdasági és politika rendszerekkel.

00:09:14.616 --> 00:09:16.856
Azt hiszem, szemtanúi lehetnénk

00:09:16.880 --> 00:09:19.810
egy soha nem látott mértékű

00:09:19.810 --> 00:09:22.536
jóléti aránytalanságnak 
és munkanélküliségnek.

00:09:22.560 --> 00:09:25.176
Hacsak nem állítanánk 
ezt az újfajta jólétet

00:09:25.200 --> 00:09:27.030
az emberiség szolgálatába,

00:09:27.640 --> 00:09:31.256
akkor pár milliárdos villogna 
az üzleti magazinok címlapján,

00:09:31.280 --> 00:09:33.720
miközben a világ többi 
része éhen pusztulna.

00:09:34.320 --> 00:09:36.616
Vajon mit tennének 
az oroszok vagy a kínaiak,

00:09:36.640 --> 00:09:39.376
ha megtudnák, hogy az egyik 
Szilícium-völgyben lévő cég

00:09:39.376 --> 00:09:42.016
bevetni készül a szuperintelligens MI-t?

00:09:42.040 --> 00:09:44.896
Ez a gép a hadviselésre is képes lenne,

00:09:44.920 --> 00:09:47.136
akár földön, akár a kibertérben,

00:09:47.160 --> 00:09:48.840
soha nem tapasztalt erőfölénnyel.

00:09:50.120 --> 00:09:51.976
Ez a győztes-mindent-visz forgatókönyv.

00:09:52.000 --> 00:09:55.136
Ezt a versenyt hat hónappal beelőzni,

00:09:55.160 --> 00:09:57.936
minimum

00:09:57.960 --> 00:09:59.456
500 000 év szükséges.

00:09:59.480 --> 00:10:04.216
Ezért már az ilyen áttörésről 
szóló leghalványabb pletyka is

00:10:04.240 --> 00:10:06.616
fajunk totális megvadulásába torkollhat.

00:10:06.640 --> 00:10:09.536
A legijesztőbb dolog azonban

00:10:09.560 --> 00:10:12.336
- az én meglátásom szerint -
jelen pillanatban az,

00:10:12.360 --> 00:10:16.656
amit az MI-kutatók mondogatnak,

00:10:16.680 --> 00:10:18.460
amikor meg akarnak nyugtatni minket.

00:10:19.000 --> 00:10:22.456
Az aggodalmakra adott 
leggyakoribb válaszuk az idő.

00:10:22.480 --> 00:10:24.536
Annyira messze van még,
hát nem tudjátok.

00:10:24.560 --> 00:10:27.000
50 vagy 100 év is eltelik addig.

00:10:27.720 --> 00:10:28.976
Az egyik kutató szerint:

00:10:29.000 --> 00:10:30.846
"A MI biztonsága miatt aggódni olyan,

00:10:30.846 --> 00:10:33.340
mintha a túlnépesedés
miatt aggódnánk a Marson."

00:10:34.116 --> 00:10:36.156
Ez a szilícium-völgyi változata annak,

00:10:36.156 --> 00:10:38.326
hogy "ne törd ezen a szép kis buksidat".

00:10:38.326 --> 00:10:39.496
(Nevetés)

00:10:39.520 --> 00:10:41.416
Senkinek nem tűnik fel,

00:10:41.440 --> 00:10:44.056
hogy az időhorizontra való hivatkozás,

00:10:44.080 --> 00:10:46.656
totál nem passzol a képbe.

00:10:46.680 --> 00:10:49.936
Ha az intelligencia csupán 
az információ feldolgozásából áll,

00:10:49.960 --> 00:10:52.616
és mi folytatjuk a gépek fejlesztését,

00:10:52.640 --> 00:10:55.520
akkor létre fogunk hozni 
valamiféle szuperintelligenciát.

00:10:56.320 --> 00:10:59.976
És fogalmunk sincs mennyi 
időbe telik majd megteremteni

00:11:00.000 --> 00:11:02.400
a biztonságos kivitelezés feltételeit.

00:11:04.200 --> 00:11:05.496
Hadd ismételjem meg:

00:11:05.520 --> 00:11:09.336
Fogalmunk sincs mennyi 
időbe telik majd megteremteni

00:11:09.360 --> 00:11:11.600
a biztonságos kivitelezés feltételeit.

00:11:12.920 --> 00:11:16.376
Ha nem tűnt volna fel, az elmúlt 
50 év sem olyan, mint az előzőek.

00:11:16.400 --> 00:11:18.856
Ez az 50 év hónapokra bontva.

00:11:18.880 --> 00:11:20.720
Ilyen régóta létezik az iPhone.

00:11:21.440 --> 00:11:24.040
Ilyen régóta megy a tévében a 
"Simpson család".

00:11:24.680 --> 00:11:27.056
50 év nem egy hosszú idő,

00:11:27.080 --> 00:11:30.240
hogy elérkezzünk fajunk egyik
legnagyobb kihívásához.

00:11:31.640 --> 00:11:35.656
Ismétlem: képtelenek vagyunk 
megfelelő érzelmi reakcióval kezelni,

00:11:35.680 --> 00:11:38.376
ami minden kétséget kizáróan előttünk áll.

00:11:38.400 --> 00:11:42.376
Stuart Russell számítógéptudósnak 
van egy szép analógiája.

00:11:42.400 --> 00:11:47.296
Azt mondta: Képzeljük el, hogy kapunk 
egy üzenetet egy idegen civilizációtól,

00:11:47.320 --> 00:11:49.016
amiben ez áll:

00:11:49.040 --> 00:11:50.576
"Földi népség,

00:11:50.600 --> 00:11:52.960
50 év múlva érkezünk.

00:11:53.800 --> 00:11:55.376
Készüljetek fel!"

00:11:55.400 --> 00:11:59.656
Elkezdenénk visszaszámlálni, 
amíg meg nem érkezik az anyahajó?

00:11:59.680 --> 00:12:02.680
Sokkal sürgetőbbnek éreznénk, mint most.

00:12:04.680 --> 00:12:06.536
Az aggodalmakra adott másik válasz,

00:12:06.560 --> 00:12:09.576
hogy ezek a gépek 
osztják a mi nézeteinket,

00:12:09.600 --> 00:12:12.216
mivel ők a mi saját kiterjesztéseink.

00:12:12.240 --> 00:12:14.056
Az agyunkba lesznek ültetve,

00:12:14.080 --> 00:12:16.440
aminek folytán limbikus 
rendszerükké válunk.

00:12:17.120 --> 00:12:19.096
Egyetlen pillanatra gondoljunk csak bele,

00:12:19.096 --> 00:12:21.736
azt javasolják,

00:12:21.760 --> 00:12:24.446
hogy az egyetlen biztonságos 
és megbízható módszer az,

00:12:24.446 --> 00:12:27.240
ha ezt a technológiát közvetlenül 
az agyunkba ültetjük be.

00:12:27.240 --> 00:12:30.666
Nos, lehet, hogy az egyetlen 
biztonságos és megbízható módszer,

00:12:30.666 --> 00:12:33.056
de a technológiák biztonsági kockázatait

00:12:33.080 --> 00:12:36.756
alaposan tesztelni kéne, 
mielőtt bármit bárki fejébe ültetnék.

00:12:36.756 --> 00:12:38.776
(Nevetés)

00:12:38.800 --> 00:12:43.940
A mélyebb probléma az, hogy egy
szuperintelligens MI létrehozása,

00:12:43.940 --> 00:12:45.926
már önmagában is egyszerűbbnek bizonyul,

00:12:45.926 --> 00:12:48.016
mint létrehozni a szuperintelligens MI-t,

00:12:48.016 --> 00:12:50.286
és megalkotni hozzá 
a szükséges idegtudományt,

00:12:50.286 --> 00:12:53.870
amely lehetővé teszi a saját elménkkel 
való gond nélküli összekapcsolását.

00:12:53.870 --> 00:12:56.396
És mivel az ezen dolgozó
vállalatok és kormányok

00:12:56.396 --> 00:12:59.656
vetélytársként tekintenek egymásra,

00:12:59.680 --> 00:13:02.936
mondván, hogy a verseny győzelme 
egyben a világ feletti győzelem is,

00:13:02.960 --> 00:13:05.416
hacsak nem rombolják le 
a következő pillanatban,

00:13:05.440 --> 00:13:08.056
akkor valószínűleg a könnyebb megoldás

00:13:08.080 --> 00:13:09.280
fog előbb megvalósulni.

00:13:10.560 --> 00:13:13.416
Nos, sajnos a problémára 
nem tudom a megoldást.

00:13:13.440 --> 00:13:16.426
Csak javasolni tudom, hogy minél 
többen gondolkodjunk el ezen.

00:13:16.426 --> 00:13:19.136
Úgy vélem, szükségünk lenne
a mesterséges intelligencia

00:13:19.136 --> 00:13:21.356
Manhattan-projektjére.

00:13:21.356 --> 00:13:24.726
Nem a megépítéséhez, mert azt hiszem,
ez elkerülhetetlen,

00:13:24.726 --> 00:13:27.886
hanem hogy megértsük, hogyan kerüljük el 
a fegyverkezési versenyt,

00:13:27.886 --> 00:13:30.836
és hozzuk létre úgy, hogy összhangban
legyen az érdekeinkkel.

00:13:30.846 --> 00:13:32.656
Ha szuperintelligens MI-ról beszélünk,

00:13:32.656 --> 00:13:34.656
amely önmaga megváltoztatására is képes,

00:13:34.656 --> 00:13:39.216
akkor egyetlen esélyünk van a kezdeti 
feltételek helyes meghatározására,

00:13:39.240 --> 00:13:41.296
és még akkor is szükség lesz az ezzel járó

00:13:41.320 --> 00:13:44.460
gazdasági és politikai 
következmények elfogadására.

00:13:45.760 --> 00:13:47.816
És amint elfogadjuk,

00:13:47.840 --> 00:13:51.840
hogy az intelligencia forrása az
információ feldolgozásában rejlik,

00:13:52.720 --> 00:13:57.520
és hogy az intelligencia alapja
egy megfelelő számítógépes rendszer,

00:13:58.360 --> 00:14:02.120
és elfogadjuk, hogy ezeket a rendszereket 
folyamatosan fejleszteni fogjuk,

00:14:03.280 --> 00:14:07.736
és hogy a megismerés horizontja 
messze meghaladja

00:14:07.760 --> 00:14:09.240
jelenlegi tudásunkat,

00:14:10.120 --> 00:14:11.646
akkor be kell majd látnunk,

00:14:11.646 --> 00:14:14.670
hogy valamiféle isten 
megalkotása felé haladunk.

00:14:15.400 --> 00:14:16.976
Most van itt a pillanat,

00:14:17.000 --> 00:14:20.033
hogy olyan istent alkossunk, 
amellyel együtt is tudunk élni.

00:14:20.120 --> 00:14:21.656
Nagyon köszönöm.

00:14:21.680 --> 00:14:26.773
(Taps)

