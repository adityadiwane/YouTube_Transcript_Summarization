WEBVTT
Kind: captions
Language: iw

00:00:00.000 --> 00:00:07.000
מתרגם: Yuri Dulkin
מבקר: Sigal Tifferet

00:00:13.000 --> 00:00:15.216
אני אדבר על כשל באינטואיציה

00:00:15.240 --> 00:00:16.840
שרבים מאיתנו סובלים ממנו.

00:00:17.480 --> 00:00:20.520
זהו למעשה כשל בזיהוי
סכנה מסוג מסויים.

00:00:21.360 --> 00:00:23.096
אני אתאר תרחיש

00:00:23.120 --> 00:00:26.376
שאני חושב שהוא גם מבעית

00:00:26.400 --> 00:00:28.160
וגם סביר שיתרחש,

00:00:28.840 --> 00:00:30.496
וזה לא שילוב טוב,

00:00:30.520 --> 00:00:32.056
כפי שמסתבר.

00:00:32.080 --> 00:00:34.536
ולמרות זאת, במקום לפחד, רובכם תרגישו

00:00:34.560 --> 00:00:36.640
שמה שאני מדבר עליו הוא די מגניב.

00:00:37.200 --> 00:00:40.176
אני אתאר איך ההתקדמויות שלנו

00:00:40.200 --> 00:00:41.976
בתחום הבינה המלאכותית

00:00:42.000 --> 00:00:43.776
עלולות לבסוף להשמיד אותנו.

00:00:43.800 --> 00:00:46.770
ולמעשה, אני חושב שקשה מאוד
לראות איך הם לא ישמידו אותנו,

00:00:46.770 --> 00:00:48.960
או ימריצו אותנו להשמיד את עצמנו.

00:00:49.400 --> 00:00:51.256
ובכל זאת, אם אתם כמוני,

00:00:51.280 --> 00:00:53.936
אתם תגלו שכיף לחשוב על דברים כאלה.

00:00:53.960 --> 00:00:57.336
והתגובה הזו היא חלק מהבעיה.

00:00:57.360 --> 00:00:59.630
התגובה הזו צריכה להדאיג אתכם.

00:00:59.920 --> 00:01:02.576
אם הייתי צריך לשכנע אתכם בשיחה הזו

00:01:02.600 --> 00:01:06.016
שאנחנו צפויים לסבול מרעב עולמי,

00:01:06.040 --> 00:01:09.096
בגלל שינויי אקלים או קטסטרופה אחרת,

00:01:09.120 --> 00:01:15.156
ושסביר מאוד שהנכדים שלכם, 
או הנכדים שלהם, יחיו כך

00:01:15.200 --> 00:01:16.400
לא הייתם חושבים,

00:01:17.440 --> 00:01:18.776
"מעניין.

00:01:18.800 --> 00:01:20.390
"אני אוהב את ההרצאה הזו."

00:01:21.200 --> 00:01:22.720
רעב זה לא כיף.

00:01:23.800 --> 00:01:27.176
מוות באמצעות מדע בדיוני,
לעומת זאת, הוא כיף.

00:01:27.200 --> 00:01:30.540
ואחד הדברים שהכי מדאיגים אותי
בנוגע להתפתחות של בינה מלאכותית כיום,

00:01:30.540 --> 00:01:35.296
הוא שאנחנו לא מסוגלים לגייס
את התגובה הרגשית המתאימה

00:01:35.320 --> 00:01:37.136
לסכנות שצפויות לנו.

00:01:37.160 --> 00:01:41.210
אני לא מסוגל לגייס את התגובה הזו,
ואני מעביר את ההרצאה הזו.

00:01:42.120 --> 00:01:44.816
זה כאילו שאנחנו ניצבים בפני שתי דלתות.

00:01:44.840 --> 00:01:46.096
מאחורי דלת מספר אחת,

00:01:46.120 --> 00:01:49.416
אנחנו מפסיקים להתקדם
בבניית מכונות אינטליגנטיות.

00:01:49.440 --> 00:01:53.456
חומרות ותוכנות המחשב שלנו
מפסיקות להשתפר מסיבה כלשהי.

00:01:53.480 --> 00:01:56.480
עכשיו קחו לעצמכם רגע
לשקול מדוע זה יכול לקרות.

00:01:57.080 --> 00:02:00.736
בהתחשב בערך הרב של
אינטלגינציה ואוטומציה עבורנו,

00:02:00.760 --> 00:02:04.280
אנחנו נמשיך לשפר את הטכנולוגיה,
אם נהיה מסוגלים לכך.

00:02:05.200 --> 00:02:06.867
מה יכול לעצור אותנו מלעשות זאת?

00:02:07.800 --> 00:02:09.600
מלחמה גרעינית כוללת?

00:02:11.000 --> 00:02:12.560
מגיפה כלל-עולמית?

00:02:14.320 --> 00:02:15.640
פגיעת אסטרואיד?

00:02:17.640 --> 00:02:20.216
ג'סטין ביבר ממונה לנשיא ארה"ב?

00:02:20.240 --> 00:02:22.520
(צחוק)

00:02:24.760 --> 00:02:29.300
הנקודה היא, משהו יצטרך להשמיד
את הציוויליזציה המוכרת לנו.

00:02:29.360 --> 00:02:33.656
אתם חייבים לדמיין כמה נורא זה צריך להיות

00:02:33.680 --> 00:02:37.016
בכדי למנוע מאיתנו לבצע
שיפורים בטכנולוגיה שלנו

00:02:37.040 --> 00:02:38.256
לתמיד.

00:02:38.280 --> 00:02:40.296
דור אחרי דור.

00:02:40.320 --> 00:02:42.490
כמעט בהגדרה, זהו הדבר הגרוע ביותר

00:02:42.490 --> 00:02:44.496
שאי-פעם קרה בהיסטוריה האנושית.

00:02:44.520 --> 00:02:45.816
אז האלטרנטיבה היחידה,

00:02:45.840 --> 00:02:48.176
וזה מה שנמצא מאחורי דלת מספר שתיים,

00:02:48.200 --> 00:02:51.336
היא שנמשיך לשפר
את המכונות האינטליגנטיות שלנו

00:02:51.360 --> 00:02:52.960
שנה אחר שנה אחר שנה.

00:02:53.720 --> 00:02:57.360
ובנקודה מסוימת, נבנה מכונות 
שהן חכמות יותר מאיתנו,

00:02:58.080 --> 00:03:00.160
וברגע שיהיו לנו מכונות חכמות מאיתנו,

00:03:00.160 --> 00:03:02.706
הן יתחילו לשפר את עצמן.

00:03:02.720 --> 00:03:05.456
ואז אנחנו מסתכנים במה
שהמתמטיקאי איי. ג'יי. גוד קרא לו

00:03:05.480 --> 00:03:07.256
"התפוצצות אינטליגנציה",

00:03:07.280 --> 00:03:09.280
שהתהליך יכול לצאת משליטתנו.

00:03:10.120 --> 00:03:12.936
לרוב זה מתואר כפי שנראה פה,

00:03:12.960 --> 00:03:16.176
כפחד מצבאות של רובוטים זדוניים

00:03:16.200 --> 00:03:17.456
שיתקפו אותנו.

00:03:17.480 --> 00:03:20.176
אבל זה לא התרחיש הסביר ביותר.

00:03:20.200 --> 00:03:25.056
זה לא שהמכונות שלנו
יהפכו באופן ספונטני למרושעות.

00:03:25.080 --> 00:03:27.696
החשש האמיתי הוא שנבנה מכונות

00:03:27.720 --> 00:03:30.076
שמוכשרות כל כך הרבה יותר מאיתנו,

00:03:30.076 --> 00:03:33.576
שהסטייה הקטנה ביותר 
בין המטרות שלנו לשלהן

00:03:33.600 --> 00:03:34.800
עלולה להשמיד אותנו.

00:03:35.960 --> 00:03:38.040
רק תחשבו על היחס שלנו לנמלים.

00:03:38.600 --> 00:03:40.256
אנחנו לא שונאים אותן.

00:03:40.280 --> 00:03:42.336
אנחנו לא עושים מאמץ מיוחד לפגוע בהן.

00:03:42.360 --> 00:03:44.736
למעשה, לפעמים אנחנו מתאמצים
כדי שלא לפגוע בהן.

00:03:44.760 --> 00:03:46.776
אנחנו צועדים מעליהן במדרכות.

00:03:46.800 --> 00:03:48.936
אבל ברגע שהנוכחות שלהן

00:03:48.960 --> 00:03:51.456
נמצאת בקונפליקט רציני עם אחת המטרות שלנו,

00:03:51.480 --> 00:03:53.957
למשל כשאנחנו בונים בנין כמו זה,

00:03:53.981 --> 00:03:55.941
אנחנו משמידים אותן ללא יסורי מצפון.

00:03:56.480 --> 00:03:59.416
החשש הוא שיום אחד אנחנו נבנה מכונות

00:03:59.440 --> 00:04:02.176
אשר במודע או שלא,

00:04:02.200 --> 00:04:04.200
יוכלו להתייחס אלינו בזלזול דומה.

00:04:05.760 --> 00:04:09.290
עכשיו, אני חושד שכל זה
נשמע מופרך עבור רבים מכם.

00:04:09.360 --> 00:04:15.696
אני בטוח שיש בכם כאלה הספקנים לגבי האפשרות
להגיע לבינה מלאכותית סופר-אינטליגנטית כזו,

00:04:15.720 --> 00:04:17.376
ובוודאי לחשוב שהיא בלתי נמנעת.

00:04:17.400 --> 00:04:21.020
אבל אז אתם צריכים למצוא פגם
באחת מהנחות היסוד הבאות.

00:04:21.044 --> 00:04:22.616
ויש רק שלוש מהן.

00:04:23.800 --> 00:04:28.519
אינטליגנציה היא ענין של עיבוד מידע 
במערכות פיזיות.

00:04:29.320 --> 00:04:31.649
למעשה, זו קצת יותר מסתם הנחה.

00:04:31.649 --> 00:04:35.416
אנחנו כבר בנינו אינטליגנציה צרה
לתוך המכונות שלנו,

00:04:35.440 --> 00:04:37.456
ורבות מהמכונות האלה מתפקדות

00:04:37.480 --> 00:04:40.120
ברמות של אינטליגנציה על-אנושית כבר כיום.

00:04:40.840 --> 00:04:43.416
ואנחנו יודעים שחומר בלבד

00:04:43.440 --> 00:04:46.190
יכול לייצר מה שנקרא "אינטליגנציה כללית",

00:04:46.190 --> 00:04:49.736
היכולת לחשוב בצורה גמישה
מעבר לתחומים רבים,

00:04:49.760 --> 00:04:52.896
כי המוח שלנו מצליח לעשות זאת. נכון?

00:04:52.920 --> 00:04:56.856
הרי יש כאן רק אטומים,

00:04:56.880 --> 00:05:01.376
וכל עוד נמשיך לבנות מערכות של אטומים

00:05:01.400 --> 00:05:04.096
שמפגינות התנהגות יותר ויותר אינטליגנטית,

00:05:04.120 --> 00:05:06.656
בסוף, אלא אם כן יפריעו לנו,

00:05:06.680 --> 00:05:10.056
בסוף, נבנה אינטליגנציה כללית

00:05:10.080 --> 00:05:11.376
בתוך המכונות שלנו.

00:05:11.400 --> 00:05:15.056
זה חיוני להבין שקצב ההתקדמות אינו משנה,

00:05:15.080 --> 00:05:18.256
כיוון שכל התקדמות מספיקה
בכדי להביא אותנו לקו הסיום.

00:05:18.280 --> 00:05:22.056
אנחנו לא צריכים את חוק מור בשביל להמשיך.
אנחנו לא צריכים התקדמות אקספוננציאלית.

00:05:22.080 --> 00:05:23.680
אנחנו רק צריכים להמשיך.

00:05:25.480 --> 00:05:28.400
ההנחה השניה היא שאנחנו נמשיך.

00:05:29.000 --> 00:05:31.760
אנחנו נמשיך לשפר 
את המכונות האינטליגנטיות שלנו.

00:05:33.000 --> 00:05:37.376
ובהתחשב בחשיבות האינטליגנציה –

00:05:37.400 --> 00:05:40.936
אינטליגנציה היא המקור של כל מה שיקר לנו

00:05:40.960 --> 00:05:43.736
או שאנחנו זקוקים לה בשביל
לשמור על כל מה שיקר לנו.

00:05:43.760 --> 00:05:46.016
זהו המשאב הכי יקר שלנו.

00:05:46.040 --> 00:05:47.576
אז אנחנו רוצים לעשות את זה.

00:05:47.600 --> 00:05:50.936
יש לנו בעיות שאנחנו צריכים לפתור נואשות.

00:05:50.960 --> 00:05:54.160
אנחנו רוצים לרפא מחלות כמו אלצהיימר וסרטן.

00:05:54.960 --> 00:05:58.896
אנחנו רוצים להבין מערכות כלכליות.
אנחנו רוצים לשפר את מדעי האקלים.

00:05:58.920 --> 00:06:01.176
אז אנחנו נעשה זאת, אם נוכל.

00:06:01.200 --> 00:06:04.486
הרכבת כבר יצאה מהתחנה,
ואין לה מעצור שנוכל למשוך בו.

00:06:05.880 --> 00:06:11.336
לבסוף, אנחנו לא עומדים
בפסגת האינטליגנציה,

00:06:11.360 --> 00:06:13.160
או אפילו קרוב אליה, ככל הנראה.

00:06:13.640 --> 00:06:15.310
וזו באמת התובנה הקריטית ביותר.

00:06:15.310 --> 00:06:17.976
זה מה שהופך את המצב שלנו לכל כך מסוכן,

00:06:18.000 --> 00:06:22.040
וזה מה שהופך את האינטואיציות שלנו
לגבי הסיכון לכל כך לא אמינות.

00:06:23.120 --> 00:06:25.840
עכשיו, תחשבו על האדם
החכם ביותר שחי אי פעם.

00:06:26.640 --> 00:06:30.056
כמעט כולם כאן ישימו את
ג'ון פון נוימן איפשהו בראש הרשימה.

00:06:30.080 --> 00:06:33.416
הרושם שפון נוימן עשה על האנשים סביבו,

00:06:33.440 --> 00:06:37.496
וזה כולל כמה מהמתמטיקאים
והפיזיקאים הגדולים של זמנו,

00:06:37.520 --> 00:06:39.456
מתועד בצורה טובה למדי.

00:06:39.480 --> 00:06:43.256
אילו חצי מהסיפורים עליו היו חצי נכונים,

00:06:43.280 --> 00:06:44.496
אין ספק בכלל

00:06:44.520 --> 00:06:46.976
שהוא אחד האנשים החכמים שחיו עלי אדמות.

00:06:47.000 --> 00:06:49.520
אז חשבו על רצף האינטליגנציה.

00:06:50.320 --> 00:06:51.749
כאן נמצא ג'ון פון נוימן.

00:06:53.560 --> 00:06:54.894
וכאן נמצאים אתם ואני.

00:06:56.120 --> 00:06:57.416
ופה יש לנו תרנגולת.

00:06:57.440 --> 00:06:59.376
(צחוק)

00:06:59.400 --> 00:07:00.616
מצטער – תרנגולת.

00:07:00.640 --> 00:07:01.896
(צחוק)

00:07:01.920 --> 00:07:04.810
אין סיבה שההרצאה הזו 
תהיה מדכאת יותר מהנדרש.

00:07:04.810 --> 00:07:07.280
(צחוק)

00:07:08.339 --> 00:07:11.816
אבל סביר מאוד שרצף האינטליגנציה

00:07:11.840 --> 00:07:14.960
ממשיך הרבה מעבר למה שאנחנו מכירים כיום,

00:07:15.880 --> 00:07:19.096
ושאם נבנה מכונות
שהן יותר אינטליגנטיות מאיתנו,

00:07:19.120 --> 00:07:21.416
סביר מאוד שהן יחקרו את הרצף הזה

00:07:21.440 --> 00:07:23.296
בדרכים שאיננו יכולים לדמיין,

00:07:23.320 --> 00:07:25.840
ולעלות עלינו בדרכים שאיננו יכולים לדמיין.

00:07:27.000 --> 00:07:31.336
וחשוב להבין שזה נכון ולו רק בזכות המהירות.

00:07:31.360 --> 00:07:36.416
נכון? דמיינו שנבנה בינה מלאכותית 
סופר-אינטליגנטית

00:07:36.440 --> 00:07:39.896
שאינה חכמה יותר מצוות ממוצע של חוקרים

00:07:39.920 --> 00:07:42.216
בסטנפורד או ב-MIT.

00:07:42.240 --> 00:07:46.490
ובכן, מעגלים אלקטרוניים פועלים 
בערך פי מיליון מהר יותר ממעגלים ביוכימיים,

00:07:46.520 --> 00:07:49.656
אז המכונה הזו תוכל לחשוב
בערך פי מיליון מהר יותר

00:07:49.680 --> 00:07:51.496
מהמוחות שבנו אותה.

00:07:51.520 --> 00:07:53.176
אז אתם נותנים לה לרוץ שבוע,

00:07:53.200 --> 00:07:58.330
והיא תבצע 20,000 שנות אדם
של עבודה אינטלקטואלית,

00:07:58.400 --> 00:08:00.360
שבוע אחרי שבוע אחרי שבוע.

00:08:01.640 --> 00:08:05.006
איך נוכל להבין, לא כל שכן להגביל,

00:08:05.006 --> 00:08:07.430
מוח שיכול להתקדם כך?

00:08:08.840 --> 00:08:11.506
בכנות, הדבר הנוסף שמדאיג,

00:08:11.506 --> 00:08:15.976
הוא שאם תדמיינו את התרחיש הטוב ביותר,

00:08:16.000 --> 00:08:20.176
דמיינו שפתאום נתקלנו בתכן
של בינה מלאכותית סופר-אינטליגנטית

00:08:20.200 --> 00:08:21.816
שאין בו שום דאגות בטיחותיות.

00:08:21.816 --> 00:08:24.856
יש לנו את התכן המושלם על הפעם הראשונה.

00:08:24.880 --> 00:08:27.096
כאילו שקיבלנו אוֹרַקְל

00:08:27.120 --> 00:08:29.136
שמתנהג בדיוק כפי שאנו מצפים.

00:08:29.160 --> 00:08:32.880
אז המכונה הזו תהיה 
המכשיר המושלם לחיסכון בעבודה.

00:08:33.680 --> 00:08:36.109
היא תוכל לתכנן את המכונה
שתוכל לבנות את המכונה

00:08:36.133 --> 00:08:37.896
שתעשה כל עבודה פיזית,

00:08:37.920 --> 00:08:39.376
תופעל בכוח השמש,

00:08:39.400 --> 00:08:42.096
פחות או יותר במחיר של חומרי הגלם.

00:08:42.120 --> 00:08:45.376
אז אנחנו מדברים על סופה
של עבודת הפרך האנושית.

00:08:45.400 --> 00:08:48.200
אנחנו מדברים גם על סופה
של רוב העבודה האינטלקטואלית.

00:08:49.200 --> 00:08:52.256
אז מה קופים כמונו יעשו בנסיבות שכאלו?

00:08:52.280 --> 00:08:56.360
טוב, נהיה חופשיים לשחק פריסבי
ולהעניק אחד לשני עיסויים.

00:08:57.840 --> 00:09:00.696
תוסיפו קצת אל.אס.די
ובחירת לבוש מפוקפקת,

00:09:00.720 --> 00:09:03.516
וכל העולם יראה כמו אירוע של ברנינג מאן.

00:09:03.516 --> 00:09:04.560
(צחוק)

00:09:06.320 --> 00:09:08.320
עכשיו, זה אולי נשמע די טוב,

00:09:09.280 --> 00:09:11.656
אבל תשאלו את עצמכם מה יקרה

00:09:11.680 --> 00:09:14.416
תחת הסדר הכלכלי והפוליטי שלנו היום?

00:09:14.440 --> 00:09:16.856
נראה סביר שאנחנו נחזה

00:09:16.880 --> 00:09:21.016
ברמות של אי-שויון כלכלי ואבטלה

00:09:21.040 --> 00:09:22.536
שלא נצפו קודם לכן מעולם.

00:09:22.560 --> 00:09:25.176
ללא רצון להשקיע מיד את כל העושר הזה

00:09:25.200 --> 00:09:26.680
לרווחת המין האנושי כולו,

00:09:27.640 --> 00:09:31.256
מספר טריליונרים יוכלו לקשט
את השערים של כתבי העת העסקיים

00:09:31.280 --> 00:09:33.720
בזמן ששאר העולם יהיה חופשי לרעוב.

00:09:34.320 --> 00:09:36.616
ומה הרוסים והסינים יעשו

00:09:36.640 --> 00:09:39.256
אם הם ישמעו שחברה כלשהי בעמק הסיליקון

00:09:39.280 --> 00:09:42.016
עומדת להשיק בינה מלאכותית 
סופר-אינטליגנטית?

00:09:42.040 --> 00:09:44.896
המכונה הזו תוכל לנהל מלחמה,

00:09:44.920 --> 00:09:47.136
בין אם ארצית או במרחב הסייבר,

00:09:47.160 --> 00:09:48.840
בעוצמה ללא תקדים.

00:09:50.120 --> 00:09:51.976
זה תרחיש שבו המנצח לוקח הכל.

00:09:52.000 --> 00:09:55.136
להקדים את התחרות הזו בשישה חודשים

00:09:55.160 --> 00:09:58.730
משמעו להקדים את העולם ב-500,000 שנה לפחות.

00:09:59.480 --> 00:10:04.216
אז נראה שאפילו שמועות על פריצת דרך שכזו

00:10:04.240 --> 00:10:06.616
יכולות לגרום למין שלנו להשתגע.

00:10:06.640 --> 00:10:09.536
עכשיו, אחד הדברים המפחידים ביותר,

00:10:09.560 --> 00:10:12.336
לראות עיני, ברגע הזה,

00:10:12.360 --> 00:10:16.656
אלה הדברים שחוקרי בינה מלאכותית אומרים

00:10:16.680 --> 00:10:18.240
כשהם רוצים להרגיע אותנו.

00:10:19.000 --> 00:10:22.456
והסיבה הנפוצה ביותר שאומרים
לנו שאין סיבה לדאגה היא זמן.

00:10:22.480 --> 00:10:24.536
זה עוד רחוק מאוד, אתם יודעים.

00:10:24.560 --> 00:10:27.000
זה כנראה רחוק מאיתנו ב-50 או 100 שנים.

00:10:27.720 --> 00:10:28.976
חוקר אחד אמר,

00:10:29.000 --> 00:10:30.576
"לדאוג מהבטיחות של בינה מלאכותית

00:10:30.600 --> 00:10:32.880
"זה כמו לדאוג מצפיפות אוכלוסין על המאדים."

00:10:34.116 --> 00:10:35.736
זו הגרסה של עמק הסיליקון

00:10:35.760 --> 00:10:38.496
ל"אל תטרידו את עצמכם בזה."

00:10:38.496 --> 00:10:39.496
(צחוק)

00:10:39.520 --> 00:10:41.416
נראה שאף אחד לא שם לב

00:10:41.440 --> 00:10:44.056
שההתייחסות לאופק הזמן

00:10:44.080 --> 00:10:46.656
מהווה נון סקוויטר מוחלט.

00:10:46.680 --> 00:10:49.936
אם אינטליגנציה היא רק ענין של עיבוד מידע,

00:10:49.960 --> 00:10:52.616
ואנחנו ממשיכים לשפר את המכונות שלנו,

00:10:52.640 --> 00:10:56.230
אנחנו נייצר סוג כלשהו של סופר-אינטליגנציה.

00:10:56.320 --> 00:10:59.976
ואין לנו מושג כמה זמן ייקח לנו

00:11:00.000 --> 00:11:02.400
לייצר את התנאים כדי
לעשות זאת באופן בטיחותי.

00:11:04.200 --> 00:11:05.496
תרשו לי להגיד את זה שוב.

00:11:05.520 --> 00:11:09.336
אין לנו מושג כמה זמן ייקח לנו

00:11:09.360 --> 00:11:12.090
לייצר את התנאים כדי
לעשות זאת באופן בטיחותי.

00:11:12.920 --> 00:11:16.376
ואם לא שמתם לב,
50 שנה זה לא מה שזה היה פעם.

00:11:16.400 --> 00:11:18.856
אלה 50 שנים מחולקים לחודשים.

00:11:18.880 --> 00:11:20.720
זו כמות הזמן שיש לנו אייפונים.

00:11:21.440 --> 00:11:24.040
זו כמות הזמן ש"הסימפסונים"
משודרים בטלויזיה.

00:11:24.680 --> 00:11:27.056
חמישים שנים זה לא הרבה זמן

00:11:27.080 --> 00:11:30.240
כדי להתמודד עם אחד האתגרים
הגדולים ביותר שהמין שלנו יתקל בהם.

00:11:31.640 --> 00:11:35.656
שוב, נראה שאנחנו לא מצליחים
לייצר את התגובה הרגשית המתאימה

00:11:35.680 --> 00:11:38.376
למשהו שאין לנו ספק שהוא מגיע.

00:11:38.400 --> 00:11:42.376
למדען המחשבים סטיוארט ראסל
יש אנלוגיה נחמדה בנושא זה.

00:11:42.400 --> 00:11:47.296
הוא אמר, דמיינו שקיבלנו מסר
מתרבות חוצנית כלשהי,

00:11:47.320 --> 00:11:49.016
שאומר:

00:11:49.040 --> 00:11:50.576
"תושבי כדור הארץ,

00:11:50.600 --> 00:11:52.960
"אנחנו נגיע לכוכב שלכם בעוד 50 שנים.

00:11:53.800 --> 00:11:55.376
תתכוננו."

00:11:55.400 --> 00:11:59.656
ועכשיו אנחנו רק סופרים את החודשים
עד שספינת האם תנחת?

00:11:59.680 --> 00:12:02.680
אנחנו נרגיש קצת יותר דחיפות
ממה שאנחנו מרגישים היום.

00:12:04.680 --> 00:12:06.536
סיבה נוספת שאומרים לנו לא לדאוג

00:12:06.560 --> 00:12:09.576
היא שהמכונות הללו
בהכרח יחלֵקו את הערכים שלנו

00:12:09.600 --> 00:12:12.216
כי הן יהיו, הלכה למעשה, שלוחות של עצמנו.

00:12:12.240 --> 00:12:14.056
הן יושתלו למוחות שלנו,

00:12:14.080 --> 00:12:16.440
ואנחנו נהפוך למעשה למערכת הלימבית שלהן.

00:12:17.120 --> 00:12:18.536
קחו רגע נוסף בכדי לשקול

00:12:18.560 --> 00:12:23.116
אם ההתקדמות הבטוחה והשקולה היחידה המומלצת,

00:12:23.120 --> 00:12:25.920
היא להשתיל את הטכנולוגיה הזו
ישירות לתוך המוח שלנו.

00:12:26.600 --> 00:12:29.976
עכשיו, זו אולי באמת הדרך
השקולה והבטוחה להתקדם,

00:12:30.000 --> 00:12:34.766
אבל מומלץ לטפל היטב בבטיחות טכנולוגיה

00:12:34.766 --> 00:12:37.226
לפני שתוקעים אותה לתוך הראש שלכם.

00:12:37.226 --> 00:12:38.776
(צחוק)

00:12:38.800 --> 00:12:44.136
הבעיה העמוקה יותר היא שבניית 
בינה מלאכותית סופר-אינטליגנטית לבדה

00:12:44.160 --> 00:12:45.896
נראית קלה יותר

00:12:45.920 --> 00:12:47.776
מבניית בינה מלאכותית סופר-אינטליגנטית

00:12:47.800 --> 00:12:49.976
וגם הבנה מלאה במדעי המוח

00:12:49.976 --> 00:12:52.740
שתאפשר לנו לשלב באופן מושלם 
אותה עם מוחנו.

00:12:52.800 --> 00:12:55.976
ובהתחשב בכך שחברות וממשלות
שעושות את העבודה הזו

00:12:56.000 --> 00:12:59.656
כנראה יראו את עצמן במרוץ כנגד כל האחרות,

00:12:59.680 --> 00:13:02.936
ובהתחשב שנצחון במרוץ הזה 
משמעותו לזכות בעולם,

00:13:02.960 --> 00:13:05.416
בתנאי שלא תהרסו אותו ברגע שאחרי,

00:13:05.440 --> 00:13:08.056
אז נראה סביר שמה שקל יותר לעשות

00:13:08.080 --> 00:13:09.560
ייעשה קודם.

00:13:10.560 --> 00:13:13.416
למרבה הצער, אין לי פתרון לבעיה הזו,

00:13:13.440 --> 00:13:15.710
מלבד להמליץ שיותר מאיתנו יחשבו עליה.

00:13:15.710 --> 00:13:18.456
אני חושב שאנחנו צריכים משהו
כמו פרויקט מנהטן

00:13:18.480 --> 00:13:20.496
לנושא של בינה מלאכותית.

00:13:20.520 --> 00:13:23.256
לא כדי לבנות אותה, כי לדעתי 
זה בלתי נמנע שנעשה זאת,

00:13:23.280 --> 00:13:26.616
אלא כדי להבין איך נמנעים ממרוץ חימוש

00:13:26.640 --> 00:13:30.136
וכדי לבנות אותה באופן שעולה 
בקנה אחד עם האינטרסים שלנו.

00:13:30.160 --> 00:13:32.296
כשמדברים על בינה מלאכותית סופר-אינטליגנטית

00:13:32.320 --> 00:13:34.576
שיכולה לשנות את עצמה,

00:13:34.600 --> 00:13:39.216
נראה שיש לנו רק הזדמנות אחת
ליצור תנאי פתיחה נכונים,

00:13:39.240 --> 00:13:41.296
ואפילו אז נצטרך לספוג

00:13:41.320 --> 00:13:44.360
את ההשלכות הפוליטיות והכלכליות
שקשורות בכך.

00:13:45.760 --> 00:13:47.816
אבל ברגע שנודה בכך

00:13:47.840 --> 00:13:51.840
שעיבוד מידע הוא המקור לאינטליגנציה,

00:13:52.720 --> 00:13:57.520
שמערכת חישובית מתאימה 
היא הבסיס לאינטליגנציה,

00:13:58.360 --> 00:14:02.120
ונקבל שאנחנו נמשיך לשפר
את המערכות האלו ללא הפסקה,

00:14:03.280 --> 00:14:07.736
ונקבל שככל הנראה אופק הקוגניציה רחוק בהרבה

00:14:07.760 --> 00:14:08.960
מזה שאנו מכירים כיום,

00:14:10.120 --> 00:14:11.336
אז עלינו לקבל

00:14:11.360 --> 00:14:14.000
שאנחנו בתהליך של בניית אל כלשהו.

00:14:15.400 --> 00:14:16.976
ועכשיו יהיה זמן טוב

00:14:17.000 --> 00:14:18.953
לוודא שזה אל שנוכל לחיות איתו.

00:14:20.120 --> 00:14:21.656
תודה רבה לכם.

00:14:21.680 --> 00:14:26.773
(מחיאות כפיים)

