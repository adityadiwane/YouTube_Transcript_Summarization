WEBVTT
Kind: captions
Language: pt-PT

00:00:00.000 --> 00:00:07.000
Tradutor: Carla Berjano
Revisora: Margarida Ferreira

00:00:12.820 --> 00:00:15.308
Eu vou falar de uma falha de intuição

00:00:15.360 --> 00:00:17.187
de que muitos de nós sofremos.

00:00:17.300 --> 00:00:20.767
É uma falha em detetar
um certo tipo de perigo.

00:00:21.180 --> 00:00:23.188
Vou descrever um cenário

00:00:23.258 --> 00:00:26.386
que considero tão aterrador

00:00:26.447 --> 00:00:28.480
como provável de acontecer.

00:00:28.723 --> 00:00:31.488
Acontece que isto
não é uma boa combinação.

00:00:31.900 --> 00:00:33.655
No entanto, em vez de assustados,

00:00:33.698 --> 00:00:36.705
a maioria de vocês acha
que aquilo de que estou a falar é fixe.

00:00:37.292 --> 00:00:40.077
Eu vou descrever
como as vantagens obtidas

00:00:40.147 --> 00:00:41.996
com a inteligência artificial

00:00:42.047 --> 00:00:44.480
podem acabar por nos destruir.

00:00:43.938 --> 00:00:46.930
Penso que é muito difícil
ver como não nos vão destruir

00:00:46.972 --> 00:00:49.198
ou inspirar a destruirmo-nos a nós mesmos.

00:00:49.383 --> 00:00:51.330
Mas se vocês forem
minimamente como eu,

00:00:51.372 --> 00:00:54.410
vão descobrir que é divertido
pensar nestas coisas.

00:00:54.015 --> 00:00:57.156
Essa resposta faz parte do problema.

00:00:57.325 --> 00:00:59.518
Essa resposta devia preocupar-vos.

00:00:59.740 --> 00:01:02.332
Se eu vos convencesse nesta palestra

00:01:02.420 --> 00:01:05.772
que vamos, provavelmente,
sofrer uma fome mundial,

00:01:05.860 --> 00:01:09.043
devido à mudança climática
ou devido a alguma outra catástrofe,

00:01:09.094 --> 00:01:12.328
e que os vossos netos,
ou os netos deles,

00:01:12.525 --> 00:01:14.960
têm a probabilidade de viver assim,

00:01:15.020 --> 00:01:16.565
não pensariam:

00:01:17.260 --> 00:01:18.777
"Que interessante.

00:01:19.180 --> 00:01:20.547
"Gosto desta palestra TED."

00:01:21.365 --> 00:01:23.249
A fome não tem graça.

00:01:23.620 --> 00:01:26.529
Mas a morte pela ficção científica
é divertida.

00:01:27.192 --> 00:01:30.723
Uma das coisas que mais me preocupa
no desenvolvimento da IA, neste ponto,

00:01:30.783 --> 00:01:35.116
é que parecemos incapazes de mobilizar
uma resposta emocional apropriada

00:01:35.140 --> 00:01:37.056
para os perigos que nos esperam.

00:01:37.116 --> 00:01:40.598
Sou incapaz de conseguir esta resposta,
e estou a fazer esta palestra.

00:01:41.940 --> 00:01:44.526
É como se estivéssemos diante
de duas portas.

00:01:44.660 --> 00:01:46.161
Atrás da porta número um,

00:01:46.185 --> 00:01:49.426
deixamos de fazer progressos
na construção de máquinas inteligentes.

00:01:49.496 --> 00:01:53.639
O hardware e o software dos computadores
deixam de melhorar, por alguma razão.

00:01:53.700 --> 00:01:56.781
Agora, pensem um momento
na razão de isto poder acontecer.

00:01:56.900 --> 00:02:00.556
Ou seja, sendo a inteligência
e a automação tão valiosas,

00:02:00.643 --> 00:02:04.300
vamos continuar a melhorar
a tecnologia, se formos capazes disso.

00:02:05.020 --> 00:02:07.041
O que é que nos pode impedir de o fazer?

00:02:07.620 --> 00:02:09.683
Uma guerra nuclear a larga escala?

00:02:10.820 --> 00:02:12.680
Uma pandemia mundial?

00:02:14.140 --> 00:02:16.169
O impacto de um asteroide?

00:02:17.460 --> 00:02:20.299
Justin Bieber tornar-se
presidente dos EUA?

00:02:20.378 --> 00:02:23.003
(Risos)

00:02:24.580 --> 00:02:28.736
A questão é, algo teria de destruir
a civilização, tal como a conhecemos

00:02:29.516 --> 00:02:33.366
Temos que imaginar
quão mau teria que ser

00:02:33.618 --> 00:02:36.781
para nos impedir de fazer
melhoramentos na tecnologia

00:02:36.940 --> 00:02:38.276
de forma permanente,

00:02:38.327 --> 00:02:40.116
geração após geração.

00:02:40.294 --> 00:02:42.276
Quase por definição,
isto é a pior coisa

00:02:42.300 --> 00:02:44.461
que já aconteceu na história
da Humanidade.

00:02:44.512 --> 00:02:45.981
Por isso, a única alternativa,

00:02:46.036 --> 00:02:48.159
— e é o que está detrás da porta dois —

00:02:48.210 --> 00:02:51.310
é que continuemos a melhorar
as nossas máquinas inteligentes

00:02:51.352 --> 00:02:53.280
ano após ano após ano.

00:02:53.540 --> 00:02:57.598
A certa altura, iremos construir
máquinas mais inteligentes que nós,

00:02:57.900 --> 00:03:00.516
e assim que tivermos máquinas
mais inteligentes que nós,

00:03:00.540 --> 00:03:02.779
elas vão começar
a melhorar-se a si mesmas.

00:03:02.847 --> 00:03:05.484
Então arriscamo-nos ao que
o matemático I.J. Good chamou

00:03:05.554 --> 00:03:07.285
uma "explosão de inteligência,"

00:03:07.336 --> 00:03:09.800
que o processo poderá
sair das nossas mãos.

00:03:09.940 --> 00:03:13.450
Isto é normalmente caricaturado,
como vemos aqui,

00:03:12.961 --> 00:03:16.996
como o medo de que tropas
de robôs malignos nos vão atacar.

00:03:17.300 --> 00:03:19.686
Mas este não é o cenário
mais provável.

00:03:20.238 --> 00:03:24.603
As nossas máquinas não se tornarão
espontaneamente maléficas.

00:03:25.018 --> 00:03:27.552
O problema é se
construímos máquinas

00:03:27.594 --> 00:03:29.686
que são muito mais
competentes que nós

00:03:29.738 --> 00:03:33.477
que à mínima divergência
entre os seus objetivos e os nossos

00:03:33.529 --> 00:03:35.129
podem destruir-nos.

00:03:35.780 --> 00:03:38.214
Basta pensar na nossa 
ligação às formigas.

00:03:38.420 --> 00:03:40.121
Não as odiamos.

00:03:40.200 --> 00:03:42.374
Não saímos do nosso caminho
para as magoar.

00:03:42.425 --> 00:03:44.746
Até fazemos o necessário
para não as magoar.

00:03:44.807 --> 00:03:46.714
Passamos por cima delas
no passeio.

00:03:46.774 --> 00:03:48.783
Mas quando a sua presença

00:03:48.829 --> 00:03:51.330
representa um conflito
com um dos nossos objetivos,

00:03:51.390 --> 00:03:53.922
digamos, quando construindo
um edifício como este,

00:03:53.973 --> 00:03:56.188
eliminamo-las sem hesitação.

00:03:56.300 --> 00:03:59.372
O problema é que iremos
um dia construir máquinas

00:03:59.423 --> 00:04:02.232
que, conscientes ou não,

00:04:02.283 --> 00:04:04.601
podem tratar-nos com
o mesmo desprezo.

00:04:05.925 --> 00:04:08.794
Suspeito que isto pareça
exagerado para a maioria.

00:04:09.289 --> 00:04:15.516
Aposto que são os mesmos que duvidam 
da possibilidade duma IA superinteligente,

00:04:15.567 --> 00:04:17.232
e, mais ainda, inevitável.

00:04:17.356 --> 00:04:20.758
Então, devem encontrar algo de errado
nas seguintes premissas.

00:04:21.145 --> 00:04:22.717
Só existem três.

00:04:23.620 --> 00:04:27.021
A inteligência é uma questão
de processamento de informação

00:04:27.067 --> 00:04:29.021
em sistemas físicos.

00:04:29.394 --> 00:04:31.636
Na verdade, isto é mais
que uma premissa.

00:04:31.697 --> 00:04:35.317
Já construímos inteligência
limitada nas nossas máquinas,

00:04:35.414 --> 00:04:37.512
e muitas dessas máquinas já funcionam

00:04:37.581 --> 00:04:40.440
a um nível de inteligência super-humana.

00:04:40.660 --> 00:04:43.236
Sabemos que mera matéria

00:04:43.296 --> 00:04:46.094
pode dar origem ao que chamamos
"inteligência geral,"

00:04:46.136 --> 00:04:49.465
a capacidade de pensar flexivelmente
em múltiplos domínios,

00:04:49.661 --> 00:04:52.506
porque o nosso cérebros conseguiu isso.

00:04:52.985 --> 00:04:56.485
Quero dizer, aqui só há átomos.

00:04:56.900 --> 00:05:01.196
Enquanto continuamos a construir
sistemas de átomos

00:05:01.220 --> 00:05:04.006
que demonstram cada vez mais
um comportamento inteligente,

00:05:04.058 --> 00:05:06.748
acabaremos — a não ser
que sejamos interrompidos —

00:05:06.818 --> 00:05:10.003
acabaremos por construir
inteligência geral

00:05:10.054 --> 00:05:11.505
nas nossas máquinas.

00:05:11.586 --> 00:05:15.740
É crucial entender que a velocidade
do progresso não importa,

00:05:14.963 --> 00:05:18.448
pois qualquer progresso
é suficiente para nos levar ao final.

00:05:18.509 --> 00:05:21.327
Não precisamos da Lei de Moore
nem de progresso exponencial

00:05:21.327 --> 00:05:22.572
para continuar.

00:05:22.645 --> 00:05:24.609
Só precisamos de continuar.

00:05:25.527 --> 00:05:28.610
A segunda premissa
é que iremos continuar.

00:05:28.820 --> 00:05:31.998
Continuaremos a melhorar
as nossas máquinas inteligentes.

00:05:32.938 --> 00:05:36.986
E devido ao valor da inteligência

00:05:37.138 --> 00:05:40.756
— quer dizer, inteligência ou é
a fonte de tudo que valorizamos

00:05:40.960 --> 00:05:44.450
ou precisamos dela para
salvaguardar tudo o que valorizamos.

00:05:43.952 --> 00:05:45.926
É o nosso recurso mais valioso.

00:05:46.014 --> 00:05:47.750
Por isso, queremos fazer isto.

00:05:47.810 --> 00:05:50.956
Temos problemas que queremos
desesperadamente resolver.

00:05:51.016 --> 00:05:54.398
Queremos curar doenças
como Alzheimer ou cancro.

00:05:54.989 --> 00:05:58.952
Queremos entender sistemas económicos
e melhorar a nossa ciência climatérica.

00:05:58.994 --> 00:06:01.268
Por isso iremos fazê-lo,
se conseguirmos,

00:06:01.365 --> 00:06:04.651
O comboio já saiu da estação,
e não há travão para acionar.

00:06:05.736 --> 00:06:10.592
Finalmente, não estamos
num pico da inteligência,

00:06:11.180 --> 00:06:13.170
provavelmente, nem perto dela.

00:06:13.460 --> 00:06:15.356
E isto é a visão crucial.

00:06:15.380 --> 00:06:17.977
Isto é o que torna
a nossa situação tão precária.

00:06:18.038 --> 00:06:22.314
Isto é o que torna tão incertas
as nossas intuições de risco.

00:06:23.140 --> 00:06:26.087
Agora, pensem na pessoa
mais inteligente de sempre.

00:06:26.605 --> 00:06:29.876
Na lista de quase toda a gente
está John von Neumann.

00:06:29.981 --> 00:06:33.372
A impressão que von Neumann
deixou nas pessoas ao seu redor,

00:06:33.414 --> 00:06:36.988
incluindo os melhores
matemáticos e físicos do seu tempo,

00:06:37.340 --> 00:06:39.276
está bem documentada.

00:06:39.490 --> 00:06:43.076
Se metade das histórias sobre ele 
são meio verdadeiras,

00:06:43.100 --> 00:06:44.297
não há dúvida

00:06:44.321 --> 00:06:47.096
que ele foi uma das pessoas
mais inteligentes que existiu.

00:06:47.147 --> 00:06:50.740
Considerem o espetro da inteligência.

00:06:50.285 --> 00:06:52.159
Aqui temos John von Neumann.

00:06:53.380 --> 00:06:54.995
E aqui estamos nós.

00:06:55.940 --> 00:06:57.454
E aqui temos uma galinha.

00:06:57.550 --> 00:06:59.050
(Risos)

00:06:59.220 --> 00:07:00.745
Desculpem, uma galinha.

00:07:00.940 --> 00:07:02.243
(Risos)

00:07:02.349 --> 00:07:05.166
Não preciso de tornar esta palestra
ainda mais deprimente.

00:07:05.427 --> 00:07:07.454
(Risos)

00:07:08.159 --> 00:07:11.836
Mas parece muito provável
que o espetro da inteligência

00:07:12.670 --> 00:07:15.225
se alargue muito mais além
do que imaginamos.

00:07:16.137 --> 00:07:18.634
Se construirmos máquinas
mais inteligentes que nós,

00:07:18.940 --> 00:07:21.499
iremos muito provavelmente
explorar este espetro

00:07:21.523 --> 00:07:23.379
de formas inimagináveis,

00:07:23.449 --> 00:07:26.223
e superar-nos em formas,
que não imaginávamos.

00:07:27.020 --> 00:07:31.092
É importante reconhecer que isto é verdade
só por virtude da velocidade.

00:07:31.425 --> 00:07:36.250
Imaginem que construímos
uma IA superinteligente

00:07:36.360 --> 00:07:39.952
que não é mais inteligente que
uma equipa normal de investigadores

00:07:40.021 --> 00:07:42.610
de Stanford ou do MIT.

00:07:41.941 --> 00:07:45.117
Os circuitos eletrónicos funcionam
um milhão de vezes mais depressa

00:07:45.160 --> 00:07:46.725
do que os circuitos bioquímicos,

00:07:46.758 --> 00:07:50.012
por isso essa máquina iria 
pensar um milhão de vezes mais rápido

00:07:50.030 --> 00:07:51.852
do que as mentes que a construíram.

00:07:51.915 --> 00:07:53.768
Pomo-la a funcionar
durante uma semana,

00:07:53.800 --> 00:07:58.690
e ela irá realizar 20 000 anos
de trabalho intelectual humano,

00:07:58.220 --> 00:08:00.343
semana após semana.

00:08:01.705 --> 00:08:04.810
Como podemos entender,
e muito menos restringir,

00:08:04.880 --> 00:08:07.614
uma mente que faz tal progresso?

00:08:08.660 --> 00:08:11.032
A outra coisa preocupante, honestamente...

00:08:11.074 --> 00:08:15.796
Imaginem o melhor cenário possível.

00:08:15.938 --> 00:08:19.996
Imaginem que chegamos ao "design"
de uma IA superinteligente

00:08:20.020 --> 00:08:21.714
sem problemas de segurança.

00:08:21.774 --> 00:08:24.676
Temos o "design" perfeito
na primeira tentativa.

00:08:24.818 --> 00:08:27.079
Seria como se nos tivessem dado um oráculo

00:08:27.121 --> 00:08:29.328
cujo comportamento seria o esperado.

00:08:30.299 --> 00:08:33.454
Esta máquina poderia ser
a máquina perfeita para poupar trabalho.

00:08:33.500 --> 00:08:36.019
Poderia conceber a máquina
para construir a máquina

00:08:36.080 --> 00:08:38.061
que pode fazer qualquer
trabalho físico,

00:08:38.106 --> 00:08:39.614
usando energia solar,

00:08:39.674 --> 00:08:41.979
mais ou menos ao custo
das matérias-primas.

00:08:42.194 --> 00:08:45.196
Estamos a falar do fim
do trabalho duro humano

00:08:45.401 --> 00:08:48.774
Estamos também a falar do fim
de maior parte do trabalho intelectual.

00:08:49.020 --> 00:08:52.157
O que fariam macacos como nós
nestas circunstâncias?

00:08:52.736 --> 00:08:57.034
Teríamos tempo para jogar ao disco,
e dar massagens uns aos outros.

00:08:57.778 --> 00:09:00.652
Adicionem LSD e algumas
escolhas duvidosas de roupa,

00:09:00.694 --> 00:09:03.188
e seria como o festival Burning Man
por todo o mundo.

00:09:03.267 --> 00:09:05.407
(Risos)

00:09:06.449 --> 00:09:08.649
Isto até pode soar bem,

00:09:09.100 --> 00:09:11.621
mas questionem-se o que poderia aconteceu

00:09:11.681 --> 00:09:14.381
na nossa ordem atual
económica e política?

00:09:14.423 --> 00:09:17.010
Parece que poderíamos testemunhar

00:09:17.430 --> 00:09:19.417
um nível de desigualdade de riqueza

00:09:19.478 --> 00:09:22.319
e desemprego como nunca vimos antes.

00:09:22.380 --> 00:09:25.210
Se não existir a vontade
de colocar essa riqueza

00:09:24.920 --> 00:09:27.436
imediatamente ao serviço
de toda a humanidade,

00:09:27.596 --> 00:09:31.603
alguns multimilionários poderiam figurar
nas capas das nossas revistas de negócios

00:09:31.636 --> 00:09:34.380
enquanto o resto do mundo
estaria à fome.

00:09:34.140 --> 00:09:36.436
E o que fariam os russos
ou os chineses

00:09:36.487 --> 00:09:39.330
se soubessem que alguma companhia
em Silicon Valley

00:09:39.390 --> 00:09:42.026
estaria prestes a implementar
uma IA superinteligente?

00:09:42.087 --> 00:09:44.625
Essa máquina seria capaz 
de fazer a guerra,

00:09:44.676 --> 00:09:47.046
terrestre ou cibernética,

00:09:47.134 --> 00:09:49.005
com um poder inédito.

00:09:49.940 --> 00:09:52.005
Isto é um cenário de
"o vencedor leva tudo".

00:09:52.065 --> 00:09:55.065
Estar seis meses adiantado
nesta competição

00:09:55.125 --> 00:09:58.540
é como estar 500 000 anos adiantado,

00:09:57.943 --> 00:09:59.276
no mínimo.

00:09:59.600 --> 00:10:04.036
Parece que os meros rumores
deste tipo de inovação

00:10:04.060 --> 00:10:06.599
poderiam descontrolar a nossa espécie.

00:10:06.969 --> 00:10:09.419
Uma das coisas mais assustadoras,

00:10:09.498 --> 00:10:12.156
na minha perspetiva, neste momento,

00:10:12.252 --> 00:10:16.594
são o tipo de coisas
que os investigadores da IA dizem

00:10:16.690 --> 00:10:18.569
quando querem tranquilizar-nos.

00:10:19.201 --> 00:10:22.412
O motivo mais comum que invocam
para não nos preocuparmos é o tempo.

00:10:22.500 --> 00:10:24.537
"Não sabem que isto ainda vem longe?

00:10:24.598 --> 00:10:27.238
"Só daqui a 50 ou 100 anos".

00:10:27.540 --> 00:10:28.941
Um investigador disse:

00:10:29.001 --> 00:10:30.796
"Preocupar-se com a segurança da IA

00:10:30.935 --> 00:10:33.763
"é como preocupar-se com
o excesso de população em Marte."

00:10:33.936 --> 00:10:35.719
Esta é a versão de Silicon Valley

00:10:35.789 --> 00:10:37.956
de "não preocupem a vossa 
cabeça com isto."

00:10:38.034 --> 00:10:39.316
(Risos)

00:10:39.467 --> 00:10:41.245
Ninguém parece notar

00:10:41.305 --> 00:10:44.740
que mencionar o horizonte do tempo

00:10:44.000 --> 00:10:46.476
é um total "não se segue".

00:10:46.536 --> 00:10:49.928
Se a inteligência é apenas uma questão
de processamento de informação,

00:10:49.989 --> 00:10:52.581
e continuarmos a melhorar
as nossas máquinas,

00:10:52.632 --> 00:10:55.758
iremos produzir
alguma forma de superinteligência.

00:10:56.240 --> 00:10:59.896
Não fazemos ideia
quanto tempo iremos demorar

00:10:59.956 --> 00:11:02.747
a criar as condições
para o fazer em segurança.

00:11:04.020 --> 00:11:05.470
Vou repetir.

00:11:05.540 --> 00:11:09.201
Não sabemos quanto tempo vamos demorar

00:11:09.261 --> 00:11:11.983
para criar as condições
para fazê-lo em segurança.

00:11:12.740 --> 00:11:16.196
Se ainda não repararam,
50 anos não é o que era dantes.

00:11:16.356 --> 00:11:18.776
Isto são 50 anos em meses.

00:11:19.070 --> 00:11:21.740
Isto é o tempo desde que temos o iPhone.

00:11:21.260 --> 00:11:24.223
Isto é o tempo em que "Os Simpsons"
têm estado na TV.

00:11:24.500 --> 00:11:26.976
50 anos não é assim tanto tempo

00:11:27.027 --> 00:11:30.787
para ir de encontro aos maiores desafios
que a nossa espécie irá enfrentar.

00:11:31.460 --> 00:11:35.539
Mais uma vez, parecemos estar a falhar
em ter uma resposta emocional apropriada

00:11:35.590 --> 00:11:38.196
para o que acreditamos
que está para vir.

00:11:38.329 --> 00:11:42.332
O cientista informático Stuart Russell
tem uma boa analogia para isto.

00:11:42.447 --> 00:11:46.925
Disse: "Imaginem que recebemos
uma mensagem de uma civilização alienígena,

00:11:47.140 --> 00:11:48.645
"que diz:

00:11:48.960 --> 00:11:50.623
" 'Habitantes da Terra,

00:11:50.665 --> 00:11:53.261
" vamos chegar ao vosso planeta
dentro de 50 anos.

00:11:53.620 --> 00:11:55.196
" 'Preparem-se'."

00:11:55.447 --> 00:11:59.021
Vamos contar os meses
até a nave-mãe aterrar?

00:11:59.636 --> 00:12:02.763
Sentiríamos mais urgência
do que o habitual.

00:12:04.363 --> 00:12:06.783
Outro motivo que nos dão
para não nos preocuparmos

00:12:06.947 --> 00:12:09.632
é que estas máquinas 
vão partilhar os nossos valores

00:12:09.673 --> 00:12:12.108
pois serão literalmente
extensões de nós próprios.

00:12:12.205 --> 00:12:14.166
Serão implantadas no nosso cérebro,

00:12:14.230 --> 00:12:16.587
e seremos os seus sistemas límbicos.

00:12:16.940 --> 00:12:18.556
Parem um momento para considerar

00:12:18.610 --> 00:12:21.556
que o caminho mais prudente e seguro,

00:12:21.580 --> 00:12:23.206
o caminho recomendado,

00:12:23.249 --> 00:12:26.303
é implantar esta tecnologia
diretamente no nosso cérebro.

00:12:26.420 --> 00:12:29.796
Isso poderá de facto ser
o caminho mais prudente e seguro,

00:12:30.174 --> 00:12:32.966
mas normalmente as questões
de segurança de uma tecnologia

00:12:33.009 --> 00:12:36.701
devem estar bem trabalhadas
antes de serem enfiadas na nossa cabeça.

00:12:36.761 --> 00:12:38.486
(Risos)

00:12:38.774 --> 00:12:43.956
O maior problema é que
construir IA superinteligente, por si só,

00:12:43.980 --> 00:12:45.743
parece ser mais fácil

00:12:45.785 --> 00:12:47.768
do que construir IA superinteligente

00:12:47.820 --> 00:12:49.759
e ter a neurociência completa

00:12:49.820 --> 00:12:52.781
que nos permita integrar facilmente
as nossas mentes com isso.

00:12:53.048 --> 00:12:56.031
Sabendo que as companhias
e os governos que fazem este trabalho

00:12:56.073 --> 00:12:59.476
provavelmente se consideram 
numa corrida contra todos os outros,

00:12:59.554 --> 00:13:02.583
sendo que ganhar esta corrida,
é ganhar o mundo,

00:13:02.780 --> 00:13:05.236
desde que não o destruam
logo a seguir,

00:13:05.350 --> 00:13:08.057
então é evidente que
o que for mais fácil

00:13:08.136 --> 00:13:09.945
será o primeiro a ser feito.

00:13:10.380 --> 00:13:13.072
Infelizmente, não tenho
a solução para este problema,

00:13:13.260 --> 00:13:15.648
apenas posso recomendar
que pensemos mais nisto.

00:13:15.709 --> 00:13:18.276
Acho que precisamos de algo
como o Projeto Manhattan

00:13:18.300 --> 00:13:20.497
no tópico da inteligência artificial.

00:13:20.558 --> 00:13:23.275
Não para a construir —acho
que o faremos inevitavelmente —

00:13:23.345 --> 00:13:26.436
mas para compreender
como evitar uma corrida ao armamento

00:13:26.478 --> 00:13:30.360
e para a construir de forma
a estar alinhada com os nossos interesses.

00:13:29.980 --> 00:13:32.116
Quando falamos na IA superinteligente

00:13:32.140 --> 00:13:34.396
que pode alterar-se a si mesma,

00:13:34.438 --> 00:13:36.496
parece que só temos uma hipótese

00:13:36.578 --> 00:13:39.123
para conseguir
as condições iniciais corretas,

00:13:39.196 --> 00:13:41.552
e mesmo assim precisaremos de absorver

00:13:41.630 --> 00:13:44.752
as consequências económicas e politicas
para conseguir tal.

00:13:45.580 --> 00:13:47.790
Mas a partir do momento em que admitirmos

00:13:48.030 --> 00:13:52.041
que o processamento de informação
é a fonte de inteligência,

00:13:52.540 --> 00:13:57.449
que algum sistema informático apropriado
é a base de inteligência,

00:13:58.180 --> 00:14:02.267
e admitirmos que iremos melhorar
estes sistemas de forma contínua,

00:14:03.100 --> 00:14:05.928
e admitirmos que o horizonte da cognição

00:14:05.961 --> 00:14:09.170
provavelmente ultrapassa
o que conhecemos agora,

00:14:09.940 --> 00:14:11.601
então temos de admitir

00:14:11.634 --> 00:14:14.383
que estamos no processo
de construir uma espécie de deus.

00:14:15.244 --> 00:14:17.610
Agora seria uma boa altura
de certificarmo-nos

00:14:17.674 --> 00:14:19.700
que seja um deus
com quem queremos viver.

00:14:19.940 --> 00:14:21.330
Muito obrigado.

00:14:21.500 --> 00:14:24.611
(Aplausos)

