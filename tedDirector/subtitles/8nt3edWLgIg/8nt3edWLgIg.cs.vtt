WEBVTT
Kind: captions
Language: cs

00:00:00.000 --> 00:00:07.000
Překladatel: Martin Bureš
Korektor: Vladimír Harašta

00:00:13.000 --> 00:00:17.076
Budu mluvit o selhání intuice,
kterým trpí mnozí z nás.

00:00:17.370 --> 00:00:20.700
Vlastně jde o neschopnost 
odhalit určitý typ nebezpečí.

00:00:21.360 --> 00:00:23.096
Popíšu vám situaci,

00:00:23.120 --> 00:00:26.376
kterou považuji za děsivou

00:00:26.400 --> 00:00:28.480
a zároveň pravděpodobnou.

00:00:28.780 --> 00:00:31.696
A jak se ukazuje,
tohle není dobrá kombinace.

00:00:32.084 --> 00:00:34.400
Ovšem i přesto si bude
většina z vás myslet,

00:00:34.420 --> 00:00:36.870
že mluvím spíš o zajímavých
než o děsivých věcech.

00:00:37.120 --> 00:00:40.176
Popíšu vám, jak nás pokroky

00:00:40.200 --> 00:00:41.976
v umělé inteligenci

00:00:42.000 --> 00:00:43.720
mohou nakonec úplně vyhubit.

00:00:43.760 --> 00:00:46.556
Vlastně myslím, že lze jen
stěží nevidět, že nás vyhubí

00:00:46.806 --> 00:00:48.980
nebo že nás k sebezničení navedou.

00:00:49.270 --> 00:00:51.256
A pokud jste na tom podobně jako já,

00:00:51.280 --> 00:00:53.886
bude vám připadat zábavné 
o těchto věcech přemýšlet.

00:00:54.126 --> 00:00:57.076
Naše reakce je součástí toho problému.

00:00:57.360 --> 00:00:59.700
Tahle reakce by nás měla znepokojovat.

00:00:59.990 --> 00:01:02.336
Kdybych vás teď přesvědčoval,

00:01:02.600 --> 00:01:06.016
že lidstvo pravděpodobně
zasáhne globální hladomor,

00:01:06.040 --> 00:01:08.836
buď kvůli klimatickým změnám
nebo kvůli jiné katastrofě,

00:01:09.120 --> 00:01:12.326
a že vaše vnoučata
či jejich vnoučata

00:01:12.660 --> 00:01:14.520
budou nejspíš žít takhle,

00:01:15.200 --> 00:01:16.690
určitě si neřeknete:

00:01:17.440 --> 00:01:20.706
„Zajímavé! To je ale hezká přednáška.“

00:01:21.250 --> 00:01:23.080
Hladomor není legrace.

00:01:23.800 --> 00:01:26.756
Na druhou stranu,
smrt ve sci-fi zábavná je.

00:01:27.200 --> 00:01:30.970
Jednou z věcí, které se teď na vývoji
umělé inteligence nejvíc obávám,

00:01:31.010 --> 00:01:35.146
je to, že zřejmě nejsme schopní
přiměřeně emocionálně reagovat

00:01:35.320 --> 00:01:37.056
na nebezpečí ležící před námi.

00:01:37.290 --> 00:01:40.700
Já sám toho nejsem schopný,
i když o tom právě přednáším.

00:01:42.120 --> 00:01:44.586
Je to jako volit cestu na rozcestí.

00:01:44.840 --> 00:01:49.256
Vstoupit prvními dveřmi znamená
zastavit rozvoj inteligentních strojů.

00:01:49.520 --> 00:01:53.256
Počítačový software a hardware se prostě
z nějakého důvodu přestane rozvíjet.

00:01:53.576 --> 00:01:56.840
Zastavme se chvilku nad tím,
kdy by taková situace mohla nastat.

00:01:57.110 --> 00:02:00.616
S ohledem na to, jakým jsou pro nás
inteligence a automatizace přínosem,

00:02:00.760 --> 00:02:04.400
budeme technologie stále zlepšovat,
pokud toho vůbec budeme schopni.

00:02:05.200 --> 00:02:06.997
Co by nás mohlo zastavit?

00:02:07.800 --> 00:02:09.840
Celosvětová nukleární válka?

00:02:11.070 --> 00:02:12.730
Globální pandemie?

00:02:14.320 --> 00:02:16.010
Dopad asteroidu?

00:02:17.640 --> 00:02:19.906
Justin Bieber prezidentem USA?

00:02:19.926 --> 00:02:22.740
(smích)

00:02:24.760 --> 00:02:28.800
Jde o to, že by naši současnou
civilizaci muselo něco zničit.

00:02:29.360 --> 00:02:33.396
Představte si, 
co hrozného by se muselo stát,

00:02:33.680 --> 00:02:37.016
abychom nebyli schopni dále
zdokonalovat naše technologie,

00:02:37.040 --> 00:02:39.976
neustále, generaci za generací.

00:02:40.320 --> 00:02:43.796
V podstatě je to ta nejhorší věc,
jaká by v dějinách mohla nastat.

00:02:44.520 --> 00:02:48.016
Čili jedinou alternativou, tou,
která nás čeká za druhými dveřmi,

00:02:48.200 --> 00:02:51.026
je postupně každým rokem
pokračovat ve zdokonalování

00:02:51.270 --> 00:02:53.250
inteligentních strojů.

00:02:53.720 --> 00:02:57.600
V určitém okamžiku vytvoříme stroje,
které budou chytřejší než my,

00:02:58.080 --> 00:03:00.350
a až budeme mít stroje,
které budou chytřejší,

00:03:00.390 --> 00:03:02.096
začnou se zdokonalovat samy.

00:03:02.700 --> 00:03:05.630
A potom nám hrozí situace,
kterou matematik I. J. Good nazval

00:03:05.680 --> 00:03:07.050
„inteligenční explozí“,

00:03:07.100 --> 00:03:09.590
tedy, že by se nám to
mohlo vymknout z rukou.

00:03:10.120 --> 00:03:12.936
Jak vidíte třeba tady,
toto téma se často zpodobňuje

00:03:12.960 --> 00:03:17.156
jako obava z armád zlovolných robotů,
kteří na nás zaútočí.

00:03:17.480 --> 00:03:19.766
Ale to není moc pravděpodobný scénář.

00:03:20.090 --> 00:03:24.746
Není to tak, že by stroje
najednou začaly být zlé.

00:03:25.080 --> 00:03:29.760
Problém je v tom, že vytvoříme stroje,
které budou o tolik schopnější než my,

00:03:29.800 --> 00:03:34.756
že i ta nejjemnější odchylka mezi
jejich a našimi cíli by nás mohla zničit.

00:03:35.890 --> 00:03:38.340
Vezměte si, jak se chováme k mravencům.

00:03:38.600 --> 00:03:40.066
Nic proti nim nemáme.

00:03:40.280 --> 00:03:42.266
Nechceme jim cíleně ubližovat.

00:03:42.310 --> 00:03:44.580
Necháme se i poštípat,
abychom jim neublížili.

00:03:44.620 --> 00:03:46.246
Na chodníku je překračujeme.

00:03:46.800 --> 00:03:48.616
Ale kdykoliv jejich přítomnost

00:03:48.810 --> 00:03:51.296
vážně narušuje naše zájmy,

00:03:51.480 --> 00:03:53.687
například při stavbě budov, jako je tahle,

00:03:53.791 --> 00:03:56.091
bez mrknutí oka je zlikvidujeme.

00:03:56.480 --> 00:03:59.330
Panuje obava, že jednou vytvoříme stroje,

00:03:59.370 --> 00:04:02.190
které by s námi mohly, vědomě či nevědomě,

00:04:02.230 --> 00:04:04.430
zacházet podobně lhostejně.

00:04:05.760 --> 00:04:08.690
Mnohým z vás to nejspíš
přijde přitažené za vlasy.

00:04:09.360 --> 00:04:11.770
Vsadím se, že někteří
z vás určitě pochybují,

00:04:11.800 --> 00:04:15.420
že je možné superinteligentní AI vytvořit,

00:04:15.670 --> 00:04:17.370
natož, že to nevyhnutelně nastane.

00:04:17.410 --> 00:04:20.640
Pak ale musíte v následujících
předpokladech najít nějaký omyl.

00:04:21.044 --> 00:04:22.736
Ty předpoklady jsou jen tři.

00:04:24.120 --> 00:04:28.710
Inteligence je výsledkem zpracování
informací ve hmotných systémech.

00:04:29.180 --> 00:04:31.549
Toto je vlastně víc než předpoklad.

00:04:31.579 --> 00:04:35.416
Do našich strojů jsme už
omezenou inteligenci zabudovali

00:04:35.440 --> 00:04:40.376
a mnohé z nich už operují
na úrovni superinteligentních lidí.

00:04:40.840 --> 00:04:43.156
A víme, že pouhá hmota

00:04:43.440 --> 00:04:46.056
může dát vzniknout
takzvané „všeobecné inteligenci“,

00:04:46.080 --> 00:04:49.486
tedy schopnosti flexibilně
se orientovat v různých oblastech.

00:04:49.760 --> 00:04:52.686
Protože náš mozek už to zvládl, že?

00:04:54.670 --> 00:04:56.856
Mozek se skládá jenom z atomů,

00:04:56.880 --> 00:05:01.376
a pokud budeme pokračovat
v budování systémů z atomů,

00:05:01.424 --> 00:05:05.360
které budou vykazovat čím dál
inteligentnější chování, tak nakonec,

00:05:05.590 --> 00:05:08.056
pokud nás něco nezastaví,

00:05:08.116 --> 00:05:11.066
zabudujeme do našich strojů
všeobecnou inteligenci.

00:05:11.180 --> 00:05:14.836
Klíčové je uvědomit si,
že na rychlosti pokroku nezáleží,

00:05:15.080 --> 00:05:18.256
protože k dosažení cílové pásky
stačí jakékoli tempo.

00:05:18.280 --> 00:05:21.950
Nepotřebujeme Mooreův zákon,
není zapotřebí exponenciální vývoj.

00:05:22.200 --> 00:05:23.880
Prostě stačí pokračovat.

00:05:25.480 --> 00:05:28.410
Druhým předpokladem je,
že budeme pokračovat.

00:05:28.850 --> 00:05:31.890
Budeme naše inteligentní stroje
nadále zdokonalovat.

00:05:33.000 --> 00:05:37.116
Protože inteligence je důležitá –

00:05:37.400 --> 00:05:40.740
myslím tím to, že inteligence je buď
zdrojem všeho, čeho si ceníme,

00:05:40.780 --> 00:05:43.986
nebo že ji potřebujeme
k zabezpečení všeho, čeho si ceníme.

00:05:44.016 --> 00:05:45.866
Je to náš nejcennější zdroj.

00:05:46.050 --> 00:05:47.576
Takže v tom chceme pokračovat.

00:05:47.600 --> 00:05:50.766
Máme problémy, které zoufale
potřebujeme vyřešit.

00:05:51.010 --> 00:05:54.440
Chceme například vyléčit
Alzheimerovu chorobu nebo rakovinu.

00:05:54.960 --> 00:05:58.520
Chceme rozumět ekonomickým systémům.
Chceme zlepšit výzkum klimatu.

00:05:58.860 --> 00:06:01.206
Takže budeme pokračovat,
pokud to půjde.

00:06:01.386 --> 00:06:04.486
Je to rozjetý vlak, který nejde zastavit.

00:06:05.880 --> 00:06:10.996
Posledním předpokladem je,
že lidé nestojí na vrcholu inteligence

00:06:11.310 --> 00:06:13.440
a pravděpodobně nejsou ani pod vrcholem.

00:06:13.750 --> 00:06:17.736
To je opravdu zásadní pohled.
Kvůli tomu jsme v tak prekérní situaci.

00:06:18.000 --> 00:06:22.150
Kvůli tomu je naše intuice
při posuzování rizik tak nespolehlivá.

00:06:23.110 --> 00:06:25.940
Vezměte si třeba
nejchytřejší lidi v dějinách.

00:06:26.640 --> 00:06:29.886
Do užšího výběru by téměř každý
zařadil Johna von Neumanna.

00:06:30.080 --> 00:06:33.416
To, jakým dojmem von Neumann
působil na lidi kolem sebe,

00:06:33.440 --> 00:06:37.326
mezi něž patřili nejlepší
matematici a fyzici té doby,

00:06:37.520 --> 00:06:39.306
je celkem dobře zdokumentováno.

00:06:39.480 --> 00:06:43.166
Kdyby jen polovina historek o něm
byla aspoň z poloviny pravdivá,

00:06:43.216 --> 00:06:46.370
tak není pochyb,
že patřil mezi nejchytřejší lidi historie.

00:06:47.000 --> 00:06:49.730
Pojďme se teď zamyslet
nad rozsahem inteligence.

00:06:50.320 --> 00:06:52.139
Tady je John von Neumann.

00:06:53.560 --> 00:06:55.404
Tady jsou lidé jako vy a já.

00:06:56.080 --> 00:06:57.486
A tady máme kuře.

00:06:57.536 --> 00:06:59.376
(smích)

00:06:59.400 --> 00:07:00.616
Pardon, kuře.

00:07:00.640 --> 00:07:01.830
(smích)

00:07:01.850 --> 00:07:05.100
Není důvod, aby tahle přednáška
byla víc depresivní, než je potřeba.

00:07:05.140 --> 00:07:07.080
(smích)

00:07:08.339 --> 00:07:13.556
Ovšem zdá se navýsost pravděpodobné,
že rozsah inteligence je mnohem širší,

00:07:13.586 --> 00:07:15.430
než jak ho v současnosti vnímáme.

00:07:15.880 --> 00:07:18.730
A pokud vytvoříme stroje
inteligentnější než jsme my,

00:07:18.950 --> 00:07:21.956
tak budou velmi pravděpodobně
zkoumat toto spektrum způsobem,

00:07:21.996 --> 00:07:23.446
který si neumíme představit,

00:07:23.496 --> 00:07:26.410
a předčí nás způsobem,
který si neumíme představit.

00:07:27.000 --> 00:07:31.166
A je důležité připustit,
že je tomu tak už díky samotné rychlosti.

00:07:31.420 --> 00:07:36.236
Představte si, že jsme
vytvořili superinteligentní AI,

00:07:36.440 --> 00:07:41.876
která je stejně chytrá jako průměrný
tým vědců na Stanfordu nebo MIT.

00:07:42.240 --> 00:07:46.286
Víme, že elektronické obvody pracují
asi milionkrát rychleji než biochemické,

00:07:46.496 --> 00:07:49.656
takže takový stroj
by přemýšlel milionkrát rychleji

00:07:49.680 --> 00:07:51.336
než lidé, kteří ho postavili.

00:07:51.520 --> 00:07:53.086
Takže ho na týden pustíte

00:07:53.116 --> 00:07:57.910
a on vykoná stejnou intelektuální práci,
jaká by lidem trvala 20 000 let.

00:07:58.400 --> 00:08:00.620
Týden za týdnem, pořád dokola.

00:08:01.640 --> 00:08:04.716
Jak bychom vůbec mohli pochopit,
tím spíš nějak omezovat,

00:08:04.766 --> 00:08:07.290
inteligenci schopnou takového pokroku?

00:08:08.840 --> 00:08:11.750
Je tu i další znepokojivá věc –

00:08:12.080 --> 00:08:15.960
představte si optimální situaci.

00:08:16.000 --> 00:08:19.910
Představte si, že se nám podaří
navrhnout superinteligentní AI,

00:08:20.090 --> 00:08:21.806
která nebude dbát na bezpečnost.

00:08:21.836 --> 00:08:24.460
Hned napoprvé budeme mít dokonalou verzi.

00:08:24.840 --> 00:08:28.956
Jako kdyby se naplnila nějaká věštba,
která by fungovala tak, jak bylo určeno.

00:08:29.160 --> 00:08:33.080
Takový stroj by lidem
dokonale usnadňoval práci.

00:08:33.584 --> 00:08:36.203
Dokázal by navrhnout stroj,
který by uměl stavět stroje

00:08:36.233 --> 00:08:37.886
schopné jakékoli manuální práce,

00:08:37.920 --> 00:08:41.496
napájené sluneční energií,
víceméně za cenu surovin.

00:08:42.670 --> 00:08:45.056
Takže mluvíme o konci lidské dřiny.

00:08:45.380 --> 00:08:48.310
Mluvíme také o konci
většiny intelektuální práce.

00:08:49.200 --> 00:08:52.256
Ale co by takové opice jako my
v téhle situaci dělaly?

00:08:52.696 --> 00:08:56.610
No, měli bychom čas hrát frisbee
a vzájemně se masírovat.

00:08:57.840 --> 00:09:00.836
Přidali bychom nějaké to LSD
a kontroverzní oblečení

00:09:00.900 --> 00:09:03.036
a svět by mohl vypadat jako „Burning Man“.

00:09:03.080 --> 00:09:05.520
(smích)

00:09:06.320 --> 00:09:08.530
Asi to zní krásně.

00:09:09.280 --> 00:09:11.656
Ale zeptejte se sami sebe, co by se stalo,

00:09:11.680 --> 00:09:14.166
kdyby ekonomika a politika
fungovaly tak jako dnes.

00:09:14.440 --> 00:09:16.596
Pravděpodobně bychom zažili

00:09:17.066 --> 00:09:21.016
takovou sociální nerovnost
a nezaměstnanost

00:09:21.040 --> 00:09:22.536
jako nikdy předtím.

00:09:22.560 --> 00:09:26.806
Zavládla by neochota dát okamžitě
toto bohatství do služeb lidstvu.

00:09:27.810 --> 00:09:31.256
Několik bilionářů by zdobilo
obálky obchodních časopisů,

00:09:31.280 --> 00:09:33.730
zatímco zbytek světa
by mohl v klidu hladovět.

00:09:34.320 --> 00:09:36.446
A co by udělali Rusové nebo Číňani,

00:09:36.640 --> 00:09:39.256
kdyby se doslechli, že se v Silicon Valley

00:09:39.280 --> 00:09:42.016
někdo chystá zapojit superinteligentní AI?

00:09:42.040 --> 00:09:44.376
Takový stroj by byl schopný vést válku,

00:09:44.590 --> 00:09:46.786
ať už pozemní nebo kybernetickou,

00:09:47.030 --> 00:09:48.980
s nebývalou silou.

00:09:50.120 --> 00:09:51.976
V této situaci by „vítěz bral vše“.

00:09:52.000 --> 00:09:54.926
Šestiměsíční náskok před konkurencí

00:09:55.160 --> 00:09:59.276
by znamenal náskok minimálně 500&nbsp;000 let,

00:09:59.480 --> 00:10:04.216
Zdá se, že i pouhé fámy
o průlomu v této oblasti

00:10:04.240 --> 00:10:06.366
by mohly náš druh dovést k nepříčetnosti.

00:10:06.520 --> 00:10:09.346
Myslím si, že jednou z nejděsivějších věcí

00:10:09.560 --> 00:10:11.646
je momentálně to,

00:10:12.360 --> 00:10:16.656
co vědci zabývající se
vývojem umělé inteligence říkají,

00:10:16.680 --> 00:10:18.490
když nás chtějí uklidnit.

00:10:19.000 --> 00:10:22.276
Nejčastěji uváděným důvodem,
proč se prý nemáme obávat, je čas.

00:10:22.480 --> 00:10:24.536
Všechno je to ještě daleko, nemyslíte?

00:10:24.560 --> 00:10:27.310
Přijde to až tak za 50 nebo 100 let.

00:10:27.720 --> 00:10:30.740
Jeden vědec řekl:
„Bát se o bezpečnost umělé inteligence

00:10:30.796 --> 00:10:33.100
je stejné, jako bát se přelidnění Marsu.“

00:10:34.076 --> 00:10:36.096
Pro Silicon Valley je to jen jiná verze:

00:10:36.136 --> 00:10:38.090
„Nech to koňovi, ten má větší hlavu.“

00:10:38.130 --> 00:10:39.496
(smích)

00:10:39.520 --> 00:10:41.256
Nikdo si nejspíš nevšímá toho,

00:10:41.450 --> 00:10:45.956
že odvolávat se na časový horizont
je naprosto nelogické.

00:10:46.350 --> 00:10:49.726
Pokud inteligence spočívá
pouze ve zpracování informací

00:10:49.960 --> 00:10:52.616
a pokud budeme dál zlepšovat své stroje,

00:10:52.640 --> 00:10:55.520
tak nějakou formu
superinteligence vytvoříme.

00:10:56.320 --> 00:10:59.976
Ale nemáme ani ponětí,
jak dlouho nám bude trvat,

00:11:00.000 --> 00:11:02.690
abychom ji vytvořili
za bezpečných podmínek.

00:11:04.070 --> 00:11:05.406
Řeknu to ještě jednou.

00:11:05.576 --> 00:11:09.336
Nemáme ani ponětí,
jak dlouho nám bude trvat,

00:11:09.360 --> 00:11:12.050
abychom ji vytvořili
za bezpečných podmínek.

00:11:12.920 --> 00:11:16.606
A možná jste si nevšimli,
že 50&nbsp;let už není, co bývalo.

00:11:16.640 --> 00:11:18.606
Tohle je 50&nbsp;let v měsících.

00:11:18.880 --> 00:11:20.930
Takhle dlouho máme iPhone.

00:11:21.440 --> 00:11:24.040
A takhle dlouho se vysílají Simpsonovi.

00:11:24.680 --> 00:11:27.056
Padesát let není moc času na to,

00:11:27.080 --> 00:11:30.370
aby se náš druh vypořádal s jedním
z nejtěžších úkolů všech dob.

00:11:31.640 --> 00:11:35.656
Ještě jednou – zdá se, že nejsme schopni
adekvátní emocionální reakce na to,

00:11:35.680 --> 00:11:38.166
co podle všeho nepochybně nastane.

00:11:38.400 --> 00:11:42.466
Odborník na počítače, Stuart Russell,
má pro tuto situaci hezké přirovnání.

00:11:42.530 --> 00:11:47.066
Máme si představit, že dostaneme
zprávu od mimozemské civilizace,

00:11:47.320 --> 00:11:48.636
ve které stojí:

00:11:49.040 --> 00:11:50.416
„Pozemšťané,

00:11:50.600 --> 00:11:53.170
za 50&nbsp;let dorazíme na vaši planetu.

00:11:53.800 --> 00:11:55.196
Připravte se.“

00:11:55.400 --> 00:11:59.046
To bychom jen počítali, kolik měsíců
zbývá do přistání vlajkové lodi?

00:11:59.680 --> 00:12:02.860
Cítili bychom o něco
naléhavější potřebu než teď.

00:12:04.570 --> 00:12:06.696
Dalším důvodem, proč se prý nemáme bát,

00:12:06.716 --> 00:12:10.076
je, že těmto strojům nezbude než
se držet našich hodnot,

00:12:10.116 --> 00:12:12.226
protože se stanou doslova naší nadstavbou.

00:12:12.256 --> 00:12:14.036
Budou implantované do našich mozků

00:12:14.080 --> 00:12:16.610
a my se v podstatě staneme
jejich limbickým systémem.

00:12:17.060 --> 00:12:18.466
Chvilku se zamyslete nad tím,

00:12:18.486 --> 00:12:21.330
že tou nejbezpečnější
a nejuváženější cestou,

00:12:21.630 --> 00:12:23.166
která se doporučuje,

00:12:23.196 --> 00:12:26.260
by měla být implantace této
technologie do našich mozků.

00:12:26.600 --> 00:12:30.006
Ona to vskutku může být
nejbezpečnější a nejuváženější cesta,

00:12:30.046 --> 00:12:32.980
ale obvykle se všichni obávají
o bezpečnost technologií,

00:12:33.010 --> 00:12:36.956
protože když máte nosit něco natrvalo,
musí to být dost dobře vyladěné.

00:12:36.986 --> 00:12:38.776
(smích)

00:12:38.800 --> 00:12:43.926
Hlubším problémem je to,
že vytvořit samotnou superinteligentní AI

00:12:44.160 --> 00:12:47.806
je pravděpodobně snazší,
než vytvářet takovou inteligenci

00:12:47.960 --> 00:12:49.666
společně s kompletní neurobiologií,

00:12:49.710 --> 00:12:52.370
která umožní bezproblémovou
integraci do naší mysli.

00:12:52.800 --> 00:12:55.866
A jelikož firmy a vlády budou tuto práci

00:12:56.096 --> 00:12:59.416
nejspíše vnímat jako
závod se všem ostatními,

00:12:59.680 --> 00:13:02.716
protože vyhrát tenhle závod
znamená dobýt svět ‒

00:13:02.960 --> 00:13:05.346
tedy za předpokladu,
že ho hned nato nezničíte ‒

00:13:05.510 --> 00:13:09.716
tak se dá čekat, že jako první se
udělá to, co bude nejjednodušší.

00:13:10.670 --> 00:13:13.186
Já bohužel nevidím
pro tento problém žádné řešení,

00:13:13.360 --> 00:13:15.726
kromě doporučení,
abychom o tom více přemýšleli.

00:13:15.840 --> 00:13:18.336
Myslím, že potřebujeme
něco jako projekt Manhattan

00:13:18.360 --> 00:13:20.236
na téma umělé inteligence.

00:13:20.520 --> 00:13:23.336
Ne kvůli jejímu vytvoření,
protože to nevyhnutelně nastane,

00:13:23.356 --> 00:13:26.446
ale abychom přišli na to,
jak se vyhnout závodům ve zbrojení

00:13:26.640 --> 00:13:29.660
a jak ji vytvořit způsobem,
který vyhoví našim zájmům.

00:13:30.100 --> 00:13:34.316
Když se bavíme o superinteligentní AI,
která se dokáže sama měnit,

00:13:34.600 --> 00:13:39.216
vypadá to, že máme jen jednu šanci,
jak správně nastavit výchozí podmínky,

00:13:39.230 --> 00:13:41.560
i když budeme muset tlumit

00:13:41.590 --> 00:13:44.740
ekonomické a politické důsledky
takto nastavených podmínek.

00:13:45.760 --> 00:13:47.816
Nicméně ve chvíli, kdy si přiznáme,

00:13:47.840 --> 00:13:51.720
že inteligence pramení
ze zpracování informací,

00:13:53.090 --> 00:13:57.520
že vhodně navržený počítačový systém
je podstatou inteligence,

00:13:58.360 --> 00:14:02.260
a až přistoupíme na to, že budeme
tyto systémy neustále vylepšovat

00:14:03.280 --> 00:14:09.116
a že horizont poznání nejspíš daleko
přesahuje vše, co v současnosti známe,

00:14:10.120 --> 00:14:14.336
pak si musíme přiznat, že jsme uprostřed
procesu vytváření jakéhosi boha.

00:14:15.400 --> 00:14:19.066
Teď by bylo dobré se ujistit,
že s tímto bohem budeme moci žít.

00:14:20.090 --> 00:14:21.379
Díky moc.

00:14:21.399 --> 00:14:25.323
(potlesk)

