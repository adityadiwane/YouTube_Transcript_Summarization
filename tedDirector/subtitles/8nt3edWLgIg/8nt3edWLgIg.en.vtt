WEBVTT
Kind: captions
Language: en

00:00:13.000 --> 00:00:15.216
I'm going to talk
about a failure of intuition

00:00:15.240 --> 00:00:16.840
that many of us suffer from.

00:00:17.480 --> 00:00:20.520
It's really a failure
to detect a certain kind of danger.

00:00:21.360 --> 00:00:23.096
I'm going to describe a scenario

00:00:23.120 --> 00:00:26.376
that I think is both terrifying

00:00:26.400 --> 00:00:28.160
and likely to occur,

00:00:28.840 --> 00:00:30.496
and that's not a good combination,

00:00:30.520 --> 00:00:32.056
as it turns out.

00:00:32.080 --> 00:00:34.536
And yet rather than be scared,
most of you will feel

00:00:34.560 --> 00:00:36.640
that what I'm talking about
is kind of cool.

00:00:37.200 --> 00:00:40.176
I'm going to describe
how the gains we make

00:00:40.200 --> 00:00:41.976
in artificial intelligence

00:00:42.000 --> 00:00:43.776
could ultimately destroy us.

00:00:43.800 --> 00:00:47.256
And in fact, I think it's very difficult
to see how they won't destroy us

00:00:47.280 --> 00:00:48.960
or inspire us to destroy ourselves.

00:00:49.400 --> 00:00:51.256
And yet if you're anything like me,

00:00:51.280 --> 00:00:53.936
you'll find that it's fun
to think about these things.

00:00:53.960 --> 00:00:57.336
And that response is part of the problem.

00:00:57.360 --> 00:00:59.080
OK? That response should worry you.

00:00:59.920 --> 00:01:02.576
And if I were to convince you in this talk

00:01:02.600 --> 00:01:06.016
that we were likely
to suffer a global famine,

00:01:06.040 --> 00:01:09.096
either because of climate change
or some other catastrophe,

00:01:09.120 --> 00:01:12.536
and that your grandchildren,
or their grandchildren,

00:01:12.560 --> 00:01:14.360
are very likely to live like this,

00:01:15.200 --> 00:01:16.400
you wouldn't think,

00:01:17.440 --> 00:01:18.776
"Interesting.

00:01:18.800 --> 00:01:20.000
I like this TED Talk."

00:01:21.200 --> 00:01:22.720
Famine isn't fun.

00:01:23.800 --> 00:01:27.176
Death by science fiction,
on the other hand, is fun,

00:01:27.200 --> 00:01:31.176
and one of the things that worries me most
about the development of AI at this point

00:01:31.200 --> 00:01:35.296
is that we seem unable to marshal
an appropriate emotional response

00:01:35.320 --> 00:01:37.136
to the dangers that lie ahead.

00:01:37.160 --> 00:01:40.360
I am unable to marshal this response,
and I'm giving this talk.

00:01:42.120 --> 00:01:44.816
It's as though we stand before two doors.

00:01:44.840 --> 00:01:46.096
Behind door number one,

00:01:46.120 --> 00:01:49.416
we stop making progress
in building intelligent machines.

00:01:49.440 --> 00:01:53.456
Our computer hardware and software
just stops getting better for some reason.

00:01:53.480 --> 00:01:56.480
Now take a moment
to consider why this might happen.

00:01:57.080 --> 00:02:00.736
I mean, given how valuable
intelligence and automation are,

00:02:00.760 --> 00:02:04.280
we will continue to improve our technology
if we are at all able to.

00:02:05.200 --> 00:02:06.867
What could stop us from doing this?

00:02:07.800 --> 00:02:09.600
A full-scale nuclear war?

00:02:11.000 --> 00:02:12.560
A global pandemic?

00:02:14.320 --> 00:02:15.640
An asteroid impact?

00:02:17.640 --> 00:02:20.216
Justin Bieber becoming
president of the United States?

00:02:20.240 --> 00:02:22.520
(Laughter)

00:02:24.760 --> 00:02:28.680
The point is, something would have to
destroy civilization as we know it.

00:02:29.360 --> 00:02:33.656
You have to imagine
how bad it would have to be

00:02:33.680 --> 00:02:37.016
to prevent us from making
improvements in our technology

00:02:37.040 --> 00:02:38.256
permanently,

00:02:38.280 --> 00:02:40.296
generation after generation.

00:02:40.320 --> 00:02:42.456
Almost by definition,
this is the worst thing

00:02:42.480 --> 00:02:44.496
that's ever happened in human history.

00:02:44.520 --> 00:02:45.816
So the only alternative,

00:02:45.840 --> 00:02:48.176
and this is what lies
behind door number two,

00:02:48.200 --> 00:02:51.336
is that we continue
to improve our intelligent machines

00:02:51.360 --> 00:02:52.960
year after year after year.

00:02:53.720 --> 00:02:57.360
At a certain point, we will build
machines that are smarter than we are,

00:02:58.080 --> 00:03:00.696
and once we have machines
that are smarter than we are,

00:03:00.720 --> 00:03:02.696
they will begin to improve themselves.

00:03:02.720 --> 00:03:05.456
And then we risk what
the mathematician IJ Good called

00:03:05.480 --> 00:03:07.256
an "intelligence explosion,"

00:03:07.280 --> 00:03:09.280
that the process could get away from us.

00:03:10.120 --> 00:03:12.936
Now, this is often caricatured,
as I have here,

00:03:12.960 --> 00:03:16.176
as a fear that armies of malicious robots

00:03:16.200 --> 00:03:17.456
will attack us.

00:03:17.480 --> 00:03:20.176
But that isn't the most likely scenario.

00:03:20.200 --> 00:03:25.056
It's not that our machines
will become spontaneously malevolent.

00:03:25.080 --> 00:03:27.696
The concern is really
that we will build machines

00:03:27.720 --> 00:03:29.776
that are so much
more competent than we are

00:03:29.800 --> 00:03:33.576
that the slightest divergence
between their goals and our own

00:03:33.600 --> 00:03:34.800
could destroy us.

00:03:35.960 --> 00:03:38.040
Just think about how we relate to ants.

00:03:38.600 --> 00:03:40.256
We don't hate them.

00:03:40.280 --> 00:03:42.336
We don't go out of our way to harm them.

00:03:42.360 --> 00:03:44.736
In fact, sometimes
we take pains not to harm them.

00:03:44.760 --> 00:03:46.776
We step over them on the sidewalk.

00:03:46.800 --> 00:03:48.936
But whenever their presence

00:03:48.960 --> 00:03:51.456
seriously conflicts with one of our goals,

00:03:51.480 --> 00:03:53.957
let's say when constructing
a building like this one,

00:03:53.981 --> 00:03:55.941
we annihilate them without a qualm.

00:03:56.480 --> 00:03:59.416
The concern is that we will
one day build machines

00:03:59.440 --> 00:04:02.176
that, whether they're conscious or not,

00:04:02.200 --> 00:04:04.200
could treat us with similar disregard.

00:04:05.760 --> 00:04:08.520
Now, I suspect this seems
far-fetched to many of you.

00:04:09.360 --> 00:04:15.696
I bet there are those of you who doubt
that superintelligent AI is possible,

00:04:15.720 --> 00:04:17.376
much less inevitable.

00:04:17.400 --> 00:04:21.020
But then you must find something wrong
with one of the following assumptions.

00:04:21.044 --> 00:04:22.616
And there are only three of them.

00:04:23.800 --> 00:04:28.519
Intelligence is a matter of information
processing in physical systems.

00:04:29.320 --> 00:04:31.935
Actually, this is a little bit more
than an assumption.

00:04:31.959 --> 00:04:35.416
We have already built
narrow intelligence into our machines,

00:04:35.440 --> 00:04:37.456
and many of these machines perform

00:04:37.480 --> 00:04:40.120
at a level of superhuman
intelligence already.

00:04:40.840 --> 00:04:43.416
And we know that mere matter

00:04:43.440 --> 00:04:46.056
can give rise to what is called
"general intelligence,"

00:04:46.080 --> 00:04:49.736
an ability to think flexibly
across multiple domains,

00:04:49.760 --> 00:04:52.896
because our brains have managed it. Right?

00:04:52.920 --> 00:04:56.856
I mean, there's just atoms in here,

00:04:56.880 --> 00:05:01.376
and as long as we continue
to build systems of atoms

00:05:01.400 --> 00:05:04.096
that display more and more
intelligent behavior,

00:05:04.120 --> 00:05:06.656
we will eventually,
unless we are interrupted,

00:05:06.680 --> 00:05:10.056
we will eventually
build general intelligence

00:05:10.080 --> 00:05:11.376
into our machines.

00:05:11.400 --> 00:05:15.056
It's crucial to realize
that the rate of progress doesn't matter,

00:05:15.080 --> 00:05:18.256
because any progress
is enough to get us into the end zone.

00:05:18.280 --> 00:05:22.056
We don't need Moore's law to continue.
We don't need exponential progress.

00:05:22.080 --> 00:05:23.680
We just need to keep going.

00:05:25.480 --> 00:05:28.400
The second assumption
is that we will keep going.

00:05:29.000 --> 00:05:31.760
We will continue to improve
our intelligent machines.

00:05:33.000 --> 00:05:37.376
And given the value of intelligence --

00:05:37.400 --> 00:05:40.936
I mean, intelligence is either
the source of everything we value

00:05:40.960 --> 00:05:43.736
or we need it to safeguard
everything we value.

00:05:43.760 --> 00:05:46.016
It is our most valuable resource.

00:05:46.040 --> 00:05:47.576
So we want to do this.

00:05:47.600 --> 00:05:50.936
We have problems
that we desperately need to solve.

00:05:50.960 --> 00:05:54.160
We want to cure diseases
like Alzheimer's and cancer.

00:05:54.960 --> 00:05:58.896
We want to understand economic systems.
We want to improve our climate science.

00:05:58.920 --> 00:06:01.176
So we will do this, if we can.

00:06:01.200 --> 00:06:04.486
The train is already out of the station,
and there's no brake to pull.

00:06:05.880 --> 00:06:11.336
Finally, we don't stand
on a peak of intelligence,

00:06:11.360 --> 00:06:13.160
or anywhere near it, likely.

00:06:13.640 --> 00:06:15.536
And this really is the crucial insight.

00:06:15.560 --> 00:06:17.976
This is what makes
our situation so precarious,

00:06:18.000 --> 00:06:22.040
and this is what makes our intuitions
about risk so unreliable.

00:06:23.120 --> 00:06:25.840
Now, just consider the smartest person
who has ever lived.

00:06:26.640 --> 00:06:30.056
On almost everyone's shortlist here
is John von Neumann.

00:06:30.080 --> 00:06:33.416
I mean, the impression that von Neumann
made on the people around him,

00:06:33.440 --> 00:06:37.496
and this included the greatest
mathematicians and physicists of his time,

00:06:37.520 --> 00:06:39.456
is fairly well-documented.

00:06:39.480 --> 00:06:43.256
If only half the stories
about him are half true,

00:06:43.280 --> 00:06:44.496
there's no question

00:06:44.520 --> 00:06:46.976
he's one of the smartest people
who has ever lived.

00:06:47.000 --> 00:06:49.520
So consider the spectrum of intelligence.

00:06:50.320 --> 00:06:51.749
Here we have John von Neumann.

00:06:53.560 --> 00:06:54.894
And then we have you and me.

00:06:56.120 --> 00:06:57.416
And then we have a chicken.

00:06:57.440 --> 00:06:59.376
(Laughter)

00:06:59.400 --> 00:07:00.616
Sorry, a chicken.

00:07:00.640 --> 00:07:01.896
(Laughter)

00:07:01.920 --> 00:07:05.656
There's no reason for me to make this talk
more depressing than it needs to be.

00:07:05.680 --> 00:07:07.280
(Laughter)

00:07:08.339 --> 00:07:11.816
It seems overwhelmingly likely, however,
that the spectrum of intelligence

00:07:11.840 --> 00:07:14.960
extends much further
than we currently conceive,

00:07:15.880 --> 00:07:19.096
and if we build machines
that are more intelligent than we are,

00:07:19.120 --> 00:07:21.416
they will very likely
explore this spectrum

00:07:21.440 --> 00:07:23.296
in ways that we can't imagine,

00:07:23.320 --> 00:07:25.840
and exceed us in ways
that we can't imagine.

00:07:27.000 --> 00:07:31.336
And it's important to recognize that
this is true by virtue of speed alone.

00:07:31.360 --> 00:07:36.416
Right? So imagine if we just built
a superintelligent AI

00:07:36.440 --> 00:07:39.896
that was no smarter
than your average team of researchers

00:07:39.920 --> 00:07:42.216
at Stanford or MIT.

00:07:42.240 --> 00:07:45.216
Well, electronic circuits
function about a million times faster

00:07:45.240 --> 00:07:46.496
than biochemical ones,

00:07:46.520 --> 00:07:49.656
so this machine should think
about a million times faster

00:07:49.680 --> 00:07:51.496
than the minds that built it.

00:07:51.520 --> 00:07:53.176
So you set it running for a week,

00:07:53.200 --> 00:07:57.760
and it will perform 20,000 years
of human-level intellectual work,

00:07:58.400 --> 00:08:00.360
week after week after week.

00:08:01.640 --> 00:08:04.736
How could we even understand,
much less constrain,

00:08:04.760 --> 00:08:07.040
a mind making this sort of progress?

00:08:08.840 --> 00:08:10.976
The other thing that's worrying, frankly,

00:08:11.000 --> 00:08:15.976
is that, imagine the best case scenario.

00:08:16.000 --> 00:08:20.176
So imagine we hit upon a design
of superintelligent AI

00:08:20.200 --> 00:08:21.576
that has no safety concerns.

00:08:21.600 --> 00:08:24.856
We have the perfect design
the first time around.

00:08:24.880 --> 00:08:27.096
It's as though we've been handed an oracle

00:08:27.120 --> 00:08:29.136
that behaves exactly as intended.

00:08:29.160 --> 00:08:32.880
Well, this machine would be
the perfect labor-saving device.

00:08:33.680 --> 00:08:36.109
It can design the machine
that can build the machine

00:08:36.133 --> 00:08:37.896
that can do any physical work,

00:08:37.920 --> 00:08:39.376
powered by sunlight,

00:08:39.400 --> 00:08:42.096
more or less for the cost
of raw materials.

00:08:42.120 --> 00:08:45.376
So we're talking about
the end of human drudgery.

00:08:45.400 --> 00:08:48.200
We're also talking about the end
of most intellectual work.

00:08:49.200 --> 00:08:52.256
So what would apes like ourselves
do in this circumstance?

00:08:52.280 --> 00:08:56.360
Well, we'd be free to play Frisbee
and give each other massages.

00:08:57.840 --> 00:09:00.696
Add some LSD and some
questionable wardrobe choices,

00:09:00.720 --> 00:09:02.896
and the whole world
could be like Burning Man.

00:09:02.920 --> 00:09:04.560
(Laughter)

00:09:06.320 --> 00:09:08.320
Now, that might sound pretty good,

00:09:09.280 --> 00:09:11.656
but ask yourself what would happen

00:09:11.680 --> 00:09:14.416
under our current economic
and political order?

00:09:14.440 --> 00:09:16.856
It seems likely that we would witness

00:09:16.880 --> 00:09:21.016
a level of wealth inequality
and unemployment

00:09:21.040 --> 00:09:22.536
that we have never seen before.

00:09:22.560 --> 00:09:25.176
Absent a willingness
to immediately put this new wealth

00:09:25.200 --> 00:09:26.680
to the service of all humanity,

00:09:27.640 --> 00:09:31.256
a few trillionaires could grace
the covers of our business magazines

00:09:31.280 --> 00:09:33.720
while the rest of the world
would be free to starve.

00:09:34.320 --> 00:09:36.616
And what would the Russians
or the Chinese do

00:09:36.640 --> 00:09:39.256
if they heard that some company
in Silicon Valley

00:09:39.280 --> 00:09:42.016
was about to deploy a superintelligent AI?

00:09:42.040 --> 00:09:44.896
This machine would be capable
of waging war,

00:09:44.920 --> 00:09:47.136
whether terrestrial or cyber,

00:09:47.160 --> 00:09:48.840
with unprecedented power.

00:09:50.120 --> 00:09:51.976
This is a winner-take-all scenario.

00:09:52.000 --> 00:09:55.136
To be six months ahead
of the competition here

00:09:55.160 --> 00:09:57.936
is to be 500,000 years ahead,

00:09:57.960 --> 00:09:59.456
at a minimum.

00:09:59.480 --> 00:10:04.216
So it seems that even mere rumors
of this kind of breakthrough

00:10:04.240 --> 00:10:06.616
could cause our species to go berserk.

00:10:06.640 --> 00:10:09.536
Now, one of the most frightening things,

00:10:09.560 --> 00:10:12.336
in my view, at this moment,

00:10:12.360 --> 00:10:16.656
are the kinds of things
that AI researchers say

00:10:16.680 --> 00:10:18.240
when they want to be reassuring.

00:10:19.000 --> 00:10:22.456
And the most common reason
we're told not to worry is time.

00:10:22.480 --> 00:10:24.536
This is all a long way off,
don't you know.

00:10:24.560 --> 00:10:27.000
This is probably 50 or 100 years away.

00:10:27.720 --> 00:10:28.976
One researcher has said,

00:10:29.000 --> 00:10:30.576
"Worrying about AI safety

00:10:30.600 --> 00:10:32.880
is like worrying
about overpopulation on Mars."

00:10:34.116 --> 00:10:35.736
This is the Silicon Valley version

00:10:35.760 --> 00:10:38.136
of "don't worry your
pretty little head about it."

00:10:38.160 --> 00:10:39.496
(Laughter)

00:10:39.520 --> 00:10:41.416
No one seems to notice

00:10:41.440 --> 00:10:44.056
that referencing the time horizon

00:10:44.080 --> 00:10:46.656
is a total non sequitur.

00:10:46.680 --> 00:10:49.936
If intelligence is just a matter
of information processing,

00:10:49.960 --> 00:10:52.616
and we continue to improve our machines,

00:10:52.640 --> 00:10:55.520
we will produce
some form of superintelligence.

00:10:56.320 --> 00:10:59.976
And we have no idea
how long it will take us

00:11:00.000 --> 00:11:02.400
to create the conditions
to do that safely.

00:11:04.200 --> 00:11:05.496
Let me say that again.

00:11:05.520 --> 00:11:09.336
We have no idea how long it will take us

00:11:09.360 --> 00:11:11.600
to create the conditions
to do that safely.

00:11:12.920 --> 00:11:16.376
And if you haven't noticed,
50 years is not what it used to be.

00:11:16.400 --> 00:11:18.856
This is 50 years in months.

00:11:18.880 --> 00:11:20.720
This is how long we've had the iPhone.

00:11:21.440 --> 00:11:24.040
This is how long "The Simpsons"
has been on television.

00:11:24.680 --> 00:11:27.056
Fifty years is not that much time

00:11:27.080 --> 00:11:30.240
to meet one of the greatest challenges
our species will ever face.

00:11:31.640 --> 00:11:35.656
Once again, we seem to be failing
to have an appropriate emotional response

00:11:35.680 --> 00:11:38.376
to what we have every reason
to believe is coming.

00:11:38.400 --> 00:11:42.376
The computer scientist Stuart Russell
has a nice analogy here.

00:11:42.400 --> 00:11:47.296
He said, imagine that we received
a message from an alien civilization,

00:11:47.320 --> 00:11:49.016
which read:

00:11:49.040 --> 00:11:50.576
"People of Earth,

00:11:50.600 --> 00:11:52.960
we will arrive on your planet in 50 years.

00:11:53.800 --> 00:11:55.376
Get ready."

00:11:55.400 --> 00:11:59.656
And now we're just counting down
the months until the mothership lands?

00:11:59.680 --> 00:12:02.680
We would feel a little
more urgency than we do.

00:12:04.680 --> 00:12:06.536
Another reason we're told not to worry

00:12:06.560 --> 00:12:09.576
is that these machines
can't help but share our values

00:12:09.600 --> 00:12:12.216
because they will be literally
extensions of ourselves.

00:12:12.240 --> 00:12:14.056
They'll be grafted onto our brains,

00:12:14.080 --> 00:12:16.440
and we'll essentially
become their limbic systems.

00:12:17.120 --> 00:12:18.536
Now take a moment to consider

00:12:18.560 --> 00:12:21.736
that the safest
and only prudent path forward,

00:12:21.760 --> 00:12:23.096
recommended,

00:12:23.120 --> 00:12:25.920
is to implant this technology
directly into our brains.

00:12:26.600 --> 00:12:29.976
Now, this may in fact be the safest
and only prudent path forward,

00:12:30.000 --> 00:12:33.056
but usually one's safety concerns
about a technology

00:12:33.080 --> 00:12:36.736
have to be pretty much worked out
before you stick it inside your head.

00:12:36.760 --> 00:12:38.776
(Laughter)

00:12:38.800 --> 00:12:44.136
The deeper problem is that
building superintelligent AI on its own

00:12:44.160 --> 00:12:45.896
seems likely to be easier

00:12:45.920 --> 00:12:47.776
than building superintelligent AI

00:12:47.800 --> 00:12:49.576
and having the completed neuroscience

00:12:49.600 --> 00:12:52.280
that allows us to seamlessly
integrate our minds with it.

00:12:52.800 --> 00:12:55.976
And given that the companies
and governments doing this work

00:12:56.000 --> 00:12:59.656
are likely to perceive themselves
as being in a race against all others,

00:12:59.680 --> 00:13:02.936
given that to win this race
is to win the world,

00:13:02.960 --> 00:13:05.416
provided you don't destroy it
in the next moment,

00:13:05.440 --> 00:13:08.056
then it seems likely
that whatever is easier to do

00:13:08.080 --> 00:13:09.280
will get done first.

00:13:10.560 --> 00:13:13.416
Now, unfortunately,
I don't have a solution to this problem,

00:13:13.440 --> 00:13:16.056
apart from recommending
that more of us think about it.

00:13:16.080 --> 00:13:18.456
I think we need something
like a Manhattan Project

00:13:18.480 --> 00:13:20.496
on the topic of artificial intelligence.

00:13:20.520 --> 00:13:23.256
Not to build it, because I think
we'll inevitably do that,

00:13:23.280 --> 00:13:26.616
but to understand
how to avoid an arms race

00:13:26.640 --> 00:13:30.136
and to build it in a way
that is aligned with our interests.

00:13:30.160 --> 00:13:32.296
When you're talking
about superintelligent AI

00:13:32.320 --> 00:13:34.576
that can make changes to itself,

00:13:34.600 --> 00:13:39.216
it seems that we only have one chance
to get the initial conditions right,

00:13:39.240 --> 00:13:41.296
and even then we will need to absorb

00:13:41.320 --> 00:13:44.360
the economic and political
consequences of getting them right.

00:13:45.760 --> 00:13:47.816
But the moment we admit

00:13:47.840 --> 00:13:51.840
that information processing
is the source of intelligence,

00:13:52.720 --> 00:13:57.520
that some appropriate computational system
is what the basis of intelligence is,

00:13:58.360 --> 00:14:02.120
and we admit that we will improve
these systems continuously,

00:14:03.280 --> 00:14:07.736
and we admit that the horizon
of cognition very likely far exceeds

00:14:07.760 --> 00:14:08.960
what we currently know,

00:14:10.120 --> 00:14:11.336
then we have to admit

00:14:11.360 --> 00:14:14.000
that we are in the process
of building some sort of god.

00:14:15.400 --> 00:14:16.976
Now would be a good time

00:14:17.000 --> 00:14:18.953
to make sure it's a god we can live with.

00:14:20.120 --> 00:14:21.656
Thank you very much.

00:14:21.680 --> 00:14:26.773
(Applause)

