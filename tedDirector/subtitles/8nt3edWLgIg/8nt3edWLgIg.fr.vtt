WEBVTT
Kind: captions
Language: fr

00:00:00.000 --> 00:00:07.000
Traducteur: MACQUERON CORENTIN
Relecteur: Morgane Quilfen

00:00:12.956 --> 00:00:15.172
Je vais vous parler
d'une erreur de jugement

00:00:15.240 --> 00:00:16.840
que beaucoup d'entre nous font.

00:00:17.480 --> 00:00:20.520
Il s'agit de l'incapacité à détecter 
un certain type de danger.

00:00:21.360 --> 00:00:23.096
Je vais décrire un scénario

00:00:23.120 --> 00:00:26.376
que je pense à la fois terrifiant

00:00:26.400 --> 00:00:28.160
et pourtant susceptible d'arriver,

00:00:28.840 --> 00:00:30.496
et ce genre de combinaison

00:00:30.520 --> 00:00:32.056
n'est pas une bonne chose.

00:00:32.080 --> 00:00:35.156
Pourtant, au lieu d'avoir peur,
la plupart d'entre vous penseront

00:00:35.156 --> 00:00:36.640
que c'est plutôt cool.

00:00:37.200 --> 00:00:40.176
Je vais décrire comment 
les avancées que nous faisons

00:00:40.200 --> 00:00:41.976
dans l'intelligence artificielle

00:00:42.000 --> 00:00:43.806
pourraient finir par nous détruire.

00:00:43.806 --> 00:00:47.256
Il semble difficile de voir comment
cela pourrait ne pas nous détruire

00:00:47.280 --> 00:00:48.960
ou nous aider à nous détruire.

00:00:49.400 --> 00:00:51.256
Pourtant, si vous me ressemblez un peu,

00:00:51.280 --> 00:00:53.936
vous allez trouver amusant
de réfléchir à ces choses-là.

00:00:53.960 --> 00:00:57.336
Et cette façon de voir les choses 
est une partie du problème.

00:00:57.360 --> 00:00:59.080
Ce sentiment devrait vous inquiéter.

00:00:59.920 --> 00:01:02.626
Et si je devais vous convaincre,
avec cette présentation,

00:01:02.626 --> 00:01:06.016
que nous allons probablement 
souffrir d'une famine généralisée,

00:01:06.040 --> 00:01:09.096
soit à cause du climat
ou de toute autre catastrophe,

00:01:09.120 --> 00:01:12.536
et que vos petits-enfants,
ou leurs petits-enfants,

00:01:12.560 --> 00:01:14.470
ont de grandes chances de vivre ainsi,

00:01:15.200 --> 00:01:16.400
vous ne penseriez pas :

00:01:17.440 --> 00:01:18.776
« Intéressant.

00:01:18.800 --> 00:01:20.000
Cool, ce TED. »

00:01:21.200 --> 00:01:22.830
La famine, ce n'est pas amusant.

00:01:23.800 --> 00:01:27.176
La mort en science-fiction,
en revanche, est un concept amusant

00:01:27.200 --> 00:01:31.176
et ce qui m'inquiète le plus à propos
du développement de l'IA en ce moment

00:01:31.200 --> 00:01:35.296
est le fait que nous semblons 
incapables de nous faire une idée

00:01:35.320 --> 00:01:37.136
des dangers qui nous attendent.

00:01:37.160 --> 00:01:40.360
Je n'ai pas de réponse,
alors que je suis ici, à vous en parler.

00:01:42.120 --> 00:01:44.816
C'est comme si nous étions
face à deux portes.

00:01:44.840 --> 00:01:46.096
Derrière la porte n°1,

00:01:46.120 --> 00:01:49.416
nous arrêtons nos progrès
dans le développement de nos machines.

00:01:49.440 --> 00:01:53.456
Hardware et software stagnent, 
pour une raison ou pour une autre.

00:01:53.480 --> 00:01:56.480
Essayez d'imaginer 
ce qui pourrait arrêter ce développement.

00:01:56.567 --> 00:01:58.785
Étant donnée l'importance 
de l'intelligence

00:01:58.816 --> 00:02:00.228
et de l'automatisation

00:02:00.297 --> 00:02:03.923
nous allons continuer à améliorer 
notre technologie si nous le pouvons.

00:02:05.200 --> 00:02:06.867
Qu'est-ce qui nous en empêcherait ?

00:02:07.800 --> 00:02:09.600
Une guerre nucléaire mondiale ?

00:02:11.000 --> 00:02:12.560
Une pandémie globale ?

00:02:14.320 --> 00:02:15.640
Un impact d'astéroïde ?

00:02:17.640 --> 00:02:20.216
Justin Bieber président des États-Unis ?

00:02:20.240 --> 00:02:22.520
(Rires)

00:02:24.760 --> 00:02:27.932
Il faudrait que la civilisation 
telle que nous la connaissons

00:02:27.932 --> 00:02:29.390
soit détruite.

00:02:29.390 --> 00:02:33.656
Il faudrait vraiment imaginer 
quelque chose de terrible

00:02:33.680 --> 00:02:37.016
pour que nous arrêtions 
de développer notre technologie

00:02:37.040 --> 00:02:38.256
totalement,

00:02:38.280 --> 00:02:40.296
génération après génération.

00:02:40.320 --> 00:02:42.456
Par définition, 
ce serait la pire chose

00:02:42.480 --> 00:02:44.496
qui serait jamais arrivée à l'humanité.

00:02:44.520 --> 00:02:45.816
Donc la seule alternative,

00:02:45.840 --> 00:02:48.176
et c'est ce qui se trouve 
derrière la porte n°2,

00:02:48.200 --> 00:02:51.336
est que nous continuions
d'améliorer l'intelligence de nos machines

00:02:51.360 --> 00:02:52.960
année après année.

00:02:53.720 --> 00:02:57.360
À un moment, nous allons construire
des machines plus intelligentes que nous,

00:02:58.080 --> 00:03:00.696
et ces machines 
plus intelligentes que nous

00:03:00.720 --> 00:03:02.696
vont commencer 
à s'améliorer elles-mêmes.

00:03:02.720 --> 00:03:05.456
Il y a le risque, théorisé
par le mathématicien I.J. Good,

00:03:05.480 --> 00:03:07.256
d'une « explosion d'intelligence »,

00:03:07.280 --> 00:03:09.280
un processus qui pourrait nous échapper.

00:03:10.120 --> 00:03:12.936
Ce phénomène
est souvent caricaturé, comme ici,

00:03:12.960 --> 00:03:16.176
avec des armées de robots malveillants

00:03:16.200 --> 00:03:17.456
qui nous attaqueraient.

00:03:17.480 --> 00:03:20.176
Mais ce n'est pas 
le scénario le plus probable.

00:03:20.200 --> 00:03:25.056
Nos machines ne vont probablement pas
devenir spontanément malveillantes.

00:03:25.080 --> 00:03:27.696
Le risque est 
que nous construisions des machines

00:03:27.720 --> 00:03:29.776
tellement plus compétentes que nous

00:03:29.800 --> 00:03:33.576
que la moindre divergence 
d'intérêt entre elles et nous

00:03:33.600 --> 00:03:34.800
pourrait nous détruire.

00:03:35.960 --> 00:03:38.040
Pensez aux fourmis.

00:03:38.600 --> 00:03:40.256
Nous ne les haïssons pas.

00:03:40.280 --> 00:03:42.336
Nous ne cherchons pas à les écraser.

00:03:42.360 --> 00:03:44.736
Nous faisons même parfois 
l'effort de les éviter.

00:03:44.760 --> 00:03:46.776
Nous les enjambons.

00:03:46.800 --> 00:03:48.936
Mais dès que leur présence

00:03:48.960 --> 00:03:51.456
nous gêne vraiment,

00:03:51.480 --> 00:03:53.957
par exemple si nous voulons 
construire un bâtiment,

00:03:53.981 --> 00:03:55.941
nous les exterminons sans scrupule.

00:03:56.480 --> 00:03:59.416
Le risque est que nous construisions 
un jour des machines

00:03:59.440 --> 00:04:02.176
qui, conscientes ou non,

00:04:02.200 --> 00:04:04.320
nous traiteraient
avec la même indifférence.

00:04:05.760 --> 00:04:08.520
Cela peut vous sembler capillotracté.

00:04:09.360 --> 00:04:15.696
Je parie que certains d'entre vous 
doutent qu'une super IA soit possible,

00:04:15.720 --> 00:04:17.376
et encore moins inévitable.

00:04:17.400 --> 00:04:21.020
Mais dans ce cas vous devez réfuter 
une des hypothèses suivantes.

00:04:21.044 --> 00:04:22.616
Et il n'y en a que trois.

00:04:23.800 --> 00:04:28.519
L'intelligence est le traitement 
de l'information dans un système physique.

00:04:29.320 --> 00:04:31.935
En fait, c'est un peu plus 
qu'une simple hypothèse.

00:04:31.959 --> 00:04:35.416
Nous avons déjà construit 
des machines vaguement intelligentes,

00:04:35.440 --> 00:04:37.456
et nombre d'entre elles font déjà preuve

00:04:37.480 --> 00:04:40.120
d'une intelligence surhumaine.

00:04:40.840 --> 00:04:43.416
Et nous savons que la matière

00:04:43.440 --> 00:04:46.056
peut produire « l'intelligence générale »,

00:04:46.080 --> 00:04:49.736
la capacité de réfléchir 
de manière croisée,

00:04:49.760 --> 00:04:52.896
parce que nos cerveaux
y sont parvenus, n'est-ce pas ?

00:04:52.920 --> 00:04:56.856
Après tout, il n'y a que
des atomes là-dedans,

00:04:56.880 --> 00:05:01.376
et tant que nous continuerons 
à construire des systèmes d'atomes

00:05:01.400 --> 00:05:04.096
qui feront montre de plus 
en plus d'intelligence,

00:05:04.120 --> 00:05:06.656
nous parviendrons,
à moins d'être interrompus,

00:05:06.680 --> 00:05:10.056
nous parviendrons à implanter
une « intelligence générale »

00:05:10.080 --> 00:05:11.376
au cœur de nos machines.

00:05:11.400 --> 00:05:15.056
Il est crucial de comprendre 
que la vitesse n'est pas le problème,

00:05:15.080 --> 00:05:18.256
car n'importe quelle vitesse 
est suffisante pour aller au bout.

00:05:18.280 --> 00:05:20.405
Nous n'avons pas besoin 
de la loi de Moore

00:05:20.448 --> 00:05:22.080
ni d'un progrès exponentiel.

00:05:22.080 --> 00:05:23.680
Il suffit de continuer à avancer.

00:05:25.480 --> 00:05:28.400
La deuxième hypothèse 
est que nous allons continuer.

00:05:29.000 --> 00:05:31.760
Nous continuerons d'améliorer
nos machines intelligentes.

00:05:33.000 --> 00:05:37.376
Et, vue la valeur de l'intelligence --

00:05:37.400 --> 00:05:40.936
l'intelligence est la source 
de tout ce qui compte

00:05:40.960 --> 00:05:43.736
ou alors elle doit protéger 
tout ce qui compte.

00:05:43.760 --> 00:05:46.016
C'est notre plus importante ressource.

00:05:46.040 --> 00:05:47.576
Donc nous allons continuer.

00:05:47.600 --> 00:05:50.936
Nous avons des problèmes 
que nous devons absolument résoudre.

00:05:50.960 --> 00:05:54.160
Nous voulons vaincre les maladies 
comme Alzheimer ou le cancer.

00:05:54.960 --> 00:05:56.840
Nous voulons comprendre l'économie.

00:05:56.840 --> 00:05:58.920
Nous voulons améliorer le climat.

00:05:58.920 --> 00:06:01.176
Donc nous allons continuer, 
si nous le pouvons.

00:06:01.200 --> 00:06:04.486
Le train a déjà quitté la gare
et il n'y a pas de frein.

00:06:05.880 --> 00:06:11.336
Et enfin, l'humain ne se trouve pas
au sommet de l'intelligence,

00:06:11.360 --> 00:06:13.160
nous en sommes même très loin.

00:06:13.640 --> 00:06:15.536
Et c'est vraiment le point crucial.

00:06:15.560 --> 00:06:17.976
C'est ce qui rend
notre situation si précaire

00:06:18.000 --> 00:06:22.040
et ce qui fait que notre compréhension 
des risques n'est pas fiable.

00:06:23.120 --> 00:06:25.840
Prenons l'humain le plus intelligent
qui ait jamais vécu.

00:06:26.640 --> 00:06:30.056
Beaucoup de gens penseraient 
à John von Neumann.

00:06:30.080 --> 00:06:33.416
L'impression que von Neumann 
laissait aux gens,

00:06:33.440 --> 00:06:37.496
y compris les plus brillants 
mathématiciens et physiciens de son temps,

00:06:37.520 --> 00:06:39.456
est plutôt bien documentée.

00:06:39.480 --> 00:06:42.198
Si ne serait-ce que la moitié 
de ce que l'on dit sur lui

00:06:42.223 --> 00:06:43.135
est à moitié vraie,

00:06:43.135 --> 00:06:44.669
il n'y a aucun doute qu'il soit

00:06:44.669 --> 00:06:46.976
l'un des plus brillants esprits
qui aient existé.

00:06:47.000 --> 00:06:49.520
Considérons le spectre de l'intelligence.

00:06:50.320 --> 00:06:51.749
Voici John von Neumann.

00:06:53.560 --> 00:06:54.894
Et nous, nous sommes ici.

00:06:56.120 --> 00:06:57.416
Et là nous avons un poulet.

00:06:57.440 --> 00:06:59.376
(Rires)

00:06:59.400 --> 00:07:00.616
Pardon, un poulet.

00:07:00.640 --> 00:07:01.896
(Rires)

00:07:01.920 --> 00:07:05.656
Pas besoin de rendre cette présentation 
plus déprimante qu'elle ne l'est déjà.

00:07:05.680 --> 00:07:07.280
(Rires)

00:07:08.339 --> 00:07:11.816
Il semble néanmoins plus que probable
que le spectre de l'intelligence

00:07:11.840 --> 00:07:14.960
s'étende beaucoup plus loin 
que nous ne pouvons le concevoir,

00:07:15.880 --> 00:07:19.096
Si nous construisons
des machines plus intelligentes que nous,

00:07:19.120 --> 00:07:21.466
elles vont très probablement 
explorer ce spectre,

00:07:21.466 --> 00:07:23.296
plus que nous ne pouvons l'imaginer,

00:07:23.320 --> 00:07:26.420
et elles nous surpasseront,
plus que nous ne pouvons l'imaginer.

00:07:27.000 --> 00:07:31.336
Il est important de noter que c'est vrai 
par la seule vertu de la vitesse.

00:07:31.360 --> 00:07:36.416
Imaginez que nous construisions
une IA super intelligente

00:07:36.440 --> 00:07:39.896
qui ne soit pas plus intelligente 
qu'une équipe moyenne de chercheurs

00:07:39.920 --> 00:07:42.216
de Stanford ou du MIT.

00:07:42.240 --> 00:07:45.216
L'électronique va environ 
un million de fois plus vite

00:07:45.240 --> 00:07:46.496
que la biochimie,

00:07:46.520 --> 00:07:49.656
de sorte que cette machine 
penserait un million de fois plus vite

00:07:49.680 --> 00:07:51.496
que les humains qui l'auraient créée.

00:07:51.520 --> 00:07:53.176
En une semaine,

00:07:53.200 --> 00:07:57.760
cette machine réaliserait le même travail 
qu'une équipe d'humains en 20 000 ans,

00:07:58.400 --> 00:08:00.360
semaine après semaine.

00:08:01.640 --> 00:08:04.736
Comment pourrions-nous comprendre, 
et encore moins limiter,

00:08:04.760 --> 00:08:07.040
un esprit qui irait à une telle vitesse ?

00:08:08.840 --> 00:08:10.976
L'autre chose inquiétante, franchement,

00:08:11.000 --> 00:08:15.976
est la suivante :
imaginez le meilleur des cas.

00:08:16.000 --> 00:08:20.176
Imaginez que nous concevions
une IA super intelligente

00:08:20.200 --> 00:08:21.576
qui ne soit pas dangereuse.

00:08:21.600 --> 00:08:24.856
Nous trouvons la parfaite solution 
du premier coup.

00:08:24.880 --> 00:08:27.096
Comme si nous avions 
un oracle à disposition

00:08:27.120 --> 00:08:29.136
se comportant
exactement comme prévu.

00:08:29.160 --> 00:08:32.880
Cette machine nous éviterait 
toute forme de travail.

00:08:33.680 --> 00:08:36.109
Elle concevrait la machine
qui ferait la machine

00:08:36.133 --> 00:08:37.896
qui pourrait faire n'importe quoi,

00:08:37.920 --> 00:08:39.376
alimentée par le soleil,

00:08:39.400 --> 00:08:42.096
plus ou moins pour le seul coût
des matières premières.

00:08:42.120 --> 00:08:45.376
On parle là de la fin
de la pénibilité humaine.

00:08:45.400 --> 00:08:48.200
Nous parlons aussi 
de la fin du travail intellectuel.

00:08:49.200 --> 00:08:52.256
Que feraient des singes comme nous 
dans une pareille situation ?

00:08:52.280 --> 00:08:56.360
Nous serions libres de jouer au frisbee
et de nous faire des massages.

00:08:57.840 --> 00:09:00.696
Avec un peu de LSD 
et quelques fringues douteuses,

00:09:00.720 --> 00:09:02.896
le monde entier serait 
comme le Burning Man.

00:09:02.920 --> 00:09:04.560
(Rires)

00:09:06.320 --> 00:09:08.320
Bon, tout ça peut sembler plutôt sympa,

00:09:09.280 --> 00:09:11.656
mais demandez-vous ce qui arriverait

00:09:11.680 --> 00:09:14.416
avec notre système politique 
et économique actuel ?

00:09:14.440 --> 00:09:16.856
Nous serions les témoins

00:09:16.880 --> 00:09:21.016
d'une inégalité des richesses
et d'un taux de chômage

00:09:21.040 --> 00:09:22.536
encore jamais vus.

00:09:22.560 --> 00:09:25.176
Sans la volonté de mettre 
ces nouvelles richesses

00:09:25.200 --> 00:09:26.680
au service de toute l'humanité,

00:09:27.640 --> 00:09:31.256
quelques milliardaires feraient 
les couvertures des revues commerciales

00:09:31.280 --> 00:09:33.720
alors que le reste du monde 
mourrait de faim.

00:09:34.320 --> 00:09:36.616
Et que feraient les Russes ou les Chinois

00:09:36.640 --> 00:09:39.256
s'ils apprenaient 
qu'une société de la Silicon Valley

00:09:39.280 --> 00:09:42.016
était sur le point de créer
une IA super intelligente ?

00:09:42.040 --> 00:09:44.896
Cette machine serait capable 
de faire la guerre,

00:09:44.920 --> 00:09:47.136
qu'elle soit physique ou numérique,

00:09:47.160 --> 00:09:48.840
avec une puissance jamais vue.

00:09:50.120 --> 00:09:51.976
Le vainqueur emporterait toute la mise.

00:09:52.000 --> 00:09:55.136
Avoir six mois d'avance 
dans une telle compétition

00:09:55.160 --> 00:09:57.936
revient à avoir 500 000 ans d'avance,

00:09:57.960 --> 00:09:59.456
au minimum.

00:09:59.480 --> 00:10:04.216
Il est donc possible 
que la moindre rumeur de ce type de percée

00:10:04.240 --> 00:10:06.616
pourrait tous nous rendre
totalement dingues.

00:10:06.640 --> 00:10:09.536
L'une des choses les plus effrayantes,

00:10:09.560 --> 00:10:12.336
de mon point de vue,

00:10:12.360 --> 00:10:16.656
sont les choses 
que les chercheurs en IA disent

00:10:16.680 --> 00:10:18.240
quand ils veulent nous rassurer.

00:10:19.000 --> 00:10:22.456
On nous dit généralement 
que nous avons encore le temps.

00:10:22.480 --> 00:10:24.536
C'est pour plus tard, 
vous pensez bien.

00:10:24.560 --> 00:10:27.000
C'est probablement 
pour dans 50 ou 100 ans.

00:10:27.720 --> 00:10:28.976
Un chercheur a dit :

00:10:29.000 --> 00:10:31.146
« S'inquiéter de l'IA,
c'est comme s'inquiéter

00:10:31.146 --> 00:10:32.880
de la surpopulation sur Mars. »

00:10:34.116 --> 00:10:35.736
La Silicon Valley sait parfois

00:10:35.760 --> 00:10:38.136
se montrer condescendante.

00:10:38.160 --> 00:10:39.496
(Rires)

00:10:39.520 --> 00:10:41.416
Personne ne semble se rendre compte

00:10:41.440 --> 00:10:44.056
que dire qu'on a le temps

00:10:44.080 --> 00:10:46.656
est totalement fallacieux 
dans ce contexte.

00:10:46.680 --> 00:10:49.936
Si l'intelligence n'est 
que le traitement de l'information

00:10:49.960 --> 00:10:52.616
et si l'on continue 
d'améliorer nos machines,

00:10:52.640 --> 00:10:55.520
alors nous produirons 
une forme de super intelligence.

00:10:56.320 --> 00:10:59.976
Et nous n'avons pas la moindre idée
du temps qu'il nous faudra

00:11:00.000 --> 00:11:02.400
pour trouver comment
le faire sans risques.

00:11:04.200 --> 00:11:05.496
Je vais me répéter.

00:11:05.520 --> 00:11:09.336
Nous n'avons pas la moindre idée 
du temps qu'il nous faudra

00:11:09.360 --> 00:11:11.600
pour trouver comment
le faire sans risques.

00:11:12.920 --> 00:11:16.376
Au cas où vous n'auriez pas remarqué,
50 ans, ce n'est pas grand chose.

00:11:16.400 --> 00:11:18.856
Voici 50 ans, en mois.

00:11:18.880 --> 00:11:20.720
Voici l'existence de l'iPhone.

00:11:21.440 --> 00:11:24.040
Voici la durée couverte 
par « Les Simpsons ».

00:11:24.680 --> 00:11:27.056
Cinquante ans, 
cela ne laisse guère de temps

00:11:27.080 --> 00:11:30.240
pour se préparer à l'un 
des plus grands défis de tous les temps.

00:11:31.640 --> 00:11:35.656
Une fois encore, nous semblons 
incapables de nous préparer

00:11:35.680 --> 00:11:38.376
à ce qui, selon toute probabilité,
va arriver.

00:11:38.400 --> 00:11:42.376
L'informaticien Stuart Russel 
propose une belle analogie à ce sujet.

00:11:42.400 --> 00:11:47.296
Il dit : « Imaginez que nous recevions 
un message des aliens

00:11:47.320 --> 00:11:49.016
qui dirait :

00:11:49.040 --> 00:11:50.576
' Habitants de la Terre,

00:11:50.600 --> 00:11:52.960
nous arriverons chez vous dans 50 ans.

00:11:53.800 --> 00:11:55.376
Soyez prêts. '

00:11:55.400 --> 00:11:59.656
Et on ne ferait que regarder 
le compte à rebours ?

00:11:59.680 --> 00:12:02.680
Non, nous nous sentirions 
un peu plus concernés que ça. »

00:12:04.680 --> 00:12:06.536
Une autre raison de ne pas s'inquiéter

00:12:06.560 --> 00:12:09.576
serait que ces machines 
partageraient notre sens des valeurs

00:12:09.600 --> 00:12:12.216
parce qu'elles seraient 
des extensions de nous-mêmes.

00:12:12.240 --> 00:12:14.236
Elles seraient greffées sur nos cerveaux,

00:12:14.236 --> 00:12:16.440
nous serions leur système limbique.

00:12:17.120 --> 00:12:18.536
Réfléchissez un peu

00:12:18.560 --> 00:12:21.736
que le moyen le plus sûr,

00:12:21.760 --> 00:12:23.096
recommandé,

00:12:23.120 --> 00:12:26.290
serait de brancher cette technologie
directement sur nos cerveaux.

00:12:26.600 --> 00:12:29.976
Cela peut sembler 
être la meilleure option,

00:12:30.000 --> 00:12:33.056
mais généralement, on essaie
d'être sûr de son coup

00:12:33.080 --> 00:12:36.736
avant de s'enfoncer 
quelque chose dans le cerveau.

00:12:36.760 --> 00:12:38.776
(Rires)

00:12:38.800 --> 00:12:42.770
Le vrai problème est
que simplement concevoir

00:12:42.770 --> 00:12:44.160
une IA super intelligente

00:12:44.160 --> 00:12:45.890
semble plus facile

00:12:45.890 --> 00:12:47.856
que de concevoir
une IA super intelligente

00:12:47.856 --> 00:12:49.576
tout en maîtrisant les neurosciences

00:12:49.600 --> 00:12:52.280
pour connecter cette IA à nos cerveaux.

00:12:52.800 --> 00:12:55.976
Si l'on tient compte du fait 
que les entreprises et gouvernements

00:12:56.000 --> 00:12:59.656
se sentent probablement en concurrence,

00:12:59.680 --> 00:13:02.936
et, puisque remporter cette course 
revient à conquérir le monde,

00:13:02.960 --> 00:13:05.416
pourvu que vous
ne le détruisiez pas juste après,

00:13:05.440 --> 00:13:08.056
il semble probable que l'on commence

00:13:08.080 --> 00:13:09.280
par le plus facile.

00:13:10.560 --> 00:13:13.340
Malheureusement,
je n'ai pas la solution de ce problème,

00:13:13.340 --> 00:13:16.296
à part recommander que
nous soyons plus nombreux à y réfléchir.

00:13:16.296 --> 00:13:18.486
Il nous faudrait une sorte 
de Projet Manhattan

00:13:18.486 --> 00:13:20.496
à propos de l'intelligence artificielle.

00:13:20.520 --> 00:13:23.256
Pas pour la concevoir, 
car nous y parviendrons,

00:13:23.280 --> 00:13:26.616
mais pour réfléchir à comment 
éviter une course aux armements

00:13:26.640 --> 00:13:30.136
et pour la concevoir d'une façon 
en accord avec nos intérêts.

00:13:30.160 --> 00:13:32.296
Quand vous parlez
d'une IA super intelligente

00:13:32.320 --> 00:13:34.576
qui pourrait se modifier elle-même,

00:13:34.600 --> 00:13:39.216
il semblerait que nous n'ayons qu'une
seule chance de faire les choses bien

00:13:39.240 --> 00:13:41.296
et encore, il nous faudra gérer

00:13:41.320 --> 00:13:44.360
les conséquences 
politiques et économiques.

00:13:45.760 --> 00:13:47.816
Mais à partir du moment où nous admettons

00:13:47.840 --> 00:13:51.840
que le traitement de l'information 
est la source de l'intelligence,

00:13:52.720 --> 00:13:57.520
qu'un système de calcul
est la base de l'intelligence,

00:13:58.360 --> 00:14:02.120
et que nous admettons que nous allons 
continuer d'améliorer ce type de système

00:14:03.280 --> 00:14:05.460
et que nous admettons que l'horizon

00:14:05.460 --> 00:14:07.385
de l'intelligence dépasse totalement

00:14:07.447 --> 00:14:08.960
ce que nous savons aujourd'hui,

00:14:10.120 --> 00:14:11.420
nous devons admettre

00:14:11.530 --> 00:14:14.620
que nous sommes engagés 
dans la conception d'un pseudo-dieu.

00:14:15.400 --> 00:14:16.969
Il nous faut donc vérifier

00:14:17.000 --> 00:14:19.278
que c'est un dieu compatible 
avec notre survie.

00:14:20.120 --> 00:14:21.656
Merci beaucoup.

00:14:21.680 --> 00:14:26.773
(Applaudissements)

