WEBVTT
Kind: captions
Language: es

00:00:00.000 --> 00:00:07.000
Traductor: Máximo Hdez
Revisor: Denise RQ

00:00:13.000 --> 00:00:15.209
Voy a hablar de un fallo intuitivo

00:00:15.209 --> 00:00:16.793
que muchos sufrimos.

00:00:17.480 --> 00:00:20.640
En realidad es la capacidad
de detectar cierto tipo de peligro.

00:00:21.470 --> 00:00:28.236
Describiré una situación que creo
que es tan aterradora como posible

00:00:28.960 --> 00:00:31.216
y veremos que eso no es
una buena combinación.

00:00:31.890 --> 00:00:33.973
Y sin embargo, en lugar de sentir miedo,

00:00:33.974 --> 00:00:37.209
la mayoría pensará que es
algo bastante interesante.

00:00:37.220 --> 00:00:41.696
Describiré cómo los avances
en el campo de la inteligencia artificial,

00:00:41.952 --> 00:00:44.037
en última instancia, podrían destruirnos.

00:00:44.037 --> 00:00:46.015
Y, de hecho, me resulta muy difícil ver

00:00:46.016 --> 00:00:49.416
que no nos destruirían
o nos ayudaran a destruirnos.

00:00:49.424 --> 00:00:51.279
Y sin embargo, si son como yo,

00:00:51.280 --> 00:00:53.935
encontrarán que es divertido
pensar en estas cosas.

00:00:53.936 --> 00:00:57.311
Y esta manera de ver las cosas
es parte del problema.

00:00:57.312 --> 00:00:59.032
Esa reacción debería preocuparles.

00:00:59.918 --> 00:01:02.581
Si tuviera que convencerles
con esta charla

00:01:02.582 --> 00:01:05.991
de que estamos al borde de sufrir
una hambruna a nivel mundial,

00:01:05.992 --> 00:01:09.047
debido al calentamiento global
o cualquier otra catástrofe,

00:01:09.048 --> 00:01:12.463
y que sus nietos o sus bisnietos,

00:01:12.464 --> 00:01:14.264
vivirán muy probablemente así,

00:01:15.200 --> 00:01:16.400
no pensarían:

00:01:17.440 --> 00:01:18.759
"Interesante.

00:01:18.760 --> 00:01:20.250
Me gusta esta charla TED".

00:01:21.200 --> 00:01:22.720
El hambre no es divertida

00:01:23.800 --> 00:01:27.172
La muerte en la ciencia ficción,
por el contrario, es algo divertido,

00:01:27.173 --> 00:01:31.126
y lo más preocupante
en el desarrollo de la IA hoy en día

00:01:31.167 --> 00:01:35.247
es que parecemos incapaces de ofrecer
una respuesta emocional adecuada

00:01:35.248 --> 00:01:37.067
frente a los peligros que se avecinan.

00:01:37.187 --> 00:01:40.387
Yo mismo soy incapaz de ello
y estoy dando esta charla.

00:01:42.125 --> 00:01:44.614
Es como si nos encontráramos
ante dos puertas.

00:01:44.615 --> 00:01:46.176
Detrás de la puerta número uno

00:01:46.177 --> 00:01:49.416
detenemos el progreso
de máquinas inteligentes.

00:01:49.417 --> 00:01:53.417
Nuestro hardware y software
se estancan simplemente, por alguna razón.

00:01:53.432 --> 00:01:56.762
Traten de reflexionar por un momento
por qué podría suceder esto.

00:01:57.083 --> 00:02:00.735
Dada la valía de la inteligencia
y la automatización,

00:02:00.736 --> 00:02:04.256
seguiremos mejorando
nuestra tecnología si es posible.

00:02:05.200 --> 00:02:06.867
¿Qué podría impedirnos hacer eso?

00:02:07.800 --> 00:02:09.600
¿Una guerra nuclear a gran escala?

00:02:11.000 --> 00:02:12.542
¿Una pandemia mundial?

00:02:14.320 --> 00:02:15.790
¿El impacto de un asteroide?

00:02:17.375 --> 00:02:20.542
¿El hecho que Justin Bieber
podría ser el presidente de los EE.UU.?

00:02:20.543 --> 00:02:22.205
(Risas)

00:02:24.760 --> 00:02:28.680
El caso es que algo tendría que destruir
la civilización tal como la conocemos.

00:02:29.360 --> 00:02:33.655
Realmente deberíamos
imaginar algo terrible

00:02:33.656 --> 00:02:36.991
para dejar de
desarrollar nuestra tecnología

00:02:36.992 --> 00:02:38.207
para siempre,

00:02:38.208 --> 00:02:40.223
generación tras generación.

00:02:40.224 --> 00:02:42.359
Casi por definición, sería lo peor

00:02:42.360 --> 00:02:44.319
en la historia humana.

00:02:44.320 --> 00:02:45.815
La única alternativa,

00:02:45.816 --> 00:02:48.391
y esto es lo que hay detrás
de la puerta número dos,

00:02:48.392 --> 00:02:51.335
es seguir mejorando
nuestras máquinas inteligentes

00:02:51.336 --> 00:02:52.936
año tras año tras año.

00:02:53.720 --> 00:02:57.360
En algún momento construiremos
máquinas más inteligentes que nosotros,

00:02:57.840 --> 00:03:00.765
y una vez que las tengamos

00:03:00.766 --> 00:03:02.625
empezarán a mejorarse a sí mismas.

00:03:02.626 --> 00:03:05.709
Y entonces corremos el riesgo
teorizado por el matemático IJ Good

00:03:05.710 --> 00:03:07.386
llamado "explosión de inteligencia"

00:03:07.506 --> 00:03:09.620
donde el proceso podría
salirse de control.

00:03:10.120 --> 00:03:12.935
Esto es a menudo caricaturizado,
como lo he hecho aquí,

00:03:12.936 --> 00:03:17.142
como el miedo a que nos ataquen
ejércitos de robots maliciosos.

00:03:17.408 --> 00:03:20.103
Pero ese no es
el escenario más probable.

00:03:20.104 --> 00:03:24.959
No es que nuestras máquinas
se volverán malignas espontáneamente.

00:03:24.960 --> 00:03:26.515
La preocupación verdadera

00:03:26.516 --> 00:03:29.631
al construir máquinas mucho
más competentes que nosotros

00:03:29.632 --> 00:03:33.407
es que la menor diferencia
entre sus objetivos y los nuestros

00:03:33.408 --> 00:03:34.608
nos podría destruir.

00:03:35.960 --> 00:03:38.629
Basta con pensar en nuestra
relación con las hormigas.

00:03:38.630 --> 00:03:40.249
No las odiamos.

00:03:40.250 --> 00:03:41.991
No vamos por la vida lastimándolas.

00:03:41.992 --> 00:03:44.987
De hecho, a veces nos tomamos
la molestia de no hacerles daño.

00:03:44.988 --> 00:03:46.776
Evitamos pisarlas en la acera.

00:03:46.792 --> 00:03:49.787
Pero cada vez que su presencia
entra seriamente en conflicto

00:03:49.788 --> 00:03:51.407
con alguno de nuestros objetivos,

00:03:51.408 --> 00:03:53.897
digamos, en la construcción
de un edificio como este,

00:03:54.017 --> 00:03:55.977
las aniquilamos sin escrúpulos.

00:03:56.480 --> 00:03:59.415
La preocupación es que algún día
construyamos máquinas

00:03:59.416 --> 00:04:01.849
que, ya sea conscientemente o no,

00:04:01.850 --> 00:04:04.200
nos puedan tratar con
una indiferencia similar.

00:04:05.760 --> 00:04:08.520
Sospecho que esto pueda
parece inverosímil para muchos.

00:04:09.375 --> 00:04:15.289
Apuesto a que hay quienes dudan
de que la superinteligente IA sea posible

00:04:15.290 --> 00:04:17.375
y mucho menos inevitable.

00:04:17.376 --> 00:04:20.995
Pero en este caso hay que refutar
uno de los siguientes supuestos.

00:04:20.996 --> 00:04:22.568
Y hay solo tres.

00:04:23.800 --> 00:04:28.519
La inteligencia es el procesamiento
de información en un sistema físico.

00:04:29.320 --> 00:04:31.934
En realidad, esto es poco
más que una suposición.

00:04:31.935 --> 00:04:35.391
Ya hemos incorporado inteligencia
limitada en nuestras máquinas,

00:04:35.392 --> 00:04:40.368
y aún así, muchas de estas máquinas actúan
a un nivel de inteligencia sobrehumana.

00:04:40.840 --> 00:04:46.045
Y sabemos que la mera materia da lugar
a lo que se llama "inteligencia general",

00:04:46.046 --> 00:04:49.687
la capacidad de pensar con
flexibilidad en múltiples campos

00:04:49.688 --> 00:04:52.823
porque nuestros cerebro
humano ya lo ha conseguido.

00:04:52.824 --> 00:04:56.759
Es decir, solo hay átomos aquí,

00:04:56.760 --> 00:05:01.255
y mientras continuemos
construyendo sistemas de átomos

00:05:01.256 --> 00:05:03.951
que exhiban un comportamiento
más y más inteligente,

00:05:03.952 --> 00:05:09.683
terminaremos implementando, a menos
que lo interrumpamos, inteligencia general

00:05:09.684 --> 00:05:11.159
en nuestras máquinas.

00:05:11.160 --> 00:05:14.789
Es crucial comprender que
la velocidad no es el problema

00:05:14.790 --> 00:05:18.255
porque cualquier velocidad
es suficiente para llegar al fin.

00:05:18.256 --> 00:05:22.031
No necesitamos la ley de Moore para
continuar ni un aumento exponencial.

00:05:22.032 --> 00:05:23.632
Solo tenemos que seguir adelante.

00:05:25.480 --> 00:05:28.400
El segundo supuesto es
que vamos a seguir adelante.

00:05:29.000 --> 00:05:31.760
Vamos a seguir mejorando
nuestras máquinas inteligentes.

00:05:34.590 --> 00:05:37.375
Y teniendo en cuenta
el valor de la inteligencia,

00:05:37.376 --> 00:05:40.911
es decir, la inteligencia es o bien
la fuente de todo lo que valoramos

00:05:40.912 --> 00:05:43.687
o la necesidad por preservar
todo lo que valoramos.

00:05:43.688 --> 00:05:45.943
Es nuestro recurso más valioso.

00:05:45.944 --> 00:05:47.479
Por eso lo queremos hacer.

00:05:47.480 --> 00:05:50.815
Tenemos problemas que necesitamos
desesperadamente resolver.

00:05:50.816 --> 00:05:54.490
Queremos curar enfermedades
como el Alzheimer y el cáncer.

00:05:55.170 --> 00:05:57.179
Queremos entender
los sistemas económicos.

00:05:57.180 --> 00:05:59.319
Queremos mejorar el clima.

00:05:59.320 --> 00:06:01.175
Vamos a hacer esto, si podemos.

00:06:01.176 --> 00:06:04.462
El tren ya salió de la estación
y no hay frenos.

00:06:05.880 --> 00:06:11.335
Por último, no estamos en
la cima de la inteligencia,

00:06:11.336 --> 00:06:13.136
ni siquiera cerca, probablemente.

00:06:13.640 --> 00:06:15.535
Y esto realmente es crucial.

00:06:15.536 --> 00:06:17.951
Esto es lo que hace nuestra
situación tan precaria,

00:06:17.952 --> 00:06:20.239
y esto es lo que hace
que nuestras intuiciones

00:06:20.240 --> 00:06:22.990
sobre los riesgos sean poco fiables.

00:06:23.000 --> 00:06:26.650
Piensen en la persona más inteligente
que jamás haya vivido.

00:06:26.667 --> 00:06:30.055
En la lista de casi todos
está John Von Neumann.

00:06:30.056 --> 00:06:33.391
La impresión que hacía Von Neumann
en las personas a su alrededor,

00:06:33.392 --> 00:06:37.447
incluyendo los más grandes
matemáticos y físicos de su época,

00:06:37.448 --> 00:06:39.383
está bastante bien documentada.

00:06:39.384 --> 00:06:43.159
Si solo la mitad de las historias
sobre él fueran una verdad a medias,

00:06:43.160 --> 00:06:44.255
no hay duda

00:06:44.256 --> 00:06:47.141
de que es una de las personas
más inteligentes que ha vivido.

00:06:47.142 --> 00:06:49.560
Así que consideren
el espectro de la inteligencia.

00:06:50.320 --> 00:06:51.919
Aquí tenemos a John Von Neumann.

00:06:53.560 --> 00:06:54.894
Y aquí estamos tú y yo.

00:06:56.120 --> 00:06:57.416
Y luego tenemos un pollo.

00:06:59.400 --> 00:07:00.776
Lo sentimos, una gallina.

00:07:01.920 --> 00:07:05.016
No hay por qué hacer esta charla
más deprimente de lo que ya es.

00:07:05.017 --> 00:07:06.876
(Risas)

00:07:08.339 --> 00:07:11.815
Sin embargo, parece muy probable
que el espectro de la inteligencia

00:07:11.816 --> 00:07:14.936
se extienda mucho más allá
de lo que actualmente concebimos,

00:07:15.880 --> 00:07:19.095
y si construimos máquinas
más inteligentes que nosotros,

00:07:19.096 --> 00:07:21.391
muy probablemente
explorarán este espectro

00:07:21.392 --> 00:07:23.247
de maneras que no podemos imaginar,

00:07:23.248 --> 00:07:25.768
y nos superarán de maneras inimaginables.

00:07:27.000 --> 00:07:31.229
Y es importante saber que esto
es cierto solo debido a la velocidad.

00:07:31.230 --> 00:07:36.415
Así que imaginen que acabamos
de construir una IA superinteligente

00:07:36.416 --> 00:07:37.971
que no fuera más inteligente

00:07:37.972 --> 00:07:42.084
que el promedio del equipo de
investigadores en Stanford o el MIT.

00:07:42.110 --> 00:07:43.835
Los circuitos electrónicos funcionan

00:07:43.836 --> 00:07:46.995
aproximadamente un millón de veces
más rápido que los bioquímicos,

00:07:46.996 --> 00:07:49.915
así que esta máquina debe pensar
un millón de veces más rápido

00:07:49.916 --> 00:07:51.615
que las mentes que la construyeron.

00:07:51.616 --> 00:07:53.305
Con una semana funcionando

00:07:53.306 --> 00:07:57.700
llevará a cabo 20 000 años
de trabajo intelectual a nivel humano,

00:07:58.400 --> 00:08:00.360
semana tras semana tras semana.

00:08:01.640 --> 00:08:04.735
¿Cómo podríamos siquiera comprender,
mucho menos restringir,

00:08:04.736 --> 00:08:07.016
una mente que progresa de esta manera?

00:08:08.840 --> 00:08:10.975
Algo más que es francamente preocupante

00:08:10.976 --> 00:08:15.951
es imaginar el mejor de los casos.

00:08:15.952 --> 00:08:19.649
Imaginemos que diseñamos
una IA superinteligente

00:08:19.650 --> 00:08:21.575
que no tiene problemas de seguridad.

00:08:21.576 --> 00:08:24.831
Tenemos el diseño perfecto a la primera.

00:08:24.832 --> 00:08:27.047
Es como si nos dieran un oráculo

00:08:27.048 --> 00:08:29.063
que se comporta exactamente
como se espera.

00:08:29.064 --> 00:08:32.994
Esta máquina sería el dispositivo
de ahorro de mano de obra perfecta.

00:08:33.520 --> 00:08:36.108
Puede diseñar la máquina
que puede construir la máquina

00:08:36.109 --> 00:08:38.071
que pueda hacer cualquier trabajo físico,

00:08:38.072 --> 00:08:39.385
impulsada por la luz solar,

00:08:39.386 --> 00:08:42.071
más o menos por el costo
de las materias primas.

00:08:42.072 --> 00:08:44.799
Estamos hablando del fin
del trabajo pesado humano.

00:08:45.490 --> 00:08:48.840
También estamos hablando del fin
de la mayoría del trabajo intelectual.

00:08:49.121 --> 00:08:52.280
Entonces, ¿qué harían simios como
nosotros en estas circunstancias?

00:08:52.292 --> 00:08:56.375
Pues, podríamos jugar
al Frisbee y darnos masajes.

00:08:57.840 --> 00:09:00.489
Tomar un poco de LSD
e inventar modas ridículas

00:09:00.490 --> 00:09:03.305
y todo el mundo podríamos
parecernos a un festival de rock.

00:09:03.306 --> 00:09:04.745
(Risas)

00:09:06.320 --> 00:09:10.726
Puede parecer muy bueno,
pero hay que preguntarse

00:09:10.746 --> 00:09:14.391
qué pasaría con nuestro orden
económico y político actual.

00:09:14.392 --> 00:09:16.807
Podríamos presenciar

00:09:16.808 --> 00:09:20.943
un nivel de desigualdad
de la riqueza y el desempleo

00:09:20.944 --> 00:09:22.439
nunca antes visto

00:09:22.440 --> 00:09:25.605
sin la voluntad de poner esta
nueva riqueza inmediatamente

00:09:25.606 --> 00:09:27.450
al servicio de toda la humanidad,

00:09:27.459 --> 00:09:31.166
y unos poco trillonarios estarían en
las portadas de las revistas de negocios

00:09:31.167 --> 00:09:34.645
mientras que el resto del mundo
tendría la libertad de morirse de hambre.

00:09:34.646 --> 00:09:37.196
Y ¿qué pasaría si los rusos
o los chinos se enteraran

00:09:37.209 --> 00:09:39.242
de que alguna empresa en Silicon Valley

00:09:39.243 --> 00:09:41.967
está a punto de crear
una IA superinteligente?

00:09:41.968 --> 00:09:44.824
Esta máquina podría ser
capaz de hacer la guerra,

00:09:44.834 --> 00:09:47.039
ya sea terrestre o cibernética,

00:09:47.040 --> 00:09:49.040
con un poder sin precedentes.

00:09:49.920 --> 00:09:51.975
En este escenario
el ganador se lleva todo.

00:09:51.976 --> 00:09:55.111
Seis meses adelante en la competencia

00:09:55.112 --> 00:09:57.887
sería una ventaja de 500 000 años,

00:09:57.888 --> 00:09:59.383
como mínimo.

00:09:59.384 --> 00:10:04.119
Parecería que incluso meros
rumores de este tipo de avance

00:10:04.120 --> 00:10:06.495
podría causar que nuestra
especie se vuelva loca.

00:10:06.496 --> 00:10:09.391
Una de las cosas más aterradoras,

00:10:09.392 --> 00:10:12.167
en mi opinión, en este momento,

00:10:12.168 --> 00:10:16.449
son el tipo de cosas que dicen
los investigadores de IA

00:10:16.450 --> 00:10:18.240
cuando quieren tranquilizarnos.

00:10:18.722 --> 00:10:22.455
Y el motivo invocado más frecuentemente
de que no nos preocupemos es el tiempo.

00:10:22.456 --> 00:10:24.511
Falta mucho para eso, no se preocupen.

00:10:24.512 --> 00:10:26.952
Eso será probablemente
dentro de 50 o 100 años.

00:10:27.720 --> 00:10:28.835
Un investigador dijo,

00:10:28.836 --> 00:10:31.500
"Preocuparse por la seguridad
y todo lo relacionado con la IA

00:10:31.501 --> 00:10:34.085
es como preocuparse por la
superpoblación en Marte".

00:10:34.125 --> 00:10:37.584
Esta es la manera de Silicon Valley
de mostrarse condescendiente.

00:10:37.585 --> 00:10:39.519
(Risas)

00:10:39.520 --> 00:10:41.416
Nadie parece darse cuenta

00:10:41.417 --> 00:10:44.031
que tomar el tiempo con referencia

00:10:44.032 --> 00:10:46.607
es una incongruencia total.

00:10:46.608 --> 00:10:50.259
Si la inteligencia es solo una cuestión
de procesamiento de la información

00:10:50.260 --> 00:10:52.616
y seguimos mejorando nuestras máquinas,

00:10:52.626 --> 00:10:55.502
produciremos algún tipo
de superinteligencia.

00:10:56.320 --> 00:11:01.165
Y no tenemos idea de cuánto tiempo
nos llevará crear las condiciones

00:11:01.166 --> 00:11:02.786
para hacerlo de forma segura.

00:11:04.200 --> 00:11:05.495
Voy a decirlo de nuevo.

00:11:05.496 --> 00:11:10.561
Y no tenemos idea de cuánto tiempo
nos llevará crear las condiciones

00:11:10.562 --> 00:11:12.182
para hacerlo de forma segura.

00:11:12.920 --> 00:11:16.375
Y si no lo han notado, 50 años
ya no son lo que solían ser.

00:11:16.376 --> 00:11:18.831
Estos son 50 años en meses.

00:11:18.832 --> 00:11:20.992
Este es el tiempo que
hemos tenido el iPhone.

00:11:21.440 --> 00:11:24.410
Este es el tiempo que "Los Simpson"
ha estado en la televisión.

00:11:24.704 --> 00:11:27.079
Cincuenta años no es tanto tiempo

00:11:27.080 --> 00:11:30.880
para lograr uno de los mayores desafíos
al que nuestra especie se ha enfrentado.

00:11:31.640 --> 00:11:35.655
Una vez más, parece que no tenemos
una respuesta emocional adecuada

00:11:35.656 --> 00:11:38.351
para lo que, con toda
probabilidad, va a pasar.

00:11:38.352 --> 00:11:42.327
El científico de la computación
Stuart Russell ofrece una gran analogía:

00:11:42.328 --> 00:11:48.264
"Imaginen que recibimos un mensaje de
una civilización extraterrestre que diga:

00:11:49.040 --> 00:11:50.575
'Gente de la Tierra,

00:11:50.576 --> 00:11:52.936
llegaremos en su planeta en 50 años.

00:11:53.800 --> 00:11:54.896
Prepárense'".

00:11:55.400 --> 00:11:58.780
¿Estaremos contando los meses
hasta que llegue la nave nodriza?

00:11:59.930 --> 00:12:03.190
¿No estaríamos un poco más preocupados?

00:12:04.340 --> 00:12:06.496
Otra razón que se nos da
para no preocuparnos

00:12:06.496 --> 00:12:09.381
es que estas máquinas no podrán
no compartir nuestros valores

00:12:09.382 --> 00:12:12.347
porque van a ser literalmente
extensiones de nosotros mismos.

00:12:12.348 --> 00:12:14.049
Se injertarán en nuestro cerebro,

00:12:14.050 --> 00:12:16.417
y prácticamente seremos
su sistema límbico.

00:12:17.120 --> 00:12:18.949
Consideren por un momento

00:12:18.950 --> 00:12:21.735
que el camino más seguro
y prudente hacia adelante,

00:12:21.736 --> 00:12:22.882
el recomendado,

00:12:22.882 --> 00:12:26.169
es la implantación de esta tecnología
directamente en nuestro cerebro.

00:12:26.170 --> 00:12:27.939
Ahora bien, esto puede ser de hecho,

00:12:27.940 --> 00:12:30.236
la manera más segura
y prudente de avanzar,

00:12:30.250 --> 00:12:33.667
pero por lo general, los problemas
de seguridad de una nueva tecnología

00:12:33.668 --> 00:12:36.849
hay que resolverlos antes
de implementarla en una cabeza.

00:12:36.850 --> 00:12:38.799
(Risas)

00:12:38.800 --> 00:12:40.339
El mayor problema es

00:12:40.340 --> 00:12:45.775
que construir una IA superinteligente
y autónoma parece más fácil

00:12:45.776 --> 00:12:47.751
que diseñar una IA superinteligente

00:12:47.752 --> 00:12:49.528
mientras se controla la neurociencia

00:12:49.542 --> 00:12:52.209
para integrar la máquina
y la mente sin problemas.

00:12:52.800 --> 00:12:55.975
Y dado que las empresas y
los gobiernos que trabajan sobre ello

00:12:55.976 --> 00:12:59.631
probablemente se vean a sí mismos en
una carrera contra todos los demás,

00:12:59.632 --> 00:13:02.759
y tomando en cuenta que ganar
esta carrera es ganar el mundo,

00:13:02.760 --> 00:13:05.545
siempre y cuando no lo destruyan
en el momento siguiente,

00:13:05.546 --> 00:13:08.055
entonces parece probable
que lo más fácil de hacer

00:13:08.056 --> 00:13:09.256
se hará primero.

00:13:10.560 --> 00:13:13.416
Ahora, por desgracia, no tengo
una solución a este problema,

00:13:13.417 --> 00:13:16.001
además de recomendarles
que piensen más sobre ello.

00:13:16.002 --> 00:13:18.339
Creo que necesitamos
un Proyecto Manhattan

00:13:18.340 --> 00:13:20.495
sobre el tema de la
inteligencia artificial.

00:13:20.496 --> 00:13:23.231
No para construirlo, porque
creo que eso es inevitable,

00:13:23.232 --> 00:13:26.567
sino para entender cómo evitar
una carrera armamentística

00:13:26.568 --> 00:13:30.063
y construirla de manera que
concuerde con nuestros intereses.

00:13:30.064 --> 00:13:32.199
Cuando hablamos de IA superinteligente

00:13:32.200 --> 00:13:34.266
que puede modificarse a sí misma,

00:13:34.890 --> 00:13:39.215
solo tenemos una oportunidad de fijar
las condiciones iniciales correctas,

00:13:39.216 --> 00:13:41.271
e incluso entonces necesitaremos gestionar

00:13:41.272 --> 00:13:44.312
las consecuencias económicas
y políticas de dicho diseño.

00:13:45.760 --> 00:13:47.815
Pero en el momento en que admitamos

00:13:47.816 --> 00:13:51.816
que el procesamiento de la información
es la fuente de la inteligencia,

00:13:52.720 --> 00:13:57.520
que algún sistema computacional
adecuado es la base de la inteligencia,

00:13:58.360 --> 00:14:02.120
y admitamos que los vamos
a mejorar de forma continua,

00:14:03.280 --> 00:14:07.399
y admitamos que el horizonte
del conocimiento supera por mucho

00:14:07.400 --> 00:14:08.960
lo que actualmente conocemos,

00:14:10.120 --> 00:14:11.555
entonces tenemos que admitir

00:14:11.556 --> 00:14:14.440
que estamos en el proceso de
construir una especie de dios.

00:14:15.400 --> 00:14:16.975
Ahora sería un buen momento

00:14:16.976 --> 00:14:19.799
para asegurarse de que sea
un dios con el que podamos vivir.

00:14:20.120 --> 00:14:21.165
Muchas gracias.

00:14:21.166 --> 00:14:22.346
(Aplausos)

