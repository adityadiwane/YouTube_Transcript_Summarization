WEBVTT
Kind: captions
Language: el

00:00:00.000 --> 00:00:07.000
Μετάφραση: Lucas Kaimaras
Επιμέλεια: Nikolaos Benias

00:00:12.870 --> 00:00:15.466
Θα σας μιλήσω για ένα σφάλμα στη λογική

00:00:15.466 --> 00:00:17.150
που έχουν πολλοί από εμάς.

00:00:17.480 --> 00:00:20.840
Στην ουσία είναι αδυναμία να δούμε
ένα ορισμένο είδος κινδύνου.

00:00:21.384 --> 00:00:23.120
Θα περιγράψω ένα σενάριο

00:00:23.120 --> 00:00:26.376
που θεωρώ ταυτόχρονα τρομακτικό,

00:00:26.376 --> 00:00:28.336
αλλά και πιθανό να συμβεί,

00:00:28.840 --> 00:00:30.496
και δεν είναι καλός συνδυασμός,

00:00:30.496 --> 00:00:31.902
όπως τελικά αποδεικνύεται.

00:00:32.080 --> 00:00:34.676
Αλλά αντί να τρομοκρατείστε,
οι περισσότεροι πιστεύετε

00:00:34.676 --> 00:00:36.860
ότι αυτό που σας λέω είναι μάλλον ωραίο.

00:00:37.200 --> 00:00:38.580
Θα σας περιγράψω

00:00:38.580 --> 00:00:41.976
πώς τα οφέλη που θα δρέψουμε
από την τεχνητή νοημοσύνη

00:00:41.976 --> 00:00:43.752
μπορούν εν τέλει να μας καταστρέψουν.

00:00:43.800 --> 00:00:46.580
Και μάλιστα το θεωρώ απίθανο
είτε να μην μας καταστρέψουν

00:00:46.580 --> 00:00:48.960
ή να μας εμπνεύσουν
να καταστραφούμε μόνοι μας.

00:00:49.400 --> 00:00:51.256
Αν όμως σκέφτεστε όπως εγώ,

00:00:51.256 --> 00:00:53.782
θα το απολαμβάνετε
να σκέφτεστε αυτά τα πράγματα.

00:00:53.960 --> 00:00:57.140
Κι αυτή η αντίδραση
είναι μέρος του προβλήματος.

00:00:57.140 --> 00:00:59.600
Εντάξει; Αυτή η αντίδραση
θα πρέπει να σας ανησυχεί.

00:00:59.920 --> 00:01:02.576
Κι αν επρόκειτο να σας πείσω
με αυτή την ομιλία

00:01:02.576 --> 00:01:05.992
ότι είναι πιθανόν
να αντιμετωπίσουμε παγκόσμιο λιμό,

00:01:05.992 --> 00:01:09.048
είτε λόγω κλιματικής αλλαγής
ή κάποιας άλλης καταστροφής,

00:01:09.048 --> 00:01:12.464
και ότι τα εγγόνια σας,
ή τα δικά τους εγγόνια,

00:01:12.464 --> 00:01:14.904
είναι πολύ πιθανόν να ζήσουν κάπως έτσι,

00:01:15.200 --> 00:01:16.400
δεν θα σκεφτόσασταν,

00:01:17.440 --> 00:01:18.776
«Ενδιαφέρον.

00:01:18.776 --> 00:01:20.636
Μου αρέσει αυτή η ομιλία TED».

00:01:21.200 --> 00:01:23.180
Ο λιμός δεν είναι κάτι διασκεδαστικό.

00:01:23.720 --> 00:01:27.266
Από την άλλη, ο θάνατος μέσω επιστημονικής
φαντασίας είναι διασκεδαστικός,

00:01:27.266 --> 00:01:31.176
και κάτι που με ανησυχεί σε αυτό το στάδιο
της εξέλιξης της τεχνητής νοημοσύνης

00:01:31.176 --> 00:01:35.272
είναι ότι αδυνατούμε να επιδείξουμε
μια κατάλληλη συναισθηματική αντίδραση

00:01:35.272 --> 00:01:37.088
στους επικείμενους κινδύνους.

00:01:37.160 --> 00:01:40.660
Ούτε εγώ μπορώ να έχω αυτή την αντίδραση
και κάνω αυτή την ομιλία.

00:01:42.120 --> 00:01:44.496
Είναι σαν να στεκόμαστε
μπροστά σε δύο πόρτες.

00:01:44.840 --> 00:01:46.096
Πίσω από την πρώτη πόρτα,

00:01:46.096 --> 00:01:49.262
παύουμε να κάνουμε πρόοδο
στην παραγωγή ευφυών μηχανών.

00:01:49.440 --> 00:01:53.316
Για κάποιον λόγο παύουμε να βελτιώνουμε
υλισμικό και λογισμικό στους υπολογιστές.

00:01:53.480 --> 00:01:56.620
Τώρα σκεφτείτε για λίγο
γιατί μπορεί να συμβεί αυτό.

00:01:57.080 --> 00:02:00.736
Εννοώ ότι, δεδομένης της μεγάλης αξίας
της ευφυίας και των αυτοματισμών,

00:02:00.736 --> 00:02:04.526
εφόσον μπορούμε, θα συνεχίσουμε
να βελτιώνουμε την τεχνολογία μας.

00:02:05.200 --> 00:02:06.987
Τι μπορεί να μας αποτρέψει;

00:02:07.800 --> 00:02:09.910
Ένας πυρηνικός πόλεμος πλήρους κλίμακας;

00:02:11.000 --> 00:02:12.920
Μια παγκόσμια πανδημία;

00:02:14.320 --> 00:02:16.190
Μια σύγκρουση με αστεροειδή;

00:02:17.640 --> 00:02:20.216
Να γίνει ο Τζάστιν Μπίμπερ
πρόεδρος των ΗΠΑ;

00:02:20.240 --> 00:02:22.520
(Γέλια)

00:02:24.760 --> 00:02:29.100
Το θέμα είναι ότι κάτι θα πρέπει
να καταστρέψει τον σύγχρονο πολιτισμό.

00:02:29.360 --> 00:02:33.576
Θα πρέπει να φανταστείτε
πόσο οδυνηρό πρέπει να είναι

00:02:33.576 --> 00:02:36.812
για να μας αποτρέψει
από το να βελτιώσουμε την τεχνολογία μας

00:02:36.812 --> 00:02:38.268
σε μόνιμη βάση,

00:02:38.268 --> 00:02:40.174
στις επόμενες γενεές.

00:02:40.320 --> 00:02:41.870
Αυτό είναι σχεδόν εξ ορισμού

00:02:41.870 --> 00:02:44.496
ό,τι χειρότερο έχει ποτέ συμβεί
στην ανθρώπινη ιστορία.

00:02:44.520 --> 00:02:45.816
Έτσι η μόνη εναλλακτική

00:02:45.816 --> 00:02:48.152
-κι αυτή βρίσκεται
πίσω από τη δεύτερη πόρτα-

00:02:48.152 --> 00:02:51.288
είναι να συνεχίσουμε να βελτιώνουμε
τις έξυπνες μηχανές μας

00:02:51.288 --> 00:02:53.208
χρόνο με τον χρόνο.

00:02:53.720 --> 00:02:57.590
Σε κάποιο σημείο, θα φτιάξουμε μηχανές
που θα είναι πιο έξυπνες από εμάς,

00:02:58.080 --> 00:03:00.470
κι όταν θα έχουμε μηχανές
εξυπνότερες από εμάς,

00:03:00.470 --> 00:03:02.496
θα αρχίσουν να βελτιώνονται μόνες τους.

00:03:02.720 --> 00:03:04.420
Τότε διακινδυνεύουμε να έχουμε κάτι

00:03:04.420 --> 00:03:07.256
που ο μαθηματικός Ι. Τζ. Γκουντ
αποκάλεσε «έκρηξη ευφυίας»,

00:03:07.256 --> 00:03:09.756
όπου η διαδικασία μπορεί
να φύγει από τον έλεγχό μας.

00:03:10.144 --> 00:03:12.960
Αυτό συχνά απεικονίζεται,
όπως σας δείχνω εδώ,

00:03:12.960 --> 00:03:16.176
ως ένας φόβος ότι στρατιές
από κακόβουλα ρομπότ

00:03:16.176 --> 00:03:17.432
θα μας επιτεθούν.

00:03:17.480 --> 00:03:19.996
Αλλά αυτό δεν είναι το πιο πιθανό σενάριο.

00:03:20.200 --> 00:03:24.946
Δεν είναι ότι οι μηχανές μας
θα γίνουν αυθόρμητα κακόβουλες.

00:03:25.090 --> 00:03:27.706
Η πραγματική ανησυχία είναι
ότι θα φτιάξουμε μηχανές

00:03:27.706 --> 00:03:29.812
που θα είναι τόσο πιο ικανές από εμάς

00:03:29.812 --> 00:03:33.576
που η ελάχιστη απόκλιση μεταξύ
των δικών τους στόχων και των δικών μας

00:03:33.576 --> 00:03:35.256
θα μπορούσε να μας καταστρέψει.

00:03:35.960 --> 00:03:38.200
Σκεφτείτε τη σχέση μας με τα μυρμήγκια.

00:03:38.600 --> 00:03:39.946
Δεν τα μισούμε.

00:03:40.280 --> 00:03:42.176
Δεν τα βλάπτουμε εκουσίως.

00:03:42.176 --> 00:03:44.712
Συχνά μάλιστα κάνουμε προσπάθεια
να μην τα βλάψουμε.

00:03:44.712 --> 00:03:46.836
Αποφεύγουμε να τα πατήσουμε
στο πεζοδρόμιο.

00:03:46.836 --> 00:03:48.936
Αλλά όταν η παρουσία τους

00:03:48.936 --> 00:03:51.432
αντιτίθεται σοβαρά με τους στόχους μας,

00:03:51.432 --> 00:03:53.909
ας πούμε, όταν φτιάχνουμε
ένα τέτοιο κτήριο,

00:03:53.909 --> 00:03:56.189
τα εκμηδενίζουμε χωρίς δεύτερη σκέψη.

00:03:56.480 --> 00:03:59.416
Η ανησυχία είναι ότι μια μέρα
θα φτιάξουμε μηχανές που,

00:03:59.416 --> 00:04:02.152
είτε ενσυνείδητα ή όχι,

00:04:02.152 --> 00:04:04.582
θα μας μεταχειριστούν
με παρόμοια αδιαφορία.

00:04:05.760 --> 00:04:08.520
Υποψιάζομαι ότι οι περισσότεροι
το θεωρείτε παρατραβηγμένο.

00:04:09.360 --> 00:04:14.190
Βάζω στοίχημα ότι κάποιοι αμφιβάλλουν
αν η υπερευφυής τεχνητή νοημοσύνη

00:04:14.190 --> 00:04:17.086
είναι εφικτή,
πολύ δε λιγότερο αναπόφευκτη.

00:04:17.400 --> 00:04:20.860
Τότε όμως πρέπει να βρείτε κάποιο λάθος
στις ακόλουθες υποθέσεις.

00:04:21.044 --> 00:04:22.616
Και είναι μόνο τρεις υποθέσεις.

00:04:23.800 --> 00:04:28.639
Η ευφυΐα είναι θέμα επεξεργασίας
της πληροφορίας σε υλικά συστήματα.

00:04:29.320 --> 00:04:31.825
Βασικά, αυτό είναι
κάτι παραπάνω από μια υπόθεση.

00:04:31.825 --> 00:04:35.412
Έχουμε ήδη ενσωματώσει
περιορισμένη ευφυΐα στις μηχανές μας,

00:04:35.412 --> 00:04:37.428
και πολλές από αυτές ήδη αποδίδουν

00:04:37.428 --> 00:04:40.518
σε επίπεδο υπεράνθρωπης ευφυΐας.

00:04:40.840 --> 00:04:43.416
Και ξέρουμε πως η απλή ύλη

00:04:43.416 --> 00:04:46.032
μπορεί να δημιουργήσει
αυτό που λέγεται «γενική ευφυΐα»,

00:04:46.032 --> 00:04:49.688
μια ικανότητα να σκεφτούμε ευέλικτα
σε πολλαπλά επίπεδα,

00:04:49.688 --> 00:04:52.604
επειδή τα μυαλά μας
τα έχουν καταφέρει, σωστά;

00:04:52.920 --> 00:04:56.856
Εξάλλου μόνο άτομα υπάρχουν εκεί,

00:04:56.880 --> 00:05:01.376
και όσο συνεχίζουμε να φτιάχνουμε
συστήματα από άτομα

00:05:01.376 --> 00:05:04.072
που επιδεικνύουν όλο και
περισσότερο ευφυή συμπεριφορά,

00:05:04.072 --> 00:05:06.608
αν δεν μας σταματήσει κάποιος,

00:05:06.608 --> 00:05:09.984
τελικά θα βάλουμε τη γενική ευφυΐα

00:05:09.984 --> 00:05:11.280
μέσα στις μηχανές μας.

00:05:11.400 --> 00:05:15.006
Είναι σημαντικό να καταλάβουμε
ότι δεν έχει σημασία ο ρυθμός της προόδου,

00:05:15.006 --> 00:05:18.412
γιατί κάθε πρόοδος είναι αρκετή
για να οδηγήσει σε αυτή την κατάληξη.

00:05:18.412 --> 00:05:21.956
Δεν χρειάζεται ο Νόμος του Μουρ,
ούτε κάποια εκθετική πρόοδος.

00:05:21.956 --> 00:05:24.026
Χρειάζεται απλώς να συνεχίσουμε.

00:05:25.480 --> 00:05:28.610
Η δεύτερη υπόθεση είναι
ότι θα συνεχίσουμε.

00:05:29.000 --> 00:05:32.130
Θα συνεχίσουμε να βελτιώνουμε
τις έξυπνες μηχανές μας.

00:05:33.000 --> 00:05:37.376
Και σύμφωνα με την αξία της ευφυΐας -

00:05:37.376 --> 00:05:40.912
εννοώ ότι είτε η ευφυΐα
είναι η πηγή για ό,τι θεωρούμε άξιο,

00:05:40.912 --> 00:05:43.818
ή χρειάζεται να περιφρουρήσουμε
οτιδήποτε θεωρούμε άξιο.

00:05:43.818 --> 00:05:46.016
Είναι ο πιο πολύτιμος πόρος που έχουμε.

00:05:46.016 --> 00:05:47.552
Έτσι, θέλουμε να το κάνουμε.

00:05:47.552 --> 00:05:50.888
Έχουμε προβλήματα και πρέπει
να τα λύσουμε επειγόντως.

00:05:50.960 --> 00:05:54.310
Θέλουμε να θεραπεύσουμε νόσους
όπως τον καρκίνο και την Αλτσχάιμερ,

00:05:54.960 --> 00:05:58.836
να καταλάβουμε οικονομικά συστήματα,
να βελτιώσουμε τη γνώση μας για το κλίμα.

00:05:58.920 --> 00:06:01.176
Γι' αυτό θα το κάνουμε, αν μπορούμε.

00:06:01.176 --> 00:06:04.682
Το τρένο έχει ήδη ξεκινήσει
και δεν έχει φρένο.

00:06:05.880 --> 00:06:11.336
Τέλος, δεν έχουμε φτάσει
στην αποκορύφωση της ευφυΐας,

00:06:11.336 --> 00:06:13.356
μάλλον ούτε που την πλησιάσαμε.

00:06:13.640 --> 00:06:15.536
Και αυτή είναι η σημαντική διαπίστωση.

00:06:15.536 --> 00:06:17.952
Αυτό κάνει τόσο επισφαλή
την κατάστασή μας,

00:06:17.952 --> 00:06:22.132
και τόσο αναξιόπιστα
τα προαίσθήματά μας σχετικά με το ρίσκο.

00:06:23.120 --> 00:06:26.050
Απλώς σκεφτείτε το πιο έξυπνο
άτομο που έχει ποτέ ζήσει.

00:06:26.640 --> 00:06:30.056
Κάποιον που όλοι σας θα έχετε κατά νου
είναι ο Τζον φον Νόιμαν.

00:06:30.080 --> 00:06:33.416
Η εντύπωση που έκανε
στους τριγύρω του ο φον Νόιμαν,

00:06:33.416 --> 00:06:37.472
συμπεριλαμβανομένων των πιο λαμπρών
μαθηματικών και φυσικών της εποχής του,

00:06:37.472 --> 00:06:39.408
έχει καταγραφεί επαρκώς.

00:06:39.480 --> 00:06:42.880
Και μόνο οι μισές ιστορίες
γι' αυτόν να αληθεύουν,

00:06:42.880 --> 00:06:44.286
χωρίς αμφιβολία

00:06:44.286 --> 00:06:46.952
είναι από τους πιο έξυπνους
ανθρώπους που έχουν ζήσει.

00:06:47.012 --> 00:06:49.690
Σκεφτείτε λοιπόν το φάσμα της ευφυΐας.

00:06:50.320 --> 00:06:52.019
Εδώ είναι ο Τζον φον Νόιμαν.

00:06:53.560 --> 00:06:55.404
Κι εδώ είμαστε εσείς κι εγώ.

00:06:56.120 --> 00:06:57.576
Μετά είναι οι κότες.

00:06:57.576 --> 00:06:59.350
(Γέλια)

00:06:59.350 --> 00:07:00.666
Συγνώμη, μια κότα.

00:07:00.666 --> 00:07:01.896
(Γέλια)

00:07:01.920 --> 00:07:05.360
Δεν υπάρχει λόγος να κάνω την ομιλία
πιο θλιβερή απ' όσο χρειάζεται.

00:07:05.360 --> 00:07:07.280
(Γέλια)

00:07:08.339 --> 00:07:11.816
Ωστόσο, φαίνεται εκπληκτικά πιθανόν
ότι το φάσμα της ευφυΐας

00:07:11.816 --> 00:07:15.446
εκτείνεται πολύ πιο πέρα
από όσο τώρα αντιλαμβανόμαστε,

00:07:15.880 --> 00:07:18.816
και αν φτιάξουμε μηχανές
πιο έξυπνες από εμάς,

00:07:19.120 --> 00:07:21.416
είναι πολύ πιθανόν
να εξερευνήσουν αυτό το φάσμα

00:07:21.416 --> 00:07:23.592
με τρόπους που δεν μπορούμε να φανταστούμε

00:07:23.592 --> 00:07:26.110
και να μας ξεπεράσουν σε αφάνταστο βαθμό.

00:07:27.000 --> 00:07:31.266
Και είναι σημαντικό να αναγνωρίσουμε
ότι αυτό αληθεύει και μόνο λόγω ταχύτητας.

00:07:31.360 --> 00:07:36.116
Σωστά; Φανταστείτε να φτιάχναμε
μια υπερευφυή τεχνητή νοημοσύνη,

00:07:36.440 --> 00:07:39.896
που δεν θα ήταν εξυπνότερη
από μια μέση ομάδα ερευνητών

00:07:39.896 --> 00:07:41.982
των πανεπιστημίων Στάνφορντ ή ΜΙΤ.

00:07:42.240 --> 00:07:44.070
Τα ηλεκτρονικά κυκλώματα λειτουργούν

00:07:44.070 --> 00:07:46.536
ένα εκατομμύριο φορές ταχύτερα
από τα βιοχημικά,

00:07:46.536 --> 00:07:49.656
έτσι αυτή η μηχανή θα σκεφτόταν
ένα εκατομμύριο φορές πιο γρήγορα

00:07:49.656 --> 00:07:51.472
από τους εγκεφάλους που την έφτιαξαν.

00:07:51.544 --> 00:07:53.200
Με λειτουργία μίας εβδομάδας

00:07:53.200 --> 00:07:58.070
θα παράξει έργο 20.000 ετών
σε επίπεδο ανθρώπινης διάνοιας,

00:07:58.400 --> 00:08:00.820
και το ίδιο θα κάνει
κάθε επόμενη εβδομάδα.

00:08:01.640 --> 00:08:04.816
Πώς μπορούμε να καταλάβουμε,
πολύ δε περισσότερο να περιορίσουμε,

00:08:04.816 --> 00:08:07.510
έναν εγκέφαλο που κάνει τέτοια πρόοδο;

00:08:08.840 --> 00:08:12.056
Για να πω την αλήθεια,
είναι επίσης ανησυχητικό

00:08:12.056 --> 00:08:15.976
να φανταστούμε το καλύτερο δυνατό σενάριο.

00:08:16.000 --> 00:08:19.800
Φανταστείτε να βρούμε ένα σχέδιο
υπερεφυούς τεχνητής νοημοσύνης

00:08:19.800 --> 00:08:21.626
που δεν έχει θέματα ασφάλειας.

00:08:21.626 --> 00:08:24.666
Θα έχουμε βρει το τέλειο σχέδιο
για πρώτη φορά.

00:08:24.880 --> 00:08:27.096
Θα είναι σαν να μας έχει δοθεί ένα μαντείο

00:08:27.096 --> 00:08:29.112
που λειτουργεί ακριβώς όπως πρέπει.

00:08:29.112 --> 00:08:33.122
Αυτή θα ήταν η τέλεια μηχανή
εξοικονόμησης έργου.

00:08:33.590 --> 00:08:36.109
Θα σχεδιάσει τη μηχανή
που θα κατασκευάσει τη μηχανή

00:08:36.109 --> 00:08:38.062
που θα κάνει κάθε σωματική εργασία,

00:08:38.062 --> 00:08:39.400
τροφοδοτούμενη από τον ήλιο,

00:08:39.400 --> 00:08:41.916
με μόνο κόστος αυτό των πρώτων υλών.

00:08:42.120 --> 00:08:44.956
Έτσι μιλάμε για το τέλος
της ανθρώπινης αγγαρείας.

00:08:45.400 --> 00:08:48.520
Επίσης μιλάμε για το τέλος
της περισσότερης διανοητικής εργασίας.

00:08:49.200 --> 00:08:52.156
Τι θα κάνουν οι πίθηκοι όπως εμείς
σε αυτή την περίπτωση;

00:08:52.280 --> 00:08:56.810
Θα έχουμε χρόνο να παίζουμε φρίσμπι
και να κάνουμε μασάζ ο ένας στον άλλον.

00:08:57.840 --> 00:09:00.766
Προσθέστε λίγο LSD και κάποιες
αμφίβολες επιλογές ένδυσης,

00:09:00.766 --> 00:09:03.426
κι όλος ο κόσμος θα μοιάζει
με φεστιβάλ Burning Man.

00:09:03.426 --> 00:09:04.910
(Γέλια)

00:09:06.320 --> 00:09:08.760
Μπορεί βέβαια να ακούγεται αρκετά καλό,

00:09:09.280 --> 00:09:11.656
αλλά αναρωτηθείτε τι θα συνέβαινε

00:09:11.656 --> 00:09:14.242
με την τωρινή οικονομική
και πολιτική κατάσταση;

00:09:14.440 --> 00:09:16.856
Μοιάζει πιθανό ότι θα βιώσουμε

00:09:16.856 --> 00:09:20.992
ένα επίπεδο ανισότητας
πλούτου και ανεργίας

00:09:20.992 --> 00:09:22.578
που δεν έχουμε ξαναδεί.

00:09:22.578 --> 00:09:25.276
Αν απουσιάσει η θέληση
να βάλουμε άμεσα αυτόν τον πλούτο

00:09:25.276 --> 00:09:27.346
στην υπηρεσία όλης της ανθρωπότητας,

00:09:27.346 --> 00:09:31.256
ελάχιστοι πολυεκατομμυριούχοι θα κοσμούν
τα εξώφυλλα των οικονομικών περιοδικών,

00:09:31.256 --> 00:09:33.996
ενώ ο υπόλοιπος κόσμος
θα είναι ελεύθερος να λιμοκτονήσει.

00:09:34.320 --> 00:09:36.616
Και τι θα κάνουν οι Ρώσοι ή οι Κινέζοι

00:09:36.616 --> 00:09:39.232
αν μάθουν ότι μια εταιρεία
στη Σίλικον Βάλεϊ

00:09:39.232 --> 00:09:41.968
πρόκειται να κυκλοφορήσει
μια υπερευφυή τεχνητή νοημοσύνη;

00:09:42.064 --> 00:09:44.920
Αυτή η μηχανή θα μπορούσε
να κηρύξει πόλεμο,

00:09:44.920 --> 00:09:47.256
είτε συμβατικό
είτε σε επίπεδο υπολογιστών,

00:09:47.256 --> 00:09:49.350
με ισχύ άνευ προηγουμένου.

00:09:50.120 --> 00:09:52.156
Είναι ένα σενάριο όπου κερδίζει μόνο ένας.

00:09:52.156 --> 00:09:55.136
Όταν προηγείσαι εδώ
έξι μήνες από τον ανταγωνισμό,

00:09:55.160 --> 00:09:57.936
είναι σαν να είσαι 500.000 χρόνια μπροστά

00:09:57.936 --> 00:09:59.152
κατά το ελάχιστο.

00:09:59.480 --> 00:10:04.216
Άρα φαίνεται ότι ακόμη και απλές φήμες
για κάτι τόσο επαναστατικό

00:10:04.216 --> 00:10:06.402
θα έκανε το είδος μας να τρελαθεί.

00:10:06.640 --> 00:10:09.536
Ένα από τα πιο τρομακτικά πράγματα,

00:10:09.536 --> 00:10:12.092
κατά την άποψή μου, αυτή τη στιγμή,

00:10:12.360 --> 00:10:16.396
είναι αυτά που λένε οι ερευνητές
της τεχνητής νοημοσύνης

00:10:16.680 --> 00:10:18.610
όταν θέλουν να μας καθησυχάσουν.

00:10:18.840 --> 00:10:22.486
Κι ο πιο κοινός λόγος που προβάλλουν
για να μην ανησυχούμε είναι ο χρόνος.

00:10:22.486 --> 00:10:24.726
Αυτό είναι πολύ μακριά, δεν το βλέπετε;

00:10:24.726 --> 00:10:27.280
Είναι περίπου 50 ή 100 χρόνια μετά.

00:10:27.640 --> 00:10:28.876
Ένας ερευνητής έχει πει,

00:10:28.876 --> 00:10:31.146
«Ανησυχώντας για ασφάλεια
στην τεχνητή νοημοσύνη

00:10:31.146 --> 00:10:33.690
είναι σαν να ανησυχούμε
για υπερπληθυσμό στον Άρη».

00:10:34.116 --> 00:10:35.926
Αυτό είναι η έκδοση της Σίλικον Βάλεϊ

00:10:35.926 --> 00:10:38.186
του «μην ζαλίζεις το όμορφο κεφαλάκι σου».

00:10:38.186 --> 00:10:39.490
(Γέλια)

00:10:39.490 --> 00:10:41.416
Κανείς δεν φαίνεται να βλέπει

00:10:41.416 --> 00:10:44.272
ότι η αναφορά στον χρονικό ορίζοντα

00:10:44.272 --> 00:10:46.396
είναι τελείως ανακόλουθη.

00:10:46.590 --> 00:10:49.796
Αν η ευφυΐα είναι απλώς θέμα
επεξεργασίας της πληροφορίας,

00:10:49.960 --> 00:10:52.276
και συνεχίζουμε
να βελτιώνουμε τις μηχανές μας,

00:10:52.640 --> 00:10:55.770
σίγουρα θα δημιουργήσουμε
κάποια μορφή υπερευφυΐας.

00:10:56.320 --> 00:10:59.976
Και δεν ξέρουμε πόσο χρόνο θα μας πάρει

00:10:59.976 --> 00:11:03.146
να δημιουργήσουμε τις συνθήκες
ώστε να το κάνουμε με ασφάλεια.

00:11:04.200 --> 00:11:05.366
Να το πω πάλι.

00:11:05.520 --> 00:11:09.336
Δεν έχουμε ιδέα πόσο χρόνο θα μας πάρει

00:11:09.336 --> 00:11:12.326
να δημιουργήσουμε τις συνθήκες
ώστε να το κάνουμε με ασφάλεια.

00:11:12.920 --> 00:11:16.376
Κι αν δεν το έχετε προσέξει,
τα 50 χρόνια δεν είναι αυτό που ήταν.

00:11:16.376 --> 00:11:18.562
Αυτό είναι 50 χρόνια σε μήνες.

00:11:18.880 --> 00:11:20.720
Τόσον καιρό έχουμε το iPhone.

00:11:21.440 --> 00:11:24.040
Τόσον καιρό παίζονται 
οι Σίμπσονς στην τηλεόραση.

00:11:24.680 --> 00:11:27.056
Τα 50 χρόνια δεν είναι τόσο πολλά

00:11:27.056 --> 00:11:30.766
για μια από τις μεγαλύτερες προκλήσεις
που θα αντιμετωπίσει το είδος μας.

00:11:31.640 --> 00:11:35.656
Και πάλι - αδυνατούμε να επιδείξουμε
κατάλληλη συναισθηματική απόκριση

00:11:35.656 --> 00:11:38.352
σε κάτι που έχουμε κάθε λόγο
να πιστεύουμε ότι θα έρθει.

00:11:38.400 --> 00:11:42.376
Ο επιστήμων υπολογιστών Στιούαρτ Ράσελ
έχει μια ωραία αναλογία γι' αυτό.

00:11:42.376 --> 00:11:47.112
Είπε, φανταστείτε ότι λάβαμε μήνυμα
από εξωγήινο πολιτισμό,

00:11:47.320 --> 00:11:48.346
που έλεγε:

00:11:49.040 --> 00:11:50.366
«Άνθρωποι της Γης,

00:11:50.600 --> 00:11:53.240
θα φτάσουμε στον πλανήτη σας σε 50 χρόνια.

00:11:53.800 --> 00:11:55.006
Ετοιμαστείτε».

00:11:55.400 --> 00:11:59.436
Και τώρα απλώς μετράμε αντίστροφα
τον χρόνο μέχρι να έρθει το διαστημόπλοιο.

00:11:59.680 --> 00:12:03.350
Θα το νιώθαμε ως κάτι λίγο
πιο επιτακτικό από ό,τι τώρα.

00:12:04.370 --> 00:12:06.426
Μας λένε επίσης να μην ανησυχούμε,

00:12:06.426 --> 00:12:09.616
γιατί οι μηχανές δεν μπορούν
παρά να έχουν τις δικές μας αξίες,

00:12:09.616 --> 00:12:12.216
καθώς κυριολεκτικά
είναι προέκταση των εαυτών μας.

00:12:12.216 --> 00:12:14.032
Θα συνδεθούν στον εγκέφαλό μας

00:12:14.032 --> 00:12:16.752
και βασικά θα γίνουμε
το μεταιχμιακό τους σύστημα.

00:12:17.120 --> 00:12:18.536
Σκεφτείτε για ένα λεπτό

00:12:18.536 --> 00:12:21.712
ότι ο ασφαλέστερος και ο μόνος
συνετός δρόμος προς τα εμπρός

00:12:21.712 --> 00:12:22.968
που προτείνεται,

00:12:22.968 --> 00:12:26.328
είναι να εμφυτεύσουμε αυτή την τεχνολογία
απευθείας στον εγκέφαλό μας.

00:12:26.600 --> 00:12:29.976
Βέβαια, μπορεί να είναι ο ασφαλέστερος
και ο μόνος συνετός δρόμος,

00:12:29.976 --> 00:12:33.032
αλλά συνήθως οι ανησυχίες
για την ασφάλεια μιας τεχνολογίας

00:12:33.032 --> 00:12:36.688
πρέπει να έχουν επιλυθεί
πριν την εμφυτεύσουμε στο κεφάλι μας.

00:12:36.688 --> 00:12:38.704
(Γέλια)

00:12:38.800 --> 00:12:42.190
Το βαθύτερο πρόβλημα είναι
ότι φαίνεται ευκολότερο

00:12:42.190 --> 00:12:45.600
να φτάξεις ένα μεμονωμένο
σύστημα τεχνητής νοημοσύνης,

00:12:45.600 --> 00:12:47.776
από το να φτιάξεις ένα σύστημα

00:12:47.776 --> 00:12:49.552
και να έχεις και επαρκή νευροεπιστήμη

00:12:49.552 --> 00:12:52.472
που να επιτρέπει την εύκολη
ενσωμάτωσή του στον εγκέφαλό μας.

00:12:52.800 --> 00:12:55.976
Και δεδομένου ότι οι κυβερνήσεις
και εταιρείες που θα το κάνουν

00:12:55.976 --> 00:12:59.632
είναι πιθανόν θα θεωρούν
ότι ανταγωνίζονται όλους τους άλλους,

00:12:59.632 --> 00:13:02.668
εφόσον όποιος το κάνει πρώτος
θα κυριαρχήσει στον κόσμο,

00:13:02.668 --> 00:13:05.454
με την προϋπόθεση
ότι δεν θα τον καταστρέψει αμέσως μετά,

00:13:05.454 --> 00:13:06.590
τότε μοιάζει πιθανό

00:13:06.590 --> 00:13:09.780
ότι θα γίνει πρώτο
οτιδήποτε γίνεται πιο εύκολα.

00:13:10.560 --> 00:13:13.190
Δυστυχώς δεν έχω κάποια λύση
γι' αυτό το πρόβλημα,

00:13:13.190 --> 00:13:16.050
εκτός από το να συστήσω
να το ξανασκεφτούμε οι περισσότεροι.

00:13:16.050 --> 00:13:18.626
Νομίζω ότι χρειαζόμαστε
κάτι σαν το Πρόγραμμα Μανχάταν

00:13:18.626 --> 00:13:20.660
προσαρμοσμένο για την τεχνητή νοημοσύνη.

00:13:20.660 --> 00:13:23.386
Όχι για να την φτιάξουμε,
γιατί μάλλον είναι αναπόφευκτο,

00:13:23.386 --> 00:13:26.616
αλλά να καταλάβουμε πώς να αποφύγουμε
έναν αγώνα εξοπλισμών

00:13:26.616 --> 00:13:29.992
και να την φτιάξουμε έτσι που
να συμβαδίζει με τα συμφέροντά μας.

00:13:30.160 --> 00:13:32.296
Όταν μιλάτε
για υπερευφυή τεχνητή νοημοσύνη

00:13:32.296 --> 00:13:34.552
που μπορεί να τροποποιήσει τον εαυτό της,

00:13:34.600 --> 00:13:39.216
φαίνεται ότι έχουμε μόνο μία πιθανότητα
να ορίσουμε τις σωστές προϋποθέσεις,

00:13:39.216 --> 00:13:41.502
και ακόμη και τότε
θα χρειαστεί να απορροφήσουμε

00:13:41.502 --> 00:13:44.750
τις οικονομικές και πολιτικές συνέπειες
της σωστής εφαρμογής.

00:13:45.760 --> 00:13:47.816
Αλλά μόλις παραδεχθούμε

00:13:47.816 --> 00:13:52.246
ότι η επεξεργασία πληροφοριών
είναι η πηγή της ευφυΐας,

00:13:52.720 --> 00:13:57.690
ότι η βάση της ευφυΐας είναι
κάποιο κατάλληλο υπολογιστικό σύστημα,

00:13:58.360 --> 00:14:02.680
και αποδεχθούμε να βελτιώνουμε
αυτά τα συστήματα συνεχώς,

00:14:03.280 --> 00:14:07.736
και παραδεχθούμε ότι ο γνωστικός ορίζοντας
μάλλον ξεπερνά κατά πολύ

00:14:07.736 --> 00:14:09.516
όσα ήδη γνωρίζουμε,

00:14:10.120 --> 00:14:11.416
τότε πρέπει να παραδεχθούμε

00:14:11.416 --> 00:14:14.540
ότι βρισκόμαστε σε διαδικασία κατασκευής
κάποιου είδους θεού.

00:14:15.400 --> 00:14:16.746
Τώρα είναι η σωστή ώρα

00:14:16.746 --> 00:14:20.129
να εξασφαλίσουμε ότι είναι ένας θεός
με τον οποίον μπορούμε να ζήσουμε.

00:14:20.129 --> 00:14:21.360
Σας ευχαριστώ πολύ.

00:14:21.360 --> 00:14:22.913
(Χειροκρότημα)

