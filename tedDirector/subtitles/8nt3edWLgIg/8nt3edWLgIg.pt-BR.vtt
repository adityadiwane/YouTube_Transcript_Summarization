WEBVTT
Kind: captions
Language: pt-BR

00:00:00.000 --> 00:00:12.000
Tradutor: Laura Amaral
Revisor: Cristiano Garcia

00:00:13.000 --> 00:00:15.216
Vou falar sobre uma falha intuitiva

00:00:15.240 --> 00:00:16.840
que afeta muitos de nós.

00:00:17.480 --> 00:00:20.520
É a falha de reconhecer certo perigo.

00:00:21.360 --> 00:00:23.096
Eu vou descrever uma situação

00:00:23.120 --> 00:00:26.376
que considero aterrorizante

00:00:26.400 --> 00:00:28.160
e provável de acontecer,

00:00:28.840 --> 00:00:30.496
e não é uma boa combinação,

00:00:30.520 --> 00:00:31.916
ao que parece.

00:00:31.930 --> 00:00:33.600
Mesmo assim, em vez de sentir medo,

00:00:33.610 --> 00:00:36.640
a maioria vai achar legal o que vou falar.

00:00:37.200 --> 00:00:40.176
Vou descrever como nossas conquistas

00:00:40.200 --> 00:00:41.976
na área da inteligência artificial

00:00:42.000 --> 00:00:43.776
poderão nos destruir.

00:00:43.800 --> 00:00:47.256
Na verdade, acho muito difícil ver
como elas não vão nos destruir,

00:00:47.280 --> 00:00:48.960
ou nos inspirar a nos destruirmos.

00:00:49.400 --> 00:00:51.256
E se você é como eu,

00:00:51.280 --> 00:00:53.936
vai descobrir que é divertido
pensar sobre essas coisas.

00:00:53.960 --> 00:00:57.336
E essa resposta é parte do problema.

00:00:57.360 --> 00:00:59.080
OK? Essa resposta deve te preocupar.

00:00:59.920 --> 00:01:02.576
Se eu quisesse te convencer nessa palestra

00:01:02.600 --> 00:01:06.016
de que a fome mundial é provável,

00:01:06.040 --> 00:01:09.096
por causa do aquecimento global
ou qualquer outra catástrofe,

00:01:09.120 --> 00:01:12.536
e de que seus netos
e os netos dos seus netos

00:01:12.560 --> 00:01:14.360
provavelmente viverão assim,

00:01:15.200 --> 00:01:16.400
você não pensaria:

00:01:17.440 --> 00:01:18.776
"Interessante.

00:01:18.800 --> 00:01:20.270
Gostei dessa Palestra TED."

00:01:21.200 --> 00:01:22.720
Fome não é divertido.

00:01:23.800 --> 00:01:27.176
Morte por ficção científica,
por outro lado, é divertida,

00:01:27.200 --> 00:01:30.696
e uma das coisas que mais me preocupa
no desenvolvimento de IA nesse ponto

00:01:30.700 --> 00:01:35.296
é que somos incapazes de organizar
uma resposta emocional apropriada

00:01:35.320 --> 00:01:37.136
para os perigos que estão à frente.

00:01:37.160 --> 00:01:40.490
Eu sou incapaz de ordenar essa resposta,
e estou dando esta palestra.

00:01:42.120 --> 00:01:44.816
É como se estivéssemos
diante de duas portas.

00:01:44.840 --> 00:01:46.096
Na porta de número 1,

00:01:46.120 --> 00:01:49.416
paramos de progredir na área de IA.

00:01:49.440 --> 00:01:53.456
Nossos softwares e hardwares param
de melhorar por algum motivo.

00:01:53.480 --> 00:01:56.480
Pare um pouco para refletir
por que isso pode acontecer.

00:01:57.080 --> 00:02:00.736
Dado que automação
e inteligência são valiosos,

00:02:00.760 --> 00:02:04.280
continuaremos a melhorar
nossa tecnologia, se isso for possível.

00:02:05.200 --> 00:02:07.157
O que poderia nos impedir de fazer isso?

00:02:07.800 --> 00:02:09.600
Uma guerra nuclear em grande escala?

00:02:11.000 --> 00:02:12.560
Uma pandemia global?

00:02:14.320 --> 00:02:15.640
A colisão de um asteroide?

00:02:17.640 --> 00:02:20.216
Justin Bieber como presidente
dos Estado Unidos?

00:02:20.240 --> 00:02:23.050
(Risadas)

00:02:24.760 --> 00:02:28.680
A questão é que algo teria que destruir
a civilização que nós conhecemos.

00:02:29.360 --> 00:02:33.656
Você precisa imaginar
o quão ruim teria que ser

00:02:33.680 --> 00:02:37.016
para impedir os avanços
da nossa tecnologia

00:02:37.040 --> 00:02:38.256
permanentemente,

00:02:38.280 --> 00:02:40.296
geração após geração.

00:02:40.320 --> 00:02:42.456
Quase que por definição,
essa é a pior coisa

00:02:42.480 --> 00:02:44.496
que já aconteceu na história humana.

00:02:44.520 --> 00:02:45.816
A única alternativa,

00:02:45.840 --> 00:02:48.176
e é o que está atrás
da porta de número 2,

00:02:48.200 --> 00:02:51.336
é que continuemos a aprimorar
nossas máquinas inteligentes,

00:02:51.360 --> 00:02:52.960
ano após ano.

00:02:53.720 --> 00:02:57.360
Em algum ponto, construiremos máquinas
mais inteligentes que nós,

00:02:58.080 --> 00:03:00.696
e uma vez que isso acontecer,

00:03:00.720 --> 00:03:02.696
elas aperfeiçoarão a si mesmas.

00:03:02.720 --> 00:03:05.456
E nós arriscamos o que o matemático
I. J. Good chamou

00:03:05.480 --> 00:03:07.256
de "explosão de inteligência",

00:03:07.280 --> 00:03:09.280
que o processo poderia fugir do controle.

00:03:10.120 --> 00:03:12.936
Isso é frequentemente
caricaturado, desse jeito aqui,

00:03:12.960 --> 00:03:16.176
como medo de que exércitos
de robôs malignos

00:03:16.200 --> 00:03:17.456
nos ataquem.

00:03:17.480 --> 00:03:20.176
Mas essa não é a situação mais provável.

00:03:20.200 --> 00:03:25.056
Não é que nossas máquinas
se tornarão espontaneamente más.

00:03:25.080 --> 00:03:27.696
A preocupação é que
construiremos máquinas

00:03:27.720 --> 00:03:29.776
que são muito mais competentes do que nós,

00:03:29.800 --> 00:03:33.576
e que a menor das divergências
entre nosso propósito e o delas

00:03:33.600 --> 00:03:34.800
possa nos destruir.

00:03:35.960 --> 00:03:38.040
Pense na nossa relação com as formigas.

00:03:38.600 --> 00:03:40.090
Nós não as odiamos.

00:03:40.090 --> 00:03:42.486
Nós não as machucamos de propósito.

00:03:42.486 --> 00:03:44.736
Na verdade, nós tentamos
não machucar elas.

00:03:44.760 --> 00:03:46.776
Evitamos pisá-las na calçada.

00:03:46.800 --> 00:03:48.936
Mas sempre que a presença delas

00:03:48.960 --> 00:03:51.456
colide com um dos nossos objetivos,

00:03:51.480 --> 00:03:53.957
digamos que ao construirmos
um prédio como este,

00:03:53.981 --> 00:03:56.471
nós exterminamos elas
sem a menor cerimônia.

00:03:56.480 --> 00:03:59.416
A preocupação é que um dia
vamos construir máquinas

00:03:59.440 --> 00:04:02.176
que, conscientes ou não,

00:04:02.200 --> 00:04:04.200
poderão nos tratar com o mesmo desprezo.

00:04:05.660 --> 00:04:08.520
Eu suspeito que isso pareça difícil
de acreditar para vocês.

00:04:09.360 --> 00:04:15.696
Aposto que alguns de vocês duvidam
que uma IA superinteligente seja possível,

00:04:15.720 --> 00:04:17.376
muito menos inevitável.

00:04:17.400 --> 00:04:21.020
Mas então você precisar achar algo
de errado com as seguintes suposições.

00:04:21.044 --> 00:04:22.616
E existem apenas três.

00:04:23.620 --> 00:04:27.529
Inteligência é uma questão
de processamento de informação

00:04:27.529 --> 00:04:29.349
em sistemas físicos.

00:04:29.349 --> 00:04:31.935
Na verdade, isso é mais
do que uma suposição.

00:04:31.959 --> 00:04:35.416
Nós já construímos inteligência limitada
em nossas máquinas,

00:04:35.440 --> 00:04:40.166
e muitas delas já trabalham num nível
de inteligência sobre-humano.

00:04:40.840 --> 00:04:43.416
E sabemos que matéria comum

00:04:43.440 --> 00:04:46.116
pode dar origem ao que é chamado
de "inteligência geral",

00:04:46.116 --> 00:04:49.730
a habilidade de pensar de forma flexível
em vários domínios,

00:04:49.730 --> 00:04:52.956
porque nossos cérebros
conseguiram isso. Certo?

00:04:52.956 --> 00:04:56.856
Existem apenas átomos aqui,

00:04:56.880 --> 00:05:01.376
e desde que continuemos a construir
sistemas de átomos

00:05:01.400 --> 00:05:04.096
que mostrem mais e mais
comportamento inteligente,

00:05:04.120 --> 00:05:06.656
a não ser que nos interrompam,

00:05:06.680 --> 00:05:10.056
vamos acabar construindo
inteligência geral

00:05:10.080 --> 00:05:11.376
dentro de nossas máquinas.

00:05:11.400 --> 00:05:15.050
É crucial perceber que a taxa
de progresso não importa,

00:05:15.050 --> 00:05:18.426
porque qualquer progresso é suficiente
para nos levar até a zona final.

00:05:18.426 --> 00:05:20.596
Não precisamos da Lei de Moore
para continuar,

00:05:20.596 --> 00:05:23.716
Não precisamos de progresso exponencial,
só precisamos continuar.

00:05:25.480 --> 00:05:28.400
A segunda suposição é
que vamos continuar.

00:05:29.000 --> 00:05:31.760
Continuaremos a melhorar
nossas máquinas inteligentes.

00:05:33.000 --> 00:05:37.376
E dado o valor da inteligência...

00:05:37.400 --> 00:05:40.936
Inteligência ou é a fonte de tudo
aquilo que valorizamos

00:05:40.960 --> 00:05:43.736
ou precisamos dela para que proteja
aquilo que valorizamos.

00:05:43.760 --> 00:05:46.016
É a nossa fonte mais valiosa.

00:05:46.040 --> 00:05:47.576
Então nós queremos fazer isso.

00:05:47.600 --> 00:05:50.936
Temos problemas que precisamos
resolver urgentemente.

00:05:50.960 --> 00:05:54.160
Queremos curar doenças
como Alzheimer e câncer.

00:05:54.960 --> 00:05:58.896
Queremos entender sistemas econômicos.
Queremos aprimorar a ciência climática.

00:05:58.920 --> 00:06:01.176
Nós faremos isso, se pudermos.

00:06:01.200 --> 00:06:04.486
O trem já saiu da estação,
e não há freio para puxar.

00:06:05.880 --> 00:06:11.336
Finalmente, não estamos
em um pico de inteligência,

00:06:11.360 --> 00:06:13.160
não estamos nem perto disso.

00:06:13.640 --> 00:06:15.536
E essa é a percepção crucial.

00:06:15.560 --> 00:06:17.976
Isso é o que faz a nossa situação
ser tão precária

00:06:18.000 --> 00:06:22.040
e a nossa intuição sobre riscos
não ser confiável.

00:06:23.110 --> 00:06:25.920
Pense na pessoa mais
inteligente que já existiu.

00:06:26.720 --> 00:06:30.106
Na lista de quase todos aqui
está John von Neumann.

00:06:30.106 --> 00:06:33.416
A impressão que ele causou
nas pessoas ao seu redor,

00:06:33.440 --> 00:06:37.496
e isso inclui os melhores matemáticos
e físicos de seu tempo,

00:06:37.520 --> 00:06:39.456
está muito bem documentada.

00:06:39.480 --> 00:06:43.200
Se apenas metade das histórias
sobre ele forem meio verdadeiras,

00:06:43.200 --> 00:06:46.916
não há dúvida de que ele foi uma das
pessoas mais inteligentes que já existiu.

00:06:46.916 --> 00:06:49.520
Considere o espectro da inteligência.

00:06:50.320 --> 00:06:51.749
Aqui temos John von Neumann.

00:06:53.560 --> 00:06:54.894
E aqui temos eu e você.

00:06:56.120 --> 00:06:57.416
E aqui temos uma galinha.

00:06:57.440 --> 00:06:59.376
(Risadas)

00:06:59.400 --> 00:07:00.616
Perdão, uma galinha.

00:07:00.640 --> 00:07:01.896
(Risadas)

00:07:01.920 --> 00:07:05.806
Não existe razão pra que eu torne isso
mais depressivo do que precisa ser.

00:07:05.806 --> 00:07:07.310
(Risadas)

00:07:08.339 --> 00:07:11.876
Parece extremamente provável, no entanto,
que esse espectro da inteligência

00:07:11.876 --> 00:07:14.960
se estenda muito além do que pensamos,

00:07:15.880 --> 00:07:19.096
e se construirmos máquinas
mais inteligentes do que nós,

00:07:19.120 --> 00:07:21.416
elas provavelmente
vão explorar esse espectro

00:07:21.440 --> 00:07:23.296
de maneiras que não podemos imaginar,

00:07:23.320 --> 00:07:25.840
e vão nos ultrapassar
de formas inimagináveis.

00:07:27.000 --> 00:07:31.336
E é importante reconhecer que isso é
verdade apenas pela velocidade.

00:07:31.360 --> 00:07:36.416
Imagine se construíssemos
uma IA superinteligente

00:07:36.440 --> 00:07:39.896
que não fosse mais inteligente do que
seu time mediano de pesquisadores

00:07:39.920 --> 00:07:42.210
em Stanford ou no MIT.

00:07:42.210 --> 00:07:45.370
Circuitos eletrônicos funcionam quase
1 milhão de vezes mais rápido

00:07:45.370 --> 00:07:46.616
do que os biológicos,

00:07:46.616 --> 00:07:49.656
então essa máquina deve pensar
1 milhão de vezes mais rápido

00:07:49.680 --> 00:07:51.490
do que a mente que a criou.

00:07:51.490 --> 00:07:53.726
Você deixa ela funcionando
por uma semana,

00:07:53.726 --> 00:07:57.820
e ela vai realizar 20 mil anos humanos
de trabalho intelectual,

00:07:58.100 --> 00:08:00.430
semana após semana.

00:08:01.640 --> 00:08:04.736
Como poderíamos entender,
muito menos restringir,

00:08:04.760 --> 00:08:07.040
uma mente progredindo dessa maneira?

00:08:08.840 --> 00:08:10.976
Outra coisa que é
preocupante, francamente,

00:08:11.000 --> 00:08:15.976
é... imagine o melhor cenário possível.

00:08:16.000 --> 00:08:20.140
Imagine que nos deparamos
com o design de uma IA superinteligente

00:08:20.140 --> 00:08:21.826
que não tem problemas de segurança.

00:08:21.826 --> 00:08:24.856
Temos o design perfeito pela primeira vez.

00:08:24.880 --> 00:08:27.096
É como se tivéssemos recebido um oráculo

00:08:27.120 --> 00:08:29.136
que se comporta como deve.

00:08:29.160 --> 00:08:32.880
Essa máquina seria o perfeito
dispositivo de economia de trabalho.

00:08:33.130 --> 00:08:35.789
Pode desenvolver a máquina
que pode construir a máquina

00:08:35.789 --> 00:08:37.936
que pode executar
qualquer trabalho físico,

00:08:37.936 --> 00:08:39.376
movido à luz solar,

00:08:39.400 --> 00:08:42.096
por mais ou menos
o custo da matéria-prima.

00:08:42.120 --> 00:08:45.376
Estamos falando do fim
do esforço físico humano.

00:08:45.400 --> 00:08:48.660
Também estamos falando do fim
da maior parte do trabalho intelectual.

00:08:49.200 --> 00:08:52.256
O que símios como nós fariam
nessas circunstâncias?

00:08:52.280 --> 00:08:56.360
Estaríamos livres para jogar Frisbee
e massagear uns aos outros.

00:08:57.840 --> 00:09:00.696
Adicione LSD e escolhas
de roupas questionáveis,

00:09:00.720 --> 00:09:03.266
e o mundo inteiro seria
como o Festival Burning Man.

00:09:03.266 --> 00:09:04.970
(Risadas)

00:09:06.320 --> 00:09:08.320
Pode até parecer muito bom,

00:09:09.280 --> 00:09:11.656
mas se pergunte: o que aconteceria

00:09:11.680 --> 00:09:14.416
sob a atual economia e ordem política?

00:09:14.440 --> 00:09:16.856
Possivelmente, iríamos presenciar

00:09:16.880 --> 00:09:21.016
um nível de desigualdade
de riqueza e desemprego

00:09:21.040 --> 00:09:22.536
nunca visto antes.

00:09:22.560 --> 00:09:25.366
Sem a vontade de colocar
imediatamente essa riqueza

00:09:25.366 --> 00:09:26.680
a serviço da humanidade,

00:09:27.640 --> 00:09:31.256
alguns poucos trilionários apareceriam
nas capas das revistas de negócios,

00:09:31.280 --> 00:09:33.720
enquanto o resto do mundo passaria fome.

00:09:34.320 --> 00:09:36.616
E o que os russos e os chineses fariam

00:09:36.640 --> 00:09:39.256
se soubessem que uma empresa
no Vale do Silício

00:09:39.280 --> 00:09:42.016
estivesse pronta para implantar
uma IA superinteligente?

00:09:42.040 --> 00:09:44.896
Essa máquina seria capaz
de travar uma guerra,

00:09:44.920 --> 00:09:47.136
terrestre ou cibernética,

00:09:47.160 --> 00:09:48.840
com poder sem precedente.

00:09:50.120 --> 00:09:51.976
É um cenário onde o vencedor leva tudo.

00:09:52.000 --> 00:09:55.136
Estar seis meses à frente da competição

00:09:55.160 --> 00:09:57.936
é como estar 500 mil anos adiantado,

00:09:57.960 --> 00:09:59.456
no mínimo.

00:09:59.480 --> 00:10:04.216
Parece que mesmo os meros
rumores desse tipo de avanço

00:10:04.240 --> 00:10:06.616
poderiam causar fúria em nossa espécie.

00:10:06.640 --> 00:10:09.536
Uma das coisas mais assustadoras,

00:10:09.560 --> 00:10:12.336
na minha opinião, nesse momento,

00:10:12.360 --> 00:10:16.656
são os tipos de coisas
que os pesquisadores de IA dizem

00:10:16.680 --> 00:10:18.240
quando querem nos tranquilizar.

00:10:19.000 --> 00:10:22.456
E eles nos dizem para não
nos preocuparmos com o tempo.

00:10:22.480 --> 00:10:24.536
Esse é um caminho longo,
se você não sabe.

00:10:24.560 --> 00:10:27.000
Provavelmente está
a 50, 100 anos de distância.

00:10:27.720 --> 00:10:28.976
Um pesquisador disse:

00:10:29.000 --> 00:10:30.716
"Se preocupar com a segurança de IA

00:10:30.716 --> 00:10:33.170
é como se preocupar
com a superpopulação em Marte."

00:10:34.026 --> 00:10:35.730
Essa é a versão do Vale do Silício

00:10:35.730 --> 00:10:38.446
para que você "não esquente
sua cabecinha linda."

00:10:38.446 --> 00:10:39.496
(Risadas)

00:10:39.520 --> 00:10:41.416
Ninguém parece perceber

00:10:41.440 --> 00:10:44.056
que fazer referência ao tempo

00:10:44.080 --> 00:10:46.656
é um argumento inválido.

00:10:46.680 --> 00:10:49.936
Se inteligência é questão
de processamento de informação,

00:10:49.960 --> 00:10:52.616
e se continuarmos a aperfeiçoar
nossas máquinas,

00:10:52.640 --> 00:10:55.520
produziremos alguma forma
de superinteligência.

00:10:56.320 --> 00:10:59.976
E não temos ideia de quanto
tempo vai demorar

00:11:00.000 --> 00:11:02.400
para fazer isso de forma segura.

00:11:04.200 --> 00:11:05.496
Vou dizer novamente.

00:11:05.520 --> 00:11:09.336
Não temos ideia de quanto tempo vai levar

00:11:09.360 --> 00:11:11.600
para fazer isso de forma segura.

00:11:12.920 --> 00:11:16.376
E se você não percebeu, 50 anos não
são mais o que costumavam ser.

00:11:16.400 --> 00:11:18.856
Isso são 50 anos em meses.

00:11:18.880 --> 00:11:20.720
Esta é a idade que tem o iPhone.

00:11:21.440 --> 00:11:24.040
Este é o tempo que Os Simpsons
está na televisão.

00:11:24.680 --> 00:11:27.056
Cinquenta anos não é muito tempo

00:11:27.080 --> 00:11:30.240
para conhecer um dos maiores desafios
que nossa espécie enfrentará.

00:11:31.640 --> 00:11:35.656
Novamente, parece que estamos falhando
em uma reação emocional adequada

00:11:35.680 --> 00:11:38.376
a algo que, com certeza, está vindo.

00:11:38.400 --> 00:11:42.376
O cientista de computação Stuart Russell
possui uma bela analogia:

00:11:42.400 --> 00:11:47.236
imagine que recebamos uma mensagem
de uma civilização alienígena,

00:11:47.242 --> 00:11:48.138
que diz:

00:11:48.794 --> 00:11:50.520
"Pessoas da Terra,

00:11:50.630 --> 00:11:53.316
chegaremos ao seu planeta em 50 anos.

00:11:53.770 --> 00:11:55.090
Se preparem."

00:11:55.453 --> 00:11:59.543
E agora estamos contando os meses
até a chegada da nave-mãe?

00:11:59.543 --> 00:12:02.569
Nós teríamos mais urgência
do que temos agora.

00:12:04.340 --> 00:12:06.406
Outro motivo dito
para não nos preocuparmos

00:12:06.406 --> 00:12:09.842
é que essas máquinas não podem evitar
compartilhar nossos valores

00:12:09.842 --> 00:12:12.488
porque eles serão literalmente
extensões de nós mesmos.

00:12:12.488 --> 00:12:14.888
Eles serão transplantados
em nossos cérebros

00:12:14.888 --> 00:12:17.214
e nos tornaremos parte
de seus sistemas límbicos.

00:12:17.224 --> 00:12:18.970
Reserve um momento para considerar

00:12:18.970 --> 00:12:22.726
que o único caminho prudente
e seguro recomendado,

00:12:23.180 --> 00:12:26.556
é implantar essa tecnologia
diretamente em nossos cérebros.

00:12:26.556 --> 00:12:30.152
Esse pode ser o caminho
mais seguro e prudente,

00:12:30.330 --> 00:12:33.426
mas, normalmente, os problemas
de segurança da tecnologia

00:12:33.426 --> 00:12:36.752
precisam ser resolvidos antes de ser
implantado dentro da sua cabeça.

00:12:36.752 --> 00:12:38.478
(Risadas)

00:12:38.740 --> 00:12:43.576
O maior problema é que construir
uma IA superinteligente por si só

00:12:44.240 --> 00:12:46.096
parece ser mais fácil

00:12:46.096 --> 00:12:47.872
do que construir IA superinteligente

00:12:47.872 --> 00:12:49.832
e ter a neurociência completa

00:12:49.832 --> 00:12:52.798
que nos permita integrar
perfeitamente nossas mentes.

00:12:52.798 --> 00:12:56.234
E dado que empresas e governos
que estão trabalhando nisso

00:12:56.234 --> 00:12:59.490
se encontram numa corrida
uns contra os outros,

00:12:59.740 --> 00:13:02.366
dado que vencer essa corrida é
conquistar o mundo,

00:13:03.010 --> 00:13:05.626
supondo que você não o destrua
no momento seguinte,

00:13:05.626 --> 00:13:08.096
então parece que o que for
mais fácil de se fazer

00:13:08.096 --> 00:13:09.742
será feito primeiro.

00:13:10.620 --> 00:13:13.236
Infelizmente, eu não tenho
a solução para esse problema,

00:13:13.240 --> 00:13:15.866
a não ser recomendar
que outros pensem sobre isso.

00:13:15.866 --> 00:13:18.342
Acho que precisamos de algo
como o Projeto Manhattan

00:13:18.342 --> 00:13:20.278
na área de inteligência artificial.

00:13:20.278 --> 00:13:23.434
Não para construir uma IA,
porque vamos inevitavelmente fazer isso,

00:13:23.434 --> 00:13:26.580
mas para entender como evitar
uma corrida armamentista

00:13:26.580 --> 00:13:29.636
e construir uma IA que esteja
alinhada aos nossos interesses.

00:13:30.160 --> 00:13:32.416
Quando se fala de uma IA superinteligente

00:13:32.416 --> 00:13:34.702
que pode modificar a si mesma,

00:13:34.702 --> 00:13:39.148
parece que temos apenas uma chance
para acertar as condições primárias

00:13:39.280 --> 00:13:41.660
e, mesmo assim, teremos que assimilar

00:13:41.660 --> 00:13:44.726
as consequências econômicas
e políticas desse acerto.

00:13:45.940 --> 00:13:47.890
Mas no momento em que admitirmos

00:13:47.890 --> 00:13:52.680
que o processamento de informação
é a fonte da inteligência,

00:13:52.700 --> 00:13:57.810
que um sistema computacional apropriado
é a base do que é a inteligência,

00:13:58.464 --> 00:14:02.920
e admitirmos que continuaremos
a aprimorar esse sistema,

00:14:02.920 --> 00:14:07.460
e que o horizonte do conhecimento
provavelmente ultrapassa

00:14:07.550 --> 00:14:09.136
tudo o que sabemos agora,

00:14:10.240 --> 00:14:11.780
então teremos que admitir

00:14:11.780 --> 00:14:14.526
que estamos no processo de criação
de algum tipo de deus.

00:14:15.310 --> 00:14:17.723
Agora seria um ótimo momento
de ter a certeza

00:14:17.723 --> 00:14:19.679
de que é um deus
com o qual podemos viver.

00:14:20.100 --> 00:14:21.153
Muito obrigado.

00:14:21.153 --> 00:14:25.303
(Aplausos)

