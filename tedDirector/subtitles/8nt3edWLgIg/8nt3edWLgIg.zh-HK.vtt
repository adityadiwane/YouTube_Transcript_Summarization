WEBVTT
Kind: captions
Language: zh-HK

00:00:00.000 --> 00:00:07.000
Translator: 潘 可儿
Reviewer: Alan Watson

00:00:12.820 --> 00:00:17.286
我想講下
一個好多人都經歷過嘅感官錯覺

00:00:17.300 --> 00:00:18.740
當呢個錯覺嚟嗰陣

00:00:18.740 --> 00:00:21.180
我哋會唔識得留意危險

00:00:21.180 --> 00:00:26.140
我亦都想講下一個我認為駭人聽聞

00:00:26.140 --> 00:00:28.670
同時又好有可能會發生嘅情景

00:00:28.670 --> 00:00:31.486
呢個情景發生嘅話，唔係一件好事嚟

00:00:31.900 --> 00:00:34.356
你哋可能唔覺得我依家講緊嘅嘢恐怖

00:00:34.380 --> 00:00:36.460
反而覺得好型

00:00:37.020 --> 00:00:41.816
所以我想講下
我哋人類喺人工智能方面取得嘅成就

00:00:41.820 --> 00:00:43.596
最终會點樣摧毀我哋

00:00:43.620 --> 00:00:45.400
而事實上，我認為好難會見到

00:00:45.400 --> 00:00:48.780
佢哋唔會摧毀我哋
或者導致我哋自我毀滅

00:00:49.220 --> 00:00:51.076
依家你哋或者同我一樣

00:00:51.100 --> 00:00:53.756
覺得諗呢啲嘢好得意

00:00:53.780 --> 00:00:55.320
正因為覺得得意

00:00:55.320 --> 00:00:57.430
亦都成為咗問題嘅一部份

00:00:57.430 --> 00:00:59.830
你哋應該擔心你哋嘅反應至真！

00:00:59.830 --> 00:01:03.040
如果我喺呢場演講度話畀你哋聽

00:01:03.040 --> 00:01:05.926
因為氣候變化或者大災難嘅原因

00:01:05.926 --> 00:01:08.620
我哋會遭遇一場饑荒

00:01:08.620 --> 00:01:14.426
而你嘅孫，或者佢哋嘅孫
會好似咁樣生活

00:01:14.426 --> 00:01:15.990
你就唔會覺得

00:01:15.990 --> 00:01:20.040
「好有趣，我鍾意呢場 TED 演講。」

00:01:21.050 --> 00:01:22.660
饑荒一啲都唔有趣

00:01:23.470 --> 00:01:26.790
但科幻小說描繪嘅死亡就好有趣

00:01:26.800 --> 00:01:30.366
呢一刻，人工智能發展
最令我最困擾嘅係

00:01:30.366 --> 00:01:36.840
我哋面對近在眼前嘅危險似乎無動於衷

00:01:36.856 --> 00:01:38.826
雖然我喺你哋面前演講

00:01:38.826 --> 00:01:41.826
但我同你哋一樣都係冇反應

00:01:41.826 --> 00:01:44.490
成件事就好似我哋企喺兩道門前面

00:01:44.490 --> 00:01:48.696
喺一號門後面，我哋唔再發展智能機器

00:01:48.696 --> 00:01:50.310
因為某啲原因

00:01:50.310 --> 00:01:52.860
我哋電腦嘅硬件同軟件都停滯不前

00:01:52.860 --> 00:01:56.746
依家嚟諗一下點解呢種情況會發生

00:01:56.746 --> 00:02:00.730
即係話，因為智能同自動化好重要

00:02:00.730 --> 00:02:04.226
所以我哋會喺許可嘅情況之下
繼續改善科技

00:02:04.906 --> 00:02:07.340
咁究竟係乜嘢會阻止我哋？

00:02:07.353 --> 00:02:09.440
一個全面嘅核戰爭？

00:02:10.780 --> 00:02:12.680
一個全球流行病？

00:02:13.980 --> 00:02:15.710
一個小行星撞擊？

00:02:17.220 --> 00:02:19.840
Justin Bieber 做咗美國總統？

00:02:19.840 --> 00:02:21.756
（笑聲）

00:02:24.630 --> 00:02:27.150
之但係，如我哋所知

00:02:27.150 --> 00:02:29.300
有一啲嘢會摧毀文明

00:02:29.300 --> 00:02:31.180
你必須要想像

00:02:31.180 --> 00:02:37.696
如果我哋一代又一代人
永遠改善唔到科技

00:02:37.696 --> 00:02:39.906
情況會有幾嚴重

00:02:39.906 --> 00:02:41.430
幾乎可以確定嘅係

00:02:41.430 --> 00:02:44.006
呢個係人類史上最壞嘅事

00:02:44.006 --> 00:02:47.950
所以唯一嘅選擇
就係二號門後嘅做法

00:02:47.950 --> 00:02:52.886
我哋繼續年復一年升級改造智能機器

00:02:52.886 --> 00:02:54.260
到咗某個地步

00:02:54.260 --> 00:02:57.770
我哋就會整出比我哋仲要聰明嘅機器

00:02:57.770 --> 00:03:00.260
一旦我哋有咗比我哋自己
仲聰明嘅機器

00:03:00.260 --> 00:03:02.466
佢哋就會自我改良

00:03:02.466 --> 00:03:06.826
到時我哋就會面臨數學家 IJ Good 
講嘅「智能爆炸」危機

00:03:06.826 --> 00:03:09.786
即係話，改良過程唔再需要人類

00:03:09.786 --> 00:03:12.950
依家，經常會有人學呢張諷刺漫畫咁

00:03:12.950 --> 00:03:16.990
描繪叛變嘅機器人會攻擊我哋

00:03:16.990 --> 00:03:20.456
但係呢個唔係最有可能發生嘅情景

00:03:20.456 --> 00:03:24.746
我哋嘅機器唔會自動變惡

00:03:24.746 --> 00:03:29.896
所以問題在於我哋製造出
比我哋更加做到嘢嘅機器嘅時候

00:03:29.906 --> 00:03:34.256
佢哋目標上同我哋嘅細微分歧
會置我哋於死地

00:03:35.850 --> 00:03:38.510
就諗下我哋同螞蟻之間嘅關係︰

00:03:38.510 --> 00:03:39.900
我哋唔討厭佢哋

00:03:39.900 --> 00:03:42.494
我哋唔會傷害佢哋

00:03:42.494 --> 00:03:44.600
甚至我哋為咗唔傷害佢哋
而會受一啲苦

00:03:44.600 --> 00:03:46.334
例如我哋會為咗唔踩到佢哋
而跨過佢哋

00:03:46.334 --> 00:03:51.060
但係一旦佢哋嘅存在
同我哋嘅其中一個目標有嚴重衝突

00:03:51.060 --> 00:03:53.527
譬如話要起一棟咁樣嘅樓

00:03:53.527 --> 00:03:56.747
我哋諗都唔諗就殺死佢哋

00:03:56.747 --> 00:03:59.921
問題係，我哋終有一日整出嘅機器——

00:03:59.921 --> 00:04:01.996
無論佢哋自己有冇意識都好

00:04:01.996 --> 00:04:04.100
同樣會冷漠咁對待我哋

00:04:05.070 --> 00:04:08.850
依家，我估對於你哋大部份人嚟講
呢件情景都係遙不可及嘅

00:04:09.250 --> 00:04:15.060
我賭你哋當中有人質疑
超級智能嘅可能性

00:04:15.060 --> 00:04:17.260
更加唔好講
人類要避免超級智能

00:04:17.260 --> 00:04:20.810
但係你哋肯定會喺下面嘅假設當中
搵到一啲謬誤

00:04:20.810 --> 00:04:22.514
呢度一共有三個假設

00:04:23.764 --> 00:04:28.640
喺物理系統裏面，智能等如訊息處理

00:04:29.280 --> 00:04:31.190
但係，呢個超過咗假設

00:04:31.190 --> 00:04:35.269
因為我哋已經喺我哋嘅機器裏面
植入咗弱人工智能

00:04:35.269 --> 00:04:40.666
而且呢啲機器好多
已經處於一個超人類智能水平

00:04:40.666 --> 00:04:45.760
同時我哋知道僅僅係物質
就可以產生所謂嘅「一般智能」

00:04:45.760 --> 00:04:49.226
一種可以喺唔同領域之間
靈活思考嘅能力

00:04:49.226 --> 00:04:52.396
咁係因為我哋嘅大腦
已經可以做到，係唔係？

00:04:52.396 --> 00:04:56.596
我嘅意思係，大腦凈係得原子

00:04:56.596 --> 00:05:01.256
只要我哋繼續加設原子系統

00:05:01.256 --> 00:05:03.856
機器就可以有更加多智能行為

00:05:03.856 --> 00:05:06.846
除非進度有咩停頓

00:05:06.846 --> 00:05:10.956
否則我哋最終會喺機器裏面
建構出一般智能

00:05:10.966 --> 00:05:14.376
明白進度嘅快慢並唔影響係好重要

00:05:14.376 --> 00:05:18.026
因為任何過程都足以令我哋返唔到轉頭

00:05:18.026 --> 00:05:19.926
我哋唔需要按照摩爾定律進行

00:05:19.926 --> 00:05:21.946
我哋唔需要指數式增長

00:05:21.946 --> 00:05:23.506
我哋只需要繼續做

00:05:25.100 --> 00:05:28.450
第二個假設就係我哋會繼續做

00:05:28.450 --> 00:05:31.850
我哋會繼續改造我哋嘅智能機器

00:05:33.260 --> 00:05:36.910
而考慮到智能嘅價值…

00:05:36.910 --> 00:05:41.060
我係話，因為有智能
我哋至會珍重事物

00:05:41.060 --> 00:05:43.736
或者我哋需要智能
去保護我哋珍重嘅一切

00:05:43.736 --> 00:05:45.936
智能係我哋最有寶貴嘅資源

00:05:45.936 --> 00:05:48.020
所以我哋想繼續發展智能

00:05:48.020 --> 00:05:50.696
我哋有極需解決嘅問題

00:05:50.696 --> 00:05:54.366
例如我哋想治療類似阿茲海默症
同癌症嘅疾病

00:05:54.366 --> 00:05:56.430
我哋想認識經濟系統

00:05:56.430 --> 00:05:58.600
我哋想改善我哋嘅氣候科學

00:05:58.600 --> 00:06:01.186
所以如果可以做到嘅話
我哋會繼續發展智能

00:06:01.186 --> 00:06:04.476
件事亦都可以比喻為︰
列車已經開出，但冇刹車掣可以踩

00:06:05.656 --> 00:06:12.906
最終，我哋唔會去到
智能嘅頂峰或者高智能水平

00:06:13.546 --> 00:06:15.396
而呢個就係非常重要嘅觀察結果

00:06:15.396 --> 00:06:17.936
就係呢個結果
將我哋置於岌岌可危嘅境地

00:06:17.936 --> 00:06:22.746
亦令到我哋對於危險嘅觸覺唔可靠

00:06:22.746 --> 00:06:25.720
依家，就諗下史上最聰明嘅人

00:06:25.720 --> 00:06:30.130
幾乎喺每個人嘅名單上面
都會有 John von Neumann

00:06:30.130 --> 00:06:32.736
我嘅意思係 John von Neumann 
畀佢周圍嘅人嘅印象

00:06:32.736 --> 00:06:37.146
包括佢畀嗰個時代最犀利嘅數學家
同物理學家嘅印象

00:06:37.146 --> 00:06:39.120
都係有紀錄低嘅

00:06:39.120 --> 00:06:43.086
如果一半關於佢嘅故事有一半係真嘅

00:06:43.086 --> 00:06:44.176
咁毫無疑問

00:06:44.176 --> 00:06:46.186
佢係有史以來其中一個最聰明嘅人

00:06:46.186 --> 00:06:49.466
所以當我哋畫一幅比較智力嘅圖

00:06:49.466 --> 00:06:53.280
喺右邊高分嘅位置
我哋有 John von Neumann

00:06:53.280 --> 00:06:55.059
喺中間有你同我

00:06:56.089 --> 00:06:57.460
去到最左邊，我哋有雞仔

00:06:57.460 --> 00:06:58.636
（笑聲）

00:06:58.636 --> 00:07:00.726
係吖，就係一隻雞仔

00:07:00.726 --> 00:07:01.616
（笑聲）

00:07:01.616 --> 00:07:04.736
我冇理由將呢個演講搞到咁灰㗎

00:07:04.736 --> 00:07:08.380
（笑聲）

00:07:08.380 --> 00:07:15.370
但好有可能智力分佈
遠比我哋目前認知嘅廣

00:07:15.370 --> 00:07:18.570
如果我哋建造出
比我哋擁有更高智慧嘅機器

00:07:18.570 --> 00:07:23.386
佢哋嘅智力好有可能會
超越我哋認知嘅最高智力

00:07:23.386 --> 00:07:26.310
同埋以無法想像嘅方式超越我哋

00:07:26.960 --> 00:07:30.990
同樣重要嘅係
單憑運算速度就可以超越我哋

00:07:30.990 --> 00:07:33.066
啱唔啱？諗下如果我哋整咗一個

00:07:33.066 --> 00:07:41.706
冇哈佛或者麻省理工研究人員
咁聰明嘅超級人工智能

00:07:41.706 --> 00:07:46.146
但電路運行速度大概
比生化電路快一百萬倍

00:07:46.146 --> 00:07:51.266
所以呢個機器嘅思考速度應該會
比佢嘅創造者快大概一百萬倍

00:07:51.266 --> 00:07:53.116
所以如果佢運行一個星期

00:07:53.116 --> 00:08:00.196
佢就可以完成人類要兩萬年
先至完成得到嘅工作

00:08:01.230 --> 00:08:02.544
而我哋又點會明白

00:08:02.544 --> 00:08:07.180
人工智能係點樣完成咁龐大嘅運算呢？

00:08:08.430 --> 00:08:13.550
另一個令人擔憂嘅事，老實講

00:08:13.550 --> 00:08:16.306
就係…不如想像一下最好嘅情形

00:08:16.306 --> 00:08:21.380
想像一下我哋設計咗一個
冇安全問題嘅超級人工智能

00:08:21.380 --> 00:08:24.766
我哋第一次擁有完美嘅設計

00:08:24.766 --> 00:08:29.036
就好似我哋摞住
一個按照預期發展嘅神諭

00:08:30.020 --> 00:08:33.400
呢個機器仲會變成完美嘅慳力設備

00:08:33.400 --> 00:08:35.700
事關機器可以生產另一款機器出嚟

00:08:35.700 --> 00:08:37.503
做任何體力勞動

00:08:37.503 --> 00:08:39.216
兼由太陽能驅動

00:08:39.216 --> 00:08:41.646
成本仲同買原材料差唔多

00:08:41.646 --> 00:08:45.126
所以，我哋唔單止講緊咕哩勞力嘅終結

00:08:45.126 --> 00:08:48.436
我哋同時講緊大部份用腦工作嘅終結

00:08:49.016 --> 00:08:52.080
咁我哋人類面對工作削減
應該何去何從？

00:08:52.080 --> 00:08:56.766
我哋會好自由咁去掟飛盤 、同人按摩

00:08:56.766 --> 00:09:00.510
服食一啲 LSD 精神藥
同埋著上怪異服飾

00:09:00.510 --> 00:09:02.716
於是成個世界都會變成火人節嘅人咁

00:09:02.716 --> 00:09:04.846
（笑聲）

00:09:06.416 --> 00:09:08.520
頭先講到嘅嘢聽起上嚟好似好好咁

00:09:09.040 --> 00:09:10.370
但係撫心自問

00:09:10.370 --> 00:09:14.006
面對目前嘅經濟政治秩序
乜嘢會發生呢？

00:09:14.426 --> 00:09:16.636
似乎我哋會目睹

00:09:16.636 --> 00:09:22.156
我哋從未見過咁嚴重嘅
貧富懸殊同失業率

00:09:22.156 --> 00:09:27.266
如果呢筆新財富唔即時用嚟服務全人類

00:09:27.266 --> 00:09:30.990
就算一啲億萬富翁使好多錢
㨘靚商業雜誌嘅封面

00:09:30.990 --> 00:09:33.716
世界上其他人都要挨餓

00:09:33.716 --> 00:09:35.740
咁如果俄羅斯人或者中國人

00:09:35.740 --> 00:09:40.990
聽到矽谷嘅一啲公司
打算使用一個超級人工智能

00:09:40.990 --> 00:09:42.150
佢哋會點諗？

00:09:42.150 --> 00:09:47.036
呢個機器有能力用未見過嘅力度
發動地面或者網絡戰爭

00:09:47.036 --> 00:09:48.980
呢個係「勝者全取」嘅情況

00:09:49.940 --> 00:09:54.826
喺呢場人工智能較量中有六個月嘅優勢

00:09:54.826 --> 00:09:59.186
就係至少要做多人類五十萬年做到嘅嘢

00:09:59.186 --> 00:10:05.986
甚至只係關於人工智能突破嘅謠言
就可以令到人類亂起上嚟

00:10:06.740 --> 00:10:11.820
依家最驚人嘅一件事，我覺得

00:10:11.820 --> 00:10:17.806
就係人工智能研究人員
安定人心時講嘅說話

00:10:18.676 --> 00:10:22.070
佢哋成日話，因為我哋有時間
所以我哋唔需要擔心

00:10:22.070 --> 00:10:24.456
「乜你唔知有排咩？

00:10:24.456 --> 00:10:26.820
仲有五十年或者一百年先到。」

00:10:27.410 --> 00:10:28.830
一位研究人員曾經咁講︰

00:10:28.830 --> 00:10:32.820
「擔心人工智能嘅安全就好似
擔心火星人口爆棚一樣。」

00:10:33.858 --> 00:10:35.804
呢句嘢等如矽谷同你講︰

00:10:35.804 --> 00:10:38.014
「你十八廿二就杞人憂天！」

00:10:38.014 --> 00:10:38.970
（笑聲）

00:10:38.970 --> 00:10:45.406
冇人意識到
攞時間嚟到講完全係無稽之談

00:10:45.906 --> 00:10:49.556
如果智能凈係同處理訊息有關

00:10:49.556 --> 00:10:52.446
同埋我哋繼續改良我哋嘅機器嘅話

00:10:52.446 --> 00:10:55.936
我哋最終會生產到超級智能

00:10:55.936 --> 00:10:58.580
但我哋唔知道要用幾長時間

00:10:58.580 --> 00:11:02.716
先可以生產安全嘅超級智能

00:11:04.076 --> 00:11:05.240
等我再講多一次

00:11:05.240 --> 00:11:08.336
我哋唔知道要用幾長時間

00:11:08.336 --> 00:11:11.316
先可以生產安全嘅超級智能

00:11:12.996 --> 00:11:14.110
如果你仲未意識到

00:11:14.110 --> 00:11:16.520
五十年嘅概念已經唔同咗喇

00:11:16.520 --> 00:11:18.804
呢幅圖顯示咗以月份計嘅五十年

00:11:18.804 --> 00:11:21.190
先係 iPhone 面世至今嘅時間

00:11:21.190 --> 00:11:24.300
再係阿森一族出現係電視上嘅時間

00:11:24.300 --> 00:11:30.210
五十年不足以畀人類應對最大挑戰

00:11:31.380 --> 00:11:35.060
再一次，我哋對於有理由發生嘅事

00:11:35.060 --> 00:11:38.066
未有採取適當嘅情緒反應

00:11:38.066 --> 00:11:42.186
對此，電腦科學家 Stuart Russell 
有一個好嘅比喻

00:11:42.186 --> 00:11:46.966
佢話︰想像一下我哋收到
一個來自外星文明嘅信息

00:11:46.966 --> 00:11:48.026
上面寫住：

00:11:48.026 --> 00:11:50.016
「地球上嘅人類，

00:11:50.016 --> 00:11:53.486
我哋五十年之後會到達你哋嘅星球。

00:11:53.486 --> 00:11:54.420
請準備好。」

00:11:55.310 --> 00:11:58.456
咁我哋依家凈係會倒數外星人來臨？

00:11:59.376 --> 00:12:02.426
我哋應該更加緊張至係

00:12:04.446 --> 00:12:07.220
另一個我哋被告知唔使擔心嘅原因係

00:12:07.220 --> 00:12:09.456
呢啲機器只會識得
將我哋嘅價值觀傳開

00:12:09.456 --> 00:12:11.776
因為佢哋係我哋人類嘅附屬嘅一部分

00:12:11.776 --> 00:12:14.156
但同時佢哋會被植入我哋嘅大腦

00:12:14.156 --> 00:12:16.516
所以我哋會成為佢哋嘅邊緣系統

00:12:16.516 --> 00:12:21.180
依家使啲時間諗下
最安全同唯一審慎嘅做法

00:12:21.180 --> 00:12:25.950
而推薦嘅做法就係
直接將呢種科技植入我哋嘅大腦

00:12:26.500 --> 00:12:29.760
呢種做法可能係最安全同唯一審慎嘅

00:12:29.760 --> 00:12:33.066
但係喺你將佢植入你個腦之前

00:12:33.066 --> 00:12:36.706
科技嘅安全問題需要解決

00:12:36.706 --> 00:12:38.526
（笑聲）

00:12:38.526 --> 00:12:40.160
更深一層嘅問題係

00:12:40.160 --> 00:12:43.880
人工智能自己整超級人工智能

00:12:43.880 --> 00:12:47.696
似乎比整一個可以喺神經科學上

00:12:47.696 --> 00:12:51.440
同我哋腦部無縫接合嘅
超級人工智能簡單

00:12:52.940 --> 00:12:59.160
考慮到從事研發人工智能嘅公司
同政府好可能會互相競爭

00:12:59.160 --> 00:13:02.540
考慮到要贏呢場比賽就要贏成個世界

00:13:02.540 --> 00:13:05.394
同埋先假設如果你下一刻
唔會糟塌人工智能嘅成果

00:13:05.394 --> 00:13:09.670
咁樣，似乎更加簡單嘅事會完成咗先

00:13:10.310 --> 00:13:12.940
但唔好彩嘅係
我除咗叫大家反思呢個問題

00:13:12.940 --> 00:13:15.296
我就再冇辦法解決呢個問題

00:13:15.296 --> 00:13:17.536
我覺得我哋喺人工智能方面

00:13:17.536 --> 00:13:19.726
需要好似「曼哈頓計劃」咁嘅計劃

00:13:20.236 --> 00:13:23.896
唔係講點樣整人工智能
因為我認為人工智能終有一日會整到

00:13:23.896 --> 00:13:26.236
而係搞清楚點樣避免一場軍備競賽

00:13:26.236 --> 00:13:29.510
同埋往符合我哋利益嘅方向
發展人工智能

00:13:29.960 --> 00:13:34.446
當你講緊可以自我改造嘅超級人工智能

00:13:34.446 --> 00:13:38.946
我哋似乎只有一個機會
令到人工智能發展得安全

00:13:38.946 --> 00:13:40.086
就算發展得安全

00:13:40.086 --> 00:13:44.766
我哋都要接受
人工智能對經濟同政治產生嘅結果

00:13:45.626 --> 00:13:52.180
但係當我哋同意
訊息處理係智能嘅起步點

00:13:52.180 --> 00:13:57.980
同意一啲適當嘅計算系統係智能嘅基礎

00:13:57.980 --> 00:14:02.850
同意我哋會不斷完善人工智能

00:14:02.850 --> 00:14:09.670
同意將來有好多嘢超越我哋認知嘅

00:14:09.670 --> 00:14:14.950
咁我哋就必須要承認
我哋正喺度創造緊某種神明

00:14:14.950 --> 00:14:19.290
依家會係一個好時機
確保佢係可以同我哋共存嘅神明

00:14:19.850 --> 00:14:21.073
好多謝你哋

00:14:21.073 --> 00:14:23.116
（掌聲）

