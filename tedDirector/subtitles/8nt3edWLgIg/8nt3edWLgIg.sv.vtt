WEBVTT
Kind: captions
Language: sv

00:00:00.000 --> 00:00:07.000
Översättare: Anette Smedberg
Granskare: Lisbeth Pekkari

00:00:13.000 --> 00:00:15.216
Jag ska prata om en intuitionsbrist

00:00:15.240 --> 00:00:16.840
som många av oss lider av.

00:00:17.480 --> 00:00:20.520
Det är ett misslyckande att
upptäcka en viss typ av fara.

00:00:21.360 --> 00:00:23.096
Jag ska beskriva ett scenario

00:00:23.120 --> 00:00:26.376
som jag tycker är både skrämmande

00:00:26.400 --> 00:00:28.160
och förmodligen kommer att ske,

00:00:28.840 --> 00:00:30.496
och det är en dålig kombination,

00:00:30.520 --> 00:00:32.056
som det visar sig.

00:00:32.080 --> 00:00:34.536
Istället för att bli rädda,
kommer många tycka

00:00:34.560 --> 00:00:36.640
att det jag pratar om är ganska häftigt.

00:00:37.200 --> 00:00:40.176
Jag ska beskriva hur de framsteg vi gör

00:00:40.200 --> 00:00:41.976
inom artificiell intelligens

00:00:42.000 --> 00:00:43.776
till slut kan förgöra oss.

00:00:43.800 --> 00:00:47.256
Faktum är, jag tror det är svårt 
att se att de inte gör det

00:00:47.280 --> 00:00:49.160
eller får oss att förgöra oss själva.

00:00:49.400 --> 00:00:51.256
Och om du tänker det minsta likt mig,

00:00:51.280 --> 00:00:53.936
tycker du att det här är
spännande att tänka på.

00:00:53.960 --> 00:00:57.336
Den inställningen är en del av problemet.

00:00:57.360 --> 00:00:59.340
OK? Den inställningen borde oroa dig.

00:00:59.920 --> 00:01:02.576
Om jag i det här föredraget
skulle övertyga dig om

00:01:02.600 --> 00:01:06.016
att vi kan drabbas av en
global svältkatastrof,

00:01:06.040 --> 00:01:09.096
på grund av klimatförändringar
eller någon annan katastrof,

00:01:09.120 --> 00:01:12.536
och att dina barnbarn,
eller deras barnbarn,

00:01:12.560 --> 00:01:14.360
sannolikt skulle leva så här,

00:01:15.200 --> 00:01:16.400
då skulle du inte tänka,

00:01:17.440 --> 00:01:18.776
"Intressant.

00:01:18.800 --> 00:01:20.000
Jag gillar föredraget."

00:01:21.200 --> 00:01:22.720
Svält är inte roligt.

00:01:23.800 --> 00:01:27.176
Död i science fiction,
å andra sidan, är roligt,

00:01:27.200 --> 00:01:31.176
och en av de saker som för närvarande
oroar mig mest i utvecklingen av AI

00:01:31.200 --> 00:01:35.296
är att vi tycks oförmögna att förespråka
ett passande emotionellt gensvar

00:01:35.320 --> 00:01:37.546
inför de risker som ligger framför oss.

00:01:37.546 --> 00:01:40.990
Jag kan inte förespråka ett gensvar,
och jag håller det här talet.

00:01:42.120 --> 00:01:44.816
Det är som att vi står framför två dörrar.

00:01:44.840 --> 00:01:46.096
Väljer vi dörr ett,

00:01:46.120 --> 00:01:49.416
slutar vi göra framsteg i utvecklandet
av intelligenta maskiner.

00:01:49.440 --> 00:01:53.456
Av någon anledning blir vår
hård- och mjukvara inte bättre.

00:01:53.480 --> 00:01:56.480
Fundera lite på varför
det skulle kunna hända.

00:01:57.080 --> 00:02:00.736
Jag menar, givet värdet av
intelligens och automation,

00:02:00.760 --> 00:02:04.280
kommer vi fortsätta utveckla
tekniken, om vi kan.

00:02:05.200 --> 00:02:06.867
Vad skulle kunna stoppa oss?

00:02:07.800 --> 00:02:09.600
Ett fullskaligt kärnvapenkrig?

00:02:11.000 --> 00:02:12.560
En global pandemi?

00:02:14.320 --> 00:02:15.640
Ett asteroidnedslag?

00:02:17.640 --> 00:02:20.216
Justin Bieber blir USAs president?

00:02:20.240 --> 00:02:22.520
(Skratt)

00:02:24.760 --> 00:02:28.680
Poängen är att, något måste förgöra
civilisationen så som vi känner den.

00:02:29.360 --> 00:02:33.656
Ni måste föreställa er
hur illa det måste vara

00:02:33.680 --> 00:02:37.016
för att hindra oss att göra
framsteg inom tekniken

00:02:37.040 --> 00:02:38.256
permanent,

00:02:38.280 --> 00:02:40.296
generation efter generation.

00:02:40.320 --> 00:02:42.456
Nästan per definition, är detta det värsta

00:02:42.480 --> 00:02:44.496
som någonsin inträffat genom historien.

00:02:44.520 --> 00:02:45.816
Så det enda alternativet,

00:02:45.840 --> 00:02:48.176
är det som finns bakom den andra dörren,

00:02:48.200 --> 00:02:51.336
att vi fortsätter att utveckla
våra intelligenta maskiner

00:02:51.360 --> 00:02:52.960
år efter år efter år.

00:02:53.720 --> 00:02:57.360
Vid en viss punkt, kommer vi att bygga
maskiner som är smartare än vi,

00:02:58.080 --> 00:03:00.696
och när vi väl har dessa smarta maskiner,

00:03:00.720 --> 00:03:02.696
kommer de att förbättra sig själva.

00:03:02.720 --> 00:03:05.456
Då riskerar vi
det matematikern IJ Good kallade,

00:03:05.480 --> 00:03:07.256
en "intelligensexplosion",

00:03:07.280 --> 00:03:09.280
dvs att processen går oss ur händerna.

00:03:10.120 --> 00:03:12.936
Detta skojas ofta bort, som här,

00:03:12.960 --> 00:03:16.080
som en rädsla för att en
armé av ondsinta robotar

00:03:16.080 --> 00:03:17.456
kommer att attackera oss.

00:03:17.480 --> 00:03:20.176
Men det är inte det troligaste scenariot.

00:03:20.200 --> 00:03:25.056
Våra maskiner kommer inte
att bli spontant illvilliga.

00:03:25.080 --> 00:03:27.696
Bekymret är att vi kommer
att bygga maskiner

00:03:27.720 --> 00:03:29.776
som är så mycket mer kompetenta än vi

00:03:29.800 --> 00:03:33.576
att den minsta lilla skillnad
mellan deras mål och våra

00:03:33.600 --> 00:03:35.080
skulle kunna förgöra oss.

00:03:35.960 --> 00:03:38.040
Tänk bara på vår inställning till myror.

00:03:38.600 --> 00:03:40.256
Vi hatar inte dem.

00:03:40.280 --> 00:03:42.336
Vi skadar inte dem medvetet.

00:03:42.360 --> 00:03:44.736
Ibland anstränger vi oss
för att inte skada dem.

00:03:44.760 --> 00:03:46.776
Vi kliver över dem på trottoaren.

00:03:46.800 --> 00:03:48.936
Men så fort deras närvaro

00:03:48.960 --> 00:03:51.456
kommer i allvarlig konflikt med våra mål,

00:03:51.480 --> 00:03:53.957
som till exempel vid husbyggnationer,

00:03:53.981 --> 00:03:55.941
förintar vi dem utan pardon.

00:03:56.480 --> 00:03:59.416
Bekymret är att vi en dag
kommer att bygga maskiner

00:03:59.440 --> 00:04:02.176
som, oavsett om de är medvetna eller ej,

00:04:02.200 --> 00:04:04.400
skulle kunna behandla oss
med samma förakt.

00:04:05.760 --> 00:04:08.520
Jag misstänker att det verkar
långsökt för många av er.

00:04:09.360 --> 00:04:15.696
Jag slår vad om att en del av er tvivlar
på att superintelligent AI är möjlig,

00:04:15.720 --> 00:04:17.376
än mindre nödvändig.

00:04:17.400 --> 00:04:21.020
Men då måste ni finna felen med
några av följande antaganden.

00:04:21.044 --> 00:04:22.616
Det finns bara tre.

00:04:23.800 --> 00:04:28.519
Intelligens handlar om processandet
av information i fysiska system.

00:04:29.320 --> 00:04:31.935
Det är egentligen mer
än bara ett antagande.

00:04:31.959 --> 00:04:35.416
Vi använder redan maskininlärning
i många maskiner,

00:04:35.440 --> 00:04:37.456
och många av dessa maskiner presterar

00:04:37.480 --> 00:04:40.120
redan på nivåer långt över
mänsklig intelligens.

00:04:40.840 --> 00:04:43.416
Och vi vet att materia

00:04:43.440 --> 00:04:46.056
kan skapa det som
kallas "generell intelligens"

00:04:46.080 --> 00:04:49.736
en förmåga att tänka flexibelt
över flera dimensioner,

00:04:49.760 --> 00:04:52.896
eftersom våra hjärnor
har klarat det. Eller hur?

00:04:52.920 --> 00:04:56.856
Jag menar, det är ju bara atomer här inne,

00:04:56.880 --> 00:05:01.376
och så länge vi fortsätter att
bygga system av atomer

00:05:01.400 --> 00:05:04.096
som uppvisar mer och mer intelligens,

00:05:04.120 --> 00:05:06.656
kommer vi till slut,
om vi inte blir avbrutna,

00:05:06.680 --> 00:05:10.056
ha byggt in generell intelligens

00:05:10.080 --> 00:05:11.376
i våra maskiner.

00:05:11.400 --> 00:05:15.056
Det är viktigt att inse att
utvecklingstakten inte spelar någon roll,

00:05:15.080 --> 00:05:18.256
eftersom minsta framsteg
räcker för att nå slutstationen.

00:05:18.280 --> 00:05:22.056
Vi behöver inte Moores lag. Utvecklingen
behöver inte vara exponentiell.

00:05:22.080 --> 00:05:23.870
Vi behöver bara fortsätta framåt.

00:05:25.480 --> 00:05:28.400
Det andra antagandet är
att vi kommer fortsätta.

00:05:29.000 --> 00:05:31.910
Vi fortsätter utvecklingen
av de intelligenta maskinerna.

00:05:33.000 --> 00:05:37.376
Och givet värdet på intelligens -

00:05:37.400 --> 00:05:40.936
jag menar, intelligens är antingen
summan av allt vi sätter värde på

00:05:40.960 --> 00:05:43.736
eller som vi behöver för att
skydda det vi värdesätter.

00:05:43.760 --> 00:05:46.016
Det är vår värdefullaste resurs.

00:05:46.040 --> 00:05:47.576
Så vi vill göra det här.

00:05:47.600 --> 00:05:50.936
Vi har problem som vi desperat måste lösa.

00:05:50.960 --> 00:05:54.160
Vi vill bota sjukdomar som
Alzheimers och cancer.

00:05:54.960 --> 00:05:58.896
Vi vill förstå de ekonomiska systemen.
Vi vill utveckla klimatforskningen.

00:05:58.920 --> 00:06:01.176
Så vi kommer att göra det, om vi kan.

00:06:01.200 --> 00:06:04.486
Tåget har redan lämnat stationen,
och det finns ingen broms.

00:06:05.880 --> 00:06:11.336
Slutligen, vi har inte
nått intelligensens topp,

00:06:11.360 --> 00:06:13.160
vi är troligen inte ens i närheten.

00:06:13.640 --> 00:06:15.536
Det är den riktigt kritiska insikten.

00:06:15.560 --> 00:06:18.146
Det är det här som gör
vår situation så vansklig,

00:06:18.146 --> 00:06:22.040
och det gör också våra
riskantaganden så opålitliga.

00:06:23.120 --> 00:06:25.840
Fundera på den smartaste person
som någonsin levt.

00:06:26.640 --> 00:06:30.056
En som finns på nästan allas lista
är John von Neumann.

00:06:30.080 --> 00:06:33.416
Jag menar, de intryck som Neumann
gjorde på folk runt honom,

00:06:33.440 --> 00:06:37.496
inkluderat de främsta matematikerna
och fysikerna under hans levnad,

00:06:37.520 --> 00:06:39.456
är ganska väldokumenterade.

00:06:39.480 --> 00:06:43.256
Om bara hälften av berättelserna
om honom är halvsanna,

00:06:43.280 --> 00:06:44.666
råder det ingen tvekan om

00:06:44.666 --> 00:06:46.976
att han är en av de smartaste någonsin.

00:06:47.000 --> 00:06:49.520
Så fundera på vidden av intelligens.

00:06:50.320 --> 00:06:51.749
Här har vi John von Neumann.

00:06:53.560 --> 00:06:54.894
Så har vi dig och mig.

00:06:56.120 --> 00:06:57.416
Så har vi kycklingar.

00:06:57.440 --> 00:06:59.376
(Skratt)

00:06:59.400 --> 00:07:00.616
Ursäkta, en kyckling.

00:07:00.640 --> 00:07:01.896
(Skratt)

00:07:01.920 --> 00:07:05.656
Det finns inga skäl att göra föredraget
mer deprimerande än det redan är.

00:07:05.680 --> 00:07:07.280
(Skratt)

00:07:08.339 --> 00:07:11.816
Det verkar dock mycket troligt, 
att spektrat av intelligens,

00:07:11.840 --> 00:07:14.960
sträcker sig mycket längre
än vi kan föreställa oss idag,

00:07:15.880 --> 00:07:19.096
och om vi bygger maskiner
som är intelligentare än vi,

00:07:19.120 --> 00:07:21.416
kommer de troligtvis
utforska detta spektrum

00:07:21.440 --> 00:07:23.296
på sätt som vi inte kan föreställa oss,

00:07:23.320 --> 00:07:25.840
och överträffa oss på
sätt som vi inte kan förstå.

00:07:27.000 --> 00:07:31.336
Det är viktigt att se att det här är sant
genom hastigheten det sker med.

00:07:31.360 --> 00:07:36.416
Ok? Föreställ er att vi
byggt en superintelligent AI

00:07:36.440 --> 00:07:39.896
som inte är smartare än din
genomsnittliga forskargrupp

00:07:39.920 --> 00:07:42.216
på Stanford eller MIT.

00:07:42.240 --> 00:07:45.096
Elektroniska kretsar är ungefär
en miljon gånger snabbare

00:07:45.096 --> 00:07:46.496
än biokemiska,

00:07:46.520 --> 00:07:49.656
så maskinen skulle tänka ungefär
en miljon gånger snabbare

00:07:49.680 --> 00:07:51.496
än hjärnorna som byggt den.

00:07:51.520 --> 00:07:53.176
Ni låter den köra i en vecka,

00:07:53.200 --> 00:07:57.760
och den kommer att processa
20 000 år av mänskligt tänkande,

00:07:58.400 --> 00:08:00.360
vecka ut och vecka in.

00:08:01.640 --> 00:08:04.736
Hur kan vi ens förstå, än mindre begränsa

00:08:04.760 --> 00:08:07.040
en kraft som gör sådana framsteg?

00:08:08.840 --> 00:08:10.976
Det andra som är uppriktigt oroande,

00:08:11.000 --> 00:08:15.976
är, föreställ dig ett bästa scenario.

00:08:16.000 --> 00:08:20.176
Föreställ dig att vi designar
en superintelligent AI

00:08:20.200 --> 00:08:21.576
utan säkerhetsproblem.

00:08:21.600 --> 00:08:24.856
Vi har den perfekta designen från början.

00:08:24.880 --> 00:08:27.096
Det är som att vi skulle få ett orakel

00:08:27.120 --> 00:08:29.136
som beter sig exakt som vi avsett.

00:08:29.160 --> 00:08:32.880
Den här maskinen skulle vara
den perfekta "jobbspararen".

00:08:33.680 --> 00:08:36.109
Den kan designa maskinen
som kan bygga maskinen

00:08:36.133 --> 00:08:37.896
som kan utföra allt fysiskt arbete,

00:08:37.920 --> 00:08:39.376
och drivs av solenergi,

00:08:39.400 --> 00:08:42.096
mer eller mindre till
kostnaden för råmaterialet.

00:08:42.120 --> 00:08:45.376
Då pratar vi om slutet
av mänsklighetens slit.

00:08:45.400 --> 00:08:48.360
Och även om slutet för mycket
av intellektuellt arbete.

00:08:49.200 --> 00:08:52.256
Så vad ska apor som vi göra
under sådana omständigheter?

00:08:52.280 --> 00:08:56.360
Ja, vi får tid över att kasta frisbee
och ge varandra massage.

00:08:57.840 --> 00:09:00.696
Lägg till lite LSD och några
underliga klädesplagg,

00:09:00.720 --> 00:09:02.896
och hela världen kan bli som Burning Man.

00:09:02.920 --> 00:09:04.560
(Skratt)

00:09:06.320 --> 00:09:08.320
Ok, det kan låta ganska trevligt,

00:09:09.280 --> 00:09:11.656
men ställ dig frågan vad som skulle ske

00:09:11.680 --> 00:09:14.416
under våra nuvarande
ekonomiska och politiska system?

00:09:14.440 --> 00:09:16.856
Det verkar troligt att vi skulle bevittna

00:09:16.880 --> 00:09:21.016
enorma skillnader i välfärd
och tillgång till arbete

00:09:21.040 --> 00:09:22.666
som vi aldrig tidigare upplevt.

00:09:22.666 --> 00:09:25.176
Saknas en vilja att
låta detta nya välstånd

00:09:25.200 --> 00:09:26.680
tjäna hela mänskligheten,

00:09:27.640 --> 00:09:31.256
kommer några triljonärer att pryda
omslagen på våra affärstidningar

00:09:31.280 --> 00:09:33.720
medan resten av världen kan få svälta.

00:09:34.320 --> 00:09:36.616
Vad skulle ryssarna eller kineserna göra

00:09:36.640 --> 00:09:39.256
om de hörde att ett
företag i Silicon Valley

00:09:39.280 --> 00:09:42.016
höll på att sjösätta en
superintelligent AI?

00:09:42.040 --> 00:09:44.896
Den maskinen skulle vara
kapabel att föra krig,

00:09:44.920 --> 00:09:47.136
antingen på land eller i cyberrymden,

00:09:47.160 --> 00:09:48.990
med aldrig tidigare skådad kraft.

00:09:50.120 --> 00:09:52.146
Det är ett vinnaren-tar-allt scenario.

00:09:52.146 --> 00:09:55.136
Att ligga sex månader före i den tävlingen

00:09:55.160 --> 00:09:57.936
är att vara 500 000 år före,

00:09:57.960 --> 00:09:59.456
minst.

00:09:59.480 --> 00:10:04.216
Det ser ut som att blott rykten
kring denna typ av genombrott

00:10:04.240 --> 00:10:06.616
kan leda till att vår art går bärsärk.

00:10:06.640 --> 00:10:09.536
En av de mest skrämmande sakerna,

00:10:09.560 --> 00:10:12.336
enligt min mening, i dag,

00:10:12.360 --> 00:10:16.656
är det som forskare inom AI säger

00:10:16.680 --> 00:10:18.240
när de vill lugna oss.

00:10:19.000 --> 00:10:22.616
Det vanligaste argumentet för att
vi inte ska oroa oss handlar om tid.

00:10:22.616 --> 00:10:24.536
Det här ligger långt fram, vet du väl.

00:10:24.560 --> 00:10:27.000
Det här är säkert 50 eller 100 år bort.

00:10:27.720 --> 00:10:28.976
En forskare har sagt,

00:10:29.000 --> 00:10:30.736
"Att oroa sig för AI-säkerhet är

00:10:30.736 --> 00:10:33.070
som att oroa sig för
ett överbefolkat Mars."

00:10:34.116 --> 00:10:35.736
Det är Silicon Valley-versionen av

00:10:35.760 --> 00:10:38.136
"oroa inte ditt söta lilla huvud."

00:10:38.160 --> 00:10:39.496
(Skratt)

00:10:39.520 --> 00:10:41.416
Ingen verkar uppmärksamma att,

00:10:41.440 --> 00:10:44.056
att referera till tidsaspekten

00:10:44.080 --> 00:10:46.656
är ett "Goddag yxskaft"-svar.

00:10:46.680 --> 00:10:49.936
Om intelligens bara handlar
om att processa information,

00:10:49.960 --> 00:10:52.616
och vi fortsätter att
utveckla våra maskiner,

00:10:52.640 --> 00:10:55.640
kommer vi till slut skapa
någon form av superintelligens.

00:10:56.320 --> 00:10:59.976
Vi har ingen aning om
hur lång tid det kommer att ta

00:11:00.000 --> 00:11:02.400
att skapa villkoren för
att göra det riskfritt.

00:11:04.200 --> 00:11:05.496
Jag tar det en gång till.

00:11:05.520 --> 00:11:09.336
Vi vet inte hur lång tid det kommer att ta

00:11:09.360 --> 00:11:11.600
för att skapa de säkra villkoren.

00:11:12.920 --> 00:11:16.546
Du har kanske inte noterat det,
men 50 år är inte vad det brukade vara.

00:11:16.546 --> 00:11:18.856
Det här är 50 år i månader.

00:11:18.880 --> 00:11:20.720
Så här länge har vi haft iPhone.

00:11:21.440 --> 00:11:24.040
Så här länge har "Simpsons" visats på TV.

00:11:24.680 --> 00:11:27.056
50 år är inte så lång tid

00:11:27.080 --> 00:11:30.390
att möta den största utmaningen
mänskligheten någonsin skådat.

00:11:31.640 --> 00:11:35.656
Än en gång, vi verkar inte kunna uppbåda
ett passande emetionellt gensvar

00:11:35.680 --> 00:11:38.376
för den framtid vi har all
anledning att tro kommer.

00:11:38.400 --> 00:11:42.376
Datorforskaren Stuart Russell
drar en intressant parallell.

00:11:42.400 --> 00:11:47.296
Han sade, föreställ er att vi fick ett
meddelande från en främmande civilisation,

00:11:47.320 --> 00:11:49.016
som sade:

00:11:49.040 --> 00:11:50.576
"Människor på Jorden,

00:11:50.600 --> 00:11:52.960
vi kommer att landa
på er planet om 50 år.

00:11:53.800 --> 00:11:55.376
Gör er redo."

00:11:55.400 --> 00:11:59.656
Och nu räknar vi bara ner månaderna
tills moderskeppet landar?

00:11:59.680 --> 00:12:02.680
Vi borde förstå att det
brådskar mer än vad vi gör.

00:12:04.680 --> 00:12:06.536
Vi hör att vi inte behöver oroa oss

00:12:06.560 --> 00:12:09.576
för maskinerna kommer
att dela våra värderingar

00:12:09.600 --> 00:12:12.216
eftersom de kommer att
vara en förlängning av oss.

00:12:12.240 --> 00:12:14.056
De kommer att ympas in i våra hjärnor,

00:12:14.080 --> 00:12:16.440
och vi kommer att vara
deras limbiska system.

00:12:17.120 --> 00:12:18.536
Reflektera nu över

00:12:18.560 --> 00:12:21.736
att den tryggaste och
förståndigaste vägen framåt,

00:12:21.760 --> 00:12:23.096
som förespråkas,

00:12:23.120 --> 00:12:25.920
är att operera in tekniken
direkt i våra hjärnor.

00:12:26.600 --> 00:12:29.976
Det kanske är den tryggaste
och smartaste vägen att ta,

00:12:30.000 --> 00:12:33.056
men vanligtvis måste
säkerhetsriskerna med en teknik

00:12:33.080 --> 00:12:36.736
vara ordentligt utredda innan
man stoppar in den i hjärnan.

00:12:36.760 --> 00:12:38.776
(Skratt)

00:12:38.800 --> 00:12:44.136
Det djupare problemet är att
utveckla en fristående superintelligent AI

00:12:44.160 --> 00:12:45.896
förmodligen är enklare

00:12:45.920 --> 00:12:47.776
än att utveckla superintelligent AI

00:12:47.800 --> 00:12:49.576
och ha koll på neurovetenskapen

00:12:49.600 --> 00:12:52.400
som låter oss sömlöst
integrera våra sinnen med den.

00:12:52.800 --> 00:12:55.976
Givet att företag och
stater som står för vetenskapen

00:12:56.000 --> 00:12:59.656
sannolikt kommer att uppfatta sig
vara i kamp med alla andra,

00:12:59.680 --> 00:13:02.986
givet att genom att vinna kapplöpningen
vinner man världen,

00:13:02.986 --> 00:13:05.416
förutsatt att de inte
förstör den i nästa steg,

00:13:05.440 --> 00:13:08.056
då verkar det troligt att
allt som är enklare att göra

00:13:08.080 --> 00:13:09.280
kommer att göras först.

00:13:10.560 --> 00:13:13.416
Tyvärr har jag ingen lösning
på det här problemet,

00:13:13.440 --> 00:13:15.740
mer än att fler av oss
måste fundera på det.

00:13:15.740 --> 00:13:18.456
Jag tror vi behöver ett Manhattan-projekt

00:13:18.480 --> 00:13:20.496
på temat artificiell intelligens.

00:13:20.520 --> 00:13:23.526
Inte för att bygga den,
det tror jag oundvikligen vi gör,

00:13:23.526 --> 00:13:26.616
men för att förstå hur vi ska
undvika en kapprustning

00:13:26.640 --> 00:13:30.136
och för att utveckla den i linje
med våra egna intressen.

00:13:30.160 --> 00:13:32.296
När man pratar om superintelligent AI

00:13:32.320 --> 00:13:34.576
som kan förändra sig själv,

00:13:34.600 --> 00:13:39.216
verkar det som att vi bara har
en chans att få det rätt från början,

00:13:39.240 --> 00:13:41.296
och även då måste vi sätta oss in i

00:13:41.320 --> 00:13:44.360
och lösa de ekonomiska och
politiska konsekvenserna.

00:13:45.760 --> 00:13:47.816
Men i den stund vi tillstår

00:13:47.840 --> 00:13:51.840
att informationsbearbetning
är källan till intelligens,

00:13:52.720 --> 00:13:57.520
att något datorstyrt beräkningsprogram
är basen för intelligens,

00:13:58.360 --> 00:14:02.120
och att vi kommer att förbättra
dessa system kontinuerligt,

00:14:03.280 --> 00:14:07.736
och vi erkänner att
kognitionshorisonten vida överstiger

00:14:07.760 --> 00:14:08.960
vad vi vet idag,

00:14:10.120 --> 00:14:11.336
då måste vi också erkänna

00:14:11.360 --> 00:14:14.000
att vi är i färd med att
skapa någon form av gud.

00:14:15.400 --> 00:14:16.976
Nu borde vara ett bra tillfälle att

00:14:17.000 --> 00:14:18.953
se till att det är en gud vi vill ha.

00:14:20.120 --> 00:14:21.656
Tack så mycket.

00:14:21.680 --> 00:14:24.633
(Applåder)

