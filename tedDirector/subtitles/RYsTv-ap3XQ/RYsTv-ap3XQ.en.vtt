WEBVTT
Kind: captions
Language: en

00:00:18.330 --> 00:00:24.330
If you ask people about what part of psychology do they think is hard,

00:00:24.330 --> 00:00:27.330
and you say, "Well, what about thinking and emotions?"

00:00:27.330 --> 00:00:30.330
Most people will say, "Emotions are terribly hard.

00:00:30.330 --> 00:00:36.330
They're incredibly complex. They can't -- I have no idea of how they work.

00:00:36.330 --> 00:00:38.330
But thinking is really very straightforward:

00:00:38.330 --> 00:00:42.330
it's just sort of some kind of logical reasoning, or something.

00:00:42.330 --> 00:00:45.330
But that's not the hard part."

00:00:45.330 --> 00:00:47.330
So here's a list of problems that come up.

00:00:47.330 --> 00:00:50.330
One nice problem is, what do we do about health?

00:00:50.330 --> 00:00:54.330
The other day, I was reading something, and the person said

00:00:54.330 --> 00:01:00.330
probably the largest single cause of disease is handshaking in the West.

00:01:00.330 --> 00:01:04.330
And there was a little study about people who don't handshake,

00:01:04.330 --> 00:01:07.330
and comparing them with ones who do handshake.

00:01:07.330 --> 00:01:12.330
And I haven't the foggiest idea of where you find the ones that don't handshake,

00:01:12.330 --> 00:01:15.330
because they must be hiding.

00:01:15.330 --> 00:01:19.330
And the people who avoid that

00:01:19.330 --> 00:01:23.330
have 30 percent less infectious disease or something.

00:01:23.330 --> 00:01:26.330
Or maybe it was 31 and a quarter percent.

00:01:26.330 --> 00:01:30.330
So if you really want to solve the problem of epidemics and so forth,

00:01:30.330 --> 00:01:34.330
let's start with that. And since I got that idea,

00:01:34.330 --> 00:01:38.330
I've had to shake hundreds of hands.

00:01:38.330 --> 00:01:43.330
And I think the only way to avoid it

00:01:43.330 --> 00:01:45.330
is to have some horrible visible disease,

00:01:45.330 --> 00:01:48.330
and then you don't have to explain.

00:01:48.330 --> 00:01:52.330
Education: how do we improve education?

00:01:52.330 --> 00:01:56.330
Well, the single best way is to get them to understand

00:01:56.330 --> 00:01:59.330
that what they're being told is a whole lot of nonsense.

00:01:59.330 --> 00:02:01.330
And then, of course, you have to do something

00:02:01.330 --> 00:02:06.330
about how to moderate that, so that anybody can -- so they'll listen to you.

00:02:06.330 --> 00:02:10.330
Pollution, energy shortage, environmental diversity, poverty.

00:02:10.330 --> 00:02:14.330
How do we make stable societies? Longevity.

00:02:14.330 --> 00:02:17.330
Okay, there're lots of problems to worry about.

00:02:17.330 --> 00:02:19.330
Anyway, the question I think people should talk about --

00:02:19.330 --> 00:02:24.330
and it's absolutely taboo -- is, how many people should there be?

00:02:24.330 --> 00:02:31.330
And I think it should be about 100 million or maybe 500 million.

00:02:31.330 --> 00:02:36.330
And then notice that a great many of these problems disappear.

00:02:36.330 --> 00:02:38.330
If you had 100 million people

00:02:38.330 --> 00:02:44.330
properly spread out, then if there's some garbage,

00:02:44.330 --> 00:02:51.330
you throw it away, preferably where you can't see it, and it will rot.

00:02:51.330 --> 00:02:56.330
Or you throw it into the ocean and some fish will benefit from it.

00:02:56.330 --> 00:02:58.330
The problem is, how many people should there be?

00:02:58.330 --> 00:03:01.330
And it's a sort of choice we have to make.

00:03:01.330 --> 00:03:04.330
Most people are about 60 inches high or more,

00:03:04.330 --> 00:03:08.330
and there's these cube laws. So if you make them this big,

00:03:08.330 --> 00:03:11.330
by using nanotechnology, I suppose --

00:03:11.330 --> 00:03:12.330
(Laughter)

00:03:12.330 --> 00:03:14.330
-- then you could have a thousand times as many.

00:03:14.330 --> 00:03:16.330
That would solve the problem, but I don't see anybody

00:03:16.330 --> 00:03:19.330
doing any research on making people smaller.

00:03:19.330 --> 00:03:24.330
Now, it's nice to reduce the population, but a lot of people want to have children.

00:03:24.330 --> 00:03:27.330
And there's one solution that's probably only a few years off.

00:03:27.330 --> 00:03:32.330
You know you have 46 chromosomes. If you're lucky, you've got 23

00:03:32.330 --> 00:03:38.330
from each parent. Sometimes you get an extra one or drop one out,

00:03:38.330 --> 00:03:42.330
but -- so you can skip the grandparent and great-grandparent stage

00:03:42.330 --> 00:03:47.330
and go right to the great-great-grandparent. And you have 46 people

00:03:47.330 --> 00:03:50.330
and you give them a scanner, or whatever you need,

00:03:50.330 --> 00:03:54.330
and they look at their chromosomes and each of them says

00:03:54.330 --> 00:03:59.330
which one he likes best, or she -- no reason to have just two sexes

00:03:59.330 --> 00:04:04.330
any more, even. So each child has 46 parents,

00:04:04.330 --> 00:04:10.330
and I suppose you could let each group of 46 parents have 15 children.

00:04:10.330 --> 00:04:12.330
Wouldn't that be enough? And then the children

00:04:12.330 --> 00:04:16.330
would get plenty of support, and nurturing, and mentoring,

00:04:16.330 --> 00:04:18.330
and the world population would decline very rapidly

00:04:18.330 --> 00:04:21.330
and everybody would be totally happy.

00:04:21.330 --> 00:04:24.330
Timesharing is a little further off in the future.

00:04:24.330 --> 00:04:27.330
And there's this great novel that Arthur Clarke wrote twice,

00:04:27.330 --> 00:04:31.330
called "Against the Fall of Night" and "The City and the Stars."

00:04:31.330 --> 00:04:34.330
They're both wonderful and largely the same,

00:04:34.330 --> 00:04:36.330
except that computers happened in between.

00:04:36.330 --> 00:04:41.330
And Arthur was looking at this old book, and he said, "Well, that was wrong.

00:04:41.330 --> 00:04:43.330
The future must have some computers."

00:04:43.330 --> 00:04:48.330
So in the second version of it, there are 100 billion

00:04:48.330 --> 00:04:56.330
or 1,000 billion people on Earth, but they're all stored on hard disks or floppies,

00:04:56.330 --> 00:04:58.330
or whatever they have in the future.

00:04:58.330 --> 00:05:02.330
And you let a few million of them out at a time.

00:05:02.330 --> 00:05:06.330
A person comes out, they live for a thousand years

00:05:06.330 --> 00:05:12.330
doing whatever they do, and then, when it's time to go back

00:05:12.330 --> 00:05:16.330
for a billion years -- or a million, I forget, the numbers don't matter --

00:05:16.330 --> 00:05:20.330
but there really aren't very many people on Earth at a time.

00:05:20.330 --> 00:05:22.330
And you get to think about yourself and your memories,

00:05:22.330 --> 00:05:27.330
and before you go back into suspension, you edit your memories

00:05:27.330 --> 00:05:30.330
and you change your personality and so forth.

00:05:30.330 --> 00:05:36.330
The plot of the book is that there's not enough diversity,

00:05:36.330 --> 00:05:39.330
so that the people who designed the city

00:05:39.330 --> 00:05:43.330
make sure that every now and then an entirely new person is created.

00:05:43.330 --> 00:05:49.330
And in the novel, a particular one named Alvin is created. And he says,

00:05:49.330 --> 00:05:53.330
maybe this isn't the best way, and wrecks the whole system.

00:05:53.330 --> 00:05:55.330
I don't think the solutions that I proposed

00:05:55.330 --> 00:05:58.330
are good enough or smart enough.

00:05:58.330 --> 00:06:02.330
I think the big problem is that we're not smart enough

00:06:02.330 --> 00:06:06.330
to understand which of the problems we're facing are good enough.

00:06:06.330 --> 00:06:10.330
Therefore, we have to build super intelligent machines like HAL.

00:06:10.330 --> 00:06:15.330
As you remember, at some point in the book for "2001,"

00:06:15.330 --> 00:06:20.330
HAL realizes that the universe is too big, and grand, and profound

00:06:20.330 --> 00:06:24.330
for those really stupid astronauts. If you contrast HAL's behavior

00:06:24.330 --> 00:06:28.330
with the triviality of the people on the spaceship,

00:06:28.330 --> 00:06:31.330
you can see what's written between the lines.

00:06:31.330 --> 00:06:34.330
Well, what are we going to do about that? We could get smarter.

00:06:34.330 --> 00:06:39.330
I think that we're pretty smart, as compared to chimpanzees,

00:06:39.330 --> 00:06:45.330
but we're not smart enough to deal with the colossal problems that we face,

00:06:45.330 --> 00:06:47.330
either in abstract mathematics

00:06:47.330 --> 00:06:52.330
or in figuring out economies, or balancing the world around.

00:06:52.330 --> 00:06:55.330
So one thing we can do is live longer.

00:06:55.330 --> 00:06:57.330
And nobody knows how hard that is,

00:06:57.330 --> 00:07:00.330
but we'll probably find out in a few years.

00:07:00.330 --> 00:07:03.330
You see, there's two forks in the road. We know that people live

00:07:03.330 --> 00:07:07.330
twice as long as chimpanzees almost,

00:07:07.330 --> 00:07:11.330
and nobody lives more than 120 years,

00:07:11.330 --> 00:07:14.330
for reasons that aren't very well understood.

00:07:14.330 --> 00:07:17.330
But lots of people now live to 90 or 100,

00:07:17.330 --> 00:07:21.330
unless they shake hands too much or something like that.

00:07:21.330 --> 00:07:26.330
And so maybe if we lived 200 years, we could accumulate enough skills

00:07:26.330 --> 00:07:31.330
and knowledge to solve some problems.

00:07:31.330 --> 00:07:33.330
So that's one way of going about it.

00:07:33.330 --> 00:07:36.330
And as I said, we don't know how hard that is. It might be --

00:07:36.330 --> 00:07:42.330
after all, most other mammals live half as long as the chimpanzee,

00:07:42.330 --> 00:07:45.330
so we're sort of three and a half or four times, have four times

00:07:45.330 --> 00:07:51.330
the longevity of most mammals. And in the case of the primates,

00:07:51.330 --> 00:07:55.330
we have almost the same genes. We only differ from chimpanzees,

00:07:55.330 --> 00:08:01.330
in the present state of knowledge, which is absolute hogwash,

00:08:01.330 --> 00:08:03.330
maybe by just a few hundred genes.

00:08:03.330 --> 00:08:06.330
What I think is that the gene counters don't know what they're doing yet.

00:08:06.330 --> 00:08:09.330
And whatever you do, don't read anything about genetics

00:08:09.330 --> 00:08:12.330
that's published within your lifetime, or something.

00:08:12.330 --> 00:08:15.330
(Laughter)

00:08:15.330 --> 00:08:19.330
The stuff has a very short half-life, same with brain science.

00:08:19.330 --> 00:08:25.330
And so it might be that if we just fix four or five genes,

00:08:25.330 --> 00:08:27.330
we can live 200 years.

00:08:27.330 --> 00:08:30.330
Or it might be that it's just 30 or 40,

00:08:30.330 --> 00:08:32.330
and I doubt that it's several hundred.

00:08:32.330 --> 00:08:36.330
So this is something that people will be discussing

00:08:36.330 --> 00:08:39.330
and lots of ethicists -- you know, an ethicist is somebody

00:08:39.330 --> 00:08:42.330
who sees something wrong with whatever you have in mind.

00:08:42.330 --> 00:08:45.330
(Laughter)

00:08:45.330 --> 00:08:49.330
And it's very hard to find an ethicist who considers any change

00:08:49.330 --> 00:08:53.330
worth making, because he says, what about the consequences?

00:08:53.330 --> 00:08:56.330
And, of course, we're not responsible for the consequences

00:08:56.330 --> 00:09:02.330
of what we're doing now, are we? Like all this complaint about clones.

00:09:02.330 --> 00:09:05.330
And yet two random people will mate and have this child,

00:09:05.330 --> 00:09:09.330
and both of them have some pretty rotten genes,

00:09:09.330 --> 00:09:13.330
and the child is likely to come out to be average.

00:09:13.330 --> 00:09:19.330
Which, by chimpanzee standards, is very good indeed.

00:09:19.330 --> 00:09:22.330
If we do have longevity, then we'll have to face the population growth

00:09:22.330 --> 00:09:26.330
problem anyway. Because if people live 200 or 1,000 years,

00:09:26.330 --> 00:09:32.330
then we can't let them have a child more than about once every 200 or 1,000 years.

00:09:32.330 --> 00:09:35.330
And so there won't be any workforce.

00:09:35.330 --> 00:09:39.330
And one of the things Laurie Garrett pointed out, and others have,

00:09:39.330 --> 00:09:44.330
is that a society that doesn't have people

00:09:44.330 --> 00:09:47.330
of working age is in real trouble. And things are going to get worse,

00:09:47.330 --> 00:09:53.330
because there's nobody to educate the children or to feed the old.

00:09:53.330 --> 00:09:55.330
And when I'm talking about a long lifetime, of course,

00:09:55.330 --> 00:10:01.330
I don't want somebody who's 200 years old to be like our image

00:10:01.330 --> 00:10:05.330
of what a 200-year-old is -- which is dead, actually.

00:10:05.330 --> 00:10:07.330
You know, there's about 400 different parts of the brain

00:10:07.330 --> 00:10:09.330
which seem to have different functions.

00:10:09.330 --> 00:10:12.330
Nobody knows how most of them work in detail,

00:10:12.330 --> 00:10:16.330
but we do know that there're lots of different things in there.

00:10:16.330 --> 00:10:18.330
And they don't always work together. I like Freud's theory

00:10:18.330 --> 00:10:22.330
that most of them are cancelling each other out.

00:10:22.330 --> 00:10:26.330
And so if you think of yourself as a sort of city

00:10:26.330 --> 00:10:32.330
with a hundred resources, then, when you're afraid, for example,

00:10:32.330 --> 00:10:36.330
you may discard your long-range goals, but you may think deeply

00:10:36.330 --> 00:10:40.330
and focus on exactly how to achieve that particular goal.

00:10:40.330 --> 00:10:43.330
You throw everything else away. You become a monomaniac --

00:10:43.330 --> 00:10:47.330
all you care about is not stepping out on that platform.

00:10:47.330 --> 00:10:51.330
And when you're hungry, food becomes more attractive, and so forth.

00:10:51.330 --> 00:10:57.330
So I see emotions as highly evolved subsets of your capability.

00:10:57.330 --> 00:11:01.330
Emotion is not something added to thought. An emotional state

00:11:01.330 --> 00:11:05.330
is what you get when you remove 100 or 200

00:11:05.330 --> 00:11:08.330
of your normally available resources.

00:11:08.330 --> 00:11:11.330
So thinking of emotions as the opposite of -- as something

00:11:11.330 --> 00:11:15.330
less than thinking is immensely productive. And I hope,

00:11:15.330 --> 00:11:19.330
in the next few years, to show that this will lead to smart machines.

00:11:19.330 --> 00:11:22.330
And I guess I better skip all the rest of this, which are some details

00:11:22.330 --> 00:11:27.330
on how we might make those smart machines and --

00:11:27.330 --> 00:11:32.330
(Laughter)

00:11:32.330 --> 00:11:37.330
-- and the main idea is in fact that the core of a really smart machine

00:11:37.330 --> 00:11:42.330
is one that recognizes that a certain kind of problem is facing you.

00:11:42.330 --> 00:11:45.330
This is a problem of such and such a type,

00:11:45.330 --> 00:11:50.330
and therefore there's a certain way or ways of thinking

00:11:50.330 --> 00:11:52.330
that are good for that problem.

00:11:52.330 --> 00:11:56.330
So I think the future, main problem of psychology is to classify

00:11:56.330 --> 00:12:00.330
types of predicaments, types of situations, types of obstacles

00:12:00.330 --> 00:12:06.330
and also to classify available and possible ways to think and pair them up.

00:12:06.330 --> 00:12:09.330
So you see, it's almost like a Pavlovian --

00:12:09.330 --> 00:12:11.330
we lost the first hundred years of psychology

00:12:11.330 --> 00:12:14.330
by really trivial theories, where you say,

00:12:14.330 --> 00:12:20.330
how do people learn how to react to a situation? What I'm saying is,

00:12:20.330 --> 00:12:25.330
after we go through a lot of levels, including designing

00:12:25.330 --> 00:12:28.330
a huge, messy system with thousands of ports,

00:12:28.330 --> 00:12:32.330
we'll end up again with the central problem of psychology.

00:12:32.330 --> 00:12:35.330
Saying, not what are the situations,

00:12:35.330 --> 00:12:37.330
but what are the kinds of problems

00:12:37.330 --> 00:12:40.330
and what are the kinds of strategies, how do you learn them,

00:12:40.330 --> 00:12:43.330
how do you connect them up, how does a really creative person

00:12:43.330 --> 00:12:48.330
invent a new way of thinking out of the available resources and so forth.

00:12:48.330 --> 00:12:50.330
So, I think in the next 20 years,

00:12:50.330 --> 00:12:55.330
if we can get rid of all of the traditional approaches to artificial intelligence,

00:12:55.330 --> 00:12:57.330
like neural nets and genetic algorithms

00:12:57.330 --> 00:13:03.330
and rule-based systems, and just turn our sights a little bit higher to say,

00:13:03.330 --> 00:13:05.330
can we make a system that can use all those things

00:13:05.330 --> 00:13:09.330
for the right kind of problem? Some problems are good for neural nets;

00:13:09.330 --> 00:13:12.330
we know that others, neural nets are hopeless on them.

00:13:12.330 --> 00:13:15.330
Genetic algorithms are great for certain things;

00:13:15.330 --> 00:13:19.330
I suspect I know what they're bad at, and I won't tell you.

00:13:19.330 --> 00:13:20.330
(Laughter)

00:13:20.330 --> 00:13:22.330
Thank you.

00:13:22.330 --> 00:13:28.330
(Applause)

